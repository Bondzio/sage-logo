<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SMM</journal-id>
<journal-id journal-id-type="hwp">spsmm</journal-id>
<journal-id journal-id-type="nlm-ta">Stat Methods Med Res</journal-id>
<journal-title>Statistical Methods in Medical Research</journal-title>
<issn pub-type="ppub">0962-2802</issn>
<issn pub-type="epub">1477-0334</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0962280212448970</article-id>
<article-id pub-id-type="publisher-id">10.1177_0962280212448970</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Bayesian non-parametric Potts model with application to pre-surgical FMRI data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Johnson</surname><given-names>Timothy D</given-names></name>
<xref ref-type="aff" rid="aff1-0962280212448970">1</xref>
<xref ref-type="corresp" rid="corresp1-0962280212448970"/>
</contrib>
<contrib contrib-type="author">
<name><surname>Liu</surname><given-names>Zhuqing</given-names></name>
<xref ref-type="aff" rid="aff1-0962280212448970">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bartsch</surname><given-names>Andreas J</given-names></name>
<xref ref-type="aff" rid="aff2-0962280212448970">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Nichols</surname><given-names>Thomas E</given-names></name>
<xref ref-type="aff" rid="aff3-0962280212448970">3</xref>
</contrib>
<aff id="aff1-0962280212448970"><label>1</label>Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109, USA</aff>
<aff id="aff2-0962280212448970"><label>2</label>Department of Neuroradiology, University of Heidelberg, Heidelberg, DE 69120, Germany</aff>
<aff id="aff3-0962280212448970"><label>3</label>Department of Statistics, University of Warwick, Coventry, CV4 7AL, UK</aff>
</contrib-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Nathoo</surname><given-names>Farouk</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Lawson</surname><given-names>Andrew B</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Dean</surname><given-names>Charmaine B</given-names></name>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0962280212448970">Timothy D Johnson, Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109, USA. Email: <email>tdjtdj@umich.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2013</year>
</pub-date>
<volume>22</volume>
<issue>4</issue>
<issue-title>Special Issue: GEOMED 2011 Imaging</issue-title>
<fpage>364</fpage>
<lpage>381</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012 Reprints and permissions: sagepub.co.uk/journalsPermissions.nav</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The Potts model has enjoyed much success as a prior model for image segmentation. Given the individual classes in the model, the data are typically modeled as Gaussian random variates or as random variates from some other parametric distribution. In this article, we present a non-parametric Potts model and apply it to a functional magnetic resonance imaging study for the pre-surgical assessment of peritumoral brain activation. In our model, we assume that the <italic>Z</italic>-score image from a patient can be segmented into activated, deactivated, and null classes, or states. Conditional on the class, or state, the <italic>Z</italic>-scores are assumed to come from some generic distribution which we model non-parametrically using a mixture of Dirichlet process priors within the Bayesian framework. The posterior distribution of the model parameters is estimated with a Markov chain Monte Carlo algorithm, and Bayesian decision theory is used to make the final classifications. Our Potts prior model includes two parameters, the standard spatial regularization parameter and a parameter that can be interpreted as the a priori probability that each voxel belongs to the null, or background state, conditional on the lack of spatial regularization. We assume that both of these parameters are unknown, and jointly estimate them along with other model parameters. We show through simulation studies that our model performs on par, in terms of posterior expected loss, with parametric Potts models when the parametric model is correctly specified and outperforms parametric models when the parametric model in misspecified.</p>
</abstract>
<kwd-group>
<kwd>decision theory</kwd>
<kwd>Dirichlet process</kwd>
<kwd>FMRI</kwd>
<kwd>hidden Markov random field</kwd>
<kwd>non-parametric Bayes</kwd>
<kwd>Potts model</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="sec1-0962280212448970"><title>1 Introduction</title>
<p>Functional magnetic resonance imaging (FMRI) has found a widespread use in the cognitive neurosciences due to its ability to non-invasively detect changes in brain activity induced by an experimental stimulus.<sup><xref ref-type="bibr" rid="bibr1-0962280212448970">1</xref></sup> The blood oxygen level dependent (BOLD) signal is the MRI contrast of blood deoxyhemoglobin and is a surrogate for neural activity. More recently, there has been a growing interest in using FMRI to assist in the pre-surgical planning of resection of brain tumors or epileptic foci.<sup><xref ref-type="bibr" rid="bibr2-0962280212448970">2</xref></sup> In these applications, the experimental paradigm used in the FMRI acquisition is carefully planned to detect changes in the BOLD signal near, around, and within the tumor in an effort to assist the surgeon in excising the tumor while preserving brain tissue regions involved in common tasks, such as speaking, as determined by the FMRI data.</p>
<p>Our motivating dataset comes from such a study. After a seizure with a speech arrest, a 32-year-old female patient presented with a WHO grade II oligodendroglioma, a particular type of a primary brain tumor derived from oligodendrocytes, in the left inferior frontal and insular lobe. Pre-surgery, the patient underwent an FMRI exam to determine vital peritumoral regions of the brain responsible for executive language functions and expressive speech. The patient was asked to recite tongue twisters followed by periods of rest because she exhibited slight impairments in the repetition of phonemically challenging phrases while other speech and language functions were not disturbed. A high-resolution FLAIR (fluid-attenuated inversion recovery)<sup><xref ref-type="bibr" rid="bibr3-0962280212448970">3</xref></sup> sagittal image of the left hemisphere is displayed in the top row of <xref ref-type="fig" rid="fig1-0962280212448970">Figure 1</xref> where the tumor is clearly visible.
<fig id="fig1-0962280212448970" position="float"><label>Figure 1.</label><caption><p>Results from the NP-Potts model. Top row: four sagittal slices of the high-resolution FLAIR image in the left hemisphere. Second row: FLAIR image with activation overlay. Bottom row: FLAIR image with deactivation overlay. The grayscale bar (color bar in electronic version) represents the posterior probability of both activation (for the second row) and deactivation (for the bottom row). The overlays are highly pixelated as the analysis is performed in the FMRI space which has a much lower resolution than the high-resolution FLAIR image. The hyperprior distribution on <italic>p</italic><sub><italic>i</italic>0</sub> is <italic>p</italic><sub><italic>i</italic>0</sub> ∼ beta(0.95(0.2<italic>N</italic>), 0.05(0.2<italic>N</italic>)). Loss function is (<xref ref-type="disp-formula" rid="disp-formula27-0962280212448970">12</xref>) with <italic>c</italic><sub>1</sub> = <italic>c</italic><sub>2</sub> = 4. NP: non-parametric; FLAIR: fluid-attenuated inversion recovery; and FMRI: functional magnetic resonance imaging.</p></caption><graphic xlink:href="10.1177_0962280212448970-fig1.tif"/>
</fig></p>
<p>In a standard FMRI analysis, a mass-univariate model is fit (e.g. with SPM<sup><xref ref-type="bibr" rid="bibr4-0962280212448970">4</xref></sup> or FSL<sup><xref ref-type="bibr" rid="bibr5-0962280212448970">5</xref>,<xref ref-type="bibr" rid="bibr6-0962280212448970">6</xref></sup>) where a regression model is fit independently at every volume element or voxel. The resulting image of statistics is thresholded such that the risk of false positives over the brain is controlled at a specified level. The most common approach is to use a threshold that controls the familywise error rate using random field theory (RFT).<sup><xref ref-type="bibr" rid="bibr7-0962280212448970">7</xref></sup> However, RFT rests on the assumption that the data arise as a realization of a smooth Gaussian random field. With no smoothing, FMRI data violate this assumption<sup><xref ref-type="bibr" rid="bibr8-0962280212448970">8</xref>,<xref ref-type="bibr" rid="bibr9-0962280212448970">9</xref></sup> and thus, the data are convolved with an isotropic Gaussian kernel prior to analysis to make this assumption hold, at least approximately. The result of this smoothing is that the activation (large, positive BOLD signal) and deactivation (large, negative BOLD signal) are spatially smeared out. Spatial diffusion of the signal is tolerable for the cognitive neurosciences as it increases the signal-to-noise ratio and gives results that may be typical for a population of subjects. In pre-surgical planning, however, spatial precision of the signal is paramount. According to electrical stimulation mapping, the size of speech-eloquent areas is known to vary between a few millimeters and a few centimeters. Thus, smoothing by a single uniform Gaussian kernel will inevitably smear out some activations and shrink or even extinguish others below the detection level. Both scenarios are highly undesirable for pre-surgical planning. Thus, we develop an image segmentation model that does not rely on such random field assumptions.</p>
<p>The Potts model<sup><xref ref-type="bibr" rid="bibr10-0962280212448970">10</xref></sup> was originally developed as a generalization of the Ising model<sup><xref ref-type="bibr" rid="bibr11-0962280212448970">11</xref>,<xref ref-type="bibr" rid="bibr12-0962280212448970">12</xref></sup> in the field of statistical mechanics. However, it has also been very successful as a prior model for image segmentation.<sup><xref ref-type="bibr" rid="bibr12-0962280212448970">12</xref>–<xref ref-type="bibr" rid="bibr15-0962280212448970">15</xref></sup> A crucial feature of the Potts model is that it does not smooth over abrupt changes in image intensity<sup><xref ref-type="bibr" rid="bibr12-0962280212448970">12</xref></sup> as does the convolution of the image with an isotropic Gaussian kernel.</p>
<p>Several authors have proposed non-parametric (NP) hidden Markov random field models (such as the Potts model).<sup><xref ref-type="bibr" rid="bibr16-0962280212448970">16</xref>–<xref ref-type="bibr" rid="bibr19-0962280212448970">19</xref></sup> These authors all assume that the number of states in the hidden Markov random field is unknown and is to be estimated. To this end, they all use the Dirichlet process (DP)<sup><xref ref-type="bibr" rid="bibr20-0962280212448970">20</xref></sup> which naturally clusters the data. These clusters become the states of the hidden Markov random field. Once the number of states is known, the image intensity at every voxel in a given state has the same (parametric) distribution. In our application, we know a priori that the BOLD activation data consists of exactly three states: activated, null, and deactivated. Thus, our model is different in that we assume that the number of states is known and model the image intensities of the voxels within each state non-parametrically. This is the key difference in our model from those previously proposed. Using one of the previously proposed models in our application would more than likely result in a model with more than three states and would be useless in identifying the activated, null, and deactivated voxels.</p>
<p>This article brings together several advanced statistical ideas including the Potts model, path sampling, mixture of Dirichlet process (MDP) priors, slice sampling (via the Swendsen–Wang algorithm), and decision theory to estimate which voxels are deactivated, activated, or null. Our simulation studies show that, in terms of minimizing the posterior expected loss, our model performs equivalently to parametric Potts models when the parametric model is correctly specified and outperforms parametric models when the parametric model is misspecified.</p>
<p>The remainder of this article is laid out as follows. In <xref ref-type="sec" rid="sec2-0962280212448970">Section 2</xref>, we introduce notation and the Potts model along with our generalization to the Bayesian NP-Potts model. Some of the key algorithmic details are outlined in <xref ref-type="sec" rid="sec3-0962280212448970">Section 3</xref>. The motivating example is analyzed with our model in <xref ref-type="sec" rid="sec4-0962280212448970">Section 4</xref>, where we contrast it to a standard Potts model and present results from our simulation studies. We finish the article with a discussion in <xref ref-type="sec" rid="sec5-0962280212448970">Section 5</xref>.</p>
</sec>
<sec id="sec2-0962280212448970"><title>2 Model</title>
<sec id="sec3-0962280212448970"><title>2.1 The Potts model</title>
<p>Let <bold>Y</bold> denote a digitized image of intensity values. Let <inline-graphic xlink:href="10.1177_0962280212448970-img1.tif"/> denote the set of voxels (volume elements) if <bold>Y</bold> is a three-dimensional image or pixels (picture elements) if <bold>Y</bold> is a two-dimensional image. The cardinality of <inline-graphic xlink:href="10.1177_0962280212448970-img2.tif"/> is <italic>N</italic>. The Potts model<sup><xref ref-type="bibr" rid="bibr10-0962280212448970">10</xref></sup> relies on a neighborhood system and its associated cliques.<sup><xref ref-type="bibr" rid="bibr13-0962280212448970">13</xref></sup> Throughout, we will assume a first-order neighborhood system. In the first-order neighborhood system, two pixels are neighbors if they share a common edge and two voxels are neighbors if they share a common face. Thus, each voxel in the interior of <bold>Y</bold> has six neighbors. Henceforth, we refer only to voxels and reference them by a single index, say <italic>i</italic>. The correspondence between voxels and pixels should be obvious. We denote the fact that two voxels <italic>i</italic>, <italic>i</italic>′ ∈ <inline-graphic xlink:href="10.1177_0962280212448970-img3.tif"/> are neighbors by <italic>i</italic> ∼ <italic>i</italic>′. The set of neighbors of voxel <italic>i</italic> ∈ <inline-graphic xlink:href="10.1177_0962280212448970-img4.tif"/> is <inline-graphic xlink:href="10.1177_0962280212448970-img13.tif"/><sub><italic>i</italic></sub> = {<italic>i</italic>′ : <italic>i</italic>′ ∼ <italic>i</italic>, <italic>i</italic>′ ∈ <italic>S</italic>}. For the first-order neighborhood system, the cliques are the singletons {<italic>i</italic>} and all sets of pairs of neighbors {<italic>i</italic>, <italic>i</italic>′}. The image intensity, a random variable, at voxel <italic>i</italic> is <italic>Y</italic><sub><italic>i</italic></sub> with <bold>Y</bold> = (<italic>Y</italic><sub>1</sub>, … , <italic>Y</italic><sub><italic>N</italic></sub>)<sup>T</sup>. (We denote random variables by upper case letters and realized values by lower case letters).</p>
<p>Assume there exists a finite hidden Markov random field,<sup><xref ref-type="bibr" rid="bibr21-0962280212448970">21</xref></sup> <bold>Z</bold> = (<italic>Z</italic><sub>1</sub>, … , <italic>Z</italic><sub><italic>N</italic></sub>)<sup>T</sup> on a finite state space <inline-graphic xlink:href="10.1177_0962280212448970-img14.tif"/> with <italic>C</italic> states. It is easy to see that this random field forms a partition of the set of voxels <inline-graphic xlink:href="10.1177_0962280212448970-img5.tif"/> with <italic>C</italic> equivalence classes.<sup><xref ref-type="bibr" rid="bibr15-0962280212448970">15</xref></sup> That is, voxels <italic>i</italic> and <italic>i</italic>′ are equivalent if <italic>Z</italic><sub><italic>i</italic></sub> = <italic>Z</italic><sub><italic>i</italic>′</sub>. For FMRI images, we partition voxels into three equivalence classes denoted as deactivated, null, and activated and label these classes −1, 0, and 1, respectively: <inline-graphic xlink:href="10.1177_0962280212448970-img15.tif"/> = {−1, 0, 1}. We will use the words ‘state’ and ‘class’ interchangeably. We denote the class to which voxel <italic>i</italic> belongs as <italic>c</italic>(<italic>i</italic>). We consider the joint distribution of <bold>Y</bold> and <bold>Z</bold> given two sets of parameters: <bold><italic>ϕ</italic></bold> and <bold><italic>θ</italic></bold>:
<disp-formula id="disp-formula1-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math1-0962280212448970"><mml:mrow><mml:mo>⪻</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mi mathvariant="bold">ϕ</mml:mi><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">)</mml:mo></mml:mrow><mml:mo mathvariant="bold">=</mml:mo><mml:mo>⪻</mml:mo><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">(</mml:mo></mml:mrow><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo mathvariant="bold">∈</mml:mo><mml:mtext mathvariant="bold">A</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo mathvariant="bold">=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">ϕ</mml:mi><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">)</mml:mo></mml:mrow><mml:mo>⪻</mml:mo><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">(</mml:mo></mml:mrow><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo mathvariant="bold">=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mi mathvariant="bold">θ</mml:mi><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula1-0962280212448970" xlink:href="10.1177_0962280212448970-eq1.tif"/></disp-formula>
where <italic>A</italic> 
⊂
 ℝ<sup><italic>N</italic></sup> is a measurable set. We assume that <bold>Z</bold> follows a Potts distribution, a special case of a Gibbs distribution, see Brémaud,<sup><xref ref-type="bibr" rid="bibr13-0962280212448970">13</xref></sup> given by</p>
<p><inline-graphic xlink:href="10.1177_0962280212448970-eq2.tif"/></p>
<p>where ℰ(<bold>z</bold>; <bold><italic>θ</italic></bold>) is the energy function, ℰ : <inline-graphic xlink:href="10.1177_0962280212448970-img16.tif"/><sup><italic>N</italic></sup> → ℝ ∪ {∞}. The energy function is typically written as a sum of Gibbs potential functions indexed by cliques <italic>Q</italic>: <italic>V</italic><sub><italic>Q</italic></sub>(<bold>z</bold>; <bold><italic>θ</italic></bold>) where <italic>V</italic><sub><italic>Q</italic></sub> ≡ 0 if <italic>Q</italic> is not a clique and for all <bold>z</bold>, <bold>z</bold>′ ∈ <inline-graphic xlink:href="10.1177_0962280212448970-img17.tif"/><sup><italic>N</italic></sup> and for cliques <italic>Q</italic> 
⊂
 <inline-graphic xlink:href="10.1177_0962280212448970-img6.tif"/>, we have <bold>z</bold>(<italic>Q</italic>) = <bold>z</bold>′(<italic>Q</italic>) ⇒ <italic>V</italic><sub><italic>Q</italic></sub>(<bold>z</bold>; <bold><italic>θ</italic></bold>) = <italic>V</italic><sub><italic>Q</italic></sub>(<bold>z</bold>′; <bold><italic>θ</italic></bold>) where for a set <italic>A</italic> ⊆ <inline-graphic xlink:href="10.1177_0962280212448970-img7.tif"/>, <bold>z</bold>(<italic>A</italic>) = {<italic>z</italic><sub><italic>i</italic></sub> : <italic>i</italic> ∈ <italic>A</italic>} (see Brémaud<sup><xref ref-type="bibr" rid="bibr13-0962280212448970">13</xref></sup> for details).</p>
<p>In what follows, we let <bold><italic>θ</italic></bold> = {β<sub>0</sub>, β<sub>1</sub>}. Our Potts distribution is
<disp-formula id="disp-formula3-0962280212448970"><label>(2)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math3-0962280212448970"><mml:mrow><mml:mo>⪻</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula3-0962280212448970" xlink:href="10.1177_0962280212448970-eq3.tif"/></disp-formula>
where the first summation is taken over all neighboring pairs and δ<sub><italic>k</italic></sub>(<italic>A</italic>) is the Dirac measure
<disp-formula id="disp-formula4-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math4-0962280212448970"><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo>:</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>∉</mml:mo><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>:</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula4-0962280212448970" xlink:href="10.1177_0962280212448970-eq4.tif"/></disp-formula>
that is, δ<sub><italic>j</italic></sub>[<italic>c</italic>(<italic>i</italic>)] = 1 if and only if <italic>z</italic><sub><italic>i</italic></sub> = <italic>z</italic><sub><italic>j</italic></sub> and zero otherwise. β<sub>0</sub> ≥ 0 is called the spatial regularization parameter and controls the strength of association between neighboring voxels. When β<sub>0</sub> = 0, class membership of a voxel is independent of its neighbors and all configurations, <bold>z</bold>, are equally probable. For β<sub>0</sub> &gt; 0, the Potts distribution favors configurations where neighboring voxels are members of the same equivalence class. The larger β<sub>0</sub>, the stronger the association between neighbors. When β<sub>0</sub> = 0, the second summation in equation (<xref ref-type="disp-formula" rid="disp-formula3-0962280212448970">2</xref>) has a nice interpretation: Pr(<bold>Z</bold> = <bold>z</bold> ∣ β<sub>0</sub> = 0, β<sub>1</sub>) = ∏<sub><italic>i</italic>∈<inline-graphic xlink:href="10.1177_0962280212448970-img8.tif"/></sub> Pr(<italic>Z</italic><sub><italic>i</italic></sub> = <italic>z</italic><sub><italic>i</italic></sub> ∣ β<sub>0</sub> = 0, β<sub>1</sub>) ∝ ∏<sub><italic>i</italic>∈<inline-graphic xlink:href="10.1177_0962280212448970-img9.tif"/></sub> exp(−β<sub>1</sub> |<italic>z</italic><sub><italic>i</italic></sub>|). Thus, for <italic>z</italic><sub><italic>i</italic></sub> ∈ {−1, 0, 1}, Pr(<italic>Z</italic><sub><italic>i</italic></sub> = <italic>z</italic><sub><italic>i</italic></sub> ∣ β<sub>0</sub> = 0, β<sub>1</sub>) ∝ exp(−β<sub>1</sub> |<italic>z</italic><sub><italic>i</italic></sub>|). In particular, Pr(<italic>Z</italic><sub><italic>i</italic></sub> = −1 ∣ β<sub>0</sub> = 0, β<sub>1</sub>) = Pr(<italic>Z</italic><sub><italic>i</italic></sub> = 1 ∣ β<sub>0</sub> = 0, β<sub>1</sub>) = exp(−β<sub>1</sub>)/(1 + 2exp(−β<sub>1</sub>)) and Pr(<italic>Z</italic><sub><italic>i</italic></sub> = 0 ∣ β<sub>0</sub> = 0, β<sub>1</sub>) = (1 + 2exp(−β<sub>1</sub>))<sup>−1</sup>. That is, the prior probability of a voxel belonging to the null state given β<sub>0</sub> = 0 and β<sub>1</sub> is (1 + 2exp(−β<sub>1</sub>))<sup>−1</sup> and the prior probability of a voxel belonging to either the deactivated or activated state is exp(−β<sub>1</sub>)/(1 + 2exp(−β<sub>1</sub>)). Note that β<sub>1</sub> ∈ ℝ.</p>
<p>The normalizing constant in equation (<xref ref-type="disp-formula" rid="disp-formula3-0962280212448970">2</xref>) is given by
<disp-formula id="disp-formula5-0962280212448970"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math5-0962280212448970"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula5-0962280212448970" xlink:href="10.1177_0962280212448970-eq5.tif"/></disp-formula>
where the outer sum is over all possible configurations: <inline-graphic xlink:href="10.1177_0962280212448970-img19.tif"/> = {<bold>z</bold> : <italic>z</italic><sub><italic>i</italic></sub> ∈ <inline-graphic xlink:href="10.1177_0962280212448970-img18.tif"/>, <italic>i</italic> ∈ <inline-graphic xlink:href="10.1177_0962280212448970-img10.tif"/>}. This normalizing constant is computationally intractable, even for moderate-sized images and classes. For example a small 4 × 4 × 4 image and three equivalence classes results in <inline-graphic xlink:href="10.1177_0962280212448970-img20.tif"/> = 3<sup>64</sup> ≈ 3.4 × 10<sup>30</sup> possible configurations. This fact complicates Bayesian computation unless one assumes β<sub>0</sub> and β<sub>1</sub> are known, a priori. However, it is possible to estimate β<sub>0</sub> and β<sub>1</sub> along with all other parameters (<xref ref-type="sec" rid="sec7-0962280212448970">Section 3.2</xref>).</p>
<p>We assume that conditional on <bold>Z</bold>, the <italic>Y</italic><sub><italic>i</italic></sub> are independent
<disp-formula id="disp-formula6-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math6-0962280212448970"><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo>∈</mml:mo><mml:mi>A</mml:mi><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant="bold">ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo mathvariant="bold">=</mml:mo><mml:munder><mml:mo>Π</mml:mo><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext><mml:mo mathvariant="bold">∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Y</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext></mml:mrow></mml:msub><mml:mo mathvariant="bold">∈</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">A</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Z</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext></mml:mrow></mml:msub><mml:mo mathvariant="bold">=</mml:mo><mml:mtext mathvariant="bold">j</mml:mtext><mml:mo mathvariant="bold">,</mml:mo><mml:mi mathvariant="bold">ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula6-0962280212448970" xlink:href="10.1177_0962280212448970-eq6.tif"/></disp-formula>
where <italic>A</italic><sub><italic>i</italic></sub> 
⊂
 ℝ and <italic>A</italic><sub>1</sub> × <italic>A</italic><sub>2</sub> × 
···
 × <italic>A</italic><sub><italic>N</italic></sub> = <italic>A</italic>. Furthermore, the conditional distribution of each <italic>Y</italic><sub><italic>i</italic></sub> is independent and identically distributed (iid)
<disp-formula id="disp-formula7-0962280212448970"><label>(4)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math7-0962280212448970"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">ϕ</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo mathvariant="bold">~</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">F</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">ϕ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold">   </mml:mi><mml:mtext>with</mml:mtext><mml:mi mathvariant="bold">   </mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">ϕ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo mathvariant="bold">~</mml:mo><mml:mtext mathvariant="bold">G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">ψ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula7-0962280212448970" xlink:href="10.1177_0962280212448970-eq7.tif"/></disp-formula>
where <italic>ϕ</italic><sub><italic>j</italic></sub> ∈ <bold><italic>ϕ</italic></bold> and the <italic>F</italic><sub><italic>j</italic></sub>(<italic>ϕ</italic><sub><italic>j</italic></sub>) and <italic>G</italic>(ψ<sub><italic>j</italic></sub>) are parametric distributions.</p>
</sec>
<sec id="sec4-0962280212448970"><title>2.2 The Bayesian NP-Potts model</title>
<p>The <italic>G</italic>(ψ<sub><italic>j</italic></sub>) given in equation (<xref ref-type="disp-formula" rid="disp-formula7-0962280212448970">4</xref>) is a parametric distribution. To relax this assumption, we will use MDP priors.<sup><xref ref-type="bibr" rid="bibr22-0962280212448970">22</xref>–<xref ref-type="bibr" rid="bibr24-0962280212448970">24</xref></sup> The model defined in equation (<xref ref-type="disp-formula" rid="disp-formula7-0962280212448970">4</xref>) is replaced by
<disp-formula id="disp-formula8-0962280212448970"><label>(5)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block" id="math8-0962280212448970"><mml:mrow><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow/><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow/><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mtext>DP</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula8-0962280212448970" xlink:href="10.1177_0962280212448970-eq8.tif"/></disp-formula>
In what follows, <italic>F</italic><sub><italic>j</italic></sub> is Gaussian and <inline-formula id="ilm1-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math1-0962280212448970"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi> </mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> where μ<sub><italic>i</italic></sub> the mean and <inline-formula id="ilm2-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math2-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi> </mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> the variance of <italic>Y</italic><sub><italic>i</italic></sub> given <italic>Z</italic><sub><italic>i</italic></sub> = <italic>j</italic>. There are two important differences to point out between the models in equations (<xref ref-type="disp-formula" rid="disp-formula7-0962280212448970">4</xref>) and (<xref ref-type="disp-formula" rid="disp-formula8-0962280212448970">5</xref>). First, in equation (<xref ref-type="disp-formula" rid="disp-formula8-0962280212448970">5</xref>), each <italic>Y</italic><sub><italic>i</italic></sub> is only conditionally independent and not iid as in equation (<xref ref-type="disp-formula" rid="disp-formula7-0962280212448970">4</xref>). This is reflected in the fact that each <italic>Y</italic><sub><italic>i</italic></sub> has its own parameter set <italic>ϕ</italic><sub><italic>i</italic></sub>. Second, the distribution of each <italic>ϕ</italic><sub><italic>i</italic></sub> is specified as a random measure <italic>G</italic><sub><italic>j</italic></sub> that follows a DP with base measure <italic>G</italic><sub><italic>j</italic>0</sub> and concentration parameter α<sub><italic>j</italic></sub>. The concentration parameter α<sub><italic>j</italic></sub> determines how concentrated <italic>G</italic><sub><italic>j</italic></sub> is about <italic>G</italic><sub><italic>j</italic>0</sub> with <italic>E</italic>(<italic>G</italic><sub><italic>j</italic></sub>) = <italic>G</italic><sub><italic>j</italic>0</sub>. Furthermore, in our study, the base measure <italic>G</italic><sub><italic>j</italic>0</sub> is Gaussian, parametrized by its mean, μ<sub><italic>j</italic>0</sub>, and variance, <inline-formula id="ilm3-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math3-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. We note that the conditional prior of <bold>Z</bold> is given in equation (<xref ref-type="disp-formula" rid="disp-formula3-0962280212448970">2</xref>). In order to finish specifying our model, we assign prior distributions to α<sub><italic>j</italic></sub>, β<sub>0</sub>, β<sub>1</sub>, μ<sub><italic>j</italic>0</sub>, and <inline-formula id="ilm4-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math4-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>
<disp-formula id="disp-formula9-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math9-0962280212448970"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>~</mml:mo><mml:mtext>gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>   </mml:mi><mml:mi>   </mml:mi><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>~</mml:mo><mml:mtext>gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>~</mml:mo><mml:mtext>uniform</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>min</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>~</mml:mo><mml:mtext>uniform</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>min</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>~</mml:mo><mml:mtext>uniform</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>~</mml:mo><mml:mtext>gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>   </mml:mi><mml:mi>   </mml:mi><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>~</mml:mo><mml:mtext>gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula9-0962280212448970" xlink:href="10.1177_0962280212448970-eq9.tif"/></disp-formula>
T specify a prior on β<sub>1</sub>, we first place a prior on Pr(<italic>Z</italic><sub><italic>i</italic></sub> = 0 ∣ β<sub>0</sub> = 0, β<sub>1</sub>) ≡ <italic>p</italic><sub><italic>i</italic>0</sub> = (1 + 2 exp(−β<sub>1</sub>))<sup>−1</sup>. Let <italic>p</italic><sub><italic>i</italic>0</sub> ∼ beta(<italic>p</italic><sub><italic>a</italic></sub>, <italic>p</italic><sub><italic>b</italic></sub>). This induces a prior on β<sub>1</sub> with density
<disp-formula id="disp-formula10-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math10-0962280212448970"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>   </mml:mi><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo>ℝ</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula10-0962280212448970" xlink:href="10.1177_0962280212448970-eq10.tif"/></disp-formula>
For the mean of the base measure for the deactivated and activated states, we assume that the mean must be negative, respectively, positive. This aids in identifying the model, yet still allows, for example, an activated voxel to have a negative intensity (of course its neighbors will more than likely be large positive numbers). The hyperprior parameters <italic>a</italic>, <italic>b</italic>, <italic>c</italic>, <italic>d</italic>, <italic>f</italic>, <italic>g</italic>, <italic>h</italic>, <italic>p</italic><sub><italic>a</italic></sub>, and <italic>p</italic><sub><italic>b</italic></sub> are assumed to be known and will be identified below in the application section. Now, each class in the Potts model is modeled non-parametrically as opposed to having a parametric structure. Note that we place a hyperprior distribution on the inverse scale parameter, β<sub>σ<italic>z</italic><sub><italic>i</italic></sub></sub>, of the gamma prior for <inline-formula id="ilm5-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math5-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. The number of components of the MDP is sensitive to both the precision parameter <inline-formula id="ilm6-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math6-0962280212448970"><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and to the variance <inline-formula id="ilm7-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math7-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> of the base measure. By placing priors on the β<sub>σ<italic>j</italic></sub> and α<sub><italic>j</italic></sub>, we allow the data to inform on the amount of precision in the DP, the variance of the base measure, <italic>G</italic><sub><italic>j</italic>0</sub> and, hence, the number of components needed to fit the distribution of each class in the Potts model. We note here that our interest is not in the number of components that make up each class, rather in minimizing the posterior expected loss defined in <xref ref-type="sec" rid="sec9-0962280212448970">Section 3.4</xref>. Simulation studies confirm that the posterior expected loss is insensitive to the prior distributions on α<sub><italic>j</italic></sub> and <inline-formula id="ilm8-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math8-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p>
<p>The model specified in equation (<xref ref-type="disp-formula" rid="disp-formula8-0962280212448970">5</xref>) is NP in the sense that the number of parameters is not fixed. The number of parameters goes to infinity as the number of voxels goes to infinity (which can happen as the size of the voxel is reduced). It is a mixture model in that if we marginalize over the θ<sub><italic>i</italic></sub>, the distribution of <italic>Y</italic><sub><italic>i</italic></sub> is a mixture of sampling distributions <italic>F</italic><sub><italic>j</italic></sub>(<italic>ϕ</italic><sub><italic>i</italic></sub>) with mixing measure <italic>G</italic><sub><italic>j</italic></sub>
<disp-formula id="disp-formula11-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math11-0962280212448970"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><graphic alternate-form-of="disp-formula11-0962280212448970" xlink:href="10.1177_0962280212448970-eq11.tif"/></disp-formula>
Thus, our model allows much more flexibility in modeling the three classes of voxels.</p>
</sec>
</sec>
<sec id="sec5-0962280212448970"><title>3 Algorithmic details</title>
<p>In this section, we highlight some of the algorithmic details of the Markov chain Monte Carlo (MCMC) sampling scheme. In particular, in <xref ref-type="sec" rid="sec6-0962280212448970">Section 3.1</xref>, we discuss the Swendsen–Wang algorithm<sup><xref ref-type="bibr" rid="bibr25-0962280212448970">25</xref></sup> that we use to efficiently update the hidden Markov random field, <bold>Z</bold>; in <xref ref-type="sec" rid="sec7-0962280212448970">Section 3.2</xref>, we discuss the path sampling algorithm<sup><xref ref-type="bibr" rid="bibr26-0962280212448970">26</xref></sup> we use to estimate the ratio of normalizing constants we need in updating the parameters in the Potts model; in <xref ref-type="sec" rid="sec8-0962280212448970">Section 3.3</xref>, we discuss the algorithm we use to update the MDP priors parameters;<sup><xref ref-type="bibr" rid="bibr27-0962280212448970">27</xref></sup> and in <xref ref-type="sec" rid="sec9-0962280212448970">Section 3.4</xref>, we discuss the loss function we adopt in making our final assignment of voxels to classes using a Bayesian decision theoretic approach.</p>
<sec id="sec6-0962280212448970"><title>3.1 Swendsen–Wang algorithm</title>
<p>The Swendsen–Wang algorithm is a particular case of the slice sampler<sup><xref ref-type="bibr" rid="bibr28-0962280212448970">28</xref></sup> that we now present. First, the full conditional posterior of <bold>Z</bold> is
<disp-formula id="disp-formula12-0962280212448970"><label>(6)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math12-0962280212448970"><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant="bold">ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo mathvariant="bold">×</mml:mo><mml:mtext mathvariant="bold">exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo mathvariant="bold">-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext><mml:mo mathvariant="bold">~</mml:mo><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:munder><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo mathvariant="bold">-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">δ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">c</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">i</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">}</mml:mo></mml:mrow><mml:mo mathvariant="bold">-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext><mml:mo mathvariant="bold">∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo mathvariant="bold">|</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext></mml:mrow></mml:msub><mml:mo mathvariant="bold">|</mml:mo><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula12-0962280212448970" xlink:href="10.1177_0962280212448970-eq12.tif"/></disp-formula>
Now define ‘bond variables,’ <italic>W</italic><sub><italic>ij</italic></sub>, for each neighbor pair <italic>i</italic> ∼ <italic>j</italic>, where <italic>W</italic><sub><italic>ij</italic></sub> is a real, non-negative random variable, as follows. Let <bold>W</bold> = {<italic>W</italic><sub><italic>ij</italic></sub> : <italic>i</italic> ∼ <italic>j</italic>} and define the conditional distribution of each <italic>W</italic><sub><italic>ij</italic></sub>, given <bold>Z</bold> to be uniform and independent with density
<disp-formula id="disp-formula13-0962280212448970"><label>(7)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math13-0962280212448970"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula13-0962280212448970" xlink:href="10.1177_0962280212448970-eq13.tif"/></disp-formula>
Hence,
<disp-formula id="disp-formula14-0962280212448970"><label>(8)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math14-0962280212448970"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>Π</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula14-0962280212448970" xlink:href="10.1177_0962280212448970-eq14.tif"/></disp-formula>
Furthermore,
<disp-formula id="disp-formula15-0962280212448970"><label>(9)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math15-0962280212448970"><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">W</mml:mtext><mml:mo>,</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">y</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>,</mml:mo><mml:mi mathvariant="bold">ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext mathvariant="bold">exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo mathvariant="bold">-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext><mml:mo mathvariant="bold">∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo mathvariant="bold">|</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext></mml:mrow></mml:msub><mml:mo mathvariant="bold">|</mml:mo><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">)</mml:mo></mml:mrow><mml:munder><mml:mo>Π</mml:mo><mml:mrow><mml:mtext mathvariant="bold">i</mml:mtext><mml:mo mathvariant="bold">~</mml:mo><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:munder><mml:mtext mathvariant="bold">I</mml:mtext><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">{</mml:mo></mml:mrow><mml:mn>0</mml:mn><mml:mo mathvariant="bold">≤</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">W</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">ij</mml:mtext></mml:mrow></mml:msub><mml:mo mathvariant="bold">≤</mml:mo><mml:mtext mathvariant="bold">exp</mml:mtext><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">(</mml:mo></mml:mrow><mml:mo mathvariant="bold">-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo mathvariant="bold">-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">δ</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="bold">j</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mtext mathvariant="bold">c</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">i</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">)</mml:mo></mml:mrow><mml:mrow><mml:mo mathvariant="bold"> </mml:mo><mml:mo mathvariant="bold">}</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula15-0962280212448970" xlink:href="10.1177_0962280212448970-eq15.tif"/></disp-formula>
To sample from the joint posterior of <bold>Z</bold> and <bold>W</bold>, we iteratively sample between equations (<xref ref-type="disp-formula" rid="disp-formula14-0962280212448970">8</xref>) and (<xref ref-type="disp-formula" rid="disp-formula15-0962280212448970">9</xref>). Integrating this joint distribution with respect to <bold>W</bold> results in the full conditional of <bold>Z</bold> given in equation (<xref ref-type="disp-formula" rid="disp-formula12-0962280212448970">6</xref>). Sampling <bold>W</bold> ∣ <bold>Z</bold> is straightforward: sample each <italic>W</italic><sub><italic>ij</italic></sub> ∣ <bold>Z</bold> from equation (<xref ref-type="disp-formula" rid="disp-formula13-0962280212448970">7</xref>). To sample a new <bold>Z</bold> ∣ <bold>W</bold>, we note the following</p>
<p><inline-graphic xlink:href="10.1177_0962280212448970-eq16.tif"/></p>
<p>That is to say, if <italic>W</italic><sub><italic>ij</italic></sub> &gt; exp(−β<sub>0</sub>), then <italic>z</italic><sub><italic>i</italic></sub> and <italic>z</italic><sub><italic>j</italic></sub> are constrained to be equal and this occurs with probability 1 − exp(−β<sub>0</sub>) (equation (<xref ref-type="disp-formula" rid="disp-formula13-0962280212448970">7</xref>)). In other words, neighbors in the same equivalence class are ‘bonded’ with probability 1 − exp(−β<sub>0</sub>). Thus, <bold>W</bold> partitions <inline-graphic xlink:href="10.1177_0962280212448970-img11.tif"/> into clusters of voxels that share the same activation state. We can think of this new partition as a refinement of the partition of equivalence classes where each new equivalence class is a cluster of voxels that share the same activation state. For more details consult Johnson and Piert.<sup><xref ref-type="bibr" rid="bibr15-0962280212448970">15</xref></sup> For any particular cluster <italic>K</italic>
<disp-formula id="disp-formula17-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math17-0962280212448970"><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Z</mml:mtext></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">j</mml:mtext><mml:mo>mid</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:munder><mml:mo>Π</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:munder><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>   </mml:mi><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><graphic alternate-form-of="disp-formula17-0962280212448970" xlink:href="10.1177_0962280212448970-eq17.tif"/></disp-formula>
where <bold>Z</bold><sub><italic>K</italic></sub> = <bold>j</bold> is shorthand notation for <italic>Z</italic><sub><italic>i</italic></sub> = <italic>j</italic> for all <italic>i</italic> ∈ <italic>K</italic>. Hence, each cluster can be updated independently according to its conditional distribution. For a nice discussion on the Swendsen–Wang algorithm see Higdon.<sup><xref ref-type="bibr" rid="bibr14-0962280212448970">14</xref></sup></p>
<p>We alternate between Swendsen–Wang updates of the state space map <bold>Z</bold> and single voxel Gibbs updates, adopting the strategy recommended by Higdon<sup><xref ref-type="bibr" rid="bibr14-0962280212448970">14</xref></sup> who found that this aids in mixing of the posterior of <bold>Z</bold>, to insure movement in large patches.</p>
</sec>
<sec id="sec7-0962280212448970"><title>3.2 Estimation of the potential energy parameters, β<sub>0</sub>, β<sub>1</sub></title>
<p>The full conditionals for β<sub>0</sub> and β<sub>1</sub> are, respectively
<disp-formula id="disp-formula18-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math18-0962280212448970"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula18-0962280212448970" xlink:href="10.1177_0962280212448970-eq18.tif"/></disp-formula>
and
<disp-formula id="disp-formula19-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math19-0962280212448970"><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math><graphic alternate-form-of="disp-formula19-0962280212448970" xlink:href="10.1177_0962280212448970-eq19.tif"/></disp-formula>
where <italic>c</italic>(β<sub>0</sub>, β<sub>1</sub>) is given in equation (<xref ref-type="disp-formula" rid="disp-formula5-0962280212448970">3</xref>). These full conditionals do not have a closed form from which we can easily sample and so we turn to the Metropolis–Hastings algorithm.<sup><xref ref-type="bibr" rid="bibr29-0962280212448970">29</xref></sup> The issue with this algorithm and the full conditionals is that we must compute the ratio of these normalizing constants when attempting to update either β<sub>0</sub> or β<sub>1</sub>. However, as mentioned previously, these constants are analytically and computationally intractable for any reasonably sized image.</p>
<p>Gelman and Meng<sup><xref ref-type="bibr" rid="bibr26-0962280212448970">26</xref></sup> demonstrate how log ratios of normalizing constants can be efficiently estimated using path sampling. This is feasible as long as one can efficiently sample from the Potts prior (i.e. the prior distribution of <bold>Z</bold> ∣ β<sub>0</sub>, β<sub>1</sub>). The Swendsen–Wang algorithm can be used for this purpose by replacing the full conditional of <bold>Z</bold> with its prior in the previous section.</p>
<p>Here, we outline the general procedure given in Gelman and Meng<sup><xref ref-type="bibr" rid="bibr26-0962280212448970">26</xref></sup> for our particular application. We first consider the ratio of normalizing constants when updating β<sub>0</sub> for fixed β<sub>1</sub>. Let λ<sub>β<sub>1</sub></sub>(<italic>a</italic>, <italic>b</italic>) ≡ ln[<italic>c</italic>(<italic>b</italic>, β<sub>1</sub>)/<italic>c</italic>(<italic>a</italic>, β<sub>1</sub>)]. Then
<disp-formula id="disp-formula20-0962280212448970"><label>(10)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math20-0962280212448970"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>∫</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> </mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>∂/</mml:mo><mml:mi>ln</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>∂</mml:mo><mml:mi>x</mml:mi><mml:mi>dx</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>∫</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> </mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>∂/</mml:mo><mml:mi>ln</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>∂</mml:mo><mml:mi>x</mml:mi><mml:mtext>d</mml:mtext><mml:mi>x</mml:mi></mml:mrow><mml:mrow/><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>∫</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> </mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mtext>d</mml:mtext><mml:mi>x</mml:mi></mml:mrow><mml:mrow/><mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mo>∫</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> </mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mtext>d</mml:mtext><mml:mi>x</mml:mi></mml:mrow><mml:mrow/><mml:mrow><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mo>∫</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> </mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mtext>d</mml:mtext><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula20-0962280212448970" xlink:href="10.1177_0962280212448970-eq20.tif"/></disp-formula>
Likewise, if λ<sub>β<sub>0</sub></sub>(<italic>a</italic>, <italic>b</italic>) ≡ ln[<italic>c</italic>(β<sub>0</sub>, <italic>b</italic>)/<italic>c</italic>(β<sub>0</sub>, <italic>a</italic>)], then
<disp-formula id="disp-formula21-0962280212448970"><label>(11)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math21-0962280212448970"><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mo>∫</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi> </mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mtext>E</mml:mtext></mml:mrow><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mtext>d</mml:mtext><mml:mi>x</mml:mi></mml:mrow></mml:math><graphic alternate-form-of="disp-formula21-0962280212448970" xlink:href="10.1177_0962280212448970-eq21.tif"/></disp-formula>
For both λ<sub>β<sub>1</sub></sub>(<italic>a</italic>, <italic>b</italic>) and λ<sub>β<sub>0</sub></sub>(<italic>a</italic>, <italic>b</italic>), we have two integrals to evaluate. We approximate the inner integrals, or expectations, using MCMC and these are precomputed and saved prior to sampling from the posterior. We draw from Pr(<bold>Z</bold> = <bold>z</bold> ∣ β<sub>0</sub>, β<sub>1</sub>) via the Swendsen–Wang algorithm (<xref ref-type="sec" rid="sec6-0962280212448970">Section 3.1</xref>) replacing the full conditional of <bold>Z</bold> with its prior and approximating the expectations on a grid of values for β<sub>0</sub> and β<sub>1</sub>. We let β<sub>0</sub> vary on [0, 2] in increments of 0.01. We determine the grid points for β<sub>1</sub> by varying <italic>p</italic><sub><italic>i</italic>0</sub> on [0.01, 0.99] in increments of 0.01 and setting β<sub>1</sub> = − ln[(1 − <italic>p</italic><sub><italic>i</italic>0</sub>)/2<italic>p</italic><sub><italic>i</italic>0</sub>]. The outer integrals are evaluated numerically using the trapezoidal rule. For values of <italic>a</italic> and <italic>b</italic> not on the grid for which the expectations are evaluated, we linearly interpolate.</p>
</sec>
<sec id="sec8-0962280212448970"><title>3.3 Posterior estimate of the MDP parameters</title>
<p>Over the past 10–15 years, there has been an explosion of Bayesian NP papers stemming from the seminal works by Ferguson<sup><xref ref-type="bibr" rid="bibr20-0962280212448970">20</xref></sup> and Antoniak<sup><xref ref-type="bibr" rid="bibr22-0962280212448970">22</xref></sup> on DPs. The first algorithm developed to sample from a DP was the Polya urn scheme by Blackwell.<sup><xref ref-type="bibr" rid="bibr30-0962280212448970">30</xref></sup> Since then, several algorithms have been developed including the stick-breaking algorithm<sup><xref ref-type="bibr" rid="bibr31-0962280212448970">31</xref></sup> and the Chinese restaurant process.<sup><xref ref-type="bibr" rid="bibr32-0962280212448970">32</xref></sup> Neal<sup><xref ref-type="bibr" rid="bibr27-0962280212448970">27</xref></sup> compared six existing algorithms and presented two new algorithms that sample from the posterior of a MDP model with non-conjugate priors. We adopt algorithm 8 from his publication to update <inline-formula id="ilm9-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math9-0962280212448970"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi> </mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and β<sub>σ</sub> and refer the interested reader to his paper.</p>
<p>The precision parameters, α<sub><italic>j</italic></sub>, are updated by the method given in Escobar and West.<sup><xref ref-type="bibr" rid="bibr23-0962280212448970">23</xref></sup> In particular for each <italic>j</italic>, define a latent random variable η<sub><italic>j</italic></sub> ∈ [0, 1] by
<disp-formula id="disp-formula22-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math22-0962280212448970"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>~</mml:mo><mml:mtext>Beta</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula22-0962280212448970" xlink:href="10.1177_0962280212448970-eq22.tif"/></disp-formula>
where <italic>k</italic><sub><italic>j</italic></sub> is the number of components in class <italic>j</italic> and <italic>N</italic><sub><italic>j</italic></sub> the number of voxels in class <italic>j</italic>. Then
<disp-formula id="disp-formula23-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math23-0962280212448970"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>~</mml:mo><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mtext> gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:mi>ln</mml:mi><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext> gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:mi>ln</mml:mi><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula23-0962280212448970" xlink:href="10.1177_0962280212448970-eq23.tif"/></disp-formula>
where π<sub>η<sub><italic>j</italic></sub></sub> = (<italic>a</italic> + <italic>k</italic><sub><italic>j</italic></sub> − 1)/(<italic>a</italic> + <italic>k</italic><sub><italic>j</italic></sub> − 1 + <italic>N</italic><sub><italic>j</italic></sub>(<italic>b</italic> − ln η<sub><italic>j</italic></sub>)). See Escobar and West<sup><xref ref-type="bibr" rid="bibr23-0962280212448970">23</xref></sup> for full details.</p>
</sec>
<sec id="sec9-0962280212448970"><title>3.4 Loss function</title>
<p>The goal of using FMRI as a pre-surgical tool is to accurately determine which voxels belong to which class. That is, to determine the equivalence classes. We will make such assignments using a Bayesian decision theoretic approach. The surgeon wants to avoid removing or cutting through regions of the brain that are responsible for vital tasks such as speech, for example. Hence, a loss function that asymmetrically treats the various types of losses that can occur may be advantageous. In our example, there are three types of losses: misclassification of a null voxel, misclassification of a deactivated voxel, and misclassification of an activated voxel. It may be more important to correctly classify voxels and regions of the brain that are activated during a speech-related task, for example, than correctly classifying those regions that are not responsible for speech as null.</p>
<p>Let <italic>w</italic><sub><italic>i</italic></sub> ∈ {−1, 0, 1} denote the true state of voxel <italic>i</italic>. Let <italic>d</italic><sub><italic>i</italic></sub> ∈ {−1, 0, 1} denote our decision about the state of voxel <italic>i</italic>. That is, <italic>w</italic><sub><italic>i</italic></sub> = −1 if voxel <italic>i</italic> is truly deactivated, <italic>w</italic><sub><italic>i</italic></sub> = 0 if voxel <italic>i</italic> has no true BOLD signal change from resting state, and <italic>w</italic><sub><italic>i</italic></sub> = 1 if voxel <italic>i</italic> is truly activated. Let <italic>c</italic><sub>1</sub> and <italic>c</italic><sub>2</sub> be positive weights and define <bold>w</bold> = (<italic>w</italic><sub>1</sub>, … , <italic>w</italic><sub><italic>N</italic></sub>) and <bold>d</bold> = (<italic>d</italic><sub>1</sub>, … , <italic>d</italic><sub><italic>N</italic></sub>). Let δ<sub><italic>ij</italic></sub> denote the Kronecker delta function which is equal to one if <italic>i</italic> = <italic>j</italic> and zero otherwise. Our loss function is
<disp-formula id="disp-formula24-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math24-0962280212448970"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">d</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">w</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula24-0962280212448970" xlink:href="10.1177_0962280212448970-eq24.tif"/></disp-formula>
When <italic>c</italic><sub>1</sub> = <italic>c</italic><sub>2</sub> = 1, the loss is simply the number of incorrectly classified voxels. When the weights are less than one, more weight is given to incorrectly classified null voxels. When <italic>c</italic><sub>1</sub> &gt; 1, more weight is given to incorrectly classified deactivated voxels, and when <italic>c</italic><sub>2</sub> &gt; 1, more weight is given to incorrectly classified activated voxels, relative to null voxels.</p>
<p>Now, let <inline-graphic xlink:href="10.1177_0962280212448970-img21.tif"/> denote the set of all possible decisions: <inline-graphic xlink:href="10.1177_0962280212448970-img22.tif"/> = {<bold>d</bold> : <italic>d</italic><sub><italic>i</italic></sub> ∈ {−1, 0, 1}, ∀ <italic>i</italic> ∈ <inline-graphic xlink:href="10.1177_0962280212448970-img12.tif"/>}. From the Bayesian perspective, the optimal decision, <bold>d</bold>*, minimizes the posterior expected loss
<disp-formula id="disp-formula25-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math25-0962280212448970"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo mathvariant="bold">=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">d</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>∈</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:munder><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>d</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula25-0962280212448970" xlink:href="10.1177_0962280212448970-eq25.tif"/></disp-formula>
That is
<disp-formula id="disp-formula26-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math26-0962280212448970"><mml:mrow><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo mathvariant="bold">=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">d</mml:mtext></mml:mrow><mml:mrow><mml:mo mathvariant="bold">*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>inf</mml:mo><mml:mrow><mml:mtext mathvariant="bold">d</mml:mtext><mml:mo mathvariant="bold">∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo mathvariant="bold">=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">d</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula26-0962280212448970" xlink:href="10.1177_0962280212448970-eq26.tif"/></disp-formula>
Define <italic>q</italic><sub><italic>ji</italic></sub> = Pr(<italic>Z</italic><sub><italic>i</italic></sub> = <italic>j</italic> ∣ <italic>Y</italic>) for <italic>j</italic> ∈ {−1, 0, 1}. Suppose we draw <italic>T</italic> samples from the posterior with <bold>z</bold><sup>(<italic>t</italic>)</sup> denoting the value of <bold>Z</bold> from draw <italic>t</italic>. Then, the posterior estimate of <italic>q</italic><sub><italic>ji</italic></sub> is <inline-formula id="ilm10-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math10-0962280212448970"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>ji</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn> </mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mi>jz</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi> </mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> and an estimator of posterior expected loss is
<disp-formula id="disp-formula27-0962280212448970"><label>(12)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math27-0962280212448970"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mi>∧</mml:mi></mml:mover></mml:mrow><mml:mrow><mml:mo>⪻</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo mathvariant="bold">=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:mtext mathvariant="bold">Y</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">d</mml:mtext><mml:mo>,</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>=</mml:mo><mml:mn>1/</mml:mn><mml:mi>T</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn> </mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>d</mml:mtext><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">z</mml:mtext></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow/><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula27-0962280212448970" xlink:href="10.1177_0962280212448970-eq27.tif"/></disp-formula>
which achieves its minimum by setting <italic>d</italic><sub><italic>i</italic></sub> = −1 if <inline-formula id="ilm11-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math11-0962280212448970"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>&gt;</mml:mo><mml:mo>max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, <italic>d</italic><sub><italic>i</italic></sub> = 0 if <inline-formula id="ilm12-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math12-0962280212448970"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>&gt;</mml:mo></mml:mrow></mml:math></inline-formula> <inline-formula id="ilm13-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math13-0962280212448970"><mml:mrow><mml:mo>max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and <italic>d</italic><sub><italic>i</italic></sub> = 1 if <inline-formula id="ilm14-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math14-0962280212448970"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>&gt;</mml:mo><mml:mo>max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Typically, one sets <italic>c</italic><sub>1</sub> = <italic>c</italic><sub>2</sub>, but this is not necessary.</p>
</sec>
</sec>
<sec id="sec10-0962280212448970"><title>4 Data analysis and simulation studies</title>
<p>In this section, we present results of a data analysis from a patient with a low-grade primary brain tumor in the left inferior frontal and insular lobe. We also present results from simulation studies, demonstrating the robustness of our model.</p>
<sec id="sec11-0962280212448970"><title>4.1 Data analysis</title>
<p>As a clinical tool, the goal of pre-surgical FMRI is to assist the neurosurgeon in the planning and intra-operative navigation of resections or biopsies of intra-axial lesions, e.g. primary brain tumors, while avoiding damage to critical cerebral regions such as those responsible for motor, speech, and language, visual, or memory functions. <xref ref-type="fig" rid="fig1-0962280212448970">Figure 1</xref>, top row, shows four sagittal slices of the FLAIR image of our example of a 32-year-old right-handed female patient suffering from a histologically proven oligodendroglioma of the left inferior frontal and insular lobe. The tumor is clearly visible in <xref ref-type="fig" rid="fig1-0962280212448970">Figure 1</xref>. The paradigm to evoke executive speech- and language-related FMRI activations in the patient's brain was designed as a simple box-car of alternating on/off-episodes lasting 30 s each. During the on-episodes, the patient was asked to silently repeat auditorily presented tongue twisters. The task was chosen because the only slight speech- and language-related deficit of the patient consisted of a slightly impaired tongue-twister repetition. During the off-episodes, the patient was asked to silently repeat ‘tock-tock-tock’ without interruption, i.e. an unchallenging word train. Standard preprocessing and analysis was applied using FSL 4.1.<sup><xref ref-type="bibr" rid="bibr5-0962280212448970">5</xref>,<xref ref-type="bibr" rid="bibr33-0962280212448970">33</xref></sup> The resulting <italic>z</italic>-statistic image is our data and we fitted the data with our NP-Potts model. Note that the <italic>z</italic>-statistic image is not smoothed (convolved with an isotropic Gaussian kernel) as is typically performed prior to analyses via RFT. Smoothing is incorporated in the model via the Potts prior. However, as mentioned in Section 1, the model also respects boundaries in the image (i.e. performs segmentation) (Winkler,<sup><xref ref-type="bibr" rid="bibr12-0962280212448970">12</xref></sup> Ch. 2).</p>
<p>We now finish the definition of our model by specifying the particular prior and hyperprior parameter values. For the DP concentration parameters, α<sub><italic>j</italic></sub>, <italic>j</italic> = −1, 0, 1, we assign a gamma distribution with shape 3 and rate 2: α<sub><italic>j</italic></sub> ∼ gamma(3, 2). The hyperprior distributions for the base measure means are defined in <xref ref-type="sec" rid="sec4-0962280212448970">Section 2.2</xref> and the precisions, <inline-formula id="ilm15-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math15-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mn>0</mml:mn> </mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>~</mml:mo><mml:mtext>gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with β<sub>σ<italic>j</italic></sub> ∼ gamma(1, 1), for <italic>j</italic> ∈ {−1, 0, 1}. For the spatial regularization parameter, we assume β<sub>0</sub> ∼ gamma(0.001, 0.001), a rather uninformative prior. For β<sub>1</sub>, we place an informative prior on <italic>p</italic><sub><italic>i</italic>0</sub>, which induces a prior on β<sub>1</sub>. FMRI experiments are designed to find activation in very focal, specific regions of the brain. Thus, in virtually all FMRI experiments, a great majority of the voxels are not activated and the BOLD signal change evoked by the paradigm is expected to be zero in these areas. Thus, a priori, we believe that each voxel has a probability of 0.95 of belonging to the background, or null region. Thus, <italic>p</italic><sub><italic>i</italic>0</sub> ∼ beta(0.95 × (0.2<italic>N</italic>), 0.05 × (0.2<italic>N</italic>)), where the beta distribution parameters have a ‘prior sample size’ interpretation. Specifically, in our specification, we are assuming a prior sample size of 20% of the number of voxels, <italic>N</italic>, with 95% of the prior sample size belonging to the null class (background) and 5% belonging to either the deactivated or activated state.</p>
<p>We fit our model to the three-dimensional 64 × 64 × 40 (pixel size: 3.0 × 3.0 × 3.45 mm<sup>3</sup>) <italic>z</italic>-statistic image and use Bayesian decision theory to decide which voxels are activated, deactivated, and null. Our collaborators want to insure that truly activated or truly deactivated voxels are not misclassified and so we agreed on setting <italic>c</italic><sub>1</sub> = <italic>c</italic><sub>2</sub> = 4 in the posterior expected loss function found in equation (<xref ref-type="disp-formula" rid="disp-formula27-0962280212448970">12</xref>), expressing the belief that false negatives are four times as costly as false positives. Thus, we classify any voxel, <italic>i</italic>, with posterior probability of activation greater than 0.2 (<inline-formula id="ilm16-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math16-0962280212448970"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>) as activated and with posterior probability of deactivation greater than 0.2 (<inline-formula id="ilm17-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math17-0962280212448970"><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>∧</mml:mi></mml:mover><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>) as deactivated. The results are shown in the last two rows of <xref ref-type="fig" rid="fig1-0962280212448970">Figure 1</xref> where we display the FLAIR images with grayscale overlay (color overlay in electronic version) depicting the probability of activated voxels (second row) and deactivated voxels (third row).</p>
<p>Clinicians are mainly interested in detecting functional activations. However, deactivations are also depicted because (a) the baseline condition comprised a low-level rhythmic fluency task for comparison and (b) brain tumors can cause paradoxical blood oxygenation changes.<sup><xref ref-type="bibr" rid="bibr34-0962280212448970">34</xref></sup> Therefore, bidirectional statistical assessments and classifications are mandatory for clinical decision-making by pre-surgical FMRI.<sup><xref ref-type="bibr" rid="bibr35-0962280212448970">35</xref></sup> In the case presented here, strong frontal and peritumoral activations relevant for resection planning and neuronavigation were delineated (<xref ref-type="fig" rid="fig1-0962280212448970">Figure 1</xref>, second row).</p>
<p>As a comparison, in <xref ref-type="fig" rid="fig2-0962280212448970">Figure 2</xref> we show results of a ‘standard’ Potts model: compare equation (<xref ref-type="disp-formula" rid="disp-formula3-0962280212448970">2</xref>) with
<disp-formula id="disp-formula28-0962280212448970"><label>(13)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math28-0962280212448970"><mml:mrow><mml:mo>⪻</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>(</mml:mo></mml:mrow><mml:mtext mathvariant="bold">Z</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">z</mml:mtext><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>~</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo> </mml:mo><mml:mo>{</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo> </mml:mo><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula28-0962280212448970" xlink:href="10.1177_0962280212448970-eq28.tif"/></disp-formula>
where β<sub>0</sub> is a random quantity to be estimated along with all other parameters. Our ‘standard’ Potts model is parametrically defined by
<disp-formula id="disp-formula29-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math29-0962280212448970"><mml:mrow><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mtext>gamma</mml:mtext></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo> </mml:mo><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>mid</mml:mo><mml:msub><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo> </mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mtext>gamma</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula29-0962280212448970" xlink:href="10.1177_0962280212448970-eq29.tif"/></disp-formula>
We will refer to this model as the normal–gamma (NG) Potts model. In the NG Potts model, only voxels with negative intensities are allowed to belong to the deactivated state and only voxels with positive intensities are allowed to belong to the activated state. Gamma* is the ‘reflected’ gamma distribution defined for non-positive random variables as follows: if <italic>Y</italic> ∈ (−∞, 0], then π(<italic>y</italic> ∣ <italic>r</italic><sub>−1</sub>, <italic>s</italic><sub>−1</sub>) ∝ |<italic>y</italic>|<sup><italic>r</italic><sub>−1</sub>−1</sup> exp(−<italic>s</italic><sub>−1</sub>|<italic>y</italic>|). One last restriction is put on the gamma and reflected gamma distributions: there must exist a mode greater than 0. This restriction was suggested by Woolrich et al.<sup><xref ref-type="bibr" rid="bibr36-0962280212448970">36</xref></sup> as it results in a sensible ‘belief about the expected shape of the activation and deactivation distributions with respect to the non-activation.’ This implies that <italic>r</italic><sub>1</sub> &gt; 1 and <italic>r</italic><sub>−1</sub> &gt; 1. This model is a slight modification from that published in Woolrich et al.,<sup><xref ref-type="bibr" rid="bibr36-0962280212448970">36</xref></sup> which is currently implemented in FSL. For the NG Potts model, we adopt improper and uninformative priors: π(μ) ∝ 1, π(σ<sup>2</sup>) ∝ 1/σ<sup>2</sup>, [<italic>r</italic><sub>ℓ</sub>] ∼ gamma(0.0001, 0.0001) and [<italic>s</italic><sub>ℓ</sub>] ∼ gamma(0.0001, 0.0001) for ℓ = −1, 1. The same loss function as with the NP-Potts model was used to determine activated, deactivated, and null voxels.
<fig id="fig2-0962280212448970" position="float"><label>Figure 2.</label><caption><p>Results from the NG model. Top row: four sagittal slices of the high-resolution FLAIR image in the left hemisphere. Second row: FLAIR image with activation overlay. Bottom row: FLAIR image with deactivation overlay. The grayscale bar (color bar in electronic version) represents the posterior probability of both activation (for the second row) and deactivation (for the bottom row). The overlays are highly pixelated as the analysis is performed in the FMRI space which has a much lower resolution than the high resolution FLAIR image. The prior on <italic>p</italic><sub><italic>i</italic>0</sub> is fixed: <italic>p</italic><sub><italic>i</italic>0</sub> = 1/3. Loss function is (<xref ref-type="disp-formula" rid="disp-formula27-0962280212448970">12</xref>) with <italic>c</italic><sub>1</sub> = <italic>c</italic><sub>2</sub> = 4. NG: normal–gamma; FLAIR: fluid-attenuated inversion recovery; and FMRI: functional magnetic resonance imaging.</p></caption><graphic xlink:href="10.1177_0962280212448970-fig2.tif"/>
</fig></p>
<p>There exists dramatic differences between the results of our NP-Potts model and the NG Potts model. The simple experimental task of coverting tongue-twister repetitions, when contrasted to a lower-level rhythm/fluency condition, is not expected to be supported by vast portions of the brain like those depicted in <xref ref-type="fig" rid="fig2-0962280212448970">Figure 2</xref>. Intra-operative mapping during awake craniotomy confirmed that these extensive activations were over-inclusive and contained a large proportion of false-positive voxels. The differences are largely attributable to the fact that the standard Potts model implicitly assumes that the a priori probability of belonging to any one class is 1/<italic>C</italic> conditional on β<sub>0</sub> = 0—or, in our case, 1/3. In our NP-Potts model, we place a hyperprior distribution on the probability of belonging to background and this has a significant impact on the number of voxels that are classified into each of the three states. Histograms, for comparison, of the three states for both the NP-Potts and the NG Potts model are shown in <xref ref-type="fig" rid="fig3-0962280212448970">Figure 3</xref>.
<fig id="fig3-0962280212448970" position="float"><label>Figure 3.</label><caption><p>Histograms of the <italic>Z</italic>-scores from the final classifications into deactivated, null, and activated states. The top histogram is from the DP–Potts model and the bottom histogram from the NG model. The black histogram corresponds to the null classified voxels and their <italic>Z</italic>-scores, The outlined histograms to the left of the null histogram correspond to the deactivated voxels and those to the right, the activated voxels DP: Dirichlet process; NG: normal–gamma.</p></caption><graphic xlink:href="10.1177_0962280212448970-fig3.tif"/>
</fig></p>
<p>In the next section, we present results from simulation studies that demonstrate the robustness of our model compared to several parametric Potts models.</p>
</sec>
<sec id="sec12-0962280212448970"><title>4.2 Simulation studies</title>
<p>We conducted a comprehensive simulation study and present the results in this section. We note here that all comparisons use the Gibbs distribution found in equation (<xref ref-type="disp-formula" rid="disp-formula3-0962280212448970">2</xref>) and not equation (<xref ref-type="disp-formula" rid="disp-formula28-0962280212448970">13</xref>). Our main goals in the simulation study are (a) to demonstrate the robustness of our model to model misspecification and (b) to demonstrate that we can accurately estimate the Potts model parameters β<sub>0</sub> and β<sub>1</sub>. To demonstrate the robustness of our model, we compare our model to several parametric Potts models and show that the NP-Potts model performs as well or better in terms of smaller posterior expected loss. To demonstrate that we can accurately estimate β<sub>0</sub> and β<sub>1</sub>, we consider the root mean squared error (RMSE), relative bias, and coverage of the 95% credible intervals.</p>
<sec id="sec13-0962280212448970"><title>4.2.1 Model robustness</title>
<p>We simulate the hidden Markov random field, <bold>Z</bold>, using the Swendsen–Wang algorithm at all combinations of β<sub>0</sub> ∈ {0, 0.15, 0.25, 0.5, 0.75, 1} and β<sub>1</sub> ∈ {ln(4), ln(2), ln(1)} (corresponding to a marginal a priori probability of a voxel belonging to the background of 1/3, 1/2, and 2/3, respectively, assuming a three class state space). For <bold>Z</bold> at each combination of β<sub>0</sub> and β<sub>1</sub>, we create an image <bold>Y</bold> by populating each voxel <italic>i</italic> as determined by <italic>z</italic><sub><italic>i</italic></sub> from one of the three distributions. For each voxel belonging to the null, or background, we draw independently from a <italic>N</italic>(0, 1) distribution. For voxels belonging to the deactivated or activated states, we draw independently from one of the four distributions: a mixture of normals, a normal, a gamma, and a log-normal. The specific distributions are given in <xref ref-type="table" rid="table1-0962280212448970">Table 1</xref> and were chosen such that segmentation is a difficult task (e.g. the resulting mixture distribution is unimodal). Each of these four models, which we call the normal–mixture (NM), the normal–normal (NN), the NG, and the normal–log-normal (NL), generates data with a parametric structure. We then fit our model and three parametric Potts models to all simulated datasets and run each algorithm for 35,000 iterations discarding the first 10,000 iterations as burn-in. The three parametric Potts models we use to fit the data are the NG (background voxels fitted with a normal distribution, the activated voxels with a gamma distribution, and the deactivated voxels with a reflected gamma), the NN model (all three states fitted with normal distributions), and the NL model (background fitted with a normal distribution, activated voxels with a log-normal distribution, and deactivated voxels with a reflected log-normal distribution). We then compare the posterior expected loss (with <italic>c</italic><sub>1</sub> = <italic>c</italic><sub>2</sub> = 1) between all models on all datasets. <xref ref-type="fig" rid="fig4-0962280212448970">Figure 4</xref> shows a bar plot of the posterior expected loss for all four models fitted to the four datasets generated from the four parametric Potts models. In this example, β<sub>0</sub> = 0.25 and β<sub>1</sub> = ln(2). From this figure, we see that the NP-Potts model performs as well (in terms of posterior expected loss) as the parametric models when the parametric model is correctly specified. When the parametric model is misspecified, the NP-Potts model outperforms (has smaller posterior expected loss) the parametric models. This trend is consistent for all simulation settings (results not shown) except when β<sub>0</sub> = 0. In <xref ref-type="table" rid="table2-0962280212448970">Table 2</xref>, we present the proportion of correctly classified voxels from this simulation study for all three states as well as the total proportion of correctly classified voxels.
<fig id="fig4-0962280212448970" position="float"><label>Figure 4.</label><caption><p>Robustness of the NP-Potts model. The NP-Potts model performs on par with the correctly specified model and performs better than misspecified models. β<sub>0</sub> = 0.25, β<sub>1</sub> = ln(2) (marginal prior probability = 0.5). The grayscale bars represent the posterior expected loss from the model used to fit the data (see legend) generated from the true model. NP: non-parametric.</p></caption><graphic xlink:href="10.1177_0962280212448970-fig4.tif"/>
</fig>
<table-wrap id="table1-0962280212448970" position="float"><label>Table 1.</label><caption><p>The four parametric models used in the simulation studies.</p></caption>
<graphic alternate-form-of="table1-0962280212448970" xlink:href="10.1177_0962280212448970-table1.tif"/>
<table frame="hsides"><thead>
<tr><th colspan="3">State</th>
</tr>
<tr><th>Deactivated</th>
<th>Null</th>
<th>Activated</th>
</tr></thead>
<tbody>
<tr>
<td>NM</td>
<td>0.75<italic>N</italic>(−2, 1) + 0.25<italic>N</italic>(−5, 1)</td>
<td><italic>N</italic>(0, 1)</td>
<td>0.75<italic>N</italic>(2, 1) + 0.25<italic>N</italic>(5, 1)</td>
</tr>
<tr>
<td>NN</td>
<td><italic>N</italic>(−2, 1)</td>
<td><italic>N</italic>(0, 1)</td>
<td><italic>N</italic>(2, 1)</td>
</tr>
<tr>
<td>NG</td>
<td>Gamma*(5, 2)</td>
<td><italic>N</italic>(0, 1)</td>
<td>Gamma(5, 2)</td>
</tr>
<tr>
<td>NL</td>
<td>ln <italic>N</italic>*(0.75, 0.25)</td>
<td><italic>N</italic>(0, 1)</td>
<td>ln <italic>N</italic>(0.75, 0.25)</td>
</tr>
</tbody>
</table>
<table-wrap-foot><fn id="table-fn1-0962280212448970"><p>NM: normal–mixture; NN: normal–normal; NG: normal–gamma; and NL: normal–log-normal.</p></fn>
<fn id="table-fn2-0962280212448970"><p>Gamma* and ln <italic>N</italic>* represent the reflected gamma and log-normal distributions.</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table2-0962280212448970" position="float"><label>Table 2.</label><caption><p>Proportion of correctly classified voxels from the simulation study.</p></caption>
<graphic alternate-form-of="table2-0962280212448970" xlink:href="10.1177_0962280212448970-table2.tif"/>
<table frame="hsides"><thead>
<tr><th>True model</th>
<th>Fitted model</th>
<th>Deactivated</th>
<th>Null</th>
<th>Activated</th>
<th>Overall</th>
</tr></thead>
<tbody>
<tr>
<td>NM</td>
<td>NP-Potts</td>
<td>0.680</td>
<td>0.931</td>
<td>0.680</td>
<td>0.845</td>
</tr>
<tr>
<td>NN</td>
<td>0.346</td>
<td>0.997</td>
<td>0.343</td>
<td>0.774</td>
</tr>
<tr>
<td>NG</td>
<td>0.284</td>
<td>0.999</td>
<td>0.281</td>
<td>0.754</td>
</tr>
<tr>
<td>NL</td>
<td>0.261</td>
<td>1.000</td>
<td>0.261</td>
<td>0.747</td>
</tr>
<tr>
<td>NN</td>
<td>NP-Potts</td>
<td>0.652</td>
<td>0.896</td>
<td>0.657</td>
<td>0.813</td>
</tr>
<tr>
<td>NN</td>
<td>0.662</td>
<td>0.892</td>
<td>0.663</td>
<td>0.813</td>
</tr>
<tr>
<td>NG</td>
<td>0.456</td>
<td>0.968</td>
<td>0.456</td>
<td>0.793</td>
</tr>
<tr>
<td>NL</td>
<td>0.000</td>
<td>1.000</td>
<td>0.000</td>
<td>0.657</td>
</tr>
<tr>
<td>NG</td>
<td>NP-Potts</td>
<td>0.734</td>
<td>0.914</td>
<td>0.757</td>
<td>0.857</td>
</tr>
<tr>
<td>NN</td>
<td>0.667</td>
<td>0.936</td>
<td>0.710</td>
<td>0.851</td>
</tr>
<tr>
<td>NG</td>
<td>0.739</td>
<td>0.914</td>
<td>0.755</td>
<td>0.857</td>
</tr>
<tr>
<td>NL</td>
<td>0.703</td>
<td>0.932</td>
<td>0.717</td>
<td>0.856</td>
</tr>
<tr>
<td>NL</td>
<td>NP-Potts</td>
<td>0.728</td>
<td>0.885</td>
<td>0.732</td>
<td>0.832</td>
</tr>
<tr>
<td>NN</td>
<td>0.840</td>
<td>0.001</td>
<td>0.813</td>
<td>0.284</td>
</tr>
<tr>
<td>NG</td>
<td>0.381</td>
<td>0.986</td>
<td>0.371</td>
<td>0.777</td>
</tr>
<tr>
<td>NL</td>
<td>0.746</td>
<td>0.879</td>
<td>0.734</td>
<td>0.831</td>
</tr>
</tbody>
</table>
<table-wrap-foot><fn id="table-fn4-0962280212448970"><p>NP: non-parametric; NM: normal–mixture; NN: normal–normal; NG: normal–gamma; and NL: normal–log-normal.</p></fn>
<fn id="table-fn5-0962280212448970"><p>The results show that the NP-Potts model performs equally well when compared to the correctly specified parametric model and outperforms the misspecified parametric models.</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table3-0962280212448970" position="float"><label>Table 3.</label><caption><p>Estimation of β<sub>0</sub> and β<sub>1</sub> in the simulation study,</p></caption>
<graphic alternate-form-of="table3-0962280212448970" xlink:href="10.1177_0962280212448970-table3.tif"/>
<table frame="hsides"><thead>
<tr><th colspan="2">Parameter</th>
</tr>
<tr><th>β<sub>0</sub> (truth: 0.5)</th>
<th>β<sub>1</sub> (truth: ln(2))</th>
</tr></thead>
<tbody>
<tr>
<td>RMSE</td>
<td>0.0167</td>
<td>0.067</td>
</tr>
<tr>
<td>rBias</td>
<td>0.014</td>
<td>−0.074</td>
</tr>
<tr>
<td>Coverage</td>
<td>0.964</td>
<td>0.957</td>
</tr>
</tbody>
</table>
<table-wrap-foot><fn id="table-fn3-0962280212448970"><p>RMSE: root mean squared error.</p></fn>
</table-wrap-foot>
</table-wrap></p>
<p>When β<sub>0</sub> = 0, there is no spatial correlation between voxels in the data. In this case, the analysis reduces to fitting a univariate distribution of image intensities with a mixture of distributions (one can think of modeling a univariate histogram with a mixture density). Obviously, in this case, a single MDP priors model suffices in fitting the distribution. The NP-Potts model breaks down as there is no information in the data or in the prior to inform how to fit the data with three NP distributions and results are inconsistent when fitting the same dataset multiple times. For our particular application (or for that matter, for any image analysis) this is not alarming to us as we expect spatial correlation between neighboring voxels. This raises the question of how much spatial regularization is necessary in the image data for our NP-Potts model to perform effectively. We found, through simulation studies, that a value of β<sub>0</sub> ≥ 0.15, a rather small amount of spatial information, results in our NP-Potts model consistently performing as well or better than the parametric Potts models in terms of posterior expected loss.</p>
</sec>
<sec id="sec14-0962280212448970"><title>4.2.2 Estimation of β<sub>0</sub> and β<sub>1</sub></title>
<p>We consider the case when the true values are β<sub>0</sub> = 0.5 and β<sub>1</sub> = ln(2). We simulate 500 datasets using the NG model and 500 datasets using the NM model (see above and <xref ref-type="table" rid="table1-0962280212448970">Table 1</xref>). For all 1000 datasets, we fit our NP-Potts model by drawing from the posterior 25,000 times after a burn-in of 10,000 draws. We consider the coverage of the 95% equal tail area credible intervals, the RMSE and the relative bias (rBias) of the posterior expected mean of β<sub>0</sub> and β<sub>1</sub> over the 1000 simulations. Results are given in <xref ref-type="table" rid="table3-0962280212448970">Table 3</xref>. The coverage of the 95% credible intervals are near the nominal levels of 0.95. β<sub>0</sub> has a slight positive bias (relative bias 0.014) while the relative bias for β<sub>1</sub> is a bit larger in the negative direction (−0.074). However, in terms of <italic>p</italic><sub><italic>i</italic>0</sub>, which is arguably more interpretable, the relative bias is −0.026, or about an average <italic>p</italic><sub><italic>i</italic>0</sub> of 0.487 over the simulations as opposed to the truth of 0.5. Overall, our model performs well and we can accurately estimate the Potts prior parameters.</p>
<p>At the end of <xref ref-type="sec" rid="sec4-0962280212448970">Section 2.2</xref>, we stated that the posterior expected loss is insensitive to the priors on α<sub><italic>j</italic></sub> and <inline-formula id="ilm18-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math18-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi> </mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. For one randomly selected simulated dataset from the NM model with true β<sub>0</sub> = 0.5 and β<sub>1</sub> = ln(2), we fit our model with different priors. For α<sub><italic>j</italic></sub>, we consider the proposed prior, <italic>G</italic>(3, 2), and three other priors: <italic>G</italic>(1, 1), <italic>G</italic>(2, 4), and <italic>G</italic>(5, 1). The resulting posterior expected losses are 0.1515, 0.1521, 0.1518, and 0.1512, respectively. For <inline-formula id="ilm19-0962280212448970"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math19-0962280212448970"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi> </mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, we consider the proposed prior, <italic>G</italic>(3, β<sub>σ<italic>j</italic></sub>) with β<sub>σ<italic>j</italic></sub> ∼ <italic>G</italic>(1, 1) and two other priors <italic>G</italic>(6, β<sub>σ<italic>j</italic></sub>) and <italic>G</italic>(1, β<sub>σ<italic>j</italic></sub>). The resulting posterior expected losses are, respectively, 0.1515, 0.1526, and 0.1527.</p>
</sec>
</sec>
</sec>
<sec id="sec15-0962280212448970"><title>5 Discussion</title>
<p>We have presented an NP-Potts model and combined several ideas from the literature, including MDP priors, advanced sampling methods (path sampling and slice sampling using the Swendsen–Wang algorithm), and decision theory to adapt loss to the pre-surgical planning context. Our simulation studies show that, in terms of the posterior expected loss, our model performs on par with parametric Potts models when the parametric model is correctly specified, and that it outperforms parametric models when the parametric model is incorrectly specified. Hence, we argue for the use of the NP-Potts model as one never knows the true distribution of the data. We have also shown that we can accurately estimate the Potts prior parameters β<sub>0</sub> and β<sub>1</sub>.</p>
<p>Computationally, 125,000 iterations of the MCMC algorithm takes about 3 h of CPU time on a MacBook Pro with 8 GB of memory and a 3.06 GHz Intel Core 2 Duo processor. This is for the 64 × 64 × 40 image in our data example. In this example, the number of voxels within the brain is 46,999. Larger images will take more computation time.</p>
<p>In <xref ref-type="sec" rid="sec7-0962280212448970">Section 3.2</xref>, we show how the Potts prior parameters, β<sub>0</sub> and β<sub>1</sub>, can be estimated using the idea of path sampling.<sup><xref ref-type="bibr" rid="bibr26-0962280212448970">26</xref></sup> The expectations in equations (<xref ref-type="disp-formula" rid="disp-formula20-0962280212448970">10</xref>) and (<xref ref-type="disp-formula" rid="disp-formula21-0962280212448970">11</xref>) have to be precomputed via MCMC simulation and the computation of these expectations is expensive. Furthermore, these expectations need to be computed for every patient since the size and shape of brains differ from patient to patient, making this implementation of the algorithm arduous and not very appealing. To circumvent this bottleneck, one could require that the <italic>Z</italic>-score maps of every patient be mapped onto a common atlas and the expectations computed once, off-line. The expectations could then be used for all patients. The down-side of this is that the data are interpolated and this induces some smoothness in the interpolated image, which may not be desirable. It would be interesting to investigate the effects of this smoothing on parameter estimates and the posterior expected loss. Another possible alternative would be to approximate the likelihood with the pseudolikelihood.<sup><xref ref-type="bibr" rid="bibr37-0962280212448970">37</xref></sup> However, use of the pseudolikelihood results in over-estimation of the smoothing parameter and tends to over-smooth the data,<sup><xref ref-type="bibr" rid="bibr38-0962280212448970">38</xref></sup> which is not a desirable outcome for presurgical planning, as argued earlier.</p>
<p>We feel it is imperative to always include the spatial regularization parameter β<sub>0</sub> in equation (<xref ref-type="disp-formula" rid="disp-formula3-0962280212448970">2</xref>). For without it, one is assuming that all voxels are independent of one another and this assumption is almost never satisfied in medical imaging. On the other hand, β<sub>1</sub> plays a big role in determining the number of voxels that are classified as active or deactive, as is evident in our data analysis.</p>
</sec>
</body>
<back><sec>
<title>Funding</title>
<p>Dr Johnson is partially funded by the US National Institutes of Health (P01-CA87634).</p></sec>
<ref-list>
<title>References</title>
<ref id="bibr1-0962280212448970"><label>1</label><citation citation-type="book"><person-group person-group-type="editor"><name><surname>Huettel</surname><given-names>SA</given-names></name><name><surname>Song</surname><given-names>AW</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group>. <source>Functional magnetic resonance imaging</source>, <edition>2nd edn</edition>. <publisher-loc>Sunderland, MA</publisher-loc>: <publisher-name>Sinauer Associates</publisher-name>, <year>2008</year>.</citation></ref>
<ref id="bibr2-0962280212448970"><label>2</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yeo</surname><given-names>DTB</given-names></name><name><surname>Meyer</surname><given-names>CR</given-names></name><name><surname>Parent</surname><given-names>MJ</given-names></name><etal/></person-group>. <article-title>Formulation of current density weighted indices for correspondence between functional MRI and electrocortical stimulation maps</article-title>. <source>J Clin Neurophysiol</source> <year>2008</year>; <volume>119</volume>: <fpage>2887</fpage>–<lpage>2897</lpage>.</citation></ref>
<ref id="bibr3-0962280212448970"><label>3</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bakshi</surname><given-names>R</given-names></name><name><surname>Ariyaratana</surname><given-names>S</given-names></name><name><surname>Benedict</surname><given-names>RH</given-names></name><etal/></person-group>. <article-title>Fluid-attenuated inversion recovery magnetic resonance imaging detects cortical and juxtacortical multiple sclerosis lesions</article-title>. <source>Arch Neurol</source> <year>2001</year>; <volume>58</volume>(<issue>5</issue>): <fpage>742</fpage>–<lpage>748</lpage>.</citation></ref>
<ref id="bibr4-0962280212448970"><label>4</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Jezzard</surname><given-names>P</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group>. <article-title>Analysis of functional MRI time-series</article-title>. <source>Hum Brain Mapp</source> <year>1994</year>; <volume>1</volume>: <fpage>153</fpage>–<lpage>171</lpage>.</citation></ref>
<ref id="bibr5-0962280212448970"><label>5</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Woolrich</surname><given-names>CF</given-names></name><etal/></person-group>. <article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title>. <source>NeuroImage</source> <year>2004</year>; <volume>23</volume>(<issue>S1</issue>): <fpage>208</fpage>–<lpage>219</lpage>.</citation></ref>
<ref id="bibr6-0962280212448970"><label>6</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Ramnani</surname><given-names>N</given-names></name><etal/></person-group>. <article-title>Variability in FMRI: a re-examination of intersession differences</article-title>. <source>Hum Brain Mapp</source> <year>2005</year>; <volume>24</volume>: <fpage>248</fpage>–<lpage>257</lpage>.</citation></ref>
<ref id="bibr7-0962280212448970"><label>7</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Worsley</surname><given-names>KJ</given-names></name><name><surname>Marrett</surname><given-names>S</given-names></name><name><surname>Neelin</surname><given-names>P</given-names></name><etal/></person-group>. <article-title>A unified statistical approach for determining significant voxels in images of cerebral activation</article-title>. <source>Hum Brain Mapp</source> <year>1996</year>; <volume>4</volume>: <fpage>48</fpage>–<lpage>73</lpage>.</citation></ref>
<ref id="bibr8-0962280212448970"><label>8</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Petersson</surname><given-names>KM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><etal/></person-group>. <article-title>Statistical limitations in functional neuroimaging. II. Signal detection and statistical inference</article-title>. <source>Philos Trans R Soc B Biol Sci</source> <year>1999</year>; <volume>354</volume>(<issue>354</issue>): <fpage>1261</fpage>–<lpage>1281</lpage>.</citation></ref>
<ref id="bibr9-0962280212448970"><label>9</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Hayasaka</surname><given-names>S</given-names></name></person-group>. <article-title>Controlling the familywise error rate in functional neuroimaging: a comparative review</article-title>. <source>Stat Meth Med Res</source> <year>2003</year>; <volume>12</volume>(<issue>5</issue>): <fpage>419</fpage>–<lpage>446</lpage>.</citation></ref>
<ref id="bibr10-0962280212448970"><label>10</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Potts</surname><given-names>RB</given-names></name></person-group>. <article-title>Some generalized order-disorder transformations</article-title>. <source>Proc Camb Philos Soc</source> <year>1952</year>; <volume>48</volume>: <fpage>106</fpage>–<lpage>109</lpage>.</citation></ref>
<ref id="bibr11-0962280212448970"><label>11</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ising</surname><given-names>E</given-names></name></person-group>. <article-title>Beitrag zur theorie des ferromagnetismus</article-title>. <source>Z Angew Phys</source> <year>1925</year>; <volume>31</volume>(<issue>1</issue>): <fpage>253</fpage>–<lpage>258</lpage>.</citation></ref>
<ref id="bibr12-0962280212448970"><label>12</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>G</given-names></name></person-group>. <source>Image analysis, random fields and Markov chain Monte Carlo methods</source>, <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2003</year>.</citation></ref>
<ref id="bibr13-0962280212448970"><label>13</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Brémaud</surname><given-names>P</given-names></name></person-group>. <source>Markov chains, Gibbs fields, Monte Carlo simulation, and queues</source>, <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>1999</year>.</citation></ref>
<ref id="bibr14-0962280212448970"><label>14</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Higdon</surname><given-names>DM</given-names></name></person-group>. <article-title>Auxiliary variable methods for Markov chain Monte Carlo with applications</article-title>. <source>J Am Stat Assoc</source> <year>1998</year>; <volume>93</volume>(<issue>442</issue>): <fpage>585</fpage>–<lpage>595</lpage>.</citation></ref>
<ref id="bibr15-0962280212448970"><label>15</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>TD</given-names></name><name><surname>Piert</surname><given-names>M</given-names></name></person-group>. <article-title>A Bayesian analysis of dual autoradiographic images</article-title>. <source>Comput Stat Data Anal</source> <year>2009</year>; <volume>53</volume>: <fpage>4570</fpage>–<lpage>4583</lpage>.</citation></ref>
<ref id="bibr16-0962280212448970"><label>16</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>da Silva</surname><given-names>ARS</given-names></name></person-group>. <article-title>A Dirichlet process mixture model for brain MRI tissue classification</article-title>. <source>Med Image Anal</source> <year>2007</year>; <volume>2</volume>: <fpage>169</fpage>–<lpage>182</lpage>.</citation></ref>
<ref id="bibr17-0962280212448970"><label>17</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Orbanz</surname><given-names>P</given-names></name><name><surname>Buhmann</surname><given-names>J</given-names></name></person-group>. <article-title>Nonparametric Bayes image segmentation</article-title>. <source>Int J Comput Vision</source> <year>2008</year>; <volume>77</volume>: <fpage>25</fpage>–<lpage>45</lpage>.</citation></ref>
<ref id="bibr18-0962280212448970"><label>18</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Du</surname><given-names>L</given-names></name><name><surname>Ren</surname><given-names>L</given-names></name><name><surname>Dunson</surname><given-names>D</given-names></name><etal/></person-group> <article-title>A Bayesian model for simultaneous image clustering, annotation and object segmentation</article-title>. In: <person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Schuurmans</surname><given-names>D</given-names></name><name><surname>Lafferty</surname><given-names>J</given-names></name><etal/></person-group> (eds). <source>Advances in neural information processing systems 22</source>, <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>2009</year>, pp. <fpage>486</fpage>–<lpage>494</lpage>.</citation></ref>
<ref id="bibr19-0962280212448970"><label>19</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chatzis</surname><given-names>SP</given-names></name><name><surname>Tsechpenakis</surname><given-names>G</given-names></name></person-group>. <article-title>The infinite hidden Markov random field model</article-title>. <source>IEEE Trans Med Imaging</source> <year>2010</year>; <volume>21</volume>(<issue>6</issue>): <fpage>1004</fpage>–<lpage>1014</lpage>.</citation></ref>
<ref id="bibr20-0962280212448970"><label>20</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ferguson</surname><given-names>TS</given-names></name></person-group>. <article-title>A Bayesian analysis of some nonparametric problems</article-title>. <source>Ann Stat</source> <year>1973</year>; <volume>1</volume>(<issue>2</issue>): <fpage>209</fpage>–<lpage>230</lpage>.</citation></ref>
<ref id="bibr21-0962280212448970"><label>21</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Besag</surname><given-names>J</given-names></name></person-group>. <article-title>Spatial interaction and the statistical analysis of lattice systems</article-title>. <source>J R Stat Soc Ser B</source> <year>1974</year>; <volume>6</volume>: <fpage>192</fpage>–<lpage>236</lpage>.</citation></ref>
<ref id="bibr22-0962280212448970"><label>22</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Antoniak</surname><given-names>CE</given-names></name></person-group>. <article-title>Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems</article-title>. <source>Ann Stat</source> <year>1974</year>; <volume>2</volume>(<issue>6</issue>): <fpage>1152</fpage>–<lpage>1174</lpage>.</citation></ref>
<ref id="bibr23-0962280212448970"><label>23</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Escobar</surname><given-names>MD</given-names></name><name><surname>West</surname><given-names>M</given-names></name></person-group>. <article-title>Bayesian density estimation and inference using mixtures</article-title>. <source>J Am Stat Assoc</source> <year>1995</year>; <volume>90</volume>(<issue>430</issue>): <fpage>577</fpage>–<lpage>588</lpage>.</citation></ref>
<ref id="bibr24-0962280212448970"><label>24</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>P</given-names></name><name><surname>Quintana</surname><given-names>FA</given-names></name></person-group>. <article-title>Nonparametric Bayesian data analysis</article-title>. <source>Stat Sci</source> <year>2004</year>; <volume>19</volume>(<issue>1</issue>): <fpage>95</fpage>–<lpage>110</lpage>.</citation></ref>
<ref id="bibr25-0962280212448970"><label>25</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Swendsen</surname><given-names>RH</given-names></name><name><surname>Wang</surname><given-names>JS</given-names></name></person-group>. <article-title>Nonuniversal critical dynamics in Monte Carlo simulations</article-title>. <source>Phys Rev Lett</source> <year>1987</year>; <volume>58</volume>: <fpage>86</fpage>–<lpage>88</lpage>.</citation></ref>
<ref id="bibr26-0962280212448970"><label>26</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Meng</surname><given-names>X</given-names></name></person-group>. <article-title>Simulating normalizing constants: from importance sampling to bridge sampling to path sampling</article-title>. <source>Stat Sci</source> <year>1998</year>; <volume>13</volume>(<issue>2</issue>): <fpage>163</fpage>–<lpage>185</lpage>.</citation></ref>
<ref id="bibr27-0962280212448970"><label>27</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>R</given-names></name></person-group>. <article-title>Markov chain sampling methods for Dirichlet process mixture models</article-title>. <source>J Comput Graph Stat</source> <year>2000</year>; <volume>9</volume>(<issue>2</issue>): <fpage>249</fpage>–<lpage>265</lpage>.</citation></ref>
<ref id="bibr28-0962280212448970"><label>28</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Robert</surname><given-names>CP</given-names></name><name><surname>Casella</surname><given-names>G</given-names></name></person-group>. <source>Monte Carlo statistical methods</source>, <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2005</year>.</citation></ref>
<ref id="bibr29-0962280212448970"><label>29</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hastings</surname><given-names>WK</given-names></name></person-group>. <article-title>Monte Carlo sampling methods using Markov chains and their applications</article-title>. <source>Biometrika</source> <year>1970</year>; <volume>57</volume>(<issue>1</issue>): <fpage>97</fpage>–<lpage>109</lpage>.</citation></ref>
<ref id="bibr30-0962280212448970"><label>30</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Blackwell</surname><given-names>D</given-names></name><name><surname>MacQueen</surname><given-names>JB</given-names></name></person-group>. <article-title>Ferguson distributions via Polya urn schemes</article-title>. <source>Ann Stat</source> <year>1973</year>; <volume>1</volume>(<issue>2</issue>): <fpage>353</fpage>–<lpage>355</lpage>.</citation></ref>
<ref id="bibr31-0962280212448970"><label>31</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ishwaran</surname><given-names>H</given-names></name><name><surname>James</surname><given-names>LF</given-names></name></person-group>. <article-title>Gibbs sampling for stick-breaking priors</article-title>. <source>J Am Stat Assoc</source> <year>2001</year>; <volume>96</volume>: <fpage>161</fpage>–<lpage>173</lpage>.</citation></ref>
<ref id="bibr32-0962280212448970"><label>32</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pitman</surname><given-names>J</given-names></name></person-group>. <article-title>Exchangeable and partially exchangeable random partitions</article-title>. <source>Probab Theory Relat Fields</source> <year>1995</year>; <volume>102</volume>(<issue>2</issue>): <fpage>145</fpage>–<lpage>158</lpage>.</citation></ref>
<ref id="bibr33-0962280212448970"><label>33</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Patenaude</surname><given-names>P</given-names></name><etal/></person-group>. <article-title>Bayesian analysis of neuroimaging data in FSL</article-title>. <source>NeuroImage</source> <year>2009</year>; <volume>45</volume>: <fpage>173</fpage>–<lpage>186</lpage>.</citation></ref>
<ref id="bibr34-0962280212448970"><label>34</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Fujiwara</surname><given-names>N</given-names></name><name><surname>Sakatani</surname><given-names>K</given-names></name><name><surname>Katayama</surname><given-names>Y</given-names></name><etal/></person-group>. <article-title>Evoked-cerebral blood oxygenation changes in false-negative activations in BOLD contrast functional MRI of patients with brain tumors</article-title>. <source>NeuroImage</source> <year>2004</year>; <volume>21</volume>(<issue>4</issue>): <fpage>1464</fpage>–<lpage>1471</lpage>.</citation></ref>
<ref id="bibr35-0962280212448970"><label>35</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bartsch</surname><given-names>AJ</given-names></name><name><surname>Homola</surname><given-names>G</given-names></name><name><surname>Biller</surname><given-names>A</given-names></name><etal/></person-group>. <article-title>Diagnostic functional MRI: illustrated clinical applications and decision-making</article-title>. <source>J Magn Reson Imaging</source> <year>2006</year>; <volume>23</volume>(<issue>6</issue>): <fpage>921</fpage>–<lpage>932</lpage>.</citation></ref>
<ref id="bibr36-0962280212448970"><label>36</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><etal/></person-group>. <article-title>Mixture models with adaptive spatial regularization for segmentation with an application to FMRI data</article-title>. <source>IEEE Trans Med Imaging</source> <year>2005</year>; <volume>24</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>11</lpage>.</citation></ref>
<ref id="bibr37-0962280212448970"><label>37</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Besag</surname><given-names>J</given-names></name></person-group>. <article-title>Statistical analysis of non-lattice data</article-title>. <source>The Statistician</source> <year>1975</year>; <volume>24</volume>(<issue>3</issue>): <fpage>179</fpage>–<lpage>195</lpage>.</citation></ref>
<ref id="bibr38-0962280212448970"><label>38</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Melas</surname><given-names>DE</given-names></name><name><surname>Wilson</surname><given-names>SP</given-names></name></person-group>. <article-title>Double Markov random fields and Bayesian image segmentation</article-title>. <source>IEEE Trans Signal Process</source> <year>2002</year>; <volume>50</volume>: <fpage>357</fpage>–<lpage>365</lpage>.</citation></ref>
</ref-list>
</back>
</article>