<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AEI</journal-id>
<journal-id journal-id-type="hwp">spaei</journal-id>
<journal-title>Assessment for Effective Intervention</journal-title>
<issn pub-type="ppub">1534-5084</issn>
<issn pub-type="epub">1938-7458</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1534508412441966</article-id>
<article-id pub-id-type="publisher-id">10.1177_1534508412441966</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>General Articles</subject>
<subj-group subj-group-type="heading">
<subject>Brief/Psychometric Report</subject>
</subj-group>
</subj-group>
</article-categories>
<title-group>
<article-title>The Influence of Alternative Scale Formats on the Generalizability of Data Obtained From Direct Behavior Rating Single-Item Scales (DBR-SIS)</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Petscher</surname><given-names>Yaacov</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Cummings</surname><given-names>Kelli D.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Biancarosa</surname><given-names>Gina</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Fien</surname><given-names>Hank</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Briesch</surname><given-names>Amy M.</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff1-1534508412441966">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kilgus</surname><given-names>Stephen P.</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff2-1534508412441966">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Chafouleas</surname><given-names>Sandra M.</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff3-1534508412441966">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Riley-Tillman</surname><given-names>T. Chris</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff4-1534508412441966">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Christ</surname><given-names>Theodore J.</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff5-1534508412441966">5</xref>
</contrib>
</contrib-group>
<aff id="aff1-1534508412441966"><label>1</label>Northeastern University, Boston, MA, USA</aff>
<aff id="aff2-1534508412441966"><label>2</label>East Carolina University, Greenville, NC, USA</aff>
<aff id="aff3-1534508412441966"><label>3</label>University of Connecticut, Storrs, CT, USA</aff>
<aff id="aff4-1534508412441966"><label>4</label>University of Missouri, Columbia, MO, USA</aff>
<aff id="aff5-1534508412441966"><label>5</label>University of Minnesota, Minneapolis, MN, USA</aff>
<author-notes>
<corresp id="corresp1-1534508412441966">Amy M. Briesch, Northeastern University, Department of Counseling and Applied Educational Psychology, 404 International Village, 360 Huntington Avenue, Boston, MA 02115, USA Email: <email>a.briesch@neu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>38</volume>
<issue>2</issue>
<issue-title>Special Series: Measurement Issues in the Assessment of Reading Fluency</issue-title>
<fpage>127</fpage>
<lpage>133</lpage>
<permissions>
<copyright-statement>© 2013 Hammill Institute on Disabilities</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">Hammill Institute on Disabilities</copyright-holder>
</permissions>
<abstract>
<p>The current study served to extend previous research on scaling construction of Direct Behavior Rating (DBR) in order to explore the potential flexibility of DBR to fit various intervention contexts. One hundred ninety-eight undergraduate students viewed the same classroom footage but rated student behavior using one of eight randomly assigned scales (i.e., differed with regard to number of gradients, length of scale, discrete vs. continuous). Descriptively, mean ratings typically fell within the same scale gradient across conditions. Furthermore, results of generalizability analyses revealed negligible variance attributable to the facet of scale type or interaction terms involving this facet. Implications for DBR scale construction within the context of intervention-related decision making are presented and discussed.</p>
</abstract>
<kwd-group>
<kwd>emotional/behavioral disorders</kwd>
<kwd>rating scales</kwd>
<kwd>social–emotional</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Although traditionally conceived of as an intervention tool (e.g., daily behavior report card; <xref ref-type="bibr" rid="bibr7-1534508412441966">Chafouleas, Riley-Tillman, &amp; McDougal, 2002</xref>; <xref ref-type="bibr" rid="bibr25-1534508412441966">Vannest et al., 2010</xref>), use of Direct Behavior Rating (DBR) has gained recent interest within an assessment context in light of the highlighted need to identify appropriate tools for use within problem-solving models (<xref ref-type="bibr" rid="bibr11-1534508412441966">Chafouleas, Volpe, Gresham, &amp; Cook, 2010</xref>). That is, within a proactive model of service delivery, decisions regarding student performance and response to instructional and behavioral supports must be made efficiently and for a greater number of students. As a result, there exists a need to identify assessment tools that are both effective (i.e., psychometrically defensible) and feasible for regular use. It has been suggested that the use of DBR may meet both requirements (<xref ref-type="bibr" rid="bibr10-1534508412441966">Chafouleas, Riley-Tillman, &amp; Sugai, 2007</xref>). DBR involves conducting a single rating of operationally defined behavior(s) of interest (e.g., academic engagement) at the end of a prespecified rating period (e.g., a math lesson or unit; <xref ref-type="bibr" rid="bibr9-1534508412441966">Chafouleas, Riley-Tillman, &amp; Sugai, 2007</xref>). Surveys of DBR use have included both teacher and school psychologist samples, with results generally suggesting moderate to high use for assessment purposes (<xref ref-type="bibr" rid="bibr8-1534508412441966">Chafouleas, Riley-Tillman, &amp; Sassu, 2006</xref>; <xref ref-type="bibr" rid="bibr20-1534508412441966">Riley-Tillman, Chafouleas, Briesch, &amp; Eckert, 2008</xref>). However, clear understanding regarding what actual usage looks like in practice, particularly with regard to issues of instrumentation, has not been provided.</p>
<p>In an effort to systematize empirical work related to building a psychometric base of evidence for DBR use in assessment, researchers have focused on investigations relevant to DBR Single-Item Scales (DBR-SIS; <xref ref-type="bibr" rid="bibr13-1534508412441966">Christ, Riley-Tillman, &amp; Chafouleas, 2009</xref>). Fundamentally, DBR-SIS can be described as a unipolar graphic rating scale, in that (a) the left end of the scale represents an absence of the behavior (i.e., 0%) whereas the right indicates a strong presence (i.e., 100%), and (b) graphic descriptions (e.g., anchors) are typically provided along the scale (<xref ref-type="bibr" rid="bibr12-1534508412441966">Christ &amp; Boice, 2009</xref>). Beyond these general categorizations, however, variability in scale format has been noted across studies with regard to the gradients and anchors used. Some investigations have used a DBR-SIS consisting of a line divided into six equal gradients (i.e., 0–5), with percentage anchors at each gradient (e.g., 0%, 20%, . . . 100%; <xref ref-type="bibr" rid="bibr9-1534508412441966">Chafouleas, Riley-Tillman, Sassu, LaFrance, &amp; Patwa, 2007</xref>). In contrast, other studies have used an 11-gradient line (i.e., 0–10), with percentage anchors included at only three points (i.e., 0%, 50%, 100%; <xref ref-type="bibr" rid="bibr4-1534508412441966">Chafouleas et al., 2010</xref>). More recent work has suggested that ratings may yield technically defensible data given that at least six gradients are used (<xref ref-type="bibr" rid="bibr5-1534508412441966">Chafouleas, Christ, &amp; Riley-Tillman, 2009</xref>; <xref ref-type="bibr" rid="bibr13-1534508412441966">Christ et al., 2009</xref>). Direct comparisons of these scale formats are needed, however, to evaluate the consistency of obtained data.</p>
<p>Literature spanning several decades supports a relation between scale construction and the technical adequacy of data. For instance, although somewhat inconsistent, numerous studies have revealed a relationship between the number of scale gradients and psychometric defensibility. <xref ref-type="bibr" rid="bibr26-1534508412441966">Weng (2004)</xref> identified a positive correlation between the number of gradients and both coefficient alpha and test–retest reliability coefficients. Researchers have also considered the influence of multiple graphic rating scale components, including line length. For example, <xref ref-type="bibr" rid="bibr19-1534508412441966">Revill, Robinson, Rosen, and Hogg (1976)</xref> compared 5-, 10-, 15-, and 20-cm lines, finding the shortest line to be associated with the greatest error. Finally, other work (e.g., <xref ref-type="bibr" rid="bibr16-1534508412441966">Preston &amp; Colman, 2000</xref>) has compared categorical and continuous scales. Some findings have indicated that raters demonstrate a preference for categorical scales given that they are (a) more consistent with natural judgments and (b) less time-intensive to code than their continuous counterparts (<xref ref-type="bibr" rid="bibr17-1534508412441966">Ramsey, 1973</xref>). Others, however, have specified that the use of a continuous scale may offer greater specificity, particularly when very few ratings are to be made or in the absence of summation, as in the case with DBR-SIS (<xref ref-type="bibr" rid="bibr12-1534508412441966">Christ &amp; Boice, 2009</xref>).</p>
<p>To date, two studies have investigated such issues (e.g., gradients, anchoring) related to DBR-SIS construction. In a study by <xref ref-type="bibr" rid="bibr5-1534508412441966">Chafouleas et al. (2009)</xref>, 125 undergraduate students were asked to indicate the total amount of time (0–60 seconds) that the target behavior (i.e., visually distracted, active manipulation) was observed on a 100-mm line marked with three qualitative anchors (i.e., <italic>never, sometimes, always</italic>). Scales differed, however, across three within-participant experimental conditions based on the number of scale gradients (i.e., 6, 10, 14) applied to the line. Generalizability study results were found to be roughly similar across scale gradient conditions, with the greatest proportions of rating variance attributable to error (i.e., <italic>pro, e</italic>; range = 35%–39%), differences between raters (i.e., <italic>o</italic>; range = 17%–23%), and changes in the rank ordering of students across time (i.e., <italic>p</italic> × <italic>o</italic>; range = 11%–17%). Results therefore suggested that the number of gradients applied to an otherwise identical scale should not affect the reliability of DBR-SIS data.</p>
<p>More recently, 81 undergraduate student participants were provided with a 100-mm, 10-gradient DBR-SIS to rate student levels of academic engagement and disruptive behavior across a series of video clips (<xref ref-type="bibr" rid="bibr21-1534508412441966">Riley-Tillman, Christ, Chafouleas, Boice-Mallach, &amp; Briesch, 2010</xref>). In this case, each participant was randomly assigned to use either a proportional scale (i.e., 0%, 20%, 90%), or an absolute scale (i.e., 1 min, 4 min). Generalizability findings indicated that neither approach to scale anchoring was associated with greater rating accuracy. However, the authors suggested that proportional scaling might be more efficient, as interpretation does not require knowledge of observation duration. One limitation associated with each of the aforementioned studies was that all ratings were conducted using a 100-mm line. Given previous evidence to suggest that scale length may influence rating error (<xref ref-type="bibr" rid="bibr19-1534508412441966">Revill et al., 1976</xref>), it is unclear whether the results specific to scaling gradients and anchoring may generalize to other scale formats.</p>
<p>Initial findings of consistency across approaches to scale construction appear to support the potential flexibility in constructing DBR-SIS; however, such flexibility and related variability in construction across studies can also limit the cumulative interpretation of findings from DBR-SIS research. For example, variations may restrict the degree to which psychometric results apply, and thus, it is recommended that direct comparisons be made to determine whether these scales function differently. In this way, those school-based professionals interested in using DBR to inform defensible intervention-based decisions would be provided guidance with regard to the flexibility of scale construction. Thus, the purpose of the current study was to expand upon existing research on DBR-SIS instrumentation through an evaluation of the influence of various scale formats on the generalizability of ratings of student behavior. These features included the number of scale gradients provided (i.e., 5 or 10), the length of the scale itself (i.e., 50 or 100 mm), and the use of either a discrete or continuous scale. Given that minimal differences between scale types have been previously noted in the literature, it was hypothesized that evidence of convergent validity across scale types would be identified in the current study.</p>
<sec id="section1-1534508412441966" sec-type="methods">
<title>Method</title>
<sec id="section2-1534508412441966">
<title>Participants and Setting</title>
<p>Participants included 198 undergraduate students enrolled in an introductory psychology course at a large southeastern university and represented a sample of convenience. Per Human Subjects Institutional Review Board–approved procedures, written informed consent was attained from each participant prior to enrollment. The majority (58%) of participants were female, and identified themselves as either White (61%) or Black (24%). Roughly 30% of participants were currently enrolled in a teacher education program.</p>
</sec>
<sec id="section3-1534508412441966">
<title>Materials</title>
<sec id="section4-1534508412441966">
<title>Videotape</title>
<p>As the design of the study required that all raters be able to view an identical snapshot of behavior, preexisting video footage of elementary school–aged students was used. Parental consent had previously been obtained for the children to serve as actors during a period of simulated classroom instruction designed to ensure sufficient variability in the behaviors of interest. Most of the simulated instruction was unscripted; however, general visual cues (e.g., signs reading “get out of seat”) were provided to ensure that students displayed a variety of both appropriate behaviors (e.g., passive and active engagement) and inappropriate behaviors (e.g., calling out, noncompliance). The final four 3-min video clips were chosen based on the fact that several children could be clearly seen in the frame exhibiting a range of behaviors typically observed in educational settings.</p>
</sec>
<sec id="section5-1534508412441966">
<title>DBR-SIS forms</title>
<p>All DBR-SIS forms required participants to observe and rate two common classroom behaviors: academic engagement (AE) and disruptive behavior (DB). For the purposes of this study, <xref ref-type="bibr" rid="bibr22-1534508412441966">Shapiro’s (2004)</xref> definition of academic engagement used in the Behavioral Observation of Students in Schools (BOSS) was adopted. That is, AE was defined as actively (e.g., writing, raising hand) <italic>or</italic> passively (e.g., listening to the teacher, reading silently) participating in classroom activities. DB was defined as any student action that interrupts regular school or classroom activities (e.g., being out of seat, calling out). Although all participants rated the same target behaviors, the graphic presentation of the DBR-SIS varied across conditions. Scales varied with regard to (a) scale type (i.e., continuous, discrete), (b) number of scale gradients provided (i.e., 5, 10), and (c) scale length (i.e., 50 mm, 100 mm). This resulted in a total of eight experimental groups with roughly 25 participants assigned to each condition.</p>
</sec>
</sec>
<sec id="section6-1534508412441966">
<title>Procedure</title>
<p>Participants were randomly assigned to an experimental condition (i.e., DBR-SIS format) on entering one of five possible experimental sessions (each 25–30 min in duration) scheduled across a 3-week period. Because of the fact that different scales were used, examiner instructions remained general. Participants were allowed 4 min to independently review the behavioral definitions, as well as an example of how to use the scale to conduct their ratings. Participants were allowed continuous access to definitions, instructions, and rating examples throughout study proceedings. Next, all participants were asked to carefully observe four of the eight students present in the classroom and to rate the AE and DB displayed by each student immediately following each 3-min clip. All participants were given up to 2 min to complete their ratings using only the scale type assigned to his or her condition. In total, each participant conducted 32 ratings (i.e., four target students × four video clips × two behaviors).</p>
</sec>
<sec id="section7-1534508412441966">
<title>Dependent Measure</title>
<p>The primary dependent variable was the DBR score assigned to each target student. Data coders used 12-inch rulers to determine the exact point at which the participants’ ratings fell. To facilitate direct comparisons across DBR types, millimeter values were subsequently converted to an 11-point (i.e., 0–10) discrete scale using a ±5-mm margin of error for each discrete value. That is, ratings falling less than 5 mm below, or 5 mm above, each decile (i.e., 10, 20, etc.) were considered to fall within that decile (e.g., 47 mm or 54 mm both equate to 5 on the discrete scale). Intercoder reliability was determined for 10% of cases, and was found to be extremely high (<italic>M</italic> = 1.00).</p>
</sec>
<sec id="section8-1534508412441966">
<title>Data Analysis</title>
<p>Subsequent to data screening, descriptive statistics were first reviewed in order to examine the degree to which ratings differed across scale types. Generalizability theory (GT; see <xref ref-type="bibr" rid="bibr2-1534508412441966">Brennan, 2001</xref>, for comprehensive discussion of GT) was then used in order to determine the percentage of rating variance attributable to relevant sources of error (i.e., facets). In the current investigation, the facet of scale type was of primary importance; however, variance attributable to differences across persons, observations, and raters was also examined. Generalizability (G) studies were conducted using a partially nested, random effects model for person (<italic>p</italic>; i.e., student) by observation (<italic>o</italic>; i.e., video clip) by rater (<italic>r</italic>; i.e., study participant) nested within scale type (<italic>s</italic>; i.e., assigned participant condition) (i.e., <italic>p</italic> × <italic>o</italic> × (<italic>r:s</italic>)). The variance components derived from a G study can be used in a decision (D) study to generate reliability-like coefficients that can be used for the purposes of both relative and absolute decision making. Given that the purpose of the current study was to assess validity (i.e., evidence of convergent validity across scale types) and not reliability, however, only G study results are presented. All variance components were derived in SPSS 17.0 using an ANOVA with Type III sum of squares.</p>
</sec>
</sec>
<sec id="section9-1534508412441966" sec-type="results">
<title>Results</title>
<sec id="section10-1534508412441966">
<title>Rating Descriptives</title>
<p>Descriptively, results suggested that ratings did not vary substantively across DBR formats, with 91% of mean ratings fluctuating no more than 1 point between DBR scales. In fact, for a given student and video clip, the mean range of DBR scores across scaling conditions was 1.12 points for AE and 1.02 points for DB. The largest range (1.70 points) was observed for the rating of Student 3’s AE during Clip 4 (see <xref ref-type="table" rid="table1-1534508412441966">Table 1</xref>).</p>
<table-wrap id="table1-1534508412441966" position="float">
<label>Table 1.</label>
<caption><p>Means and Standard Deviations for Academic Engagement Across Groups by Clip and Student</p></caption>
<graphic alternate-form-of="table1-1534508412441966" xlink:href="10.1177_1534508412441966-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="16">Clip 1<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">Group 1<hr/></th>
<th align="center" colspan="2">Group 2<hr/></th>
<th align="center" colspan="2">Group 3<hr/></th>
<th align="center" colspan="2">Group 4<hr/></th>
<th align="center" colspan="2">Group 5<hr/></th>
<th align="center" colspan="2">Group 6<hr/></th>
<th align="center" colspan="2">Group 7<hr/></th>
<th align="center" colspan="2">Group 8<hr/></th>
</tr>
<tr>
<th align="left">Student</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6.14</td>
<td>2.06</td>
<td>6.56</td>
<td>2.43</td>
<td>6.41</td>
<td>2.50</td>
<td>6.00</td>
<td>2.06</td>
<td>6.17</td>
<td>2.06</td>
<td>5.79</td>
<td>2.96</td>
<td>5.86</td>
<td>2.17</td>
<td>6.56</td>
<td>2.42</td>
</tr>
<tr>
<td>2</td>
<td>7.84</td>
<td>2.33</td>
<td>7.68</td>
<td>2.49</td>
<td>7.83</td>
<td>2.41</td>
<td>7.04</td>
<td>2.59</td>
<td>7.46</td>
<td>2.90</td>
<td>8.04</td>
<td>1.99</td>
<td>8.02</td>
<td>2.20</td>
<td>8.27</td>
<td>1.92</td>
</tr>
<tr>
<td>3</td>
<td>6.59</td>
<td>3.05</td>
<td>7.08</td>
<td>3.09</td>
<td>7.57</td>
<td>2.69</td>
<td>6.72</td>
<td>2.63</td>
<td>7.28</td>
<td>2.91</td>
<td>7.91</td>
<td>2.44</td>
<td>7.52</td>
<td>2.85</td>
<td>7.29</td>
<td>3.21</td>
</tr>
<tr>
<td>4</td>
<td>5.63</td>
<td>2.69</td>
<td>5.84</td>
<td>2.79</td>
<td>5.37</td>
<td>2.22</td>
<td>5.46</td>
<td>2.32</td>
<td>4.67</td>
<td>3.14</td>
<td>6.23</td>
<td>2.86</td>
<td>6.27</td>
<td>2.38</td>
<td>6.13</td>
<td>2.76</td>
</tr>
<tr>
<th/>
<th align="center" colspan="16">Clip 2<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">Group 1<hr/></th>
<th align="center" colspan="2">Group 2<hr/></th>
<th align="center" colspan="2">Group 3<hr/></th>
<th align="center" colspan="2">Group 4<hr/></th>
<th align="center" colspan="2">Group 5<hr/></th>
<th align="center" colspan="2">Group 6<hr/></th>
<th align="center" colspan="2">Group 7<hr/></th>
<th align="center" colspan="2">Group 8<hr/></th>
</tr>
<tr>
<th align="left">Student</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
<tr>
<td>1</td>
<td>6.20</td>
<td>1.71</td>
<td>6.82</td>
<td>2.14</td>
<td>6.62</td>
<td>2.63</td>
<td>6.71</td>
<td>1.62</td>
<td>7.15</td>
<td>1.86</td>
<td>6.45</td>
<td>2.81</td>
<td>5.95</td>
<td>2.48</td>
<td>6.75</td>
<td>1.93</td>
</tr>
<tr>
<td>2</td>
<td>7.59</td>
<td>2.67</td>
<td>7.76</td>
<td>2.44</td>
<td>7.68</td>
<td>3.05</td>
<td>7.31</td>
<td>2.30</td>
<td>8.04</td>
<td>2.73</td>
<td>8.38</td>
<td>2.04</td>
<td>8.25</td>
<td>2.58</td>
<td>7.96</td>
<td>2.91</td>
</tr>
<tr>
<td>3</td>
<td>7.80</td>
<td>2.44</td>
<td>7.94</td>
<td>2.58</td>
<td>8.15</td>
<td>2.59</td>
<td>7.58</td>
<td>2.61</td>
<td>7.96</td>
<td>2.62</td>
<td>8.09</td>
<td>2.59</td>
<td>8.70</td>
<td>1.80</td>
<td>7.93</td>
<td>3.06</td>
</tr>
<tr>
<td>4</td>
<td>6.90</td>
<td>2.27</td>
<td>7.46</td>
<td>1.95</td>
<td>6.98</td>
<td>2.54</td>
<td>6.98</td>
<td>2.04</td>
<td>7.22</td>
<td>2.53</td>
<td>7.28</td>
<td>2.67</td>
<td>7.41</td>
<td>2.34</td>
<td>8.27</td>
<td>1.64</td>
</tr>
<tr>
<th/>
<th align="center" colspan="16">Clip 3<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">Group 1<hr/></th>
<th align="center" colspan="2">Group 2<hr/></th>
<th align="center" colspan="2">Group 3<hr/></th>
<th align="center" colspan="2">Group 4<hr/></th>
<th align="center" colspan="2">Group 5<hr/></th>
<th align="center" colspan="2">Group 6<hr/></th>
<th align="center" colspan="2">Group 7<hr/></th>
<th align="center" colspan="2">Group 8<hr/></th>
</tr>
<tr>
<th align="left">Student</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
<tr>
<td>1</td>
<td>2.27</td>
<td>2.19</td>
<td>2.82</td>
<td>1.93</td>
<td>2.98</td>
<td>2.53</td>
<td>3.31</td>
<td>2.54</td>
<td>2.54</td>
<td>2.56</td>
<td>2.36</td>
<td>1.98</td>
<td>2.50</td>
<td>2.13</td>
<td>2.64</td>
<td>2.02</td>
</tr>
<tr>
<td>2</td>
<td>7.61</td>
<td>1.83</td>
<td>8.16</td>
<td>2.11</td>
<td>8.13</td>
<td>2.41</td>
<td>7.67</td>
<td>1.87</td>
<td>7.80</td>
<td>2.62</td>
<td>8.21</td>
<td>2.00</td>
<td>8.09</td>
<td>2.11</td>
<td>7.80</td>
<td>2.50</td>
</tr>
<tr>
<td>3</td>
<td>8.12</td>
<td>1.89</td>
<td>7.02</td>
<td>3.13</td>
<td>7.77</td>
<td>2.50</td>
<td>6.94</td>
<td>2.54</td>
<td>7.61</td>
<td>2.90</td>
<td>8.17</td>
<td>2.37</td>
<td>7.39</td>
<td>2.90</td>
<td>7.95</td>
<td>2.64</td>
</tr>
<tr>
<td>4</td>
<td>8.71</td>
<td>1.78</td>
<td>8.58</td>
<td>1.60</td>
<td>8.49</td>
<td>1.68</td>
<td>8.19</td>
<td>1.50</td>
<td>8.30</td>
<td>2.30</td>
<td>9.06</td>
<td>1.36</td>
<td>8.93</td>
<td>1.35</td>
<td>8.91</td>
<td>1.24</td>
</tr>
<tr>
<th/>
<th align="center" colspan="16">Clip 4<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">Group 1<hr/></th>
<th align="center" colspan="2">Group 2<hr/></th>
<th align="center" colspan="2">Group 3<hr/></th>
<th align="center" colspan="2">Group 4<hr/></th>
<th align="center" colspan="2">Group 5<hr/></th>
<th align="center" colspan="2">Group 6<hr/></th>
<th align="center" colspan="2">Group 7<hr/></th>
<th align="center" colspan="2">Group 8<hr/></th>
</tr>
<tr>
<th align="left">Student</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
<tr>
<td>1</td>
<td>3.24</td>
<td>2.17</td>
<td>4.29</td>
<td>2.35</td>
<td>4.04</td>
<td>2.37</td>
<td>3.71</td>
<td>2.53</td>
<td>3.28</td>
<td>2.14</td>
<td>3.94</td>
<td>2.55</td>
<td>4.07</td>
<td>2.24</td>
<td>3.84</td>
<td>2.51</td>
</tr>
<tr>
<td>2</td>
<td>0.52</td>
<td>1.39</td>
<td>0.78</td>
<td>1.95</td>
<td>0.77</td>
<td>1.74</td>
<td>1.19</td>
<td>2.16</td>
<td>0.20</td>
<td>0.72</td>
<td>1.15</td>
<td>2.76</td>
<td>0.86</td>
<td>1.88</td>
<td>0.60</td>
<td>1.76</td>
</tr>
<tr>
<td>3</td>
<td>5.90</td>
<td>2.26</td>
<td>7.60</td>
<td>2.25</td>
<td>6.45</td>
<td>2.73</td>
<td>6.33</td>
<td>2.38</td>
<td>6.33</td>
<td>2.56</td>
<td>6.60</td>
<td>2.58</td>
<td>7.34</td>
<td>1.78</td>
<td>6.93</td>
<td>2.41</td>
</tr>
<tr>
<td>4</td>
<td>8.16</td>
<td>1.95</td>
<td>8.59</td>
<td>2.06</td>
<td>7.78</td>
<td>2.55</td>
<td>7.98</td>
<td>1.42</td>
<td>7.98</td>
<td>2.24</td>
<td>8.47</td>
<td>2.02</td>
<td>8.55</td>
<td>1.85</td>
<td>8.69</td>
<td>1.82</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section11-1534508412441966">
<title>Generalizability Analyses</title>
<p>GT was used to evaluate whether DBR outcomes were similar across scaling conditions, as well as to consider alternative facets or interactions to which rating variance may be attributed. Variance component analyses revealed several consistencies across the behaviors examined (see <xref ref-type="table" rid="table2-1534508412441966">Table 2</xref>). First, the greatest proportion of rating variance (42% AE, 40% DB) was explained by the interaction between persons and occasions. This indicates that the rank order of students varied substantially from one video clip to the next, which follows from the fact that video clips were purposively selected based on behavioral variability demonstrated. Most pertinent to the purpose of the study, nearly all facets and interactions involving scale type (i.e., scale type, persons by scale type, occasions by scale type, persons by occasions by scale type) were found to contribute negligible variance to the model. Across scales, no overall rating differences were identified and the rank order of students did not change. A notable percentage of variance (12% AE, 5% DB) was attributable, however, to the term involving raters nested within scale types.</p>
<table-wrap id="table2-1534508412441966" position="float">
<label>Table 2.</label>
<caption><p>Full Model Generalizability Study Results: <italic>Person, Scale, Rater:Scale, Occasion</italic></p></caption>
<graphic alternate-form-of="table2-1534508412441966" xlink:href="10.1177_1534508412441966-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">AE<hr/></th>
<th align="center" colspan="2">DB<hr/></th>
</tr>
<tr>
<th align="left">Facet(s)</th>
<th align="center">Var</th>
<th align="center">% Var</th>
<th align="center">Var</th>
<th align="center">% Var</th>
</tr>
</thead>
<tbody>
<tr>
<td>Person</td>
<td>0.48</td>
<td>5</td>
<td>3.81</td>
<td>28</td>
</tr>
<tr>
<td>Scale type</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Observation</td>
<td>0.09</td>
<td>1</td>
<td>0.37</td>
<td>3</td>
</tr>
<tr>
<td>Rater:scale type</td>
<td>1.24</td>
<td>12</td>
<td>0.70</td>
<td>5</td>
</tr>
<tr>
<td>Person × scale type</td>
<td>0.03</td>
<td>0</td>
<td>0.03</td>
<td>0</td>
</tr>
<tr>
<td>Person × observation</td>
<td>4.45</td>
<td>42</td>
<td>5.57</td>
<td>40</td>
</tr>
<tr>
<td>Observation × scale type</td>
<td>0.01</td>
<td>0</td>
<td>0.01</td>
<td>0</td>
</tr>
<tr>
<td>Person × scale × observation</td>
<td>0.01</td>
<td>0</td>
<td>0.01</td>
<td>0</td>
</tr>
<tr>
<td>Error<sup><xref ref-type="table-fn" rid="table-fn2-1534508412441966">a</xref></sup></td>
<td>4.281</td>
<td>41</td>
<td>3.30</td>
<td>24</td>
</tr>
<tr>
<td>Total</td>
<td>10.57</td>
<td>100<sup><xref ref-type="table-fn" rid="table-fn3-1534508412441966">b</xref></sup></td>
<td>13.79</td>
<td>100<sup><xref ref-type="table-fn" rid="table-fn3-1534508412441966">b</xref></sup></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1534508412441966">
<p><italic>Note</italic>. AE = academic engagement; DB = disruptive behavior; Var = variance calculated using Type III sum of squares; %Var = percentage of total variance.</p>
</fn>
<fn id="table-fn2-1534508412441966">
<label>a.</label>
<p>Includes residual along with interactions involving <italic>r:s.</italic></p>
</fn>
<fn id="table-fn3-1534508412441966">
<label>b.</label>
<p>Values rounded to 100%.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Although many consistencies were observed across behaviors, two differences should be noted. First, the facet of person explained 28% of the variance in ratings of DB but only 5% in the case of AE, suggesting that student engagement levels were more consistent when averaged across raters and observations. Related, a significant proportion of the rating variance for AE (41%) remained unexplained by facets of the model in comparison to a smaller proportion for DB (24%). Finally, the nested term involving the main effect of raters and the interaction between raters and scale types explained a greater proportion of variance for AE (12%) than for DB (5%). Although this term cannot be neatly interpreted given the confounded effects, this generally suggests that raters were more consistent in their assessment of DB than AE.</p>
</sec>
</sec>
<sec id="section12-1534508412441966" sec-type="discussion">
<title>Discussion</title>
<p>The purpose of the current study was to examine the influence of DBR scale construction on obtained ratings. Overall, ratings were not found to vary significantly across the DBR-SIS formats employing different gradients (5 vs. 10), lengths (50 mm vs. 100 mm), and scaling approaches (continuous vs. discrete). This is consistent with previous scaling research, which found no statistically significant differences between scales employing either (a) 5 or 10 scale gradients (<xref ref-type="bibr" rid="bibr1-1534508412441966">Bendig, 1954</xref>; <xref ref-type="bibr" rid="bibr5-1534508412441966">Chafouleas et al., 2009</xref>; Matell &amp; Jacoby, 1971), or (b) continuous or categorical scales (<xref ref-type="bibr" rid="bibr14-1534508412441966">Cicchetti, Showalter, &amp; Tyrer, 1985</xref>; <xref ref-type="bibr" rid="bibr18-1534508412441966">Rasmussen, 1989</xref>). Raw ratings converted to an 11-point discrete scale were found to be relatively similar across groups, with mean DBR scores falling within 1.12 points when rating AE and 1.02 points when rating DB.</p>
<p>Results of generalizability analyses also supported the finding of minimal differences across DBR scale formats. Analyses demonstrated that negligible variance (0%) was attributable to the facet of scale type (indicating no overall differences in rating behavior across scales) or the interaction between person and scale type (indicating no changes in the rank order of students across scales). A small proportion of variance was, however, attributable to the term involving raters nested within scale types (12% AE, 5% DB), generally suggesting that how a particular scale was used differed from one rater to another. Such a finding is not surprising given that significant rating differences have been noted across DBR users in the absence of rater training (<xref ref-type="bibr" rid="bibr3-1534508412441966">Briesch, Chafouleas, &amp; Riley-Tillman, 2010</xref>; <xref ref-type="bibr" rid="bibr6-1534508412441966">Chafouleas, Christ et al., 2007</xref>). This finding does, however, further support the need to consider DBR recordings within rater. In addition, results were consistent with the findings of <xref ref-type="bibr" rid="bibr4-1534508412441966">Chafouleas, Briesch, and colleagues (2010)</xref>, in which greater differences between raters were noted when rating AE (8%) than DB (1%). The higher saliency of disruptive behavior may explain why ratings have been more consistent, and highlights the fact that different behavior targets may warrant varying levels of rater training.</p>
<p>It is also worth noting that the greatest proportion of variance in ratings of both AE and DB (i.e., roughly 40%) was attributable to the interaction between persons and occasions, thus indicating that the rank order of students varied widely from one observation occasion to the next. Such a finding was not surprising given the fact that video clips were purposively selected to ensure behavioral variability. These results do, however, lend support to the potential role of DBR-SIS in monitoring student behavior, particularly with regard to sensitivity to change. Capacity for sensitivity to change is particularly important to ensure that behavioral data are reflective of actual variation in the trend or level of student behavior. The size of the variance component for the interaction between persons and occasions thus supports that DBR-SIS data are sufficiently sensitive to behavioral changes over time. This is a key finding for classroom teachers and support personnel interested in using DBR to monitor student response to intervention and detect changes in behavior over time.</p>
<p>Finally, 24% (DB) to 41% (AE) of the observed rating variance was subsumed under the residual error term, suggesting the influence of other factors that were either uncontrolled for in the model (e.g., time of day) or uninterpretable because of the nesting of facets. A recent study, for example, found that the interaction between raters and persons (i.e., differential rating of particular students) accounted for a significant proportion of the variance (20%) in DBR ratings (<xref ref-type="bibr" rid="bibr3-1534508412441966">Briesch et al., 2010</xref>). Because of the nesting of raters within scale types within the current study, the effect of this interaction could not be independently estimated; however, this effect may help to account for some degree of the residual error observed.</p>
<sec id="section13-1534508412441966">
<title>Limitations</title>
<p>One limitation of the current study was that all ratings originally made on a continuous scale were converted to discrete values. This was deemed necessary in order to make meaningful comparisons between the ratings; however, some degree of rating variability was inevitably lost. Although the eight scales appear relatively equivalent, mean ratings would likely appear somewhat different had the continuous values been used. Second, despite precedence for the methodology within DBR literature (e.g., <xref ref-type="bibr" rid="bibr5-1534508412441966">Chafouleas et al., 2009</xref>; <xref ref-type="bibr" rid="bibr21-1534508412441966">Riley-Tillman et al., 2010</xref>) and beyond (<xref ref-type="bibr" rid="bibr23-1534508412441966">Sterling-Turner &amp; Watson, 2002</xref>; <xref ref-type="bibr" rid="bibr24-1534508412441966">Sterling-Turner, Watson, Wildmon, Watkins, &amp; Little, 2001</xref>), the use of undergraduate students as research participants is considered a limitation. Although previous research has highlighted similarities between teacher-generated ratings and those conducted by external raters who do not have competing demands on their attention (i.e., research assistants) (e.g., Chafouleas, <xref ref-type="bibr" rid="bibr3-1534508412441966">Briesch, et al., 2010</xref>), it is unknown whether or not undergraduate students are as generally attuned to classroom behaviors as actual teachers or even graduate students. Therefore, potential differences in cognitive set may attenuate generalization to a population of actual educators. This limitation to external validity was deemed necessary in order to simultaneously explore a number of different DBR scales; however, further research is therefore needed to determine whether the current findings would generalize when conducted with actual teachers in applied classroom settings. Third, the nature of the experimental design, in which participants were randomly assigned to scale conditions, served to limit the data analyses that could be conducted. Had the design been one that was fully crossed (e.g., all participants rate all students using all scales), it would have been possible to examine the proportion of variance in ratings attributable specifically to rater differences. The benefits in terms of supplemental evidence, however, were outweighed by the costs in terms of participant resources, and the defined primary purpose of this study.</p>
</sec>
<sec id="section14-1534508412441966">
<title>Implications for Researchers and Practitioners</title>
<p>Overall, results support one of the proposed features of DBR-SIS; namely, that the method may be flexibly constructed (<xref ref-type="bibr" rid="bibr10-1534508412441966">Chafouleas, Riley-Tillman, &amp; Sugai, 2007</xref>). This holds implications for school-based users of DBR-SIS as it suggests that choices of (a) a continuous or categorical scale, (b) number of scale gradients, and (c) length of DBR-SIS graphic line can be based on the population and target of measurement, without potential sacrifice to technical adequacy. For example, those individuals working with younger students may prefer to use a smaller number of scale gradients in order to assist with scale interpretability. In addition, scales may be flexibly selected to align with the intervention (and corresponding target behavior) of interest. When implementing an intervention designed to increase compliance, for example, raters may find it more natural to make categorical judgments (e.g., never, sometimes, always compliant) than to use a continuous scale (e.g., 0%, 50%, 100% compliant). The ability to flexibly fit scale development to the idiosyncrasies of individual cases may therefore enhance the extent to which DBR-SIS data inform intervention-related decisions. Scales may be adapted to provide information closely matched to referral concerns, resulting in more appropriate conclusions. This is analogous to the manner in which a practitioner may choose the systematic direct observation coding system (e.g., event recording, interval sampling) that is best suited to the problem behavior dimension and referral question.</p>
<p>Despite the current findings, we suggest that in the absence of replication, it should not be assumed that the current results will automatically apply to any DBR-SIS format. For example, it would be inadvisable to assume evidence of the concurrent validity of a 50-mm, 5-point continuous DBR-SIS also applies to a 100-mm, 10-point continuous DBR-SIS. Although results support such generalization within the current study, caution is recommended in application to past or future research on DBR-SIS.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>Special thanks are extended to Teri LeBel and Christina Boice-Mallach for their assistance with the preparation of materials and data collection.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article:</p>
<p>Preparation of this article was supported by a grant from the Institute for Education Sciences, U.S. Department of Education (R324B060014). Opinions expressed herein do not necessarily reflect the position of the U.S. Department of Education, and such endorsements should not be inferred.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bendig</surname><given-names>A. W.</given-names></name>
</person-group> (<year>1954</year>). <article-title>Reliability and the number of rating scale categories</article-title>.<source>Journal of Applied Psychology</source>, <volume>38</volume>, <fpage>38</fpage>–<lpage>40</lpage>.</citation>
</ref>
<ref id="bibr2-1534508412441966">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2001</year>). <source>Generalizability theory</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr3-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Briesch</surname><given-names>A. M.</given-names></name>
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Generalizability and dependability of behavior assessment methods to estimate academic engagement: A comparison of systematic direct observation and Direct Behavior Rating</article-title>. <source>School Psychology Review</source>, <volume>39</volume>, <fpage>408</fpage>–<lpage>421</lpage>.</citation>
</ref>
<ref id="bibr4-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Briesch</surname><given-names>A. M.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Black</surname><given-names>A.</given-names></name>
<name><surname>Kilgus</surname><given-names>S. P.</given-names></name>
</person-group> (<year>2010</year>). <article-title>An investigation of the generalizability and dependability of Direct Behavior Rating– Single Item Scales (DBR-SIS) to measure academic engagement and disruptive behavior of middle school students</article-title>. <source>Journal of School Psychology</source>, <volume>48</volume>, <fpage>219</fpage>–<lpage>246</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jsp.2010.02.001</pub-id></citation>
</ref>
<ref id="bibr5-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Generalizability of scaling gradients on direct behavior ratings</article-title>. <source>Educational and Psychological Measurement</source>, <volume>69</volume>, <fpage>157</fpage>–<lpage>173</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0013164408322005</pub-id></citation>
</ref>
<ref id="bibr6-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Christ</surname><given-names>T.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Briesch</surname><given-names>A. M.</given-names></name>
<name><surname>Chanese</surname><given-names>J. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Generalizability and dependability of Daily Behavior Report Cards to measure social behavior of preschoolers</article-title>. <source>School Psychology Review</source>, <volume>36</volume>, <fpage>63</fpage>–<lpage>79</lpage>.</citation>
</ref>
<ref id="bibr7-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>McDougal</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Good, bad, or in-between: How does the daily behavior report card rate?</article-title> <source>Psychology in the Schools</source>, <volume>39</volume>, <fpage>157</fpage>–<lpage>169</lpage>.</citation>
</ref>
<ref id="bibr8-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Sassu</surname><given-names>K. A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Acceptability and reported use of daily behavior report cards among teachers</article-title>. <source>Journal of Positive Behavior Interventions</source>, <volume>8</volume>, <fpage>174</fpage>–<lpage>182</lpage>.</citation>
</ref>
<ref id="bibr9-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Sassu</surname><given-names>K. A.</given-names></name>
<name><surname>LaFrance</surname><given-names>M. J.</given-names></name>
<name><surname>Patwa</surname><given-names>S. S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Daily behavior report cards: An investigation of the consistency of on-task data across raters and methods</article-title>. <source>Journal of Positive Behavior Interventions</source>, <volume>9</volume>, <fpage>30</fpage>–<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr10-1534508412441966">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Sugai</surname><given-names>G.</given-names></name>
</person-group> (<year>2007</year>). <source>School-based behavioral assessment: Informing intervention and instruction</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr11-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Volpe</surname><given-names>R. J.</given-names></name>
<name><surname>Gresham</surname><given-names>F. M.</given-names></name>
<name><surname>Cook</surname><given-names>C. R.</given-names></name>
</person-group> (<year>2010</year>). <article-title>School-based behavioral assessment within problem-solving models: Current status and future directions</article-title>. <source>School Psychology Review</source>, <volume>39</volume>, <fpage>343</fpage>–<lpage>349</lpage>.</citation>
</ref>
<ref id="bibr12-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Boice</surname><given-names>C. H.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Rating scale items: A brief review of nomenclature, components, and formatting to inform the development of Direct Behavior Rating (DBR)</article-title>. <source>Assessment for Effective Intervention</source>, <volume>34</volume>, <fpage>242</fpage>–<lpage>250</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1534508409336182</pub-id></citation>
</ref>
<ref id="bibr13-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Foundation for the development and use of Direct Behavior Rating (DBR) to assess and evaluate student behavior</article-title>. <source>Assessment for Effective Intervention</source>, <volume>34</volume>, <fpage>201</fpage>–<lpage>213</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1534508409340390</pub-id></citation>
</ref>
<ref id="bibr14-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cicchetti</surname><given-names>D. V.</given-names></name>
<name><surname>Showalter</surname><given-names>D.</given-names></name>
<name><surname>Tyrer</surname><given-names>P. J.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Scale categories on levels of interrater reliability: A Monte Carlo investigation</article-title>. <source>Applied Psychological Measurement</source>, <volume>9</volume>, <fpage>31</fpage>–<lpage>36</lpage>. doi:<pub-id pub-id-type="doi">10.1177/014662168500900103</pub-id></citation>
</ref>
<ref id="bibr15-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Matell</surname><given-names>M. S.</given-names></name>
<name><surname>Jacoby</surname><given-names>J.</given-names></name>
</person-group> (<year>1972</year>). <article-title>Is there an optimal number of alternatives for Likert-scale items? Effects of testing time and scale properties</article-title>. <source>Journal of Applied Psychology</source>, <volume>56</volume>, <fpage>506</fpage>–<lpage>509</lpage>. doi:<pub-id pub-id-type="doi">10.1037/h0033601</pub-id></citation>
</ref>
<ref id="bibr16-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Preston</surname><given-names>C. C.</given-names></name>
<name><surname>Colman</surname><given-names>A. M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Optimal number of response categories in rating scales: Reliability, validity, discriminating power, and respondent preferences</article-title>. <source>Acta Psychologica</source>, <volume>104</volume>, <fpage>1</fpage>–<lpage>15</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0001-6918(99)00050-5</pub-id></citation>
</ref>
<ref id="bibr17-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ramsey</surname><given-names>J. O.</given-names></name>
</person-group> (<year>1973</year>). <article-title>The effect of number of categories in rating scales on precision of estimation of scale values</article-title>. <source>Psychometrika</source>, <volume>38</volume>, <fpage>513</fpage>–<lpage>532</lpage>. doi:<pub-id pub-id-type="doi">10.1007/BF02291492</pub-id></citation>
</ref>
<ref id="bibr18-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rasmussen</surname><given-names>J. L.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Analysis of Likert-scale data: A reinterpretation of Gregoire and Driver</article-title>. <source>Psychological Bulletin</source>, <volume>105</volume>, <fpage>167</fpage>–<lpage>170</lpage>. doi:<pub-id pub-id-type="doi">10.1037/0033-2909.105.1.167</pub-id></citation>
</ref>
<ref id="bibr19-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Revill</surname><given-names>S. I.</given-names></name>
<name><surname>Robinson</surname><given-names>J. O.</given-names></name>
<name><surname>Rosen</surname><given-names>M.</given-names></name>
<name><surname>Hogg</surname><given-names>M. I.</given-names></name>
</person-group> (<year>1976</year>). <article-title>The reliability of a linear analogue for evaluating pain</article-title>. <source>Anasthesia</source>, <volume>31</volume>, <fpage>1191</fpage>–<lpage>1198</lpage>.</citation>
</ref>
<ref id="bibr20-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Briesch</surname><given-names>A. M.</given-names></name>
<name><surname>Eckert</surname><given-names>T. L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Daily behavior report cards and systematic direct observation: An investigation of the acceptability, reported training and use, and decision reliability among school psychologists</article-title>, <source>Journal of Behavioral Education</source>, <volume>17</volume>, <fpage>313</fpage>–<lpage>327</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10864-008-9070-5</pub-id></citation>
</ref>
<ref id="bibr21-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Chafouleas</surname><given-names>S. M.</given-names></name>
<name><surname>Boice-Mallach</surname><given-names>C. H.</given-names></name>
<name><surname>Briesch</surname><given-names>A. M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The impact of observation duration on the accuracy of data obtained from Direct Behavior Rating (DBR)</article-title>. <source>Journal of Positive Behavior Interventions</source>, <volume>13</volume>, <fpage>119</fpage>–<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr22-1534508412441966">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shapiro</surname><given-names>E. S.</given-names></name>
</person-group> (<year>2004</year>). <source>Academic skills problems workbook</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr23-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sterling-Turner</surname><given-names>H. E.</given-names></name>
<name><surname>Watson</surname><given-names>T. S.</given-names></name>
</person-group> (<year>2002</year>). <article-title>An analog investigation of the relationship between treatment acceptability and treatment integrity</article-title>. <source>Journal of Behavioral Education</source>, <volume>11</volume>, <fpage>39</fpage>–<lpage>50</lpage>. doi:<pub-id pub-id-type="doi">10.1023/A:1014333305011</pub-id></citation>
</ref>
<ref id="bibr24-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sterling-Turner</surname><given-names>H. E.</given-names></name>
<name><surname>Watson</surname><given-names>T. S.</given-names></name>
<name><surname>Wildmon</surname><given-names>M.</given-names></name>
<name><surname>Watkins</surname><given-names>C.</given-names></name>
<name><surname>Little</surname><given-names>E.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Investigating the relationship between training type and treatment integrity</article-title>. <source>School Psychology Quarterly</source>, <volume>16</volume>, <fpage>56</fpage>–<lpage>67</lpage>. doi:<pub-id pub-id-type="doi">10.1521/scpq.16.1.56.19157</pub-id></citation>
</ref>
<ref id="bibr25-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vannest</surname><given-names>K.</given-names></name>
<name><surname>Davis</surname><given-names>J.</given-names></name>
<name><surname>Davis</surname><given-names>C.</given-names></name>
<name><surname>Mason</surname><given-names>B. A.</given-names></name>
<name><surname>Burke</surname><given-names>M. D.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Effective intervention for behavior with a Daily Behavior Report Card</article-title>. <source>School Psychology Review</source>, <volume>39</volume>, <fpage>654</fpage>–<lpage>672</lpage>.</citation>
</ref>
<ref id="bibr26-1534508412441966">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weng</surname><given-names>L.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Impact of the number of response categories and anchor labels on coefficient alpha and test-retest reliability</article-title>. <source>Educational and Psychological Measurement</source>, <volume>64</volume>, <fpage>956</fpage>–<lpage>972</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0013164404268674</pub-id></citation>
</ref>
</ref-list>
</back>
</article>