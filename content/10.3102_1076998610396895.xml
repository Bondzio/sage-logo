<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JEB</journal-id>
<journal-id journal-id-type="hwp">spjeb</journal-id>
<journal-title>Journal of Educational and Behavioral Statistics</journal-title>
<issn pub-type="ppub">1076-9986</issn>
<issn pub-type="epub">1935-1054</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/1076998610396895</article-id>
<article-id pub-id-type="publisher-id">10.3102_1076998610396895</article-id>
<title-group>
<article-title>Mixed and Mixture Regression Models for Continuous Bounded Responses Using the Beta Distribution</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Verkuilen</surname>
<given-names>Jay</given-names>
</name>
<aff id="aff1-1076998610396895">City University of New York</aff>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Smithson</surname>
<given-names>Michael</given-names>
</name>
<aff id="aff2-1076998610396895">The Australian National University</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="other" id="fn2-1076998610396895">
<p>JAY VERKUILEN is Assistant Professor of Educational Psychology in the Graduate Center, City University of New York, 365 Fifth Avenue New York, NY 10016 U.S.A.; <email>jverkuilen@gc.cuny.edu</email>. His research interests include nonlinear and generalized linear models, item response theory and paired comparison models.</p>
</fn>
<fn fn-type="other" id="fn3-1076998610396895">
<p>MICHAEL SMITHSON is Professor in the Psychology Department at The Australian National University, Canberra A.C.T. 0200, Australia; <email>Michael.Smithson@anu.edu.au</email>. His research interests include judgment and decision making under uncertainty, nonlinear and generalized linear models, and fuzzy logic methods for the human sciences.</p>
</fn>
<fn fn-type="other" id="fn4-1076998610396895">
<p>The authors would like to thank an anonymous reviewer and the editor for valuable insights and suggestions.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>37</volume>
<issue>1</issue>
<fpage>82</fpage>
<lpage>113</lpage>
<history>
<date date-type="received">
<day>10</day>
<month>12</month>
<year>2010</year>
</date>
<date date-type="rev-recd">
<day>24</day>
<month>8</month>
<year>2010</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>10</month>
<year>2010</year>
</date>
</history>
<permissions>
<copyright-statement>© American Educational Research Association 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>Doubly bounded continuous data are common in the social and behavioral sciences. Examples include judged probabilities, confidence ratings, derived proportions such as percent time on task, and bounded scale scores. Dependent variables of this kind are often difficult to analyze using normal theory models because their distributions may be quite poorly modeled by the normal distribution. The authors extend the beta-distributed generalized linear model (GLM) proposed in Smithson and Verkuilen (2006) to discrete and continuous mixtures of beta distributions, which enables modeling dependent data structures commonly found in real settings. The authors discuss estimation using both deterministic marginal maximum likelihood and stochastic Markov chain Monte Carlo (MCMC) methods. The results are illustrated using three data sets from cognitive psychology experiments.</p>
</abstract>
<kwd-group>
<kwd>beta distribution</kwd>
<kwd>general linear model</kwd>
<kwd>mixed model</kwd>
<kwd>mixture model</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1076998610396895">
<title>1. Introduction</title>
<p>Continuous dependent variables with scales bounded at both ends are fairly common throughout the social and behavioral sciences. Examples from psychology are the percentage of time attending one stimulus versus another as in habituation studies, subjective probability or confidence ratings common in cognitive research, or total scores on a symptom questionnaire applied in a community setting. In behavioral economics examples include most utility scales (usually assumed to be bounded at both ends) and proportional measures such as allocations to investments in a portfolio and leverage (the ratio of debt to assets plus debt). These continuous doubly bounded dependent variables frequently present special problems for modelers. Chief among these are uncorrectable skew and heteroscedasticity, both often arising from the fact that as the mean response moves toward either scale boundary, the variance tends to decrease and skew tends to increase. In short, in a bounded response space, the moments of the distribution are inextricably linked. Furthermore, the closer one is to a boundary the stronger this linkage is.</p>
<p>Several authors (<xref ref-type="bibr" rid="bibr26-1076998610396895">Kieschnick &amp; McCullogh, 2003</xref>; <xref ref-type="bibr" rid="bibr37-1076998610396895">Paolino, 2001</xref>; <xref ref-type="bibr" rid="bibr48-1076998610396895">Smithson &amp; Verkuilen, 2006</xref>) have presented arguments that traditional methods such as regression-linearizing and variance-stabilizing transformations or robust estimators are not always appropriate for modeling this kind of limited dependent variable. They propose an alternative in the form of using the beta distribution to model such variables. The beta distribution is a well-known member of the exponential family of distributions for which the regularity conditions that help ensure maximum likelihood estimates exist and are well defined. Nevertheless, the literature on generalized linear models (GLMs) for beta-distributed dependent variables (i.e., beta regression) is sparse. In the otherwise exhaustive <italic>Handbook of Beta Distributions</italic>, there is no mention of beta regression (<xref ref-type="bibr" rid="bibr23-1076998610396895">Gupta &amp; Nadarajah, 2004</xref>), although there is a sizeable parallel literature on beta-binomial regression (an early instance is <xref ref-type="bibr" rid="bibr13-1076998610396895">Crowder, 1978</xref>), suited to modeling proportions of counts where the beta distribution is used as a hierarchically specified random effect to address overdispersion.</p>
<p>An early example of beta regression is <xref ref-type="bibr" rid="bibr5-1076998610396895">Brehm and Gates's (1993)</xref> model of police compliance with supervision. However, <xref ref-type="bibr" rid="bibr37-1076998610396895">Paolino (2001)</xref> was the first to employ the mean-precision parameterization of the beta distribution that greatly simplifies interpretation. Apparently independently, <xref ref-type="bibr" rid="bibr18-1076998610396895">Ferrari and Cribari-Neto (2004)</xref> derived a similar beta regression model that recently has been implemented in the SAS GLIMMIX procedure (<xref ref-type="bibr" rid="bibr44-1076998610396895">SAS, 2008</xref>), and likewise <xref ref-type="bibr" rid="bibr26-1076998610396895">Kieschnick and McCullogh (2003)</xref> compared the performance of a beta regression model for proportions, as employed in economics and finance research, with several alternatives and concluded that it is often the best option. <xref ref-type="bibr" rid="bibr34-1076998610396895">Noël and Dauvier (2007)</xref> and <xref ref-type="bibr" rid="bibr33-1076998610396895">Noël (2008)</xref> have presented unfolding and dominance item-response models for continuous doubly bounded scale items based on the beta distribution.</p>
<p>Some of the above-mentioned versions of beta regression treat the precision parameter as a nuisance. Recently <xref ref-type="bibr" rid="bibr48-1076998610396895">Smithson and Verkuilen (2006)</xref> followed and extended Paolino’s model by explicitly modeling precision (and therefore dispersion) as well as the mean response, utilizing the extended GLM framework for joint modeling of means and dispersions described in Chapter 10 of <xref ref-type="bibr" rid="bibr30-1076998610396895">McCullagh and Nelder (1989)</xref> and developed by <xref ref-type="bibr" rid="bibr49-1076998610396895">Smyth (1989)</xref> for random variables in the exponential family of distributions. There are good reasons for regarding the modeling of dispersion as important in its own right. We concur with <xref ref-type="bibr" rid="bibr9-1076998610396895">Carroll’s (2003)</xref> observation that ignoring variance structure or treating it as simply a nuisance not only can lead to inefficient estimation but to misleading conclusions. <xref ref-type="bibr" rid="bibr48-1076998610396895">Smithson and Verkuilen (2006)</xref> presented real-world examples of this and the present article includes additional illustrations.</p>
<p>Likelihood maximization has been the dominant approach in estimating beta regression models (e.g., <xref ref-type="bibr" rid="bibr37-1076998610396895">Paolino, 2001</xref>; <xref ref-type="bibr" rid="bibr48-1076998610396895">Smithson &amp; Verkuilen, 2006</xref>), with <xref ref-type="bibr" rid="bibr53-1076998610396895">Vasconcellos and Cribari-Neto (2005)</xref> and <xref ref-type="bibr" rid="bibr36-1076998610396895">Ospina, Cribari-Neto, and Vasconcellos (2006)</xref> proposing bias-correcting adjustments. <xref ref-type="bibr" rid="bibr15-1076998610396895">Epsinheira, Ferrari, and Cribari-Neto (2008a</xref>, <xref ref-type="bibr" rid="bibr16-1076998610396895">2008b</xref>) explore alternative weighted standardized residuals and influence diagnostics for the Ferrari-Cribari-Neto beta GLM. <xref ref-type="bibr" rid="bibr8-1076998610396895">Buckley (2002)</xref> implemented the Paolino model in a Bayesian Markov chain Monte Carlo (MCMC) procedure, and <xref ref-type="bibr" rid="bibr48-1076998610396895">Smithson and Verkuilen (2006)</xref> also did so with their version.</p>
<p>A number of gaps remain in our knowledge about beta regression models. Chief among them is the absence of methods for handling dependent observations. In this article, we develop and explore generalized linear mixed models (GLMMs) and related dependent-observation models for beta-distributed dependent variables. Justifications for GLMMs as a way of handling dependencies are well known. The dependencies among observations may arise naturally from effects such as autocorrelation over time, within-subject correlations in repeated measures experiments, or clumping due to within-cluster homogeneity. We begin by reprising the independent-observations beta regression model and then developing an overall framework for beta GLMMs, including both mixed and mixture distribution models. A simulation example is presented, comparing maximum likelihood and Bayesian MCMC estimations, along with a discussion of variance partition issues and goodness of fit. Three examples of applications to real data are provided.</p>
</sec>
<sec id="section2-1076998610396895">
<title>2. Regression With the Beta Distribution</title>
<sec id="section3-1076998610396895">
<title>2.1. Independent Observations</title>
<p>This section summarizes a longer discussion found in <xref ref-type="bibr" rid="bibr48-1076998610396895">Smithson and Verkuilen (2006)</xref>. Let <inline-formula id="inline-formula1-1076998610396895">
<mml:math id="mml-inline1-1076998610396895">
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula2-1076998610396895">
<mml:math id="mml-inline2-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> are shape parameters.<sup><xref ref-type="fn" rid="fn1-1076998610396895">1</xref></sup> The probability density function (PDF) is
<disp-formula id="disp-formula1-1076998610396895"><label>1</label>
<mml:math id="mml-disp1-1076998610396895">
<mml:mi>f</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:msup>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula1-1076998610396895" xlink:href="10.3102_1076998610396895-eq1.tif"/>
</disp-formula>where <inline-formula id="inline-formula3-1076998610396895">
<mml:math id="mml-inline3-1076998610396895">
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is the complete gamma function. The two shape parameters pull density toward 1 (<inline-formula id="inline-formula4-1076998610396895">
<mml:math id="mml-inline4-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>) or 0 (<inline-formula id="inline-formula5-1076998610396895">
<mml:math id="mml-inline5-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>). Standard results show that
<disp-formula id="disp-formula2-1076998610396895"><label>2</label>
<mml:math id="mml-disp2-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula2-1076998610396895" xlink:href="10.3102_1076998610396895-eq2.tif"/>
</disp-formula>and
<disp-formula id="disp-formula3-1076998610396895"><label>3</label>
<mml:math id="mml-disp3-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">var</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula3-1076998610396895" xlink:href="10.3102_1076998610396895-eq3.tif"/>
</disp-formula>The standard parameterization of the beta is most useful when it is used as a prior, where the parameters correspond to degrees of freedom. However, it is inconvenient for a regression model. Let <inline-formula id="inline-formula6-1076998610396895">
<mml:math id="mml-inline6-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and precision parameter <inline-formula id="inline-formula7-1076998610396895">
<mml:math id="mml-inline7-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, which can be inverted to show <inline-formula id="inline-formula8-1076998610396895">
<mml:math id="mml-inline8-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula9-1076998610396895">
<mml:math id="mml-inline9-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. Upon reparameterization,
<disp-formula id="disp-formula4-1076998610396895"><label>4</label>
<mml:math id="mml-disp4-1076998610396895">
<mml:mi>f</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:msup>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula4-1076998610396895" xlink:href="10.3102_1076998610396895-eq4.tif"/>
</disp-formula>We use the mean-precision parameterization henceforth, denoting such a distribution as <inline-formula id="inline-formula10-1076998610396895">
<mml:math id="mml-inline10-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
</mml:math>
</inline-formula>. Whenever, <inline-formula id="inline-formula11-1076998610396895">
<mml:math id="mml-inline11-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula>, the distribution has a mode on the interior of <inline-formula id="inline-formula12-1076998610396895">
<mml:math id="mml-inline12-1076998610396895">
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. Whenever <inline-formula id="inline-formula13-1076998610396895">
<mml:math id="mml-inline13-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula>, the distribution is bimodal, with <inline-formula id="inline-formula14-1076998610396895">
<mml:math id="mml-inline14-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> controlling the relative height of the anti-modes, and <inline-formula id="inline-formula15-1076998610396895">
<mml:math id="mml-inline15-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula> implies a J- or L-shaped distribution, depending on whether <inline-formula id="inline-formula16-1076998610396895">
<mml:math id="mml-inline16-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>0.5</mml:mn>
</mml:math>
</inline-formula> or <inline-formula id="inline-formula17-1076998610396895">
<mml:math id="mml-inline17-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>0.5</mml:mn>
</mml:math>
</inline-formula>, respectively. When <inline-formula id="inline-formula18-1076998610396895">
<mml:math id="mml-inline18-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.5</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula19-1076998610396895">
<mml:math id="mml-inline19-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula>, the distribution is uniform. Finally, the distribution reflects about 0.5 in that if <inline-formula id="inline-formula20-1076998610396895">
<mml:math id="mml-inline20-1076998610396895">
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, then <inline-formula id="inline-formula21-1076998610396895">
<mml:math id="mml-inline21-1076998610396895">
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:math>
</inline-formula>
</p>
<p>To form a regression model, consider two design matrices <inline-formula id="inline-formula22-1076998610396895">
<mml:math id="mml-inline22-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> associated with the location and dispersion, respectively, so that <inline-formula id="inline-formula23-1076998610396895">
<mml:math id="mml-inline23-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">x</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">v</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> are their <inline-formula id="inline-formula24-1076998610396895">
<mml:math id="mml-inline24-1076998610396895">
<mml:mi>i</mml:mi>
</mml:math>
</inline-formula>th row vectors. It is assumed that <inline-formula id="inline-formula25-1076998610396895">
<mml:math id="mml-inline25-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> are of full rank (typically they have a column vector <bold>1</bold> for an intercept). Define two vectors of regression weights, <inline-formula id="inline-formula26-1076998610396895">
<mml:math id="mml-inline26-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mtext>β</mml:mtext>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mtext>δ</mml:mtext>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, which are related to <inline-formula id="inline-formula27-1076998610396895">
<mml:math id="mml-inline27-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula28-1076998610396895">
<mml:math id="mml-inline28-1076998610396895">
<mml:mtext>φ</mml:mtext>
</mml:math>
</inline-formula> through two link functions as
<disp-formula id="disp-formula5-1076998610396895"><label>5</label>
<mml:math id="mml-disp5-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">x</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mtext>β</mml:mtext>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula5-1076998610396895" xlink:href="10.3102_1076998610396895-eq5.tif"/>
</disp-formula>

<disp-formula id="disp-formula6-1076998610396895"><label>6</label>
<mml:math id="mml-disp6-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">v</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mtext>δ</mml:mtext>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula6-1076998610396895" xlink:href="10.3102_1076998610396895-eq6.tif"/>
</disp-formula>(More generally, <inline-formula id="inline-formula29-1076998610396895">
<mml:math id="mml-inline29-1076998610396895">
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula30-1076998610396895">
<mml:math id="mml-inline30-1076998610396895">
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> may be used for location and dispersion link functions.) We use the logit/log pair in this article, which restrict <inline-formula id="inline-formula31-1076998610396895">
<mml:math id="mml-inline31-1076998610396895">
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula32-1076998610396895">
<mml:math id="mml-inline32-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> as required. Other link functions such as the probit or complementary log–log for location or reciprocal for dispersion could be used as well. We believe the logit link is the most broadly useful for the location submodel and the log for the dispersion submodel, but this is a matter for specific consideration in light of theory and data. For instance, theory may support the use of a complementary log–log link for location, as is the case for a discrete time survival model (<xref ref-type="bibr" rid="bibr17-1076998610396895">Fahrmeier &amp; Tutz, 2001</xref>).</p>
<p>In summary, beta regression assumes
<disp-formula id="disp-formula7-1076998610396895"><label>7</label>
<mml:math id="mml-disp7-1076998610396895">
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfenced>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula7-1076998610396895" xlink:href="10.3102_1076998610396895-eq7.tif"/>
</disp-formula>It should be noted that the <italic>unconditional</italic> distributions can be markedly different from the beta distribution. For instance, a beta distribution cannot be bimodal on the interior of the unit interval, but the distribution of a 50% Bernoulli mixture of <inline-formula id="inline-formula33-1076998610396895">
<mml:math id="mml-inline33-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>5</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and a <inline-formula id="inline-formula34-1076998610396895">
<mml:math id="mml-inline34-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>5</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> will be.</p>
<p>Let <inline-formula id="inline-formula35-1076998610396895">
<mml:math id="mml-inline35-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">x</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula36-1076998610396895">
<mml:math id="mml-inline36-1076998610396895">
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">v</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula37-1076998610396895">
<mml:math id="mml-inline37-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">x</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">v</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> are vectors of regressors for the <inline-formula id="inline-formula38-1076998610396895">
<mml:math id="mml-inline38-1076998610396895">
<mml:mi>i</mml:mi>
</mml:math>
</inline-formula>th case. The likelihood function is
<disp-formula id="disp-formula8-1076998610396895"><label>8</label>
<mml:math id="mml-disp8-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="script">L</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>β</mml:mtext>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>δ</mml:mtext>
<mml:mo stretchy="false">;</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∏</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>n</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:msubsup>
<mml:mi>y</mml:mi>
<mml:mi>i</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula8-1076998610396895" xlink:href="10.3102_1076998610396895-eq8.tif"/>
</disp-formula>Taking the natural logarithm gives
<disp-formula id="disp-formula9-1076998610396895"><label>9</label>
<mml:math id="mml-disp9-1076998610396895">
<mml:mtable columnalign="right left" columnspacing="thickmathspace" displaystyle="true" rowspacing=".5em">
<mml:mtr>
<mml:mtd>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="script">L</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>β</mml:mtext>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>δ</mml:mtext>
<mml:mo stretchy="false">;</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>n</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfenced>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mrow>
<mml:mspace width="1em"/>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfenced>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mrow>
<mml:mspace width="1em"/>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfenced>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
<graphic alternate-form-of="disp-formula9-1076998610396895" xlink:href="10.3102_1076998610396895-eq9.tif"/>
</disp-formula>The score function and Hessian of this likelihood can be derived using the chain rule; they are rather unwieldy expressions and thus relegated to the <xref ref-type="app" rid="app1-1076998610396895">Appendix</xref>. The most important conclusion to draw from them is the fact that <inline-formula id="inline-formula39-1076998610396895">
<mml:math id="mml-inline39-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula40-1076998610396895">
<mml:math id="mml-inline40-1076998610396895">
<mml:mtext>φ</mml:mtext>
</mml:math>
</inline-formula> are not separable, unlike, say, the mean and dispersion parameters in the Gaussian case.</p>
</sec>
<sec id="section4-1076998610396895">
<title>2.2. Estimation in the Independence Case</title>
<p>The beta forms a two-parameter exponential family and, for models lacking dispersion regressors that are free of the usual trouble spots for any GLM such as improperly scaled design matrices or collinearity, is generally quite numerically tractable. The estimation theory for the independent case has been examined in great detail by <xref ref-type="bibr" rid="bibr36-1076998610396895">Ospina et al. (2006)</xref>, who note that beta regression for independent observations satisfies all usual regularity conditions for maximum likelihood estimation (MLE). Estimates of location parameters are very close to unbiased even for modest samples while estimates for the precision parameter tend to be somewhat optimistic, that is, too large, with the bias increasing as the true precision increases. They propose a correction for this bias, which we do not pursue here. In our experience, behavioral science data tend to have relatively lower values of the precision parameter and are less biased as a consequence. With a dispersion submodel, estimation is trickier and better starting values for the parameters are required. We have found the best procedure is to get starting values by fitting a reasonable location-only model before considering dispersion covariates.</p>
<p>MLE proceeds through numerical maximization of the log likelihood using a Newton or quasi-Newton method with standard errors for the parameters coming from the inverse of the final Hessian, that is, observed information, or using Fisher scoring and expected information. In general, Bayesian estimation seems to be a bit more forgiving than MLE in the analyses we have conducted because the priors serve to smooth the likelihood. Unsurprisingly, given that both are likelihood based, where MLE has problems, in particular, when <inline-formula id="inline-formula41-1076998610396895">
<mml:math id="mml-inline41-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula>, MCMC usually exhibits poor mixing as well.</p>
<sec id="section5-1076998610396895">
<title>2.2.1. Handling Boundary Observations</title>
<p>One important problem with data on a bounded interval is the presence of exact boundary observations, that is, 0 or 1. These observations occur in real data but are impossible given the sample space of (0, 1). For instance, if the response scale is, say, a discrete response scale on {0, 1,…, 100} that is being analyzed as a continuous score transformed into a proportion, exact values of 0 or 100 can and do occur. Even in a slider response with extremely high resolution, it is not unlikely that responses will pile up a bit at the boundary, causing a probably unimportant but notable deviation from the model. Points on the interior such as the midpoint may also occur more often than is entirely consistent with the model.</p>
<p>Provided that boundary observations do not represent qualitatively different responses but instead are reasonably assumed to be the result of finite precision of measurement, it is reasonable to proceed with analysis by “cheating” the observations slightly away from the boundary, via either of two methods. The first rescales all observations by a small amount:
<disp-formula id="disp-formula10-1076998610396895"><label>10</label>
<mml:math id="mml-disp10-1076998610396895">
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">new</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">old</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula10-1076998610396895" xlink:href="10.3102_1076998610396895-eq10.tif"/>
</disp-formula>for a small <inline-formula id="inline-formula42-1076998610396895">
<mml:math id="mml-inline42-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula>. Larger values of <inline-formula id="inline-formula43-1076998610396895">
<mml:math id="mml-inline43-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> shrink the data more toward 0.5, and thus, <inline-formula id="inline-formula44-1076998610396895">
<mml:math id="mml-inline44-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> should be kept as small as possible. Reasonable choices of <inline-formula id="inline-formula45-1076998610396895">
<mml:math id="mml-inline45-1076998610396895">
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:math>
</inline-formula> will, of course, depend on the application. For instance, with a discrete response scale on <inline-formula id="inline-formula46-1076998610396895">
<mml:math id="mml-inline46-1076998610396895">
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>100</mml:mn>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> a reasonable choice might be <inline-formula id="inline-formula47-1076998610396895">
<mml:math id="mml-inline47-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mn>100</mml:mn>
</mml:math>
</inline-formula>. The second method replaces 0 by <inline-formula id="inline-formula48-1076998610396895">
<mml:math id="mml-inline48-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> and 1 by <inline-formula id="inline-formula49-1076998610396895">
<mml:math id="mml-inline49-1076998610396895">
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, leaving the other observations unchanged. <xref ref-type="bibr" rid="bibr1-1076998610396895">Aitchison (2003)</xref> discusses the boundary problem in detail, favoring the rescaling approach due to the fact that it preserves the mean of the data and maintains certain collapsibility conditions that are unimportant in the present context.</p>
<p>We advise use of sensitivity analysis to ensure that estimates and inference are not notably affected by the choice of <inline-formula id="inline-formula50-1076998610396895">
<mml:math id="mml-inline50-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. One selling point for the beta distribution as compared to, say, logit-transforming observations and then assuming normality of the transformed responses is the fact that the log likelihood of the logit-normal near the boundary is particularly sensitive while the log likelihood of the beta is less so. In particular, examining the log likelihood of the logit-normal shows that it contains exponential terms while the beta’s derivatives are algebraic, implying that small perturbations in the logit-normal are amplified compared to the beta.</p>
</sec>
</sec>
<sec id="section6-1076998610396895">
<title>2.3. The Mixed Model</title>
<p>It is somewhat unfortunate that the usual terminology distinguishes between mixed and mixture models as, conceptually, they are essentially the same. In both cases, one or more missing independent variables constant within a cluster are posited to account for dependence among observations in a cluster. Upon conditioning on these variables and observed explanatory variables, the observations are assumed independent. Beta regression in the independent case states that, conditional on the regressors <inline-formula id="inline-formula51-1076998610396895">
<mml:math id="mml-inline51-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, the response variable is distributed beta. The mixed model extension asserts that, conditional on the regressors and the additional missing information, the response variable is distributed beta. We develop the two-level model equations here for general location and dispersion link functions <inline-formula id="inline-formula52-1076998610396895">
<mml:math id="mml-inline52-1076998610396895">
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula53-1076998610396895">
<mml:math id="mml-inline53-1076998610396895">
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, respectively, assuming balanced data for simplicity, that is, equal numbers of observations within each cluster. The general principles carry over to higher level structures and unbalanced data.</p>
<p>We follow the basic scheme given in <xref ref-type="bibr" rid="bibr31-1076998610396895">McCulloch, Searle, and Neuhaus (2009</xref>, chap. 8), which we take as a basic reference. <xref ref-type="bibr" rid="bibr40-1076998610396895">Pinheiro and Chao (2006)</xref> provide a very useful summary of the estimation theory for mixed location models. Much less is known about estimation of dispersion mixtures or location/dispersion mixtures. The discussion found in <xref ref-type="bibr" rid="bibr25-1076998610396895">Johnson (2003)</xref>, where a mixed location/dispersion model was considered for discrete ordinal variables is helpful. Proactive Monte Carlo simulation of models that are likely to be used in a given study may be useful if one is in doubt about estimation properties (<xref ref-type="bibr" rid="bibr51-1076998610396895">Steiger, 2001</xref>). To do this, simply generate replicated data sets using a random number generator and estimate the desired model on these replications.</p>
<p>Let <inline-formula id="inline-formula54-1076998610396895">
<mml:math id="mml-inline54-1076998610396895">
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>I</mml:mi>
</mml:math>
</inline-formula> index subjects and <inline-formula id="inline-formula55-1076998610396895">
<mml:math id="mml-inline55-1076998610396895">
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>J</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> index observations within the <inline-formula id="inline-formula56-1076998610396895">
<mml:math id="mml-inline56-1076998610396895">
<mml:mi>i</mml:mi>
</mml:math>
</inline-formula>th subject. For simplicity of presentation here we assume that <inline-formula id="inline-formula57-1076998610396895">
<mml:math id="mml-inline57-1076998610396895">
<mml:mi>J</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>J</mml:mi>
</mml:math>
</inline-formula>; that is, each subject has exactly the same number of responses, so there are <inline-formula id="inline-formula58-1076998610396895">
<mml:math id="mml-inline58-1076998610396895">
<mml:mi>I</mml:mi>
<mml:mi>J</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>N</mml:mi>
</mml:math>
</inline-formula> total observations. Consider four matrices of regressors, <inline-formula id="inline-formula59-1076998610396895">
<mml:math id="mml-inline59-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">Z</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">W</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. <bold>X</bold> and <bold>V</bold> are as previously. <bold>Z</bold> and <bold>W</bold> are the regressors for random effects <bold>b</bold> and <bold>d</bold>, respectively. Then
<disp-formula id="disp-formula11-1076998610396895"><label>11</label>
<mml:math id="mml-disp11-1076998610396895">
<mml:msup>
<mml:mi>g</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">x</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mtext>β</mml:mtext>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">z</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula11-1076998610396895" xlink:href="10.3102_1076998610396895-eq11.tif"/>
</disp-formula>

<disp-formula id="disp-formula12-1076998610396895"><label>12</label>
<mml:math id="mml-disp12-1076998610396895">
<mml:msup>
<mml:mi>h</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">v</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mtext>δ</mml:mtext>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">w</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">d</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula12-1076998610396895" xlink:href="10.3102_1076998610396895-eq12.tif"/>
</disp-formula>We do not assume that <bold>b</bold> is independent of <bold>d</bold>, though this may often be sensible in practice. Denote this joint distribution as <inline-formula id="inline-formula60-1076998610396895">
<mml:math id="mml-inline60-1076998610396895">
<mml:mi>q</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">d</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:math>
</inline-formula> In this article, we will use either multivariate normal or multinomial mixing distributions or a combination of multivariate normal and multinomial. The likelihood function is
<disp-formula id="disp-formula13-1076998610396895"><label>13</label>
<mml:math id="mml-disp13-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="script">L</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>β</mml:mtext>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>δ</mml:mtext>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">d</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">;</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">Z</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">W</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∏</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>I</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∏</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>J</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mi>f</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula13-1076998610396895" xlink:href="10.3102_1076998610396895-eq13.tif"/>
</disp-formula>Unfortunately, as is well known, it is not possible to estimate <inline-formula id="inline-formula61-1076998610396895">
<mml:math id="mml-inline61-1076998610396895">
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> or <inline-formula id="inline-formula62-1076998610396895">
<mml:math id="mml-inline62-1076998610396895">
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="bold">d</mml:mi>
</mml:mrow>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> directly in a statistically consistent manner. Instead, it is usual to integrate out the random effects by averaging over the random parameters (here, the integral sign should be understood as either a continuous integral or finite sum, as the case may be):
<disp-formula id="disp-formula14-1076998610396895"><label>14</label>
<mml:math id="mml-disp14-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="script">L</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>β</mml:mtext>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>δ</mml:mtext>
<mml:mo stretchy="false">;</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∏</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>I</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:msub>
<mml:mo stretchy="false">∫</mml:mo>
<mml:mrow>
<mml:mi>B</mml:mi>
<mml:mo stretchy="false">×</mml:mo>
<mml:mi>D</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∏</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>J</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mi>f</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
<mml:mi>q</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">d</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi>d</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>d</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">d</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula14-1076998610396895" xlink:href="10.3102_1076998610396895-eq14.tif"/>
</disp-formula>Strategies for doing this are much the same for nearly any GLMM, as shown in <xref ref-type="bibr" rid="bibr40-1076998610396895">Pinheiro and Chao (2006)</xref>. In this article we use Gauss-Hermite quadrature, leading to the marginal maximum likelihood (MML) solution, and fully Bayesian estimation, which uses MCMC through the Gibbs sampler to approximate the appropriate integrals. The GLIMMIX procedure in SAS implements the penalized quasi-likelihood approach for a location-only model (<xref ref-type="bibr" rid="bibr44-1076998610396895">SAS Institute, 2008</xref>). This approach approximates the integrals for a multivariate normal mixing distribution by a particular Taylor series expansion. The penalized quasi-likelihood approach has yet to be thoroughly studied. It is relatively fast and can accommodate larger numbers of random effects than Gauss-Hermite quadrature. However, the properties it exhibits for categorical data seem to be present in dealing with beta-distributed variables, namely, that it is biased when the random effects are relatively large. We do not consider it further here. For the data sets we have considered, ranging between a few hundred and a few thousand observations with between one and seven random effects, MML and MCMC seem to perform as expected from other GLMM contexts. MML by quadrature works for between one and three random effects and we have obtained sensible estimates from MCMC estimation for up to seven random effects in a not unreasonable amount of time (i.e., minutes to hours).</p>
<p>An important special case is the finite mixture model, which asserts that, conditional on regressors <inline-formula id="inline-formula63-1076998610396895">
<mml:math id="mml-inline63-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, the response variable is distributed as a finite weighted sum of <inline-formula id="inline-formula64-1076998610396895">
<mml:math id="mml-inline64-1076998610396895">
<mml:mi>C</mml:mi>
</mml:math>
</inline-formula> beta random variables:
<disp-formula id="disp-formula15-1076998610396895"><label>15</label>
<mml:math id="mml-disp15-1076998610396895">
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>c</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>C</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>f</mml:mi>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mi>y</mml:mi>
</mml:mfenced>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula15-1076998610396895" xlink:href="10.3102_1076998610396895-eq15.tif"/>
</disp-formula>where <inline-formula id="inline-formula65-1076998610396895">
<mml:math id="mml-inline65-1076998610396895">
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">≤</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mo stretchy="false">≤</mml:mo>
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula66-1076998610396895">
<mml:math id="mml-inline66-1076998610396895">
<mml:mrow>
<mml:msub>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mi>c</mml:mi>
</mml:msub>
</mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula>, and
<disp-formula id="disp-formula16-1076998610396895"><label>16</label>
<mml:math id="mml-disp16-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mi>f</mml:mi>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mi>y</mml:mi>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>φ</mml:mtext>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">V</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">X</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>φ</mml:mtext>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">V</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">X</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfenced>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula16-1076998610396895" xlink:href="10.3102_1076998610396895-eq16.tif"/>
</disp-formula>The <inline-formula id="inline-formula67-1076998610396895">
<mml:math id="mml-inline67-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>, in turn, may also have a submodel with its own regressors <inline-formula id="inline-formula68-1076998610396895">
<mml:math id="mml-inline68-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">Q</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. For <inline-formula id="inline-formula69-1076998610396895">
<mml:math id="mml-inline69-1076998610396895">
<mml:mi>c</mml:mi>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mi>C</mml:mi>
</mml:math>
</inline-formula>,
<disp-formula id="disp-formula17-1076998610396895"><label>17</label>
<mml:math id="mml-disp17-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>c</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="italic">γ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mi>i</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>q</mml:mi>
</mml:mrow>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula17-1076998610396895" xlink:href="10.3102_1076998610396895-eq17.tif"/>
</disp-formula>Both MLE and Bayesian estimation methods may be used for this model, subject to the caveats mentioned earlier. <xref ref-type="bibr" rid="bibr46-1076998610396895">Smithson, Merkle, and Verkuilen (2009)</xref> explore mixture models in more depth and present examples of their application.</p>
</sec>
<sec id="section7-1076998610396895">
<title>2.4. Remarks on Estimation</title>
<p>We now offer some remarks on the two estimation strategies used in this article in the context of mixed and mixture models. First, we discuss issues that are common to both methods and then remark on aspects that apply to each individually.</p>
<p>Estimation of a GLMM is markedly more complex than in the independent observations case. In particular, conditions guaranteeing global concavity of the log likelihood do not hold here, and it is crucial to get good starting values. Because numerical optimization algorithms are only locally convergent, having good starting values greatly speeds estimation. Similarly, for MCMC, starting too far from the region of high posterior density will slow convergence. We have found that estimating a sequence of simpler models and feeding previous parameter estimates into later estimations greatly facilitates the fitting process. For instance, a fixed effects model with no random components should provide decent starting values for the fixed effects in a mixed model and a simplified random effects only model often provides a good guess for the mixing components. Even providing the correct signs of the coefficients will help convergence. The other area where estimation can go awry for the beta is in the matter of the log-gamma function and its derivatives near 0. These functions will underflow for argument values of approximately <inline-formula id="inline-formula70-1076998610396895">
<mml:math id="mml-inline70-1076998610396895">
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:msup>
<mml:mn>10</mml:mn>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>8</mml:mn>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, which can be obtained for very small values of <inline-formula id="inline-formula71-1076998610396895">
<mml:math id="mml-inline71-1076998610396895">
<mml:mtext>φ</mml:mtext>
</mml:math>
</inline-formula> or whenever <inline-formula id="inline-formula72-1076998610396895">
<mml:math id="mml-inline72-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">≈</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> or <inline-formula id="inline-formula73-1076998610396895">
<mml:math id="mml-inline73-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">≈</mml:mo>
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula>. Users programming their own routines need to be sure to properly scale their independent variables (e.g., by standardizing all of them to have mean 0 and variance 1) and bound predicted values away from such values.</p>
<p>MML estimation involves using a double iteration to fit the models, alternating between a quadrature step (using Gaussian quadrature) to integrate out the random effects and an optimization step to update the fixed effect parameters of the marginal likelihood. This process is computationally intensive. It is incumbent on the user to make a careful choice of the number of quadrature nodes and to have good starting values for model parameters. Adaptive Gaussian quadrature is slower (often much slower) but more accurate and requires fewer quadrature nodes. Ensuring that the solution is not sensitive to the number of quadrature nodes by manually varying that number is highly advisable. In our experience, beta regression seems to require more quadrature nodes than, say, logistic regression, which is perhaps unsurprising because the likelihood function is more complex than for the Bernoulli. The only other major area of trouble is when the maximum of the log-likelihood function is near a degeneracy. In this case, the Hessian will be poorly conditioned and optimization will usually fail (MCMC usually exhibits poor mixing in this case as well). Developing a regularization strategy for MML, for example, by adding a small number of pseudo-observations to shrink the likelihood toward a relevant null model and smooth the log likelihood, would constitute a useful research topic.</p>
<p>MCMC estimation generally performs well, both in successfully estimating complex models and in accuracy and interval coverage. Our conclusions regarding the strengths and weaknesses of this approach are largely in line with <xref ref-type="bibr" rid="bibr6-1076998610396895">Browne and Draper’s (2006)</xref> comparisons with likelihood and quasi-likelihood approaches. For medium to large random effects and moderate to large samples, Bayesian MCMC estimates and interval coverage seem to be reasonably well behaved. In particular, MCMC interval coverage fidelity appears superior to that provided by asymptotic Gaussian confidence intervals. MCMC estimates also are less biased than the quasi-likelihood ones (especially regarding variance components). Furthermore, in our experience MCMC often can successfully estimate complex models where likelihood-based approaches fail or are intractable, particularly when the number of random effects is larger than 3.</p>
<p>However, MCMC interval coverage fidelity is not well studied for small-to-medium samples in non-Gaussian random effects or hierarchical models. <xref ref-type="bibr" rid="bibr6-1076998610396895">Browne and Draper (2006)</xref> found that undercoverage in logistic random-effects models could be corrected for by using appropriate posterior summary measures, but as they point out this is not a satisfactory solution. Likewise, MCMC may not perform as well for models in which random effects are small, but in general neither does any other known method. Finally, large data sets, models with many parameters, or models that yield full conditional distributions that are not log-concave can cause MCMC to be time-consuming, although this is likely to become less of an issue as computing power increases.</p>
<p>It has been widely noted that the choice of priors for variance parameters in random effects models can be problematic. We recommend avoiding the use of improper priors and evaluating the effects of choosing alternative “noninformative” priors even when they are proper, for reasons summarized by <xref ref-type="bibr" rid="bibr11-1076998610396895">Congdon (2003</xref>, p. 21). Of course it may be reasonable to use an informative prior for substantive and/or technical reasons. See <xref ref-type="bibr" rid="bibr10-1076998610396895">Chen, Ibrahim, Shao, and Weiss (2003)</xref> for recent investigations regarding informative priors for GLMMs. In the analyses reported here, we adopted two commonly recommended priors for variance parameters: A uniform prior for <inline-formula id="inline-formula74-1076998610396895">
<mml:math id="mml-inline74-1076998610396895">
<mml:msup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> on <inline-formula id="inline-formula75-1076998610396895">
<mml:math id="mml-inline75-1076998610396895">
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> (e.g., <xref ref-type="bibr" rid="bibr21-1076998610396895">Gelman &amp; Rubin, 1992</xref>), and a <inline-formula id="inline-formula76-1076998610396895">
<mml:math id="mml-inline76-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Gamma</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> prior for <inline-formula id="inline-formula77-1076998610396895">
<mml:math id="mml-inline77-1076998610396895">
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:msup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula78-1076998610396895">
<mml:math id="mml-inline78-1076998610396895">
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:math>
</inline-formula> is a small positive number. We found very little difference between results using either of these priors in the examples presented here, but both <xref ref-type="bibr" rid="bibr6-1076998610396895">Browne and Draper (2006)</xref> and <xref ref-type="bibr" rid="bibr20-1076998610396895">Gelman (2006)</xref> provide useful criticisms and alternatives regarding the effects of using these priors for small sample sizes. At present the only reliable check is sensitivity analysis.</p>
</sec>
<sec id="section8-1076998610396895">
<title>2.5. Goodness of Fit and Model Checking</title>
<p>A full explication of model evaluation, model checking, and model comparison for beta GLMMs is beyond the scope of this article. These topics raise some open questions in the GLMM literature generally. We briefly review the available tools for evaluating and comparing beta GLMs and indicate unresolved issues. We also address some of these issues in the examples.</p>
<p>Global model comparison may be achieved using well-known measures such as the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and Deviance Information Criterion (DIC). These measures are all essentially attempts to estimate relative Kullback-Liebler divergence based on the optimized log likelihood and a penalty term representing model complexity. For MML, comparative measures of fit based on the likelihood (e.g., AIC and BIC) and the log-likelihood chi-square test for comparing nested models all are applicable. For Bayesian modelers, the posterior log likelihoods and DIC may be used. These measures are all subject to the usual caveats that apply in other contexts and we emphasize that they are best used in a comparative fashion. One particular warning that applies to users of beta regression is that all integration constants need to be included in the computation of these measures when they are used to compare across distributions (<xref ref-type="bibr" rid="bibr39-1076998610396895">Pawitan, 2001</xref>).</p>
<p>Model evaluation and checking involve two related issues: How well the model “predicts” the data and whether there are unduly influential observations (e.g., outliers). The first issue may be dealt with via simulations from the posterior predictive density. Typically the match between the replicated and actual data is evaluated by comparing their cumulative relative frequencies. A similar approach can be used in MML as each point predicts a density, conditional on the regressors and parameter estimates. The main idea is generating replicated data by predicting <inline-formula id="inline-formula79-1076998610396895">
<mml:math id="mml-inline79-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula80-1076998610396895">
<mml:math id="mml-inline80-1076998610396895">
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and then examining the resulting beta density.</p>
<p>Lack of a deviance or an appropriate residual is an undesirable feature of the beta GLMM. Clearly, the usual notion of a standardized residual is not adequate even for beta GLMs, because these models typically are heteroscedastic. <xref ref-type="bibr" rid="bibr18-1076998610396895">Ferrari and Cribari-Neto (2004)</xref> proposed a deviance residual for the beta GLM, but <xref ref-type="bibr" rid="bibr15-1076998610396895">Epsinheira et al. (2008a)</xref> found this residual to be inaccurate, particularly when the location submodel estimates are close to 0 or 1. Epsinheira et al. propose and evaluate two residuals based on the Fisher scoring algorithm. <xref ref-type="bibr" rid="bibr16-1076998610396895">Epsinheira et al. (2008b)</xref> develop a Cook-like distance measure of the influence of deleted observations on parameter estimates. The Bayesian tools for influence diagnostics include the conditional predictive ordinate (CPO; <xref ref-type="bibr" rid="bibr54-1076998610396895">Weiss &amp; Cho, 1998</xref>), which is readily applicable to beta GLMMs and probably is the most well-established option for these models. This method is essentially an approximation to the jackknife and, more broadly, deletion of suspect observations (or blocks of them) provides a useful means to diagnose models. <xref ref-type="bibr" rid="bibr28-1076998610396895">Longford (2001)</xref> proposes a similar approach in a likelihood context, using the parametric bootstrap to simulate the distribution of residuals at various levels in a multilevel model. An algorithm such as Atkinson’s forward search seems promising in this context (<xref ref-type="bibr" rid="bibr3-1076998610396895">Atkinson, 1994</xref>). The principal disadvantage of approaches such as the CPO, parametric bootstrap, or forward search is, of course, computational intensity. <xref ref-type="bibr" rid="bibr35-1076998610396895">Ntzoufras (2009)</xref> provides CPO code in WinBUGS.</p>
</sec>
</sec>
<sec id="section9-1076998610396895">
<title>3. A Simulation Study</title>
<p>To demonstrate the basic properties of mixed beta regression, we have performed parameter recovery simulation studies, one of whose results we show here. The simulated data and code are available from the authors. The study in question had 100 replications of <inline-formula id="inline-formula81-1076998610396895">
<mml:math id="mml-inline81-1076998610396895">
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>100</mml:mn>
</mml:math>
</inline-formula> subjects and <inline-formula id="inline-formula82-1076998610396895">
<mml:math id="mml-inline82-1076998610396895">
<mml:mi>J</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>4</mml:mn>
</mml:math>
</inline-formula> items per subject. The items had means and precisions given in <xref ref-type="table" rid="table1-1076998610396895">Table 1</xref>
and were mixed by subject with a Gaussian random intercept in the location submodel. It should be noted that these items have markedly non-Gaussian marginal densities: Given the configuration of the parameters, the first item is left skewed and exhibits mild bimodality, the middle two items are uniform, and the third item is right skewed. The resulting model equations are
<disp-formula id="disp-formula18-1076998610396895"><label>18</label>
<mml:math id="mml-disp18-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula18-1076998610396895" xlink:href="10.3102_1076998610396895-eq18.tif"/>
</disp-formula>

<disp-formula id="disp-formula19-1076998610396895"><label>19</label>
<mml:math id="mml-disp19-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula19-1076998610396895" xlink:href="10.3102_1076998610396895-eq19.tif"/>
</disp-formula>

<disp-formula id="disp-formula20-1076998610396895"><label>20</label>
<mml:math id="mml-disp20-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mover>
<mml:mrow>
<mml:mo stretchy="false">~</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">i</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal">.</mml:mo>
<mml:mi mathvariant="normal">i</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal">.</mml:mo>
<mml:mi mathvariant="normal">d</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:mover>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">N</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula20-1076998610396895" xlink:href="10.3102_1076998610396895-eq20.tif"/>
</disp-formula>The true values of the parameters are given in <xref ref-type="table" rid="table1-1076998610396895">Table 1</xref>.</p>
<table-wrap id="table1-1076998610396895" position="float">
<label>Table 1.</label>
<caption>
<p>Results of the 4 × 1 Simulation Study</p>
</caption>
<graphic alternate-form-of="table1-1076998610396895" xlink:href="10.3102_1076998610396895-table1.tif"/>
<table>
<thead>
<tr>
<th/>
<th colspan="3">Inv. Link</th>
<th colspan="3">Link</th>
</tr>
<tr>
<th>Parameter</th>
<th>True</th>
<th>MML</th>
<th>Bayes</th>
<th>True</th>
<th>MML</th>
<th>Bayes</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<inline-formula id="inline-formula83-1076998610396895">
<mml:math id="mml-inline83-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−1.39</td>
<td>−1.300 (0.150)</td>
<td>−1.288 (0.151)</td>
<td>0.2</td>
<td>0.210 (0.025)</td>
<td>0.217 (0.026)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula84-1076998610396895">
<mml:math id="mml-inline84-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>0</td>
<td>0.010 (0.100)</td>
<td>0.009 (0.105)</td>
<td>0.5</td>
<td>0.500 (0.026)</td>
<td>0.502 (0.026)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula85-1076998610396895">
<mml:math id="mml-inline85-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mn>3</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>0</td>
<td>−0.010 (0.110)</td>
<td>−0.007 (0.113)</td>
<td>0.5</td>
<td>0.500 (0.028)</td>
<td>0.498 (0.028)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula86-1076998610396895">
<mml:math id="mml-inline86-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mn>4</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>1.39</td>
<td>1.370 (0.130)</td>
<td>1.359 (0.130)</td>
<td>0.8</td>
<td>0.800 (0.021)</td>
<td>0.795 (0.021)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula87-1076998610396895">
<mml:math id="mml-inline87-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>0</td>
<td>0.160 (0.160)</td>
<td>0.153 (0.156)</td>
<td>1</td>
<td>1.180 (0.190)</td>
<td>1.180 (0.188)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula88-1076998610396895">
<mml:math id="mml-inline88-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>0.69</td>
<td>0.680 (0.130)</td>
<td>0.669 (0.130)</td>
<td>2</td>
<td>1.980 (0.250)</td>
<td>1.968 (0.255)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula89-1076998610396895">
<mml:math id="mml-inline89-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mn>3</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>0.69</td>
<td>0.680 (0.120)</td>
<td>0.675 (0.117)</td>
<td>2</td>
<td>1.990 (0.240)</td>
<td>1.978 (0.233)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula90-1076998610396895">
<mml:math id="mml-inline90-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mn>4</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>1.1</td>
<td>1.100 (0.130)</td>
<td>1.094 (0.129)</td>
<td>3</td>
<td>3.030 (0.380)</td>
<td>3.010 (0.382)</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula91-1076998610396895">
<mml:math id="mml-inline91-1076998610396895">
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>0.5</td>
<td>0.430 (0.100)</td>
<td>0.412 (0.111)</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1076998610396895">
<p>
<italic>Note:</italic> MML = marginal maximum likelihood.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>This model is quite similar to several commonly employed in the behavioral sciences, including unidimensional item response theory models or those for repeated measures experiments. We fit the model in many different ways but report the results from two runs, one using MML estimation and the other using Bayesian MCMC estimation. Given good starting values, MML convergence is rapid, each replication taking under 30 seconds to run on a 2.4 GHz Intel Core 2 Duo with 2 GB of RAM running Windows XP Pro using SAS 9.1.3 NLMIXED (<xref ref-type="bibr" rid="bibr43-1076998610396895">SAS Institute, 2006</xref>). Simulation studies with MCMC are slower, but each run took no more than 5 minutes to converge running winBUGS 1.4.3 (<xref ref-type="bibr" rid="bibr50-1076998610396895">Spiegelhalter, Thomas, Best, &amp; Lunn, 2004</xref>) on a similar machine.</p>
<p>As can be seen in the table, the parameters are nearly unbiased even for this relatively modest sample, though of course inside the link they are more biased (<xref ref-type="bibr" rid="bibr39-1076998610396895">Pawitan, 2001</xref>). Further examination shows that all parameters both inside the link and as linear predictors have reasonably symmetric distributions; location parameters are better approximated by a Gaussian than dispersion parameters but the skew exhibited for the latter is quite mild. The only parameter that exhibits any sort of downward bias and nonnormality is <inline-formula id="inline-formula92-1076998610396895">
<mml:math id="mml-inline92-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, although its true value is still within the standard error of estimate. With more observations at Level 2, this bias is alleviated. The random effects are recovered adequately—<inline-formula id="inline-formula93-1076998610396895">
<mml:math id="mml-inline93-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">cor</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi>b</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.63</mml:mn>
</mml:math>
</inline-formula>—at least as well as one would expect from only four observations per subject. As one would expect, a substantial amount of shrinkage is observed, as can be seen from <inline-formula id="inline-formula94-1076998610396895">
<mml:math id="mml-inline94-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. This shrinkage is alleviated by a larger sample at Level 2, that is, by more items, exactly as expected. In short, mixed beta regression has statistical properties fairly similar to other generalized linear mixed models that have been extensively studied over the last 10 years.</p>
<sec id="section10-1076998610396895">
<title>3.1. Partitioning the Variance</title>
<p>One common issue in GLMMs involves partitioning the variance into that due to different levels in the model. For instance, in a two-level model with a random interecept, what proportion of the variation in the dependent variable is attributable to fixed effects, unexplained Level 1 variability and the Level 2 variability accounted for by the random intercept? This was discussed quite thoroughly by Goldstein and colleagues (<xref ref-type="bibr" rid="bibr7-1076998610396895">Browne, Subramanian, Jones, &amp; Goldstein, 2005</xref>; <xref ref-type="bibr" rid="bibr22-1076998610396895">Goldstein, Browne, &amp; Rasbash, 2002</xref>) in the context of mixed binary regression. In particular, in a linear mixed model, the question is much more straightforward to answer than in a nonlinear model such as mixed beta regression. Mixed beta regression suffers the same problems as mixed binary regression regarding the impact of nonlinearity. In particular, because the variance is a function of the mean, it is not possible to compute straightforward quantities such as the intraclass correlation frequently used in a mixed model context to measure the proportion of dependence at the cluster level.</p>
<p>We have adapted the methods they proposed, which we illustrate here. The approximation method, as explained in <xref ref-type="bibr" rid="bibr22-1076998610396895">Goldstein et al. (2002)</xref>, uses a first-order Taylor expansion of the link function at the mean of the distribution of the appropriate random effects. <xref ref-type="bibr" rid="bibr7-1076998610396895">Browne et al. (2005)</xref> extend their approach to mixed models with more than two levels. We adapt their approach for the logistic link by incorporating the dispersion effect parameter <inline-formula id="inline-formula95-1076998610396895">
<mml:math id="mml-inline95-1076998610396895">
<mml:mtext>φ</mml:mtext>
</mml:math>
</inline-formula>. We then illustrate both methods using one of the random samples from the <inline-formula id="inline-formula96-1076998610396895">
<mml:math id="mml-inline96-1076998610396895">
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:mn>4</mml:mn>
</mml:math>
</inline-formula> simulated data.</p>
<p>We start with an estimated location submodel
<disp-formula id="disp-formula21-1076998610396895"><label>21</label>
<mml:math id="mml-disp21-1076998610396895">
<mml:mrow>
<mml:msub>
<mml:mi>μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi><mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo><mml:mfrac>
<mml:mrow>
<mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo>
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mi>k</mml:mi>
</mml:munder>

</mml:mstyle><mml:msub>
<mml:mi>β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo><mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>

</mml:mrow>
<mml:mo>)</mml:mo></mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo>
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mi>k</mml:mi>
</mml:munder>

</mml:mstyle><mml:msub>
<mml:mi>β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo><mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>

</mml:mrow>
<mml:mo>)</mml:mo></mml:mrow>
</mml:mrow>
</mml:mfrac>

</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula21-1076998610396895" xlink:href="10.3102_1076998610396895-eq21.tif"/>
</disp-formula>with <inline-formula id="inline-formula97-1076998610396895">
<mml:math id="mml-inline97-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">N</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>u</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and dispersion submodel <inline-formula id="inline-formula98-1076998610396895">
<mml:math id="mml-inline98-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mi>+</mml:mi>
<mml:mtext>exp</mml:mtext>
<mml:mo>(</mml:mo>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mo>δ</mml:mo>
<mml:mn>0</mml:mn></mml:msub>
<mml:mo>)</mml:mo>
</mml:math>
</inline-formula>. The Level 1 variance is
<disp-formula id="disp-formula22-1076998610396895"><label>22</label>
<mml:math id="mml-disp22-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal" stretchy="false">=</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula22-1076998610396895" xlink:href="10.3102_1076998610396895-eq22.tif"/>
</disp-formula>where <inline-formula id="inline-formula99-1076998610396895">
<mml:math id="mml-inline99-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="thinmathspace"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> denotes the predicted value for a combination of <inline-formula id="inline-formula100-1076998610396895">
<mml:math id="mml-inline100-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> values.</p>
<p>In a two-level model, following <xref ref-type="bibr" rid="bibr22-1076998610396895">Goldstein et al.’s (2002)</xref> first-order Taylor expansion of the location submodel, our adaptation of their formulation yields the following approximate estimate of the Level 2 variance,
<disp-formula id="disp-formula23-1076998610396895"><label>23</label>
<mml:math id="mml-disp23-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi>u</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal" stretchy="false">=</mml:mo>
<mml:mtext mathvariant="normal"> </mml:mtext>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">exp</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>J</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:msubsup>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mi>u</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
<graphic alternate-form-of="disp-formula23-1076998610396895" xlink:href="10.3102_1076998610396895-eq23.tif"/>
</disp-formula>for any given combination of <inline-formula id="inline-formula101-1076998610396895">
<mml:math id="mml-inline101-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> values. Finally, the approximate variance partition coefficient (VPC, the proportion of variation accounted for at Level 2) is
<disp-formula id="disp-formula24-1076998610396895"><label>24</label>
<mml:math id="mml-disp24-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">τ</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">≈</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
<mml:mrow>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">+</mml:mo>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula24-1076998610396895" xlink:href="10.3102_1076998610396895-eq24.tif"/>
</disp-formula>The simulation approach includes the following steps:<list list-type="order">
<list-item>
<p>We start with an estimated location submodel.</p>
</list-item>
<list-item>
<p>Generate a large number (say, <inline-formula id="inline-formula102-1076998610396895">
<mml:math id="mml-inline102-1076998610396895">
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>5</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mspace width="-.1667em"/>
<mml:mn>000</mml:mn>
</mml:math>
</inline-formula>) of random Level 2 residual values <inline-formula id="inline-formula103-1076998610396895">
<mml:math id="mml-inline103-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>m</mml:mi>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">N</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>b</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, for <italic>m</italic> from 1 to <italic>M</italic>.</p>
</list-item>
<list-item>
<p>Select one or more appropriate combinations of values for the covariates <inline-formula id="inline-formula104-1076998610396895">
<mml:math id="mml-inline104-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>. For each such combination, use sample estimates of the relevant parameters to compute <italic>M</italic> predicted values</p>
</list-item>
</list>

<disp-formula id="disp-formula25-1076998610396895"><label>25</label>
<mml:math id="mml-disp25-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">exp</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:munder>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi mathvariant="italic">k</mml:mi>
</mml:mrow>
</mml:munder>
</mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mrow>
<mml:mi mathvariant="italic">k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">x</mml:mi>
<mml:mrow>
<mml:mi mathvariant="italic">j</mml:mi>
<mml:mi mathvariant="italic">k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo mathvariant="normal" stretchy="false">+</mml:mo>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">b</mml:mi>
<mml:mi mathvariant="italic">m</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">exp</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:munder>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mi>k</mml:mi>
</mml:munder>
</mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>m</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula25-1076998610396895" xlink:href="10.3102_1076998610396895-eq25.tif"/>
</disp-formula>In our <inline-formula id="inline-formula105-1076998610396895">
<mml:math id="mml-inline105-1076998610396895">
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:mn>4</mml:mn>
</mml:math>
</inline-formula> example, we do this for each of the four combinations of 0–1 values for the dummy <inline-formula id="inline-formula106-1076998610396895">
<mml:math id="mml-inline106-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> variables.<list list-type="order">
<list-item>
<p>Compute the variance <inline-formula id="inline-formula107-1076998610396895">
<mml:math id="mml-inline107-1076998610396895">
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>b</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula> of the <inline-formula id="inline-formula108-1076998610396895">
<mml:math id="mml-inline108-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>.</p>
</list-item>
<list-item>
<p>Compute <inline-formula id="inline-formula109-1076998610396895">
<mml:math id="mml-inline109-1076998610396895">
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mi>u</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>.</p>
</list-item>
<list-item>
<p>The approximate VPC is <inline-formula id="inline-formula110-1076998610396895">
<mml:math id="mml-inline110-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">τ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>.</p>
</list-item>
</list>The approximation method is illustrated here; a similar demonstration of the simulation method is available from the authors. In our sample of 100 observations from our repeated-measures simulation, the sample means are <inline-formula id="inline-formula111-1076998610396895">
<mml:math id="mml-inline111-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.1807</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula112-1076998610396895">
<mml:math id="mml-inline112-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.5136</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula113-1076998610396895">
<mml:math id="mml-inline113-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>3</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.5106</mml:mn>
</mml:math>
</inline-formula>, and <inline-formula id="inline-formula114-1076998610396895">
<mml:math id="mml-inline114-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>4</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.7974</mml:mn>
</mml:math>
</inline-formula>. The sample <inline-formula id="inline-formula115-1076998610396895">
<mml:math id="mml-inline115-1076998610396895">
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> are <inline-formula id="inline-formula116-1076998610396895">
<mml:math id="mml-inline116-1076998610396895">
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1.438</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula117-1076998610396895">
<mml:math id="mml-inline117-1076998610396895">
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>2.019</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula118-1076998610396895">
<mml:math id="mml-inline118-1076998610396895">
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>3</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1.770</mml:mn>
</mml:math>
</inline-formula>, and <inline-formula id="inline-formula119-1076998610396895">
<mml:math id="mml-inline119-1076998610396895">
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>4</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>2.787</mml:mn>
</mml:math>
</inline-formula>. Finally, the sample <inline-formula id="inline-formula120-1076998610396895">
<mml:math id="mml-inline120-1076998610396895">
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>u</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.3067</mml:mn>
</mml:math>
</inline-formula>. The Level 1 variances are shown in the upper row of <xref ref-type="table" rid="table2-1076998610396895">Table 2</xref>
. For example, <inline-formula id="inline-formula121-1076998610396895">
<mml:math id="mml-inline121-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.0607</mml:mn>
</mml:math>
</inline-formula>. The Level 2 variances for the approximation method are listed in the row beneath the Level 1 variances. For instance,
<disp-formula id="disp-formula26-1076998610396895">
<mml:math id="mml-disp26-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi>u</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal" stretchy="false">=</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msubsup>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mi>u</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfenced>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mtext mathvariant="normal"> </mml:mtext>
<mml:mo mathvariant="normal" stretchy="false">=</mml:mo>
<mml:mtext mathvariant="normal"> </mml:mtext>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">.0067</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal">.</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula26-1076998610396895" xlink:href="10.3102_1076998610396895-eq26.tif"/>
</disp-formula>The VPC results are shown in the lower part of <xref ref-type="table" rid="table2-1076998610396895">Table 2</xref>, for both the approximation and the simulation (from a typical simulation run) methods. They are reasonably close, with the simulation method returning a somewhat lower VPC for the second and third categories than the approximation method does. In either case, the VPC magnitudes reflect moderate clustering due to Level 2 effects, as can be seen from the <inline-formula id="inline-formula122-1076998610396895">
<mml:math id="mml-inline122-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">τ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> magnitudes which range from about .10 to .19.</p>
<table-wrap id="table2-1076998610396895" position="float">
<label>Table 2.</label>
<caption>
<p>Approximation and Simulation Method VPC Results</p>
</caption>
<graphic alternate-form-of="table2-1076998610396895" xlink:href="10.3102_1076998610396895-table2.tif"/>
<table>
<thead>
<tr>
<th align="left">
</th>
<th align="left">
<inline-formula id="inline-formula123-1076998610396895">
<mml:math id="mml-inline123-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</th>
<th align="left">
<inline-formula id="inline-formula124-1076998610396895">
<mml:math id="mml-inline124-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</th>
<th align="left">
<inline-formula id="inline-formula125-1076998610396895">
<mml:math id="mml-inline125-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mn>3</mml:mn>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</th>
<th align="left">
<inline-formula id="inline-formula126-1076998610396895">
<mml:math id="mml-inline126-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mn>4</mml:mn>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</th>
</tr>
<tr>
<th align="left">
</th>
<th align="left">0.0607</th>
<th align="left">0.0827</th>
<th align="left">0.0902</th>
<th align="left">0.0427</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">
</td>
<td align="left">
<inline-formula id="inline-formula127-1076998610396895">
<mml:math id="mml-inline127-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</td>
<td align="left">
<inline-formula id="inline-formula128-1076998610396895">
<mml:math id="mml-inline128-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</td>
<td align="left">
<inline-formula id="inline-formula129-1076998610396895">
<mml:math id="mml-inline129-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>3</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</td>
<td align="left">
<inline-formula id="inline-formula130-1076998610396895">
<mml:math id="mml-inline130-1076998610396895">
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>4</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>
</td>
</tr>
<tr>
<td align="left">Approximation</td>
<td align="left">0.0067</td>
<td align="left">0.0191</td>
<td align="left">0.0192</td>
<td align="left">0.0080</td>
</tr>
<tr>
<td align="left">Simulation</td>
<td align="left">0.0072</td>
<td align="left">0.0165</td>
<td align="left">0.0165</td>
<td align="left">0.0081</td>
</tr>
<tr>
<td align="left">
</td>
<td align="left">
<inline-formula id="inline-formula131-1076998610396895">
<mml:math id="mml-inline131-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">τ</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td align="left">
<inline-formula id="inline-formula132-1076998610396895">
<mml:math id="mml-inline132-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">τ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td align="left">
<inline-formula id="inline-formula133-1076998610396895">
<mml:math id="mml-inline133-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">τ</mml:mi>
<mml:mn>3</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td align="left">
<inline-formula id="inline-formula134-1076998610396895">
<mml:math id="mml-inline134-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">τ</mml:mi>
<mml:mn>4</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>
</td>
</tr>
<tr>
<td align="left">Approximation</td>
<td align="left">0.0997</td>
<td align="left">0.1879</td>
<td align="left">0.1751</td>
<td align="left">0.1580</td>
</tr>
<tr>
<td align="left">Simulation</td>
<td align="left">0.1060</td>
<td align="left">0.1666</td>
<td align="left">0.1550</td>
<td align="left">0.1604</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
</sec>
<sec id="section11-1076998610396895">
<title>4. Applications</title>
<p>We provide three real examples of mixed or mixture beta regressions using data from several cognitive experiments. The first example shows a mixture model applied to judged probabilities elicited in an experiment by <xref ref-type="bibr" rid="bibr24-1076998610396895">Gurr (2009)</xref>. It demonstrates the performance of a finite mixture of betas in a relatively simple setting. The second example is a more complicated reanalysis of judged probability data from Fox and Rottensreich (2006). This example illustrates the fact that modeling the dispersion structure often leads to different conclusions than modeling location alone, more in line with theoretical expectations. The third example considers numerical confidence ratings taken from <xref ref-type="bibr" rid="bibr42-1076998610396895">Roy and Liersch (2009)</xref>. This model includes both location and dispersion submodel random effects.</p>
<sec id="section12-1076998610396895">
<title>4.1. Partition Effects on Probability Judgments by a Finite Mixture Model</title>
<p>On grounds of insufficient reason, a probability of <inline-formula id="inline-formula135-1076998610396895">
<mml:math id="mml-inline135-1076998610396895">
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mi>K</mml:mi>
</mml:math>
</inline-formula> is assigned to <inline-formula id="inline-formula136-1076998610396895">
<mml:math id="mml-inline136-1076998610396895">
<mml:mi>K</mml:mi>
</mml:math>
</inline-formula> mutually exclusive possible events when nothing is known about the likelihood of those events. <xref ref-type="bibr" rid="bibr19-1076998610396895">Fox and Rottenstreich (2003)</xref> found evidence that subjective probability judgments are typically biased toward this distribution. <xref ref-type="bibr" rid="bibr47-1076998610396895">Smithson and Segale (2009)</xref> extended the study of partition effects to judgments of lower and upper probabilities and introduced a beta regression mixture modeling approach to analyzing this influence.</p>
<p>
<xref ref-type="bibr" rid="bibr24-1076998610396895">Gurr (2009)</xref> investigated partition priming effects by setting up an ambiguous sample space and introducing an experimental manipulation to influence perceptions of this space. One hundred fifty-five undergraduate students at The Australian National University (108 females, 43 males, and 4 unspecified) were recruited for his study. Their ages ranged from 17 to 43 years (<inline-formula id="inline-formula137-1076998610396895">
<mml:math id="mml-inline137-1076998610396895">
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>21.41</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula138-1076998610396895">
<mml:math id="mml-inline138-1076998610396895">
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>4.46</mml:mn>
</mml:math>
</inline-formula>). The participants completed two probability judgment tasks. The first task was a “no-information” condition and participants were told that “In this block you will be asked to make decisions with very little information. A man, Geoff, works as an engineer in a big hydraulic power production firm. Geoff has three acquaintances, Pat, James and Chris.” They were then asked to assign probabilities to the three acquaintances being hydraulic power engineers in Geoffs firm. The second “information” condition repeated the task but provided information about each acquaintances engineering qualifications.</p>
<p>The resultant sample space is ambiguous because the number of positions available in Geoff’s firm and the number of potential applicants to those positions are not specified. The hypothesis associated with this part of the study was that participants would be more likely to return “ignorance” priors (i.e., assigning equal probabilities to all three targets) under the no-information condition. In particular, it was expected that the no-information condition would elicit more ignorance priors comprising probabilities of 1/2 for each target, by virtue of participants thinking in terms of a two-alternative sample space for each target.</p>
<p>Participants also were randomly assigned to one of two priming conditions, one asking them which applicant was most likely to get a position in Geoff’s firm (the comparison prime) before they were asked for their probability assignments and the other not asking this question. The major hypothesis for this aspect of the experiment was that the comparison prime would make participants more likely to make their probabilities additive (i.e., summing to 1 across the three targets) because it would cue them to think that there might be only one position available. The provision of three targets enables a distinction between judges expressing ignorance by assigning probabilities of 1/2 to all three targets and who express ignorance by assigning 1/3 to all targets, whereas using only two targets would conflate additivity with ignorance prior assignments.</p>
<p>Initially, probability assignments in the two tasks were analysed separately using mixture beta-regression models via MLE methods, using SAS 9.2 and IBM SPSS Statistics 18 (IBM SPSS results are reported here). No-effects models were determined by comparing model fit for a single-distribution, two-component, and three-component mixture models. All two-component mixture models fixed one component distribution’s mean either at 1/2 or 1/3 while freeing the other mean to be estimated, and the three-component model fixed two component means at 1/2 and 1/3 while freeing the third component mean to be estimated. The fixed-means component distributions were modeled by <inline-formula id="inline-formula139-1076998610396895">
<mml:math id="mml-inline139-1076998610396895">
<mml:msub>
<mml:mi>f</mml:mi>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Uniform</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi>c</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, with <inline-formula id="inline-formula140-1076998610396895">
<mml:math id="mml-inline140-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> given values of .001, .01, and .05. We report findings for <inline-formula id="inline-formula141-1076998610396895">
<mml:math id="mml-inline141-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.01</mml:mn>
</mml:math>
</inline-formula>, as the other values yielded similar results.</p>
<p>Beginning with the no-information condition task, both two-component mixture models (one with a component mean fixed at 1/2 and the other fixed at 1/3) outperform the single-distribution model, difference in log-likelihood chi-squares: <inline-formula id="inline-formula142-1076998610396895">
<mml:math id="mml-inline142-1076998610396895">
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">χ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>108.08</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula143-1076998610396895">
<mml:math id="mml-inline143-1076998610396895">
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>.0005</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula144-1076998610396895">
<mml:math id="mml-inline144-1076998610396895">
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">χ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>50.48</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula145-1076998610396895">
<mml:math id="mml-inline145-1076998610396895">
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>.0005</mml:mn>
</mml:math>
</inline-formula>, respectively, and a three-component model (with two-component means fixed at 1/2 and 1/3) outperforms the best of the two-component models, <inline-formula id="inline-formula146-1076998610396895">
<mml:math id="mml-inline146-1076998610396895">
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">χ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>135.66</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula147-1076998610396895">
<mml:math id="mml-inline147-1076998610396895">
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>.0005</mml:mn>
</mml:math>
</inline-formula>. Turning to the information condition, both two-component mixture models (one with a component mean fixed at 1/2 and the other fixed at 1/3) outperform the single-distribution model, difference in log-likelihood chi-squares: <inline-formula id="inline-formula148-1076998610396895">
<mml:math id="mml-inline148-1076998610396895">
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">χ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>29.75</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula149-1076998610396895">
<mml:math id="mml-inline149-1076998610396895">
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>.0005</mml:mn>
</mml:math>
</inline-formula>, and <inline-formula id="inline-formula150-1076998610396895">
<mml:math id="mml-inline150-1076998610396895">
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">χ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>102.09</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula151-1076998610396895">
<mml:math id="mml-inline151-1076998610396895">
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>.0005</mml:mn>
</mml:math>
</inline-formula>, respectively and a three-component model (with two component means fixed at 1/2 and 1/3) outperforms the best of the two-component models, <inline-formula id="inline-formula152-1076998610396895">
<mml:math id="mml-inline152-1076998610396895">
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">χ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>26.62</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula153-1076998610396895">
<mml:math id="mml-inline153-1076998610396895">
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mn>.0005</mml:mn>
</mml:math>
</inline-formula>. The three-component model therefore is adopted for both the no-information and information tasks. The distributions for both tasks under the two experimental conditions are shown in <xref ref-type="fig" rid="fig1-1076998610396895">Figure 1</xref>
.</p>
<fig id="fig1-1076998610396895" position="float">
<label>Figure 1.</label>
<caption>
<p>Component distributions for prime by information conditions.</p>
</caption>
<graphic xlink:href="10.3102_1076998610396895-fig1.tif"/>
</fig>
<p>A mixed three-component mixture model was estimated in WinBUGs 1.4.3 to assess the joint effects of the prime and the no-information versus information conditions. This model is limited to a random intercept in the location submodel, due to there being only two data points per participant. The location submodel is
<disp-formula id="disp-formula27-1076998610396895">
<mml:math id="mml-disp27-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula27-1076998610396895" xlink:href="10.3102_1076998610396895-eq27.tif"/>
</disp-formula>

<disp-formula id="disp-formula28-1076998610396895">
<mml:math id="mml-disp28-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mn>3</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula28-1076998610396895" xlink:href="10.3102_1076998610396895-eq28.tif"/>
</disp-formula>

<disp-formula id="disp-formula29-1076998610396895"><label>26</label>
<mml:math id="mml-disp29-1076998610396895">
<mml:mi>g</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>3</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula29-1076998610396895" xlink:href="10.3102_1076998610396895-eq29.tif"/>
</disp-formula>where <inline-formula id="inline-formula154-1076998610396895">
<mml:math id="mml-inline154-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>b</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. The dispersion submodel is
<disp-formula id="disp-formula30-1076998610396895"><label>27</label>
<mml:math id="mml-disp30-1076998610396895">
<mml:mi>h</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>c</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mrow>
<mml:mi>c</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula30-1076998610396895" xlink:href="10.3102_1076998610396895-eq30.tif"/>
</disp-formula>

<disp-formula id="disp-formula31-1076998610396895">
<mml:math id="mml-disp31-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mrow>
<mml:mi>c</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:mfenced>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula31-1076998610396895" xlink:href="10.3102_1076998610396895-eq31.tif"/>
</disp-formula>For <inline-formula id="inline-formula155-1076998610396895">
<mml:math id="mml-inline155-1076998610396895">
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula> and 2, we used informative priors to simulate compressed component distributions centered at 1/2 and 1/3, whereas for <inline-formula id="inline-formula156-1076998610396895">
<mml:math id="mml-inline156-1076998610396895">
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>3</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
</inline-formula> we used a noninformative prior. Finally, the composition submodel is
<disp-formula id="disp-formula32-1076998610396895">
<mml:math id="mml-disp32-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mrow>
<mml:mn>3</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>10</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>11</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mi>q</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:math>
<graphic alternate-form-of="disp-formula32-1076998610396895" xlink:href="10.3102_1076998610396895-eq32.tif"/>
</disp-formula>

<disp-formula id="disp-formula33-1076998610396895"><label>28</label>
<mml:math id="mml-disp33-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mrow>
<mml:mn>3</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>20</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>21</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mi>q</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:math>
<graphic alternate-form-of="disp-formula33-1076998610396895" xlink:href="10.3102_1076998610396895-eq33.tif"/>
</disp-formula>

<disp-formula id="disp-formula34-1076998610396895">
<mml:math id="mml-disp34-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mrow>
<mml:mn>3</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">γ</mml:mi>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mi>j</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula34-1076998610396895" xlink:href="10.3102_1076998610396895-eq34.tif"/>
</disp-formula>where <inline-formula id="inline-formula157-1076998610396895">
<mml:math id="mml-inline157-1076998610396895">
<mml:msub>
<mml:mi>q</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> for the no-prime condition and 1 for the prime.</p>
<p>We used a two-chain model with a 10,000 iteration burn-in and estimates based on the subsequent 10,000 iterations. The parameter estimates, standard errors, and 95% credible intervals are shown in <xref ref-type="table" rid="table3-1076998610396895">Table 3</xref>
. The priming effect on relative composition is similar in both conditions, as can be seen from the credible intervals for <inline-formula id="inline-formula158-1076998610396895">
<mml:math id="mml-inline158-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>112</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>111</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula159-1076998610396895">
<mml:math id="mml-inline159-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>212</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>211</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>. The large positive <inline-formula id="inline-formula160-1076998610396895">
<mml:math id="mml-inline160-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>111</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula161-1076998610396895">
<mml:math id="mml-inline161-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>112</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> values suggest that the presence of the comparison prime increases the proportion of additive responses (i.e., those summing to 1). However, the small <inline-formula id="inline-formula162-1076998610396895">
<mml:math id="mml-inline162-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>211</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula163-1076998610396895">
<mml:math id="mml-inline163-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>212</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> values indicate that it has no discernible impact on the proportion of responses centered on 1/2. On the other hand, the large negative <inline-formula id="inline-formula164-1076998610396895">
<mml:math id="mml-inline164-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>202</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>201</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> difference suggests that the no-information condition increases the proportion of responses summing to 1/2, whereas the weak <inline-formula id="inline-formula165-1076998610396895">
<mml:math id="mml-inline165-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>102</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">η</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>101</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> difference shows that it has little or no effect on the proportion of additive responses. Thus, both of the aforementioned hypotheses received some support.</p>
<table-wrap id="table3-1076998610396895" position="float">
<label>Table 3.</label>
<caption>
<p>Beta-Regression Mixture Models for Gurr (2009) Study</p>
</caption>
<graphic alternate-form-of="table3-1076998610396895" xlink:href="10.3102_1076998610396895-table3.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>
</th>
<th>
</th>
<th>Credible</th>
<th>Interval</th>
</tr>
<tr>
<th>Parameter</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
<th>Lower</th>
<th>Upper</th>
</tr>
</thead>
<tbody>
<tr>
<td>Location</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula166-1076998610396895">
<mml:math id="mml-inline166-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mrow>
<mml:mn>01</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.330</td>
<td>0.129</td>
<td>−0.570</td>
<td>−0.065</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula167-1076998610396895">
<mml:math id="mml-inline167-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mrow>
<mml:mn>02</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.109</td>
<td>0.107</td>
<td>−0.307</td>
<td>0.100</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula168-1076998610396895">
<mml:math id="mml-inline168-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mi>b</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>1.002</td>
<td>0.087</td>
<td>0.840</td>
<td>1.180</td>
</tr>
<tr>
<td>Dispersion</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula169-1076998610396895">
<mml:math id="mml-inline169-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mn>11</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>8.737</td>
<td>0.352</td>
<td>8.037</td>
<td>9.402</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula170-1076998610396895">
<mml:math id="mml-inline170-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mrow>
<mml:mn>12</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>6.980</td>
<td>0.522</td>
<td>6.120</td>
<td>8.180</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula171-1076998610396895">
<mml:math id="mml-inline171-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mrow>
<mml:mn>21</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>16.800</td>
<td>0.318</td>
<td>16.170</td>
<td>17.410</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula172-1076998610396895">
<mml:math id="mml-inline172-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mrow>
<mml:mn>22</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>15.900</td>
<td>0.316</td>
<td>15.270</td>
<td>16.510</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula173-1076998610396895">
<mml:math id="mml-inline173-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mrow>
<mml:mn>31</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>2.550</td>
<td>0.303</td>
<td>1.972</td>
<td>3.165</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula174-1076998610396895">
<mml:math id="mml-inline174-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mrow>
<mml:mn>32</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>3.092</td>
<td>0.280</td>
<td>2.568</td>
<td>3.643</td>
</tr>
<tr>
<td>Compos.</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula175-1076998610396895">
<mml:math id="mml-inline175-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>101</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−1.409</td>
<td>0.381</td>
<td>−2.233</td>
<td>−0.731</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula176-1076998610396895">
<mml:math id="mml-inline176-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>102</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−1.782</td>
<td>0.488</td>
<td>−2.864</td>
<td>−0.973</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula177-1076998610396895">
<mml:math id="mml-inline177-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>201</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.640</td>
<td>0.261</td>
<td>−1.169</td>
<td>−0.143</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula178-1076998610396895">
<mml:math id="mml-inline178-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>202</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−1.567</td>
<td>0.327</td>
<td>−2.238</td>
<td>−0.968</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula179-1076998610396895">
<mml:math id="mml-inline179-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>111</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>1.645</td>
<td>0.455</td>
<td>0.803</td>
<td>2.579</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula180-1076998610396895">
<mml:math id="mml-inline180-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>112</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>1.329</td>
<td>0.571</td>
<td>0.304</td>
<td>2.557</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula181-1076998610396895">
<mml:math id="mml-inline181-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>211</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.176</td>
<td>0.425</td>
<td>−1.005</td>
<td>0.647</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula182-1076998610396895">
<mml:math id="mml-inline182-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>212</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.505</td>
<td>0.565</td>
<td>−1.674</td>
<td>0.588</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula183-1076998610396895">
<mml:math id="mml-inline183-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>102</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>101</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.373</td>
<td>0.615</td>
<td>−1.642</td>
<td>0.788</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula184-1076998610396895">
<mml:math id="mml-inline184-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>202</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>201</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.927</td>
<td>0.417</td>
<td>−1.756</td>
<td>−0.132</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula185-1076998610396895">
<mml:math id="mml-inline185-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>112</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>111</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.315</td>
<td>0.731</td>
<td>−1.702</td>
<td>1.169</td>
</tr>
<tr>
<td> <inline-formula id="inline-formula186-1076998610396895">
<mml:math id="mml-inline186-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>212</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">η</mml:mi>
<mml:mrow>
<mml:mn>211</mml:mn>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>−0.328</td>
<td>0.710</td>
<td>−1.749</td>
<td>1.072</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The estimated and observed proportions are shown in <xref ref-type="table" rid="table4-1076998610396895">Table 4</xref>
. There is a slight tendency to underestimate the proportions of additive responses in the no-prime condition and to overestimate them in the prime condition for the information-condition task, but otherwise the composition structure is recovered well. Of the 310 mean component classification scores produced by the MCMC run, 288 (92.9%) of these were within 0.1 of their correct classification, so the model correctly assigned most cases to a component distribution.</p>
<table-wrap id="table4-1076998610396895" position="float">
<label>Table 4.</label>
<caption>
<p>Recovery of Mixture Composition Structure</p>
</caption>
<graphic alternate-form-of="table4-1076998610396895" xlink:href="10.3102_1076998610396895-table4.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Component</th>
<th>Estimated</th>
<th>Observed</th>
<th>Estimated</th>
<th>Observed</th>
</tr>
<tr>
<th>Condition</th>
<th>Mean</th>
<th>No Prime</th>
<th>No Prime</th>
<th>Prime</th>
<th>Prime</th>
</tr>
</thead>
<tbody>
<tr>
<td>
</td>
<td>1/3</td>
<td>0.138</td>
<td>0.156</td>
<td>0.468</td>
<td>0.470</td>
</tr>
<tr>
<td>No-info</td>
<td>1/2</td>
<td>0.298</td>
<td>0.299</td>
<td>0.163</td>
<td>0.167</td>
</tr>
<tr>
<td>
</td>
<td>Free</td>
<td>0.564</td>
<td>0.545</td>
<td>0.369</td>
<td>0.359</td>
</tr>
<tr>
<td>
</td>
<td>1/3</td>
<td>0.122</td>
<td>0.143</td>
<td>0.361</td>
<td>0.346</td>
</tr>
<tr>
<td>Information</td>
<td>1/2</td>
<td>0.152</td>
<td>0.156</td>
<td>0.072</td>
<td>0.077</td>
</tr>
<tr>
<td>
</td>
<td>Free</td>
<td>0.726</td>
<td>0.701</td>
<td>0.568</td>
<td>0.577</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section13-1076998610396895">
<title>4.2. Dispersion Modeling to Address Response Style in Judged Probabilities</title>
<p>
<xref ref-type="bibr" rid="bibr45-1076998610396895">See, Fox, and Rottenstreich (2006)</xref> presented a series of studies of the influence that the number of possibilities (the state-space partition) has on judged probabilities. In their Study 1, participants observed a set of randomly ordered restaurant receipts for various meals on various days by a person named “Joe.” The receipts contained two pieces of information: day of the week (Sunday, Monday, …, Saturday) and meal category (breakfast, lunch, and dinner). Participants were told they would be asked to judge the likelihood of the events they had observed. See et al.’s primary hypothesis was that subjective probability estimates would be biased toward the ignorance prior suggested by the target attribute’s partition: meal of the day (three alternatives, therefore an ignorance prior of 1/3) or day of the week (1/7). Following the learning phase, 267 participants were asked to estimate the likelihoods of all 10 attributes (meal categories and days of the week). One peculiarity of their data is the substantial number of exact boundary observations, particularly concentrated in certain participants. These values needed to be adjusted slightly. In our analysis here, we replaced exact 0 values (12 in all) with 0.001 and exact 1 (5 in all) with 0.999. The exact 1 responses are particularly important because these are highly unlikely responses given the stochastic specification of the model.</p>
<p>To assess the relative influence of the partition and the information provided during the learning phase, See et al. conducted separate regression analyses for each participant. Their model was derived from <xref ref-type="bibr" rid="bibr19-1076998610396895">Fox and Rottenstreich’s (2003)</xref> ignorance prior model, which can be written as follows:
<disp-formula id="disp-formula35-1076998610396895"><label>29</label>
<mml:math id="mml-disp35-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mrow>
<mml:mi>a</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mi>a</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula35-1076998610396895" xlink:href="10.3102_1076998610396895-eq35.tif"/>
</disp-formula>where <inline-formula id="inline-formula187-1076998610396895">
<mml:math id="mml-inline187-1076998610396895">
<mml:msub>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> is the probability of the <italic>j</italic>th target judged by the <italic>i</italic>th subject, <inline-formula id="inline-formula188-1076998610396895">
<mml:math id="mml-inline188-1076998610396895">
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula189-1076998610396895">
<mml:math id="mml-inline189-1076998610396895">
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mrow>
<mml:mi>a</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> are the number of distinct states that the <italic>j</italic>th focal target and its alternatives, respectively, can take on (i.e., 1 and 2 for meals and 1 and 6 for days of the week), and <inline-formula id="inline-formula190-1076998610396895">
<mml:math id="mml-inline190-1076998610396895">
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula191-1076998610396895">
<mml:math id="mml-inline191-1076998610396895">
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mi>a</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> are the frequencies with which the <italic>j</italic>th focal target and its alternatives appeared during the learning phase. They reported the median intercept (−0.13), regression coefficients (<inline-formula id="inline-formula192-1076998610396895">
<mml:math id="mml-inline192-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.36</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula193-1076998610396895">
<mml:math id="mml-inline193-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.46</mml:mn>
</mml:math>
</inline-formula>), and multiple <inline-formula id="inline-formula194-1076998610396895">
<mml:math id="mml-inline194-1076998610396895">
<mml:msup>
<mml:mi>R</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> (.61) for these regressions.</p>
<p>One obvious weakness with this analysis is the fact that each participant only provides 10 observations, which means each individual-level regression only has six degrees of freedom. We reanalyzed their data using a beta GLMM approach. All models shown here were fit using WinBUGs 1.4.3. We ran two-chain models with a burn-in of 5,000 iterations and an additional 10,000 iterations for estimation purposes. (MML was also attempted. The final Hessian was singular but coefficient estimates were very close to those from MCMC.) In these models, <inline-formula id="inline-formula195-1076998610396895">
<mml:math id="mml-inline195-1076998610396895">
<mml:mi>i</mml:mi>
</mml:math>
</inline-formula> indexes subjects, <inline-formula id="inline-formula196-1076998610396895">
<mml:math id="mml-inline196-1076998610396895">
<mml:mi>j</mml:mi>
</mml:math>
</inline-formula> indexes stimuli, and <inline-formula id="inline-formula197-1076998610396895">
<mml:math id="mml-inline197-1076998610396895">
<mml:mi>k</mml:mi>
</mml:math>
</inline-formula> indexes dependent variables including the intercept. In line with convention, when <inline-formula id="inline-formula198-1076998610396895">
<mml:math id="mml-inline198-1076998610396895">
<mml:mi>k</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula199-1076998610396895">
<mml:math id="mml-inline199-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula200-1076998610396895">
<mml:math id="mml-inline200-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>0</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = intercept. We began with a model incorporating a random intercept in the location submodel (Model 0):
<disp-formula id="disp-formula36-1076998610396895"><label>30</label>
<mml:math id="mml-disp36-1076998610396895">
<mml:msub>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula36-1076998610396895" xlink:href="10.3102_1076998610396895-eq36.tif"/>
</disp-formula>

<disp-formula id="disp-formula37-1076998610396895"><label>31</label>
<mml:math id="mml-disp37-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munder>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mi>k</mml:mi>
</mml:munder>
</mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula37-1076998610396895" xlink:href="10.3102_1076998610396895-eq37.tif"/>
</disp-formula>

<disp-formula id="disp-formula38-1076998610396895"><label>32</label>
<mml:math id="mml-disp38-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula38-1076998610396895" xlink:href="10.3102_1076998610396895-eq38.tif"/>
</disp-formula>where <inline-formula id="inline-formula201-1076998610396895">
<mml:math id="mml-inline201-1076998610396895">
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mrow>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula202-1076998610396895">
<mml:math id="mml-inline202-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, and <inline-formula id="inline-formula203-1076998610396895">
<mml:math id="mml-inline203-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. We then estimated a model with random <inline-formula id="inline-formula204-1076998610396895">
<mml:math id="mml-inline204-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> and independent random-effects terms (Model 1), so that the right-hand side of <xref ref-type="disp-formula" rid="disp-formula40-1076998610396895">Equation 33</xref> is replaced with <inline-formula id="inline-formula205-1076998610396895">
<mml:math id="mml-inline205-1076998610396895">
<mml:mrow>
<mml:munder>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mi>k</mml:mi>
</mml:munder>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfenced>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula206-1076998610396895">
<mml:math id="mml-inline206-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, and a model with correlated random-effects (Model 2), in which <inline-formula id="inline-formula207-1076998610396895">
<mml:math id="mml-inline207-1076998610396895">
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is replaced with <inline-formula id="inline-formula208-1076998610396895">
<mml:math id="mml-inline208-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">i</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mo fence="true" maxsize="1.2em" minsize="1.2em" stretchy="true" symmetric="true">(</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="bold">0</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">Σ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo fence="true" maxsize="1.2em" minsize="1.2em" stretchy="true" symmetric="true">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>.</p>
<p>
<xref ref-type="table" rid="table4-1076998610396895">Table 4</xref> shows the posterior mean <inline-formula id="inline-formula209-1076998610396895">
<mml:math id="mml-inline209-1076998610396895">
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
<mml:mtext mathvariant="normal"> </mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="script">L</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> figures for the three models, indicating clear gains in model fit for each of them. That said, the location and dispersion fixed-effects submodels were nearly unchanged in these three models. The Model 2 estimates and 95% credibility intervals are <inline-formula id="inline-formula210-1076998610396895">
<mml:math id="mml-inline210-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>0.193</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>0.259</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>0.130</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>; <inline-formula id="inline-formula211-1076998610396895">
<mml:math id="mml-inline211-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.233</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>0.161</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>0.303</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>; <inline-formula id="inline-formula212-1076998610396895">
<mml:math id="mml-inline212-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.502</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>0.449</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>0.555</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>; and <inline-formula id="inline-formula213-1076998610396895">
<mml:math id="mml-inline213-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2.886</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2.944</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2.827</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>. These results show a somewhat different picture from See et al.’s median regression coefficients, suggesting a greater influence from the learning component and a lesser one from the partition.</p>
<p>Other models are possible. One obvious alternative is a model that incorporates a random intercept in the dispersion submodel instead of one in the location submodel. Including random intercepts in both submodels resulted in models that failed to converge in WinBUGS. However, a model with a random intercept in the dispersion submodel alone converged well. Moreover, it considerably outperformed all of the models presented thus far (posterior mean <inline-formula id="inline-formula214-1076998610396895">
<mml:math id="mml-inline214-1076998610396895">
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
<mml:mtext mathvariant="normal"> </mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="script">L</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>5783.86</mml:mn>
</mml:math>
</inline-formula>). More importantly, this model yields a somewhat different location submodel from the other approach. The model estimates and 95% credibility intervals are <inline-formula id="inline-formula215-1076998610396895">
<mml:math id="mml-inline215-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>0.149</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>0.198</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>0.101</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>; <inline-formula id="inline-formula216-1076998610396895">
<mml:math id="mml-inline216-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.395</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>0.339</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>0.444</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>; <inline-formula id="inline-formula217-1076998610396895">
<mml:math id="mml-inline217-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.403</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">&lt;</mml:mo>
</mml:math>
</inline-formula>;
and <inline-formula id="inline-formula218-1076998610396895">
<mml:math id="mml-inline218-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>3.012</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>3.130</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2.896</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>. These results suggest that the partition effect is nearly equal to the effect of the information provided in the learning phase, more in line with See et al.’s findings.</p>
<p>What, then, does the dispersion random effect mean here? If one examines the raw data as well as <inline-formula id="inline-formula219-1076998610396895">
<mml:math id="mml-inline219-1076998610396895">
<mml:mover accent="true">
<mml:mi>d</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula>, it appears that it essentially addresses systematic differences in response style away from the primed anchors. A location random effect here would mean that participants systematically anchored off the primes. There seems to be no evidence for this in these data, which is unsurprising given the theory.</p>
<p>
<xref ref-type="bibr" rid="bibr45-1076998610396895">See et al. (2006)</xref> also collected confidence judgements and knowledge judgments to measure how knowledgable their subjects were. The motivation behind this was that more knowledgeable subjects might be less susceptible to priming effects. A subject-level knowledgeability covariate could be incorporated into a more elaborate model, with a cross-level knowledgeability-by-partition interaction term to ascertain whether greater knowledgeability lessens the partition effect.</p>
<p>As a comparison and more in line with the original analysis, we also attempted to fit logit-normal mixed models by transforming the dependent variable and assuming it is normal. These models exhibited substantial sensitivity to the choice of <inline-formula id="inline-formula220-1076998610396895">
<mml:math id="mml-inline220-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, the shrinkage parameter in <xref ref-type="disp-formula" rid="disp-formula11-1076998610396895">Equation 11</xref>. In particular, the variance of the random effect was pulled substantially by choice of <inline-formula id="inline-formula221-1076998610396895">
<mml:math id="mml-inline221-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. This is, no doubt, because the usage of boundary values varies drastically within participants.</p>
</sec>
<sec id="section14-1076998610396895">
<title>4.3. Location and Scale Mixing: Numerical Confidence Ratings</title>
<p>The data used for this example come from <xref ref-type="bibr" rid="bibr42-1076998610396895">Roy and Liersch (2009)</xref>, who considered the “better than average” effect, also known as the Lake Wobegon Effect, the well-known finding that more than 50% of subjects are willing to rate themselves as being better than average, impossible in a symmetric distribution (e.g., <xref ref-type="bibr" rid="bibr2-1076998610396895">Alicke &amp; Govorun, 2005</xref>). The participants are 35 female undergraduates at a large Midwestern university (an additional 20 male participants were dropped from the analysis here to avoid dealing with a substantial gender by skill interaction). They provided numerical confidence ratings (on a scale of 0–100) of their ability to perform ten skills: riding a bike (bke), dancing (dnc), driving a car (drv), sensing emotion (emo), karate (kte), performing magic tricks (mag), musical performance (mus), speaking in public (spk), playing “ball” sports (spt), and tying a shoe (sho).</p>
<p>These skills are expected to have substantially varying difficulties in the study population—for instance, karate and magic are both expected to be difficult while tying shoes is expected to be easy. The experimenters hoped to see if participants would rate themselves as being worse than average on difficult skills and better on easy skills, which would imply substantially skewed responses, both left and right skewed, as difficulty varied. Of course, it would not be surprising for there to be substantial differences in response style in these data, in location, dispersion, or both, much like what was observed for the judged probabilities in the previous example. A location difference indicates a systematic anchor point of “generalized perceived self-competency” for a given participant while a dispersion difference indicates how spread a given participant’s responses are around that. Box plots of the ratings are shown in <xref ref-type="fig" rid="fig2-1076998610396895">Figure 2</xref>
.</p>
<fig id="fig2-1076998610396895" position="float">
<label>Figure 2.</label>
<caption>
<p>Roy and Liersch (2009) confidence ratings for 35 female participants.</p>
</caption>
<graphic xlink:href="10.3102_1076998610396895-fig2.tif"/>
</fig>
<p>We rescaled the response to lie inside the unit interval by letting
<disp-formula id="disp-formula39-1076998610396895">
<mml:math id="mml-disp39-1076998610396895">
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>.0025</mml:mn>
<mml:mo stretchy="false">+</mml:mo>
<mml:mn>.9975</mml:mn>
<mml:mo stretchy="false">∗</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">raw</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mn>100</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula39-1076998610396895" xlink:href="10.3102_1076998610396895-eq39.tif"/>
</disp-formula>As can be seen, the distributions are substantially skewed (particularly for the harder skills) and are overdispersed relative to the beta conditional on skill alone. There are some notable outliers, particularly for the difficult skills such as karate and magic, about which more below. The full specification will be given; simpler versions are devised by restricting one or more parameters to 0. Let <inline-formula id="inline-formula222-1076998610396895">
<mml:math id="mml-inline222-1076998610396895">
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>55</mml:mn>
</mml:math>
</inline-formula> index subjects and <inline-formula id="inline-formula223-1076998610396895">
<mml:math id="mml-inline223-1076998610396895">
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>10</mml:mn>
</mml:math>
</inline-formula> index skills. We assume that observed response <inline-formula id="inline-formula224-1076998610396895">
<mml:math id="mml-inline224-1076998610396895">
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> is beta-distributed conditional on skill and random effects; that is,
<disp-formula id="disp-formula40-1076998610396895"><label>33</label>
<mml:math id="mml-disp40-1076998610396895">
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">skill</mml:mi>
</mml:mrow>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Beta</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula40-1076998610396895" xlink:href="10.3102_1076998610396895-eq40.tif"/>
</disp-formula>where
<disp-formula id="disp-formula41-1076998610396895"><label>34</label>
<mml:math id="mml-disp41-1076998610396895">
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">∗</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">skill</mml:mi>
</mml:mrow>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula41-1076998610396895" xlink:href="10.3102_1076998610396895-eq41.tif"/>
</disp-formula>

<disp-formula id="disp-formula42-1076998610396895"><label>35</label>
<mml:math id="mml-disp42-1076998610396895">
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>φ</mml:mtext>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">∗</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="normal">skill</mml:mi>
</mml:mrow>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula42-1076998610396895" xlink:href="10.3102_1076998610396895-eq42.tif"/>
</disp-formula>The random effects are multivariate normal; that is,
<disp-formula id="disp-formula43-1076998610396895"><label>36</label>
<mml:math id="mml-disp43-1076998610396895">
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mtable columnspacing="1em" rowspacing="4pt">
<mml:mtr>
<mml:mtd>
<mml:mi>b</mml:mi>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mi>d</mml:mi>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">~</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">MVN</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="bold">0</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">Σ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula43-1076998610396895" xlink:href="10.3102_1076998610396895-eq43.tif"/>
</disp-formula>For the unrestricted model, <inline-formula id="inline-formula225-1076998610396895">
<mml:math id="mml-inline225-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">Σ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> is unstructured. Imposing <inline-formula id="inline-formula226-1076998610396895">
<mml:math id="mml-inline226-1076998610396895">
<mml:mrow>
<mml:mi mathvariant="normal">cov</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> would force the random effects to be independent and of course setting a variance term to 0 in effect drops the term from the model. We use the logit-exponential links and in this case no column of 1s is used for the design matrices. In sum, the models fit here posit separate beta distributions for each skill, possibly mixed in both location and dispersion.</p>
<p>We consider four different models for these data. The first model fits only fixed location and dispersion effects for skills with no random effects. The second model considers a location random effect only. The third fits a dispersion random effect only. The fourth considers random effects for both location and dispersion, allowing these to be correlated. The parameter estimates shown here are from the MML estimation but MCMC estimates are very similar. As there are many model parameters, we consider only the random effects and fit statistics for the moment. Examination of the reproduced means for Model 4 shows that it almost exactly recovers the raw data means for each skill (as indeed do the others). However, as can be seen from the BIC statistics, Models 3 and 4 are quite soundly preferred to the other two, again suggesting substantial differences among subjects in scale usage. However, Model 4 has some modest support over Model 3 (and indeed would have more if <inline-formula id="inline-formula227-1076998610396895">
<mml:math id="mml-inline227-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ρ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>b</mml:mi>
<mml:mi>d</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> were constrained to 0), which suggests some location anchoring on the part of subjects. There are 20 fixed effects parameters, which we do not show to conserve space. Model 4 has generally more extreme parameter estimates, as is common in random effects models.</p>
<p>All is not well with Model 4, however. To evaluate it in more detail, we computed the Pearson correlation between <inline-formula id="inline-formula228-1076998610396895">
<mml:math id="mml-inline228-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula229-1076998610396895">
<mml:math id="mml-inline229-1076998610396895">
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> by skill. These correlations range between 0.4 and 0.7 for most skills. However, two skills, karate and music, have correlations that are effectively 0. As is evident in the boxplot, the confidence ratings for karate are extraordinarily dispersed. Many participants rate their confidence at 0 and the distribution is L-shaped. However, a few participants rate their karate ability as being very high (perhaps accurately?). Music performance is not as dispersed but behaves similarly. We speculate that this effect might be due to the fact that both skills are ones in which some participants have much more objective information about their ability compared to others, such as dancing or driving. Switching to a different random effects distribution might accommodate this overdispersion better (<xref ref-type="table" rid="table5-1076998610396895">Table 6</xref>
).</p>
<table-wrap id="table5-1076998610396895" position="float">
<label>Table 5.</label>
<caption>
<p>Posterior Mean -2log <italic>L</italic> for Three Models</p>
</caption>
<graphic alternate-form-of="table5-1076998610396895" xlink:href="10.3102_1076998610396895-table5.tif"/>
<table>
<thead>
<tr>
<th align="left">
</th>
<th align="left">
-2log <italic>L</italic>
</th>
<th align="left">Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Model 2</td>
<td align="left">−5193.08</td>
<td align="left">
</td>
</tr>
<tr>
<td align="left">Model 3</td>
<td align="left">−5337.02</td>
<td align="left">143.94</td>
</tr>
<tr>
<td align="left">Model 4</td>
<td align="left">−5474.13</td>
<td align="left">137.11</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table6-1076998610396895" position="float">
<label>Table 6.</label>
<caption>
<p>Random Effects Parameter Estimates and Fit Statistics for the Confidence Rating Data</p>
</caption>
<graphic alternate-form-of="table5-1076998610396895" xlink:href="10.3102_1076998610396895-table6.tif"/>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Model 1</th>
<th>Model 2</th>
<th>Model 3</th>
<th>Model 4</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<inline-formula id="inline-formula230-1076998610396895">
<mml:math id="mml-inline230-1076998610396895">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>b</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>—</td>
<td>0.37</td>
<td>—</td>
<td>0.34</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula231-1076998610396895">
<mml:math id="mml-inline231-1076998610396895">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>b</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>—</td>
<td>—</td>
<td>0.68</td>
<td>0.70</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula232-1076998610396895">
<mml:math id="mml-inline232-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">ρ</mml:mi>
<mml:mo accent="true" mathvariant="normal" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>b</mml:mi>
<mml:mi>d</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>0.16</td>
</tr>
<tr>
<td>
<inline-formula id="inline-formula233-1076998610396895">
<mml:math id="mml-inline233-1076998610396895">
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
<mml:mtext mathvariant="normal"> </mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="script">L</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>−193.6</td>
<td>−205.2</td>
<td>−256.0</td>
<td>−266.2</td>
</tr>
<tr>
<td>
<italic>df</italic>
</td>
<td>20</td>
<td>21</td>
<td>21</td>
<td>23</td>
</tr>
<tr>
<td>BIC</td>
<td>−122.5</td>
<td>−130.5</td>
<td>−181.3</td>
<td>−184.5</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1076998610396895">
<p>
<italic>Note</italic>: BIC = Bayesian Information Criterion.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>To assess the effect of these outliers on the random effects estimates, we refit Model 4 omitting karate and music. In this case, the fit statistics are no longer comparable, but parameter estimates should be similar. The fixed effects are very similar but given the specification of the model this is to be expected. Examining <inline-formula id="inline-formula234-1076998610396895">
<mml:math id="mml-inline234-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>b</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.45</mml:mn>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula235-1076998610396895">
<mml:math id="mml-inline235-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>d</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.81</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula236-1076998610396895">
<mml:math id="mml-inline236-1076998610396895">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="italic">ρ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>b</mml:mi>
<mml:mi>d</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0.22</mml:mn>
</mml:math>
</inline-formula>. This is comforting because the random effects parameters would be more likely to be distorted by the outliers in other parts of the model, but they are not. By contrast, the logit-normal model parallel of Model 4 exhibits very poor convergence, even when started from the same point as the beta model.</p>
</sec>
</sec>
<sec id="section15-1076998610396895">
<title>5. Discussion and Conclusion</title>
<p>The principal focus of this article has been a GLMM framework for dependent random variables with conditional beta distributions. In particular, we have shown that for subjective rating data, a random effects dispersion model is highly useful in modeling individual heterogeneity in response style. Response style is a pervasive problem with self-report data such as numerical confidence ratings, judged probabilities, sliders, and so on, all of which are used quite commonly by psychologists and other behavioral scientists. While in many cases it is simply a nuisance behavior, in other cases, response style heterogeneity might be an important study outcome in its own right, for instance, when group differences are expected as in attitude polarization (<xref ref-type="bibr" rid="bibr14-1076998610396895">Dolnicar &amp; Grun, 2007</xref>). We have seen in two examples how partition-priming effects on probability judgments are more accurately described by response-style heterogeneity than by a shift in the mean response.</p>
<p>This framework may be extended and developed in several ways. We briefly discuss the following starting points for such developments: Alternative estimation techniques, model diagnostics, special mixture models, alternative link functions, and alternative distributions.</p>
<p>Beginning with estimation methods, at least three major approaches remain to be explored. This article dealt with MML and Bayesian MCMC techniques but omitted restricted maximum likelihood (REML), penalized quasi-likelihood (PQL), and generalized least squares (GLS) approaches. These techniques were neglected primarily for lack of software tools with which to investigate them. As mentioned earlier, there is a limited implementation of PQL estimation in the SAS GLIMMIX procedure. Investigations of REML and GLS await software developments.</p>
<p>We now turn to extensions of the GLMM itself. Some applications require “<inline-formula id="inline-formula237-1076998610396895">
<mml:math id="mml-inline237-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>-inflated” models, where <inline-formula id="inline-formula238-1076998610396895">
<mml:math id="mml-inline238-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> is a specific value on the unit interval. The most well-known situation is where <inline-formula id="inline-formula239-1076998610396895">
<mml:math id="mml-inline239-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> (see <xref ref-type="bibr" rid="bibr26-1076998610396895">Kieschnick &amp; McCullogh, 2003</xref>), and a natural model here is a mixture of a beta random variable and one whose PDF concentrates its entire mass at 0. However, situations can arise such as the judged probability example (<xref ref-type="bibr" rid="bibr47-1076998610396895">Smithson &amp; Segale, 2009</xref>) in which values near <inline-formula id="inline-formula240-1076998610396895">
<mml:math id="mml-inline240-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> should be included in the “inflation” component distribution. This component distribution therefore can no longer concentrate all of its mass at <inline-formula id="inline-formula241-1076998610396895">
<mml:math id="mml-inline241-1076998610396895">
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>, although it will have a small variance. Mixture models with a component distribution of this kind are notoriously difficult to estimate, and alternative estimation methods have yet to be fully investigated. One promising avenue is a Bayesian approach utilizing an informative prior for the inflation component distribution. <xref ref-type="bibr" rid="bibr46-1076998610396895">Smithson et al. (2009)</xref> and <xref ref-type="bibr" rid="bibr32-1076998610396895">Merkle, Verkuilen, and Smithson (2009)</xref> investigate and demonstrate the utility of these models in analyzing such phenomena as scale response style, “nice number” bias, and nonadditivity in probability judgments.</p>
<p>More general extensions of the GLMM may be obtained by considering alternative link functions and distributions. As indicated earlier, the logit is not the only twice-differentiable link function that transforms the unit interval to the real line. Little is known about how these alternative link functions perform in comparison with the logit.</p>
<p>Likewise, several alternatives to the beta distribution have been proposed. <xref ref-type="bibr" rid="bibr27-1076998610396895">Kumaraswamy (1980)</xref> provided a two-parameter (<inline-formula id="inline-formula242-1076998610396895">
<mml:math id="mml-inline242-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo mathvariant="normal" stretchy="false">,</mml:mo>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>) distribution closely related to the beta. A Kumaraswamy-distributed random variable is the <inline-formula id="inline-formula243-1076998610396895">
<mml:math id="mml-inline243-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">αth</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> root of a Beta(1, <inline-formula id="inline-formula244-1076998610396895">
<mml:math id="mml-inline244-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>) random variable. This distribution’s main advantage over the beta distribution is the closed form of its density and cumulative density functions, but the expressions for its moments do not have closed forms whereas those for the beta distribution do.</p>
<p>A more popular alternative is Johnson’s <inline-formula id="inline-formula245-1076998610396895">
<mml:math id="mml-inline245-1076998610396895">
<mml:msub>
<mml:mi>S</mml:mi>
<mml:mi>B</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> distribution, which begins with a linear scaling to the (0,1) interval, followed by a logit transformation and finally a linear scaling to the standard normal distribution. The logit-logistic (LL) distribution (<xref ref-type="bibr" rid="bibr52-1076998610396895">Tadikamalla &amp; Johnson, 1982</xref>) replaces the normal with the standard logistic distribution. The chief advantage of the LL over the <inline-formula id="inline-formula246-1076998610396895">
<mml:math id="mml-inline246-1076998610396895">
<mml:msub>
<mml:mi>S</mml:mi>
<mml:mi>B</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> distribution is that its CDF has a simple closed invertible form. The shape domain of the LL distribution includes that of the beta distribution. However, its moments do not have closed-form expressions. A third alternative is the simplex distribution (<xref ref-type="bibr" rid="bibr4-1076998610396895">Barndorff-Nielsen &amp; Jorgensen, 1991</xref>), which is constructed from the inverse Gaussian distribution in a manner similar to the way the beta is contsructed from the gamma distribution. Like the beta parameterization used in GLMs, the simplex distribution is parameterized in terms of a mean and standard deviation. Unlike the beta model, the simplex distribution is naturally a deviance-based model. <xref ref-type="bibr" rid="bibr26-1076998610396895">Kieschnick and McCullogh (2003)</xref> compare it with their beta regression model on two data sets. Qiu, Song, and Tan (2007) develop a simplex distribution mixed model.</p>
<p>Fourth, the logit-normal and logit-logistic distributions are members of the otherwise unexplored symmetric family that utilizes an invertible transformation <inline-formula id="inline-formula247-1076998610396895">
<mml:math id="mml-inline247-1076998610396895">
<mml:mi>H</mml:mi>
<mml:mo stretchy="false">:</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="normal">∞</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi mathvariant="normal">∞</mml:mi>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo stretchy="false">→</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula> such that <inline-formula id="inline-formula248-1076998610396895">
<mml:math id="mml-inline248-1076998610396895">
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>H</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, and a standardized PDF, <inline-formula id="inline-formula249-1076998610396895">
<mml:math id="mml-inline249-1076998610396895">
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, whose support is the real line. The resultant PDF of <inline-formula id="inline-formula250-1076998610396895">
<mml:math id="mml-inline250-1076998610396895">
<mml:mi>X</mml:mi>
</mml:math>
</inline-formula> is
<disp-formula id="disp-formula44-1076998610396895">
<mml:math id="mml-disp44-1076998610396895">
<mml:mi>f</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msup>
<mml:mi>H</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:msup>
<mml:mi>H</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula44-1076998610396895" xlink:href="10.3102_1076998610396895-eq44.tif"/>
</disp-formula>The <inline-formula id="inline-formula251-1076998610396895">
<mml:math id="mml-inline251-1076998610396895">
<mml:mi>H</mml:mi>
</mml:math>
</inline-formula> function can be any CDF whose quantile function has a closed form expression. For some of these distributions, as with the logit-logistic, the cumulative density and quantile functions can be explicitly written, and they have natural location and dispersion submodels. However, very little is known about this subfamily.</p>
<p>Finally, <xref ref-type="bibr" rid="bibr12-1076998610396895">Cox (1996)</xref> and <xref ref-type="bibr" rid="bibr38-1076998610396895">Papke and Wooldridge (1996)</xref> apply a quasi-likelihood approach to modeling a variable on the unit interval. The quasi-likelihood approach specifies the first and second moments of the conditional distribution as functions of the mean but does not specify the full distribution; in this sense it is a second-order analog of maximum likelihood. This approach is useful when the relationship between the mean and variance is not captured by a standard distribution. <xref ref-type="bibr" rid="bibr26-1076998610396895">Kieschnick and McCullogh (2003)</xref> compare a special case of Papke and Wooldridge’s model with their beta regression model on two data sets and find that both work reasonably well and give similar results. A strategy using a semiparametric model based on the Dirichlet process might also be a useful strategy to consider (<xref ref-type="bibr" rid="bibr29-1076998610396895">McAuliffe, Blei, &amp; Jordan, 2006</xref>).</p>
<p>Summing up, this article has extended beta regression to deal with dependent observations in a general way, along the lines of the multilevel modeling literature. The resulting GLMM has been shown to be effective in a simulation study and three real-world applications and, as indicated earlier, it has applicability in a wide range of disciplines where doubly bounded constructs are commonplace. We also have demonstrated the practicality of MLL and MCMC estimation of this GLMM under a variety of conditions. Code for the examples in this article and additional resources are available at <ext-link ext-link-type="uri" xlink:href="http://dl.dropbox.com/u/1857674/betareg/betareg.html">http://dl.dropbox.com/u/1857674/betareg/betareg.html</ext-link>. As should be apparent from this section, there is considerable potential for extensions and developments in modeling doubly bounded dependent variables, but the state of the art there is rapidly approaching a richness and sophistication comparable to the models available for other kinds of variables.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-1076998610396895">
<title>Appendix</title>
<sec id="section16-1076998610396895">
<title>Score and Information Matrix</title>
<p>To simplify the presentation of the score and Hessian, we only consider the log-density of one observation. These terms are all sums and can be pieced together afterward as needed. Let
<disp-formula id="disp-formula45-1076998610396895"><label>A1</label>
<mml:math id="mml-disp45-1076998610396895">
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula45-1076998610396895" xlink:href="10.3102_1076998610396895-eq45.tif"/>
</disp-formula>that is, the digamma function. The mean-precision parameterized beta density is
<disp-formula id="disp-formula46-1076998610396895"><label>A2</label>
<mml:math id="mml-disp46-1076998610396895">
<mml:mi>f</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:msup>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula46-1076998610396895" xlink:href="10.3102_1076998610396895-eq46.tif"/>
</disp-formula>Taking the natural logarithm, expanding fully and rearranging the terms slightly gives
<disp-formula id="disp-formula47-1076998610396895"><label>A3</label>
<mml:math id="mml-disp47-1076998610396895">
<mml:mtable columnalign="right left" columnspacing="thickmathspace" displaystyle="true" rowspacing=".5em">
<mml:mtr>
<mml:mtd>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mtext> </mml:mtext>
<mml:mi>f</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd/>
<mml:mtd>
<mml:mrow>
<mml:mspace width="1em"/>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mspace width="thinmathspace"/>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mspace width="thinmathspace"/>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>.</mml:mo>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
<graphic alternate-form-of="disp-formula47-1076998610396895" xlink:href="10.3102_1076998610396895-eq47.tif"/>
</disp-formula>Components of the score function are given by the vector of first partials and the Hessian by the matrix of second partials. These are
<disp-formula id="disp-formula48-1076998610396895"><label>A4</label>
<mml:math id="mml-disp48-1076998610396895">
<mml:mtable columnalign="right left" columnspacing="thickmathspace" displaystyle="true" rowspacing=".5em">
<mml:mtr>
<mml:mtd/>
<mml:mtd>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mi>f</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mtext> </mml:mtext>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">arctanh</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">;</mml:mo>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd/>
<mml:mtd>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mi>f</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mtext>φ</mml:mtext>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mspace width="thinmathspace"/>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">arctanh</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
<graphic alternate-form-of="disp-formula48-1076998610396895" xlink:href="10.3102_1076998610396895-eq48.tif"/>
</disp-formula>and
<disp-formula id="disp-formula49-1076998610396895"><label>A5</label>
<mml:math id="mml-disp49-1076998610396895">
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msup>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mi>f</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:msup>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:msup>
<mml:mtext>φ</mml:mtext>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:msup>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:msup>
<mml:mi/>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">+</mml:mo>
<mml:msup>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:msup>
<mml:mi/>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">;</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula49-1076998610396895" xlink:href="10.3102_1076998610396895-eq49.tif"/>
</disp-formula>

<disp-formula id="disp-formula50-1076998610396895">
<mml:math id="mml-disp50-1076998610396895">
<mml:mtable columnalign="right left" columnspacing="thickmathspace" displaystyle="true" rowspacing=".5em">
<mml:mtr>
<mml:mtd>
<mml:mfrac>
<mml:mrow>
<mml:mrow>
<mml:msup>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mrow>
<mml:mi mathvariant="normal">log</mml:mi>
</mml:mrow>
<mml:mi>f</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∂</mml:mi>
<mml:msup>
<mml:mtext>φ</mml:mtext>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:mfrac>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo stretchy="false">=</mml:mo>
<mml:msup>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:msup>
<mml:mi/>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:msup>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:msup>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:msup>
<mml:mi/>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
<mml:msup>
<mml:mi mathvariant="italic">ψ</mml:mi>
<mml:msup>
<mml:mi/>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">μ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">;</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula50-1076998610396895" xlink:href="10.3102_1076998610396895-eq50.tif"/>
</disp-formula>

<disp-formula id="disp-formula51-1076998610396895">
<mml:math id="mml-disp51-1076998610396895">
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msup>
<mml:mo>∂</mml:mo>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>f</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∂</mml:mo><mml:mi>μ</mml:mi><mml:mtext>φ</mml:mtext>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mtext>arctanh</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>φ</mml:mtext><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Ψ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>φ</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mtext>φ</mml:mtext><mml:mi>μ</mml:mi><mml:msup>
<mml:mi>Ψ</mml:mi>
<mml:mo>'</mml:mo>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo><mml:mtext>φ</mml:mtext><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mtext>φ</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msup>
<mml:mi>Ψ</mml:mi>
<mml:mo>'</mml:mo>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo><mml:mtext>φ</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">),</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula51-1076998610396895" xlink:href="10.3102_1076998610396895-eq51.tif"/>
</disp-formula>respectively. Depending on the algorithm and software used, these quantities may or may not be necessary. Modern software such as SAS uses symbolic differentiation to compute the needed derivatives and only require the likelihood. Other algorithms use numerical approximations to the derivatives. However, they are of theoretical importance. In particular, the fact that <inline-formula id="inline-formula252-1076998610396895">
<mml:math id="mml-inline252-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula253-1076998610396895">
<mml:math id="mml-inline253-1076998610396895">
<mml:mtext>φ</mml:mtext>
</mml:math>
</inline-formula> appear in the cross-partial of the Hessian shows that the location and scale parameters are not separable, unlike in models such as the Gaussian.</p>
<p>To consider a regression model for link pair <inline-formula id="inline-formula254-1076998610396895">
<mml:math id="mml-inline254-1076998610396895">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">μ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">X</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">β</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula255-1076998610396895">
<mml:math id="mml-inline255-1076998610396895">
<mml:mtext>φ</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">W</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">W</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, it is necessary to use the chain rule, considering the various partial derivatives. See <xref ref-type="bibr" rid="bibr36-1076998610396895">Ospina et al. (2006)</xref>.</p>
</sec>
</app>
</app-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-1076998610396895">
<label>1.</label>
<p>The <italic>four-parameter beta distribution</italic> allows for the boundaries to be different than (0, 1) by including two additional parameters. Assuming these boundaries are known a priori the beta distribution on interval (l, <italic>u</italic>) is a linear transformation of one on (0, 1) and thus, without loss of generality, the boundaries can be ignored. If the boundaries are unknown, estimation becomes highly irregular. We believe it is rarely the case that a doubly bounded distribution such as the beta would be a plausible model when reasonable bounds are unknown a priori.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Aitchison</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2003</year>). <source>The statistical analysis of compositional data</source>
<publisher-loc>London, England</publisher-loc>: <publisher-name>The Blackburn Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Alicke</surname>
<given-names>M. D.</given-names>
</name>
<name>
<surname>Govorun</surname>
<given-names>O.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>The better-than-average effect</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Alicke</surname>
<given-names>M. D.</given-names>
</name>
<name>
<surname>Dunning</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Krueger</surname>
<given-names>J. I.</given-names>
</name>
</person-group> (Eds.), <source>The self in social judgment</source>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Psychology Press, 85–108</publisher-name>.</citation>
</ref>
<ref id="bibr3-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Atkinson</surname>
<given-names>A. C.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Fast very robust methods for the detection of multiple outliers</article-title>. <source>Journal of the American Statistical Association</source>, <volume>89</volume>, <fpage>1329</fpage>–<lpage>1339</lpage>.</citation>
</ref>
<ref id="bibr4-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Barndorff-Nielsen</surname>
<given-names>O. E.</given-names>
</name>
<name>
<surname>Jorgensen</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1991</year>). <article-title>Some parametric models on the simplex</article-title>. <source>Journal of Multivariate Analysis</source>, <volume>39</volume>, <fpage>106</fpage>–<lpage>116</lpage>.</citation>
</ref>
<ref id="bibr5-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brehm</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Gates</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>1993</year>). <article-title>Donut shops and speed traps: Evaluating models of supervision on police behavior</article-title>. <source>American Journal of Political Science</source>, <volume>37</volume>, <fpage>555</fpage>–<lpage>581</lpage>.</citation>
</ref>
<ref id="bibr6-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Browne</surname>
<given-names>W. J.</given-names>
</name>
<name>
<surname>Draper</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>A comparison of Bayesian and likelihood-based methods for fitting multilevel models</article-title>. <source>Bayesian Analysis</source>, <volume>1</volume>, <fpage>473</fpage>–<lpage>514</lpage>.</citation>
</ref>
<ref id="bibr7-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Browne</surname>
<given-names>W. J.</given-names>
</name>
<name>
<surname>Subramanian</surname>
<given-names>S. V.</given-names>
</name>
<name>
<surname>Jones</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Goldstein</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Variance partitioning in multilevel logistic models that exhibit over-dispersion</article-title>. <source>Journal of the Royal Statistical Society, Series A</source>, <volume>168</volume>, <fpage>599</fpage>–<lpage>613</lpage>.</citation>
</ref>
<ref id="bibr8-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Buckley</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Estimation of models with beta-distributed dependent variables: A replication and extension of Paolino (2001)</article-title>. <source>Political Analysis</source>, <volume>11</volume>, <fpage>1</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr9-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Carroll</surname>
<given-names>R. J.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Variances are not always nuisance parameters</article-title>. <source>Biometrics</source>, <volume>59</volume>, <fpage>211</fpage>–<lpage>220</lpage>.</citation>
</ref>
<ref id="bibr10-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>M.-H.</given-names>
</name>
<name>
<surname>Ibrahim</surname>
<given-names>J. G.</given-names>
</name>
<name>
<surname>Shao</surname>
<given-names>Q.-M.</given-names>
</name>
<name>
<surname>Weiss</surname>
<given-names>R. E.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Prior elicitation for model selection and estimation in generalized linear mixed models</article-title>. <source>Journal of Statistical Planning and Inference</source>, <volume>111</volume>, <fpage>57</fpage>–<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr11-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Congdon</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2003</year>). <source>Applied Bayesian modelling</source>
<publisher-loc>Chichester, England</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr12-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cox</surname>
<given-names>C.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Nonlinear quasi-likelihood models: Applications to continuous proportions</article-title>. <source>Computational Statistics and Data Analysis</source>, <volume>21</volume>, <fpage>449</fpage>–<lpage>461</lpage>.</citation>
</ref>
<ref id="bibr13-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crowder</surname>
<given-names>M. J.</given-names>
</name>
</person-group> (<year>1978</year>). <article-title>Beta-binomial ANOVA for proportions</article-title>. <source>Applied Statistics</source>, <volume>27</volume>, <fpage>34</fpage>–<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr14-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dolnicar</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Grun</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Cross-cultural differences in survey response patterns</article-title>. <source>International Marketing Review</source>, <volume>24</volume>, <fpage>127</fpage>–<lpage>143</lpage>.</citation>
</ref>
<ref id="bibr15-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Epsinheira</surname>
<given-names>P. L.</given-names>
</name>
<name>
<surname>Ferrari</surname>
<given-names>S. L. P.</given-names>
</name>
<name>
<surname>Cribari-Neto</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>2008a</year>). <article-title>On beta regression residuals</article-title>. <source>Journal of Applied Statistics</source>, <volume>35</volume>, <fpage>407</fpage>–<lpage>419</lpage>.</citation>
</ref>
<ref id="bibr16-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Epsinheira</surname>
<given-names>P. L.</given-names>
</name>
<name>
<surname>Ferrari</surname>
<given-names>S. L. P.</given-names>
</name>
<name>
<surname>Cribari-Neto</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>2008b</year>). <article-title>Influence diagnostics in beta regression</article-title>. <source>Computational Statistics and Data Analysis</source>, <volume>52</volume>, <fpage>4417</fpage>–<lpage>4431</lpage>.</citation>
</ref>
<ref id="bibr17-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fahrmeir</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Tutz</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2001</year>). <source>Multivariate statistical modelling based on generalized linear models</source>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr18-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ferrari</surname>
<given-names>S. L. P.</given-names>
</name>
<name>
<surname>Cribari-Neto</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Beta regression for modeling rates and proportions</article-title>. <source>Journal of Applied Statistics</source>, <volume>31</volume>, <fpage>799</fpage>–<lpage>815</lpage>.</citation>
</ref>
<ref id="bibr19-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fox</surname>
<given-names>C. R.</given-names>
</name>
<name>
<surname>Rottenstreich</surname>
<given-names>Y.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Partition priming in judgment under uncertainty</article-title>. <source>Psychological Science</source>, <volume>14</volume>, <fpage>195</fpage>–<lpage>200</lpage>.</citation>
</ref>
<ref id="bibr20-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gelman</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Prior distributions for variance parameters in hierarchical models</article-title>. <source>Bayesian Analysis</source>, <volume>1</volume>, <fpage>516</fpage>–<lpage>534</lpage>.</citation>
</ref>
<ref id="bibr21-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gelman</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Rubin</surname>
<given-names>D. B.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>Inference from iterative simulation using multiple sequences (with discussion)</article-title>. <source>Statistical Science</source>, <volume>7</volume>, <fpage>457</fpage>–<lpage>511</lpage>.</citation>
</ref>
<ref id="bibr22-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Goldstein</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Browne</surname>
<given-names>W. J.</given-names>
</name>
<name>
<surname>Rasbash</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Partitioning variation in multilevel models</article-title>. <source>Understanding Statistics</source>, <volume>1</volume>, <fpage>223</fpage>–<lpage>231</lpage>.</citation>
</ref>
<ref id="bibr23-1076998610396895">
<citation citation-type="book"><person-group person-group-type="editor">
<name>
<surname>Gupta</surname>
<given-names>A. K.</given-names>
</name>
<name>
<surname>Nadarajah</surname>
<given-names>S.</given-names>
</name>
</person-group> (Eds.). (<year>2004</year>). <source>Handbook of beta distribution and its applications</source>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Marcel Dekker</publisher-name>.</citation>
</ref>
<ref id="bibr24-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author"><name><surname>Gurr</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <source>Partition dependence: Investigating the principle of insufficient reason, uncertainty and dispositional predictors</source>. <comment>Unpublished Honours thesis</comment>, <publisher-name>The Australian National University</publisher-name>, <publisher-loc>Canberra, Australia</publisher-loc>.
</citation>
</ref>
<ref id="bibr25-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Johnson</surname>
<given-names>T. R.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>On the use of heterogeneous thresholds ordinal regression models to account for individual differences in response style</article-title>. <source>Psychometrika</source>, <volume>68</volume>, <fpage>563</fpage>–<lpage>583</lpage>.</citation>
</ref>
<ref id="bibr26-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kieschnick</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>McCullogh</surname>
<given-names>B. D.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Regression analysis of variates observed on (0,1): Percentages, proportions, and fractions</article-title>. <source>Statistical Modelling</source>, <volume>3</volume>, <fpage>193</fpage>–<lpage>213</lpage>.</citation>
</ref>
<ref id="bibr27-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kumaraswamy</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>1980</year>). <article-title>A generalized probability density function for double-bounded random processes</article-title>. <source>Journal of Hydrology</source>, <volume>46</volume>, <fpage>79</fpage>–<lpage>88</lpage>.</citation>
</ref>
<ref id="bibr28-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Longford</surname>
<given-names>N. T.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Simulation-based diagnostics in random-coefficient models</article-title>. <source>Journal of the Royal Statistical Society, Series A</source>, <volume>164</volume>, <fpage>259</fpage>–<lpage>273</lpage>.</citation>
</ref>
<ref id="bibr29-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>McAuliffe</surname>
<given-names>J. D.</given-names>
</name>
<name>
<surname>Blei</surname>
<given-names>D. M.</given-names>
</name>
<name>
<surname>Jordan</surname>
<given-names>M. I.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Nonparametric empirical Bayes for the Dirichlet process mixture model</article-title>. <source>Statistics and Computing</source>, <volume>16</volume>, <fpage>5</fpage>–<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr30-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>McCullagh</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Nelder</surname>
<given-names>J. A.</given-names>
</name>
</person-group> (<year>1989</year>). <source>Generalized linear models</source>
<edition>2nd ed.</edition>
<publisher-loc>London, England</publisher-loc>: <publisher-name>Chapman and Hall</publisher-name>.</citation>
</ref>
<ref id="bibr31-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>McCulloch</surname>
<given-names>C. E.</given-names>
</name>
<name>
<surname>Searle</surname>
<given-names>S. R.</given-names>
</name>
<name>
<surname>Neuhaus</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Generalized, linear, and mixed models</source>
<edition>2nd ed.</edition>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr32-1076998610396895">
<citation citation-type="confproc">
<person-group person-group-type="author"><name><surname>Merkle</surname><given-names>E.</given-names></name><name><surname>Smithson</surname><given-names>M.</given-names></name><name><surname>Verkuilen</surname><given-names>J.</given-names></name></person-group> (<year>2009</year>). <source>Hierarchical Bayesian beta models for subjective probabilities</source>. <conf-name>Paper presented at the Joint Annual Convention for the Society of Mathematical Psychology and the European Mathematical Psychology Group</conf-name>, <conf-loc>Amsterdam</conf-loc>, <comment>August 1–4</comment>.
</citation>
</ref>
<ref id="bibr33-1076998610396895">
<citation citation-type="confproc">
<person-group person-group-type="author"><name><surname>Noël</surname><given-names>Y</given-names></name></person-group>. (<year>2008</year> <month>August</month>). <source>When extreme responses are substantial: A generalized beta response model of behavior change</source>. <conf-name>Paper presented at the Joint Annual Convention for the Society of Mathematical Psychology and the European Mathematical Psychology Group</conf-name>, <conf-loc>Amsterdam, the Netherlands</conf-loc>.
</citation>
</ref>
<ref id="bibr34-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Noël</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Dauvier</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A beta item response model for continuous bounded responses</article-title>. <source>Applied Psychological Measurement</source>, <volume>31</volume>, <fpage>47</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr35-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ntzoufras</surname>
<given-names>I.</given-names>
</name>
</person-group> (<year>2009</year>). <source>Bayesian modeling using WinBUGS</source>
<publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr36-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ospina</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Cribari-Neto</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Vasconcellos</surname>
<given-names>K. L. P.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Improved point and interval estimates for a beta regression model</article-title>. <source>Computational Statistics and Data Analysis</source>, <volume>51</volume>, <fpage>960</fpage>–<lpage>981</lpage>.</citation>
</ref>
<ref id="bibr37-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Paolino</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Maximum likelihood estimation of models with beta-distributed dependent variables</article-title>. <source>Political Analysis</source>, <volume>9</volume>, <fpage>325</fpage>–<lpage>346</lpage>.</citation>
</ref>
<ref id="bibr38-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Papke</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Wooldridge</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Econometric methods for fractional response variables with an application to 401(K) plan participation rates</article-title>. <source>Journal of Applied Econometrics</source>, <volume>11</volume>, <fpage>619</fpage>–<lpage>632</lpage>.</citation>
</ref>
<ref id="bibr39-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author"><name><surname>Pawitan</surname><given-names>Y.</given-names></name></person-group> (<year>2001</year>). <source>In all likelihood: Statistical modeling and inference using likelihood</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Clarendon Press</publisher-name>.
</citation>
</ref>
<ref id="bibr40-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pinheiro</surname>
<given-names>J. C.</given-names>
</name>
<name>
<surname>Chao</surname>
<given-names>E. C.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Efficient Laplacian and adaptive Gaussian quadrature algorithms for multilevel generalized linear models</article-title>. <source>Journal of Computational and Graphical Statistics</source>, <volume>15</volume>, <fpage>58</fpage>–<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr41-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Qiu</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Song</surname>
<given-names>P. X. K.</given-names>
</name>
<name>
<surname>Tan</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Simplex mixed-effects models for longitudinal proportional data</article-title>. <source>Scandinavian Journal of Statistics</source>, <volume>35</volume>, <fpage>577</fpage>–<lpage>596</lpage>.</citation>
</ref>
<ref id="bibr42-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author"><name><surname>Roy</surname><given-names>M.</given-names></name><name><surname>Liersch</surname><given-names>M.</given-names></name></person-group> (<year>2009</year>). <source>Most people are above average, sometimes</source>. <comment>Unpublished manuscript</comment>, <publisher-name>Elizabethtown College</publisher-name>, <publisher-loc>Elizabethtown, PA</publisher-loc>.
</citation>
</ref>
<ref id="bibr43-1076998610396895">
<citation citation-type="book">
<collab collab-type="author">SAS 9.1.3</collab>. (<year>2006</year>). <publisher-loc>Cary, NC, U.S.A.</publisher-loc>: <publisher-name>SAS Institute, Inc</publisher-name>.
</citation>
</ref>
<ref id="bibr44-1076998610396895">
<citation citation-type="book">
<collab collab-type="author">SAS 9.2</collab> (<year>2008</year>). <publisher-loc>Cary, NC, U.S.A.</publisher-loc>: <publisher-name>SAS Institute, Inc</publisher-name>.
</citation>
</ref>
<ref id="bibr45-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>See</surname>
<given-names>K. E.</given-names>
</name>
<name>
<surname>Fox</surname>
<given-names>C. R.</given-names>
</name>
<name>
<surname>Rottenstreich</surname>
<given-names>Y. S.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Between ignorance and truth: Partition dependence and learning in judgment under uncertainty</article-title>. <source>Journal of Experimental Psychology: Learning, Memory and Cognition</source>, <volume>32</volume>, <fpage>1385</fpage>–<lpage>1402</lpage>.</citation>
</ref>
<ref id="bibr46-1076998610396895">
<citation citation-type="confproc">
<person-group person-group-type="author"><name><surname>Smithson</surname><given-names>M. J.</given-names></name><name><surname>Merkle</surname><given-names>E.</given-names></name><name><surname>Verkuilen</surname><given-names>J.</given-names></name></person-group> (<year>2009</year> <month>August</month>). <source>Beta regression finite mixture models of polarization and priming</source>. <conf-name>Paper presented at the Joint Annual Convention for the Society of Mathematical Psychology and the European Mathematical Psychology Group</conf-name>, <conf-loc>Amsterdam, the Netherlands</conf-loc>.
</citation>
</ref>
<ref id="bibr47-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smithson</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Segale</surname>
<given-names>C.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Partition priming in judgments of imprecise probabilities</article-title>. <source>Journal of Statistical Theory and Practice</source>, <volume>3</volume>, <fpage>169</fpage>–<lpage>182</lpage>.</citation>
</ref>
<ref id="bibr48-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smithson</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Verkuilen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>A better lemon squeezer? Maximum likelihood regression with beta-distributed dependent variables</article-title>. <source>Psychological Methods</source>, <volume>11</volume>, <fpage>54</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr49-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smyth</surname>
<given-names>G. K.</given-names>
</name>
</person-group> (<year>1989</year>). <article-title>Generalized linear models with varying dispersion</article-title>. <source>Journal of the Royal Statistical Society, Series B</source>, <volume>51</volume>, <fpage>47</fpage>–<lpage>60</lpage>.</citation>
</ref>
<ref id="bibr50-1076998610396895">
<citation citation-type="book">
<person-group person-group-type="author"><name><surname>Spiegelhalter</surname><given-names>D.</given-names></name><name><surname>Thomas</surname><given-names>A.</given-names></name><name><surname>Best</surname><given-names>N.</given-names></name><name><surname>Lunn</surname><given-names>D.</given-names></name></person-group> (<year>2004</year>). <article-title>WinBUGS Version 1.4.3. Computer software Cambridge</article-title>, <publisher-loc>England</publisher-loc>: <publisher-name>Medical Research Council, Biostatistics Unit</publisher-name>.
</citation>
</ref>
<ref id="bibr51-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Steiger</surname>
<given-names>J. H.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Driving fast in reverse: The relationship between software development, theory and education in structural equation modeling</article-title>. <source>Journal of the American Statistical Association</source>, <volume>96</volume>, <fpage>331</fpage>–<lpage>338</lpage>.</citation>
</ref>
<ref id="bibr52-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tadikamalla</surname>
<given-names>P. R.</given-names>
</name>
<name>
<surname>Johnson</surname>
<given-names>N. L.</given-names>
</name>
</person-group> (<year>1982</year>). <article-title>Systems of frequency curves generated by transformation of logistic variables</article-title>. <source>Biometrika</source>, <volume>69</volume>, <fpage>461</fpage>–<lpage>465</lpage>.</citation>
</ref>
<ref id="bibr53-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vasconcellos</surname>
<given-names>K. L. P.</given-names>
</name>
<name>
<surname>Cribari-Neto</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Improved maximum likelihood estimation in a new class of beta regression models</article-title>. <source>Brazilian Journal of Probability and Statistics</source>, <volume>19</volume>, <fpage>13</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr54-1076998610396895">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Weiss</surname>
<given-names>R. E.</given-names></name>
<name><surname>Cho</surname><given-names>M.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Bayesian marginal influence assessment</article-title>. <source>Journal of Statistical Planning and Inference</source>, <volume>71</volume>, <fpage>163</fpage>–<lpage>177</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>