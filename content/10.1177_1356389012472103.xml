<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="news" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EVI</journal-id>
<journal-id journal-id-type="hwp">spevi</journal-id>
<journal-title>Evaluation</journal-title>
<issn pub-type="ppub">1356-3890</issn>
<issn pub-type="epub">1461-7153</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1356389012472103</article-id>
<article-id pub-id-type="publisher-id">10.1177_1356389012472103</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>News from the community</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>News from the community</article-title>
</title-group>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2013</year>
</pub-date>
<volume>19</volume>
<issue>1</issue>
<fpage>97</fpage>
<lpage>101</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<sec id="section1-1356389012472103">
<title>The BetterEvaluation website</title>
<p>BetterEvaluation is a new website that aims to improve the quality of evaluations. It went live in October 2012 at <ext-link ext-link-type="uri" xlink:href="http://www.betterevaluation.org">www.betterevaluation.org</ext-link> and received nearly 5000 hits from 138 countries in its first month. BetterEvaluation’s intention is to improve programs and projects by helping evaluators and those commissioning evaluations to choose the right evaluation methods for their situation and to use them well, so that quality evaluations inform learning and improvement. It does this by highlighting useful material from other sources, combining this with documented experience of evaluation practice and organizing these using a hierarchical taxonomy of evaluation tasks; methods and options; and relevant resources.</p>
<p>The site enables and encourages evaluators, managers and decision makers to share their knowledge and practical experience. While the site has a particular focus on the evaluation of development projects and programs, the evaluation tasks, methods and resources on the site are relevant to any evaluation. Anyone with an interest in evaluation is invited to join BetterEvaluation to share knowledge and good examples from their own practice, to explore challenges and to contribute to building accessible information about evaluation practice and theory.</p>
<p>BetterEvaluation has been developed by RMIT University’s Centre for Applied Social Research (Australia), the Overseas Development Institute (UK), Institutional Learning and Change, an initiative of the Consultative Group on International Agricultural Research (Italy), and Pact, a Washington-based NGO that supports local capacity building worldwide. The development of the site included demonstrations and workshops across the world in Australia, South Africa, Finland, Ghana, Italy, and the USA.</p>
<p>The project has received financial support from the Rockefeller Foundation, AusAID (Australia’s international aid agency), and IFAD (the International Fund for Agricultural Development, a specialized agency of the United Nations).</p>
</sec>
<sec id="section2-1356389012472103">
<title>Comprehensive evaluation of international institutions: Evaluation experts discuss improvements</title>
<p>Interest is growing in a relatively new kind of evaluation: the comprehensive evaluation of international institutions. Unlike traditional project- or program-related evaluations, comprehensive evaluations typically assess the overall performance and achievements of an entire organization, fund, or corporation. Costing anywhere between $0.5 million and $18 million and taking up to five years to complete, comprehensive evaluations are commissioned by members, stakeholders and donors to improve decision-making regarding resource allocation and funding commitments for international agencies and funds, yet on a rather ad-hoc basis. They can be a recurring event or commissioned as a one-off, in response to internal or external shocks or crises; some institutions escape evaluation altogether, and others may be subject to reviews that lack rigor and balance. About 17 comprehensive evaluations have been carried out in the last 10 years at a cost of more than $30 million.</p>
<p>Considering the amount of time and money being spent on these evaluations, one might expect the end products to prove a bountiful tool for learning and comparison of international organizations and aid. However, according to a group of independent evaluators and experts currently examining the topic, the opposite is true, and while the evaluations may have provided benefits to the individual international institutions under review, they tend to share no common indicators and thus provide no comparisons that could be used by the international-development community to assess overall aid effectiveness. Also, as opposed to project and program evaluations, which share common principles, guidelines, and definitions, evaluators carrying out comprehensive evaluation are still without such common tools to perform assessments and reviews. ‘The Comprehensive Evaluation Initiative’ (CEI) was formed in response to these systematic weaknesses in the approach to comprehensive evaluations, and is working towards identifying ways to enhance their effectiveness. This contribution discusses the initiative’s progress to date and some of the arguments surrounding the practice of comprehensive evaluations.</p>
<p>The first step towards a discussion about comprehensive evaluations began in 2011 when the Global Environment Facility (GEF) Evaluation Office, with the support of the Independent Evaluation Office of the International Fund for Agricultural Development (IFAD) commissioned a preliminary desk study on the topic. Completed in August 2011, the study looked at a number of comprehensive evaluations carried out by international institutions looking for commonalities and synergies between them. The study assessed 17 such evaluations and concluded that they offered very little ground for comparison between the various institutions in terms of impact and effectiveness. The study also found a disconnect between most of these evaluations and the wider discourse on aid effectiveness.</p>
<p>Rob D. van den Berg, Director of the GEF Evaluation Office and involved in initiating the CEI, explained how the CEI came about. He said:<disp-quote>
<p>Our work started with an overview study which found some problematic systematic weaknesses and a lack of compatibility within these Comprehensive Evaluations. They represent a loss of opportunity; you have these big evaluations and spend a lot of money but they tend to be focused on just one organization which means you cannot draw any conclusions that go beyond the organization.</p>
</disp-quote></p>
<p>Derek Poate, an experienced evaluator who has worked on comprehensive evaluations, explained further:<disp-quote>
<p>Although these comprehensive evaluations have become a widely used way of periodic evaluation of institutions, funds and programs, the absence of an overarching body to document and guide the approach has meant that most of these exercises were unique, starting more or less from scratch (or, ‘how it was done last time’) and without any comparative experience to guide the design and management.</p>
</disp-quote></p>
<p>As a result of these issues, a concerned group of evaluation specialists from international institutions in addition to independent consultants launched a series of more in-depth case studies to better understand how comprehensive evaluations were designed and used, and draw lessons about how to make them more relevant and better value for money going forward. These cases studies included reviews of comprehensive evaluations of the GEF, the International Fund for Agricultural development (IFAD), the Food and Agricultural Organization (FAO), the United Nations Educational, Scientific, and Cultural organization (UNESCO), the Global Fund for AIDS, Tuberculosis, and Malaria (which was reviewed by the Independent Evaluation Group of the World Bank), and a bilateral perspective from the Swedish Agency for Development Evaluation (SADEV).</p>
<p>In June 2012, evaluation professionals and specialists from the Independent Evaluation Group (IEG) of the World Bank, the Organization for Economic Co-operation and Development (OECD), the United Nations Development Programme (UNDP), and the European Investment Bank (EIB), alongside independent consultants and evaluation experts from national governments, gathered at a workshop in Paris to discuss the case-study findings. During the two-day event, participants reviewed the reasons why international organizations, institutions and funds undertake comprehensive evaluations, the influence they have, and the lessons that can be applied to the methods of conducting such evaluations going forward. The overall aim of the workshop was to find ways of improving the methods and approaches used in comprehensive evaluations and, where possible, to promote incorporation of these into the regular evaluation programming of institutions to address questions on performance, effectiveness and achievements more comprehensively.</p>
<p>The workshop proved a fertile ground for discussion and a general consensus was reached among the attendees that comprehensive evaluations could be used to compare international institutions and identify gaps in performance and effectiveness, and thus help build a more coherent and effective aid system. However, in their current form they are too fragmented and varied in terms of method and application to act as such and therefore the group concluded there was a need to encourage a wider discussion on best practices, standards and guidelines to help guide evaluators, and create better opportunities for comparison.</p>
<p>Several lessons were drawn from the workshop, which set the stage for further discussions and talks within the global development aid and evaluation communities. Participants agreed that comprehensive evaluations will be most effective where the institutions in question already have a strong and consistent evaluation system operating at the project and program level. Not only should this improve the quality of the comprehensive evaluation but could help reduce shocks to the governance and management teams and improve uptake of the recommendations since they are already used to the practice of evaluation. Many participants in the workshop felt that this was demonstrated by the case studies which showed that organizations without a consistent internal evaluation system in place tended to have weaker uptake of recommendations coming out of a comprehensive evaluation, which often led to a completely stalled process.</p>
<p>The case studies also revealed organizations have different reasons for carrying out comprehensive evaluations, the most common being upcoming replenishment processes, a crisis situation, overall institutional-reform processes, or, in the case of global funds, lack of evaluative evidence on the functioning and achievements of the organization as a whole. Furthermore, it became apparent that while comprehensive evaluations have significantly influenced funding and budget decisions, they have tended to fall short on evaluating actual impact and aid effectiveness, which are the key elements by which to draw comparisons between institutions.</p>
<p>Comprehensive evaluations have also tended to neglect the evaluation of governance arrangements, which is an essential determinant of development effectiveness. Few evaluations included assessments of governance performance; for those that did, recommendations on improvements were rarely implemented. Workshop participants agreed that this raised interesting questions around whether the body overseeing a comprehensive evaluation should, to some extent, have authority over the governance body of the organization being evaluated in order to ensure all aspects and structures within the institution are evaluated.</p>
<p>The workshop also revealed a number of differing opinions. For example, while it was agreed that the independence of an evaluation is key both to establishing its credibility and acceptability and to an institution’s willingness to absorb and respond to its findings and recommendations, opinions varied on whether a comprehensive evaluation should be undertaken by an in-house evaluation team or by an external independent evaluator. On the one hand, an external group of evaluators could bring more independence and objectivity, while on the other hand an internal office would have more experience and institutional knowledge of the organization being evaluated.</p>
<p>These questions were explored further during a conference held by the European Evaluation Society (EES) in Helsinki in October where GEF EO Director Rob D. van den Berg gave a presentation on the Comprehensive Evaluation Initiative before opening the topic up for discussion between a panel of four evaluation experts and the audience. Indran Naidoo, Director of UNDP Evaluation Office, was on the expert panel and also took part in the Paris workshop. Speaking after the Helsinki conference, he outlined his thoughts and concerns about the growth of comprehensive evaluations, in particular referring to the importance of a solid in-house evaluation system as a necessity for improved comprehensive evaluations. He reported:<disp-quote>
<p>Both the presentation and panel were excellent because they brought the issue to the table – these Comprehensive Evaluations are happening and we need to engage with them when they do. But the position of the UN Evaluation Group is clear: we don’t see comprehensive evaluations as taking over from the normal in-house evaluation function. These big evaluations often come in through the back door, some are for particular issues like when a board needs restructuring, or where there is a loss of confidence in the organization, then they might engage with a comprehensive evaluation. This is unusual and not something that I, as an evaluation head, am going to encourage. Instead I think we should keep focusing on building up existing evaluation capacity within organizations. Comprehensive Evaluations may be necessary in certain circumstances but they should only be used where there is no existing evaluation function.</p>
</disp-quote></p>
<p>On this issue of evaluation capacity, Mr Naidoo went further and said:<disp-quote>
<p>Some people have said that the capacity to carry out these evaluations does not exist, especially in Southern countries, but I don’t think that is true.</p>
</disp-quote></p>
<p>He also challenged the idea of comprehensive evaluations providing independence, saying:<disp-quote>
<p>We should not outsource such a key function as evaluation, it reduces accountability. One does not get independence from outsourcing, one gets independence by building credibility into the evaluation process. It is weak, lame and self-serving to say that you can only get independence by outsourcing an evaluation.</p>
</disp-quote></p>
<p>So what about the future? In terms of next steps, those at the workshop agreed that while it would not be possible to produce a blueprint or guidebook to carrying our comprehensive evaluations, there is scope to agree on common principles and indicators, and that in order to do this these evaluations need to be discussed more widely to share learning and ideas on how to establish such common principles. The GEF Evaluation Office has taken the first steps towards facilitating this wider discussion by launching an online community of practice, called the Comprehensive Evaluation Platform for Knowledge Exchange (CEPKE). The CEPKE went live in October and is designed to provide an informal platform for evaluation specialists and other interested professionals to share ideas, information, and experiences related to carrying out comprehensive evaluations of institutions. It contains information about past evaluations, the Paris workshop, and discussion forums where experts can gather advice and share ideas on how to carry out solid comprehensive evaluations. Furthermore, IFAD’s Independent Evaluation Office has commissioned a study by Paul Isenman that explores possibilities for incorporating the lessons from comprehensive evaluations in current discussions on aid effectiveness and in further work on evaluation guidelines, sourcebooks and good practice libraries. The study will be published in CEPKE and lead to further interactions on that platform.</p>
<p>Derek Poate, who attended the Paris workshop and was also on the panel at the Helsinki conference, summarized the progress of the Comprehensive Evaluation Initiative to date. He said:<disp-quote>
<p>From the broad review and in-depth case studies we now see that there are key issues about governance, selection of evaluation questions, the importance of building on self-evaluation and the need to fund and manage the way evaluation findings and conclusions are followed up by management. There may also be complex design issues in trying to ensure that questions reflect the policy concerns of the institution but also respond to wider issues of the aid effectiveness agenda. The case studies provide some useful pointers, but the knowledge exchange platform should create a facility that commissioners of institutional evaluations and prospective evaluators can interact with, to help plan and manage their work. I am aware of recent and current institutional evaluations that have governance arrangements that do not meet high standards of independence and also lack strong technical advisory support. In the future, the CEPKE could provide impartial advice to help overcome such problems.</p>
</disp-quote></p>
<p>Contributed by Sara Trab Nielsen (Consultant, GEF Evaluation Office) and Sophie Edwards (Consultant, GEF Evaluation Office).</p>
</sec>
</body>
</article>