<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPM</journal-id>
<journal-id journal-id-type="hwp">spepm</journal-id>
<journal-title>Educational and Psychological Measurement</journal-title>
<issn pub-type="ppub">0013-1644</issn>
<issn pub-type="epub">1552-3888</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0013164411434638</article-id>
<article-id pub-id-type="publisher-id">10.1177_0013164411434638</article-id>
<title-group>
<article-title>Bayesian Comparison of Alternative Graded Response Models for Performance Assessment Applications</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Zhu</surname><given-names>Xiaowen</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411434638">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Stone</surname><given-names>Clement A.</given-names></name>
<xref ref-type="aff" rid="aff2-0013164411434638">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0013164411434638"><label>1</label>Xi’an Jiaotong University, Xi’an, China</aff>
<aff id="aff2-0013164411434638"><label>2</label>University of Pittsburgh, Pittsburgh, PA, USA</aff>
<author-notes>
<corresp id="corresp1-0013164411434638">Xiaowen Zhu, Department of Sociology &amp; Institute for Empirical Social Science Research (IESSR), Xi’an Jiaotong University, Xi'an, Shaanxi, 710049, China Email: <email>xwzhu@mail.xjtu.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>72</volume>
<issue>5</issue>
<fpage>774</fpage>
<lpage>799</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This study examined the relative effectiveness of Bayesian model comparison methods in selecting an appropriate graded response (GR) model for performance assessment applications. Three popular methods were considered: deviance information criterion (DIC), conditional predictive ordinate (CPO), and posterior predictive model checking (PPMC). Using these methods, several alternative GR models were compared with Samejima’s unidimensional GR model, including the one-parameter GR model, the rating scale model, simple- and complex-structure two-dimensional GR models, and the GR model for testlets. Results from a simulation study indicated that these methods appeared to be equally accurate in selecting the preferred model. However, CPO and PPMC can be used to compare models at the item level, and PPMC can also be used to compare both the relative and absolute fit of different models.</p>
</abstract>
<kwd-group>
<kwd>Bayesian model comparison</kwd>
<kwd>DIC</kwd>
<kwd>CPO</kwd>
<kwd>posterior predictive model checking</kwd>
<kwd>graded response models</kwd>
<kwd>multidimensionality</kwd>
<kwd>local independence</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>With the increased popularity of including performance-based items in large-scale assessments, standard unidimensional polytomous item response theory (IRT) models are commonly used to analyze performance assessment data. However, the underlying assumptions (e.g., unidimensionality and local item independence) may be violated for performance assessment responses (<xref ref-type="bibr" rid="bibr16-0013164411434638">Lane &amp; Stone, 2006</xref>), and thus, more complex polytomous models may be needed for analysis. For example, a multidimensional polytomous model may be more appropriate for responses exhibiting multidimensionality, or a polytomous model for testlets may be more suitable for assessments that involve a subset of items with a common stimulus. To choose an appropriate model for a particular testing application, model comparison techniques can be employed.</p>
<p>For more complex IRT models, it has become increasingly common to estimate models using Bayesian methods with Markov Chain Monte Carlo (MCMC) algorithms. For these methods, different types of Bayesian approaches have been discussed for comparing competing models: (a) Bayesian information-based criteria, such as deviance information criterion (DIC; <xref ref-type="bibr" rid="bibr29-0013164411434638">Spiegelhalter, Best, Carlin, &amp; van der Linde, 2002</xref>) and (b) indices based on Bayes factors (BF; <xref ref-type="bibr" rid="bibr14-0013164411434638">Kass &amp; Raftery, 1995</xref>), such as the pseudo-Bayes factor (PsBF; <xref ref-type="bibr" rid="bibr9-0013164411434638">Geisser &amp; Eddy, 1979</xref>; <xref ref-type="bibr" rid="bibr10-0013164411434638">Gelfand, Dey, &amp; Chang, 1992</xref>) and the conditional predictive ordinate (CPO; <xref ref-type="bibr" rid="bibr15-0013164411434638">Kim &amp; Bolt, 2007</xref>). Also, the posterior predictive model checking (PPMC) method can be used for model comparisons, though it is essentially a Bayesian tool for evaluating model data-fit (e.g., <xref ref-type="bibr" rid="bibr3-0013164411434638">Béguin &amp; Glas, 2001</xref>; <xref ref-type="bibr" rid="bibr19-0013164411434638">Li, Bolt, &amp; Fu, 2006</xref>; <xref ref-type="bibr" rid="bibr26-0013164411434638">Sinharay, 2005</xref>).</p>
<p>Research comparing different model comparison methods in IRT applications has been limited. <xref ref-type="bibr" rid="bibr19-0013164411434638">Li et al. (2006)</xref> investigated the performance of three Bayesian methods (DIC, PsBF, and PPMC) in selecting a testlet model for dichotomous item responses. They found that PsBF and PPMC were effective in identifying the data-generating testlet models, but DIC tended to favor more complex testlet models. <xref ref-type="bibr" rid="bibr12-0013164411434638">Kang and Cohen (2007)</xref> examined the accuracy of the G<sup>2</sup> likelihood test statistic, Akaike’s information criterion (AIC; <xref ref-type="bibr" rid="bibr2-0013164411434638">Akaike, 1974</xref>), Bayesian information criterion (BIC; <xref ref-type="bibr" rid="bibr25-0013164411434638">Schwarz, 1978</xref>), DIC, and PsBF indices in selecting the most appropriate dichotomous IRT model. The PsBF index was found to be the more accurate index. The DIC index tended to select a more complex model, and conversely, the BIC index tended to select a simpler model. <xref ref-type="bibr" rid="bibr13-0013164411434638">Kang, Cohen, and Sung (2009)</xref> later compared the performance of the AIC, BIC, DIC, and PsBF indices with respect to the selection of unidimensional polytomous IRT models. They found that BIC was accurate and consistent in selecting the true or simulated polytomous model. Performance of AIC was similar to BIC whereas DIC and PsBF were less accurate in some conditions when the true model was the graded response (GR) model.</p>
<p>The above findings indicate that model comparison methods perform inconsistently in Bayesian IRT applications. Their performance has been dependent on specific conditions as well as specific models that were compared. The purpose of this study was to extend previous research specifically to performance assessment applications and to investigate the effectiveness of different Bayesian methods in selecting a model from a set of alternative models that are applicable to performance assessments. The set of models included <xref ref-type="bibr" rid="bibr24-0013164411434638">Samejima’s (1969)</xref> unidimensional two-parameter GR (2P-GR) model, a one-parameter version of the GR model (1P-GR), the rating scale (RS) model (<xref ref-type="bibr" rid="bibr21-0013164411434638">Muraki, 1990</xref>), the simple- and complex-structure two-dimensional GR models (<xref ref-type="bibr" rid="bibr7-0013164411434638">De Ayala, 1994</xref>), and the GR model for testlets (<xref ref-type="bibr" rid="bibr32-0013164411434638">Wang, Bradlow, &amp; Wainer, 2002</xref>). All models were estimated in WinBUGS (<xref ref-type="bibr" rid="bibr30-0013164411434638">Spiegelhalter, Thomas, Best, &amp; Lunn, 2003</xref>) using MCMC methods, and compared using three Bayesian model comparison approaches: the DIC index, the CPO index, and the PPMC method.</p>
<sec id="section1-0013164411434638">
<title>Model Comparison Criteria</title>
<sec id="section2-0013164411434638">
<title>Deviance Information Criterion</title>
<p>DIC (<xref ref-type="bibr" rid="bibr29-0013164411434638">Spiegelhalter et al., 2002</xref>) is an information-based index similar to other information criteria indices in that it weights both model fit and model complexity in identifying a preferred model. The DIC index is widely used with Bayesian estimation with MCMC methods and defined as</p>
<p><disp-formula id="disp-formula1-0013164411434638">
<mml:math display="block" id="math1-0013164411434638">
<mml:mrow>
<mml:mi>DIC</mml:mi>
<mml:mo>=</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0013164411434638" xlink:href="10.1177_0013164411434638-eq1.tif"/>
</disp-formula></p>
<p>The first term, <inline-formula id="inline-formula1-0013164411434638">
<mml:math display="inline" id="math2-0013164411434638">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, is the posterior mean of the deviance between the data and a model and defined as</p>
<p><disp-formula id="disp-formula2-0013164411434638">
<mml:math display="block" id="math3-0013164411434638">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mi>D</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>η</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi>y</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi>log</mml:mi>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">y</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0013164411434638" xlink:href="10.1177_0013164411434638-eq2.tif"/>
</disp-formula></p>
<p>where <bold>y</bold> represents the response data, <bold>η</bold> denotes the model parameters, and <inline-formula id="inline-formula2-0013164411434638">
<mml:math display="inline" id="math4-0013164411434638">
<mml:mrow>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">y</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> is the likelihood function. The second term in <xref ref-type="disp-formula" rid="disp-formula1-0013164411434638">Equation (1)</xref>, <italic>p</italic><sub><italic>D</italic></sub>, represents the effective number of parameters in the model or model complexity. It is defined as the difference between the posterior mean of the deviance and the deviance at the posterior mean of model parameters <inline-formula id="inline-formula3-0013164411434638">
<mml:math display="inline" id="math5-0013164411434638">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mtext>η</mml:mtext>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>:</p>
<p><disp-formula id="disp-formula3-0013164411434638">
<mml:math display="block" id="math6-0013164411434638">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mtext>η</mml:mtext>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>D</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mtext>η</mml:mtext>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-0013164411434638" xlink:href="10.1177_0013164411434638-eq3.tif"/>
</disp-formula></p>
<p>Note that DIC is a Bayesian generalization (Speigelhalter et al., 2002) of AIC and is related to BIC. DIC was developed since MCMC estimation uses prior information and the actual number of parameters cannot be clearly identified as required for the AIC and BIC indices. As for AIC and BIC, smaller values of DIC indicate better fit. DIC was the focus of this study since it is more commonly used for comparing models estimated with Bayesian methods and is more readily accessible to researchers using WinBUGS.</p>
</sec>
<sec id="section3-0013164411434638">
<title>Conditional Predictive Ordinate</title>
<p>Bayes factors (BF; <xref ref-type="bibr" rid="bibr14-0013164411434638">Kass &amp; Raftery, 1995</xref>) have traditionally been used with Bayesian methods to compare competing models based on the observed data. The BF for comparing Model 1 (M<sub>1</sub>) with Model 2 (M<sub>2</sub>) is defined by the posterior odds of M<sub>1</sub> to M<sub>2</sub> divided by the prior odds of M<sub>1</sub> to M<sub>2</sub>. Using Bayes theorem, the BF reduces to the ratio of marginal likelihoods of the data (<italic>y</italic>) under each model:</p>
<p><disp-formula id="disp-formula4-0013164411434638">
<mml:math display="block" id="math7-0013164411434638">
<mml:mrow>
<mml:mi>BF</mml:mi>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">/</mml:mo>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">/</mml:mo>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mfrac>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-0013164411434638" xlink:href="10.1177_0013164411434638-eq4.tif"/>
</disp-formula></p>
<p>A BF larger than 1 supports selection of M<sub>1</sub> and a value less than 1 supports selection of M<sub>2</sub>. Research has shown that there are several issues with using this method in practice. For instance, the calculation of BF from MCMC output becomes difficult for complex models, and it is not well defined for improper priors (<xref ref-type="bibr" rid="bibr19-0013164411434638">Li et al., 2006</xref>). As a result, PsBF (<xref ref-type="bibr" rid="bibr9-0013164411434638">Geisser &amp; Eddy, 1979</xref>; <xref ref-type="bibr" rid="bibr10-0013164411434638">Gelfand et al., 1992</xref>) and CPO (<xref ref-type="bibr" rid="bibr15-0013164411434638">Kim &amp; Bolt, 2007</xref>) indices have been discussed as surrogates for BF.</p>
<p>Both PsBF and CPO indices require calculation of cross-validation predictive densities. Let <bold>y</bold><sub>(<italic>r</italic>), obs</sub> denote the set of observations <bold>y</bold><sub>obs</sub> with the <italic>r</italic><sup>th</sup> observation omitted, and let <bold>η</bold> denote all the parameters under the assumed model. The cross-validation predictive density is defined as</p>
<p><disp-formula id="disp-formula5-0013164411434638">
<mml:math display="block" id="math8-0013164411434638">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mo>∫</mml:mo>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>d</mml:mi>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-0013164411434638" xlink:href="10.1177_0013164411434638-eq5.tif"/>
</disp-formula></p>
<p>The density <inline-formula id="inline-formula4-0013164411434638">
<mml:math display="inline" id="math9-0013164411434638">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> indicates the values of <italic>y</italic><sub><italic>r</italic></sub> that are likely when the model is estimated from all observations except <italic>y</italic><sub><italic>r</italic></sub>. In the context of item response data, <italic>y</italic><sub><italic>r</italic></sub> represents a single examinee’s response to an individual item. The product of <inline-formula id="inline-formula5-0013164411434638">
<mml:math display="inline" id="math10-0013164411434638">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> across all observations can be used as an estimate of the marginal likelihood in <xref ref-type="disp-formula" rid="disp-formula4-0013164411434638">Equation (4)</xref>. The PsBF of Model 1 against Model 2 is defined as</p>
<p><disp-formula id="disp-formula6-0013164411434638">
<mml:math display="block" id="math11-0013164411434638">
<mml:mrow>
<mml:mi>PsBF</mml:mi>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:munderover>
<mml:mo>Π</mml:mo>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mtext>obs</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mtext>obs</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:munderover>
<mml:mo>Π</mml:mo>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mtext>obs</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mtext>obs</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mfrac>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula6-0013164411434638" xlink:href="10.1177_0013164411434638-eq6.tif"/>
</disp-formula></p>
<p>where <italic>R</italic> denotes the total number of item responses from all examinees. When comparing the models at the item level, <italic>R</italic> equals the number of examinees <italic>N</italic>. When comparing the models at the test level, <italic>R</italic> equals the number of the responses for all examinees to all items (i.e., <italic>R</italic> = <italic>N</italic>×<italic>I</italic>, where <italic>I</italic> is the total number of items). As for the BF index, a PsBF greater than 1 supports selection of Model 1 and a value less than 1 supports selection of Model 2.</p>
<p>The CPO index for a model can be defined as</p>
<p><disp-formula id="disp-formula7-0013164411434638">
<mml:math display="block" id="math12-0013164411434638">
<mml:mrow>
<mml:mi>CP</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>O</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>Π</mml:mo>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mtext>obs</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mtext>obs</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula7-0013164411434638" xlink:href="10.1177_0013164411434638-eq7.tif"/>
</disp-formula></p>
<p>The model with larger CPO value is preferred. It is easy to see that the same conclusions will be obtained using either CPO or PsBF indices, so only the CPO index was used in the current study.</p>
</sec>
<sec id="section4-0013164411434638">
<title>Posterior Predictive Model Checking</title>
<p>PPMC (<xref ref-type="bibr" rid="bibr23-0013164411434638">Rubin, 1984</xref>) is a flexible Bayesian model-checking tool that has recently proved useful in assessing different aspects of fit for IRT models (e.g., <xref ref-type="bibr" rid="bibr8-0013164411434638">Fu, Bolt, &amp; Li, 2005</xref>; <xref ref-type="bibr" rid="bibr18-0013164411434638">Levy, Mislevy, &amp; Sinharay, 2009</xref>; <xref ref-type="bibr" rid="bibr26-0013164411434638">Sinharay, 2005</xref>, <xref ref-type="bibr" rid="bibr27-0013164411434638">2006</xref>; <xref ref-type="bibr" rid="bibr28-0013164411434638">Sinharay, Johnson, &amp; Stern, 2006</xref>; <xref ref-type="bibr" rid="bibr37-0013164411434638">Zhu &amp; Stone, 2011</xref>). PPMC involves simulating data using posterior distributions for model parameters and comparing features of simulated data against observed data. A discrepancy measure computed on both observed and simulated data is used to evaluate whether differences reflect potential misfit of a model (<xref ref-type="bibr" rid="bibr11-0013164411434638">Gelman, Carlin, Stern, &amp; Rubin, 2003</xref>). The comparison of realized (or observed) and posterior predictive (or simulated) values for the discrepancy measure can be performed using graphical displays as well as by computing posterior predictive <italic>p</italic> (PPP) values. PPP values provide numerical summaries of model fit. They reflect the relative occurrence of a value for a discrepancy measure <italic>D</italic>(<bold>y</bold>, <bold>η</bold>) based on the observed data <bold>y</bold> in the distribution of discrepancy values from replicated values <italic>D</italic>(<bold>y</bold><sup>rep</sup>, <bold>η</bold>):</p>
<p><disp-formula id="disp-formula8-0013164411434638">
<mml:math display="block" id="math13-0013164411434638">
<mml:mrow>
<mml:mi>PPP</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>D</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>rep</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>≥</mml:mo>
<mml:mi>D</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">y</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="bold">y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mo>∫</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mo>∫</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi>D</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>rep</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mo>≥</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>D</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">y</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>rep</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi mathvariant="bold">y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>d</mml:mi>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="bold">y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>rep</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:mi>d</mml:mi>
<mml:mi mathvariant="bold">η</mml:mi>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula8-0013164411434638" xlink:href="10.1177_0013164411434638-eq8.tif"/>
</disp-formula></p>
<p>where <bold>y</bold> represents the response data, <bold>y</bold><sup>rep</sup> represents the replicated data based on the model, and <bold>η</bold> is a vector of model parameters. PPP values near 0.5 indicate that there is no systematic difference between the realized and predictive discrepancies, and thus indicate adequate fit of the model. PPP values near 0 or 1 (e.g., values &lt;.05 or &gt;.95) suggest conversely that the realized discrepancies are inconsistent with the posterior predictive discrepancy values, and thus indicate inadequate model-data-fit (e.g., <xref ref-type="bibr" rid="bibr11-0013164411434638">Gelman et al., 2003</xref>; <xref ref-type="bibr" rid="bibr18-0013164411434638">Levy et al., 2009</xref>; <xref ref-type="bibr" rid="bibr26-0013164411434638">Sinharay, 2005</xref>).</p>
<p>PPMC has been found useful in model comparisons when Bayesian estimation is used (e.g., <xref ref-type="bibr" rid="bibr3-0013164411434638">Béguin &amp; Glas, 2001</xref>; <xref ref-type="bibr" rid="bibr19-0013164411434638">Li et al., 2006</xref>; <xref ref-type="bibr" rid="bibr26-0013164411434638">Sinharay, 2005</xref>). The relative fit of a set of alternative candidate models can be evaluated by comparing the numbers of extreme PPP values across items (or item pairs). A model with fewer numbers of items (or item pairs) with extreme PPP values is considered to fit the data better than an alternative model with larger numbers of items (or item pairs) with extreme PPP values.</p>
<p>Selecting appropriate discrepancy measures is an important consideration in applications of the PPMC method. Discrepancy measures should be chosen to reflect sources of potential misfit most relevant to a testing application. In this study, the proposed measures included four item-level measures: <italic>the item score distribution, the item–total score correlation, Yen’s Q<sub>1</sub> statistic</italic> (<xref ref-type="bibr" rid="bibr34-0013164411434638">Yen, 1981</xref>), and <italic>the pseudo-count fit</italic> (e.g., <xref ref-type="bibr" rid="bibr31-0013164411434638">Stone, 2000</xref>); and two pairwise measures: <italic>global odds ratios</italic> (<italic>global ORs</italic>; <xref ref-type="bibr" rid="bibr1-0013164411434638">Agresti, 2002</xref>), and <italic>Yen’s Q<sub>3</sub> statistic</italic> (<xref ref-type="bibr" rid="bibr35-0013164411434638">Yen, 1993</xref>). These particular measures were selected from a broader set based on their usefulness to detect IRT model-data-fit in previous testing applications (cf., <xref ref-type="bibr" rid="bibr8-0013164411434638">Fu et al., 2005</xref>; <xref ref-type="bibr" rid="bibr18-0013164411434638">Levy et al., 2009</xref>; <xref ref-type="bibr" rid="bibr26-0013164411434638">Sinharay, 2005</xref>, <xref ref-type="bibr" rid="bibr27-0013164411434638">2006</xref>; <xref ref-type="bibr" rid="bibr28-0013164411434638">Sinharay et al., 2006</xref>; <xref ref-type="bibr" rid="bibr37-0013164411434638">Zhu &amp; Stone, 2011</xref>).</p>
<sec id="section5-0013164411434638">
<title>1. Item-level measures</title>
<p>The <italic>item score distribution</italic> is an intuitive measure for assessing item-level fit. It represents the number of examinees responding to each response category for each item. The difference between observed and posterior predictive item score distributions can be summarized using a goodness-of-fit statistic (<inline-formula id="inline-formula6-0013164411434638">
<mml:math display="inline" id="math14-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>χ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>). For polytomous items, this statistic can be defined as</p>
<p><disp-formula id="disp-formula9-0013164411434638">
<mml:math display="block" id="math15-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>χ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:munderover>
<mml:mfrac>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>O</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jk</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jk</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jk</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula9-0013164411434638" xlink:href="10.1177_0013164411434638-eq9.tif"/>
</disp-formula></p>
<p>where <italic>M<sub>j</sub></italic> is the highest score on Item <italic>j</italic>, and <italic>O</italic><sub><italic>jk</italic></sub> and <italic>E</italic><sub><italic>jk</italic></sub> are the observed and posterior predicted number of examinees scoring in response category <italic>k</italic> on Item <italic>j</italic>, respectively. <italic>E</italic><sub><italic>jk</italic></sub> is calculated by summing the probabilities of responding to category <italic>k</italic> on Item <italic>j</italic> across all <italic>N</italic> examinees: <inline-formula id="inline-formula7-0013164411434638">
<mml:math display="inline" id="math16-0013164411434638">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jk</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ijk</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula></p>
<p>The <italic>item–total score correlation</italic> measure is the correlation between examinees’ total test scores and their scores on a particular item. This measure reflects item discrimination and therefore should be sensitive to any violations of the equal discrimination assumption. <xref ref-type="bibr" rid="bibr28-0013164411434638">Sinharay et al. (2006)</xref> found that this correlation was effective in detecting misfit of 1PL models to data based on 2PL or 3PL models. Therefore, it was expected that this measure would be also useful in discriminating between the 1P-GR and 2P-GR models to be compared in this study. Pearson correlations were used to estimate associations between the five-category items and total test scores.</p>
<p>A number of other statistics have been proposed to assess the fit of IRT models at the item level (e.g., <xref ref-type="bibr" rid="bibr22-0013164411434638">Orlando &amp; Thissen, 2000</xref>; <xref ref-type="bibr" rid="bibr31-0013164411434638">Stone, 2000</xref>; <xref ref-type="bibr" rid="bibr34-0013164411434638">Yen, 1981</xref>). All these statistics compare observed response score distributions and model predictions at discrete levels of ability for each item. These item-fit statistics performed well for detecting item misfit in the frequentist framework, and several were also found effective with PPMC (e.g., <xref ref-type="bibr" rid="bibr27-0013164411434638">Sinharay, 2006</xref>; <xref ref-type="bibr" rid="bibr37-0013164411434638">Zhu &amp; Stone, 2011</xref>). In this study, two of these statistics were used with PPMC: <italic>Yen’s Q<sub>1</sub></italic> (<xref ref-type="bibr" rid="bibr34-0013164411434638">Yen, 1981</xref>) and the <italic>pseudo-count fit</italic> statistic (e.g., <xref ref-type="bibr" rid="bibr31-0013164411434638">Stone, 2000</xref>). The former is a traditional item-fit index and the latter is an alternative index developed specifically for performance assessment applications.</p>
<p><italic>Yen’s Q<sub>1</sub> item-fit statistic</italic> is a goodness-of-fit statistic defined as</p>
<p><disp-formula id="disp-formula10-0013164411434638">
<mml:math display="block" id="math17-0013164411434638">
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mtext>χ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>10</mml:mn>
</mml:mrow>
</mml:munderover>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>K</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>N</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mfrac>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>O</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jk</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jk</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>E</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jk</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula10-0013164411434638" xlink:href="10.1177_0013164411434638-eq10.tif"/>
</disp-formula></p>
<p>where <italic>N<sub>j</sub></italic> is the number of examinees within ability subgroup <italic>j, O<sub>jk</sub></italic> and <italic>E<sub>jk</sub></italic> are the observed and predicted proportion of responses to category <italic>k</italic> for ability subgroup <italic>j</italic>, respectively. In <italic>Yen’s statistic</italic>, examinees are divided into 10 ability subgroups of approximately equal size after they are rank-ordered by their ability estimates. The expected proportion to a response category for a subgroup is the mean of the probabilities of responses to that category for all the examinees in that subgroup.</p>
<p>The <italic>pseudo-count fit statistic</italic> is a goodness-of-fit statistic developed to account for the imprecision in ability estimation for short tests, such as performance assessments. Whereas <italic>Yen’s Q<sub>1</sub></italic> cross-classifies examinees into only one cell of the item-fit table based on each item response and point estimate of ability, the <italic>pseudo-count fit statistic</italic> assigns each examinee to multiple ability groups based on posterior expectations (posterior probabilities for each discrete ability level). Summing the posterior expectations across all examinees provides a pseudo-observed score distribution. Model-based predictions for score categories at each ability level are calculated in the same way as in <italic>Yen’s Q<sub>1</sub></italic> statistic.</p>
<p>It may be worthy to note that a difference between these statistics and the <italic>item-score distribution</italic> index is that the observed and model-based predictions are compared at different discrete levels of the ability parameter in IRT models. Thus, summations in the equations are across response categories and discrete ability levels.</p>
</sec>
<sec id="section6-0013164411434638">
<title>2. Pairwise measures</title>
<p><italic>Yen’s Q<sub>3</sub></italic> index (<xref ref-type="bibr" rid="bibr34-0013164411434638">Yen, 1981</xref>) and an <italic>OR</italic> measure were used as possible pairwise measures in this study. These measures are classic local-dependence indices and reflect the association between responses to item pairs. <italic>Yen’s Q<sub>3</sub></italic> statistic measures the correlations between pairs of items after accounting for the latent ability. For Items <italic>j</italic> and <italic>j, Q<sub>3</sub></italic> is defined as the correlation of deviation scores across all examinees: <inline-formula id="inline-formula8-0013164411434638">
<mml:math display="inline" id="math18-0013164411434638">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>Q</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>3</mml:mn>
<mml:mi>jj</mml:mi>
<mml:mo>′</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>r</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>d</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>d</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>′</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, where <italic>d<sub>j</sub></italic> is the deviation between observed and expected performance on Item <italic>j</italic>. Alternatively, <xref ref-type="bibr" rid="bibr6-0013164411434638">Chen and Thissen (1997)</xref> used OR to evaluate local dependence for dichotomous items. The OR for dichotomous item pairs (<italic>j</italic> and <italic>j*</italic>) are computed from 2 × 2 tables by <italic>n</italic><sub>00</sub><italic>n</italic><sub>11</sub>/<italic>n</italic><sub>01</sub><italic>n</italic><sub>10</sub>, where <italic>n</italic><sub><italic>pq</italic></sub> is the observed number of examinees having response <italic>p</italic> (0 or 1) on Item <italic>j</italic> and response <italic>q</italic> (0 or 1) on Item <italic>j*</italic>.</p>
<p>In contrast with dichotomously scored items, multiple ORs can be computed with polytomous items since the contingency table is <italic>R</italic>×<italic>C</italic> (<italic>R</italic> &gt; 2 and <italic>C</italic> &gt; 2). For this study, a <italic>global OR</italic> (<xref ref-type="bibr" rid="bibr1-0013164411434638">Agresti, 2002</xref>) was used as a possible discrepancy measure. For any two items, the <italic>R</italic>×<italic>C</italic> contingency table may be reduced to a 2 × 2 contingency table by dichotomizing the response categories of each item. The <italic>global OR</italic> was then defined as the cross-ratio of this pooled 2 × 2 table. In this study, the dichotomization was based on score rubrics often used with performance assessments. For items with 5 response categories (0-4), Categories 3 and 4 were treated as “correct” responses, and Categories 0, 1, and 2 were treated as “incorrect” responses.</p>
<p>Both <italic>Yen’s Q<sub>3</sub></italic> and <italic>OR</italic> measures have been found to be effective in detecting multidimensionality and local dependence among item responses with PPMC (e.g., <xref ref-type="bibr" rid="bibr18-0013164411434638">Levy et al., 2009</xref>; <xref ref-type="bibr" rid="bibr28-0013164411434638">Sinharay et al., 2006</xref>; <xref ref-type="bibr" rid="bibr37-0013164411434638">Zhu &amp; Stone, 2011</xref>). Therefore, they were expected to be useful in comparing unidimensional models with multidimensional IRT models or testlet models considered in this study. Although <italic>Q<sub>3</sub></italic> has been found to be more effective than the <italic>OR</italic> measure when evaluating the fit of a single IRT model with PPMC (e.g., <xref ref-type="bibr" rid="bibr18-0013164411434638">Levy et al., 2009</xref>; <xref ref-type="bibr" rid="bibr37-0013164411434638">Zhu &amp; Stone, 2011</xref>), both measures were used in the present study to evaluate their respective usefulness in the context of a model comparison approach.</p>
</sec>
</sec>
</sec>
<sec id="section7-0013164411434638">
<title>Alternative Graded Response Models for Performance Assessments</title>
<p><xref ref-type="bibr" rid="bibr24-0013164411434638">Samejima’s (1969)</xref> 2P-GR model is a commonly used polytomous model for performance assessment applications. For Item <italic>j</italic> with (<italic>m<sub>j</sub></italic>+ 1) response categories (0, 1, 2, . . ., <italic>m<sub>j</sub></italic>), the probability that an examinee receives a category score <italic>x</italic> (<italic>x</italic> = 1, 2, . . ., <italic>m<sub>j</sub></italic>) or higher <inline-formula id="inline-formula9-0013164411434638">
<mml:math display="inline" id="math19-0013164411434638">
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ijx</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> is modeled by the logistic deviate e<sup>Z</sup>, where <inline-formula id="inline-formula10-0013164411434638">
<mml:math display="inline" id="math20-0013164411434638">
<mml:mrow>
<mml:mtext>z</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>⌊</mml:mo>
<mml:mi>D</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jx</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>⌋</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, <italic>D</italic> is the scaling constant (1.7 or 1), θ<italic><sub>i</sub></italic> is examinee ability, <italic>a<sub>j</sub></italic> is the discrimination (slope) parameter for Item <italic>j</italic>, and <italic>b<sub>jx</sub></italic> is a threshold parameter for Category <italic>x</italic> of Item <italic>j</italic>. The probability of a particular response in Category <italic>x</italic> for an examinee on Item <italic>j</italic><inline-formula id="inline-formula11-0013164411434638">
<mml:math display="inline" id="math21-0013164411434638">
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ijx</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> is then defined as the difference between the cumulative probabilities for two adjacent categories: <inline-formula id="inline-formula12-0013164411434638">
<mml:math display="inline" id="math22-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula13-0013164411434638">
<mml:math display="inline" id="math23-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ijx</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, with two constraints <inline-formula id="inline-formula14-0013164411434638">
<mml:math display="inline" id="math24-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula15-0013164411434638">
<mml:math display="inline" id="math25-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula>.</p>
<p>For the standard 2P-GR model, one slope parameter is estimated for each item. Under the assumption that all items have the same discrimination, the 2P-GR model is reduced to a 1P-GR model with <inline-formula id="inline-formula16-0013164411434638">
<mml:math display="inline" id="math26-0013164411434638">
<mml:mrow>
<mml:mtext>z</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>⌊</mml:mo>
<mml:mi>Da</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jx</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>⌋</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. Therefore, the 1P model is a restricted version of the standard GR model with a common slope parameter.</p>
<p>Another restricted case of the 2P-GR model is <xref ref-type="bibr" rid="bibr21-0013164411434638">Muraki’s (1990)</xref> RS model. In this model, <inline-formula id="inline-formula17-0013164411434638">
<mml:math display="inline" id="math27-0013164411434638">
<mml:mrow>
<mml:mtext>z</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>⌊</mml:mo>
<mml:mi>D</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>⌋</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> and the threshold parameters (<italic>b<sub>jx</sub></italic>) in the 2P-GR model are partitioned into two terms: a location parameter (<italic>b<sub>j</sub></italic>) for each item, and one set of category threshold parameters (<italic>c<sub>x</sub></italic>) that applies to all items. The RS model is a restricted version of the 2P-GR model since category threshold parameters are assumed to be equal across all items in the RS model, whereas they are free to vary across items in the 2P-GR model. As a result, the number of parameters in the RS model is reduced greatly as compared with the standard 2P-GR model. The RS model was originally developed for analyzing responses to items with a rating-scale type response format. However, <xref ref-type="bibr" rid="bibr16-0013164411434638">Lane and Stone (2006)</xref> pointed out that the RS model may be appropriate for performance assessments. When a general rubric is used as the basis for developing specific item rubrics, the response scales and the differences between score levels may be the same across the set of items.</p>
<p>The above models are appropriate for analyzing item responses that are assumed to be determined by one latent trait (i.e., unidimensional). When a performance assessment is designed to measure more than one ability, the item responses exhibit a multidimensional structure. <xref ref-type="bibr" rid="bibr7-0013164411434638">De Ayala (1994)</xref> discussed a multidimensional version of the GR model, where <inline-formula id="inline-formula18-0013164411434638">
<mml:math display="inline" id="math28-0013164411434638">
<mml:mrow>
<mml:mtext>z</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mi>D</mml:mi>
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>h</mml:mi>
</mml:mrow>
</mml:munder>
<mml:msub>
<mml:mrow>
<mml:mi>a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jh</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>h</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jx</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>and θ<italic><sub>h</sub></italic> is the ability level on dimension <italic>h, a<sub>jh</sub></italic> is the discrimination (slope) parameter of Item <italic>j</italic> on dimension <italic>h</italic>, and <italic>b<sub>jx</sub></italic> is the threshold parameter for Category <italic>x</italic> of Item <italic>j</italic>.</p>
<p>Finally, in performance assessments, a single stimulus (e.g., passage) may be used with several items (<xref ref-type="bibr" rid="bibr35-0013164411434638">Yen, 1993</xref>). The responses to these items are therefore likely to be locally dependent, and standard IRT models may be inappropriate. A modified GR model for testlets was proposed by <xref ref-type="bibr" rid="bibr32-0013164411434638">Wang, Bradlow, and Wainer (2002)</xref>, where <inline-formula id="inline-formula19-0013164411434638">
<mml:math display="inline" id="math29-0013164411434638">
<mml:mrow>
<mml:mtext>z</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>⌊</mml:mo>
<mml:mi>D</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>a</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jx</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>id</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>⌋</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. In this model, a random testlet effect (<inline-formula id="inline-formula20-0013164411434638">
<mml:math display="inline" id="math30-0013164411434638">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>id</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>) is introduced to reflect an interaction for Person <italic>i</italic> with testlet <italic>d</italic>(<italic>j</italic>). Ability θ<italic><sub>i</sub></italic> is typically assumed to have a <italic>N</italic>(0, 1) distribution, and <inline-formula id="inline-formula21-0013164411434638">
<mml:math display="inline" id="math31-0013164411434638">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>id</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>is assumed to be distributed as <italic>N</italic>(0, <inline-formula id="inline-formula22-0013164411434638">
<mml:math display="inline" id="math32-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>). The variance of <inline-formula id="inline-formula23-0013164411434638">
<mml:math display="inline" id="math33-0013164411434638">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>id</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> varies across testlets, and its value indicates the amount of local dependence in each testlet. As <inline-formula id="inline-formula24-0013164411434638">
<mml:math display="inline" id="math34-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>increases, the amount of local dependence increases. When <inline-formula id="inline-formula25-0013164411434638">
<mml:math display="inline" id="math35-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula>, the items within the testlet are conditionally independent.</p>
</sec>
<sec id="section8-0013164411434638">
<title>Simulation Study</title>
<sec id="section9-0013164411434638">
<title>Design of the Simulation Study</title>
<p>A simulation study was conducted to evaluate the use of Bayesian model comparison methods for selecting a model among alternative or competing GR models for performance assessment applications. The set of alternative models reflected sources of potential misfit for standard GR models and reflected models that may be theoretically more appropriate for some types of performance assessments.</p>
<p><xref ref-type="table" rid="table1-0013164411434638">Table 1</xref> presents the design and specific conditions used in this simulation, and <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref> presents item parameters for each GR model. For each condition, 20 response data sets were generated based on the data-generating model (<italic>Mg</italic>) with each data set containing responses for 2,000 simulated examinees to 15 polytomous items with 5 response categories. The test length and the number of response categories were fixed at typical values in performance assessment applications. Note that performance assessments are comprised commonly of forms with fewer items than multiple-choice type tests (<xref ref-type="bibr" rid="bibr16-0013164411434638">Lane &amp; Stone, 2006</xref>). A large sample size of 2,000 was used to ensure that model parameters were estimated precisely, and model selection results would not be affected by any inaccuracy in model parameter estimation.</p>
<table-wrap id="table1-0013164411434638" position="float">
<label>Table 1.</label>
<caption><p>Design and Conditions in this Simulation Study</p></caption>
<graphic alternate-form-of="table1-0013164411434638" xlink:href="10.1177_0013164411434638-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Data generating model (<italic>Mg</italic>)</th>
<th align="center">Data analysis model (<italic>Ma</italic>)</th>
<th align="center">Condition Number</th>
</tr>
</thead>
<tbody>
<tr>
<td>Two-parameter unidimensional GR (2P-GR)</td>
<td>2P-GR vs. 1P-GR vs. RS</td>
<td>1</td>
</tr>
<tr>
<td>Two-dimensional simple-structure GR (2D simple-GR)</td>
<td>2P-GR vs. 2D simple-GR</td>
<td>2</td>
</tr>
<tr>
<td>Two-dimensional complex-structure GR (2D complex-GR)</td>
<td>2P-GR vs. 2D complex-GR</td>
<td>3</td>
</tr>
<tr>
<td>Testlet GR</td>
<td>2P-GR vs. testlet GR</td>
<td>4</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013164411434638">
<p><italic>Note</italic>. GR = graded response; 1P-GR = 1-parameter unidimensional GR model; RS = rating scale model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table2-0013164411434638" position="float">
<label>Table 2.</label>
<caption><p>Item Parameters for Alternative Graded Response (GR) Models Under Conditions 1 to 4</p></caption>
<graphic alternate-form-of="table2-0013164411434638" xlink:href="10.1177_0013164411434638-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="5">2P-GR and testlet GR<hr/></th>
<th align="center" colspan="2">2D simple-GR<hr/></th>
<th align="center" colspan="2">2D complex-GR<hr/></th>
</tr>
<tr>
<th align="left">Item</th>
<th align="center">a</th>
<th align="center">b1</th>
<th align="center">b2</th>
<th align="center">b3</th>
<th align="center">b4</th>
<th align="center">a1</th>
<th align="center">a2</th>
<th align="center">a1</th>
<th align="center">a2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1.0</td>
<td>−2.0</td>
<td>−1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.5</td>
</tr>
<tr>
<td>2</td>
<td>1.0</td>
<td>−1.5</td>
<td>−0.5</td>
<td>0.5</td>
<td>1.5</td>
<td>1.7</td>
<td>0.0</td>
<td>1.0</td>
<td>0.5</td>
</tr>
<tr>
<td>3</td>
<td>1.0</td>
<td>−1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>2.0</td>
<td>2.4</td>
<td>0.0</td>
<td>1.0</td>
<td>0.5</td>
</tr>
<tr>
<td>4</td>
<td>1.0</td>
<td>−3.0</td>
<td>−1.5</td>
<td>−0.5</td>
<td>1.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.5</td>
</tr>
<tr>
<td>5</td>
<td>1.0</td>
<td>−1.0</td>
<td>0.5</td>
<td>1.5</td>
<td>3.0</td>
<td>1.7</td>
<td>0.0</td>
<td>1.0</td>
<td>0.5</td>
</tr>
<tr>
<td>6</td>
<td>1.7</td>
<td>−2.0</td>
<td>−1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>2.4</td>
<td>0.0</td>
<td>1.7</td>
<td>0.0</td>
</tr>
<tr>
<td>7</td>
<td>1.7</td>
<td>−1.5</td>
<td>−0.5</td>
<td>0.5</td>
<td>1.5</td>
<td>1.0</td>
<td>0.0</td>
<td>1.7</td>
<td>0.0</td>
</tr>
<tr>
<td>8</td>
<td>1.7</td>
<td>−1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>2.0</td>
<td>1.7</td>
<td>0.0</td>
<td>1.7</td>
<td>0.0</td>
</tr>
<tr>
<td>9</td>
<td>1.7</td>
<td>−3.0</td>
<td>−1.5</td>
<td>−0.5</td>
<td>1.0</td>
<td>0.0</td>
<td>2.4</td>
<td>1.7</td>
<td>0.0</td>
</tr>
<tr>
<td>10</td>
<td>1.7</td>
<td>−1.0</td>
<td>0.5</td>
<td>1.5</td>
<td>3.0</td>
<td>0.0</td>
<td>1.0</td>
<td>1.7</td>
<td>0.0</td>
</tr>
<tr>
<td>11</td>
<td>2.4</td>
<td>−2.0</td>
<td>−1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>0.0</td>
<td>1.7</td>
<td>2.4</td>
<td>0.0</td>
</tr>
<tr>
<td>12</td>
<td>2.4</td>
<td>−1.5</td>
<td>−0.5</td>
<td>0.5</td>
<td>1.5</td>
<td>0.0</td>
<td>2.4</td>
<td>2.4</td>
<td>0.0</td>
</tr>
<tr>
<td>13</td>
<td>2.4</td>
<td>−1.0</td>
<td>0.0</td>
<td>1.0</td>
<td>2.0</td>
<td>0.0</td>
<td>1.0</td>
<td>2.4</td>
<td>0.0</td>
</tr>
<tr>
<td>14</td>
<td>2.4</td>
<td>−3.0</td>
<td>−1.5</td>
<td>−0.5</td>
<td>1.0</td>
<td>0.0</td>
<td>1.7</td>
<td>2.4</td>
<td>0.0</td>
</tr>
<tr>
<td>15</td>
<td>2.4</td>
<td>−1.0</td>
<td>0.5</td>
<td>1.5</td>
<td>3.0</td>
<td>0.0</td>
<td>2.4</td>
<td>2.4</td>
<td>0.0</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Condition 1 was used to evaluate the effectiveness of the three model comparison methods in discriminating between the 2P-GR, 1P-GR, and RS models for the unidimensional responses simulated under the 2P-GR model. As shown in <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref>, the configuration of item parameters for the 2P-GR model involved a combination of three slope parameters: 1.0, 1.7, and 2.4 (reflecting low, average, and high discrimination) and five threshold parameter configurations (reflecting different difficulty levels): (a) −2.0, −1.0, 0.0, 1.0; (b) −1.5, −0.5, 0.5, 1.5; (c) −1.0, 0.0, 1.0, 2.0; (d) −3.0, −1.5, −0.5, 1.0; (e) −1.0, 0.5, 1.5, 3.0. Ability parameters were randomly simulated from a <italic>N</italic>(0, 1) distribution.</p>
<p>In Condition 2, the simulated test measured two dimensions but each item measured only one dimension (i.e., simple-structure case). Specifically, the first eight items were designed to measure one dimension, and the remaining seven items measured the second dimension. As shown in <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref>, the slope parameters on Dimension 1 (<italic>a<sub>1</sub></italic>) ranged from 1.0 to 2.4 for Items 1 to 8, and 0s for Items 9 to 15. The slope parameters on Dimension 2 (<italic>a<sub>2</sub></italic>) were 0 for Items 1 to 8 and ranged from 1.0 to 2.4 for Items 9 to 15. The threshold parameters for these 15 items were the same as for the 2P-GR model. Ability parameters on two dimensions were randomly selected from a bivariate normal distribution (0, 1) with a fixed correlation of 0.60, a correlation that may represent a value typical for large-scale performance assessments (cf., <xref ref-type="bibr" rid="bibr17-0013164411434638">Lane, Stone, Ankenmann, &amp; Liu, 1995</xref>). The responses were simulated based on the multidimensional GR model, and the three methods were used to compare the fit of the unidimensional 2P-GR model and the two-dimensional simple-structure (2D simple) GR model.</p>
<p>For Condition 3, two-dimensional complex-structure (2D complex case) responses were simulated to reflect a performance assessment that not only measures a dominant ability (e.g., math word problems) but also consists of items that measure a nuisance or construct-irrelevant dimension (e.g., reading). All 15 items measured the dominant dimension, but the first 5 items were designed to also measure a nuisance dimension (see <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref>). The degree to which item performance depended on the nuisance ability was captured by the ratio of the slope (<italic>a</italic><sub>1</sub>) for the dominant ability to the slope (<italic>a</italic><sub>2</sub>) for the nuisance ability. The ratio of <italic>a</italic><sub>2</sub> to <italic>a</italic><sub>1</sub> was set to 0.5 for the first five items. The thresholds (<italic>b</italic><sub>1</sub><italic>-b</italic><sub>4</sub>) and slope parameters for the dominant ability (<italic>a</italic><sub>1</sub>) were the same as for the 2P-GR model (see <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref>). The values of <italic>a</italic><sub>2</sub> were 0.5 for Items 1 to 5 and 0.0 for other items. Ability parameters for two dimensions were randomly selected from a bivariate normal distribution (0, 1) with a correlation of 0.3. A low level of correlation was used because the second dimension was assumed to be either a nuisance dimension or construct-irrelevant dimension. The responses were simulated based on the multidimensional GR model, and the three methods were used to compare the fit of the unidimensional 2P-GR model and the 2D complex GR model.</p>
<p>In Condition 4, responses to a test consisting of a testlet were generated under the modified GR model for testlets. As shown in <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref>, item parameters for this testlet GR model were the same as that for the 2P-GR model, except one testlet was simulated (Items 6, 7, and 8). Ability parameters were randomly selected from <italic>N</italic>(0, 1), and the testlet effect <inline-formula id="inline-formula26-0013164411434638">
<mml:math display="inline" id="math36-0013164411434638">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>jd</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> was randomly selected from a <italic>N</italic>(0, <inline-formula id="inline-formula27-0013164411434638">
<mml:math display="inline" id="math37-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>) for the items in the testlet. The variance of the testlet effect <inline-formula id="inline-formula28-0013164411434638">
<mml:math display="inline" id="math38-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> was specified at 1.0 reflecting a moderate degree of dependence among the testlet items (<xref ref-type="bibr" rid="bibr5-0013164411434638">Bradlow, Wainer, &amp; Wang, 1999</xref>; <xref ref-type="bibr" rid="bibr19-0013164411434638">Li et al., 2006</xref>; <xref ref-type="bibr" rid="bibr32-0013164411434638">Wang et al., 2002</xref>).</p>
<p>For each condition, 20 data sets of 2,000 item responses were generated using SAS based on the <italic>Mg</italic> model described in <xref ref-type="table" rid="table1-0013164411434638">Table 1</xref>. Using the specified model parameters (i.e., item and ability), the probabilities of each simulated examinee responding to each item response category were calculated, and these probabilities were used to obtain simulated discrete item responses (see <xref ref-type="bibr" rid="bibr36-0013164411434638">Zhu, 2009</xref>, for complete results). For each of the generated data sets within each condition, different analysis models (<italic>Ma</italic>) were estimated, and the three model comparison methods (DIC, CPO, and PPMC) were then used to compare those models. A preferred model was selected based on each method. The effectiveness of these three methods was measured by the number of times the <italic>Mg</italic> was selected as the preferred model across the 20 replications.</p>
<p>In addition to test-level model comparisons within replications, item-level results for both CPO and the discrepancy measures used with the PPMC method were examined. While test-level results are useful in the model comparison process, item-level results may provide diagnostic information regarding the specific misfitting items, which in turn may suggest alternative models.</p>
</sec>
<sec id="section10-0013164411434638">
<title>Bayesian Estimation of Different Models</title>
<p>All the GR models in this study were estimated using MCMC methods and WinBUGS 1.4 (<xref ref-type="bibr" rid="bibr30-0013164411434638">Spiegelhalter et al., 2003</xref>). For the standard 2P-GR model, the following priors were specified: θ<sub><italic>i</italic></sub>~Normal(0, 1) for Person <italic>a</italic><sub><italic>i</italic></sub>~Log normal(0, 1), <italic>b</italic><sub><italic>j</italic>1</sub>~Normal(0, 0.25), and <italic>b</italic><sub><italic>j</italic>(<italic>k</italic>+1)</sub>~Normal(0, 0.25)<italic>I</italic>(<italic>b</italic><sub><italic>jk</italic></sub>) for Item <italic>j</italic>, where the notation <italic>I</italic>(<italic>b</italic><sub><italic>jk</italic></sub>) indicates that the threshold <italic>b</italic><sub><italic>j</italic>(<italic>k</italic>+1)</sub> was sampled to be larger than <italic>b</italic><sub><italic>jk</italic></sub> as required under the GR model. For the 1P-GR model, all priors were the same as for the 2P-GR model except that one slope was estimated. For the RS model, the priors were θ<sub><italic>i</italic></sub>~Normal(0, 1) for Person <italic>i</italic>, <italic>a</italic><sub><italic>i</italic></sub>~Log normal(0, 1) and <italic>b</italic><sub><italic>j</italic></sub>~Normal(0, 0.25) for Item <italic>j</italic>, and <italic>c</italic><sub>1</sub>~Log normal(0, 1) and <italic>c</italic><sub>(<italic>k</italic>+1)</sub>~Normal(0, 0.25)<italic>I</italic>(<italic>c</italic><sub><italic>k</italic></sub>) with a constraint of <inline-formula id="inline-formula29-0013164411434638">
<mml:math display="inline" id="math39-0013164411434638">
<mml:mrow>
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:munder>
<mml:msub>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula>. It should be noted that, consistent with WinBUGS, precision parameters that are the inverse of variance parameters were used in these prior distribution specifications. These prior specifications were similar to those used in previous research (e.g., <xref ref-type="bibr" rid="bibr13-0013164411434638">Kang et al., 2009</xref>).</p>
<p>For the 2D simple-GR model, the prior distributions for item parameters were specified as for the 2P-GR model. The abilities on two dimensions were assigned multivariate normal priors, with means of 0 and variances of 1, and the correlation between two abilities was assigned a normal prior with mean equal to the true correlation of 0.6 and variance of 0.25. This approach was used to address the metric indeterminacy problem in a manner similar to that used by <xref ref-type="bibr" rid="bibr33-0013164411434638">Yao and Boughton (2007)</xref>.</p>
<p>For estimating the 2D complex-GR model, it is important to solve both metric indeterminacy and rotational indeterminacy. As for 2D simple-GR model, the metric indeterminacy problem was addressed by assigning the abilities on two dimension means of 0 and variances of 1. To solve the rotational indeterminacy issue, the two ability axes were constrained to be orthogonal, and the slope parameters for the nuisance dimension were fixed at 0 for the last 10 items (items measuring the dominant dimension only). Thus, any relationship between the dimensions was derived from the items that were common to both dimensions rather than from a specified correlation between dimensions (see <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref>, Condition 3). Priors for abilities followed a multivariate normal distribution with means 0 and a variance–covariance matrix equal to the identity matrix. These approaches were similar to those used by <xref ref-type="bibr" rid="bibr4-0013164411434638">Bolt and Lall (2003)</xref>. Prior distributions for other item parameters were the same as defined for the 2P-GR model. It should be noted that although use of an orthogonal solution affects the evaluation of item parameter recovery, model-based expectations (i.e., response category probabilities) are invariant to rotations (e.g., orthogonal vs. oblique). Thus, the solution for rotational indeterminacy should not affect results evaluating model fit and model comparisons.</p>
<p>For the testlet GR model, prior distributions for the item parameters and the examinees’ abilities were specified as for the GR models. The testlet effect was assigned a normal prior with mean of 0 and random variance of <inline-formula id="inline-formula30-0013164411434638">
<mml:math display="inline" id="math40-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>. As in <xref ref-type="bibr" rid="bibr5-0013164411434638">Bradlow et al. (1999)</xref> and <xref ref-type="bibr" rid="bibr19-0013164411434638">Li et al. (2006)</xref>, the hyperparameter <inline-formula id="inline-formula31-0013164411434638">
<mml:math display="inline" id="math41-0013164411434638">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> was specified as an inverse chi-square distribution with degrees of freedom equal to 0.5, indicating a lack of information about this parameter.</p>
<p>Convergence of the Markov chain for each GR model was evaluated using multiple diagnostics, such as history plots, autocorrelation plots, and multiple chains to ensure that target posterior distributions were being sampled (<xref ref-type="bibr" rid="bibr15-0013164411434638">Kim &amp; Bolt, 2007</xref>). Based on the results (see <xref ref-type="bibr" rid="bibr36-0013164411434638">Zhu, 2009</xref>, for details), one chain of 5,000 iterations was run for the 2P-GR, 1P-GR, RS, and testlet GR models, and the first 3,000 iterations were discarded. The remaining 2,000 iterations were thinned by taking every other iteration to reduce serial dependencies across iterations. The resulting 1,000 iterations formed the posterior sample and were used in estimating model parameters and conducting model comparisons. For the 2D simple-GR and 2D complex-GR models, a longer chain of 8,000 iterations was run. The first 5,000 iterations were discarded, and the remaining 3,000 iterations were thinned by taking every third iteration to obtain a posterior sample of size 1,000.</p>
<p>Convergence was also assessed by examining parameter recovery for each model using the root mean square error (RMSE) between the true (i.e., generating) and posterior estimates or mean values from the posterior distributions across the 20 replications. Thus, the RMSEs for a specific model were obtained when the data-generating model was the same as the analysis model (i.e., <italic>Mg</italic> = <italic>Ma</italic>). RMSE results indicated that the parameters were recovered well (see <xref ref-type="bibr" rid="bibr36-0013164411434638">Zhu, 2009</xref>, for complete results). For example, the average RMSE across all 15 items was 0.07 for both slope and threshold parameters for the 2P-GR model; for the 2D simple-GR model, the RMSE for the interdimensional correlation was 0.016; and for the testlet GR model, the RMSE for the testlet variance was 0.037. The low RMSE values indicated that a posterior sample of 1,000 was adequate for accurate recovery of item parameters for each GR model.</p>
</sec>
<sec id="section11-0013164411434638">
<title>Computation of Model Comparison Criteria</title>
<p>Estimates of the DIC index for different models were directly available from WinBUGS. The computation of the CPO index was implemented by first computing the CPO at the level of an individual item response (CPO<italic><sub>ij</sub></italic>) in WinBUGS by using,</p>
<p><disp-formula id="disp-formula11-0013164411434638">
<mml:math display="block" id="math42-0013164411434638">
<mml:mrow>
<mml:mi>CP</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>O</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:mfrac>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>t</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>η</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mfrac>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula11-0013164411434638" xlink:href="10.1177_0013164411434638-eq11.tif"/>
</disp-formula></p>
<p>where <italic>y</italic><sub><italic>ij</italic></sub> is the response of an examinee <italic>i</italic> on a particular Item <italic>j, T</italic> is the total number of MCMC draws after the chain has converged, and <inline-formula id="inline-formula32-0013164411434638">
<mml:math display="inline" id="math43-0013164411434638">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>η</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> is the likelihood of the observed item response <italic>y</italic><sub><italic>ij</italic></sub> based on the sampled parameter values at draw <italic>t</italic>. An item-level CPO index for a model was obtained in SAS by taking the product of the values of <italic>CPO<sub>ij</sub></italic> across all examinees: <inline-formula id="inline-formula33-0013164411434638">
<mml:math display="inline" id="math44-0013164411434638">
<mml:mrow>
<mml:mi>CP</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>O</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>Π</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mi>CP</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>O</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, where <italic>N</italic> is the total number of examinees. In addition, a CPO index for the overall test was computed by taking the product of the item-level CPO<italic><sub>j</sub></italic> across all items: <inline-formula id="inline-formula34-0013164411434638">
<mml:math display="inline" id="math45-0013164411434638">
<mml:mrow>
<mml:mi>CPO</mml:mi>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>Π</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>I</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mi>CP</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>O</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, where <italic>I</italic> is the total number of items on the test. The logarithm of the CPO index was used for comparing models in this study. Recall that the preferred model was the model with the smaller DIC value or the larger CPO value.</p>
<p>The different models in each condition were also compared using the PPMC method. All six discrepancy measures discussed previously were used with PPMC for Condition 1. However, for Conditions 2 to 4, only the two pair-wise discrepancy measures (<italic>global OR</italic> and <italic>Yen’s Q<sub>3</sub></italic> index) were used since these types of measures have been found most useful for detecting multidimensionality or local dependence (e.g., <xref ref-type="bibr" rid="bibr18-0013164411434638">Levy et al., 2009</xref>; <xref ref-type="bibr" rid="bibr28-0013164411434638">Sinharay et al., 2006</xref>; <xref ref-type="bibr" rid="bibr37-0013164411434638">Zhu &amp; Stone, 2011</xref>). To compare different models using PPMC, the frequency of extreme PPP values (i.e., values &lt;0.05 or &gt;0.95 as discussed earlier) for the selected measures was computed for each model across items or item pairs. When <italic>Ma</italic> = <italic>Mg</italic>, it was expected that no or few extreme PPP values would be observed. When the alternative model was estimated, however, more items or item pairs with extreme PPP values would be expected. The preferred model was the model with the fewest number of items and item pairs with extreme PPP values. Note that among all the six discrepancy measures used in this study, three measures (the <italic>item score distribution</italic>, the <italic>global OR</italic>, and <italic>Yen’s Q3</italic>) and their PPP values were computed in WinBUGS, and the other three measures (the <italic>item–total score correlation, Yen’s Q1</italic>, and the <italic>pseudo-count fit statistic</italic>) were computed in SAS. The interested reader can consult (<xref ref-type="bibr" rid="bibr36-0013164411434638">Zhu, 2009</xref>) for the SAS and WinBUGS code to compute these various measures.</p>
</sec>
</sec>
<sec id="section12-0013164411434638" sec-type="results">
<title>Results</title>
<sec id="section13-0013164411434638">
<title>Condition 1 (2P-GR vs. 1P-GR vs. RS Models)</title>
<p><xref ref-type="table" rid="table3-0013164411434638">Table 3</xref> presents the average values (Mean) for the model comparison indices as well as the frequencies (Freq) that each model was selected as the preferred model across the 20 replications. For the item-level discrepancy measures (i.e., <italic>item score distribution, item–total score correlation, Yen’s Q1, pseudo-count fit statistic</italic>), the means represent the average numbers of extreme PPP values (i.e., &lt;0.05 or &gt;0.95) across the 15 items and the 20 replications. For the pairwise measures (<italic>global OR</italic> and <italic>Yen’s Q3</italic>), the means represent the average number of extreme PPP values across the 105 item pairs and replications. Note that these values were all rounded to the nearest integer.</p>
<table-wrap id="table3-0013164411434638" position="float">
<label>Table 3.</label>
<caption><p>Model Selection Results Across Items or Item Pairs (Overall Test): Condition 1</p></caption>
<graphic alternate-form-of="table3-0013164411434638" xlink:href="10.1177_0013164411434638-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">DIC<hr/></th>
<th align="center" colspan="2">CPO<hr/></th>
<th align="center" colspan="2">Item score distribution<hr/></th>
<th align="center" colspan="2">Item–total correlation<hr/></th>
<th align="center" colspan="2">Yen’s <italic>Q</italic><sub>1</sub><hr/></th>
<th align="center" colspan="2">Pseudo-count fit statistic<hr/></th>
<th align="center" colspan="2">Global OR<hr/></th>
<th align="center" colspan="2">Yen’s <italic>Q</italic><sub>3</sub><hr/></th>
</tr>
<tr>
<th align="left">Model</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
</tr>
</thead>
<tbody>
<tr>
<td>2P-GR<sup><xref ref-type="table-fn" rid="table-fn3-0013164411434638">a</xref></sup></td>
<td>73960</td>
<td>20</td>
<td>−16076</td>
<td>20</td>
<td>0</td>
<td>20</td>
<td>0</td>
<td>20</td>
<td>0</td>
<td>20</td>
<td>0</td>
<td>20</td>
<td>6</td>
<td>20</td>
<td>7</td>
<td>20</td>
</tr>
<tr>
<td>1P-GR</td>
<td>75498</td>
<td>0</td>
<td>−16405</td>
<td>0</td>
<td>0</td>
<td>20</td>
<td>12</td>
<td>0</td>
<td>10</td>
<td>0</td>
<td>10</td>
<td>0</td>
<td>79</td>
<td>0</td>
<td>48</td>
<td>0</td>
</tr>
<tr>
<td>RS</td>
<td>74484</td>
<td>0</td>
<td>−16190</td>
<td>0</td>
<td>14</td>
<td>0</td>
<td>13</td>
<td>0</td>
<td>8</td>
<td>0</td>
<td>13</td>
<td>0</td>
<td>31</td>
<td>0</td>
<td>13</td>
<td>0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013164411434638">
<p><italic>Note</italic>. DIC = deviance information criterion; CPO = conditional predictive ordinate; GR = graded response; RS = rating scale model; Freq = frequency; OR = odds ratio; 2P-GR = two-parameter GR model; 1P-GR = one-parameter version of the GR model.</p>
</fn>
<fn id="table-fn3-0013164411434638">
<label>a.</label>
<p>The model was the data-generating (i.e., true) model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>As shown in this table, the mean DIC value was the smallest for the true model (i.e., 2P-GR) and the largest for the 1P-GR model. Similarly, the mean test-level CPO value was the largest for the true model and the smallest for the 1P-GR model. Thus, both DIC and CPO results indicated that the 2P-GR model fit the data better than the RS model, which in turn fit the data better than the 1P-GR model. Moreover, the generating or true model was consistently selected as the preferred model across the 20 replications using the DIC and CPO indices.</p>
<p>Regarding the PPMC results in <xref ref-type="table" rid="table3-0013164411434638">Table 3</xref>, the 2P-GR model had the lowest average values for all discrepancy measures, providing evidence that the 2P-GR model was preferred. For example, for the <italic>item-total correlation</italic> measure, no item had extreme PPP values for the true model (Mean = 0). However, across the 15 items, there were an average of 12 items with extreme PPP values for the 1P-GR model, and an average of 13 items with extreme PPP values for the RS model. For <italic>Yen’s Q<sub>3</sub></italic> measure, across 105 item pairs an average of 7 pairs had extreme PPP values for the true model, but an average of 48 and 13 item pairs had extreme PPP values for the 1P-GR and RS models, respectively. In addition, all measures, except the <italic>item score distribution</italic> measure, consistently identified the true model as the preferred model over the 20 replications. Using the <italic>item score distribution</italic> measure, both the 2P-GR and 1P-GR models were identified as preferred models since no items with extreme PPP values were observed for either model. This result indicated that this measure may be ineffective in discriminating between 2P- and 1P-GR models. Since this measure does not compare observed and expected values at different discrete values of ability, as in <italic>Yen’s Q<sub>1</sub></italic> and the <italic>pseudo-count fit statistic</italic>, it is not sensitive to discrepancies between observed and expected values that may exist at specific ability levels. Furthermore, positive and negative discrepancies may cancel each other when the discrepancies are collapsed across ability levels as in the <italic>item score distribution statistic</italic>.</p>
<p>The previous comparisons focused on the selection of a preferred model for the overall test. Although overall model comparisons are useful, comparing models at the item level can provide additional diagnostic information. For example, information regarding item-level fit could be used to identify a subset of items that should be reexamined, modified, or replaced. Among the Bayesian comparison methods considered, CPO and PPMC can be used to compare the models at both test- and item-levels. For Condition 1, the item-level CPO results (not shown here) indicated that the 2P-GR model was preferred for each of the 15 items. With respect to the item-level PPMC results, median PPP values for all four item-level discrepancy measures were around 0.50 for the 2P-GR model. In contrast, extreme PPP values were observed for a majority of items when the 1P-GR or RS models were estimated with all but the <italic>item score distribution</italic> discrepancy measure. The differences in the numbers of extreme PPP values between the models indicated that the 2P-GR was preferred.</p>
<p>It is worthwhile to note that both the DIC and CPO indices are used to compare the relative fit of different models. Thus, they may be used to identify which model among a set of candidate models provides a better fit to the data, but they may not be used to evaluate the degree of fit in an absolute sense. When one or more models among the set of candidate models are appropriate, the better fitting model can be selected by using either DIC or CPO index. However, when the models to be compared are not appropriate or do not fit the data, using these indices may be misleading. For example, if only the 1P-GR and RS models were compared using the DIC or CPO indices for Condition 1, the RS model would be preferred over the 1P-GR model based on the results in <xref ref-type="table" rid="table3-0013164411434638">Table 3</xref>. However, the RS model is not really appropriate since the true model was the 2P-GR model.</p>
<p>In contrast to the DIC and CPO indices, the PPMC method can be also used to evaluate the absolute fit of different models. As shown in <xref ref-type="table" rid="table3-0013164411434638">Table 3</xref>, all effective discrepancy measures had a number of extreme PPP values for the 1P-GR and RS models, indicating that neither of these models fit the simulated 2P-GR data. As expected, no or few PPP values were extreme for the estimated 2P-GR model. This indicated that the 2P model was not only preferred over the other two models, but it also fit the data.</p>
</sec>
<sec id="section14-0013164411434638">
<title>Condition 2 (2P-GR vs. 2D Simple-GR Models)</title>
<p><xref ref-type="table" rid="table4-0013164411434638">Table 4</xref> (see Condition 2) presents the results comparing the estimation of a unidimensional 2P-GR model with a 2D simple-GR model (true or generating model) for the simulated 2D simple-structure data. The table provides mean values for each model comparison index, as well as the frequencies the true model (i.e., 2D simple-GR) was preferred across the 20 replications.</p>
<table-wrap id="table4-0013164411434638" position="float">
<label>Table 4.</label>
<caption><p>Model Selection Results Across Items (Overall Test): Conditions 2 to 4</p></caption>
<graphic alternate-form-of="table4-0013164411434638" xlink:href="10.1177_0013164411434638-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="2">DIC<hr/></th>
<th align="center" colspan="2">CPO<hr/></th>
<th align="center" colspan="2">PPMC (global OR)<hr/></th>
<th align="center" colspan="2">PPMC (Yen’s <italic>Q</italic><sub>3</sub>)<hr/></th>
</tr>
<tr>
<th align="left">Condition</th>
<th align="center">Model</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
<th align="center">Mean</th>
<th align="center">Freq</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>2D simple-GR<sup><xref ref-type="table-fn" rid="table-fn5-0013164411434638">a</xref></sup></td>
<td>75434</td>
<td>20</td>
<td>−16430</td>
<td>20</td>
<td>7</td>
<td>20</td>
<td>4</td>
<td>20</td>
</tr>
<tr>
<td/>
<td>2P-GR</td>
<td>78854</td>
<td>0</td>
<td>−17143</td>
<td>0</td>
<td>75</td>
<td>0</td>
<td>102</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>2D complex-GR<sup><xref ref-type="table-fn" rid="table-fn5-0013164411434638">a</xref></sup></td>
<td>71905</td>
<td>20</td>
<td>−15645</td>
<td>20</td>
<td>4</td>
<td>18</td>
<td>4</td>
<td>20</td>
</tr>
<tr>
<td/>
<td>2P-GR</td>
<td>72093</td>
<td>0</td>
<td>−15670</td>
<td>0</td>
<td>8</td>
<td>2</td>
<td>15</td>
<td>0</td>
</tr>
<tr>
<td>4</td>
<td>Testlet GR<sup><xref ref-type="table-fn" rid="table-fn5-0013164411434638">a</xref></sup></td>
<td>74170</td>
<td>20</td>
<td>−16145</td>
<td>20</td>
<td>6</td>
<td>18</td>
<td>5</td>
<td>20</td>
</tr>
<tr>
<td/>
<td>2P-GR</td>
<td>74924</td>
<td>0</td>
<td>−16287</td>
<td>0</td>
<td>10</td>
<td>2</td>
<td>21</td>
<td>0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0013164411434638">
<p><italic>Note</italic>. DIC = deviance information criterion; CPO = conditional predictive ordinate; PPMC = posterior predictive model checking; OR = odds ratio; GR = graded response; 2P = two parameter; 2D = two-dimensional; Freq = frequency.</p>
</fn>
<fn id="table-fn5-0013164411434638">
<label>a.</label>
<p>The model was the data-generating (i.e., true) model for each condition.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The lower DIC value and the higher CPO value for the 2D simple-GR model indicated that the true model was preferred for the overall test. For the PPMC results, on average, only 7 out of 105 item pairs with extreme PPP values for the <italic>global OR</italic> measure (or 4 for <italic>Yen’s Q<sub>3</sub></italic> index) were observed when the true model was used to analyze the data. However, when the unidimensional 2P-GR model was estimated, there were a large number of item pairs with extreme PPP values—75 and 102 pairs for the <italic>global OR</italic> and <italic>Yen’s Q<sub>3</sub></italic> index, respectively. Thus, the PPMC results indicated that the two-dimensional model was preferred over the unidimensional GR model and also indicated that this model provided adequate model fit with respect to associations among item-pair responses. Also, it is apparent that the three methods appeared to perform equally well with regard to the frequency of selecting the two-dimensional GR model as the preferred model at the overall test level. All the indices identified the true model for each of the 20 replications.</p>
<p>As for Condition 1, item-level results were examined to diagnose any particular source of model misfit. The item-level CPO results are presented in <xref ref-type="table" rid="table5-0013164411434638">Table 5</xref> (see Condition 2), including the mean CPO values for each estimated model as well as the frequencies the true model was selected as the preferred model across the 20 replications. For each item, the average CPO value (across the 20 replications) for the two-dimensional model was larger than the value for the unidimensional model. This indicated that the two-dimensional model, which was preferred for the overall test, was also preferred for each item. The large differences for the item-level CPO values between the two models provided strong evidence that the two-dimensional model fit each item significantly better than the unidimensional model. In addition, the two-dimensional model was chosen as the preferred model for each of the 20 replications.</p>
<table-wrap id="table5-0013164411434638" position="float">
<label>Table 5.</label>
<caption><p>Item-Level CPO Index Results for Each Item: Conditions 2 to 4</p></caption>
<graphic alternate-form-of="table5-0013164411434638" xlink:href="10.1177_0013164411434638-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">Condition 2<hr/></th>
<th align="center" colspan="3">Condition 3<hr/></th>
<th align="center" colspan="3">Condition 4<hr/></th>
</tr>
<tr>
<th align="left">Item</th>
<th align="center">2D Simple-GR<sup><xref ref-type="table-fn" rid="table-fn7-0013164411434638">a</xref></sup></th>
<th align="center">2P-GR</th>
<th align="center">Freq</th>
<th align="center">2D Complex-GR<sup><xref ref-type="table-fn" rid="table-fn7-0013164411434638">a</xref></sup></th>
<th align="center">2P-GR</th>
<th align="center">Freq</th>
<th align="center">Testlet GR<sup><xref ref-type="table-fn" rid="table-fn7-0013164411434638">a</xref></sup></th>
<th align="center">2P-GR</th>
<th align="center">Freq</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>−1274</td>
<td>−1291</td>
<td>20</td>
<td>−1201</td>
<td>−1205</td>
<td>19</td>
<td>−1269</td>
<td>−1269</td>
<td>15</td>
</tr>
<tr>
<td>2</td>
<td>−1162</td>
<td>−1209</td>
<td>20</td>
<td>−1238</td>
<td>−1242</td>
<td>19</td>
<td>−1293</td>
<td>−1294</td>
<td>15</td>
</tr>
<tr>
<td>3</td>
<td>−998</td>
<td>−1077</td>
<td>20</td>
<td>−1209</td>
<td>−1212</td>
<td>19</td>
<td>−1266</td>
<td>−1267</td>
<td>16</td>
</tr>
<tr>
<td>4</td>
<td>−1214</td>
<td>−1231</td>
<td>20</td>
<td>−1076</td>
<td>−1081</td>
<td>20</td>
<td>−1119</td>
<td>−1120</td>
<td>14</td>
</tr>
<tr>
<td>5</td>
<td>−1001</td>
<td>−1044</td>
<td>20</td>
<td>−1071</td>
<td>−1076</td>
<td>20</td>
<td>−1204</td>
<td>−1204</td>
<td>16</td>
</tr>
<tr>
<td>6</td>
<td>−999</td>
<td>−1075</td>
<td>20</td>
<td>−1114</td>
<td>−1115</td>
<td>14</td>
<td>−1121</td>
<td>−1162</td>
<td>20</td>
</tr>
<tr>
<td>7</td>
<td>−1301</td>
<td>−1319</td>
<td>20</td>
<td>−1140</td>
<td>−1139</td>
<td>10</td>
<td>−1151</td>
<td>−1193</td>
<td>20</td>
</tr>
<tr>
<td>8</td>
<td>−1133</td>
<td>−1180</td>
<td>20</td>
<td>−1113</td>
<td>−1114</td>
<td>13</td>
<td>−1124</td>
<td>−1166</td>
<td>20</td>
</tr>
<tr>
<td>9</td>
<td>−848</td>
<td>−923</td>
<td>20</td>
<td>−983</td>
<td>−984</td>
<td>17</td>
<td>−986</td>
<td>−987</td>
<td>18</td>
</tr>
<tr>
<td>10</td>
<td>−1209</td>
<td>−1227</td>
<td>20</td>
<td>−979</td>
<td>−978</td>
<td>15</td>
<td>−986</td>
<td>−987</td>
<td>16</td>
</tr>
<tr>
<td>11</td>
<td>−1133</td>
<td>−1184</td>
<td>20</td>
<td>−961</td>
<td>−959</td>
<td>14</td>
<td>−968</td>
<td>−970</td>
<td>18</td>
</tr>
<tr>
<td>12</td>
<td>−1032</td>
<td>−1114</td>
<td>20</td>
<td>−988</td>
<td>−989</td>
<td>14</td>
<td>−991</td>
<td>−993</td>
<td>16</td>
</tr>
<tr>
<td>13</td>
<td>−1277</td>
<td>−1297</td>
<td>20</td>
<td>−958</td>
<td>−959</td>
<td>14</td>
<td>−962</td>
<td>−965</td>
<td>19</td>
</tr>
<tr>
<td>14</td>
<td>−1002</td>
<td>−1049</td>
<td>20</td>
<td>−807</td>
<td>−808</td>
<td>18</td>
<td>−811</td>
<td>−814</td>
<td>20</td>
</tr>
<tr>
<td>15</td>
<td>−845</td>
<td>−923</td>
<td>20</td>
<td>−806</td>
<td>−808</td>
<td>12</td>
<td>−814</td>
<td>−816</td>
<td>20</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0013164411434638">
<p><italic>Note</italic>. CPO = conditional predictive ordinate; GR = graded response; 2P = two-parameter; 2D = two-dimensional; Freq = frequency.</p>
</fn>
<fn id="table-fn7-0013164411434638">
<label>a.</label>
<p>The model was the data-generating (i.e., true) model for each condition.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>With regard to the PPMC results, pie plots were used rather than tables of PPP values to examine and explore patterns in item-level results. As an example, the pie plots in <xref ref-type="fig" rid="fig1-0013164411434638">Figure 1</xref> (Condition 2) provide median PPP values for <italic>Yen’s Q<sub>3</sub></italic> measure when items responses were simulated under a 2D simple-GR model and two competing models were estimated: the 2P-GR model and the 2D simple-GR model. In each plot, there is one pie for each item pair, and the proportion of a circle that is filled corresponds to the magnitude of median PPP value across the 20 replications. An empty pie reflects a PPP-value of 0.0, whereas a fully filled pie reflects a PPP value of 1.0. As shown from the pie plots, when a unidimensional GR model was estimated (upper left plot), median PPP values for all item pairs were extreme and the items fell into two clusters—Items 1 to 8 in one, and Items 9 to 15 in another. This pattern indicated that a unidimensional model did not fit the data and a two-dimensional model might be considered more appropriate to model the data. As expected, when a two-dimensional model (i.e., true model) was estimated (upper right plot), median PPP values were around 0.5 for all pairs, suggesting this model fit the data. Pie plots based on the <italic>global OR</italic> measure exhibited a similar pattern.</p>
<fig id="fig1-0013164411434638" position="float">
<label>Figure 1.</label>
<caption><p>Display of median PPP values for item pairs for <italic>Yen’s Q</italic><sub>3</sub> for Conditions 2 to 4</p>
<p><italic>Note</italic>. PPP = posterior predictive <italic>p</italic>; 2P = two-parameter; GR = graded response; 2D = two-dimensional.</p></caption>
<graphic xlink:href="10.1177_0013164411434638-fig1.tif"/>
</fig>
</sec>
<sec id="section15-0013164411434638">
<title>Condition 3 (2P-GR vs. 2D Complex-GR Models)</title>
<p>In Condition 3, the generated data were 2D with complex structure. Items 1 to 5 measured a dominant dimension as well as a nuisance dimension, and Items 6 to 15 only measured the dominant dimension (see <xref ref-type="table" rid="table2-0013164411434638">Table 2</xref>). A unidimensional 2P-GR model and a two-dimensional GR model were estimated and compared.</p>
<p>The model comparison results for the overall test are presented in <xref ref-type="table" rid="table4-0013164411434638">Table 4</xref> (see Condition 3). The lower DIC value and higher CPO value for the 2D complex-GR model (true model) indicated that this multidimensional model was preferred over the unidimensional GR model. For the PPMC indices, 4 out of 105 item pairs demonstrated extreme PPP values for both pair-wise measures when the two-dimensional model was estimated. In contrast, when the unidimensional GR model was estimated, more item pairs demonstrated extreme PPP values—8 and 15 pairs for the <italic>global OR</italic> measure and <italic>Yen’s Q<sub>3</sub></italic> index, respectively.</p>
<p>The DIC index, the CPO index, and the PPMC method using <italic>Yen’s Q<sub>3</sub></italic> measure appeared to perform equally well with regard to the frequency of selecting the two-dimensional GR model as the preferred model. Based on the model comparisons for these indices, the true model was selected as the preferred model every time (20) over the 20 replications. However, when the <italic>global OR</italic> measure was used, the unidimensional GR model was selected incorrectly as the preferred model for 2 of the 20 replications. This example again illustrates that the choice of the discrepancy measure may affect the performance of the PPMC application in comparing different models.</p>
<p>The item-level CPO results (means and frequencies) for this condition are presented in <xref ref-type="table" rid="table5-0013164411434638">Table 5</xref> (see Condition 3). For Items 1 to 5, which measured both the dominant and nuisance dimensions, the item-level CPO identified the generating two-dimensional model as the preferred model for 19 to 20 times over the 20 replications. However, for the items that only measured the dominant dimension (Items 6-15), the two-dimensional model was identified as the preferred model with lower frequencies (10 to 18 over the 20 replications). This would be expected since the unidimensional GR model should be appropriate for these items.</p>
<p>It may be worthy to note that the differences between the item-level CPO values were quite small for the two models, particularly for the items reflecting one dimension (Items 6-15). <xref ref-type="bibr" rid="bibr30-0013164411434638">Spiegelhalter et al. (2003)</xref> discussed that any difference in DIC less than 5 units for two models may not indicate sufficient evidence to favor one model over another. Though there are no discussed guidelines available for CPO, the item-level CPO results for this condition may indicate that a difference of less than 3 units may not provide sufficient evidence supporting one model over another. However, the amount of difference in CPO necessary to suggest a significant difference between models needs further investigation.</p>
<p>The pie plots in <xref ref-type="fig" rid="fig1-0013164411434638">Figure 1</xref> (Condition 3) display median PPP values for <italic>Yen’s Q<sub>3</sub></italic> measure for this condition. All the PPP values were around 0.5 when the true 2D complex-GR model was estimated (middle right plot) and therefore provided evidence of model fit. In contrast, when the unidimensional GR model was estimated (middle left plot), all the PPP values were extreme for the item pairs involving the first five items, but around 0.5 for the other item pairs. This pattern indicated that the unidimensional GR model was not appropriate for Items 1 to 5, but was appropriate for Items 6 to 15. Additionally, the close to 0 PPP values for the item pairs among Items 1 to 5 indicated that the realized correlations among these five items were consistently larger than the predicted correlations under the unidimensional GR model.</p>
</sec>
<sec id="section16-0013164411434638">
<title>Condition 4 (2P-GR vs. Testlet GR Models)</title>
<p>In Condition 4, the responses to a test with locally dependent items (testlet) were simulated. Items 6, 7, and 8 were designed to reflect a testlet with moderate dependence among the items. <xref ref-type="table" rid="table4-0013164411434638">Table 4</xref> (see Condition 4) presents the results for comparing a GR model for testlets with a unidimensional GR model. The lower DIC value, the higher CPO value, and the fewer item pairs with extreme PPP values (i.e., values &lt;0.05 or &gt;0.95) for the testlet GR model all indicated that this complex model fit the overall test better than the unidimensional GR model. Furthermore, the DIC index, the CPO index, and the PPMC method using <italic>Yen’s Q<sub>3</sub></italic> measure appeared to perform equally well. Based on these measures, the testlet GR model was selected as the preferred model every time (20) over the 20 replications. However, when the <italic>global OR</italic> measure was used with PPMC, the testlet GR model was chosen as the preferred model for 18 of the 20 replications. As for Condition 3, use of <italic>Yen’s Q<sub>3</sub></italic> as a discrepancy measure appeared to be slightly more effective than use of the <italic>global OR</italic> measure.</p>
<p><xref ref-type="table" rid="table5-0013164411434638">Table 5</xref> (Condition 4) presents results for the item-level model comparisons. For the items in the testlet (Items 6, 7, and 8), the mean CPO values for the testlet GR model were much larger (~42 units) than the values for the unidimensional model, and the testlet GR model was chosen as the preferred model for all the 20 replications. For the simulated locally independent items, the mean CPO values for the two models were very close. The mean CPO values for the testlet GR model were less than 3 units larger than the values for the unidimensional GR model. Based on the CPO results for Condition 3, less than 3 units in difference may indicate insufficient evidence for selecting the complex testlet GR model as the preferred model. Nonetheless, if these small differences were considered evidence in favor of the testlet model, the testlet GR model would be selected as the preferred model for these items for 14 to 20 replications. Thus, more research is needed to determine what differences in CPO values should be used to select one model over another.</p>
<p>The two pie plots at the bottom of <xref ref-type="fig" rid="fig1-0013164411434638">Figure 1</xref> (see Condition 4) illustrate the median PPP values for <italic>Yen’s Q<sub>3</sub></italic> measure for the testlet and unidimensional GR models, respectively. As can be observed, when the testlet GR model was estimated (bottom right plot), all PPP values were around 0.5, suggesting model-data-fit for the testlet GR model. In contrast, when the unidimensional GR model was estimated (bottom left plot), all PPP values were extreme for item pairs reflecting local dependence (Items 6, 7, and 8), but around 0.5 for pairs among simulated locally independent items. Additionally, PPP values of nearly 0 for item pairs comprising the testlet indicated that the realized correlations among these items were consistently larger than predicted correlations under the unidimensional GR model. These results indicated that the unidimensional GR model was not appropriate for Items 6, 7, and 8, but was appropriate for the other items.</p>
<p>It should be noted that results for the 2D complex-structure model (Condition 3) and the testlet model (Condition 4) were very similar. As noted by a reviewer, this was because of the similarity between the 2D complex-structure model used herein and the testlet model. In both models all items measure a single primary dimension while a subset of items in both the 2D complex-structure and testlets models are related further because of another source of shared variance.</p>
</sec>
</sec>
<sec id="section17-0013164411434638" sec-type="discussion">
<title>Discussion</title>
<p>The purpose of this study was to evaluate the relative performance of three Bayesian model comparison methods (DIC, CPO, and PPMC) in selecting a preferred model among a set of alternative GR models for performance assessment applications. The alternative models considered in this study reflected sources of potential misfit and reflected more complex IRT models that might be theoretically more appropriate for some types of performance assessment data but require Bayesian estimation methods. For the conditions examined in this study, the results of this study indicated that these three methods appeared to be equally accurate in selecting the true model as the preferred model when considering all items simultaneously (test level). However, CPO and PPMC were found to be more informative than DIC since information about model fit was also available at the item level.</p>
<p>Consistent with previous studies (<xref ref-type="bibr" rid="bibr19-0013164411434638">Li et al., 2006</xref>; <xref ref-type="bibr" rid="bibr26-0013164411434638">Sinharay, 2005</xref>), the PPMC approach was found to be effective for performing model comparisons in the performance assessment context. Moreover, an advantage of PPMC applications is that they can be used to compare both the relative and absolute fit of different models. In contrast, the DIC and CPO model comparison indices only consider the relative fit of different models. If the true model is among the candidate models, DIC or CPO can be used to select the preferred model that should also fit the data. However, in practice, it is not known whether the true model is included in the set of candidate models. Thus, it is important that the assessment of absolute model fit be combined with model comparison methods. Ideally, model selection (e.g., DIC/CPO) and model fit (PPMC) methods should both be used for real testing applications. As a result of their simplicity, DIC or CPO could be first used to choose a preferred model from a set of candidate models. Using multiple discrepancy measures, the PPMC method could then be applied to evaluate the fit of the selected model at the test and item levels. In this sense, these methods are not competing in nature, but are instead complementary.</p>
<p>Contrary to previous studies, DIC, CPO, and PPMC appeared to perform equally well in this study, whereas their performance has been found in previous research to depend on specific conditions as well as models compared. This finding may be due to the large sample size used in this study that helped ensure accuracy in model parameter estimation. As discussed in <xref ref-type="bibr" rid="bibr13-0013164411434638">Kang et al. (2009)</xref>, sample size may affect the behavior of model comparison indices. For example, the performances of AIC and BIC in their study were also quite similar with a large sample size. They also found that a large sample size was required to select the correct model for DIC and PsBF.</p>
<p>For PPMC applications, results from this study also indicate that the choice of discrepancy measures affects the performance of model comparison methods. If the specific discrepancy measure is not effective, PPMC will be less effective than DIC and CPO. As shown in Conditions 3 and 4, when <italic>Yen’s Q<sub>3</sub></italic> measure was used with PPMC, the PPMC index performed equally well with DIC and CPO. However, the performance of the <italic>global OR</italic> measure with PPMC was less effective than using the DIC and CPO indices.</p>
<p>It is worthy to note that the results from this study also offer implications for using test-level versus item-level model comparisons in testing applications. In Condition 4, for example, the test-level model comparisons favored the testlet GR model whereas the item-level model comparisons favored the testlet GR model only for a subset of items. Mixed item formats are often included in testing applications (e.g., multiple-choice items and constructed response items), and in these types of applications, different IRT models may be estimated for different subsets of items. Similarly in performance assessments, different models could be estimated for different subsets of items based on theoretical or substantive grounds. A subset of items in a performance assessment may use a common stimulus, which may cause local dependence among the items whereas no common stimuli may be used in the remaining items. Thus, a mixed modeling approach using both a testlet GR model and a GR model could be employed in an IRT application to item responses.</p>
<p>Although comparing models for sets of items (test level) is often the focus, item-level results may provide added value to the analysis. For example, the test-level CPO index is a summary index across all items. If most of the individual items reflect small differences on item-level CPO values between two competing models, the aggregated test-level CPO difference might be misinterpreted. For this case, if the focus is on the test-level CPO results, a more complex model may be favored over a simpler model. But if item-level CPO results are further examined, small CPO difference for each individual item may suggest that the simpler model is adequate and model parsimony may favor the use of the simpler model for the overall test.</p>
<p>While item-level results may provide useful diagnostic information regarding the source of misfit, the specific misfitting items, and possible alternative models, the use of item-level results complicates the discussion by the possibility that different models may be suggested for different items. For example, the PPMC results for Condition 3 indicated that the misfit of a unidimensional GR model to the responses to the first five items was due to the higher than expected correlations among these items. This might suggest a possible alternative model in which a second dimension is considered for these five items. However, it is also important that substantive knowledge about the items and relationships between items be considered when selecting a final model. For example, if a testlet GR model is indicated for a subset of items, and there is no substantive basis for the observed local dependence, the subset of items could be examined to identify the source of apparent local dependence. Similarly, test items could be examined for construct-irrelevant variance given any observed multidimensionality in the item responses.</p>
<p>Though the conditions in this study were carefully designed, some factors were fixed at values realistic to typical performance assessments. Therefore, results may not generalize to other conditions not considered herein. For example, this study was limited in terms of the length of tests (15 items), number of response categories (5 categories), specific polytomous model estimated (graded model), and number of modeled dimensions (2 dimensions). Future research could consider different sample sizes or test lengths to evaluate the effect of sample size and test length on the performance of these model comparison methods. Other factors, such as the number of dimensions, other multidimensional structures, and varying the testlet effect could be also explored. Higher correlations between dimensions could be considered as correlations may be higher in some performance assessment applications. For the PPMC method, a <italic>global OR</italic> was used in this study. Since the dichotomization used in the <italic>global OR</italic> may likely result in loss of information, other more informative OR measures could be employed for model comparison. For example, a conditional OR (Mantel–Haenszel) statistic (<xref ref-type="bibr" rid="bibr1-0013164411434638">Agresti, 2002</xref>), or the Liu–Agresti cumulative common OR (<xref ref-type="bibr" rid="bibr20-0013164411434638">Liu &amp; Agresti, 1996</xref>) for ordinal variables, could prove more powerful than the <italic>global OR</italic> for evaluating the unidimensionality or local independence assumptions for polytomous items.</p>
<p>Another possible limitation to this study was that 20 replications at each combination of experimental conditions were used to compare their performance. Although this was a small number, it was consistent with previous research focusing on Bayesian methods. Furthermore standard deviations across replications indicated relatively stable results with low variability in indices across replications. For example, the mean and standard deviation for the CPO index across 20 replications for simulated two-dimensional item responses were −16430.11 and 57.02, respectively. Although more replications could be used to obtain even more reliable results, the general patterns in the results that were observed would not likely change with additional replications. Last, although the design of the study reflects a performance assessment context, model comparison results should extend to other testing applications that use polytomously scored items (e.g., psychological assessments). For example, model comparisons involving the RS model or multidimensional models have direct implications for these types of applications. Also, model comparisons in this study revolved around item responses based on more complex models since the constructs measured in real performance assessments usually are complex. Future research could consider conditions in which the generating model is a less complex model (e.g., RS model) than the comparison model (e.g., 2P graded model). Such comparisons would provide additional information about the use of the model comparison methods in other testing applications, such as psychological assessments with RSs.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0013164411434638">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Agresti</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <source>Categorical data analysis</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr2-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Akaike</surname><given-names>H.</given-names></name>
</person-group> (<year>1974</year>). <article-title>A new look at the statistical model identification</article-title>. <source>IEEE Transactions on Automatic Control</source>, <volume>19</volume>, <fpage>716</fpage>-<lpage>723</lpage>.</citation>
</ref>
<ref id="bibr3-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Béguin</surname><given-names>A. A.</given-names></name>
<name><surname>Glas</surname><given-names>C. A. W.</given-names></name>
</person-group> (<year>2001</year>). <article-title>MCMC estimation and some model-fit analysis of multidimensional IRT models</article-title>. <source>Psychometrika</source>, <volume>66</volume>, <fpage>541</fpage>-<lpage>562</lpage>.</citation>
</ref>
<ref id="bibr4-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bolt</surname><given-names>D. M.</given-names></name>
<name><surname>Lall</surname><given-names>V. F.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Estimation of compensatory and noncompensatory multidimensional item response models using Markov chain Monte Carlo</article-title>. <source>Applied Psychological Measurement</source>, <volume>27</volume>, <fpage>395</fpage>-<lpage>414</lpage>.</citation>
</ref>
<ref id="bibr5-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bradlow</surname><given-names>E. T.</given-names></name>
<name><surname>Wainer</surname><given-names>H.</given-names></name>
<name><surname>Wang</surname><given-names>X.</given-names></name>
</person-group> (<year>1999</year>). <article-title>A Bayesian random effects model for testlets</article-title>. <source>Psychometrika</source>, <volume>64</volume>, <fpage>153</fpage>-<lpage>168</lpage>.</citation>
</ref>
<ref id="bibr6-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Thissen</surname><given-names>D.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Local dependence indexes for item pairs using item response theory</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>22</volume>, <fpage>265</fpage>-<lpage>289</lpage>.</citation>
</ref>
<ref id="bibr7-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>De Ayala</surname><given-names>R. J.</given-names></name>
</person-group> (<year>1994</year>). <article-title>The influence of dimensionality on the graded response model</article-title>. <source>Applied Psychological Measurement</source>, <volume>18</volume>, <fpage>155</fpage>-<lpage>170</lpage>.</citation>
</ref>
<ref id="bibr8-0013164411434638">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Fu</surname><given-names>J.</given-names></name>
<name><surname>Bolt</surname><given-names>D. M.</given-names></name>
<name><surname>Li</surname><given-names>Y.</given-names></name>
</person-group> (<year>2005</year>). <source>Evaluating item fit for a polytomous fusion model using posterior predictive checks</source>. <conf-name>Paper presented at the annual meeting of the National Council on Measurement in Education</conf-name>, <conf-loc>Montreal, Canada</conf-loc>.</citation>
</ref>
<ref id="bibr9-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Geisser</surname><given-names>S.</given-names></name>
<name><surname>Eddy</surname><given-names>W. F.</given-names></name>
</person-group> (<year>1979</year>). <article-title>A predictive approach to model selection</article-title>. <source>Journal of the American Statistical Association</source>, <volume>74</volume>, <fpage>153</fpage>-<lpage>160</lpage>.</citation>
</ref>
<ref id="bibr10-0013164411434638">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gelfand</surname><given-names>A. E.</given-names></name>
<name><surname>Dey</surname><given-names>D. K.</given-names></name>
<name><surname>Chang</surname><given-names>H.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Model determination using predictive distributions with implementation via sampling-based methods</article-title>. In <person-group person-group-type="editor">
<name><surname>Bernardo</surname><given-names>J. M.</given-names></name>
<name><surname>Berger</surname><given-names>J. O.</given-names></name>
<name><surname>Dawid</surname><given-names>A. P.</given-names></name>
<name><surname>Smith</surname><given-names>A. F. M.</given-names></name>
</person-group> (Eds.), <source>Bayesian statistics</source> (pp. <fpage>147</fpage>-<lpage>167</lpage>). <publisher-loc>Oxford, England</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0013164411434638">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gelman</surname><given-names>A.</given-names></name>
<name><surname>Carlin</surname><given-names>J. B.</given-names></name>
<name><surname>Stern</surname><given-names>H. S.</given-names></name>
<name><surname>Rubin</surname><given-names>D. B.</given-names></name>
</person-group> (<year>2003</year>). <source>Bayesian data analysis</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Chapman &amp; Hall</publisher-name>.</citation>
</ref>
<ref id="bibr12-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kang</surname><given-names>T.</given-names></name>
<name><surname>Cohen</surname><given-names>A. S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>IRT model selection methods for dichotomous items</article-title>. <source>Applied Psychological Measurement</source>, <volume>31</volume>, <fpage>331</fpage>-<lpage>358</lpage>.</citation>
</ref>
<ref id="bibr13-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kang</surname><given-names>T.</given-names></name>
<name><surname>Cohen</surname><given-names>A. S.</given-names></name>
<name><surname>Sung</surname><given-names>H-J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Model selection indices for polytomous items</article-title>. <source>Applied Psychological Measurement</source>, <volume>33</volume>, <fpage>499</fpage>-<lpage>518</lpage>.</citation>
</ref>
<ref id="bibr14-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kass</surname><given-names>R. E.</given-names></name>
<name><surname>Raftery</surname><given-names>A. E.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Bayes factors</article-title>. <source>Journal of the American Statistical Association</source>, <volume>90</volume>, <fpage>773</fpage>-<lpage>795</lpage>.</citation>
</ref>
<ref id="bibr15-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kim</surname><given-names>J.</given-names></name>
<name><surname>Bolt</surname><given-names>D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Estimating item response theory models using Markov Chain Monte Carlo methods</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>26</volume>(<issue>4</issue>), <fpage>38</fpage>-<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr16-0013164411434638">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lane</surname><given-names>S.</given-names></name>
<name><surname>Stone</surname><given-names>C. A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Performance assessment</article-title>. In <person-group person-group-type="editor">
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (Ed.), <source>Educational measurement</source> (<edition>4th ed.</edition>, pp. <fpage>387</fpage>-<lpage>424</lpage>). <publisher-loc>Westport, CT</publisher-loc>: <publisher-name>American Council on Education/Praeger</publisher-name>.</citation>
</ref>
<ref id="bibr17-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lane</surname><given-names>S.</given-names></name>
<name><surname>Stone</surname><given-names>C. A.</given-names></name>
<name><surname>Ankenmann</surname><given-names>R. D.</given-names></name>
<name><surname>Liu</surname><given-names>M.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Examination of the assumptions and properties of the graded item response model: An example using a mathematics performance assessment</article-title>. <source>Applied Measurement in Education</source>, <volume>8</volume>, <fpage>313</fpage>-<lpage>340</lpage>.</citation>
</ref>
<ref id="bibr18-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Levy</surname><given-names>R.</given-names></name>
<name><surname>Mislevy</surname><given-names>R. J.</given-names></name>
<name><surname>Sinharay</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Posterior predictive model checking for multidimensionality in item response theory</article-title>. <source>Applied Psychological Measurement</source>, <volume>33</volume>, <fpage>519</fpage>-<lpage>537</lpage>.</citation>
</ref>
<ref id="bibr19-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y.</given-names></name>
<name><surname>Bolt</surname><given-names>D. M.</given-names></name>
<name><surname>Fu</surname><given-names>J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>A comparison of alternative models for testlets</article-title>. <source>Applied Psychological Measurement</source>, <volume>30</volume>, <fpage>3</fpage>-<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr20-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>I.-M.</given-names></name>
<name><surname>Agresti</surname><given-names>A.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Mantel-Haenszel-type inference for cumulative odds ratios with a stratified ordinal response</article-title>. <source>Biometricis</source>, <volume>52</volume>, <fpage>1223</fpage>-<lpage>1234</lpage>.</citation>
</ref>
<ref id="bibr21-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Muraki</surname><given-names>E.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Fitting a polytomous item response model to Likert-type data</article-title>. <source>Applied Psychological Measurement</source>, <volume>14</volume>, <fpage>59</fpage>-<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr22-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Orlando</surname><given-names>M.</given-names></name>
<name><surname>Thissen</surname><given-names>D.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Likelihood-based item-fit indices for dichotomous item response theory models</article-title>. <source>Applied Psychological Measurement</source>, <volume>24</volume>, <fpage>50</fpage>-<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr23-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rubin</surname><given-names>D. B.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Bayesianly justifiable and relevant frequency calculations for the applied statistician</article-title>. <source>Annals of Statistics</source>, <volume>12</volume>, <fpage>1151</fpage>-<lpage>1172</lpage>.</citation>
</ref>
<ref id="bibr24-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Samejima</surname><given-names>F.</given-names></name>
</person-group> (<year>1969</year>). <article-title>Estimation of latent ability using a response pattern of graded scores</article-title>. <source>Psychometrika Monograph Supplement, 17</source>.</citation>
</ref>
<ref id="bibr25-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schwarz</surname><given-names>G.</given-names></name>
</person-group> (<year>1978</year>). <article-title>Estimating the dimension of a model</article-title>. <source>Annals of Statistics</source>, <volume>6</volume>, <fpage>461</fpage>-<lpage>464</lpage>.</citation>
</ref>
<ref id="bibr26-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sinharay</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Assessing fit of unidimensional item response theory models using a Bayesian approach</article-title>. <source>Journal of Educational Measurement</source>, <volume>42</volume>, <fpage>375</fpage>-<lpage>394</lpage>.</citation>
</ref>
<ref id="bibr27-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sinharay</surname><given-names>S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Bayesian item fit analysis for unidimensional item response theory models</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>59</volume>, <fpage>429</fpage>-<lpage>449</lpage>.</citation>
</ref>
<ref id="bibr28-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sinharay</surname><given-names>S.</given-names></name>
<name><surname>Johnson</surname><given-names>M. S.</given-names></name>
<name><surname>Stern</surname><given-names>H. S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Posterior predictive assessment of item response theory models</article-title>. <source>Applied Psychological Measurement</source>, <volume>30</volume>, <fpage>298</fpage>-<lpage>321</lpage>.</citation>
</ref>
<ref id="bibr29-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spiegelhalter</surname><given-names>D. J.</given-names></name>
<name><surname>Best</surname><given-names>N.</given-names></name>
<name><surname>Carlin</surname><given-names>B. P.</given-names></name>
<name><surname>van der Linde</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Bayesian measures of model complexity and fit</article-title>. <source>Journal of the Royal Statistical Society</source>, <volume>64</volume>, <fpage>583</fpage>-<lpage>640</lpage>.</citation>
</ref>
<ref id="bibr30-0013164411434638">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Spiegelhalter</surname><given-names>D. J.</given-names></name>
<name><surname>Thomas</surname><given-names>A.</given-names></name>
<name><surname>Best</surname><given-names>N.</given-names></name>
<name><surname>Lunn</surname><given-names>D.</given-names></name>
</person-group> (<year>2003</year>). <source>WINBUGS Version 1.4 user’s manual</source> [Computer software manual]. <publisher-loc>Cambridge, England</publisher-loc>: <publisher-name>MRC Biostatistics Unit</publisher-name>.</citation>
</ref>
<ref id="bibr31-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stone</surname><given-names>C. A.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Monte Carlo based null distribution for an alternative goodness-of-fit test statistic in IRT models</article-title>. <source>Journal of Educational Measurement</source>, <volume>37</volume>, <fpage>58</fpage>-<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr32-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
<name><surname>Bradlow</surname><given-names>E. T.</given-names></name>
<name><surname>Wainer</surname><given-names>H.</given-names></name>
</person-group> (<year>2002</year>). <article-title>A general Bayesian model for testlets: Theory and applications</article-title>. <source>Applied Psychological Measurement</source>, <volume>26</volume>, <fpage>109</fpage>-<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr33-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yao</surname><given-names>L.</given-names></name>
<name><surname>Boughton</surname><given-names>K. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>A multidimensional item response modeling approach for improving subscale proficiency estimation and classification</article-title>. <source>Applied Psychological Measurement</source>, <volume>31</volume>, <fpage>83</fpage>-<lpage>105</lpage>.</citation>
</ref>
<ref id="bibr34-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yen</surname><given-names>W. M.</given-names></name>
</person-group> (<year>1981</year>). <article-title>Using simulation results to choose a latent trait model</article-title>. <source>Applied Psychological Measurement</source>, <volume>5</volume>, <fpage>245</fpage>-<lpage>262</lpage>.</citation>
</ref>
<ref id="bibr35-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yen</surname><given-names>W. M.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Scaling performance assessments: Strategies for managing local item dependence</article-title>. <source>Journal of Educational Measurement</source>, <volume>30</volume>, <fpage>187</fpage>-<lpage>213</lpage>.</citation>
</ref>
<ref id="bibr36-0013164411434638">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
</person-group> (<year>2009</year>). <source>Checking fit of item response models for performance assessments using Bayesian analysis</source>. <comment>Unpublished dissertation</comment>. <publisher-name>University of Pittsburgh</publisher-name>.</citation>
</ref>
<ref id="bibr37-0013164411434638">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>X.</given-names></name>
<name><surname>Stone</surname><given-names>C. A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Assessing fit of unidimensional graded response models using Bayesian methods</article-title>. <source>Journal of Educational Measurement</source>, <volume>48</volume>, <fpage>81</fpage>-<lpage>97</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>