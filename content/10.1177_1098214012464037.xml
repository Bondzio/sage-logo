<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AJE</journal-id>
<journal-id journal-id-type="hwp">spaje</journal-id>
<journal-title>American Journal of Evaluation</journal-title>
<issn pub-type="ppub">1098-2140</issn>
<issn pub-type="epub">1557-0878</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1098214012464037</article-id>
<article-id pub-id-type="publisher-id">10.1177_1098214012464037</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Arguments for a Common Set of Principles for Collaborative Inquiry in Evaluation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Cousins</surname>
<given-names>J. Bradley</given-names>
</name>
<xref ref-type="aff" rid="aff1-1098214012464037">1</xref>
<xref ref-type="corresp" rid="corresp1-1098214012464037"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Whitmore</surname>
<given-names>Elizabeth</given-names>
</name>
<xref ref-type="aff" rid="aff2-1098214012464037">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shulha</surname>
<given-names>Lyn</given-names>
</name>
<xref ref-type="aff" rid="aff3-1098214012464037">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-1098214012464037">
<label>1</label>University of Ottawa, Ottawa, ON, Canada</aff>
<aff id="aff2-1098214012464037">
<label>2</label>Carleton University, Ottawa, ON, Canada</aff>
<aff id="aff3-1098214012464037">
<label>3</label>Queen’s University, Kingston, ON, Canada</aff>
<author-notes>
<corresp id="corresp1-1098214012464037">J. Bradley Cousins, Faculty of Education, University of Ottawa, 145 Jean Jacques Lussier, Ottawa, ON, Canada K1N 6NS. Email: <email>bcousins@uottawa.ca</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>34</volume>
<issue>1</issue>
<fpage>7</fpage>
<lpage>22</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Evaluation Association</copyright-holder>
</permissions>
<abstract>
<p>In this article, we critique two recent theoretical developments about collaborative inquiry in evaluation—using logic models as a means to understand theory, and efforts to compartmentalize versions of collaborative inquiry into discrete genres—as a basis for considering future direction for the field. We argue that collaborative inquiry in evaluation is about relationships between trained evaluation specialists and nonevaluator stakeholders (i.e., members of the program community, intended program beneficiaries, or other persons with an interest in the program) and that practice should, in the first instance, be sensitive to stakeholder interests and context, and it should be principle-driven.</p>
</abstract>
<kwd-group>
<kwd>collaborative inquiry</kwd>
<kwd>program evaluation</kwd>
<kwd>complexity theory</kwd>
<kwd>principles of practice</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1098214012464037">
<title>The Problem</title>
<p>Collaborative approaches in evaluation are increasingly recognized as important in the development evaluation as well as the evaluation contexts in developed countries. For example, recent critiques of development evaluation have pointed to the overrepresentation of donor interests, as opposed to the interests of local program communities and the privileging of the <italic>accountability</italic> function of evaluation (e.g., <xref ref-type="bibr" rid="bibr12-1098214012464037">Carden, 2010</xref>; <xref ref-type="bibr" rid="bibr40-1098214012464037">Hay, 2010</xref>). Adherence to conventional, evaluation designs and approaches to impact evaluation that tend to privilege rigor over the importance of context (e.g., randomized control trials [RCTs] and quasi-experimental designs) has been framed as potentially disruptive to program development and implementation and oppressive with regard to the <italic>learning</italic> function of evaluation (<xref ref-type="bibr" rid="bibr21-1098214012464037">Dahler-Larsen, 2009</xref>; <xref ref-type="bibr" rid="bibr57-1098214012464037">Preskill, 2008</xref>).</p>
<p>Recent think tank sessions at American and European evaluation professional meetings have led to the identification of a variety of alternative approaches to the statistical counterfactual in the context of impact evaluation (e.g., <xref ref-type="bibr" rid="bibr63-1098214012464037">Rugh, Steinke, Cousins, &amp; Bamberger, 2010</xref>). The importance of understanding context for programs and interventions has been underlined as being essential to such evaluation. Similar arguments derive from developments in complexity theory as applied to evaluation (see, e.g., <xref ref-type="bibr" rid="bibr56-1098214012464037">Patton, 2010</xref>; <xref ref-type="bibr" rid="bibr58-1098214012464037">Ramalingam &amp; Jones, 2008</xref>; <xref ref-type="bibr" rid="bibr59-1098214012464037">Rihani, 2002</xref>; <xref ref-type="bibr" rid="bibr74-1098214012464037">Woodhill, 2007</xref>).</p>
<p>Some of the alternative modes of inquiry that were identified by <xref ref-type="bibr" rid="bibr63-1098214012464037">Rugh et al. (2010</xref>; e.g., most significant change technique and contribution analysis) lend themselves quite readily to participatory and collaborative approaches to understanding program effects. In this light, some would argue—and we count ourselves among them—that learning from evaluation, a recognized strength of collaborative approaches, <italic>is</italic> a form of accountability in and of itself.</p>
<p>In North America, participatory and collaborative approaches to evaluation are the most often used approach for cross-cultural inquiry associated with interventions for aboriginal and indigenous peoples (<xref ref-type="bibr" rid="bibr9-1098214012464037">Brant-Castellano, 1986</xref>; <xref ref-type="bibr" rid="bibr20-1098214012464037">Chouinard &amp; Cousins, 2007</xref>; <xref ref-type="bibr" rid="bibr44-1098214012464037">Hoare, Levy, &amp; Robinson, 1993</xref>; <xref ref-type="bibr" rid="bibr45-1098214012464037">Jackson, McCaskill, &amp; Hall, 1982</xref>). There is little doubt that such approaches are well suited to cross-cultural evaluation contexts as opposed to more traditional social sciences approaches. As an example, <xref ref-type="bibr" rid="bibr16-1098214012464037">Cousins et al. (2010</xref>) integrated participatory evaluation (PE) with an approach similar to the most significant change technique (Davies &amp; Dart, 2005; Serrat, 2009) in a multiple case study design investigating the effects of a national Canadian strategy for aboriginal youth suicide prevention.</p>
<p>With evident growth in the popularity and credibility of collaborative approaches to inquiry, it is increasingly important to consider ways to advance the field along theoretical and practical lines. Along with many others, we have been actively engaged in such pursuits. Yet we find that some recent developments in this respect may not always be as productive as initially thought. In this article, we review and critique two such developments: using program logic models to represent collaborative evaluation theory and compartmentalizing collaborative approaches. Following a review and comment on these developments, we propose some ideas about the development of fundamental principles for collaborative inquiry, principles that could serve as guidelines to practice. But first we provide some thoughts about the evolutionary trajectory of collaborative inquiry in our field.</p>
</sec>
<sec id="section2-1098214012464037">
<title>Background</title>
<p>Collaborative inquiry in evaluation traces its origins to development evaluation, research, and theory of over 30 years ago (<xref ref-type="bibr" rid="bibr72-1098214012464037">Whitmore, 1998a</xref>). An international participatory research network was set up in the 1970s, with headquarters in India, and the first of a series of major international seminars was held in Tanzania in 1979 (<xref ref-type="bibr" rid="bibr37-1098214012464037">Hall, 1975</xref>, <xref ref-type="bibr" rid="bibr38-1098214012464037">1981</xref>, <xref ref-type="bibr" rid="bibr39-1098214012464037">1992</xref>; Kassam &amp; Mustapha, 1982). Rapid rural appraisal (RRA; <xref ref-type="bibr" rid="bibr13-1098214012464037">Chambers, 1981</xref>), participatory research, and participatory action research (PAR) are all forerunners of what has come to be known as PE taking place in Latin America (<xref ref-type="bibr" rid="bibr1-1098214012464037">Alcocer et al., 1997</xref>; <xref ref-type="bibr" rid="bibr25-1098214012464037">Fals-Borda, 1981</xref>, <xref ref-type="bibr" rid="bibr26-1098214012464037">1992</xref>, <xref ref-type="bibr" rid="bibr27-1098214012464037">2006</xref>), Asia (<xref ref-type="bibr" rid="bibr8-1098214012464037">Armonia &amp; Campilan, 1997</xref>; <xref ref-type="bibr" rid="bibr28-1098214012464037">Fernandes &amp; Tandon, 1981</xref>; <xref ref-type="bibr" rid="bibr69-1098214012464037">Tandon, 1981</xref>, <xref ref-type="bibr" rid="bibr70-1098214012464037">2008</xref>), Africa (<xref ref-type="bibr" rid="bibr48-1098214012464037">Kassam &amp; Mustafa, 1982</xref>; <xref ref-type="bibr" rid="bibr55-1098214012464037">PAMFORK, 1997</xref>), and elsewhere (<xref ref-type="bibr" rid="bibr11-1098214012464037">Campos &amp; Coupal, 1996</xref>; <xref ref-type="bibr" rid="bibr24-1098214012464037">Estrella &amp; Gaventa, 1998</xref>; <xref ref-type="bibr" rid="bibr34-1098214012464037">Feuerstein, 1986</xref>; <xref ref-type="bibr" rid="bibr46-1098214012464037">Jackson &amp; Kassam, 1998</xref>; <xref ref-type="bibr" rid="bibr52-1098214012464037">Narayan, 1994</xref>). These alternative approaches developed at least in part as a reaction to positivist models of inquiry that were seen as exploitive and detached from urgent social and economic problems.</p>
<p>In 1993, David Fetterman introduced “Empowerment Evaluation” to the North American evaluation community in his presidential address at the annual conference of the American Evaluation Association (AEA) in Dallas (<xref ref-type="bibr" rid="bibr29-1098214012464037">Fetterman, 1994</xref>). Given the radical departure of empowerment evaluation from more conventional evaluation approaches, this proposed new direction for evaluation was as courageous as it was thought provoking. Perhaps, not surprisingly, empowerment evaluation met with stiff opposition from evaluation luminaries including <xref ref-type="bibr" rid="bibr68-1098214012464037">Stufflebeam (1994</xref>) and <xref ref-type="bibr" rid="bibr64-1098214012464037">Scriven (1997</xref>). Not long after Fetterman’s address, an AEA Topical Interest Group was created called <italic>Collaborative, Participatory and Empowerment Evaluation</italic> and its membership has been growing ever since.<sup><xref ref-type="fn" rid="fn4-1098214012464037">1</xref></sup> Also in the early to mid-90s we started to publish papers about a participatory approach to evaluation, which had as its goals program problem solving and improvement and the enhancement of evaluation use (<xref ref-type="bibr" rid="bibr17-1098214012464037">Cousins &amp; Earl, 1992</xref>, <xref ref-type="bibr" rid="bibr18-1098214012464037">1995</xref>; <xref ref-type="bibr" rid="bibr66-1098214012464037">Shulha &amp; Wilson, 1995</xref>).</p>
<p>Coupled with the earlier work taking place in international development contexts, the menu of choices with respect to collaborative modes of applied research and evaluation was growing rapidly. This diversity of activity, and attendant conceptual overlap and confusion, prompted us to think about fundamental theoretical aspects of collaborative inquiry, which we ultimately published as a chapter in <italic>New Directions for Evaluation</italic> titled “Framing Participatory Evaluation” (<xref ref-type="bibr" rid="bibr19-1098214012464037">Cousins &amp; Whitmore, 1998</xref>). That paper, as it turns out, was widely received and ultimately selected to be reprinted in a 20-year anniversary issue of AEA’s <italic>New Directions for Evaluation</italic> (<xref ref-type="bibr" rid="bibr50-1098214012464037">Matheson, 2007</xref>).</p>
<p>We would argue that there were two important contributions in that paper. First, we differentiated among forms of collaborative inquiry in evaluation with different ideological interests. On one hand, there is transformative PE (T-PE), an approach that is normative in form and function and associated with such ideals as emancipation, empowerment, and self-determination. On the other hand, there is practical PE (P-PE), a pragmatic problem-solving approach to evaluation that promotes the use of evaluation findings and process. While T-PE is predominantly grounded in a <italic>political</italic> justification and P-PE in <italic>pragmatic</italic> considerations, the two are not mutually exclusive; and they both embrace a third, philosophical rationale that is about the development of deeper levels of meaning through collaboration. The two streams represent different emphases or desired end points and are therefore at different points on a common continuum.</p>
<p>As a second important contribution, we built on earlier work (<xref ref-type="bibr" rid="bibr17-1098214012464037">Cousins &amp; Earl, 1992</xref>) to provide a tool (see <xref ref-type="fig" rid="fig1-1098214012464037">Figure 1</xref>) for differentiating alternative collaborative approaches along three fundamental dimensions: control of technical evaluation decisions (evaluator vs. stakeholder), stakeholder selection for participation (diverse group vs. homogeneous primary user group), and depth of participation by stakeholders (participation in selected aspects vs. comprehensive participation in technical evaluative processes). The idea is that any given collaborative inquiry project, at any given point in time, could be described by locating its coordinates it in three-dimensional space (see <xref ref-type="fig" rid="fig1-1098214012464037">Figure 1</xref>).</p>
<fig id="fig1-1098214012464037" position="float">
<label>Figure 1.</label>
<caption>
<p>Dimensions of form in collaborative inquiry (from <xref ref-type="bibr" rid="bibr15-1098214012464037">Cousins &amp; Chouinard, 2012</xref>).</p>
</caption>
<graphic xlink:href="10.1177_1098214012464037-fig1.tif"/>
</fig>
<p>As we had hoped, the three-dimensional process framework became the focus for ongoing analysis, discussion, and debate with colleagues and students. Eventually, we came to the view that one of these dimensions—stakeholder selection—was confounded and therefore conceptually inadequate (<xref ref-type="bibr" rid="bibr71-1098214012464037">Weaver &amp; Cousins, 2004</xref>). We ultimately teased apart this dimension into three distinct dimensions of form or inquiry processes. The resulting framework consisted of five dimensions of form: we unpacked stakeholder selection into three more parsimonious dimensions—manageability (manageable vs. unmanageable), power relations (conflicted vs. neutral), and stakeholder selection (diverse vs. homogenous)—and added them to existing dimensions of control of technical decision making and depth of participation. <xref ref-type="fig" rid="fig2-1098214012464037">Figure 2</xref> details the specific elements of the framework, which we argued ameliorated the aforementioned confound.</p>
<fig id="fig2-1098214012464037" position="float">
<label>Figure 2.</label>
<caption>
<p>Five dimensions of form in collaborative inquiry (from <xref ref-type="bibr" rid="bibr71-1098214012464037">Weaver &amp; Cousins, 2004</xref>).</p>
</caption>
<graphic xlink:href="10.1177_1098214012464037-fig2.tif"/>
</fig>
<p>Again, any given collaborative inquiry, at any given point in time can be described according to its location on each of the five dimensions. The results can be displayed using a “radar-gram” to show variation across the five dimensions. <xref ref-type="fig" rid="fig3-1098214012464037">Figure 3</xref> shows hypothetical differentiation among P-PE, T-PE, and more conventional stakeholder-based evaluation. We have since used this reformulation to describe and compare collaborative evaluation projects (e.g., <xref ref-type="bibr" rid="bibr14-1098214012464037">Cousins, 2005</xref>).</p>
<fig id="fig3-1098214012464037" position="float">
<label>Figure 3.</label>
<caption>
<p>Hypothetical dimensions of form in collaborative inquiry (adapted from <xref ref-type="bibr" rid="bibr71-1098214012464037">Weaver &amp; Cousins, 2004</xref>).</p>
</caption>
<graphic xlink:href="10.1177_1098214012464037-fig3.tif"/>
</fig>
<p>Some years later, <xref ref-type="bibr" rid="bibr22-1098214012464037">Daigneault and Jacob (2009</xref>) took issue with this development and built a solid argument as to why the original formulation was conceptually superior. We are persuaded by their argument, which goes something like this: generally in the field, PE is conceptually sloppy stacked up against the eight criteria for evaluating the functionality of a concept offered by <xref ref-type="bibr" rid="bibr35-1098214012464037">Gerring (1999</xref> cited by <xref ref-type="bibr" rid="bibr22-1098214012464037">Daigneault &amp; Jacob, 2009</xref>). The most serious shortcomings are parsimony, internal coherence, and external differentiation (i.e., effectively differentiating between participatory and nonparticipatory inquiry). According to the authors, the three dimensions put forward by <xref ref-type="bibr" rid="bibr19-1098214012464037">Cousins and Whitmore (1998</xref>) “. . . happen to be PE’s fundamental attributes or constitutive dimensions” (p. 334). They are all <italic>necessary</italic> conditions for PE and <italic>jointly sufficient</italic> for membership in this category. Other dimensions such as Weaver and Cousins’ (2004) “manageability” and “power relations” add value through augmenting analytic power but according to Daigneault and Jacob “. . . they do not constitute the necessary attributes of PE” (p. 336). Manageability and power relations do not define PE because they can apply equally to nonparticipatory approaches such as conventional technocratic program evaluations. Control of technical decision making, stakeholder selection and depth of participation are therefore seen as constitutive of PE. The authors also argue that the three dimensions add an impressive degree of internal coherence and are able to successfully differentiate participatory and nonparticipatory inquiry (i.e., external differentiation). Further, they can accommodate a wide range of approaches to collaborative inquiry including empowerment evaluation, democratic evaluation, and fourth-generation evaluation.</p>
<p>Based on this analysis, <xref ref-type="bibr" rid="bibr22-1098214012464037">Daigneault and Jacob (2009</xref>) provide an approach to measuring PE that seems meritorious and worthy of empirical application in research on collaborative inquiry. We like these ideas, but perhaps what we take first and foremost from their careful critique and analysis is that “we had it right the first time.” While attributes such as manageability and power relations are important considerations about PE—as are others such as multiplicity in methods or capacity building (<xref ref-type="bibr" rid="bibr10-1098214012464037">Burke, 1998</xref>)—they are not fundamental to the conceptualization of collaborative inquiry.</p>
<p>Since that time, as foreshadowed above, two separate streams of activity have prompted us to conclude that it may be time to pause and perhaps rethink new directions for collaborative inquiry in evaluation in the interest of advancing the field. We now turn to our review and critique of recent efforts to (i) represent theory in collaborative inquiry using logic models and (ii) compartmentalize different families or genres of collaborative inquiry.</p>
<sec id="section3-1098214012464037">
<title>Contemplating the Logic of Logic Models</title>
<p>Marv Alkin has contributed to the development and understanding of theory in evaluation perhaps as much as or more than anyone in the field. Among his many accomplishments, he sponsored and captured “debates in evaluation” among noted contributors (<xref ref-type="bibr" rid="bibr2-1098214012464037">Alkin, 1990</xref>), provided a 25-year retrospective on theory in evaluation (<xref ref-type="bibr" rid="bibr3-1098214012464037">Alkin, 1991</xref>), and more recently published <italic>Evaluation Roots </italic>(<xref ref-type="bibr" rid="bibr4-1098214012464037">Alkin, 2004</xref>, in press) an edited volume that features the evaluation theory tree, a conceptual framework codeveloped with Tina Christie (<xref ref-type="bibr" rid="bibr6-1098214012464037">Alkin &amp; Christie, 2004</xref>).</p>
<p>At the 2007 meeting of AEA in Denver, Alkin invited one of us (Cousins) to participate in a panel session that involved other contributors and some of Marv’s graduate students (Hansen, Luskin, and Wallace). The focus for the session was a series of draft logic models generated by the students that corresponded to respective “evaluation theories:” P-PE (Cousins), transformative evaluation (Mertens), utilization-focused evaluation (UFE; Patton), and empowerment evaluation (Fetterman). In the session, the students presented the models and then invited reaction from the respective theory proponents. The initial idea led to further work and development on the part of Alkin and his students. Papers were drafted for a session at AEA in 2010, comparing and contrasting various theoretical perspectives using visual representations (<xref ref-type="bibr" rid="bibr41-1098214012464037">Hansen &amp; Wallace, 2010</xref>; <xref ref-type="bibr" rid="bibr49-1098214012464037">Luskin, 2010</xref>). This time the focus was on P-PE (Cousins), value-engaged evaluation (Greene), and emergent realist evaluation (Mark, Henry, and Julnes). Ultimately, products from this line of inquiry, including discussant and commentary articles, will appear as a special issue (<xref ref-type="bibr" rid="bibr7-1098214012464037">Alkin, Vo, &amp; Hansen, in press</xref>).</p>
<p>At the core of it, this work is motivated by an interest in understanding the similarities and differences among various evaluation approaches or theories using logic models as a basic visual representation. Through using such representations, salient features of respective approaches can be identified, thereby illuminating the anticipated relations between evaluators and stakeholders and assisting evaluation practitioners who choose to follow a particular theorist’s prescriptions (<xref ref-type="bibr" rid="bibr41-1098214012464037">Hansen &amp; Wallace, 2010</xref>). Five basic elements are depicted: assumptions, evaluation context, evaluation activities, evaluation consequences/effects, and external factors (<xref ref-type="bibr" rid="bibr42-1098214012464037">Hansen, Alkin, &amp; Wallace, in press</xref>). The authors assert that through operational specificity, the approach may assist evaluation practitioners with implementation, aid in the identification of theoretical deficiencies, and facilitate research on evaluation.</p>
<p>A separate, parallel initiative was recently undertaken by <xref ref-type="bibr" rid="bibr43-1098214012464037">Harner (2012</xref>) in his doctoral research. Harner had a similar objective to the UCLA group but focused exclusively on developing a model of TP-E and relied on the collection of original data from evaluation theorists and practitioners. His study employed a multiple methods, cascading design and generated a logic model representation of T-PE that elaborated principles, actions, and outcomes of the process. The design permitted him to differentiate the relative weights of variables associated with T-PE practice.</p>
<p>According to <xref ref-type="bibr" rid="bibr51-1098214012464037">Miller (in press</xref>) in her commentary on the UCLA papers to be published in the special issue, the use of a standardized approach to develop logic models of evaluation theories provides a unique way to represent each theory’s relative emphasis on its ideological, operational, and intervention components. These are the elements or sublogics of evaluation theory offered by Smith (2010, cited by Miller, in press). P-PE as an example, when compared with value-engaged and emergent realist theories, turns out to most closely align with the interventionist theoretical element. Miller elaborated other potential benefits of the logic modeling approach such as facilitating comparative analysis among theories, illuminating training needs for evaluators, and exposing conceptual inconsistencies or paradoxes within theories. It could be argued that Miller’s analysis would apply to Harner’s (2012) results as well.</p>
<p>We understand the merit and the potential benefits of such approaches to visualizing theory. Yet at some level, with respect to collaborative inquiry, we have discomfort with this line of inquiry. That the approach centers on the conceptualization of PE as “a theory” or “a model” reflects one of our chief concerns. We have long taken the position that PE is an alternative “approach” to evaluation. Studies that attempt to codify P-PE, T-PE, or any other approach to collaborative inquiry in evaluation as a logic model run the risk of misrepresenting them as something less fluid and flexible, and perhaps more prescriptive, than they were ever intended to be. In our opinion, the interests and forms of collaborative inquiry are best informed by the contextual exigencies emerging from the community of program practice and the information needs of various actors associated with the program. Such considerations ultimately shape, through negotiation, the expressed purposes and form that the inquiry will take. For example, in contexts where there is a relatively high degree of agreement about the goals of an intervention and that evaluation can assist with the development and improvement of the program, a P-PE approach that focuses on balanced control, limited stakeholder diversity, and substantial depth of participation might be warranted (see, e.g., Cousins &amp; Shulha, 2008). Likewise, through involving diverse groups of stakeholders in evaluation knowledge production, T-PE might assist in developing community capacity for inquiry and a sense of self-determination in contexts where agreement about goals is not high or where capacity for collective understanding and problem solving is constrained (see, e.g., <xref ref-type="bibr" rid="bibr47-1098214012464037">Kar &amp; Chambers, 2008</xref>). But these collaborative inquiry processes, in our mind, are up for negotiation with the relevant and interested members of the program community.</p>
<p>In her remarks, Miller (in press) laid out similar concerns. She suggested that the logic modeling approach runs the risks of underrepresenting context, marginalizing cultural and cross-cultural considerations, and privileging a mechanistic representation to the detriment of capturing the dynamic character of evaluation practice. In our view, context, culture, and dynamic negotiated practice are part and parcel of collaborative inquiry in evaluation. As such, Miller’s concerns apply to efforts to develop visual representations of evaluation theory with respect to collaborative inquiry in evaluation. Having said that, we do acknowledge Alkin’s point that evaluation theories represented visually are ideals and their application in practice will be very much influenced by context.
<disp-quote>
<p>Yes, we do portray the ideal or most likely mode of each theory in our logic models. As you know from my writings, I am very concerned about context and understand that context shapes and modifies that ideal approach (personal communication, Marvin Alkin, November 15, 2011).</p>
</disp-quote>Nonetheless, we remain reluctant to commit to an understanding of P-PE or T-PE as a theory or model, as opposed to an approach and we think of context very much as a starting point for defining the practical application of collaborative inquiry in evaluation. Miller’s concerns are also quite relevant to another stream of activity that gives us pause, namely, efforts to compartmentalize collaborative inquiry in evaluation.</p>
</sec>
<sec id="section4-1098214012464037">
<title>Rethinking Compartmentalization</title>
<p>At the 2009 and 2010 annual AEA conferences, the Collaborative, Participatory and Empowerment (CPE) Topical Interest Group (TIG) has openly addressed the question of how best to differentiate among these three family members. One of us (Shulha) has been directly involved in, and become increasingly more uncomfortable with, these discussions. At the 2010 meeting, the group appeared to be reaching consensus about differentiating these approaches (personal communication with David Fetterman and Abe Wandersman, November, 2010) mostly along the lines of one of our dimensions of form in collaborative inquiry, technical control. These ideas continue to move forward, most recently in a March 2011 series of webinars sponsored by the CPE-TIG (<xref ref-type="bibr" rid="bibr33-1098214012464037">Fetterman &amp; Wandersman, 2011</xref>; <xref ref-type="bibr" rid="bibr62-1098214012464037">Rodriguez-Campos &amp; O’Sullivan, 2011</xref>; <xref ref-type="bibr" rid="bibr76-1098214012464037">Zukoski &amp; Luluqisen, 2011</xref>) and in articles published as part of a special issue of a peer reviewed journal (<xref ref-type="bibr" rid="bibr54-1098214012464037">O’Sullivan, 2012</xref>; <xref ref-type="bibr" rid="bibr61-1098214012464037">Rodriguez-Campos, 2012</xref>).</p>
<p>Collaborative evaluation, it is argued by CPE-TIG proponents, is about evaluators in leadership roles working with stakeholders to produce evaluative knowledge. The evaluator is “in charge” and the stakeholder participant’s role is to engage with other participants and the evaluator. As recently put by <xref ref-type="bibr" rid="bibr61-1098214012464037">Rodriguez-Campos (2012</xref>),
<disp-quote>
<p>[Evaluators] also contribute to a genuinely collaborative atmosphere, i.e. one in which everyone feels represented in an appropriate and fair way. Even though <italic>evaluators are in charge</italic> of the collaborative evaluation, they create an ongoing engagement between evaluators and stakeholders. (p. 227, our emphasis)</p>
</disp-quote>Identifiable benefits of collaborative evaluation are strong evaluation designs, enhanced data collection, and analysis and results that stakeholders understand and use (<xref ref-type="bibr" rid="bibr62-1098214012464037">Rodriguez-Campos &amp; O’Sullivan, 2011</xref>; O’Sullivan, 2012). Books by O’<xref ref-type="bibr" rid="bibr53-1098214012464037">Sullivan (2004</xref>) and <xref ref-type="bibr" rid="bibr60-1098214012464037">Rodriguez-Campos (2005</xref>) are identified as exemplars of the approach.</p>
<p>Empowerment evaluation (<xref ref-type="bibr" rid="bibr30-1098214012464037">Fetterman, 2001</xref>; <xref ref-type="bibr" rid="bibr32-1098214012464037">Fetterman &amp; Wandersman, 2005</xref>), on the other hand, is located at the other end of the control spectrum. It is about divesting control of technical decision making to stakeholders at the outset. “People take charge of their evaluation with the assistance of an empowerment evaluator” (<xref ref-type="bibr" rid="bibr33-1098214012464037">Fetterman &amp; Wandersman, 2011</xref>). The benefits associated with empowerment evaluation are capacity building, the production of measurable outcomes, contributions to sustainability, and enhanced knowledge utilization.</p>
<p>The representation of the third member of the CPE-TIG family, PE, remains somewhat of an enigma to us. What sets this approach apart, we thought we understood, was that control begins with the evaluator but is divested to program community members over time and with experience (personal communication with David Fetterman and Abe Wandersman, November, 2010), yet no mention of this is made in the webinar handout (<xref ref-type="bibr" rid="bibr76-1098214012464037">Zukoski &amp; Luluqisen, 2011</xref>), although the authors support a balance of control shared between evaluators and stakeholders, which does differentiate it from the other CPE family members. The PE webinar handout seems to more or less privilege PE as an overarching framework with practical and transformative thrusts. For example, it lists such “methods” as empowerment evaluation, PAR, and UFE as part of its repertoire. The rationale for PE, according to the webinar leaders, is long, varied, and really quite all encompassing: identify locally relevant questions; improve program performance; empower participants; build capacity; develop leaders and build teams; sustain organizational learning and growth; transform and improve programs; and use findings to create action plans to make improvements.</p>
<p>Contrary to our colleagues in CPE-TIG, we prefer to frame collaborative evaluation or collaborative inquiry in evaluation as an umbrella term that encompasses such approaches as PE, transformative evaluation, empowerment evaluation, democratic deliberative evaluation, PAR, and fourth-generation evaluation. But this is not our central concern with the business of compartmentalization exercises; rather, we are unclear as to the principal motivations for this activity. Why is it important to have sharp distinctions among these approaches and to whose benefit? Possibly it is because some evaluators self-identify with particular approaches. If this is so, does it imply that “empowerment evaluators” or “collaborative evaluators” bring their specific approach with them—3-step, 10-step, and step-by-step—to each opportunity for collaborative inquiry in evaluation? Again, we underscore that it is the exigencies of context and the information needs of program community members that ought to shape the inquiry. What comes out in terms of what the evaluation practitioner actually does will be different, as any given context will demand, and negotiation is essential. To that end, going in with preconceived notions about what to do, and hows to do it inevitably carries with it some risks. For example, in related discussion Donna Mertens commented:
<disp-quote>
<p>I am sure I would not agree that turning control over to stakeholders would be a transformative participatory approach; it could be chaos. There needs to be a partnership rather than a relinquishing of responsibility on the part of the evaluator. (personal communication, December 2010)</p>
</disp-quote>We share the concern. In a context that does not warrant such an approach, it runs the risk of setting people/stakeholders up with false expectations and ultimately may end up doing more harm than good, however unintended. It is essential that evaluators engaging in any form of collaborative inquiry seriously consider the potential for unintended effects and their implications for programs and communities (<xref ref-type="bibr" rid="bibr73-1098214012464037">Whitmore, 1998b</xref>).</p>
</sec>
<sec id="section5-1098214012464037">
<title>Reconsidering Direction for Collaborative Inquiry in Evaluation</title>
<p>To this point we have reviewed two distinct developments in the field concerning the theory and practice of collaborative inquiry in evaluation. Our conclusions are recapped as follows: First, we recognize the merit and potential benefits associated with using visual representations to understand and compare theories of evaluation. Yet we have concerns that portraying collaborative evaluative inquiry as a theoretical model rather than an approach represents it as being more rigid and pre-ordinate and less dynamic and flexible than it was ever intended to be, remembering that the complexities of any given context cannot be adequately captured in such a modeling process. Second, we find an investment in compartmentalizing genres of collaborative, participatory, and empowerment evaluation unwarranted and ultimately unproductive. We fail to understand the utility of this direction as a means of advancing theory and practice in the field.</p>
<p>Our conclusions lead us to pause and rethink future direction for the field. We conclude this article with some thoughts about the essential aspects of collaborative inquiry in evaluation and suggested strategies to move toward a principle-driven conception of practice.</p>
</sec>
<sec id="section6-1098214012464037">
<title>The Essential Aspects of Collaborative Inquiry in Evaluation</title>
<p>We recently considered the essential aspects of PE (<xref ref-type="bibr" rid="bibr65-1098214012464037">Shulha, 2010</xref>), which apply more generally to collaborative inquiry in evaluation. Thinking about the latter as a class of approaches rather than as methods, models, or techniques allows evaluators to foreground the purpose, people, and context of their collaborative work. In our view, this must be the starting point for any collaborative process.</p>
<p>Considerations of purpose help us to understand the call—either implicitly or explicitly—for collaborative inquiry in the first place. What are the information needs of the program community and to what extent do they vary? How has context shaped those information needs? How can evaluation, and specifically collaborative approaches, help stakeholders to meet identified needs? What are the advantages of the collaborative approach over others for meeting the needs? Purposes may vary widely both within and across stakeholder groups. To whose interests should the inquiry attend and why? The call for collaborative inquiry is mediated by context and this implies social, historical, ecological, and cultural complexities. Collaboration must therefore be negotiated between evaluators and members of the program community, broadly defined, if collaborative inquiry in evaluation is to be meaningful, productive, and healthy for communities of program practice.</p>
<p>As a platform for negotiation we rely on our primary justifications for collaborative inquiry, described above. Political, pragmatic, and philosophical rationales for collaborative inquiry are not mutually exclusive but they align in varying degrees with stakeholder predispositions and ultimately shape the collaborative process and desired consequences toward either more practical (e.g., P-PE) or transformative (e.g., T-PE) ends. Practical motivations would include meeting demands for both accountability for responsible action and learning for improvement and positive change. Transformative interests include, for example, capacity building through developing evaluation habits of mind, questioning assumptions, and challenging the status quo (process use). That collaborative evaluative inquiry can generate new knowledge and insights and can contribute directly to ongoing developmental decisions and processes are other potential benefits and drivers that may shape the inquiry.</p>
<p><xref ref-type="fig" rid="fig4-1098214012464037">Figure 4</xref> helps us to grasp the essential processes of negotiation and relationship building. It is through dialogue and deliberation among evaluators and stakeholder communities that the complexities of context are understood and that the principal drivers for collaborative inquiry are identified, critiqued, and clarified. It is this process that will ultimately shape the inquiry and set the stage for deciding control, diversity of participation, and stakeholder engagement with the inquiry.</p>
<fig id="fig4-1098214012464037" position="float">
<label>Figure 4.</label>
<caption>
<p>Essential features of collaborative inquiry in evaluation (adapted from Shulha, 2010).</p>
</caption>
<graphic xlink:href="10.1177_1098214012464037-fig4.tif"/>
</fig>
<p>Understanding and embracing the concepts in <xref ref-type="fig" rid="fig4-1098214012464037">Figure 4</xref> and the relationships among them help us move beyond concerns about codifying, describing, or differentiating specific forms of collaborative inquiry and into a space where the ultimate form and shape of participation is principle-driven, not method-, model-, or specific approach-driven. It is in this direction that we would choose to invest in the interests of advancing the field.</p>
<p><xref ref-type="fig" rid="fig4-1098214012464037">Figure 4</xref> represents the program context as the ever-present filter through which subsequent activities and decisions flow. Understanding the context in which we work is central to what we do, why we do what we do and how, or the methods we use. As a context changes so will the appropriateness of decisions around the purpose and form of collaboration. The Cynefin framework (<xref ref-type="bibr" rid="bibr67-1098214012464037">Snowdon &amp; Boone, 2007</xref>) can be helpful in clarifying our understanding of the contexts in which we operate. “Cynefin” is a Welsh word (pronounced ‘coon-ev’in) meaning “habitat, acquainted, accustomed or familiar, being both noun and adjective, and thus requiring context to understand its meaning in any given instance” (<xref ref-type="bibr" rid="bibr56-1098214012464037">Patton, 2010</xref>, p. 106). <xref ref-type="bibr" rid="bibr67-1098214012464037">Snowdon and Boone (2007</xref>) developed this to guide organizational decision making and <xref ref-type="bibr" rid="bibr56-1098214012464037">Patton (2010</xref>) has adapted it to the evaluation context. In <xref ref-type="fig" rid="fig5-1098214012464037">Figure 5</xref>, we draw upon both in the interest of informing our thinking about collaborative approaches to inquiry.</p>
<fig id="fig5-1098214012464037" position="float">
<label>Figure 5.</label>
<caption>
<p>A modified Cynefin framework (Adapted from Snowdon &amp; Boone, 2007 and Patton, 2010).</p>
</caption>
<graphic xlink:href="10.1177_1098214012464037-fig5.tif"/>
</fig>
<p>As we can see in the lower right hand sector of <xref ref-type="fig" rid="fig5-1098214012464037">Figure 5</xref>, In <italic>simple</italic> situations, evaluation can be quite straightforward; for example, one measures the difference before and after an intervention. A controlled, predictable environment is assumed and clear cause and effect relationships are relatively easily discernible. There is a high degree of agreement about what the problem is and what should be the path to the solution. The evaluator assesses the facts of a situation, categorizes them, and draws conclusions based on established practice. “Best practices” are established and it is assumed that what worked in one context will work in another.</p>
<p>In a <italic>complicated</italic> context (see <xref ref-type="fig" rid="fig5-1098214012464037">Figure 5</xref> upper right sector), there may be more than one right answer and though there may be a clear cause and effect relationship, it may not be entirely evident (<xref ref-type="bibr" rid="bibr67-1098214012464037">Snowdon &amp; Boone, 2007</xref>). An evaluator must analyze a situation and look at the pros and cons of different options or possibilities. The context is not as controllable as in a simple situation, but it does nonetheless have some degree of predictability. Good practice rather than best practices is more appropriate here.</p>
<p><italic>Complex</italic> situations are unpredictable and in constant flux. As shown in the upper left sector of <xref ref-type="fig" rid="fig5-1098214012464037">Figure 5</xref>, Data are incomplete, there is no right answer; rather, over time, patterns can be discerned and a path forward emerges. In complex contexts, there are many opportunities for creativity and innovation; therefore, instead of attempting to impose a given method or draw conclusions too quickly, evaluation practice remains receptive to the unanticipated. Evaluators focus on identifying the initial conditions, monitoring and documenting what emerges, providing timely feedback, facilitating reflective practice among stakeholders, and embedding evaluative thinking in the process (<xref ref-type="bibr" rid="bibr56-1098214012464037">Patton, 2010</xref>).</p>
<p>In <italic>chaotic</italic> situations (see <xref ref-type="fig" rid="fig5-1098214012464037">Figure 5</xref> lower left sector), searching for right answers is pointless. There is no time for input; someone must take charge and decide what to do. The most important thing is to act immediately to “stop the bleeding” and establish some kind of order. An evaluator has a limited role here, other than to make immediate recommendations or even decisions, if leadership is otherwise lacking (<xref ref-type="bibr" rid="bibr67-1098214012464037">Snowdon &amp; Boone 2007</xref>).</p>
<p>The multiple and evolving contexts in which evaluators work are more often than not complex; that is, they are dynamic, constantly changing, and unpredictable; what <xref ref-type="bibr" rid="bibr75-1098214012464037">Zimmerman (2000</xref>) characterizes as inherent in “the messiness of real life.” Given this, it is far more productive to embrace the uncertainty, and rather than trying to control a situation, to seek out the unexpected, or what <xref ref-type="bibr" rid="bibr36-1098214012464037">Guijt (2008</xref>) calls “surprises,” as sources of learning. The key in complexity thinking is that it invites us to change the metaphor from “systems as machines” to “systems as living entities” (<xref ref-type="bibr" rid="bibr75-1098214012464037">Zimmerman, 2000</xref>). In the field of evaluation, Patton regards complexity as “the great unexplored frontier” (<xref ref-type="bibr" rid="bibr56-1098214012464037">Patton, 2010</xref>).</p>
<p>Complex systems do have patterns of behavior that appear stable within a larger stream of organizing activities. What is lost in focusing on the regularity of program patterns is how the individuals who give these patterns their meaning are continuously addressing pressures that emerge within and around them—essentially <italic>learning </italic>(<xref ref-type="bibr" rid="bibr23-1098214012464037">Davis, Sumara, &amp; Luce Kapler, 2008</xref>). When evaluators enter such systems they will, to a greater or lesser degree, cause a disturbance in the stream if not in one or more program patterns. Evaluators who are sensitive to the adaptations that arise in response to their presence and demonstrate their own willingness to learn appear better positioned to understand the contexts in which they are operating, the optimal purposes for evaluation, and the deliberations necessary to yield an appropriate form of collaboration.</p>
</sec>
<sec id="section7-1098214012464037">
<title>Toward a Coherent Set of Principles for Collaborative Inquiry in Evaluation</title>
<p>We conclude with sentiments that the field would be best served by serious work to develop principles of practice that allow ample flexibility to do what seems best given diversity in stakeholder interests, contextual complexity, cultural diversity, evaluator–stakeholder relations, and the like. In short, collaborative inquiry in evaluation is about <italic>approaches</italic> that should remain dynamic and adaptable to the exigencies of the evaluation context.</p>
<p>We acknowledge that there are prior attempts to consider or develop principles of practice in this regard. <xref ref-type="bibr" rid="bibr19-1098214012464037">Cousins and Whitmore (1998</xref>), for example, identified several categories of ideas that ought to be taken into account in collaborative inquiry, ideas that could easily serve as clues to principles of practice. The categories were power relations and their ramifications, ethics, participant selection, technical quality, cross-cultural issues, training, and conditions enabling PE. <xref ref-type="bibr" rid="bibr32-1098214012464037">Fetterman and Wandersman (2005</xref>) tackled the problem in a more direct way. Through a serious process of consultation, dialogue, and deliberation they generated a list of 10 principles. The process was laudable. But, despite their overt commitment to empowerment evaluation and the fostering of self-determination and transformative ends, the scope of the resulting set of principles extended well beyond this perspective. A review reveals that many would be appropriate in contexts where empowerment was not the driving force for evaluation. In addition, simultaneous adherence to all of the principles is most likely impossible.</p>
<p>Yet a directed approach, of the sort used by Fetterman and Wandersman is what is required. What is needed now, in our view, is a collaborative developmental process, involving evaluators and stakeholders to establish a working set of principles for collaborative inquiry as one holistic approach in evaluation. The principles would not be written in stone but rather they would be the subject of continuous analysis and renewal through dialogue and systematic inquiry. The strategies for developing these principles need to be fully and openly collaborative (we need to walk the talk) engaging diverse groups of people in an open and democratic, dialogic process. Moreover, we would propose that a set of working principles be subject to field testing and inquiry and that such inquiry should be, in and of itself, collaborative.</p>
</sec>
</sec>
<sec id="section8-1098214012464037">
<title>Conclusion</title>
<p>The delineation of a specific set of principle-development strategies is beyond the scope of this article, and it would not appropriately reflect the collaborative process we advocate. But, in addition to a reconsideration of the essential aspects of collaborative inquiry, this general direction is what we would offer as the article’s main contribution. To begin the conversation, we initiated a dialogic process with a think tank at the 2011 AEA conference. Follow-up conversations and perhaps even systematic inquiry are emerging from the initial think tank.</p>
<p>We have criticized recent developments in the field that are intended to advance theory and practice in collaborative inquiry in evaluation. Our intention has been to inform an agenda for future direction for the field. It is our hope that the ideas presented here will at least engender ongoing constructive discussion and dialogue.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="other" id="fn1-1098214012464037"><label>Authors’ Notes</label>
<p>A previous version of this article was presented at the Global Assembly of the International Development Evaluation Association (IDEAS), Amman, Jordan, April 2011. The authors thank Nathalie Gilbert for her assistance with the article.</p></fn>
<fn fn-type="conflict" id="fn2-1098214012464037"><label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn>
<fn fn-type="financial-disclosure" id="fn3-1098214012464037"><label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p></fn>
</fn-group>
<notes>
<title>Note</title>
<fn-group>
<fn fn-type="other" id="fn4-1098214012464037">
<label>1.</label>
<p>In 2011, at 1302 members, the TIG for Collaborative Participatory and Empowerment Evaluation numbers among the top five most popular AEA TIGs behind Quantitative Methods, Qualitative Methods, Non-Profits and Foundations and slightly ahead of Evaluation Use. www.eval.org.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1098214012464037">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Alcocer</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Lizárraga</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Delgadillo</surname>
<given-names>J.</given-names>
</name>
<etal/>
</person-group>. (<year>1997</year>). <source>A survey of the practice of participatory monitoring and evaluation methods in Bolivia</source>. <conf-name>Paper prepared for the International Workshop on participatory monitoring and evaluation: Experiences and lessons</conf-name>. <conf-loc>Cavite, Philippines</conf-loc>.</citation>
</ref>
<ref id="bibr2-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (<year>1990</year>). <source>Debates on evaluation</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr3-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (<year>1991</year>). <article-title>Evaluation theory</article-title>. In <person-group person-group-type="editor">
<name>
<surname>McLaughlin</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Phillips</surname>
<given-names>D.</given-names>
</name>
</person-group> (Eds.), <source>Evaluation at quarter century</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (Ed.). (<year>2004</year>). <source>Evaluation roots: Tracing theorists’ views and influences</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr5-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (Ed.). (<year>2012</year>). <source>Evaluation roots: Tracing theorists’ views and influences</source> (<edition>2nd ed</edition>.). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr6-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
<name>
<surname>Christie</surname>
<given-names>C. A.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>An evaluation theory tree</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (Ed.), <source>Evaluation roots: Tracing theorists’ views and influences</source> (pp. <fpage>12</fpage>–<lpage>65</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr7-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
<name>
<surname>Vo</surname>
<given-names>A. T.</given-names>
</name>
<name>
<surname>Hansen</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>not yet published</year>). <article-title>Using logic models to facilitate comparisons of evaluation theory</article-title>. <source>Evaluation and Program Planning</source>.</citation>
</ref>
<ref id="bibr8-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Armonia</surname>
<given-names>R. C.</given-names>
</name>
<name>
<surname>Campilan</surname>
<given-names>D. M.</given-names>
</name>
</person-group> (<year>1997</year>). <source>Participatory monitoring and evaluation: The Asian experience</source>. <comment>Regional overview paper for the International Workshop on participatory monitoring and evaluation: Experiences and lessons. Cavite, Philippines</comment>.</citation>
</ref>
<ref id="bibr9-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brant-Castellano</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>1986</year>). <article-title>Collective wisdom: Participatory research and Canada’s native people</article-title>. <source>Convergence: An International Journal of Adult Education</source>, <volume>19</volume>, <fpage>50</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr10-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Burke</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Evaluating for change: Reflections on participatory methodology</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Whitmore</surname>
<given-names>E.</given-names>
</name>
</person-group> (Ed.), <source>Understanding and practicing participatory evaluation. New Directions in Evaluation, No. 80</source> (pp. <fpage>43</fpage>–<lpage>56</lpage>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr11-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Campos</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Coupal</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>1996</year>). <source>Who are the question makers? A participatory evaluation handbook</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>UNDP Office of Evaluation and Strategic Planning</publisher-name>.</citation>
</ref>
<ref id="bibr12-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Carden</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Introduction to the forum on evaluation field building in South Asia</article-title>. <source>American Journal of Evaluation</source>, <volume>31</volume>, <fpage>291</fpage>–<lpage>221</lpage>.</citation>
</ref>
<ref id="bibr13-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chambers</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>1981</year>). <article-title>Rapid rural appraisal: Rationale and repertoire</article-title>. <source>Public Administration and Development</source>, <volume>1</volume>, <fpage>95</fpage>–<lpage>106</lpage>.</citation>
</ref>
<ref id="bibr14-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Will the real empowerment evaluation please stand up? A critical friend perspective</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Fetterman</surname>
<given-names>D. M.</given-names>
</name>
<name>
<surname>Wandersman</surname>
<given-names>A.</given-names>
</name>
</person-group> (Eds.), <source>Empowerment evaluation principles in practice</source> (pp. <fpage>183</fpage>–<lpage>208</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr15-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Chouinard</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2012</year>). <source>Participatory evaluation up close: A review and integration of research-based knowledge</source>. <publisher-loc>Charlotte, NC</publisher-loc>: <publisher-name>Information Age Press</publisher-name>.</citation>
</ref>
<ref id="bibr16b-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cousins</surname><given-names>J. B.</given-names></name>
<name><surname>Shulha</surname><given-names>L. M.</given-names></name></person-group>
 (<year>2008</year>). <article-title>Complexities in setting program standards in collaborative evaluation</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Brandon</surname><given-names>P.</given-names></name>
<name><surname>Smith</surname><given-names>N.</given-names></name></person-group> (Eds.), <source>Fundamental issues in evaluation</source> (pp. <fpage>139</fpage>–<lpage>158</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr16-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Decent</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Kenny</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Moore</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Pruden</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Sanderson</surname>
<given-names>K.</given-names>
</name>
<etal/>
</person-group>. (<year>2010, June</year>). <source>Multiple case study of community initiatives: National aboriginal youth suicide prevention strategy</source>. <publisher-loc>Ottawa, Canada</publisher-loc>: <publisher-name>Centre for Research on Evaluation and Community Services</publisher-name>.</citation>
</ref>
<ref id="bibr17-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Earl</surname>
<given-names>L. M.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>The case for participatory evaluation</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>14</volume>, <fpage>397</fpage>–<lpage>418</lpage>.</citation>
</ref>
<ref id="bibr18-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Earl</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Participatory evaluation in education: Studies in evaluation use and organizational learning</article-title>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Falmer</publisher-name>.</citation>
</ref>
<ref id="bibr19-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Whitmore</surname>
<given-names>E.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Framing participatory evaluation</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Whitmore</surname>
<given-names>E.</given-names>
</name>
</person-group> (Ed.), <source>Understanding and practicing participatory evaluation. New Directions in Evaluation, No. 80</source> (pp. <fpage>3</fpage>–<lpage>23</lpage>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey Bass</publisher-name>.</citation>
</ref>
<ref id="bibr20-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chouinard</surname>
<given-names>J. A.</given-names>
</name>
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Culturally competent evaluation for Aboriginal communities: A review of the empirical literature</article-title>. <source>Journal of Multidisciplinary Evaluation</source>, <volume>4</volume>, <fpage>40</fpage>–<lpage>57</lpage>.</citation>
</ref>
<ref id="bibr21-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Dahler-Larsen</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Learning-oriented educational evaluation in contemporary society</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Ryan</surname>
<given-names>K. E.</given-names>
</name>
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
</person-group> (Eds.), <source>Sage international handbook on educational evaluation</source> (pp. <fpage>307</fpage>–<lpage>322</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr22-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Daigneault</surname>
<given-names>P.-M.</given-names>
</name>
<name>
<surname>Jacob</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Toward accurate measurement of participation: Rethinking the conceptualization and operationalization of participatory evaluation</article-title>. <source>American Journal of Evaluation</source>, <volume>30</volume>, <fpage>330</fpage>–<lpage>348</lpage>.</citation>
</ref>
<ref id="bibr23a-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Davies</surname><given-names>R.</given-names></name>
<name><surname>Dart</surname><given-names>J.</given-names></name></person-group> 
(<year>2005</year>). <article-title>The <italic>‘most significant change’</italic> (MSC) Technique: A guide to its use</article-title>. <publisher-loc>Cambridge</publisher-loc>, <publisher-name>UK Authors</publisher-name>, <ext-link ext-link-type="uri" xlink:href="http://www.mande.co.uk/docs/MSCGuide.pdf">http://www.mande.co.uk/docs/MSCGuide.pdf</ext-link>.</citation>
</ref>
<ref id="bibr23-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Davis</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Sumara</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Luce-Kapler</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Engaging minds: Changing teaching in complex times</source> (<edition>2nd ed</edition>.). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge, Taylor &amp; Francis Group</publisher-name>.</citation>
</ref>
<ref id="bibr24-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Estrella</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Gaventa</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1998</year>). <source>Who counts reality? Participatory monitoring and evaluation: A literature review</source>. <comment>Institute for Development Studies Working Paper 70</comment>. <publisher-loc>Brighton, England</publisher-loc>: <publisher-name>IDS, University of Sussex</publisher-name>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="www.ids.ac.uk">www.ids.ac.uk</ext-link>
</citation>
</ref>
<ref id="bibr25-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fals Borda</surname>
<given-names>O.</given-names>
</name>
</person-group> (<year>1981</year>). <article-title>Science and the common people</article-title>. <source>Journal of Social Studies</source>, <volume>11</volume>, <fpage>1</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr26-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fals Borda</surname>
<given-names>O.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>Evolution and convergence in participatory action research</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Friederes</surname>
<given-names>J. S.</given-names>
</name>
</person-group> (Ed.), <source>A world of communities: Participatory research perspectives</source> (pp. <fpage>14</fpage>–<lpage>19</lpage>). <publisher-loc>North York, ON</publisher-loc>: <publisher-name>Captus University Publications</publisher-name>.</citation>
</ref>
<ref id="bibr27-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fals Borda</surname>
<given-names>O.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>The north-south convergence: A 30 year first person assessment of PAR</article-title>. <source>Action Research</source>, <volume>4</volume>, <fpage>351</fpage>–<lpage>358</lpage>.</citation>
</ref>
<ref id="bibr28-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fernandes</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Tandon</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>1981</year>). <source>Participatory research and evaluation: Experiments in research as a process of liberation</source>. <publisher-loc>New Delhi, India</publisher-loc>: <publisher-name>Indian Social Institute</publisher-name>.</citation>
</ref>
<ref id="bibr29-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fetterman</surname>
<given-names>D. W.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Empowerment evaluation</article-title>. <source>Evaluation Practice</source>, <volume>15</volume>, <fpage>1</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr30-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fetterman</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2001</year>). <source>Foundations of empowerment evaluation</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr32-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fetterman</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Wandersman</surname>
<given-names>A.</given-names>
</name>
</person-group> (Eds.). (<year>2005</year>). <source>Empowerment evaluation principles in practice</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr33-1098214012464037">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Fetterman</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Wandersman</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Webinar on participatory evaluation. Part 2 of CPE-TIG sponsored series</article-title>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="www.eval.org">www.eval.org</ext-link>
</citation>
</ref>
<ref id="bibr34-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Feuerstein</surname>
<given-names>M. T.</given-names>
</name>
</person-group> (<year>1986</year>). <source>Partners in evaluation: Evaluating development and community programs with participants</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Macmillan</publisher-name>.</citation>
</ref>
<ref id="bibr35-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gerring</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>What makes a concept good. A critical framework for understanding concept formation in the social sciences</article-title>. <source>Polity</source>, <volume>31</volume>, <fpage>357</fpage>–<lpage>393</lpage>.</citation>
</ref>
<ref id="bibr36-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Guijt</surname>
<given-names>I.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Seeking surprise: Rethinking monitoring for collective thinking in rural resource management</source>. <publisher-loc>Wageningen, Netherlands</publisher-loc>: <publisher-name>University of Wageningen</publisher-name>.</citation>
</ref>
<ref id="bibr37-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hall</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1975</year>). <article-title>Participatory research: An approach for change</article-title>. <source>Convergence, An International Journal of Adult Education</source>, <volume>8</volume>, <fpage>24</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr38-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hall</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1981</year>). <article-title>Participatory research, popular knowledge and power: A personal reflection</article-title>. <source>Convergence, An International Journal of Adult Education</source>, <volume>14</volume>, <fpage>5</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr39-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hall</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>From margin to centre? The development and purpose of participatory research</article-title>. <source>American Sociologist</source>, <volume>23</volume>, <fpage>15</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr40-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hay</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Evaluation field building in South Asia: Reflections, anecdotes, and questions</article-title>. <source>American Journal of Evaluation</source>, <volume>31</volume>, <fpage>222</fpage>–<lpage>231</lpage>.</citation>
</ref>
<ref id="bibr41-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Hansen</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Wallace</surname>
<given-names>T. L</given-names>
</name>
</person-group> (<year>2010</year>). <source>Creating visual representations of evaluation theories</source>. <comment>Working paper (draft)</comment>. <publisher-loc>Los Angeles, CA</publisher-loc>: <publisher-name>University of California</publisher-name>.</citation>
</ref>
<ref id="bibr42-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hansen</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
<name>
<surname>Wallace</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>Epub ahead of print</year>). <article-title>Depicting the logic of three evaluation theories</article-title>. <source>Evaluation and Program Planning</source>.</citation>
</ref>
<ref id="bibr43-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Harner</surname>
<given-names>M. A.</given-names>
</name>
</person-group> (<year>2012</year>). <source>Theory building through praxis discourse: A theory- and practice-informed model of transformative participatory evaluation</source>. <comment>Unpublished doctoral dissertation, Claremont Graduate University, Claremont: CA</comment>.</citation>
</ref>
<ref id="bibr44-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hoare</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Levy</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Robinson</surname>
<given-names>M. P.</given-names>
</name>
</person-group> (<year>1993</year>). <article-title>Participatory action research in Native communities: Cultural opportunities and legal obligations</article-title>. <source>Canadian Journal of Native Studies</source> <volume>13</volume>, <fpage>43</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr45-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jackson</surname>
<given-names>E. T.</given-names>
</name>
<name>
<surname>McCaskill</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Hall</surname>
<given-names>B.</given-names>
</name>
</person-group> (Eds.). (<year>1982</year>). <article-title>Learning for self-determination: Community based options for Native training and research</article-title>. <source>Canadian Journal of Native Studies</source>, <volume>2</volume>, <fpage>1</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr46-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Jackson</surname>
<given-names>E. T.</given-names>
</name>
<name>
<surname>Kassam</surname>
<given-names>Y.</given-names>
</name>
</person-group> (<year>1998</year>). <source>Knowledge shared: Participatory evaluation in development cooperation</source>. <publisher-loc>West Hartford, CT</publisher-loc>: <publisher-name>Kumarian</publisher-name>.</citation>
</ref>
<ref id="bibr47-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kar</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Chambers</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Handbook on community-led total sanitation</source>. <publisher-loc>Brighton, England</publisher-loc>: <publisher-name>Plan International UK &amp; Institute for Development Studies, University of Sussex</publisher-name>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="www.communityledtotalsanitation.org/sites/communityledtotalsanitation.org/files/cltshandbook.pdf">www.communityledtotalsanitation.org/sites/communityledtotalsanitation.org/files/cltshandbook.pdf</ext-link>
</citation>
</ref>
<ref id="bibr48-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kassam</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Mustafa</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>1982</year>). <source>Participatory research: An emerging alternative methodology in social science research</source>. <publisher-loc>Toronto, Canada</publisher-loc>: <publisher-name>ICAE</publisher-name>.</citation>
</ref>
<ref id="bibr49-1098214012464037">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Luskin</surname>
<given-names>R. J. C.</given-names>
</name>
</person-group> (<year>2010</year>). <source>Comparing the outcomes of three theories of evaluation</source>. <conf-name>Paper presented at the annual meeting of the American Evaluation Association</conf-name>, <conf-loc>San Antonio</conf-loc>.</citation>
</ref>
<ref id="bibr50-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Matheson</surname>
<given-names>S.</given-names>
</name>
</person-group> (Ed.). (<year>2007</year>). <source>Enduring issues in evaluation: 20th anniversary of the association between NDE and AEA. New Directions for Evaluation. No. 144</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-ssssBass</publisher-name>.</citation>
</ref>
<ref id="bibr51-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Miller</surname>
<given-names>R. L.</given-names>
</name>
</person-group> (<year>Epub ahead of print</year>). <article-title>Logic models: A useful way to study theories of evaluation practice?</article-title> <source>Evaluation and Program Planning</source>.</citation>
</ref>
<ref id="bibr52-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Narayan</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1994</year>). <source>Participatory evaluation: Tools for managing change in water and sanitation</source>. <comment>World Bank Technical Paper No. 207</comment>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>World Bank</publisher-name>.</citation>
</ref>
<ref id="bibr53-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>O'Sullivan</surname>
<given-names>R. G.</given-names>
</name>
</person-group> (<year>2004</year>). <source>Practicing evaluation: A collaborative approach</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr54-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>O'Sullivan</surname>
<given-names>R. G.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Collaborative evaluation within a framework of stakeholder-oriented evaluation approaches</article-title>. <source>Evaluation and Program Planning</source>, <volume>35</volume>, <fpage>518</fpage>–<lpage>522</lpage>.</citation>
</ref>
<ref id="bibr55-1098214012464037">
<citation citation-type="book">
<collab collab-type="author">PAMFORK</collab>. (<year>1997</year>). <source>Participatory Methodologies Forum in Kenya</source>. <comment>Report of the workshop in using participatory methodologies for monitoring and evaluation. Nairobi, Kenya: South-South Sharing Forum</comment>.</citation>
</ref>
<ref id="bibr56-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Patton</surname>
<given-names>M. Q.</given-names>
</name>
</person-group> (<year>2010</year>). <source>Developmental evaluation: Applying complexity concepts to enhance innovation and use</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford Press</publisher-name>.</citation>
</ref>
<ref id="bibr57-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Preskill</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Evaluation’s second act: Spotlight on learning</article-title>. <source>American Journal of Evaluation</source>, <volume>29</volume>, <fpage>127</fpage>–<lpage>138</lpage>.</citation>
</ref>
<ref id="bibr58-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ramalingam</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Jones</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Reba</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Young</surname>
<given-names>J</given-names>
</name>
</person-group>.) (<year>2008</year>). <source>Exploring the science of complexity: Ideas and implications for development and humanitarian efforts</source>. <comment>Working Paper 285</comment>. <edition>2nd ed</edition>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Overseas Development Institute (ODI</publisher-name>).</citation>
</ref>
<ref id="bibr59-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Rihani</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2002</year>). <source>Complexity systems theory and development practice: Understanding non-linear realities</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Zed Books</publisher-name>.</citation>
</ref>
<ref id="bibr60-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Rodriguez-Campos</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2005</year>). <source>Collaborative evaluation: A step-by-step model for the evaluator</source>. <publisher-loc>Tamarac, FL</publisher-loc>: <publisher-name>Lumina Press</publisher-name>.</citation>
</ref>
<ref id="bibr61-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rodriguez-Campos</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Advances in collaborative evaluation</article-title>. <source>Evaluation and Program Planning</source>, <volume>35</volume>, <fpage>523</fpage>–<lpage>528</lpage>.</citation>
</ref>
<ref id="bibr62-1098214012464037">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Rodriguez-Campos</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>O’Sullivan</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2011</year>). <source>Webinar on collaborative evaluation</source>. <comment>Part 3 of CPE-TIG sponsored series. Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="www.eval.org">www.eval.org</ext-link>
</citation>
</ref>
<ref id="bibr63-1098214012464037">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Rugh</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Steinke</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Bamberger</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2010</year>). <source>Summary of discussion of the ‘Alternative to the Statistical Counterfactual Think Tank.’</source> <conf-name>Paper presented at the American Evaluation Association</conf-name>, <conf-loc>San Antonio</conf-loc>.</citation>
</ref>
<ref id="bibr64-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scriven</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Empowerment evaluation examined</article-title>. <source>Evaluation Practice</source>, <volume>18</volume>, <fpage>165</fpage>–<lpage>175</lpage>.</citation>
</ref>
<ref id="bibr65a-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Serrat</surname><given-names>O.</given-names></name></person-group> (<year>2009</year>). <article-title>The Most Significant Change Technique</article-title>. <source>Knowledge Solutions</source>, <comment>25 (January)</comment>, <volume>1–4</volume>, <ext-link ext-link-type="uri" xlink:href="www.adb.org/knowledgesolutions">www.adb.org/knowledgesolutions</ext-link>.</citation>
</ref>
<ref id="bibr65-1098214012464037">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Shulha</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2010, Nov</year>.). <source>Essential aspects of participatory evaluation</source>. <conf-name>Paper presented at the annual meeting of the American Evaluation Association</conf-name>, <conf-loc>San Antonio</conf-loc>.</citation>
</ref>
<ref id="bibr66-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Shulha</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Wilson</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Collaborative evaluation case example</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Earl</surname>
<given-names>L.</given-names>
</name>
</person-group> (Eds.), <source>Participatory evaluation in education: Studies in evaluation use and organizational learning</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Falmer</publisher-name>.</citation>
</ref>
<ref id="bibr67-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Snowdon</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Boone</surname>
<given-names>M. E.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A leader’s framework for decision making</article-title>. <source>Harvard Business Review</source>, <volume>85</volume>, <fpage>69</fpage>–<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr68-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stufflebeam</surname>
<given-names>D. L.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Empowerment evaluation, objectivist evaluation, and evaluation standards: Where the future of evaluation should not go and where it needs to go</article-title>. <source>American Journal of Evaluation</source>, <volume>15</volume>, <fpage>321</fpage>–<lpage>338</lpage>.</citation>
</ref>
<ref id="bibr69-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tandon</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>1981</year>). <article-title>Participatory research in the empowerment of people</article-title>. <source>Convergence</source>, <volume>14</volume>, <fpage>20</fpage>–<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr70-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tandon</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Participation, citizenship and democracy: Reflections on 25 years of PRIA</article-title>. <source>Community Development Journal</source>, <volume>43</volume>, <fpage>284</fpage>–<lpage>296</lpage>.</citation>
</ref>
<ref id="bibr71-1098214012464037">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Weaver</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Cousins</surname>
<given-names>J. B.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Unpacking the participatory process</article-title>. <source>Journal of Multidisciplinary Evaluation</source>, <volume>1</volume>, <fpage>19</fpage>–<lpage>40</lpage>.</citation>
</ref>
<ref id="bibr72-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Whitmore</surname>
<given-names>E.</given-names>
</name>
</person-group> (<year>1998a</year>). <source>Understanding and practicing participatory evaluation. New Directions in Evaluation, No. 80</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr73-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Whitmore</surname>
<given-names>E.</given-names>
</name>
</person-group> (<year>1998b</year>). <article-title>We need to rebuild this house. The role of empowerment evaluation of a Mexican farmers’ cooperative</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Jackson</surname>
<given-names>E. T.</given-names>
</name>
<name>
<surname>Kassam</surname>
<given-names>Y.</given-names>
</name>
</person-group> (Eds.), <source>Knowledge shared: Participatory evaluation in development cooperation</source> (pp. <fpage>217</fpage>–<lpage>230</lpage>). <publisher-loc>West Hartford, CN</publisher-loc>: <publisher-name>Kumarian Press</publisher-name>.</citation>
</ref>
<ref id="bibr74-1098214012464037">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Woodhill</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>M&amp;E as learning: Rethinking the dominant paradigm</article-title>: In <person-group person-group-type="editor">
<name>
<surname>deGraaf</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Camerson</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Sombatpanit</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Pieri</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Woodhill</surname>
<given-names>J.</given-names>
</name>
</person-group> (Eds.), <source>Monitoring and evaluation of social conservation and watershed development projects</source> (pp. <fpage>83</fpage>–<lpage>107</lpage>). <publisher-loc>Enfield, New Hampshire</publisher-loc>: <publisher-name>Science Publishers</publisher-name>.</citation>
</ref>
<ref id="bibr75-1098214012464037">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Zimmerman</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>A complexity science primer: What is complexity science and why should I learn about it?</article-title> <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="www.plexusinstitute.com/edgeware">www.plexusinstitute.com/edgeware</ext-link>
</citation>
</ref>
<ref id="bibr76-1098214012464037">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Zukoski</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Luluqisen</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Webinar on participatory evaluation</article-title>. <comment>Part 2 of CPE-TIG sponsored series. Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="www.eval.org">www.eval.org</ext-link>
</citation>
</ref>
</ref-list>
</back>
</article>