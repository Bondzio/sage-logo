<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LTJ</journal-id>
<journal-id journal-id-type="hwp">spltj</journal-id>
<journal-title>Language Testing</journal-title>
<issn pub-type="ppub">0265-5322</issn>
<issn pub-type="epub">1477-0946</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0265532213480336</article-id>
<article-id pub-id-type="publisher-id">10.1177_0265532213480336</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Developing the assessment literacy of university proficiency test users</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Inbar-Lourie</surname><given-names>Ofra</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>O’Loughlin</surname><given-names>Kieran</given-names></name>
</contrib>
<aff id="aff1-0265532213480336">The University of Melbourne, Australia</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0265532213480336">Kieran O’Loughlin, Melbourne Graduate School of Education, Level 2, 100 Leicester St., The University of Melbourne, Victoria 3010, Australia. Email: <email>kjo@unimelb.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>3</issue>
<issue-title>Special Issue on Language Assessment Literacy</issue-title>
<fpage>363</fpage>
<lpage>380</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The rapidly increasing use of English language proficiency test scores by universities around the world to select international students has resulted in a range of admissions, marketing, academic and teaching support staff interacting with the tests in different ways. To date, there has been little research investigating the assessment literacy needs of these test score users. This article focuses on a study into the nature of these needs, how well they are currently being met and how they might be best addressed in the future. Data about these questions in relation to the most widely accepted test in Australian universities, IELTS (International English Language Testing System), were collected from an online survey completed by 50 members of staff who used the test in their work and from follow-up interviews with 15 of these survey respondents. The results indicated that the participants mostly needed information about IELTS for advising prospective students about English language entry requirements and making admissions decisions. They mainly focused on the minimum test scores required for entry and believed their informational needs were reasonably well met by their institution’s entry regulations and the IELTS official website. The most popular methods for learning about the IELTS test were information sessions and online tutorials. The results and their implications are discussed in detail.</p>
</abstract>
<kwd-group>
<kwd>Assessment literacy</kwd>
<kwd>proficiency testing</kwd>
<kwd>test score users</kwd>
<kwd>university admissions</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The concept of language assessment literacy is underpinned by a view of language assessment as a social practice rather than simply as a technical activity (<xref ref-type="bibr" rid="bibr14-0265532213480336">Inbar-Lourie, 2008</xref>; McNamara &amp; Roever, 2006). It potentially includes the acquisition of a range of skills related to test production, test score interpretation and use, and test evaluation in conjunction with the development of a critical understanding about the roles and functions of assessment within education and society. Furthermore, its relevance is not restricted to language testing specialists and language teachers but also to educational policy developers and test users, such as university admissions staff. As <xref ref-type="bibr" rid="bibr29-0265532213480336">Taylor (2009</xref>, p. 25) argues:
<disp-quote>
<p>an appropriate level of assessment literacy needs to be nurtured not just among engineers and technicians who are actively involved in test development or research activities, or even among applied linguists and language teachers involved in delivering language education, but much more broadly in the public domain if a better understanding of the function and values of assessment tools and their outcomes are to be realised across society.</p>
</disp-quote>
</p>
<p>There is a growing literature on the education of language testers and, more recently, language teachers (<xref ref-type="bibr" rid="bibr3-0265532213480336">Brindley, 2001</xref>; <xref ref-type="bibr" rid="bibr4-0265532213480336">Brown &amp; Bailey, 2008</xref>; <xref ref-type="bibr" rid="bibr7-0265532213480336">Davies, 2008</xref>; <xref ref-type="bibr" rid="bibr9-0265532213480336">Fulcher, 2012</xref>; <xref ref-type="bibr" rid="bibr14-0265532213480336">Inbar-Lourie, 2008</xref>; <xref ref-type="bibr" rid="bibr17-0265532213480336">Jin, 2010</xref>; <xref ref-type="bibr" rid="bibr18-0265532213480336">Kleinsasser, 2005</xref>; <xref ref-type="bibr" rid="bibr19-0265532213480336">Malone, 2008</xref>; <xref ref-type="bibr" rid="bibr22-0265532213480336">O’Loughlin, 2006</xref>). Most recently, <xref ref-type="bibr" rid="bibr9-0265532213480336">Fulcher (2012)</xref> comprehensively defines assessment literacy for language teachers in the following terms: first, the knowledge, skills and abilities they require to work with standardized or classroom based tests; second, their familiarity with test processes and awareness of the principles and concepts guiding practice; and, third, their ability to place knowledge about language assessment within wider historical, social, political and philosophical frameworks to better understand and evaluate its practices.</p>
<p>To date, however, these discussions have neglected the assessment literacy needs of another important group of stakeholders, that is, test users, such as university staff, who interpret and use language test scores. These test users are a very diverse group of individuals; in the university context, for example, they include admissions, marketing, academic and English language (preparation and in-course support) staff, who may require different levels of knowledge about English entry requirements in general and proficiency tests in particular for their specific professional roles. These roles may include setting minimum entry levels on proficiency tests for university entry, advising prospective students about entry requirements and newly enrolled students about future language learning, designing publications and guides for prospective students, and, most importantly, making admissions decisions. Given the high-stakes nature of their work, it is critical that the staff who exercise these roles do so in an informed and ethical manner in the interests of valid test interpretation and use. Without appropriate levels of assessment literacy, there is a serious risk of the misuse of language tests that powerfully impact the life chances of individuals applying for university places (<xref ref-type="bibr" rid="bibr26-0265532213480336">Shohamy, 2001</xref>).</p>
<sec id="section1-0265532213480336">
<title>The interpretation and use of proficiency test scores for university admission</title>
<p>Standardized proficiency tests are widely used around the world for assessing the readiness of students for whom English is an additional language to study in English-medium universities. In many universities, especially at the undergraduate level, the scores from one of these tests may be the only evidence of English language ability used. <xref ref-type="bibr" rid="bibr28-0265532213480336">Spolsky (2008</xref>, p. 300) suggests that ‘an oversimplified view of the ease of producing meaningful measurement is held by the general public and educational administrators alike.’ He argues that the language testing field must assume some responsibility for this state of affairs. He claims that language testers have mistakenly focused on reducing measurement error in tests rather than trying to understand the risk of making decisions about the fate of human beings using fallible language tests. They have also continued to produce unidimensional scales that fail to do justice to the complex multidimensional nature of language ability. Most importantly, <xref ref-type="bibr" rid="bibr28-0265532213480336">Spolsky (2008</xref>, p. 302) argues:
<disp-quote>
<p>We have also continued to ignore a critical factor in our use of proficiency measures to make admissions for immigration or other decisions about individuals, the fact that proficiency is dynamic and that a single measure at one time does not permit predicting what it will be later.</p>
</disp-quote>
</p>
<p>In relation to making university admissions decisions, <xref ref-type="bibr" rid="bibr28-0265532213480336">Spolsky (2008</xref>, p. 303) advocates the use of an aptitude test as well as a proficiency test to provide ‘a better prediction of how well and how quickly a foreign student would fit into an English-language environment.’</p>
<p>While more than one form of evidence relating to English proficiency is considered in some UK and US university contexts (<xref ref-type="bibr" rid="bibr2-0265532213480336">Banerjee, 2003</xref>; <xref ref-type="bibr" rid="bibr30-0265532213480336">Van Nelson, Nelson &amp; Malone, 2004</xref>), this is normally not the case in other countries. In Australia, for example, decisions about English proficiency are usually based on a single source of evidence, such as scores from a standardized language test. The problems associated with this practice are exacerbated by the increasing commodification of proficiency test scores in a highly competitive educational marketplace: cut scores decisions in Australian universities may be based not so much in relation to the language skills needed to study successfully but on boosting revenue by recruiting the greatest possible number of full-fee-paying international students (<xref ref-type="bibr" rid="bibr31-0265532213480336">Victorian Ombudsman, 2011</xref>).</p>
<p>As <xref ref-type="bibr" rid="bibr21-0265532213480336">Messick (1996)</xref> has suggested, the validity of a test hinges critically on the interpretation of test scores and the uses to which they are directed. University staff need to be educated to take greater responsibility for the valid and ethical uses of proficiency test scores. These responsibilities are set out clearly in the <italic>Standards for Educational and Psychological Testing</italic> (<xref ref-type="bibr" rid="bibr1-0265532213480336">American Educational Research Association, American Psychological Association, and National Council on Measurement in Education, 1999</xref>, p.112), the guidelines on test use of the <xref ref-type="bibr" rid="bibr16-0265532213480336">International Test Commission (2000)</xref> and the <xref ref-type="bibr" rid="bibr15-0265532213480336">International Language Testing Association’s (2007)</xref> Code of Practice. These standards and guidelines imply that university members of staff need to develop a strong understanding of the tests that they interpret and use for high-stakes admissions decisions. This responsibility includes setting educationally responsible minimum entry scores, understanding the limitations of scores from a single proficiency test, as <xref ref-type="bibr" rid="bibr28-0265532213480336">Spolsky (2008)</xref> suggests, and, where feasible, using additional data about such factors as an individual applicant’s language aptitude, motivation and language learning background in making decisions about their readiness to commence study in an English-medium institution (<xref ref-type="bibr" rid="bibr25-0265532213480336">Rees, 1999</xref>). A key question which arises here is what particular knowledge and skills university staff need to acquire about proficiency testing in order to fulfill their responsibilities as test users and how they might best do so.</p>
</sec>
<sec id="section2-0265532213480336">
<title>Interpretation and use of the IELTS test</title>
<p>This article focuses on understanding the assessment literacy needs of university staff working with one high-stakes, standardized English proficiency test, the IELTS (International English Language Testing System), in Australian universities. In recent years there has been a growing interest in studying how this test is used in higher education institutions in a number of countries (<xref ref-type="bibr" rid="bibr5-0265532213480336">Coleman, Starfield &amp; Hagan, 2003</xref>; <xref ref-type="bibr" rid="bibr6-0265532213480336">Coley, 1999</xref>; <xref ref-type="bibr" rid="bibr8-0265532213480336">Deakin, 1997</xref>; <xref ref-type="bibr" rid="bibr20-0265532213480336">McDowell &amp; Merrylees, 1998</xref>; Rea-Dickins, Kiely &amp; Yu, 2004; <xref ref-type="bibr" rid="bibr23-0265532213480336">O’Loughlin, 2011</xref>).</p>
<p><xref ref-type="bibr" rid="bibr5-0265532213480336">Coleman, Starfield and Hagan (2003)</xref> examined the attitudes of both students and staff (administrative and academic) towards IELTS in three institutions in Australia, China and the UK. They found that, while all participants in the study were generally positively disposed towards the test, overall students were more knowledgeable than staff about the test and more convinced that the institution’s IELTS entry level was appropriate for the course of study they were undertaking. Staff generally felt that the IELTS scores should be higher and that many students’ English language ability was not adequate for their chosen course of study. Perhaps the most troubling finding was that the university staff (administrative and academic) in the three participating institutions demonstrated low understanding of the meaning of IELTS scores.</p>
<p>In her doctoral research project, <xref ref-type="bibr" rid="bibr2-0265532213480336">Banerjee (2003)</xref> examined the use of proficiency test scores, including the IELTS, in the selection of students for postgraduate degree programs at a UK university. She found that the selection of international students at the University of Lancaster was a complex, holistic decision-making process based on the recommendation of an academic staff member taking into account a wide range of criteria. Yet, like <xref ref-type="bibr" rid="bibr5-0265532213480336">Coleman, Starfield and Hagan (2003)</xref>, <xref ref-type="bibr" rid="bibr2-0265532213480336">Banerjee (2003)</xref> found that academic admissions officers were not very knowledgeable about the meaning of proficiency test scores, including IELTS.</p>
<p>In a study conducted at another UK university, the University of Bristol, <xref ref-type="bibr" rid="bibr24-0265532213480336">Rea-Dickins, Kiely &amp; Yu (2007)</xref> also found that university admissions staff were not always sufficiently knowledgeable about the meaning of IELTS scores. They argued for stronger training of admissions tutors in order that they become better informed about the meanings of IELTS score profiles. This included awareness of and access to the IELTS website.</p>
<p><xref ref-type="bibr" rid="bibr23-0265532213480336">O’Loughlin (2011)</xref> examined the use of IELTS within a large faculty of a major Australian university. The author reported first, that there was no rational basis for setting IELTS minimum entry scores in this context. There was also no tracking of student success in order to validate these entry requirements. Second, applicants’ entry scores were not considered in relation to other relevant individual factors as recommended in these guidelines. Third, IELTS scores were not used to guide future English language learning. This suggested that there had been no beneficial educational consequences flowing from use of the test. The author concluded therefore that the selection policy and procedures in this context did not match the standards required for the valid and ethical interpretation and use of test scores.</p>
<p><xref ref-type="bibr" rid="bibr23-0265532213480336">O’Loughlin (2011)</xref> also reported variable levels of knowledge about IELTS (both of the test and the scores it produces) amongst university staff, including a lack of understanding as to what different IELTS scores imply about a student’s language ability, their readiness for university study, and their need for further English development. However, it was noted there may be little motivation for some of these test users to improve their assessment literacy within the Australian university admissions regime where readiness to study in the medium of English is judged simply on whether an applicant has attained the minimum IELTS scores (or other acceptable alternatives) required by the university.</p>
<p>IELTS is the most widely accepted English proficiency test in Australian universities although a broad range of alternatives including other proficiency tests, educational qualifications, preparatory English programs and previous university study in English are also deemed to satisfy their English entry requirements (<xref ref-type="bibr" rid="bibr6-0265532213480336">Coley, 1999</xref>). Each university sets its own entry standards for local and international students although these standards must be consistent with the Federal Government’s regulations for issuing international student visas. In order to enter a program of study, all applicants must normally meet just two key criteria: the minimum academic and English requirements for that program. These two requirements are treated separately in the selection process, and applicants must demonstrate that they have clearly fulfilled each of them before they are selected. This is the current policy context in which the assessment literacy of IELTS test score users in Australian universities needs to be explored.</p>
</sec>
<sec id="section3-0265532213480336">
<title>Aims of the study</title>
<p>The study investigated the assessment literacy needs of a range of university staff in relation to the IELTS test in Australian higher education and how well they are currently being met. These needs relate to a range of possible issues, including the purpose and content of the test, the meaning of test scores, the appropriateness of cut-off levels, the test’s validity, reliability, predictive power and comparability with other accepted forms of evidence of English proficiency. The key research questions addressed in this study were as follows:</p>
<list id="list1-0265532213480336" list-type="order">
<list-item><p>What are the assessment literacy needs of university IELTS test score users?</p></list-item>
<list-item><p>How well are these needs currently being met?</p></list-item>
<list-item><p>What other approach(es) could be adopted to meet these needs?</p></list-item>
</list>
</sec>
<sec id="section4-0265532213480336" sec-type="methods">
<title>Methodology</title>
<p>The study focused on the assessment literacy needs of a range of staff at two large metropolitan Australian universities, hereafter referred to as University A and University B. The two universities were included in the study to obtain a broader sample of participants. In both universities more than 25% of the student population has been international over the last five years. The IELTS test is the most well-established and well-known form of English proficiency evidence used for student selection at both institutions.</p>
<p>The study aimed to recruit volunteer participants working in a range of roles across both universities including admissions, marketing, academic, and both pre-course and in-course support language staff. It was anticipated that some staff would be currently using IELTS test scores directly in their work while others would be using them more indirectly or perhaps not at all. It was decided, however, that no staff who volunteered would be excluded from the project since their readiness to participate indicated that the IELTS test had some relevance to their work. Participants identified themselves as either test users or non-users at the first stage of data collection. The focus in this article is on the data gathered from the test users.</p>
<p>The study focused on both the ‘subjective’ needs (i.e., those identified by the study participants themselves as IELTS test users) and their ‘objective’ needs (i.e., those identified by other parties such as, in this study, the owners of IELTS (the British Council, Cambridge ESOL and IDP/IELTS Australia), as well as by the researcher and research assistant in relation to assessment literacy. The specific objective needs of IELTS test users were analysed by referring to the <xref ref-type="bibr" rid="bibr12-0265532213480336"><italic>IELTS Handbook</italic> (2007)</xref>, the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref>, the IELTS official website (<ext-link ext-link-type="uri" xlink:href="http://www.ielts.org">www.ielts.org</ext-link>), as well as the websites of the two universities targeted in the study.</p>
<p>In the first instance, data was gathered using an online survey powered by Survey Monkey™ (<ext-link ext-link-type="uri" xlink:href="http://www.surveymonkey.com">www.surveymonkey.com</ext-link>). Except for one open-ended item, the survey used exclusively forced-choice items. The first section of the survey asked respondents to identify their professional roles, whether they used the IELTS test and for what purposes. Only those who self-identified as test users in this first section were then directed to the second section of the survey in which they answered questions about the IELTS test information they used, the sources they used to access it and how useful it was to their work. In the third section, all respondents were required to read and evaluate the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref> in terms of its usefulness and informativeness and then to choose between other methods of learning about the test.</p>
<p>Semi-structured interviews were conducted on an individual basis with all survey respondents who volunteered to take part approximately one month after they had completed the survey. In the interviews these participants were first shown their questionnaire responses to refresh their memories and then invited to clarify and elaborate on their questionnaire responses. Field notes were taken during the interviews and audio-recordings were made of the interviews for subsequent analysis.</p>
<p>The survey responses were analysed in terms of comparative frequencies and, where appropriate, percentages. Each interview was summarized and the main themes were identified. Salient quotations which reflected these themes were then transcribed and coded in terms of whether they were (a) representative of all interviewees, (b) representative of a particular sub-group, for example marketing or academic staff, or (c) individual comments.</p>
</sec>
<sec id="section5-0265532213480336" sec-type="results">
<title>Results</title>
<sec id="section6-0265532213480336">
<title>Participant information</title>
<p>A total of 84 participants (43 participants from University A and 41 participants from University B) completed the online survey. In Questions 2 and 3 of the survey, participants were asked to identify whether they used the IELTS test in their jobs as well as to specify their work area. A total of 50 (23 from University A and 27 from University B) of these participants identified themselves as IELTS test users in the survey. The focus of this article is on the combined data collected from these participants who directly used the test in their work. <xref ref-type="fig" rid="fig1-0265532213480336">Figure 1</xref> provides a breakdown of the work areas of these test users in terms of frequencies and percentages.</p>
<fig id="fig1-0265532213480336" position="float">
<label>Figure 1.</label>
<caption>
<p>Work areas of IELTS test users (<italic>N</italic> = 50).</p>
</caption>
<graphic xlink:href="10.1177_0265532213480336-fig1.tif"/>
</fig>
<p>Staff interviews were conducted following completion of the staff surveys. Semi-structured interviews were conducted with all of the survey respondents who volunteered to take part in them. Fifteen (7 from University A and 8 from University B) of the 50 IELTS test users who completed the survey were interviewed. These interviewees included 2 admissions, 6 marketing, 2 academic 3 language and 2 other staff members.</p>
</sec>
<sec id="section7-0265532213480336">
<title>Research questions</title>
<p>The results for each of the three research questions are reported below.</p>
<sec id="section8-0265532213480336">
<title>Research Question 1: What are the assessment literacy needs of university IELTS test score users?</title>
<p>This question was addressed using data from Questions 4 and 5 of the survey and associated interviews. Question 4 asked: <italic>‘What do you need the IELTS test for in your current job? Please tick ALL responses that apply.’</italic> <xref ref-type="fig" rid="fig2-0265532213480336">Figure 2</xref> shows the uses of the IELTS test for the 50 IELTS test users in terms of frequencies. The two main purposes identified by the survey respondents at both universities were advising prospective students about English language entry requirements and making student admission decisions. The interviews clarified which categories of staff carried out these two functions. While only admissions and some academic staff made admission decisions, staff in all categories (including marketing, language and other staff as well as admissions and academic staff) had an advisory role with prospective students. The range of matters these staff might provide to students included the IELTS entry requirements for particular programs, the English programs that were available to prepare for the test, where and how to register for the IELTS test, the nature and scoring of the test, and how much English instruction they might need to attain the scores required to enter different academic programs. However, individual interviewees normally mentioned only one or sometimes two of these topics. It is noteworthy that only five respondents used the IELTS test to set cut-off levels for university entry. This suggested either that the survey respondents were not sufficiently senior to do such work or that setting and revising minimum entry requirements were not frequently undertaken at either university. Only five respondents indicated and identified other purposes for using the test, which were communicating with other university staff and overseas agents, and making scholarship decisions.</p>
<fig id="fig2-0265532213480336" position="float">
<label>Figure 2.</label>
<caption>
<p>Purposes for using the IELTS test (<italic>N</italic> = 50).</p>
</caption>
<graphic xlink:href="10.1177_0265532213480336-fig2.tif"/>
</fig>
<p>Question 5 on the survey asked the respondents <italic>‘What specific information about the IELTS test do you use in your job? Please tick ALL responses that apply.’</italic> Fifteen topics, identified by the researcher and researcher assistant as potentially most relevant to the work of university staff on the basis of information about the test given on the IELTS official website (<ext-link ext-link-type="uri" xlink:href="http://www.ielts.org">www.ielts.org</ext-link>), the <xref ref-type="bibr" rid="bibr12-0265532213480336"><italic>IELTS Handbook</italic> (2007)</xref>, the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref> and the <xref ref-type="bibr" rid="bibr13-0265532213480336"><italic>IELTS Scores Explained</italic> DVD (2009)</xref>, were listed as options for this topic. These topics were therefore included as the core informational needs of university staff. <xref ref-type="fig" rid="fig3-0265532213480336">Figure 3</xref> shows the results for the 50 IELTS test users for each of the fifteen topics listed in terms of frequencies. It indicates that more than 50% of the IELTS test users at both universities reported using information on the last four of these topics: the minimum IELTS entry scores for entry to courses at their university, the different components of the IELTS test, how long the IELTS test scores are valid, and the relationship between IELTS test scores and other evidence of English proficiency accepted by their university. Between 33% and 50% of respondents indicated that they also accessed information about the distinction between the Academic and General Training modules, the validity and reliability of the IELTS test scores, the meaning of the overall IELTS band scores and the recognition of the test locally and internationally.</p>
<fig id="fig3-0265532213480336" position="float">
<label>Figure 3.</label>
<caption>
<p>Information about the IELTS test used in the workplace (<italic>N</italic> = 50).</p>
</caption>
<graphic xlink:href="10.1177_0265532213480336-fig3.tif"/>
</fig>
<p>In the interviews, participants were asked whether any of the other types of specific information that they didn’t initially select in Question 5 of the survey might be helpful in their jobs. Ten of the 15 interviewees answered ‘no’ to this question because they did not require additional information about the IELTS test for their work. For example, one marketing staff member suggested that such information ‘would not be relevant because I operationalize a policy which is already in place’. In a similar vein, an admissions staff member stated that “a lot of these things [i.e., specific information about IELTS] I don’t know, but it wouldn’t make a difference [if I did]”. The other five test users suggested they might use some of the information they had not initially selected in Question 5. For example, one marketing staff member suggested she might use the information about the free online test report verification service that was explained in the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref>. In more general terms, another marketing participant indicated she would be personally interested to know more about the test so that she could allay some of students’ concerns about the test.</p>
<p>The survey and interview results for this research question suggested that university staff are mostly concerned with knowing about or having access to ‘surface’ information about the IELTS test, especially the minimum test scores required for entry to particular courses at their respective universities. To a lesser extent they also use information about more ‘interpretative’ aspects of test quality such as the meaning, validity and reliability of the test scores. However, given that many of them also advised prospective students, one might expect more of them to have accessed and used a wider range of information about the test such as how to best prepare for the test, the nature and scoring of the test and how the scores are reported.</p>
</sec>
<sec id="section9-0265532213480336">
<title>Research Question 2: How well are these needs currently being met?</title>
<p>This question was addressed using data collected from Question 6 and 7 of the survey and associated interview questions. Question 6 in the survey asked <italic>‘Which sources of information about the IELTS test do you mainly use in your job? Please TICK all responses that apply.’</italic> The majority of users (84% – 42/50) reported that they accessed their institution’s English language entry regulations which are available on each university’s website. Slightly less than half of them (42% – 21/50) used the IELTS official website. Only six of the 50 users indicated that they accessed the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref> which could be downloaded from the IELTS website. None of the respondents in this study used the <xref ref-type="bibr" rid="bibr13-0265532213480336"><italic>IELTS Scores Explained</italic> DVD (2009)</xref> which must be purchased from the IELTS partners. <xref ref-type="fig" rid="fig4-0265532213480336">Figure 4</xref> shows the results for this question as frequencies.</p>
<fig id="fig4-0265532213480336" position="float">
<label>Figure 4.</label>
<caption>
<p>Sources of information about the IELTS test accessed by users (<italic>N</italic> = 50).</p>
</caption>
<graphic xlink:href="10.1177_0265532213480336-fig4.tif"/>
</fig>
<p>Of the 15 IELTS users interviewed, nine said they accessed their institution’s English language entry regulations available on their websites, mainly for checking information about the minimum acceptable IELTS scores for particular courses. Nine interviewees said they were aware of one or more of the other sources of information listed and the remaining six had no knowledge of them. Only two of them knew about the DVD but had not used it. Six of them said that they would ask colleagues in the language centre or IELTS test centre attached to their institutions if they required additional information about the test. For example, one marketing staff member said she did not know about the sources listed in the survey other than her own university’s English language regulations and said she “would ask for information from colleagues” in their affiliated language centre if needed.</p>
<p>Most interviewees suggested that there needed to be links on their respective institution’s website where information about its English language entry regulations were given to the IELTS official website. However, subsequent checking showed that both university websites did include these links, although they were perhaps not sufficiently prominent. One marketing staff member also suggested that better information could be included on her institution’s website to make it “more user-friendly to students by giving information about the test, how to practise and prepare”.</p>
<p>Question 7 of the survey asked <italic>‘How useful is the information you currently access about each of the following aspects of the IELTS test? Please tick the appropriate number on the scale from 0–3, 0 = Not Useful, 1 = Slightly Useful, 2 = Useful and 3 = Very Useful’</italic>. The aspects listed were the same as those in Question 5. <xref ref-type="fig" rid="fig5-0265532213480336">Figure 5</xref> shows the results for this question in terms of frequencies.</p>
<fig id="fig5-0265532213480336" position="float">
<label>Figure 5.</label>
<caption>
<p>Usefulness of the information accessed about the IELTS test (<italic>N</italic> = 50).</p>
</caption>
<graphic xlink:href="10.1177_0265532213480336-fig5.tif"/>
</fig>
<p>On the one hand, 60% (30/50) of respondents saw the information provided about the minimum IELTS test scores for entry into specific courses at their university as being most useful. On the other hand, 48% of IELTS users (24/50) considered that the information about (a) how candidates can prepare for the test and (b) the IELTS test centres and how to register for the IELTS test’ was not useful. Again, this is surprising given that 78% (39/50) of them indicated that they advised prospective students about English language requirements in Question 4 of the survey (see <xref ref-type="fig" rid="fig2-0265532213480336">Figure 2</xref>). Presumably, this advisory role would require them to provide more information than simply the minimum test scores required for entry into a particular course to these students.</p>
<p>The most common theme in the interviews was that if a selected topic was marked as not or slightly useful, it was because the interviewees did not see it as relevant to their job. One academic staff member involved in selection explained that information about certain aspects of IELTS (e.g. how the different components of the IELTS test are scored) was not useful. He indicated that knowing about them would make no difference to the selection process as it currently operated where decisions are based solely on whether the applicants have achieved minimum test scores and that his judgment was therefore not required. Comparing past practices to those of the present day he suggested that
<disp-quote>
<p>you would work a lot harder and try to know more about these things when I was able to exercise some judgment in borderline cases, but once people are beholden to numbers that judgment goes out of the window … why do you have to bother knowing about things … with IELTS? The way the university does it, it’s just magic numbers.</p>
</disp-quote>
</p>
<p>This comment underscores a serious problem with current selection practices in Australian universities where admissions staff members only need to check whether an applicant has achieved the minimum acceptable IELTS scores to enter their course. As this interviewee suggests, they are usually not permitted to exercise judgment around admissions decisions and therefore do not require any real understanding of the test for this purpose. The rationale for this approach is that it is more equitable (i.e., every applicant is judged in exactly the same way) and efficient than a more multi-evidenced, holistic one. However, given the uncertainty of proficiency test scores (<xref ref-type="bibr" rid="bibr28-0265532213480336">Spolsky, 2008</xref>), the fairness of entry decisions about English proficiency based on a single set of test scores is highly questionable.</p>
<p>In Questions 8 and 9 of the survey, data was also gathered on the perceived usefulness of a publication published by the IELTS partners for educational institutions, governments, professional bodies and commercial organizations, the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref>. The respondents were first required to read its contents before answering Question 8: ‘<italic>How informative is each section of the IELTS Guide?</italic>’ on a four-point scale: not informative, slightly informative, informative and very informative. All sections of the <xref ref-type="bibr" rid="bibr11-0265532213480336">IELTS Guide (2009)</xref> were found to be informative or very informative by 74% or more of respondents. The interviewees who dissented from this majority view claimed that it gave insufficient detail about the meaning of the different band levels and needed to include more research evidence to support its claim that it is the best test to use. Question 9 was an optional open-ended item which asked them ‘<italic>What other information do you think is missing from the IELTS Guide?</italic>’ One third of the comments recorded (12/36) focused on the need for more information about the meaning and interpretation of IELTS scores. An important issue raised by several respondents here was the so-called ‘jagged profile’ where there is considerable variation between one or more of an applicant’s band scores in the four tests of listening, speaking, reading and writing. This can mean that the applicant does not meet the specified minimum scores in one or more of these individual tests even though they have attained a satisfactory overall score. The same academic staff interviewee quoted above suggested in relation to this issue:
<disp-quote>
<p>It would be lovely to see a person with [an overall band score of] 7.5 or 8.0 with a band of 5.0 or 5.5 in one area. Then you would be able to show samples of that to your PVC [Pro-Vice Chancellor] and say ‘Why wouldn’t I take that student?’ It would mean something, because the PVC probably knows less than me about this.</p>
</disp-quote>
</p>
<p>This comment highlights potential instances where discretionary judgement could be appropriately used to make admissions decisions. However, this would require a higher level of understanding of test scores than is necessary under the current selection regime, including an understanding of how the scores in each of the four sub-tests are derived and the relative significance of each of these scores as indicators of applicant’s language abilities to pursue their preferred course of study. A score of 5.5 in listening, for example, may not have as negative implications for academic success than the same score for writing. Finally, several interviewees suggested that the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref> could be improved by including more targetted material about the test that is relevant for different staff roles.</p>
<p>Overall, the results for research question 2 indicated that the needs of most, but certainly not all, IELTS test users in this study were reasonably well met. More than 80% of respondents at both universities relied on their institution’s English language entry regulations and, to a lesser extent, the IELTS official website for information about the IELTS test. They considered that the most useful information provided to them related to the minimum IELTS entry scores for entry to specific courses, followed by the relationship between IELTS test scores and other evidence of readiness to study in the medium of English, and the international and local recognition of the IELTS. This suggested they took a rather narrow view of their informational needs, especially in relation to advising prospective students. A strong majority of the IELTS users found all sections of the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref> to be informative although some respondents would have liked to know more about how to understand and interpret IELTS scores.</p>
</sec>
<sec id="section10-0265532213480336">
<title>Research Question 3: What other approach(es) could be adopted to meet these needs?</title>
<p>The question was addressed using data from Question 10 of the survey and related interview questions in the interviews. Question 10 asked respondents <italic>‘What other ways of learning about the IELTS test would be useful? Please tick all responses that apply.’</italic></p>
<p><xref ref-type="fig" rid="fig6-0265532213480336">Figure 6</xref> provides the results for Question 10. It indicates that the most popular alternatives to the IELTS Guide were online tutorials and information sessions. In the interviews the majority (10/15 or 67%) of the interviewees preferred these ways of learning about the IELTS test to the more static document, the <xref ref-type="bibr" rid="bibr11-0265532213480336"><italic>IELTS Guide</italic> (2009)</xref>. The main reason given by the interviewees who most preferred online tutorials was the ease of access and control in terms of how and when they could use them. They also emphasized their potential comprehensiveness and links to other helpful resources such as academic journal articles and research papers.</p>
<fig id="fig6-0265532213480336" position="float">
<label>Figure 6.</label>
<caption>
<p>Other useful ways of learning about the IELTS test (<italic>N</italic> = 50).</p>
</caption>
<graphic xlink:href="10.1177_0265532213480336-fig6.tif"/>
</fig>
<p>However, a small minority of interviewees were not in favour of online tutorials. One marketing interviewee said that, “you have a lot of on-line things: and you wouldn’t do them … unless they are mandatory, people wouldn’t do them”. The interviewees who preferred information sessions and workshops highlighted their interactive nature as the chief advantage. For instance, those interviewees who favoured workshops saw them as an opportunity for staff to deepen their understanding of IELTS scores through group discussion facilitated by experts.</p>
<p>An interesting alternative to the options listed in Question 10 of the survey suggested by two of the interviewees was the opportunity for staff to take a free IELTS test. According to a language staff interviewee, this strategy would help to dispel some misconceptions about the test on the part of some test users:
<disp-quote>
<p>I am not sure what admission and other staff members know about IELTS … I think it [sitting the test] would let them learn more directly about some … basic things, like there is no pass and fail in IELTS … there are a lot of myths and misconceptions like that … IELTS in a way has become a magic word.</p>
</disp-quote>
</p>
<p>This is an important recommendation that underscores the value to university staff of sitting the test in order to demystify it. This kind of first-hand knowledge of the test is all too rare amongst student advisors, marketing, admissions and academic staff.</p>
<p>According to one admissions staff member, sitting the test would also help student advisors to have “more empathy and understanding when talking to students” which, in turn, would “make the students feel confident you know what you are talking about.” A major problem currently is that applicants are not always sure that they are given accurate advice about preparing for and taking the test, and then understanding the meaning and implications of their test scores.</p>
</sec>
</sec>
</sec>
<sec id="section11-0265532213480336" sec-type="discussion">
<title>Discussion</title>
<p>From one perspective, the implications of the main findings in this study are relatively straightforward. The IELTS partners should produce mostly online educational materials, including a tutorial, that build the assessment literacy of test users on a fairly limited range of topics so that they can fulfill their main roles in advising prospective students and making admission decisions within the current assessment régime in Australian universities. It is also desirable to cater to different kinds of university staff within the online tutorial format. In recent years, the IELTS partners have made a considerable effort to place more information in the public domain about the content and purpose of the IELTS test, the meaning and interpretation of band scores and the ways IELTS scores can be interpreted and used validly, reliably and responsibly in decision-making in higher education contexts. In response to the findings of this study in particular they are in the process of developing an online tutorial for university test users to supplement their revised test user guide, information sessions, workshops and multi-media products.</p>
<p>Yet, from a deeper perspective, there are fundamental questions to be addressed about current admission policies and practices in Australian universities which, in turn, raise issues for what assessment literacy about the IELTS test (and indeed all of the other measures used for the same purpose) is now and what it might become in the future. Currently, university applicants are assessed in a simple, lockstep manner. They must first of all meet the academic requirements and, second, the English entry requirements of a particular course at the university of their choice. As far as English proficiency tests are concerned, they must have obtained the minimum acceptable scores (which are often subject to downward pressure because of competition with other universities to attract full-fee-paying international students), and there is usually no flexibility around this decision. However, given the limitations and “inevitable uncertainty” (<xref ref-type="bibr" rid="bibr27-0265532213480336">Spolsky, 1995</xref>, p. 358) of all proficiency test scores in terms of their validity and reliability, they should be carefully interpreted in relation to other relevant information about applicants’ language ability to strengthen the admissions decision. Such a shift in approach by Australian universities would require a major change in selection policy and procedures. The IELTS partners are clearly aware of this important issue. The <xref ref-type="bibr" rid="bibr12-0265532213480336"><italic>IELTS Handbook</italic> (2007</xref>, p. 5), in particular, suggests that “receiving institutions should also consider a candidate’s IELTS results in the context of a number of factors including age and motivation, educational and cultural background, first language and language learning history.” This crucial recommendation correctly implies that IELTS minimum entry scores should not necessarily be rigidly applied in the decision-making process but, instead, should be interpreted in relation to these kinds of individual factors. Institutions need to take responsibility for gathering relevant information about individual applicants and implementing this more contextual, holistic approach to the interpretation and use of test scores. However, they need guidance from the IELTS partners, language testing experts and university authorities on how to do so. Similar procedures, which have been adopted at individual universities in countries such as the UK and the USA, also need to be mapped and evaluated to provide the IELTS partners with models of good practice. This is an important area for future development of resources for IELTS users. Adaptive and interactive online tutorials that provide more advanced, in-depth training to those members of staff who require it may be the best option currently available for this purpose.</p>
<p>If admissions staff in Australian universities were required to make informed, holistic judgments about the language proficiency of international applicants, then they (and the academic, marketing and English language staff who would need to understand how admission decisions were made) would need to be much better informed about the IELTS and other proficiency tests. Within such an admissions culture the term “test wiseness,” as redeployed by <xref ref-type="bibr" rid="bibr29-0265532213480336">Taylor (2009)</xref>, could be taken to mean the understandings that university staff need to acquire to promote and/or uphold valid and ethical test use as well as positive test impact. Of particular relevance here is one aspect of <xref ref-type="bibr" rid="bibr9-0265532213480336">Fulcher’s (2012</xref>, p. 125) definition of assessment literacy, that is, “familiarity with test processes [and] awareness of [the] principles and concepts that guide and underpin practice” which would include validity, reliability, test fairness, interpretation and use of test scores, and related ethical considerations. Clearly, most test users do not need either the knowledge, skills and abilities to construct and evaluate language tests or an understanding of the wider historical, social, political and philosophical contexts of language testing, the other two of Fulcher’s components of assessment literacy. However, better knowledge of the processes, principles and concepts of language testing would enable university staff to exercise their responsibilities as informed test users and, it is hoped, to become advocates of valid and ethical practice (including both advice-giving and decision-making) in their educational institutions.</p>
</sec>
<sec id="section12-0265532213480336" sec-type="conclusions">
<title>Conclusion</title>
<p>This study has identified the need for university proficiency test users to be educated for the valid and responsible interpretation and use of test scores and not simply trained to check whether applicants have met pre-specified minimum scores. This is consistent with a more cautious and informed view of language testing. A key element of this education is understanding the fallibility of proficiency test scores. Such a perspective necessitates adopting a more holistic judgment of an individual’s communicative language ability which, in turn, relies on using multiple sources of evidence to make important decisions affecting their life chances such as university entry. This kind of approach may help to mitigate the potentially negative effects resulting from overreliance on scores from a single proficiency test in this decision-making process. It may also help test users to provide more personalized and informed advice to university applicants about language learning and testing. This is a very important role performed by university staff but one which has not been given sufficient attention in the literature. Given their focus on the interpretation and use of test scores, these test users have more restricted but certainly no less important assessment literacy needs than language testers and teachers who need to acquire a much broader knowledge base ‘grounded in theory and epistemological beliefs, and connected to other bodies of knowledge in education, linguistics and applied linguistics’ (<xref ref-type="bibr" rid="bibr14-0265532213480336">Inbar-Lourie, 2008</xref>, p. 396). The challenge is to make university staff aware of and take seriously their responsibilities as ethical test users whose advice-giving and decision-making may have far-reaching impacts on the lives of their prospective students.</p>
<p>Finally, the methodology used in the study on which this article is based provides a working model for other institution-based research into language assessment literacy. Such local studies could help to initiate a wider agenda of research sponsored by university authorities, as well as government and test agencies, since all stakeholders and test users have an interest in language assessments being understood and used appropriately. This kind of system-wide research would also help to enrich the theorization of assessment literacy for the various audiences to which language tests address themselves.</p>
</sec>
</body>
<back>
<ack>
<p>I owe thanks to the university staff who participated in the study, Reza Tasviri for his valuable assistance with the data collection, data analysis and preparation of this report, and to Basil Alzougool for his work on design of the survey and analysis of the results. I am also grateful to the reviewers of an earlier version of the article for their helpful suggestions.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The study on which this article was based was funded by IELTS Australia through the IELTS joint-funded research program.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0265532213480336">
<citation citation-type="book">
<collab>American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education</collab> (<year>1999</year>). <source>Standards for educational and psychological testing</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr2-0265532213480336">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Banerjee</surname><given-names>J. V.</given-names></name>
</person-group> (<year>2003</year>). <source>Interpreting and using proficiency test scores</source>. (Unpublished PhD thesis). <publisher-name>Lancaster University</publisher-name>, <publisher-loc>Lancaster, UK</publisher-loc>.</citation>
</ref>
<ref id="bibr3-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brindley</surname><given-names>G.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Language assessment and professional development</article-title>. In <person-group person-group-type="editor">
<name><surname>Elder</surname><given-names>C.</given-names></name>
<name><surname>Brown</surname><given-names>A.</given-names></name>
<name><surname>Hill</surname><given-names>K.</given-names></name>
<name><surname>Iwashita</surname><given-names>N.</given-names></name>
<name><surname>Lumley</surname><given-names>T.</given-names></name>
<name><surname>McNamara</surname><given-names>T.</given-names></name>
<name><surname>O’Loughlin</surname><given-names>K.</given-names></name>
</person-group> (Eds.), <source>Experimenting with uncertainty: Essays in honour of Alan Davies</source> (pp. <fpage>126</fpage>–<lpage>136</lpage>). <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>J. D.</given-names></name>
<name><surname>Bailey</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Language testing courses: What are they in 2007?</article-title> <source>Language Testing</source>, <volume>25</volume>(<issue>3</issue>), <fpage>349</fpage>–<lpage>383</lpage>.</citation>
</ref>
<ref id="bibr5-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Coleman</surname><given-names>D. S.</given-names></name>
<name><surname>Starfield</surname><given-names>S.</given-names></name>
<name><surname>Hagan</surname><given-names>A.</given-names></name>
</person-group> (<year>2003</year>). <source>The attitudes of IELTS stakeholders: Student and staff perceptions of IELTS in Australia, UK and Chinese tertiary institutions</source>. (IELTS research reports, <volume>Vol. 5</volume>). <publisher-loc>Canberra</publisher-loc>: <publisher-name>IDP Education Australia</publisher-name>.</citation>
</ref>
<ref id="bibr6-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Coley</surname><given-names>M.</given-names></name>
</person-group> (<year>1999</year>). <article-title>The English language entry requirements of Australian universities for students of non-English speaking background</article-title>. <source>Higher Education Research &amp; Development</source>, <volume>18</volume>(<issue>1</issue>), <fpage>7</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr7-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Davies</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Textbook trends in teaching language testing</article-title>. <source>Language Testing</source>, <volume>25</volume>(<issue>3</issue>), <fpage>327</fpage>–<lpage>347</lpage>.</citation>
</ref>
<ref id="bibr8-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Deakin</surname><given-names>G.</given-names></name>
</person-group> (<year>1997</year>). <article-title>IELTS in context: Issues in EAP for overseas students</article-title>. <source>EA Journal</source>, <volume>15</volume>(<issue>2</issue>), <fpage>7</fpage>–<lpage>1</lpage>.</citation>
</ref>
<ref id="bibr9-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fulcher</surname><given-names>G.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Assessment literacy for the language classroom</article-title>. <source>Language Assessment Quarterly</source>, <volume>9</volume>(<issue>2</issue>), <fpage>113</fpage>–<lpage>132</lpage>.</citation>
</ref>
<ref id="bibr10-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hyatt</surname><given-names>D.</given-names></name>
<name><surname>Brooks</surname><given-names>G.</given-names></name>
</person-group> (<year>2009</year>). <source>Investigating stakeholders’ perceptions of IELTS as an entry requirement for higher education in the UK</source>. (IELTS research reports, <volume>Vol. 10</volume>). <publisher-loc>Manchester</publisher-loc>: <publisher-name>The British Council and Canberra: IDP Education Australia</publisher-name>.</citation>
</ref>
<ref id="bibr11-0265532213480336">
<citation citation-type="book">
<collab>IELTS Guide</collab> (<year>2009</year>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge ESOL, the British Council and IDP Education Australia</publisher-name>.</citation>
</ref>
<ref id="bibr12-0265532213480336">
<citation citation-type="book">
<collab>IELTS Handbook</collab> (<year>2007</year>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge ESOL, the British Council and IDP Education Australia</publisher-name>.</citation>
</ref>
<ref id="bibr13-0265532213480336">
<citation citation-type="book">
<collab>IELTS Scores explained [DVD]</collab> (<year>2009</year>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge ESOL</publisher-name>.</citation>
</ref>
<ref id="bibr14-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Inbar-Lourie</surname><given-names>O.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Constructing a language assessment knowledge base: A focus on language assessment courses</article-title>. <source>Language Testing</source>, <volume>25</volume>(<issue>3</issue>), <fpage>385</fpage>–<lpage>402</lpage>.</citation>
</ref>
<ref id="bibr15-0265532213480336">
<citation citation-type="web">
<collab>International Language Testing Association</collab> (<year>2007</year>). <source>Guidelines for practice</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.iltaonline.com">www.iltaonline.com</ext-link></citation>
</ref>
<ref id="bibr16-0265532213480336">
<citation citation-type="web">
<collab>International Testing Commission</collab> (<year>2000</year>). <source>International guidelines for test use</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.intestcom.org">www.intestcom.org</ext-link></citation>
</ref>
<ref id="bibr17-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jin</surname><given-names>Y.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The place of language testing and assessment in the professional preparation of foreign language teachers in China</article-title>. <source>Language Testing</source>, <volume>27</volume>(<issue>4</issue>), <fpage>555</fpage>–<lpage>584</lpage>.</citation>
</ref>
<ref id="bibr18-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kleinsasser</surname><given-names>R. C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Transforming a postgraduate level assessment course: A second language teacher educator’s narrative</article-title>. <source>Prospect</source>, <volume>20</volume>(<issue>3</issue>), <fpage>77</fpage>–<lpage>102</lpage>.</citation>
</ref>
<ref id="bibr19-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Malone</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Training in language assessment</article-title>. In <person-group person-group-type="editor">
<name><surname>Shohamy</surname><given-names>E.</given-names></name>
<name><surname>Hornberger</surname><given-names>N. H.</given-names></name>
</person-group> (Eds.), <source>Encyclopaedia of language and education (2nd ed., Vol. 7, pp. 225–239).</source> <publisher-name>Springer Science and Business Media LLC</publisher-name>.</citation>
</ref>
<ref id="bibr20-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McDowell</surname><given-names>C.</given-names></name>
<name><surname>Merrylees</surname><given-names>B.</given-names></name>
</person-group> (<year>1998</year>). <source>Survey of receiving institutions’ use and attitude to IELTS</source> (IELTS Research Reports, <volume>Vol. 1</volume>). <publisher-loc>Canberra</publisher-loc>: <publisher-name>IDP Education Australia</publisher-name>.</citation>
</ref>
<ref id="bibr21-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Messick</surname><given-names>S.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Validity and washback in language testing</article-title>. <source>Language Testing</source>, <volume>13</volume>, <fpage>241</fpage>–<lpage>256</lpage>.</citation>
</ref>
<ref id="bibr22-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>O’Loughlin</surname><given-names>K.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Learning about second language assessment: insights from a postgraduate student on-line forum</article-title>. <source>University of Sydney Papers in TESOL</source>, <volume>1</volume>(<issue>1</issue>), <fpage>71</fpage>–<lpage>85</lpage>.</citation>
</ref>
<ref id="bibr23-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>O’Loughlin</surname><given-names>K.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The interpretation and use of proficiency test scores in university selection: How valid and ethical are they?</article-title> <source>Language Assessment Quarterly</source>, <volume>8</volume>(<issue>2</issue>), <fpage>146</fpage>–<lpage>160</lpage>.</citation>
</ref>
<ref id="bibr24-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rea-Dickins</surname><given-names>P. R.</given-names></name>
<name><surname>Kiely</surname><given-names>R.</given-names></name>
<name><surname>Yu</surname><given-names>G.</given-names></name>
</person-group> (<year>2007</year>). <source>Student identity, learning and progression: The affective and academic impact of IELTS on ‘successful’ candidates</source>. (IELTS research reports, <volume>Vol. 7</volume>). <publisher-loc>Manchester</publisher-loc>: <publisher-name>British Council and Canberra: IDP Education Australia</publisher-name>.</citation>
</ref>
<ref id="bibr25-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rees</surname><given-names>J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Counting the cost of international assessment: Why universities</article-title> <month>may</month> <article-title>need a second opinion</article-title>. <source>Assessment and Evaluation in Higher Education</source>, <volume>24</volume>(<issue>4</issue>), <fpage>427</fpage>–<lpage>438</lpage>.</citation>
</ref>
<ref id="bibr26-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shohamy</surname><given-names>E.</given-names></name>
</person-group> (<year>2001</year>). <source>The power of tests</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Longman</publisher-name>.</citation>
</ref>
<ref id="bibr27-0265532213480336">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Spolsky</surname><given-names>B.</given-names></name>
</person-group> (<year>1995</year>). <source>Measured words: The development of objective language testing</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr28-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spolsky</surname><given-names>B.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Introduction – <italic>Language Testing</italic> at 25: Maturity and responsibility?</article-title> <source>Language Testing</source>, <volume>25</volume>(<issue>3</issue>), <fpage>297</fpage>–<lpage>305</lpage>.</citation>
</ref>
<ref id="bibr29-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Developing assessment literacy</article-title>. <source>Annual Review of Applied Linguistics</source>, <volume>29</volume>, <fpage>21</fpage>–<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr30-0265532213480336">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van Nelson</surname><given-names>C.</given-names></name>
<name><surname>Nelson</surname><given-names>J. S.</given-names></name>
<name><surname>Malone</surname><given-names>B. G.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Predicting success of international graduate students in an American university</article-title>. <source>College and University Journal</source>, <volume>80</volume>(<issue>1</issue>), <fpage>19</fpage>–<lpage>27</lpage>.</citation>
</ref>
<ref id="bibr31-0265532213480336">
<citation citation-type="book">
<collab>Victorian Ombudsman</collab> (<year>2011</year>). <source>Investigation into how universities deal with international students</source>. <publisher-loc>Melbourne</publisher-loc>: <publisher-name>Victorian Government</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>