<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">IBE</journal-id>
<journal-id journal-id-type="hwp">spibe</journal-id>
<journal-title>Indoor and Built Environment</journal-title>
<issn pub-type="ppub">1420-326X</issn>
<issn pub-type="epub">1423-0070</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1420326X11423163</article-id>
<article-id pub-id-type="publisher-id">10.1177_1420326X11423163</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Original Papers</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Recognition of Human Home Activities via Depth Silhouettes and ℜ Transformation for Smart Homes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Jalal</surname><given-names>Ahmad</given-names></name>
<xref ref-type="aff" rid="aff1-1420326X11423163">a</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Uddin</surname><given-names>Md. Zia</given-names></name>
<xref ref-type="aff" rid="aff2-1420326X11423163">b</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kim</surname><given-names>Jeong Tai</given-names></name>
<xref ref-type="aff" rid="aff3-1420326X11423163">c</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Kim</surname><given-names>Tae-Seong</given-names></name>
<xref ref-type="aff" rid="aff1-1420326X11423163">a</xref>
<xref ref-type="corresp" rid="corresp1-1420326X11423163"/>
</contrib></contrib-group>
<aff id="aff1-1420326X11423163"><label>a</label>Department of Biomedical Engineering, Kyung Hee University, Yongin, Korea</aff>
<aff id="aff2-1420326X11423163"><label>b</label>Department of Electronic Engineering, Inha University, Incheon, Korea</aff>
<aff id="aff3-1420326X11423163"><label>c</label>Department of Architecture Engineering, Kyung Hee University, Yongin, Korea</aff>
<author-notes>
<corresp id="corresp1-1420326X11423163">Tae-Seong Kim, Department of Biomedical Engineering, Kyung Hee University, 1 Seocheon-dong, Giheung-gu, Yongin-si, Gyeonggi-do 446-701, Republic of Korea. Tel. +82-31-201-3731, Fax +82-31-201-3666, E-Mail <email>tskim@khu.ac.kr</email></corresp></author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>21</volume>
<issue>1</issue>
<issue-title>Special Issue on: Sustainable Development for Future Building</issue-title>
<issue-title>Edited by: Jeong Tai Kim</issue-title>
<fpage>184</fpage>
<lpage>190</lpage>
<history>
<date date-type="accepted"><day>22</day><month>7</month><year>2011</year></date>
</history>
<permissions>
<copyright-statement>© The Author(s), 2011. Reprints and permissions: http://www.sagepub.co.uk/journalsPermissions.nav</copyright-statement>
<copyright-year>2011</copyright-year>
<copyright-holder content-type="society">International Society of the Built Environment</copyright-holder>
</permissions>
<abstract>
<p>This paper presents a novel human home activity recognition (HAR) system designed for smart homes that utilize depth silhouettes and ℜ transformation to continuously recognize the daily activities of the elderly and disabled in an indoor environment for better lifecare and e-healthcare services. Previously, ℜ transformation has been applied only on binary silhouettes that provide only the shape information of human activities. In this work, ℜ transformation was utilized on depth silhouettes such that the depth information of human body parts could be used in HAR in addition to the shape information. In ℜ transformation, 2D directional projection maps are computed via Radon transform, and then 1D feature profiles, which are translation and scaling invariants. Then, by applying the principle component analysis and linear discriminant analysis, the prominent activity features would be extracted. Finally, Hidden Markov Models would be used to train and recognize daily home activities. The results showed a mean recognition rate of 96.55% over ten typical home activities, whereas the same system utilizing binary silhouettes could achieve only 85.75%. The proposed methodology should be useful in designing and developing a compact HAR system that can be practically used in a smart environment including smart home for the care of the elderly, infirmed or disabled people.</p></abstract>
<kwd-group>
<kwd>Human activity recognition</kwd>
<kwd>Depth silhouettes</kwd>
<kwd>R transformation</kwd>
<kwd>Smart home system</kwd></kwd-group>
</article-meta>
</front>
<body>
<sec id="sec1-1420326X11423163" sec-type="intro"><title>Introduction</title>
<p>To preserve and promote healthy life of the elderly in an indoor environment, smart homes are being considered as a potential solution. In smart homes, smart home systems would provide various services: such as supporting the disabled users via specialised interface and sensitive remote control; monitoring the resident’s lifestyle via wearable systems and infrared sensors; delivering therapy for their better health care and emotional comfort of people living alone via interactive robots. To make these services feasible, one of the key technologies is to recognize various human activities at homes: i.e., a human activity recognition (HAR) system that can recognize daily activities of residents including the elderly. General approach of the video-based HAR system would be to extract some significant features from video and use these features to train a classifier and perform recognition [<xref ref-type="bibr" rid="bibr1-1420326X11423163">1</xref>]. A HAR system can keep a continuous observation on basic human activities of daily living, enabling various services such as lifecare from physical damage, nursing, rehabilitation and health assistance to make a more intelligent home environment [<xref ref-type="bibr" rid="bibr1-1420326X11423163">1</xref>]. In recent years, HAR has become a key component in a smart home system, providing various applications such as home activity monitoring and e-healthcare [<xref ref-type="bibr" rid="bibr2-1420326X11423163">2</xref>]. Numerous smart home projects (e.g., Microsoft Easyliving project and House_n group at Massachusetts Institute of Technology) [<xref ref-type="bibr" rid="bibr1-1420326X11423163">1</xref>] are currently underway.</p>
<p>In the video-based HAR, typically binary silhouettes [<xref ref-type="bibr" rid="bibr3-1420326X11423163">3</xref>] are used for human activity representation. However, they produce ambiguity among the same silhouettes for different postures of different activities due to their limited pixel value (i.e., 0 or 1, thus within the binary shape of a posture, no information is presented). To extract activity shape features, principle component analysis (PCA) and independent component analysis (ICA) have been used [<xref ref-type="bibr" rid="bibr2-1420326X11423163">2</xref>,<xref ref-type="bibr" rid="bibr4-1420326X11423163">4</xref>] producing spatial features that are global and local, respectively. However, these spatial features are sensitive to scaling and translation of silhouettes. If body silhouettes are not properly aligned, satisfactory recognition rate may not be achieved.</p>
<p>To overcome these limitations in binary silhouettes and their PCA and ICA features, a HAR system utilizing depth silhouettes and ℜ transformation are introduced and investigated by the present study. The advantages of this proposed approach include: (i) depth silhouettes would provide extra information about body parts in depth grayscale values; (ii) ℜ transformed profiles of depth silhouettes are invariant to enable scaling and translation; (iii) ℜ transformation would produce a highly compact representation of silhouettes in 1D and (iv) ℜ transformation would reduce a burden of body silhouette alignment due to their invariance properties.</p>
<p>R transformation [<xref ref-type="bibr" rid="bibr5-1420326X11423163">5</xref>] was first introduced to classify objects from images and extract distinct directional features of each binary shape. In ref. [<xref ref-type="bibr" rid="bibr6-1420326X11423163">6</xref>], Singh et al. proposed Radon transform to identify the binary skeleton representation for the human pose recognition. In ref. [<xref ref-type="bibr" rid="bibr7-1420326X11423163">7</xref>], Souvenir et al. used ℜ transformation to learn multiple viewpoints of human motion in action recognition using binary silhouettes. In ref. [<xref ref-type="bibr" rid="bibr3-1420326X11423163">3</xref>], ℜ transform was applied on binary silhouettes to describe the spatial information of different human activities. However, the binary silhouettes could cause limited recognition performance due to a lack of information in the flat pixel intensity, and thus making this impossible to differentiate between the far and near distance of the human body parts. Recently, to overcome the limitation of the binary silhouettes, depth-based silhouette representation for human activity has been suggested [<xref ref-type="bibr" rid="bibr8-1420326X11423163">8</xref>–<xref ref-type="bibr" rid="bibr10-1420326X11423163">10</xref>], since body parts can be differentiated in depth silhouettes by means of different intensity values.</p>
<p>In the proposed HAR system, initially, ℜ transformation is applied on the depth silhouettes. ℜ transformation would compute a 2D angular projection map of an activity silhouette via Radon transform, then converts the 2D Radon transformed map into a 1D ℜ transform profile. Then, to extract features from the ℜ transformed profiles of depth silhouettes, PCA and then linear discriminant analysis (LDA) would be applied to make them more robust. Finally, the features would then be utilised in Hidden Markov Models (HMMs) for the recognition of 10 daily home activities. Ten most common home human activities are recognized in this study, namely: cleaning, cooking, eating, exercise (dumb-belling), lying-down, rushing, sitting-down, standing-up, taking-an-object and walking. The proposed system could be an essential component of a smart home system for continuous observation of daily human home activities.</p>
</sec>
<sec id="sec2-1420326X11423163" sec-type="methods"><title>Methodology</title>
<p>Our HAR system consists of depth silhouette extraction, feature representation and extraction (including ℜ transformation, PCA and LDA), and modelling of HMMs (including codebook generation, training and recognition). <xref ref-type="fig" rid="fig1-1420326X11423163">Figure 1</xref> shows the overall flow of our proposed HAR system.
<fig id="fig1-1420326X11423163" position="float"><label>Fig. 1.</label><caption><p>Overall flow of our proposed human activity recognition system.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig1.tif"/></fig></p>
<sec id="sec3-1420326X11423163"><title>Depth Silhouette Pre-processing</title>
<p>To capture depth images of activities, a ZCAM<sup>TM</sup> (3DV Systems Ltd) was utilized [<xref ref-type="bibr" rid="bibr11-1420326X11423163">11</xref>]. Activity silhouettes were extracted from depth images and resized to a matrix of 50 × 50. <xref ref-type="fig" rid="fig2-1420326X11423163">Figure 2(a)</xref> and (b) shows typical binary and depth silhouette of a rushing activity.
<fig id="fig2-1420326X11423163" position="float"><label>Fig. 2.</label><caption><p>Two sequences of (a) binary and (b) depth silhouettes of a rushing activity.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig2.tif"/></fig></p>
<p>In <xref ref-type="fig" rid="fig2-1420326X11423163">Figure 2</xref>, it is clear that binary silhouettes contain limited information due to their flat pixel intensity values (i.e., 0 or 1) over the human body (i.e., only shape information is available) while depth silhouettes show discernable parts in depth greyscale values in addition to the shape information.</p>
</sec>
<sec id="sec4-1420326X11423163"><title>Feature Representation</title>
<p>To derive translation and scaling invariant features from the depth activity silhouettes, ℜ transformation was used in the proposed approach. First, Radon transform computes a 2D projection of a depth silhouette [<xref ref-type="bibr" rid="bibr6-1420326X11423163">6</xref>] along specified view angles. It is applied on the depth silhouette to establish a mapping between the image coordinate <italic>f</italic>(<italic>x,y</italic>) and the Radon coordinate ℜ (<italic>ρ,θ</italic>) system.</p>
<p>Let <italic>f</italic>(<italic>x,y</italic>) as the depth silhouette, its Radon transform ℜ (<italic>ρ,θ</italic>) would be computed by
<disp-formula id="disp-formula1-1420326X11423163"><label>(1)</label><graphic xlink:href="10.1177_1420326X11423163-eq1.tif"/></disp-formula>
where <italic>δ</italic> is the Dirac delta, ρ ∈ [−∞,∞] and <italic>θ</italic> ∈ [0,<italic>π</italic>].</p>
<p>Then ℜ transform [<xref ref-type="bibr" rid="bibr3-1420326X11423163">3</xref>,<xref ref-type="bibr" rid="bibr5-1420326X11423163">5</xref>] would be used to transform the 2D Radon projection to make a 1D ℜ transform profile for every silhouette.
<disp-formula id="disp-formula2-1420326X11423163"><label>(2)</label><graphic xlink:href="10.1177_1420326X11423163-eq2.tif"/></disp-formula>
</p>
<p>Basically, ℜ transform is the sum of the squared Radon transform values along the specified angle <italic>θ</italic>, providing a highly compact shape representation and thus reflecting the time-sequential profiles of each daily home human activity. <xref ref-type="fig" rid="fig3-1420326X11423163">Figure 3</xref> shows the general flow of ℜ transformation using a set of depth (walking) activity silhouettes.
<fig id="fig3-1420326X11423163" position="float"><label>Fig. 3.</label><caption><p>Overall flow of Radon transform using depth silhouette activities.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig3.tif"/></fig></p>
<p>As aforementioned, ℜ transform has some properties that could make them useful in the field of activity recognition. The ℜ transformed profiles are invariance to translation and scaling. These invariance properties would make HAR insensitive to the subject differences such as their position, height, size etc. as long as they perform the same activity. <xref ref-type="fig" rid="fig4-1420326X11423163">Figure 4</xref> shows the ℜ transformed profiles of the original silhouette and its translated and scaled silhouettes, respectively. Due to their invariance, all profiles are identical.
<fig id="fig4-1420326X11423163" position="float"><label>Fig. 4.</label><caption><p>Invariance of ℜ transformation results: (a) original silhouette and (b) translated (c) scaled silhouette of (a). Note that all ℜ profiles are identical.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig4.tif"/></fig></p>
<p><xref ref-type="fig" rid="fig5-1420326X11423163">Figure 5</xref> shows the ℜ transform time-sequential profiles of our 10 daily home activities. In this study, the following activities are recognized: cleaning (vacuuming), cooking, eating, exercise (dumb-belling), lying-down, rushing, sitting-down, standing-up, take-an-object and walking. These 3D representations of time evolving ℜ transform profiles are derived from a video clip of 10 consecutive frames for each activity, showing that the transformation can effectively characterise and differentiate the 10 home human activities.
<fig id="fig5-1420326X11423163" position="float"><label>Fig. 5.</label><caption><p>Plots of time-sequential ℜ transform profiles from the depth silhouettes of the 10 daily home human activities.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig5.tif"/></fig></p>
</sec>
<sec id="sec5-1420326X11423163"><title>PCA and LDA for Silhouette Feature Extraction</title>
<p>After computing the ℜ transformation profiles, PCA [<xref ref-type="bibr" rid="bibr4-1420326X11423163">4</xref>] was used to extract the dominant features. The principal components (PCs) of the ℜ transformed profiles of the depth silhouettes are determined by <xref ref-type="disp-formula" rid="disp-formula3-1420326X11423163">Equation (3)</xref>:
<disp-formula id="disp-formula3-1420326X11423163"><label>(3)</label><graphic xlink:href="10.1177_1420326X11423163-eq3.tif"/></disp-formula>
where <italic>P</italic><sub><italic>i</italic></sub> is the PCA projection on the ℜ transformed profiles,<inline-formula id="ilm1-1420326X11423163"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math1-1420326X11423163"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> is the zero mean vector of ℜ transformation profiles, and <italic>V</italic><sub><italic>e</italic></sub> is the leading eigenvectors corresponding to the first top eigenvalues.</p>
<p>Finally, LDA [<xref ref-type="bibr" rid="bibr12-1420326X11423163">12</xref>] would find a set of discriminant vectors <italic>F</italic><sub><italic>LDA</italic></sub> that would maximise the ratio of the determinant of the between <italic>S</italic><sub><italic>B</italic></sub> and within <italic>S</italic><sub><italic>W</italic></sub> class scatter matrix as illustrated by <xref ref-type="disp-formula" rid="disp-formula4-1420326X11423163">Equation (4)</xref>:
<disp-formula id="disp-formula4-1420326X11423163"><label>(4)</label><graphic xlink:href="10.1177_1420326X11423163-eq4.tif"/></disp-formula>
</p>
<p>Thus, the feature vectors using LDA on PC-R features can be represented as <italic>L</italic><sub><italic>PC</italic>-<italic>RTrans</italic></sub> = R<sub><italic>T</italic></sub>(<italic>θ</italic>)<italic>P</italic><sub><italic>i</italic></sub><italic>F</italic><sub><italic>LD A</italic></sub>, respectively, where <italic>L</italic><sub><italic>PC</italic>-<italic>RTrans</italic></sub> indicates the LDA on PC-R features representation for the <italic>i</italic><sup><italic>th</italic></sup> depth silhouette image.</p>
<p><xref ref-type="fig" rid="fig6-1420326X11423163">Figure 6(a)</xref> and (b) shows the 3D plots of the features after applying the proposed feature extraction methods on binary and depth silhouettes of our home activities, respectively. In <xref ref-type="fig" rid="fig6-1420326X11423163">Figure 6(a)</xref>, a pair of activities such as (cooking and exercise) and (walking and rushing) are very close to each other, therefore overlaps in the features among the prototypes of similar activities thus causing low recognition rate. Whereas in <xref ref-type="fig" rid="fig5-1420326X11423163">Figure 5(b)</xref>, all activities are well separated among each other, indicating good feature representation.
<fig id="fig6-1420326X11423163" position="float"><label>Fig. 6.</label><caption><p>Plots of the linear discriminant analysis (LDA) on PC-R features on (a) the 1,500 binary silhouettes and (b) 1,500 depth silhouettes of our home activities.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig6.tif"/></fig></p>
</sec>
<sec id="sec6-1420326X11423163"><title>Activity Recognition via HMM</title>
<p>HMM is based on a number of finite states connected by transitions where every state contains transition probability to other state and symbol observation probability. In this study, for modelling, training, and recognizing the home human activity, HMM [<xref ref-type="bibr" rid="bibr13-1420326X11423163">13</xref>] was used since it would deal with the sequential silhouette data with a probabilistic learning process. In a discrete HMM, the feature vectors should be symbolised. The clustering algorithm introduced by Linde, Buzo and Gray (LBG) [<xref ref-type="bibr" rid="bibr14-1420326X11423163">14</xref>] was used to generate a codebook of vectors for HMM.</p>
<p><xref ref-type="fig" rid="fig7-1420326X11423163">Figure 7</xref> shows the procedure of a codebook generation and symbol selection on the LDA of PC features of the ℜ transform profiles. <xref ref-type="fig" rid="fig8-1420326X11423163">Figure 8</xref> shows the structure and transition probabilities of a walking HMM before and after training with the codebook size of 32 [<xref ref-type="bibr" rid="bibr13-1420326X11423163">13</xref>].
<fig id="fig7-1420326X11423163" position="float"><label>Fig. 7.</label><caption><p>A codebook generation and symbol selection.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig7.tif"/></fig>
<fig id="fig8-1420326X11423163" position="float"><label>Fig. 8.</label><caption><p>Walking Hidden Markov Model (HMM) structure and transition probabilities (a) before and (b) after training.</p></caption><graphic xlink:href="10.1177_1420326X11423163-fig8.tif"/></fig></p>
</sec></sec>
<sec id="sec7-1420326X11423163"><title>Experimental Setting and Results</title>
<p>To test the system, a depth silhouette database of the 10 daily home activities was built and recorded multiple test sets from five different subjects. The collected video clips were split into several clips where each clip would contain 10 consecutive frames. A total of 15 clips from each activity were used to build the training feature space and the whole training data contained a total of 1,500 depth silhouettes. Initially, each depth silhouette vector with its size of 1 × 2,500 was transformed via ℜ transformation, producing a 1D profile of 1 × 180. The PCA reduced dimension of each vector to 1 × 100, and finally LDA was performed ending with a feature vector of 1 × 9 per silhouette.</p>
<p>In the depth silhouette-based home activity database, 15 video clips were applied for training and 45 video clips for testing image sequences, respectively. <xref ref-type="table" rid="table1-1420326X11423163">Table 1</xref> shows the recognition results from the binary and depth activity silhouettes, respectively, utilizing the different features sets. The proposed LDA on PC-R features approach on the depth silhouette has demonstrated significantly superior recognition of 96.55% over that of the binary silhouettes (i.e., 85.75%).
<table-wrap id="table1-1420326X11423163" position="float"><label>Table 1.</label><caption><p>Recognition results of different feature extraction approaches using both binary and depth silhouettes</p></caption>
<graphic alternate-form-of="table1-1420326X11423163" xlink:href="10.1177_1420326X11423163-table1.tif"/>
<table frame="hsides"><thead align="left">
<tr>
<th>Approaches</th>
<th>Home human activities</th>
<th>Recognition rate (binary)</th>
<th>Recognition rate (depth)</th>
<th>Mean (binary)</th>
<th>Mean (depth)</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="10">PC-R features</td>
<td>Cleaning</td>
<td>72.50</td>
<td>87.50</td>
<td rowspan="10">72.40</td>
<td rowspan="10">88.65</td>
</tr>
<tr>
<td>Cooking</td>
<td>51.50</td>
<td>79</td>
</tr>
<tr>
<td>Eating</td>
<td>84</td>
<td>100</td>
</tr>
<tr>
<td>Dumbbell Exercise</td>
<td>57.50</td>
<td>84.50</td>
</tr>
<tr>
<td>Lying down</td>
<td>100</td>
<td>100</td>
</tr>
<tr>
<td>Rushing</td>
<td>66.50</td>
<td>86.50</td>
</tr>
<tr>
<td>Sitting down</td>
<td>63</td>
<td>84</td>
</tr>
<tr>
<td>Standing up</td>
<td>81.50</td>
<td>90.50</td>
</tr>
<tr>
<td>Taking-an-Object</td>
<td>72</td>
<td>83.50</td>
</tr>
<tr>
<td>Walking</td>
<td>75.50</td>
<td>91</td>
</tr>
<tr>
<td rowspan="10">LDA on PC-R features</td>
<td>Cleaning</td>
<td>85.50</td>
<td>94.50</td>
<td rowspan="10">85.75 s</td>
<td rowspan="10">96.55</td>
</tr>
<tr>
<td>Cooking</td>
<td>75.50</td>
<td>91</td>
</tr>
<tr>
<td>Eating</td>
<td>97.50</td>
<td>100</td>
</tr>
<tr>
<td>Dumbbell exercise</td>
<td>78</td>
<td>96</td>
</tr>
<tr>
<td>Lying down</td>
<td>100</td>
<td>100</td>
</tr>
<tr>
<td>Rushing</td>
<td>82</td>
<td>96</td>
</tr>
<tr>
<td>Sitting down</td>
<td>80.50</td>
<td>95.50</td>
</tr>
<tr>
<td>Standing up</td>
<td>88.50</td>
<td>92.50</td>
</tr>
<tr>
<td>Taking-an-object</td>
<td>82.50</td>
<td>100</td>
</tr>
<tr>
<td>Walking</td>
<td>87.50</td>
<td>100</td>
</tr>
</tbody></table></table-wrap></p>
</sec>
<sec id="sec8-1420326X11423163" sec-type="conclusions"><title>Conclusion</title>
<p>A depth silhouette and ℜ transformation-based HAR system designed for smart homes have been presented in this paper. In our results, the use of depth silhouettes and ℜ transformation has demonstrated a recognition rate up to 96.55% over the conventional systems utilising the binary silhouettes achieved 72.40% and 85.75%, respectively. The proposed methodology should be useful in designing and developing a compact HAR system that can be practically used in a smart environment including smart home.</p>
</sec></body>
<back>
<ack><title>Acknowledgement</title>
<p>This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MEST) (No. 2011-0001031).</p></ack>
<ref-list>
<title>References</title>
<ref id="bibr1-1420326X11423163"><label>1</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>M</given-names></name><name><surname>Esteve</surname><given-names>D</given-names></name><name><surname>Escriba</surname><given-names>C</given-names></name><name><surname>Campo</surname><given-names>E</given-names></name></person-group>: <article-title>A review of smart homes-Present state and future challenges</article-title>: <source>Comput Meth Progr Biomed</source> <year>2008</year>;<volume>91</volume>(<issue>1</issue>):<fpage> 55</fpage>–<lpage>81</lpage>.</citation></ref>
<ref id="bibr2-1420326X11423163"><label>2</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Uddin</surname><given-names>MZ</given-names></name><name><surname>Lee</surname><given-names>JJ</given-names></name><name><surname>Kim</surname><given-names>TS</given-names></name></person-group>: <article-title>Independent shape component-based human activity recognition via Hidden Markov Model</article-title>: <source>Journal of Applied Intelligence</source> <year>2010</year>;<volume>33</volume>:<fpage>193</fpage>–<lpage>206</lpage>.</citation></ref>
<ref id="bibr3-1420326X11423163"><label>3</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Tan</surname><given-names>T</given-names></name></person-group>: <article-title>Human activity recognition based on ℜ transform</article-title>: <source>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</source>, <publisher-loc>USA</publisher-loc>, <fpage>17</fpage>–<lpage>22</lpage> <month>June</month>, <year>2007</year>, <comment>1-8. 1420326X11423163: 1-4244-1180-7</comment>.</citation></ref>
<ref id="bibr4-1420326X11423163"><label>4</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Bhanu</surname><given-names>B</given-names></name></person-group>: <article-title>Human activity recognition in thermal infrared imagery</article-title>: <source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)</source>, <publisher-loc>Washington, DC, USA</publisher-loc>, <day>25</day> <month>June</month>, <volume>3</volume>, <year>2005</year>, <fpage>17</fpage>–<lpage>25</lpage>.</citation></ref>
<ref id="bibr5-1420326X11423163"><label>5</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tabbone</surname><given-names>S</given-names></name><name><surname>Wendling</surname><given-names>L</given-names></name><name><surname>Salmon</surname><given-names>JP</given-names></name></person-group>: <article-title>A new shape descriptor defined on the Radon transform</article-title>: <source>Comput Vis Image Understand</source> <year>2006</year>;<volume>102</volume>:<fpage>42</fpage>–<lpage>51</lpage>.</citation></ref>
<ref id="bibr6-1420326X11423163"><label>6</label><citation citation-type="confproc"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>M</given-names></name><name><surname>Mandal</surname><given-names>M</given-names></name><name><surname>Basu</surname><given-names>A</given-names></name></person-group>: <article-title>Pose recognition using the Radon transform</article-title>: <source>Proceedings of 48th Midwest Symposium on Circuits and Systems</source>, <volume>2</volume>, <fpage>7</fpage>–<lpage>10</lpage> <month>Aug</month>, <year>2005</year>, <fpage>1091</fpage>–<lpage>1094</lpage>.</citation></ref>
<ref id="bibr7-1420326X11423163"><label>7</label><citation citation-type="confproc"><person-group person-group-type="author"><name><surname>Souvenir</surname><given-names>R</given-names></name><name><surname>Babba</surname><given-names>J</given-names></name></person-group>: <article-title>Learning the viewpoint manifold for action recognition</article-title>: <source>Proceedings of 26th IEEE conference on Computer Vision and Pattern Recognition 2008</source>, <publisher-loc>Anchorage AK</publisher-loc>, <fpage>23</fpage>–<lpage>28</lpage> <month>June</month>, <fpage>1</fpage>–<lpage>7</lpage>. <comment>1420326X11423163: 1063-6919</comment>.</citation></ref>
<ref id="bibr8-1420326X11423163"><label>8</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Salinas</surname><given-names>RM</given-names></name><name><surname>Carnicer</surname><given-names>RM</given-names></name><name><surname>Cuevas</surname><given-names>FJM</given-names></name><name><surname>Poyato</surname><given-names>AC</given-names></name></person-group>: <article-title>Depth silhouettes for gesture recognition</article-title>: <source>Pattern Recogn Lett</source> <year>2008</year>;<volume>29</volume>(<issue>3</issue>):<fpage>319</fpage>–<lpage>329</lpage>.</citation></ref>
<ref id="bibr9-1420326X11423163"><label>9</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Uddin</surname><given-names>MZ</given-names></name><name><surname>Kim</surname><given-names>TS</given-names></name><name><surname>Kim</surname><given-names>JT</given-names></name></person-group>: <article-title>Video-based indoor human gait recognition using depth imaging and Hidden Markov Model</article-title>: <source>A smart system for smart home: Indoor Built Environ</source> <year>2011</year>;<volume>20</volume>(<issue>1</issue>):<fpage>120</fpage>–<lpage>128</lpage>.</citation></ref>
<ref id="bibr10-1420326X11423163"><label>10</label><citation citation-type="confproc"><person-group person-group-type="author"><name><surname>Frueh</surname><given-names>C</given-names></name><name><surname>Zakhor</surname><given-names>A</given-names></name></person-group>: <article-title>Capturing 2½D depth and texture of time-varying scenes using structured infrared light</article-title>: <source>Proceedings of 5th International Conference on 3-D Digital Imaging and Modelling</source>, <publisher-loc>Canada</publisher-loc>, <fpage>13</fpage>–<lpage>16</lpage> <month>June</month>, <year>2005</year>, <fpage>318</fpage>–<lpage>325</lpage>. <comment>1420326X11423163: 0-7695-2327-7</comment>.</citation></ref>
<ref id="bibr11-1420326X11423163"><label>11</label><citation citation-type="web"><person-group person-group-type="author"><name><surname>Maizels</surname><given-names>A</given-names></name><name><surname>Shpunt</surname><given-names>A</given-names></name><name><surname>Sharon</surname><given-names>O</given-names></name><name><surname>Berliner</surname><given-names>T</given-names></name><name><surname>Rais</surname><given-names>D</given-names></name></person-group>: <source>PrimeSense</source>. <ext-link ext-link-type="uri" xlink:href="http://www.primesense.com">http://www.primesense.com</ext-link>. <comment>Accessed on 21 August 2011.</comment></citation></ref>
<ref id="bibr12-1420326X11423163"><label>12</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Aradhya</surname><given-names>VNM</given-names></name><name><surname>Kumar</surname><given-names>GH</given-names></name><name><surname>Noushath</surname><given-names>S</given-names></name></person-group>: <article-title>Fisher Linear Discriminant Analysis and Connectionist Model for efficient image recognition</article-title>: <source>Studies in Computational Intelligence</source> <year>2008</year>;<volume>83</volume>:<fpage> 269</fpage>–<lpage>278</lpage>.</citation></ref>
<ref id="bibr13-1420326X11423163"><label>13</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Rabiner</surname><given-names>LR</given-names></name></person-group>: <article-title>A tutorial on Hidden Markov Models and selected applications in speech recognition</article-title>: <source>Proceedings of the IEEE</source>, <publisher-loc>Austin TX</publisher-loc>, <month>Feb</month>; <volume>77</volume>(<issue>2</issue>), <year>1989</year>, <fpage>257</fpage>–<lpage>286</lpage>. <comment>1420326X11423163: 1558601244</comment>.</citation></ref>
<ref id="bibr14-1420326X11423163"><label>14</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Linde</surname><given-names>Y</given-names></name><name><surname>Buzo</surname><given-names>A</given-names></name><name><surname>Gray</surname><given-names>RM</given-names></name></person-group>: <article-title>An algorithm for vector quantiser design</article-title>: <source>IEEE Trans Comm</source> <year>1980</year>;<volume>28</volume>:<fpage>84</fpage>–<lpage>95</lpage>.</citation></ref></ref-list>
</back>
</article>