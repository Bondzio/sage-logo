<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EDR</journal-id>
<journal-id journal-id-type="hwp">spedr</journal-id>
<journal-title>Educational Researcher</journal-title>
<issn pub-type="ppub">0013-189X</issn>
<issn pub-type="epub">1935-102X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/0013189X12474065</article-id>
<article-id pub-id-type="publisher-id">10.3102_0013189X12474065</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Feature Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The ACT of Enrollment</article-title>
<subtitle>The College Enrollment Effects of State-Required College Entrance Exam Testing</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Klasik</surname><given-names>Daniel</given-names></name>
<xref ref-type="aff" rid="aff1-0013189X12474065">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0013189X12474065"><label>1</label>Stanford University, Stanford, CA</aff>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2013</year>
</pub-date>
<volume>42</volume>
<issue>3</issue>
<fpage>151</fpage>
<lpage>160</lpage>
<history>
<date date-type="received">
<day>27</day>
<month>6</month>
<year>2012</year>
</date>
<date date-type="rev-recd">
<day>30</day>
<month>9</month>
<year>2012</year>
</date>
<date date-type="rev-recd">
<day>9</day>
<month>12</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>12</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© 2013 AERA</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>Since 2001 Colorado, Illinois, and Maine have all enacted policies that require high school juniors to take college entrance exams—the SAT or the ACT. One goal of these policies was to increase college enrollment based on the belief that requiring students to take these exams would make students more likely to consider college as a viable option. Relying on quasi-experimental methods and synthetic control comparison groups, this article presents the effects of this state-mandated college entrance exam testing. Based on both state- and individual-level analyses, I find evidence that entrance exam policies were associated with increases in overall college enrollment in Illinois and that such policies re-sorted students in all three states between different types of institutions. In particular, Colorado saw an increase in enrollment at private 4-year institutions, whereas Illinois and Maine both saw a decrease in enrollment at pubic 2-year institutions. Increases in enrollment at schools that require entrance exams for admissions support the hypothesis that lack of exam scores can present barriers to college entry.</p>
</abstract>
<kwd-group>
<kwd>admissions</kwd>
<kwd>colleges</kwd>
<kwd>econometric analysis</kwd>
<kwd>educational policy</kwd>
<kwd>higher education</kwd>
<kwd>policy</kwd>
<kwd>policy analysis</kwd>
<kwd>postsecondary education</kwd>
<kwd>quasi-experimental analysis</kwd>
<kwd>regression analyses</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Increasing college enrollment rates has long been a goal for both the federal and state governments. For many years, financial aid policies have been one of the major levers used by government to reach this end. Although generally effective at dealing with the <italic>financial</italic> barriers to college-going, these policies are quite expensive and ignore other potential enrollment obstacles. Some states and local municipalities have recently begun trying non-aid-based, and as yet untested, methods for increasing college enrollment. These policies work to address nonfinancial barriers to enrollment. This article evaluates the effect of one such method: the statewide requirement of college entrance exam testing.</p>
<p>College entrance exams such as the ACT and SAT are required or recommended for admission to nearly all of the nation’s 4-year colleges and universities and are likewise used in the admissions process at many 2-year colleges. Many students interested in attending such schools, however, often fail to sit for these exams. For example, only 70% of students who in 10th grade expressed a desire to earn a 4-year college degree had taken the SAT or ACT by their senior year of high school (<xref ref-type="bibr" rid="bibr26-0013189X12474065">National Center for Education Statistics [NCES], 2004</xref>). Failure to take college entrance exams can often mean the end of students’ 4-year college degree hopes, limited options for admission or advanced course placement at community colleges, or reduced eligibility for some forms of college scholarships.</p>
<p>Policies that require that students take a college entrance exam eliminate this failure as a reason not to enroll in college. In this article, I study the effect of such college entrance exam requirements on college enrollment in Colorado, Illinois, and Maine. Specifically, I seek to answer the following research question: Do state college entrance exam requirements change the college enrollment choices of students in these states?</p>
<p>To answer this question, I use institutional enrollment data from the Integrated Postsecondary Educational Data System (IPEDS) and individual student-level data from the October supplement of the Current Population Survey (CPS). I treat the state-by-state adoption of college entrance exam requirements as a natural experiment, allowing me to use a comparative interrupted time series model to compare the enrollment changes in states requiring these exams to synthetic control states that did not.</p>
<sec id="section1-0013189X12474065">
<title>Background</title>
<sec id="section2-0013189X12474065">
<title>State Policy and College Enrollment</title>
<p>At present, the vast majority of federally and state-funded policies designed to improve college attendance address financial barriers to enrollment. In a sample of five states, <xref ref-type="bibr" rid="bibr28-0013189X12474065">Perna, Rowan-Kenyon, Bell, Thomas, and Li (2008)</xref> found that more than 90% of state policies directed toward a college enrollment outcome provided some sort of financial support for college. The remaining programs addressed either students’ academic preparation for college or their knowledge about college.</p>
<p>This emphasis on financial barriers is not without warrant—research has consistently found links between such policies and increased college attendance. One of the more well-known state aid policies is the Georgia Helping Outstanding Pupils Educationally (HOPE) Scholarship, which provides funding to high-achieving students who attend college in-state. Two studies in particular have examined how this merit-based financial assistance program has affected college enrollment among Georgia students. <xref ref-type="bibr" rid="bibr17-0013189X12474065">Dynarski (2000)</xref> found that the HOPE scholarship increased college enrollment for Georgia students by roughly 7 percentage points.</p>
<p><xref ref-type="bibr" rid="bibr17-0013189X12474065">Dynarski’s (2000)</xref> study addressed how the HOPE scholarship affected <italic>whether</italic> but not <italic>where</italic> students enrolled in college. This issue was subsequently addressed by <xref ref-type="bibr" rid="bibr16-0013189X12474065">Cornwell, Mustard, and Sridhar (2006)</xref>. <xref ref-type="bibr" rid="bibr16-0013189X12474065">Cornwell et al. (2006)</xref> studied the HOPE effect on college enrollment by institutional type. They found large increases in enrollment in 4-year private colleges and little, or negative, change in enrollment at 2-year institutions. Other studies of state aid programs such as the CalGrant program in California (<xref ref-type="bibr" rid="bibr21-0013189X12474065">Kane, 2003</xref>) and the District of Columbia Tuition Assistance Grant Program (<xref ref-type="bibr" rid="bibr3-0013189X12474065">Abraham &amp; Clark, 2006</xref>) found similar positive enrollment effects of state efforts to increase enrollment by providing financial assistance for college.</p>
<p>Although responsible for modest gains in enrollments, these programs have two main drawbacks. First, they are expensive. Georgia paid $13,650 per <italic>new</italic> student who enrolled who otherwise would not have. Second, efforts to alleviate financial barriers to college enrollment come relatively late in the process—usually after students have applied to, or even been accepted to, college. As a result, financial aid policies cannot help students who were discouraged from attending college before they applied.</p>
<p>Indeed, although the enormous cost of college attendance surely plays a large role in preventing students interested in a college degree from enrolling, it is not the only barrier to college attendance. Many students who aspire to a college degree may not enroll for simpler reasons: they fail to complete the different steps of what can be a complicated college admissions process (<xref ref-type="bibr" rid="bibr22-0013189X12474065">Klasik, 2012</xref>). Many researchers have noted that groups of students who are traditionally underrepresented in college, those from minority and low-income families, fail to complete various application steps at the same rate as White or wealthy students (<xref ref-type="bibr" rid="bibr6-0013189X12474065">Avery &amp; Kane, 2004</xref>; <xref ref-type="bibr" rid="bibr7-0013189X12474065">Berkner &amp; Chavez, 1997</xref>; <xref ref-type="bibr" rid="bibr13-0013189X12474065">Cabrera &amp; La Nasa, 2001</xref>; <xref ref-type="bibr" rid="bibr19-0013189X12474065">Horn, 1997</xref>; <xref ref-type="bibr" rid="bibr22-0013189X12474065">Klasik, 2012</xref>).</p>
<p>With regard to the specific step of taking college entrance exams, <xref ref-type="bibr" rid="bibr6-0013189X12474065">Avery and Kane (2004)</xref> note that only 32% of a sample of largely minority urban students had taken any college entrance exam by the fall of their senior year whereas nearly 98% of a sample of wealthier suburban students had done so. Ultimately, 25% of the urban students enrolled in a 4-year college, compared to 88% of the suburban students. Even more directly, <xref ref-type="bibr" rid="bibr22-0013189X12474065">Klasik (2012)</xref> emphasized the specific importance of taking the SAT or ACT for college enrollment. Klasik found that few students enrolled in 4-year institutions without having taken the SAT or ACT. In fact, taking the SAT or ACT was one of the steps most predictive of later college application and enrollment.</p>
</sec>
<sec id="section3-0013189X12474065">
<title>State College Entrance Exam Requirements</title>
<p>Many states now recognize the obstacle presented by college entrance exams. In an attempt to raise college enrollment among their students, six states require all high school students to take one of these exams and have contracted with the ACT or the College Board (which administers the SAT) to pay for these tests. Colorado and Illinois were the first states to require the ACT for all high school juniors beginning in the spring of 2001. Maine required all of its high school juniors to take the SAT beginning in the spring of 2006. Michigan, Kentucky, and Wyoming have also begun requiring either the ACT or the SAT. Because their policies have been in place the longest and have enough postpolicy data to analyze, the remainder of this article focuses on the policies in Colorado, Illinois, and Maine.</p>
<p>Although the exams were used primarily to fulfill No Child Left Behind testing requirements, state officials in all three states cited a desire to focus student attention on their postsecondary options as a motivation for their particular use of college entrance exams (<xref ref-type="bibr" rid="bibr9-0013189X12474065">Betebenner &amp; Howe, 2001</xref>; <xref ref-type="bibr" rid="bibr20-0013189X12474065">Illinois State Board of Education, 2006</xref>; <xref ref-type="bibr" rid="bibr29-0013189X12474065">State of Maine, 2007</xref>). Their belief was that providing students with entrance exam scores would cause college enrollment rates to increase. So great was the motivation to boost college enrollments that all three states adopted exam requirements even as they acknowledged that the exams did not fully capture state high school learning standards (<xref ref-type="bibr" rid="bibr9-0013189X12474065">Betebenner &amp; Howe, 2001</xref>; <xref ref-type="bibr" rid="bibr15-0013189X12474065">College Board, 2005</xref>; <xref ref-type="bibr" rid="bibr20-0013189X12474065">Illinois State Board of Education, 2006</xref>).</p>
</sec>
</sec>
<sec id="section4-0013189X12474065">
<title>Conceptual Framework</title>
<p>There are two main ways in which college exam requirements can affect students’ college choices. First, receipt of exam scores provides students with additional information on which to base their college enrollment decisions. Second, by making the exam a requirement rather than an option, states may increase students’ buy-in to the college admission process by moving them further along in the process before allowing them to opt out. Each of these mechanisms is discussed below.</p>
<p>Students often lack information about their own academic ability and likelihood of successful college completion (<xref ref-type="bibr" rid="bibr4-0013189X12474065">Altonji, 1993</xref>; <xref ref-type="bibr" rid="bibr25-0013189X12474065">Manski, 1993</xref>). This uncertainty limits students’ abilities to make optimal decisions about college enrollment. College entrance exam scores are valuable pieces of information in the college search process: they provide an explicit measure that students can use to assess their likelihood of college admission by comparing it to a school’s admissions criteria. Such comparisons may reveal preparedness for college that a student may not have otherwise realized. Thus, college entrance exam requirements may provide students with information that will help them make better decisions about college. Given this additional information, it is difficult to predict the schools at which students will choose to enroll.</p>
<p>College entrance exam requirements may also alter students’ college enrollment choices by changing students’ default behavior—their path of least resistance. Before the entrance exam requirements, a student would have to take specific action to register for an entrance exam. With the state’s requirement, the default action for students becomes the taking of an entrance exam. Research has shown that such small changes in defaults can have powerful effects in a number of domains (<xref ref-type="bibr" rid="bibr8-0013189X12474065">Beshears, Choi, Laibson, &amp; Madrian, 2009</xref>). In the domain of retirement saving, employees are more likely to contribute to such plans if they are enrolled in them automatically (<xref ref-type="bibr" rid="bibr14-0013189X12474065">Choi, Laibson, Madrian, &amp; Metrick, 2002</xref>; <xref ref-type="bibr" rid="bibr24-0013189X12474065">Madrian &amp; Shea, 2001</xref>). Like retirement saving, college enrollment involves the completion of small steps that can result in large future financial rewards. Despite these clear payoffs, each has a lower than desired uptake. Although more work is required to enroll in college than taking an entrance exam, changing this default clearly creates additional investment in college-going and keeps students moving down the path to college enrollment. At the most basic level, if failure to take an exam disqualifies students from consideration at some schools, then changing the default test-taking behavior automatically removes this hurdle. If this mechanism is at work, we would expect increases in enrollment at schools that require the SAT or ACT for admission.</p>
<p>Both of these mechanisms serve as what <xref ref-type="bibr" rid="bibr30-0013189X12474065">Thaler and Sunstein (2009)</xref> refer to as a “nudge”: a small change to the way an individual approaches a choice that can affect his or her decisions. Two recent studies have shown that despite the enormity of the college enrollment choice, nudges can have an effect on students’ postsecondary enrollment decisions. <xref ref-type="bibr" rid="bibr27-0013189X12474065">Pallais (2009)</xref> found that a $6 reduction in the cost of submitting four ACT score reports caused students to submit more score reports to a wider variety of colleges. This behavior, in turn, was connected to an increase in the likelihood of submitting an additional college application. <xref ref-type="bibr" rid="bibr10-0013189X12474065">Bettinger, Long, Oreopoulos, and Sanbonmatsu (2009)</xref> demonstrated in a randomized trial that helping students complete their Free Application for Federal Student Aid resulted in a 7.7 percentage point increase in the likelihood that those students enrolled in college. Thus, seemingly minor changes in the college enrollment process resulted in observable increases in not only the likelihood that students applied to and enroll in college, but which colleges students considered.</p>
</sec>
<sec id="section5-0013189X12474065">
<title>Empirical Strategy</title>
<p>Analysis of the effect of college entrance exam requirements proceeds in two parts. The first part looks at changes in total college enrollments at the state level whereas the second part considers changes in the individual likelihood that a given student enrolls in a particular type of college. A more technical and detailed description of the methods is given in the appendix to the online version of this article.</p>
<sec id="section6-0013189X12474065">
<title>State-Level Analysis</title>
<p>The state-by-state adoption of required college entrance exams allows for a straightforward comparative interrupted-time-series design.<sup><xref ref-type="fn" rid="fn1-0013189X12474065">1</xref></sup> The underlying strategy of this method is to compare the trend of outcomes in states with the entrance exam requirements—the treatment—to the same trends in a set of states that did not require entrance exams before and after the treatment was in place. This method relies on the assumption that the comparison states serve as a good representation of what would have happened in the entrance exam states in the absence of an entrance exam requirement. If we believe this assumption holds, then any difference in outcomes between the entrance exam states and the comparison states after the enactment of an entrance exam requirement can be attributed to the policy.</p>
<p>I used two main techniques to ensure the strength of this assumption. First, I selected the states to use as comparisons such that their pretreatment characteristics and outcome trends closely aligned with the characteristics and outcome trends in the entrance exam requirement states. I describe this method below. Second, in the regression estimation of the effect of the entrance exam policies, I accounted for many factors that could affect the validity of the comparison group in the post-policy time period. Both state and year fixed effects work towards this goal. Respectively, these fixed effects account for all factors of a given state that are related to an outcome that are constant over time and the characteristics of a given year that are related to an outcome but do not vary between states.</p>
<p>These fixed effects account for a large amount of potential confounding factors but are not able to account for state-by-year effects—nonpolicy variables that may explain changes in outcomes that change in particular states in particular years. Here we are concerned that some variable not related to an entrance exam requirement changed in entrance exam states in the same year the requirements went into effect and that changes in outcome may be related to this variable and not the exam policies themselves. For example, if Colorado’s entrance exam requirement implementation coincided with a notable increase in the size of the population of students graduating from high school, there may be an increase in the number of students enrolling in college, but it may not be due to the new policy. To avoid such problems, I controlled for as many of these potential state-by-year confounders as possible. Specifically, I controlled for the percentage of college-age (18–19 years) state residents from different racial groups, number of high school seniors in the state, state high school graduation rate, average annual state unemployment, state expenditure per public school student, the percentage of a state’s institutions of different types (public, private, etc.), and average state performance on the ACT or the SAT. Note that we expect average state ACT or SAT scores to change in states that require that all students take one of them simply because of the change in the composition of test takers. To account for this as best as possible, I control not for a state’s actual average ACT score but rather a state’s over- or underperformance on that exam relative to what would be expected given the percentage of students in that state who took the exam. In other words, I control for a state’s average ACT or SAT score after adjusting for the exam participation rate in that state.<sup><xref ref-type="fn" rid="fn2-0013189X12474065">2</xref></sup></p>
<p>We might also be concerned that the nature and magnitude of any effects of entrance exam requirements may vary depending on state context. For example, Illinois has a higher proportion of private 4-year institutions than Colorado. If one of the effects of entrance exam requirements is to shift students’ attention to private institutions, then this effect may be more apparent in Illinois than Colorado. For this reason, I perform each analysis separately for Colorado, Illinois, and Maine.</p>
<sec id="section7-0013189X12474065">
<title>Synthetic controls</title>
<p>The presence of just one state receiving the entrance exam requirement treatment in each analysis necessitates choosing a set of comparison states that match the treated state well. I found the set of comparison states by using synthetic control methods. The synthetic control method developed out of exactly the need to find an appropriate comparison group for a single treated unit. <xref ref-type="bibr" rid="bibr1-0013189X12474065">Abadie, Diamond, and Hainmueller (2007)</xref> and <xref ref-type="bibr" rid="bibr2-0013189X12474065">Abadie and Gardeazabal (2003)</xref> explain the details of the method. In short, the synthetic control method assigns weights between 0 and 1 to each of a set of candidate comparison states such that the weights sum to 1 and the weighted average of various pretreatment characteristics in the comparison states—including values of the pretreatment outcome—match those in the treated state as close as is mathematically possible.</p>
<p>Eligible comparison states included all states in the United States except for those who have adopted, or are about to adopt, college entrance exam requirements. I matched comparison units by all of the state-by-year covariates listed above, as well as the pretreatment trend of the given enrollment outcome. Enrollment outcomes included the log of aggregated total enrollment of the colleges in a given state for: all degree granting colleges, 2- and 4-year schools, public or private 4-year colleges, schools that require entrance exams for admission, schools that accept only full-time students, and enrollment at schools based on Carnegie-defined admissions selectivity. Log units allow for the interpretation of the effect estimates in terms of percentage change in the outcomes.</p>
<p>Synthetic controls are similar to, but not the same as propensity score matching. In propensity score matching, the goal is to match units based on their likelihood of receiving a given treatment. The three treated units and up to 46 comparison units in this analysis are not enough to estimate effectively the likelihood a state adopts an entrance exam requirement. The synthetic control method instead creates weighted matches to treatment states with the primary goal of aligning the pretreatment trends of a given outcome between treated and nontreated states.</p>
</sec>
<sec id="section8-0013189X12474065">
<title>Synthetic fit</title>
<p>To assess the quality of a synthetic control match, I evaluated its root mean squared prediction error (RMSPE)—a measure of the difference between the outcome in treated and comparison units in the pretreatment period. Overall, the RMSPE of the matches ranged from 0.013 to 0.298. <xref ref-type="table" rid="table1-0013189X12474065">Table 1</xref> presents the results of the match for enrollment at selective colleges in Colorado, which had the median RMSPE (RMSPE = 0.042). This match resulted from a synthetic control group of five states—California, Nevada, Utah, Washington, and Wisconsin— whose weights are given in <xref ref-type="table" rid="table2-0013189X12474065">Table 2</xref>.</p>
<table-wrap id="table1-0013189X12474065" position="float">
<label>Table 1</label>
<caption><p>Descriptive Statistics for Enrollment in Inclusive Colleges in Colorado and a Synthetic Colorado</p></caption>
<graphic alternate-form-of="table1-0013189X12474065" xlink:href="10.3102_0013189X12474065-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Pretreatment Covariate</th>
<th align="center">Colorado</th>
<th align="center">Synthetic Colorado</th>
</tr>
</thead>
<tbody>
<tr>
<td>Enrollment in selective colleges, 1992 (ln)</td>
<td>8.53</td>
<td>8.49</td>
</tr>
<tr>
<td>Enrollment in selective colleges, 1996 (ln)</td>
<td>8.60</td>
<td>8.58</td>
</tr>
<tr>
<td>Enrollment in selective colleges, 2001 (ln)</td>
<td>8.80</td>
<td>8.82</td>
</tr>
<tr>
<td>12th-grade cohort (ln)</td>
<td>10.62</td>
<td>10.40</td>
</tr>
<tr>
<td>High school graduation rate (%)</td>
<td>83.02</td>
<td>80.86</td>
</tr>
<tr>
<td>Entrance exam residual</td>
<td>19.77</td>
<td>16.97</td>
</tr>
<tr>
<td>Average state unemployment (%)</td>
<td>4.59</td>
<td>4.77</td>
</tr>
<tr>
<td>Asian (%)</td>
<td>2.43</td>
<td>3.58</td>
</tr>
<tr>
<td>Black (%)</td>
<td>4.72</td>
<td>4.47</td>
</tr>
<tr>
<td>Hispanic (%)</td>
<td>18.11</td>
<td>12.43</td>
</tr>
<tr>
<td>Native American (%)</td>
<td>0.94</td>
<td>1.54</td>
</tr>
<tr>
<td>4-year institutions (%)</td>
<td>50.35</td>
<td>44.68</td>
</tr>
<tr>
<td>Private institutions (%)</td>
<td>19.25</td>
<td>19.54</td>
</tr>
<tr>
<td>For-profit institutions (%)</td>
<td>39.39</td>
<td>34.28</td>
</tr>
<tr>
<td>Private 4-year institutions (%)</td>
<td>16.57</td>
<td>16.54</td>
</tr>
<tr>
<td>For-profit 4-year institutions (%)</td>
<td>13.74</td>
<td>8.56</td>
</tr>
<tr>
<td>Public 2-year institutions (%)</td>
<td>21.33</td>
<td>26.49</td>
</tr>
<tr>
<td>Private 2-year institutions (%)</td>
<td>2.68</td>
<td>3.01</td>
</tr>
<tr>
<td>For-profit 2-year institutions (%)</td>
<td>25.64</td>
<td>25.72</td>
</tr>
<tr>
<td>Institutions requiring entrance exam scores (%)</td>
<td>52.64</td>
<td>44.57</td>
</tr>
<tr>
<td>State expenditures per student ($)</td>
<td>$57,870.74</td>
<td>$49,954.95</td>
</tr>
<tr>
<td>RMSPE</td>
<td/>
<td>0.042</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013189X12474065">
<p><italic>Note</italic>. Graduation rate is of 12th-grade students. Racial composition is of the college-going (ages 18–19) population. White is omitted. Postsecondary composition is of all 2- and 4-year degree-granting institutions. Two-year, public, and public-4-year are omitted. Entrance exam residual gives the state’s average over- and underperformance on the ACT after adjusting for examination rate. ACT scores have been converted to the 1,600-point SAT score. RMSPE = root mean squared prediction error.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table2-0013189X12474065" position="float">
<label>Table 2</label>
<caption><p>Synthetic Control Weights for Enrollment in Selective Colleges in Colorado</p></caption>
<graphic alternate-form-of="table2-0013189X12474065" xlink:href="10.3102_0013189X12474065-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Control State</th>
<th align="center">Synthetic Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td>Utah</td>
<td>0.485</td>
</tr>
<tr>
<td>Nevada</td>
<td>0.298</td>
</tr>
<tr>
<td>Wisconsin</td>
<td>0.111</td>
</tr>
<tr>
<td>California</td>
<td>0.077</td>
</tr>
<tr>
<td>Washington</td>
<td>0.028</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section9-0013189X12474065">
<title>Inference</title>
<p>With the synthetic control weights in hand, I calculate the effect of entrance exam requirements by performing a weighted linear regression with dummy variables indicating treated states in postpolicy years, the fixed effects and control variables described above, and using the synthetic control weights. The coefficient of the treated-state postpolicy dummy gives the treatment effect—it captures the difference in outcome between the entrance exam state and the comparison states after enactment of the entrance exam requirement relative to the same difference before the requirement.</p>
<p>This analysis differs from most regression analyses because of the small number of treated units in this analysis. As a result, the large-sample assumptions needed for traditional methods of statistical inference do not hold. Thus, the standard errors generated by typical regression estimation are not valid. Instead, I use an exact inference test as described by <xref ref-type="bibr" rid="bibr1-0013189X12474065">Abadie et al. (2007)</xref>. Using this technique, I first calculate the effect of a placebo treatment on all nontreated comparison states. I produce <italic>p</italic> values for the likelihood of a Type I error by calculating what percentage of these effects are as or more extreme than the effect I estimate for the states that did receive the treatment. I limit these comparisons to placebo units whose synthetic controls have an RMSPE no more than five times as large as that of the given treated unit. This restriction helps account for the fact that some synthetic matches are closer than others and thus helps free the <italic>p</italic> values from bias stemming from poor matches.</p>
</sec>
</sec>
<sec id="section10-0013189X12474065">
<title>Individual-Level Analysis</title>
<p>The state-level analysis described above estimates the effect of entrance exam requirements on aggregate state-level college enrollment for different types of colleges. To provide another perspective on the effect of entrance exam requirements I performed a second analysis that used individual-level data to examine the effects of entrance exam requirements on the likelihood a particular individual enrolled in different types of colleges.</p>
<p>I also conducted the individual-level analysis as a comparative interrupted time series: I compared the likelihood of enrollment in a particular type of college between individuals who lived in exam-requiring and comparison states before and after the exam requirements were enacted. State and year fixed effects control for the effect of living in a particular state, or graduating high school in a particular year. Because I evaluated the effect of a policy that occurred in a particular state in a given year, threats to the validity of the effect estimates must come from omitted variables that are also state-by-year (<xref ref-type="bibr" rid="bibr5-0013189X12474065">Angrist &amp; Pischke, 2009</xref>). Thus, I controlled for the same state-by-year covariates that were included in the state-level analysis except for the state racial composition variables: I account for race as an individual variable. Controls for individual characteristics help add precision to the estimates. In addition to race, these individual controls included a student’s gender and whether the individual lived in a city.</p>
<p>To choose comparison units in a way that maximized the match between treated and control units in terms of state-by-year factors, I followed the example of <xref ref-type="bibr" rid="bibr18-0013189X12474065">Fitzpatrick (2008)</xref> and used the synthetic control weights from the state-level analysis to determine comparison observations. Thus, I used analysis weights that were the product of the data’s original sample weights and the synthetic control weights that corresponded to an individual’s state of residence. This weighting means that comparison individuals come from states with comparable pretreatment characteristics and outcome trajectories, with greater weights given to students from more closely matching states.</p>
<p>Estimates of the effect of mandatory entrance exams came from a weighted linear probability model of college enrollment with dummy variables that indicated whether a student lived in an exam requirement state after the policy was enacted (the treatment), state and year fixed effects, and controls for the covariates described above.<sup><xref ref-type="fn" rid="fn3-0013189X12474065">3</xref></sup> The coefficient of the treated-state postpolicy dummy gives the difference in likelihood of a given outcome for students living in a treated state, relative to individuals in comparison states in the posttreatment period. For this set of analyses, I examined whether individuals enrolled in college at all, enrolled at a 2- or 4-year college, or were enrolled full-time. In contrast to the state-level analysis, the individual-level analysis allows for the analysis of college attendance across state lines. This ability eliminates the need for the assumption I made in the state-level analysis that students affected by the entrance exam policies would attend college in-state. As in the state-level analyses, I ran the individual-level models separately for Colorado, Illinois, and Maine.</p>
</sec>
</sec>
<sec id="section11-0013189X12474065">
<title>Data</title>
<sec id="section12-0013189X12474065">
<title>State-Level Data</title>
<p>For the state-level analysis, I use IPEDS college enrollment data for every year from 1988 to 2009 (except for 1999, when data were not collected). IPEDS is an annual survey of all postsecondary institutions that accept federal money and is gathered by the NCES. The ideal data for this study would be a count of all students from each state who attend college. IPEDS only started to collect enrollment data by state-of-residence in 2000 and even then only collected it every other year. Thus, I assume that if a student is uncertain enough about her college choice that taking standardized exams will affect her decision, the student probably will attend school in-state. The reasonableness of this assumption is backed by prior research that showed that students generally attend colleges close to home (e.g., <xref ref-type="bibr" rid="bibr23-0013189X12474065">Long, 2004</xref>). Although the closest college to some students may be in a neighboring state, I also assume that the cost difference of attending college in-state versus out-of-state will, on average, encourage students to attend close, in-state colleges. These assumptions allow me to analyze changes in statewide college enrollment, which I measure by aggregating the number of first-time, 1st-year students reported by degree-granting institutions within each state.</p>
<p>These data were combined with state–year covariates drawn from other sources. Annual state unemployment data were drawn from the Bureau of Labor Statistics, whereas annual state 12th-grade enrollment and total high school graduates come from the Common Core of Data collected by NCES. Data on the percentage of 18- to 19-year-old White, Hispanic, Asian, Black, and Native American in each state were merged from Census data. Data on state average ACT and SAT scores and participation rates came from the ACT, Inc., and the College Board.<sup><xref ref-type="fn" rid="fn4-0013189X12474065">4</xref></sup></p>
<p>This data set is valuable for analysis because it allows for the study of enrollment changes at the rich variety of institution types identified in IPEDS. This feature is particularly valuable for addressing questions about how entrance exam requirements affect student preferences for different types of schools. However, aggregated state data limits statistical power because there are so few states on which to base the analysis. Synthetic controls serve to alleviate this limitation.</p>
</sec>
<sec id="section13-0013189X12474065">
<title>Individual-Level Data</title>
<p>Individual-level data came from the October Supplement of the CPS. I limited the sample to individuals aged 18–19—those of an age plausibly to be on-time college freshmen. The final sample contained 54,385 potential observations between 1994 and 2009. I merged this data with the state–year covariates used in the state-level analysis. Weighted descriptive statistics for the individual-level CPS observations are given in <xref ref-type="table" rid="table3-0013189X12474065">Table 3</xref>.</p>
<table-wrap id="table3-0013189X12474065" position="float">
<label>Table 3</label>
<caption><p>Weighted Descriptive Statistics of Individual Data from Treated and Synthetic Control States</p></caption>
<graphic alternate-form-of="table3-0013189X12474065" xlink:href="10.3102_0013189X12474065-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">National</th>
<th align="center">Colorado</th>
<th align="center">Synthetic Colorado</th>
<th align="center">Illinois</th>
<th align="center">Synthetic Illinois</th>
<th align="center">Maine</th>
<th align="center">Synthetic Maine</th>
</tr>
</thead>
<tbody>
<tr>
<td>Male</td>
<td>50.59</td>
<td>51.78</td>
<td>50.61</td>
<td>48.87</td>
<td>49.35</td>
<td>50.68</td>
<td>50.19</td>
</tr>
<tr>
<td>White</td>
<td>63.87</td>
<td>69.70</td>
<td>51.43</td>
<td>64.22</td>
<td>55.64</td>
<td>96.71</td>
<td>85.10</td>
</tr>
<tr>
<td>Black</td>
<td>15.11</td>
<td>5.15</td>
<td>9.86</td>
<td>17.41</td>
<td>17.00</td>
<td>0.56</td>
<td>2.46</td>
</tr>
<tr>
<td>Hispanic</td>
<td>15.49</td>
<td>19.50</td>
<td>31.93</td>
<td>14.59</td>
<td>20.70</td>
<td>0.96</td>
<td>6.53</td>
</tr>
<tr>
<td>Other race</td>
<td>6.20</td>
<td>6.37</td>
<td>9.89</td>
<td>4.26</td>
<td>8.06</td>
<td>1.77</td>
<td>6.57</td>
</tr>
<tr>
<td>City resident</td>
<td>6.83</td>
<td>4.27</td>
<td>14.58</td>
<td>21.10</td>
<td>6.10</td>
<td>0.00</td>
<td>0.28</td>
</tr>
<tr>
<td>Total no. of observations</td>
<td>54,385</td>
<td>899</td>
<td>13,536</td>
<td>1,918</td>
<td>14,661</td>
<td>801</td>
<td>6,078</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013189X12474065">
<p><italic>Note</italic>. Table gives weighted percentage of sample that falls in each category.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Because they contain many more observations than the state-level analysis, analyses of these data allow for more precision in the estimate of the entrance exam requirement effects. But these data have the drawback that they are not as rich as the state-level data in providing detail about where students attend college. The CPS only includes whether a student is enrolled in a 2- or 4-year college and whether the student is enrolled full- or part-time.</p>
</sec>
<sec id="section14-0013189X12474065">
<title>Power</title>
<p>Because of their limited observations, these two data sets are limited in their statistical power, but should work together to provide a more complete picture of the effect of statewide test-requirement policies than either one could on its own. The methods I use go far in improving the statistical power of the analyses. In the state-level analysis, I am able to detect differences in overall college enrollment rates at the α = 0.1 level of 0.050, 0.050, and 0.098 in Colorado, Illinois, and Maine.<sup><xref ref-type="fn" rid="fn5-0013189X12474065">5</xref></sup> In the individual-level analysis, I am able to detect differences in the likelihood of any college enrollment at the α = 0.1 level of 0.087, 0.11, and 0.075 with 80% power in Colorado, Illinois, and Maine, respectively (calculated according to <xref ref-type="bibr" rid="bibr11-0013189X12474065">Bloom, 1995</xref>).</p>
</sec>
</sec>
<sec id="section15-0013189X12474065" sec-type="results">
<title>Results</title>
<p>Results for the analysis of the state-level data are presented in <xref ref-type="table" rid="table4-0013189X12474065">Table 4</xref>, and results for the individual-level analysis are presented in <xref ref-type="table" rid="table5-0013189X12474065">Table 5</xref>. I describe each set of results in turn.</p>
<table-wrap id="table4-0013189X12474065" position="float">
<label>Table 4</label>
<caption><p>Estimated Effect of Entrance Exam Requirements on State Enrollment Changes, by State and College Type</p></caption>
<graphic alternate-form-of="table4-0013189X12474065" xlink:href="10.3102_0013189X12474065-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">All Colleges</th>
<th align="center">4-Year</th>
<th align="center">2-Year</th>
<th align="center">Private 4-Year</th>
<th align="center">Public 4-Year</th>
<th align="center">Public 2-Year</th>
<th align="center">Inclusive</th>
<th align="center">Selective</th>
<th align="center">More Selective</th>
<th align="center">Test Required</th>
<th align="center">100% Full- Time Students</th>
</tr>
</thead>
<tbody>
<tr>
<td>Colorado</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> ACT requirement</td>
<td>0.015</td>
<td>−0.070</td>
<td>0.068</td>
<td>0.104<sup><xref ref-type="table-fn" rid="table-fn4-0013189X12474065">†</xref></sup></td>
<td>−0.022</td>
<td>0.154</td>
<td>0.231</td>
<td>−0.036</td>
<td>0.247<xref ref-type="table-fn" rid="table-fn4-0013189X12474065">*</xref></td>
<td>0.112</td>
<td>−0.016</td>
</tr>
<tr>
<td> RMSPE of control</td>
<td>0.065</td>
<td>0.042</td>
<td>0.089</td>
<td>0.108</td>
<td>0.032</td>
<td>0.089</td>
<td>0.298</td>
<td>0.042</td>
<td>0.089</td>
<td>0.075</td>
<td>0.031</td>
</tr>
<tr>
<td> No. of valid placebos</td>
<td>45</td>
<td>42</td>
<td>40</td>
<td>41</td>
<td>41</td>
<td>39</td>
<td>40</td>
<td>45</td>
<td>37</td>
<td>42</td>
<td>40</td>
</tr>
<tr>
<td>Illinois</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> ACT requirement</td>
<td>0.003</td>
<td>0.127<xref ref-type="table-fn" rid="table-fn4-0013189X12474065">*</xref></td>
<td>−0.167</td>
<td>0.088</td>
<td>−0.099<sup><xref ref-type="table-fn" rid="table-fn4-0013189X12474065">†</xref></sup></td>
<td>−0.214<sup><xref ref-type="table-fn" rid="table-fn4-0013189X12474065">†</xref></sup></td>
<td>0.015</td>
<td>−0.068</td>
<td>−0.071</td>
<td>−0.04</td>
<td>−0.073</td>
</tr>
<tr>
<td> RMSPE of control</td>
<td>0.032</td>
<td>0.013</td>
<td>0.063</td>
<td>0.024</td>
<td>0.032</td>
<td>0.063</td>
<td>0.040</td>
<td>0.020</td>
<td>0.026</td>
<td>0.023</td>
<td>0.016</td>
</tr>
<tr>
<td> No. of valid placebos</td>
<td>42</td>
<td>32</td>
<td>39</td>
<td>32</td>
<td>41</td>
<td>38</td>
<td>28</td>
<td>37</td>
<td>32</td>
<td>35</td>
<td>39</td>
</tr>
<tr>
<td>Maine</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> SAT requirement</td>
<td>−0.102<sup><xref ref-type="table-fn" rid="table-fn4-0013189X12474065">†</xref></sup></td>
<td>0.005</td>
<td>−0.004</td>
<td>0.074</td>
<td>−0.027</td>
<td>−0.149<sup><xref ref-type="table-fn" rid="table-fn4-0013189X12474065">†</xref></sup></td>
<td>0.015</td>
<td>0.044</td>
<td>−0.044</td>
<td>0.142<xref ref-type="table-fn" rid="table-fn4-0013189X12474065">*</xref></td>
<td>0.021</td>
</tr>
<tr>
<td> RMSPE of control</td>
<td>0.055</td>
<td>0.041</td>
<td>0.092</td>
<td>0.042</td>
<td>0.049</td>
<td>0.107</td>
<td>0.063</td>
<td>0.071</td>
<td>0.041</td>
<td>0.053</td>
<td>0.035</td>
</tr>
<tr>
<td> No. of valid placebos</td>
<td>46</td>
<td>43</td>
<td>41</td>
<td>39</td>
<td>44</td>
<td>42</td>
<td>37</td>
<td>46</td>
<td>32</td>
<td>42</td>
<td>41</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0013189X12474065">
<p><italic>Note</italic>. Estimates of the effect of the given state’s testing requirement on the log of total state enrollment at the given college type. Covariate coefficients not reported. Estimation includes outcome data from 1994 to 2009. Number of valid placebos counts the number of placebo cases where the synthetic match had RMSPE at most 5 times as high as that of the given case. Indications of statistical significance are based on the percentage of placebo cases with estimated effects as or more extreme than the given case, as described in the text. RMSPE = root mean squared prediction error.</p>
</fn>
<fn id="table-fn4-0013189X12474065">
<label>†</label>
<p><italic>p</italic> &lt; .1. *<italic>p</italic> &lt; .05.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table5-0013189X12474065" position="float">
<label>Table 5</label>
<caption><p>Estimated Effect of SAT or ACT Requirements on Individual Likelihood of College Enrollment</p></caption>
<graphic alternate-form-of="table5-0013189X12474065" xlink:href="10.3102_0013189X12474065-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Any College</th>
<th align="center">4-Year College</th>
<th align="center">2-Year College</th>
<th align="center">Full-Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Colorado</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> ACT requirement</td>
<td>0.101</td>
<td>0.099</td>
<td>0.127<xref ref-type="table-fn" rid="table-fn6-0013189X12474065">**</xref></td>
<td>0.125<xref ref-type="table-fn" rid="table-fn6-0013189X12474065">*</xref></td>
</tr>
<tr>
<td/>
<td>(0.087)</td>
<td>(0.114)</td>
<td>(0.035)</td>
<td>(0.055)</td>
</tr>
<tr>
<td> <italic>N</italic></td>
<td>12,816</td>
<td>7,131</td>
<td>9,892</td>
<td>17,249</td>
</tr>
<tr>
<td>Illinois</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> ACT requirement</td>
<td>0.096<xref ref-type="table-fn" rid="table-fn6-0013189X12474065">*</xref></td>
<td>0.180<xref ref-type="table-fn" rid="table-fn6-0013189X12474065">**</xref></td>
<td>−0.072</td>
<td>0.191<xref ref-type="table-fn" rid="table-fn6-0013189X12474065">**</xref></td>
</tr>
<tr>
<td/>
<td>(0.044)</td>
<td>(0.053)</td>
<td>(0.044)</td>
<td>(0.047)</td>
</tr>
<tr>
<td> <italic>N</italic></td>
<td>14,809</td>
<td>17,405</td>
<td>12,800</td>
<td>15,838</td>
</tr>
<tr>
<td>Maine</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> SAT requirement</td>
<td>0.007</td>
<td>0.066</td>
<td>0.017</td>
<td>0.067</td>
</tr>
<tr>
<td/>
<td>(0.049)</td>
<td>(0.04)</td>
<td>(0.030)</td>
<td>(0.048)</td>
</tr>
<tr>
<td> <italic>N</italic></td>
<td>6,302</td>
<td>6,517</td>
<td>6,363</td>
<td>5,470</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0013189X12474065">
<p><italic>Note</italic>. All regressions include controls for individual and state–year covariates as well as state and year fixed effects. Standard errors are in parentheses.</p>
</fn>
<fn id="table-fn6-0013189X12474065">
<label>*</label>
<p><italic>p</italic> &lt; .05. **<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<sec id="section16-0013189X12474065">
<title>State-Level Outcomes</title>
<sec id="section17-0013189X12474065">
<title>Changes in overall college enrollment</title>
<p>As seen in <xref ref-type="table" rid="table4-0013189X12474065">Table 4</xref>, there were no notable changes in enrollment in Colorado or Illinois institutions that were associated with each state’s ACT requirement. Maine, however, saw roughly a 10% drop in the number of students enrolled in its colleges.</p>
</sec>
<sec id="section18-0013189X12474065">
<title>Two- versus 4-year enrollment</title>
<p>Although there was some evidence for overall change in enrollment levels, it may be the case that enrollment changes were focused in 2- or 4-year institutions. In fact, Illinois saw a roughly 12% increase in enrollment in its 4-year institutions. In the 4-year sector, changes in enrollment appeared to disproportionately benefit private institutions. In all three test-requiring states, estimates of changes in enrollment were all greater for private 4-year schools than for public ones. Colorado in particular saw just over a 10% increase in its private 4-year enrollment.</p>
<p>In contrast, Maine and Illinois saw decreases in 2-year college enrollment, decreases that were particularly concentrated in the state’s public 2-year colleges. Colorado, however, saw small, but not significant, increases in 2-year college enrollment.</p>
</sec>
<sec id="section19-0013189X12474065">
<title>Enrollment based on admissions policies</title>
<p>Given the prominent role college entrance exams play in admissions procedures, it is also reasonable to expect changes in college enrollment based on admissions policies. As <xref ref-type="table" rid="table4-0013189X12474065">Table 4</xref> shows, in each state with entrance exam requirements, Carnegie-classified “inclusive” and “selective” 4-year schools saw no notable change in enrollment levels, although all effect estimates for enrollment at inclusive schools were positive. Colorado saw a statistically significant 25% increase in enrollment at “more selective” 4-year colleges.</p>
<p>Estimates for enrollment changes at schools that required entrance exams for admission were also positive in Colorado and Maine, though only Maine’s 14% enrollment increase reached statistical significance. Finally, schools that only admitted full-time students saw no significant or notable change in levels of enrollment in any of the entrance-exam-requiring states.</p>
</sec>
</sec>
<sec id="section20-0013189X12474065">
<title>Individual-Level Outcomes</title>
<p>The analysis of the individual-level CPS data gives the change in likelihood that a student enrolls in a college of a particular type. As seen in <xref ref-type="table" rid="table5-0013189X12474065">Table 5</xref>, Colorado students were significantly more likely to enroll in 2-year colleges or enroll full-time, but no more or less likely to enroll in college overall. Illinois students were nearly 10 percentage points more likely to enroll in any college after the enactment of the entrance exam requirement. They were also significantly more likely to enroll in 4-year colleges and, like Colorado students, more likely to enroll full-time. There were no statistically significant changes in the likelihood of college enrollment in Maine.</p>
</sec>
</sec>
<sec id="section21-0013189X12474065" sec-type="discussion">
<title>Discussion</title>
<p>This article presents the effects of state-mandated college entrance exam testing in three states using quasi-experimental methods and synthetic control comparison groups. I find evidence that testing policies in these states re-sorted students between different types of institutions. Overall, Illinois students were more likely to enroll in 4-year colleges. Furthermore, Colorado saw an increase in enrollment at private and “more selective” 4-year institutions, whereas Illinois and Maine saw a decrease in enrollment at public 2-year institutions. There is also some evidence in Colorado and Maine, after the test requirements, that students were more likely to enroll at schools that required the SAT or ACT for admission. And that Colorado and Illinois students were more likely to be enrolled full-time.</p>
<p>Interpreting the state- and individual-level results together illustrates how college enrollment changed in Colorado, Illinois, and Maine as a result of each state’s SAT or ACT requirement. State-level results allow us to see a detailed picture of changes in total enrollment in colleges of many types <italic>within a given state</italic>. The individual-level results, however, offer the change in likelihood of college enrollment for students from a given state regardless of the state in which they enroll. Thus, a student who leaves her state to go to college will contribute to a decrease in enrollment in the state-level analysis, but will be captured as a college enrollee in the individual-level analysis.</p>
<p>In Illinois, for example, the significant increase in likelihood of enrollment in 4-year colleges in the individual analysis confirms the finding that exam requirements were associated with an increase in 4-year college enrollment in the state-level analysis. Furthermore, we see that the individual-level analysis shows an increase in the likelihood that Illinois students enrolled in college at all, but there did not appear to be a significant overall increase in enrollment at Illinois colleges in the state-level analysis. This apparent contradiction indicates that the effect of the Illinois exam requirement was to shift students into Illinois’s 4-year colleges as well as to some colleges out of state.</p>
<p>The general increase in private 4-year college enrollment in Colorado is interesting given that <xref ref-type="bibr" rid="bibr12-0013189X12474065">Bound and Turner (2007)</xref> found these institutions to be among the least responsive to changes in demand for college. There are two possible explanations for this increase despite this inelasticity. First, private colleges may be more likely to purchase student contact information from the ACT and College Board in order to build their recruitment mailing lists. Therefore, increases in enrollment at these schools may have resulted from these schools sending informational material to more students in test-requiring states. Second, colleges can use entrance exam scores to determine eligibility for merit-based scholarships. Thus, students who faced entrance exam requirements may have had more access to merit-based aid to alleviate the cost of tuition at private institutions. This merit–aid argument may also help explain the increased likelihood of full-time enrolment in the individual-level data: access to greater aid may have allowed students to afford to enroll full- rather than part-time.</p>
<p>Colorado’s and Maine’s positive increases in enrolment at schools that require the SAT or ACT for admission are also notable findings. This effect indicates that a lack of entrance exam scores may present a barrier to enrollment at certain colleges. This barrier disappears with exam requirements, allowing students more freedom to enroll in a school that best supports their academic needs. If students are able to optimize their choices in this way, entrance exam requirements may have long-term benefits such as lower transfer or dropout rates and higher completion and graduation rates.</p>
<sec id="section22-0013189X12474065">
<title>Are College Entrance Exam Requirements Worth It?</title>
<p>Georgia spent about $189 million on its HOPE scholarship program in 1998–1999 and saw a roughly 7% increase in college enrollment (<xref ref-type="bibr" rid="bibr17-0013189X12474065">Dynarski, 2000</xref>). This amounted to the state spending about $13,650 for every new student the program brought into college. In contrast, for example, Colorado spends about $1.6 million annually to administer the ACT. So, in order for Colorado’s ACT requirement to be at least as cost-effective as the Georgia HOPE Scholarship as a method of promoting college enrollment, it would only have to induce a 0.09 percentage point increase in college enrollment rates.<sup><xref ref-type="fn" rid="fn6-0013189X12474065">6</xref></sup> Such results, even with perfect data, would be difficult to discern statistically, yet many of the findings presented here exceed these thresholds, even if they do not reach statistical significance. It thus seems likely that states should be able to achieve these modest increases in postsecondary enrollment. Even if they do not increase enrollment, the re-sorting of students into institutions caused by entrance exam requirements may justify the policy cost if this re-sorting is accompanied by improved college outcomes such as higher graduation rates.</p>
</sec>
</sec>
<sec id="section23-0013189X12474065" sec-type="conclusions">
<title>Conclusion</title>
<p>In the past decade, several states have tried to increase the percentage of their high school graduates who attend college by requiring that all students take a college entrance exam in their junior year of high school. As state budgets grow progressively tighter, state policy makers concerned with educational outcomes should increasingly be tempted by nudges toward college enrollment. This temptation makes it vital that researchers seek to understand how state policy landscapes and existing incentives interact with these nudges to influence student college enrollment behavior. For example, the Colorado system grants automatic college admission to students who meet a given entrance exam threshold; Illinois explicitly tried to help counselors and students understand how to use ACT scores, whereas Maine turned to the SAT as it faced NCLB-based assessment sanctions. Each of these could be one of many reasons for the between-state differences in the results presented here.</p>
<p>Just as financial aid does not address the only barrier to college enrollment, so too are entrance exam requirements only one possible nudge states can give students. Some states, for example, have considered making application to college a requirement. Given that college entrance exams are effective at causing students to rethink their college choices, it is important to think about where else in the college enrollment process nudges could affect student decisions. Furthermore, it is important to think about whether these nudges affect not only students’ initial college enrollment choices but ultimately their college success. This is a rich area for policy intervention to positively affect students’ decisions about college.</p>
</sec>
</body>
<back>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0013189X12474065">
<label>1</label>
<p>In this design, I am only looking for a relative change in the level of the time series after the enactment of entrance exam requirements. Alternatively, this method could be referred to as a nonparametric difference-in-difference design.</p>
</fn>
<fn fn-type="other" id="fn2-0013189X12474065">
<label>2</label>
<p>Details of this adjustment are given in the methodological appendix.</p>
</fn>
<fn fn-type="other" id="fn3-0013189X12474065">
<label>3</label>
<p>Models were also run as logistic regressions, but the results were not qualitatively different from the linear probability models. The results of the latter are reported for ease of interpretation whereas results from the former are available from the author on request.</p>
</fn>
<fn fn-type="other" id="fn4-0013189X12474065">
<label>4</label>
<p>Although not publically available from the College Board website, these data have been collected from the College Board over time and are available from the National Center for Education Statistics.</p>
</fn>
<fn fn-type="other" id="fn5-0013189X12474065">
<label>5</label>
<p>Power calculations are given for the overall change in college enrollment. Power calculations for other outcomes are available from the author upon request. Note there is no formal test of power for the synthetic control method. Instead I present the minimum detectable difference at the α = 0.1 level given the distribution of root mean squared prediction error in the placebo units.</p>
</fn>
<fn fn-type="other" id="fn6-0013189X12474065">
<label>6</label>
<p>This calculation ignores the cost of providing a different No Child Left Behind (NCLB) accountability exam if the ACT were not used. That the ACT serves the double purpose of meeting an NCLB requirement and potentially promotes college enrollment makes it that much more valuable</p>
</fn>
</fn-group>
</notes>
<bio>
<title>Author</title>
<p><bold>DANIEL KLASIK</bold> is a PhD candidate in Education Policy at the Stanford University School of Education, 520 Galvez Mall, Stanford, CA 94305; <italic><email>djklasik@stanford.edu</email></italic>. His research focuses on the factors that influence students’ decisions about whether and where to attend college.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Abadie</surname><given-names>A.</given-names></name>
<name><surname>Diamond</surname><given-names>A.</given-names></name>
<name><surname>Hainmueller</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <source>Synthetic control methods for comparative case studies: Estimating the effect of California’s tobacco control program</source> (Working Paper No. 12831). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research</publisher-name>.</citation>
</ref>
<ref id="bibr2-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abadie</surname><given-names>A.</given-names></name>
<name><surname>Gardeazabal</surname><given-names>J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The economic costs of conflict: A case study of the Basque Country</article-title>. <source>The American Economic Review</source>, <volume>93</volume>, <fpage>113</fpage>–<lpage>132</lpage>.</citation>
</ref>
<ref id="bibr3-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abraham</surname><given-names>K. G.</given-names></name>
<name><surname>Clark</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Financial aid and student’s college decisions: Evidence from the District of Columbia Tuition Assistance Grant Program</article-title>. <source>Journal of Human Resources</source>, <volume>41</volume>, <fpage>578</fpage>–<lpage>610</lpage>.</citation>
</ref>
<ref id="bibr4-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Altonji</surname><given-names>J.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The demand for and return to education when education outcomes are uncertain</article-title>. <source>Journal of Labor Economics</source>, <volume>11</volume>, <fpage>48</fpage>–<lpage>83</lpage>.</citation>
</ref>
<ref id="bibr5-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Angrist</surname><given-names>J. D.</given-names></name>
<name><surname>Pischke</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <source>Mostly harmless econometrics: An empiricist’s companion</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Avery</surname><given-names>C.</given-names></name>
<name><surname>Kane</surname><given-names>T.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Student perceptions of college opportunities: The Boston COACH program</article-title>. In <person-group person-group-type="editor">
<name><surname>Hoxby</surname><given-names>C.</given-names></name>
</person-group> (Ed.), <source>College choices: The economics of where to go, when to go, and how to pay for it</source> (pp. <fpage>355</fpage>–<lpage>391</lpage>). <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr7-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Berkner</surname><given-names>L.</given-names></name>
<name><surname>Chavez</surname><given-names>L.</given-names></name>
</person-group> (<year>1997</year>). <source>Access to postsecondary education for the 1992 high school graduates</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Dept. of Education, Office of Educational Research and Improvement, National Center for Education Statistics</publisher-name>.</citation>
</ref>
<ref id="bibr8-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Beshears</surname><given-names>J.</given-names></name>
<name><surname>Choi</surname><given-names>J. J.</given-names></name>
<name><surname>Laibson</surname><given-names>D.</given-names></name>
<name><surname>Madrian</surname><given-names>B. C.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The importance of default options for retirement saving outcomes: Evidence from the United States</article-title>. In <person-group person-group-type="editor">
<name><surname>Brown</surname><given-names>J.</given-names></name>
<name><surname>Liebman</surname><given-names>J.</given-names></name>
<name><surname>Wise</surname><given-names>D. A.</given-names></name>
</person-group> (Eds.), <source>Social security policy in a changing environment</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr9-0013189X12474065">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Betebenner</surname><given-names>D.</given-names></name>
<name><surname>Howe</surname><given-names>K.</given-names></name>
</person-group> (<year>2001</year>). <source>Implications for the use of the ACT within the Colorado Student Assessment Program</source>. <publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>National Education Policy Center</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://nepc.colorado.edu/files/ACT_CSAP_Report.pdf">http://nepc.colorado.edu/files/ACT_CSAP_Report.pdf</ext-link></citation>
</ref>
<ref id="bibr10-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bettinger</surname><given-names>E. P.</given-names></name>
<name><surname>Long</surname><given-names>B. T.</given-names></name>
<name><surname>Oreopoulos</surname><given-names>P.</given-names></name>
<name><surname>Sanbonmatsu</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <source>The role of simplification and information in college decisions: Results from the H&amp;R Block FAFSA experiment</source> (Working Paper No. 15361). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research</publisher-name>.</citation>
</ref>
<ref id="bibr11-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bloom</surname><given-names>H. S.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Minimum detectable effects: A simple way to report the statistical power of experimental designs</article-title>. <source>Evaluation Review</source>, <volume>19</volume>, <fpage>547</fpage>–<lpage>556</lpage>.</citation>
</ref>
<ref id="bibr12-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bound</surname><given-names>J.</given-names></name>
<name><surname>Turner</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Cohort crowding: How resources affect college attainment</article-title>. <source>Journal of Public Economics</source>, <volume>91</volume>, <fpage>877</fpage>–<lpage>899</lpage>.</citation>
</ref>
<ref id="bibr13-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cabrera</surname><given-names>A.</given-names></name>
<name><surname>La Nasa</surname><given-names>S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>On the path to college: Three critical tasks facing America’s disadvantaged</article-title>. <source>Research in Higher Education</source>, <volume>42</volume>, <fpage>119</fpage>–<lpage>149</lpage>.</citation>
</ref>
<ref id="bibr14-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Choi</surname><given-names>J. J.</given-names></name>
<name><surname>Laibson</surname><given-names>D.</given-names></name>
<name><surname>Madrian</surname><given-names>B. C.</given-names></name>
<name><surname>Metrick</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Optimal defaults</article-title>. <source>American Economic Review</source>, <volume>93</volume>, <fpage>180</fpage>–<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr15-0013189X12474065">
<citation citation-type="gov">
<collab>College Board</collab>. (<year>2005</year>). <source>Report for the state of Maine on the alignment of the SAT and the PSAT/NMSQT® to the Maine Learning Results</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>College Board</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.maine.gov/education/mhsa/docs/sat_alignment_study.pdf">http://www.maine.gov/education/mhsa/docs/sat_alignment_study.pdf</ext-link></citation>
</ref>
<ref id="bibr16-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cornwell</surname><given-names>C.</given-names></name>
<name><surname>Mustard</surname><given-names>D. B.</given-names></name>
<name><surname>Sridhar</surname><given-names>D. J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The enrollment effects of merit-based financial aid</article-title>. <source>Journal of Labor Economics</source>, <volume>24</volume>, <fpage>761</fpage>–<lpage>786</lpage>.</citation>
</ref>
<ref id="bibr17-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dynarski</surname><given-names>S.</given-names></name>
</person-group> (<year>2000</year>). <article-title>HOPE for whom? Financial aid for the middle class and its impact on college attendance</article-title>. <source>National Tax Journal</source>, <volume>53</volume>, <fpage>629</fpage>–<lpage>661</lpage>.</citation>
</ref>
<ref id="bibr18-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fitzpatrick</surname><given-names>M. D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Starting school at four: The effect of universal pre-kindergarten on children’s academic achievement</article-title>. <source>B.E. Journal of Economic Analysis &amp; Policy</source>, <volume>8</volume>, Article <fpage>46</fpage>.</citation>
</ref>
<ref id="bibr19-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Horn</surname><given-names>L.</given-names></name>
</person-group> (<year>1997</year>). <source>Confronting the odds: Students at risk and the pipeline to higher education</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>US Department of Education, Office of Educational Research and Improvement, National Center for Education Statistics</publisher-name>.</citation>
</ref>
<ref id="bibr20-0013189X12474065">
<citation citation-type="web">
<collab>Illinois State Board of Education</collab>. (<year>2006</year>). <source>Prairie State Achievement Examination technical manual: 2006 testing cycle</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.isbe.net/assessment/pdfs/psae/tech_manual06.pdf">http://www.isbe.net/assessment/pdfs/psae/tech_manual06.pdf</ext-link></citation>
</ref>
<ref id="bibr21-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kane</surname><given-names>T. J.</given-names></name>
</person-group> (<year>2003</year>). <source>A quasi-experimental estimate of the impact of financial aid on college-going</source> (Working Paper No. 9703). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research</publisher-name>.</citation>
</ref>
<ref id="bibr22-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Klasik</surname><given-names>D.</given-names></name>
</person-group> (<year>2012</year>). <article-title>The college application gauntlet: A systematic analysis of the steps to four-year college enrollment</article-title>. <source>Research in Higher Education</source>, <volume>53</volume>, <fpage>506</fpage>–<lpage>549</lpage>.</citation>
</ref>
<ref id="bibr23-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Long</surname><given-names>B. T.</given-names></name>
</person-group> (<year>2004</year>). <article-title>How have college decisions changed over time? An application of the conditional logistic choice model</article-title>. <source>Journal of Econometrics</source>, <volume>121</volume>, <fpage>271</fpage>–<lpage>296</lpage>.</citation>
</ref>
<ref id="bibr24-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Madrian</surname><given-names>B. C.</given-names></name>
<name><surname>Shea</surname><given-names>D. F.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The power of suggestion: Inertia in 401(k) participation and savings behavior</article-title>. <source>Quarterly Journal of Economics</source>, <volume>116</volume>, <fpage>1149</fpage>–<lpage>1187</lpage>.</citation>
</ref>
<ref id="bibr25-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Manski</surname><given-names>C.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Adolescent econometricians: How do youth infer the returns to schooling?</article-title> In <person-group person-group-type="editor">
<name><surname>Clotfelter</surname><given-names>C.</given-names></name>
<name><surname>Rothschild</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <source>Studies of supply and demand in higher education</source> (pp. <fpage>43</fpage>–<lpage>57</lpage>). <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr26-0013189X12474065">
<citation citation-type="book">
<collab>National Center for Education Statistics, United States Department of Education (NCES)</collab>. (<year>2004</year>). <source>Education Longitudinal Study of 2002 (ELS: 2002), First follow-up, 2004</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr27-0013189X12474065">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Pallais</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Small differences that matter: Mistakes in applying to college, unpublished manuscript</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://scholar.harvard.edu/sites/scholar.iq.harvard.edu/files/apallais/files/4030.pdf">http://scholar.harvard.edu/sites/scholar.iq.harvard.edu/files/apallais/files/4030.pdf</ext-link></citation>
</ref>
<ref id="bibr28-0013189X12474065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Perna</surname><given-names>L. W.</given-names></name>
<name><surname>Rowan-Kenyon</surname><given-names>H.</given-names></name>
<name><surname>Bell</surname><given-names>A.</given-names></name>
<name><surname>Thomas</surname><given-names>S. L.</given-names></name>
<name><surname>Li</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>A typology of federal and state programs designed to promote college enrollment</article-title>. <source>Journal of Higher Education</source>, <volume>79</volume>, <fpage>243</fpage>–<lpage>267</lpage>.</citation>
</ref>
<ref id="bibr29-0013189X12474065">
<citation citation-type="gov">
<collab>State of Maine</collab>. (<year>2007</year>). <source>Maine high school assessment</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.maine.gov/education/mhsa/">http://www.maine.gov/education/mhsa/</ext-link></citation>
</ref>
<ref id="bibr30-0013189X12474065">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thaler</surname><given-names>R. H.</given-names></name>
<name><surname>Sunstein</surname><given-names>C. R.</given-names></name>
</person-group> (<year>2009</year>). <source>Nudge: Improving decisions about health, wealth, and happiness</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Penguin</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>