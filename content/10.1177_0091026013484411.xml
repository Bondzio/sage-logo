<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PPM</journal-id>
<journal-id journal-id-type="hwp">spppm</journal-id>
<journal-title>Public Personnel Management</journal-title>
<issn pub-type="ppub">0091-0260</issn>
<issn pub-type="epub">1945-7421</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0091026013484411</article-id>
<article-id pub-id-type="publisher-id">10.1177_0091026013484411</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Exploring the Topography of Performance and Effectiveness of U.S. Federal Agencies</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Joaquin</surname><given-names>M. Ernita</given-names></name>
<xref ref-type="aff" rid="aff1-0091026013484411">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Park</surname><given-names>Sung Min</given-names></name>
<xref ref-type="aff" rid="aff2-0091026013484411">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0091026013484411"><label>1</label>San Francisco State University, CA, USA</aff>
<aff id="aff2-0091026013484411"><label>2</label>Sungkyunkwan University, Seoul, Korea</aff>
<author-notes>
<corresp id="corresp1-0091026013484411">Sung Min Park, Department of Public Administration and Graduate School of Governance, Sungkyunkwan University, Seoul, 110-745, Korea. Email: <email>sm28386@skku.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>42</volume>
<issue>1</issue>
<fpage>55</fpage>
<lpage>74</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>We have a proliferation of tools to evaluate federal agencies’ performance and effectiveness. This article explores how effectiveness and performance values are distributed across government agencies based on three well-known assessment instruments used during the Bush administration: the Office of Management and Budget (OMB) Management Scorecard, the Performance Assessment Rating Tool (PART), and the Best Places to Work (BPTW) survey. A cluster analysis of the scores from these assessment tools allows us to examine the topography of the agencies in terms of the relationship between the tools and the context of performance, namely, the type of mission carried out by the agencies. Depending on the policy mission type, some agencies fare better in some assessment measures than others. By comparing scores from PART and OMB Scorecard with the BPTW survey, we also find a complex picture when leadership-driven performance metrics are compared with the results of an employee-based assessment of organizational effectiveness.</p>
</abstract>
<kwd-group>
<kwd>performance measurement</kwd>
<kwd>PART</kwd>
<kwd>scorecard</kwd>
<kwd>BPTW</kwd>
<kwd>policy typology</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0091026013484411" sec-type="intro">
<title>Introduction</title>
<p>The last decade saw increased efforts to improve government through various performance assessment systems. From the Clinton administration’s National Performance Review (NPR), the mantra of making government cost less yet work better evolved to the results-oriented philosophy of the recent Bush administration. The theory was that managers who were supposedly hampered by stringent rules in human, financial, and other management areas needed freedom to manage in ways that would result in more efficiency and accountability to their political superiors (<xref ref-type="bibr" rid="bibr1-0091026013484411">Barzelay, 1992</xref>).</p>
<p>During the last few decades, cutbacks provided the rationale to push managers to raise organizational performance while lowering costs. But to do this effectively, performance assessment tools were needed to supplement resource allocation decisions in government, a notion that is not new and can be traced to performance budgeting. From its origins at the local level to its adoption in the federal budget in the late 1940s and in many states in the 1990s, the promise of performance budgeting is that organizational performance can be measured through relevant yardsticks that would value the use of scarce resources and promote accountability to agency principals (<xref ref-type="bibr" rid="bibr29-0091026013484411">Rubin, 1998</xref>). Today the federal government has many such yardsticks in its arsenal, including the Office of Management and Budget (OMB) Scorecard for the 2001 President’s Management Agenda (PMA), the Performance Assessment Rating Tool (PART), and the Best Places to Work (BPTW) survey.</p>
</sec>
<sec id="section2-0091026013484411">
<title>Federal Performance and Effectiveness Assessment Tools</title>
<p>This study investigates federal agency and employee performance based on their scores from three types of performance and effectiveness measures: the OMB Scorecard, the PART, and the BPTW survey (see <xref ref-type="table" rid="table1-0091026013484411">Table 1</xref>). In this article, two cluster models reveal how different types of performance are distributed across the federal government.</p>
<table-wrap id="table1-0091026013484411" position="float">
<label>Table 1.</label>
<caption>
<p>Federal Agency Performance and Effectiveness Assessment Tools: PART, Scorecard, and BPTW.</p>
</caption>
<graphic alternate-form-of="table1-0091026013484411" xlink:href="10.1177_0091026013484411-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">PART</th>
<th align="center">Characteristics and measures</th>
</tr>
</thead>
<tbody>
<tr>
<td>OMB Scorecard</td>
<td>The Scorecard was a traffic-light scoring system that graded 26 agencies quarterly on the 5 PMA components: (a) human capital, (b) competitive sourcing, (c) e-government, (d) budget and performance integration, and (e) financial management. OMB and agencies set “Green Plans” and milestones that agencies must strive to meet for each component. All agencies started in red. Yellow indicated progress and green means that agencies have met their goals and targets. Agencies could be downgraded from green to yellow to red again. OMB published the Scorecard throughout the Bush administration, hinting the budgetary sanctions connected to the tool.</td>
</tr>
<tr>
<td>PART scores</td>
<td>Approximately 30 questions are required for federal managers to answer about program sections and a set of weights is assigned to each section, resulting in a final cumulative PART score: Program Purpose and Design scores reflect whether the program has a clear purpose and if the program is designed to meet that purpose (20%). Strategic Planning scores reflect whether the program’s agency has established suitable annual goals and long-term goals for its programs (10%). Program Management scores indicate whether the program has good management including appropriate financial oversight controls and program improvement methods (20%). Program Results scores indicate whether the program is achieving performance results according to strategic planning goals (50%). PART results were meant to inform budgetary decisions in Congress and OMB.</td>
</tr>
<tr>
<td>BPTW index scores and rankings</td>
<td>The <italic>BPTW in the Federal Government</italic> rankings were designed to improve the public sector work environment and organizational effectiveness. Partnership for Public Service and American University’s ISPPI use data from the Office of Personnel Management’s Federal Human Capital Survey to rank 279 federal agencies and subcomponents. The <italic>BPTW</italic> score is calculated both for the organization as a whole and also for specific demographic groups. Agencies and subcomponents are also scored in 10 workplace environment (“best in class”) categories (<ext-link ext-link-type="uri" xlink:href="http://www.bestplacestowork.org/BPTW/about/">www.bestplacestowork.org/BPTW/about/</ext-link>).</td>
</tr>
<tr>
<td>Number of cases</td>
<td>26 agencies’ scores</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0091026013484411"><p>Note: PART = Performance Assessment Rating Tool; BPTW = Best Places to Work; OMB = Office of Management and Budget; PMA = president’s management agenda; ISPPI = Institute for the Study of Public Policy Implementation.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>U.S. federal agencies in recent decades have been evaluated under three major assessment systems: the Government Performance and Results Act (GPRA), the OMB Scorecard for the PMA, and the PART. While GPRA was a product of bipartisan congressional efforts in 1993, PART and the Scorecard represented the Bush administration’s desire to implement management reforms following its idea of a more businesslike, results-oriented government.<sup><xref ref-type="fn" rid="fn1-0091026013484411">1</xref></sup> GPRA tries to link agency mission with performance by means of strategic planning and performance planning (<xref ref-type="bibr" rid="bibr21-0091026013484411">Moynihan, 2008</xref>).</p>
<p>PART evaluated program performance and effectiveness and its results were used to inform budgetary decision making in OMB and Congress, although without much success. The Bush administration created PART to focus on selected agency programs only each year. For each program, OMB asked a series of standard questions on program purpose and design, strategic planning, program management, and program results, and weighted the scores in a final calculation to reflect 20% for program purpose and design, 10% for strategic planning, 20% for program management, and 50% for program results. Thus, final PART score for a program was a percentage-based score from 0% to 100%, with 100% reflecting the top score. OMB reported the scores for each section and then classified final scores into four categories of effectiveness: Ineffective (final PART scores between 0% and 49%), Adequate (final PART scores between 50% and 69%), Moderately effective (final PART scores between 70% and 84%), and Effective (final part scores between 85% and 100%). A fifth category was called Results Not Demonstrated.</p>
<p>Unlike PART’s focus on evaluating agency programs, the Scorecard focused on agency implementation of the 2001 PMA in five key areas: (a) human capital management, (b) competitive sourcing of commercial services, (c) financial management, (d) e-government, and (e) budget and performance integration. The Scorecard worked under the Bush administration in the following way: OMB assigned each agency a score for each agenda item under the PMA using traffic lights of red, yellow, or green. With a set of criteria developed with the President’s Management Council (PMC), OMB since 2002 assessed each agency on their progress in implementing those five key components every quarter. Agencies were enticed to move from “red” indicating unsatisfactory performance or a starting condition of weakness in that area, to “yellow” indicating progress toward a set goal or deliverables within strict timelines, to “green” representing achievement of the milestones. Most agencies were red at the beginning, but many moved to green by the end of the Bush administration.</p>
<p>Complementing these two top-down, external measures of performance, we looked at an internal, bottom-up measure of agency effectiveness, namely, the BPTW rankings. This is consistent with the literature that broadening participation in assessment contributes to improvement in governance (<xref ref-type="bibr" rid="bibr38-0091026013484411">Yang &amp; Holzer, 2006</xref>). The BPTW annual rankings in the federal government are dubbed “the most comprehensive and authoritative rating and analysis of employee engagement in the federal government” (<xref ref-type="bibr" rid="bibr4-0091026013484411">The BPTW, 2007</xref>). The Partnership for Public Service and American University’s Institute for the Study of Public Policy Implementation (ISPPI) use data from the Office of Personnel Management’s Federal Human Capital Survey to rank 279 federal agencies and subcomponents. Agencies and subcomponents are ranked on an index score of overall employee engagement. The BPTW score is provided for the organization as a whole and also for specific demographic groups. Agencies and subcomponents also are scored in 10 workplace environment (“best in class”) categories such as effective leadership, teamwork, strategic management, performance culture, training and development, diversity management, employee skills/mission match, and work–life balance (WLB; <xref ref-type="bibr" rid="bibr4-0091026013484411">The BPTW, 2007</xref>). WLB policies in human resource management became pronounced when the Clinton administration directed agencies to establish a program to encourage and support WLB. The average percentage of positive responses to each item (e.g., <italic>agree</italic> or <italic>strongly agree; satisfied</italic> or <italic>very satisfied</italic>) was calculated into agency and subcomponent scores. The composite scores range from 0% to 100%. Agencies in the BPTW survey are ranked according to the extent to which federal employees consider their workloads reasonable and feasible, and managers support a balance between work and life. By comparing scores from PART and OMB Scorecard with the results the BPTW, we could see whether leader-driven performance metrics paralleled the results of an employee-based assessment of organizational effectiveness.</p>
</sec>
<sec id="section3-0091026013484411">
<title>Imperfect Tools: Issues in PART, Scorecard, and BPTW</title>
<p>The Bush administration’s OMB acknowledged that the president’s agenda was controversial and difficult to implement (<xref ref-type="bibr" rid="bibr35-0091026013484411">Styles, 2002</xref>). With the White House imposing its definition of success in the five PMA areas, some agencies thought the Scorecard was not viable with its across-the-board, one-size-fits-all approach to evaluating organizational performance. Some agencies performed better than others in moving from the red to the yellow and green columns, and many appealed to OMB for some flexibility in creating efficiencies (<xref ref-type="bibr" rid="bibr15-0091026013484411">Joaquin, 2009</xref>). <xref ref-type="bibr" rid="bibr5-0091026013484411">Breul (2007)</xref> noted that the emphasis of the management agenda, embodied by the OMB Scorecard, was not just to improve management as we know it of traditional administrative areas but also to evaluate how agencies conduct their programs in a manner “consistent with the policies of the incumbent administration” (p. 21). Thus, the Scorecard not only emphasized efficiency but also showed accountability to the democratic principal, in this case the president. As the White House publicized the quarterly scores, the message was that taxpayers hold agencies accountable and that poor scores would be humiliating to the agencies, that is, the shame management approach. Six years after the Scorecard was launched, OMB reported in January 2008 that 87% of agencies were rated as at least moderately acceptable, with scores better than red, up from 78% in 2007 and 20% in 2001.</p>
<p>The “balanced scorecard” approach was popular in the private sector long before the Bush administration launched its OMB Scorecard. <xref ref-type="bibr" rid="bibr17-0091026013484411">Kaplan and Norton (1996)</xref>, who popularized the balanced scorecard, promoted the idea for application in government as a way of communicating strategy through an integrated set of measurements that included financial and nonfinancial items. Its use in federal bureaucracy was chronicled a success in defense, aviation, and tax agencies (<xref ref-type="bibr" rid="bibr36-0091026013484411">Whittaker, 2003</xref>). In state governments, a balanced scorecard allowed the Texas Auditor’s Office a better way to translate and communicate mission objectives (<xref ref-type="bibr" rid="bibr18-0091026013484411">Kerr, 2002</xref>). However, to other scholars, the scorecard approach is lacking in that it neglects the promotion of democratic-constitutional values, including individual rights, constitutional integrity, transparency, and the rule of law (<xref ref-type="bibr" rid="bibr28-0091026013484411">Rosenbloom, 2007</xref>). Scholars recommend including “impact statements” that correspond to each of the red, yellow, and green lights/scores on the Scorecard, in terms of agency performance in promoting the values mentioned above. Adopting such a scorecard may take more time and money and a moderate learning curve, but <xref ref-type="bibr" rid="bibr28-0091026013484411">Rosenbloom (2007)</xref> argues that not measuring these may be tantamount to neglect of the democratic insights in public administration.</p>
<p>The problems with the PART program echo those of the OMB Scorecard, for example, that the definition of successful performance rested in the hands of those at the top, with little control in evaluation by those who knew and carried out the mission. <xref ref-type="bibr" rid="bibr21-0091026013484411">Moynihan (2008)</xref> synthesized the most important criticisms of PART in his book on performance management. First, PART represented “the interests, biases, and views of the OMB” (<xref ref-type="bibr" rid="bibr21-0091026013484411">Moynihan, 2008</xref>, p. 144). By doing so, OMB defined the issues and shaped interpretation of what is considered relevant in making budgetary decisions in the federal government. OMB dominated the dialogue on which goals were important (<xref ref-type="bibr" rid="bibr26-0091026013484411">Radin, 2006</xref>). Second, the Government Accountability Office found that PART ratings were ambiguously used, for example, some agencies might have received the rating “Results Not Demonstrated” simply when agencies and OMB disagreed on the appropriate program performance measures, a fundamental issue in any performance assessment initiative. Third, watchdogs called PART overly simplistic, unable to indicate why a program might fail or recognize that some programs are more complex than others (<xref ref-type="bibr" rid="bibr21-0091026013484411">Moynihan, 2008</xref>).</p>
<p>BPTW rating and analysis solely depends upon employees’ survey responses on their job attitudes and perceptions without using objectively measured indicators, which seriously dampen the reliability and validity of the results. Perhaps the most serious concern is that a single-source (mono-method) bias is possible, as “self-reported” and “perceived” measures were used. Some authors contend, however, that mono-method bias is not as serious a problem as often assumed (<xref ref-type="bibr" rid="bibr25-0091026013484411">Park &amp; Rainey, 2008</xref>). While we note the limitation of each of these performance measurement tools, we believe that to some extent their scores can be compared with one another to find out where agencies converge in terms of performance.</p>
</sec>
<sec id="section4-0091026013484411" sec-type="methods">
<title>Research Framework and Method</title>
<p>Our study is exploratory. We have two goals: First, we wish to know which performance and effectiveness indicators are significant in clustering agency respondents; second, to know what the topography of agencies suggests about the assessment measures used in the federal government today, when the context of performance is examined. For the first goal, we use a cluster analysis to segment and classify U.S. federal agencies. Two research questions are proposed.</p>
<list id="list1-0091026013484411" list-type="simple"><list-item><p><italic>Research Question 1:</italic> Will federal agencies be differently clustered and segmented based on their performance indicators?</p></list-item>
<list-item><p><italic>Research Question 2:</italic> If so, which among these different indicators in federal agencies will be most pronounced?</p></list-item></list>
<p><italic>Performance</italic> is one of two dimensions assessed here. Performance indicators usually encompass two primary dimensions such as “management” and “program,” but this study’s focus is on analyzing the former one, which measures the variables of “structure and process” as well as “the outcomes of these management systems and activities” (<xref ref-type="bibr" rid="bibr31-0091026013484411">Selden &amp; Sowa, 2004</xref>, p. 398).</p>
<p>The literature is replete with studies of agency performance. Apart from seeing the topography of performance, we also probe the topography of <italic>effectiveness</italic>, that is, how effectiveness values are distributed and clustered in agencies. Like performance indicators, organizational effectiveness is a multidimensional concept, especially in public organizations, which usually precedes performance even if performance scores do not always suggest effectiveness. Organizational effectiveness refers to outcome and output measures. Outputs are “the immediate results of organization activities” and outcomes are “measures of the extent that organizations attain their goals or ultimate purposes” (<xref ref-type="bibr" rid="bibr3-0091026013484411">Berman, 2006</xref>, p. 6). In the public sector, organizational and agency effectiveness includes such elements as (a) outputs, that is, quality and quantity of public services; (b) innovativeness in services and processes; and (c) responsiveness and accountability, including satisfaction of customers and citizens. Several organizational and socioeconomical factors such as financial and technological resources, task and job design, leadership and organizational culture, and task, mission, and public service motivation play pivotal roles to enhance the level of effectiveness (<xref ref-type="bibr" rid="bibr27-0091026013484411">Rainey &amp; Steinbauer, 1999</xref>). We believe that organizational performance rests in the hands of those who implement policy and carry out the agency mission. Many performance measures focus on outcome measures but an argument can be made that performance may be predicted from the input side, in which an agency becomes attractive to workers for the kind of management, benefits, work dynamics, and mission it offers. In this light, employee satisfaction with an agency might indicate organizational effectiveness.</p>
<p>For the second goal, we look at one contextual variable, that of the agency’s policy mission. Results from performance assessments never escape disagreement on interpretation whenever it is used by parties external to the agency, as consequences could show up in budgetary levels, scope of agency power, or scale of its programs. OMB had tried to allay agency fears of cutback based on PART results (<xref ref-type="bibr" rid="bibr7-0091026013484411">Daniels, 2002</xref>), but, as many have noted, dialogue on performance assessments should not simply involve how to change programs but to understand what kind of information is being generated and to understand the context of performance (<xref ref-type="bibr" rid="bibr21-0091026013484411">Moynihan, 2008</xref>). We are, therefore, interested in whether differences in the clustering of agencies according to those assessment measures are influenced by a contextual variable: the agency’s given policy missions. The work of <xref ref-type="bibr" rid="bibr19-0091026013484411">Theodore Lowi (1964)</xref> in classifying four major types of agency missions guides us in this task. Lowi originally identified four types of policy mission: (a) regulatory, (b) distributive, (c) redistributive, and (d) constituent agencies, which appear to be a catch-all category in the typology. How would federal agencies be clustered and segmented in terms of Lowi’s typology? <xref ref-type="table" rid="table2-0091026013484411">Table 2</xref> classifies the agencies under study using Lowi’s typology (<xref ref-type="bibr" rid="bibr19-0091026013484411">Lowi, 1964</xref>) based on an understanding of the main types of services the agencies provide.</p>
<table-wrap id="table2-0091026013484411" position="float">
<label>Table 2.</label>
<caption>
<p>Agencies in the Study Classified by Lowi’s Typology.</p>
</caption>
<graphic alternate-form-of="table2-0091026013484411" xlink:href="10.1177_0091026013484411-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Regulatory agencies</th>
<th align="center">Distributive agencies</th>
</tr>
</thead>
<tbody>
<tr>
<td>Formulate or Implement rules imposing obligations on individuals and providing sanction for nonconformance</td>
<td>Distribute tangible benefits and intangible ones (research outputs, information, distributive public goods, and insurance benefits)</td>
</tr>
<tr>
<td>Agencies in the sample</td>
<td>Agencies in the sample</td>
</tr>
<tr>
<td> Commerce, DOE, DOJ, DOL, Treasury (<italic>n</italic> = 5)</td>
<td> USDA, DOI, DOT, Army Corps (<italic>n</italic> = 4)</td>
</tr>
<tr>
<th align="center">Redistributive agencies</th>
<th align="center">Constituent agencies</th>
</tr>
<tr>
<td>Redistribute benefits to one group of people like poor and unemployed by taxing another group of people, such as rich and employed</td>
<td>Carry out a residual group of policies that do not fit other three: serving government in general or the nation as a whole</td>
</tr>
<tr>
<td>Agencies in the sample</td>
<td>DOD, DHS, State, VA, Agencies in the sample</td>
</tr>
<tr>
<td> Education, HHS, HUD, EPA, SBA, SSA (<italic>n</italic> = 6)</td>
<td> GSA, NASA, NSF, OPM, USAID, OMB, Smithsonian (<italic>n</italic> = 11)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0091026013484411"><p>Note: DOE = Department of Education; DOJ = Department of Justice; DOL = Department of Labor; USDA = U.S. Department of Agriculture; DOI = Department of Interior; DOT = Department of Transportation; DOD = Department of Defense; DHS = Department of Homeland Security; VA = Veterans Affairs; HHS = Health and Human Services; HUD = Housing and Urban Development; EPA = Environmental Protection Agency; SBA = Small Business Administration; SSA = Social Security Administration; GSA = General Services Administration; NASA = National Aeronautics and Space Administration; NSF = National Science Foundation; OPM = Office of Personnel Management; USAID = U.S. Agency For International Development; OMB = Office of Management and Budget.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>To generate the topography, we used data consisting of (a) the end-of-2007 (4th quarter) scores for each of the 26 agencies (heretofore referred to as PMC agencies) listed on OMB Scorecard, such as Department of Agriculture, Department of Labor, and OMB; (b) the PART “Effective” percentage scores for each of those PMC agencies for the same year; and (3) the results of the federal employee survey BPTW for the same year, including scores for strategic management, teamwork, leadership, performance culture, training and development, WLB management, and diversity management. The 2007 period was chosen as the Bush administration was winding down and the performance assessment programs were coming to a close.</p>
<p>With a cluster analysis program, <italic>ClustanGraphics</italic>, hierarchical and nonhierarchical cluster (i.e., K-means cluster analysis or quick cluster) analyses of the sample were performed. Because outliers can cause problems in cluster analyses, we first screened for these. There were no significant outliers. Hierarchical clustering was used to determine the appropriate number of clusters by examining a range of segments based on changes in the agglomeration schedule, icicle plot, dendrogram, and cluster membership.<sup><xref ref-type="fn" rid="fn2-0091026013484411">2</xref></sup> Factors were clustered using Ward’s method, and distance was calculated using squared Euclidean distance, the recommended distance measure when performing Ward’s method (<xref ref-type="bibr" rid="bibr13-0091026013484411">Hair, Anderson, Tatham, &amp; Black, 1998</xref>). After obtaining a cluster solution, K-means cluster analysis was used to indicate which indicators were significant in clustering federal agencies.<sup><xref ref-type="fn" rid="fn3-0091026013484411">3</xref></sup></p>
</sec>
<sec id="section5-0091026013484411" sec-type="results">
<title>Results and Discussion</title>
<p><xref ref-type="table" rid="table3-0091026013484411">Table 3</xref> shows the descriptive statistics of all the variables used in this study including (a) Scorecard scores of human capital management, competitive sourcing of commercial services, financial management, electronic government, and budget and performance integration; (b) PART effectiveness scores; and (c) BPTW scores. First, the result of Scorecard statistics show that human capital management has the highest score (<italic>M</italic> = 1.65) among the Scorecard variables. It suggests that most of federal agencies have succeeded in managing human capital compared with other management reform initiatives. Second, the mean percentage of PART effective scores is 21.04% (out of 100%), which implies that most of the federal agency programs were not evaluated as highly effective. Third, we found that teamwork is the highest one among BPTW scores (<italic>M</italic> = 71.148). The finding suggests that many federal employees are satisfied with teamwork relationships with coworkers, supervisors, and managers. Most variables of skewness or kurtosis are all between −2 &lt; s(k) &lt; 2, and we concluded that these variables are approximately normally distributed. Relative multivariate kurtosis (1. 654 &lt; 2.0) also indicates approximate multivariate normality.</p>
<table-wrap id="table3-0091026013484411" position="float">
<label>Table 3.</label>
<caption>
<p>Descriptive Statistics.</p>
</caption>
<graphic alternate-form-of="table3-0091026013484411" xlink:href="10.1177_0091026013484411-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Variables</th>
<th align="center">Valid <italic>N</italic> (federal agencies)</th>
<th align="center"><italic>M</italic> (<italic>SD</italic>)</th>
<th align="center">Minimum (lowest rating)</th>
<th align="center">Maximum (highest rating)</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">OMB scorecard variables (five categories)</td>
</tr>
<tr>
<td> Human capital management</td>
<td>26</td>
<td>1.65 (0.485)</td>
<td>1.00</td>
<td>2.00</td>
</tr>
<tr>
<td> Competitive sourcing of commercial services</td>
<td>26</td>
<td>1.11 (0.765)</td>
<td>0.00</td>
<td>2.00</td>
</tr>
<tr>
<td> Financial management</td>
<td>26</td>
<td>1.15 (0.924)</td>
<td>0.00</td>
<td>2.00</td>
</tr>
<tr>
<td> Electronic government</td>
<td>26</td>
<td>1.19 (0.567)</td>
<td>0.00</td>
<td>2.00</td>
</tr>
<tr>
<td> Budget and performance integration</td>
<td>26</td>
<td>1.53 (0.508)</td>
<td>1.00</td>
<td>2.00</td>
</tr>
<tr>
<td colspan="5">PART variable (%)</td>
</tr>
<tr>
<td> PART effective scores</td>
<td>23</td>
<td>21.04 (21.83)</td>
<td>0.00</td>
<td>100</td>
</tr>
<tr>
<td colspan="5">BPTW scores (%)</td>
</tr>
<tr>
<td> Strategic management</td>
<td>25</td>
<td>56.35 (0.496)</td>
<td>46.3</td>
<td>68.4</td>
</tr>
<tr>
<td> Teamwork</td>
<td>25</td>
<td>71.14 (3.88)</td>
<td>63.4</td>
<td>81.8</td>
</tr>
<tr>
<td> Leadership</td>
<td>25</td>
<td>50.898 (5.43)</td>
<td>40.2</td>
<td>62.4</td>
</tr>
<tr>
<td> Performance culture</td>
<td>25</td>
<td>46.26 (6.60)</td>
<td>32.7</td>
<td>60.6</td>
</tr>
<tr>
<td> Training and development</td>
<td>25</td>
<td>58.56 (5.95)</td>
<td>46.1</td>
<td>70.5</td>
</tr>
<tr>
<td> WLB management</td>
<td>25</td>
<td>60.94 (4.13)</td>
<td>51.8</td>
<td>67.9</td>
</tr>
<tr>
<td> Diversity management</td>
<td>25</td>
<td>58.81 (5.23)</td>
<td>49.5</td>
<td>70.3</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0091026013484411"><p>Note: OMB = Office of Management and Budget; PART = Performance Assessment Rating Tool; BPTW = Best Places to Work; WLB = work–life balance.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Two agency cluster models emerge from the data. The models identify how different types of performance and effectiveness values are distributed across the government or within the different agencies. The results of the agency clustering indicate that in Model 1, which we dub the <italic>Performance Topography</italic>, there are three different clusters of agencies in Scorecard and PART scores. In Model 2, which we dub the <italic>Effectiveness Topography</italic>, there are also three clusters in BPTW scores.</p>
<p>The Performance Topography (<xref ref-type="table" rid="table4-0091026013484411">Table 4</xref>) includes the following groups:</p>
<list id="list2-0091026013484411" list-type="bullet">
<list-item><p>Cluster 1—High Human Resource and Financial Management agencies—Commerce, Energy, Labor, Environmental Protection Agency, General Services Administration, Office of Personnel Management, Social Security Administration;</p></list-item>
<list-item><p>Cluster 2—High Contracting and E-Government agencies—Education, Health and Human Services, Justice, Interior, National Aeronautics Space Administration;</p></list-item>
<list-item><p>Cluster 3—High PART Agencies—Defense, State, Treasury, Smithsonian Institution.</p></list-item>
</list>
<table-wrap id="table4-0091026013484411" position="float">
<label>Table 4.</label>
<caption>
<p>The Performance Configuration of Federal Agencies: K-Means Cluster Analysis Results for Scorecard and PART Scores.</p>
</caption>
<graphic alternate-form-of="table4-0091026013484411" xlink:href="10.1177_0091026013484411-table4.tif"/>
<table-wrap-foot>
<fn id="table-fn4-0091026013484411"><p>Note: PART = Performance Assessment Rating Tool; EPA = Environmental Protection Agency; GSA = General Services Administration; OPM = Office of Personnel Management; SSA = Social Security Administration; HHS = Health and Human Services; NASA = National Aeronautics and Space Administration; DOD = Department of Defense.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Cluster 1 is the largest in the <italic>Performance Topography</italic>, composed of 7 agencies out of the 26. It suggests that agencies focused more on two of the five components of the PMA: human capital management and financial management. It has been observed during the Bush administration these two components did not generate as much difficulty for agencies to pursue, compared with the competitive sourcing (A-76) agenda of the president, which attracted so much political debate (<xref ref-type="bibr" rid="bibr15-0091026013484411">Joaquin, 2009</xref>). It is interesting to note, looking at <xref ref-type="table" rid="table4-0091026013484411">Table 4</xref>, that the cluster of agencies with high PART scores was the smallest group, suggesting that it was more difficult to obtain high performance scores thru PART than the OMB Scorecard. This may be due to PART’s more detailed evaluation criteria and the fact that it evaluated programs rather than agency performance overall.</p>
<p>The <italic>Effectiveness Topography</italic> (<xref ref-type="table" rid="table5-0091026013484411">Table 5</xref>) includes the following groups:</p>
<list id="list3-0091026013484411" list-type="bullet">
<list-item><p>Cluster 1—Performance-Oriented agencies—Commerce, Environmental Protection Agency, State, National Aeronautics Space Administration, National Science Foundation, OMB;</p></list-item>
<list-item><p>Cluster 2—WLB-Oriented agencies—Army Corps of Engineers, Defense, Education, Energy, Health and Human Services, Justice, Treasury, Social Security Administration;</p></list-item>
<list-item><p>Cluster 3—Teamwork and Leadership–Oriented PART agencies—Justice, General Services Administration, Labor, Treasury, U.S. Agency for International Development, Energy.</p></list-item></list>
<table-wrap id="table5-0091026013484411" position="float">
<label>Table 5.</label>
<caption>
<p>The Effectiveness Configuration of Federal Agencies: K-Means Cluster Analysis Results for BPTW Scores.</p>
</caption>
<graphic alternate-form-of="table5-0091026013484411" xlink:href="10.1177_0091026013484411-table5.tif"/>
<table-wrap-foot>
<fn id="table-fn5-0091026013484411"><p>Note: BPTW = Best Places to Work; WLB = work–life balance; EPA = Environmental Protection Agency; NASA = National Aeronautics and Space Administration; NSF = National Science Foundation; OMB = Office of Management and Budget; DOD = Department of Defense; HHS = Health and Human Services; SSA = Social Security Administration; GSA = General Services Administration; USAID = U.S. Agency For International Development.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The <italic>Effectiveness Topography</italic> based on the seven managerial criteria (strategic management, teamwork, leadership, performance culture, training and development, WLB management, and diversity management) shows Cluster 2 as the largest, with high BPTW scores on WLB management. This WLB value-oriented group is composed of eight agencies, indicating that agencies pursuing the WLB managerial values and systems are more largely distributed in the federal bureaucracy than agencies driven by other effectiveness values. This cluster has the highest agency mean score of 60.94. The second largest cluster, performance management–oriented agencies, includes six federal agencies. The <italic>Effectiveness Topography</italic> shows that both of WLB and Performance Management values are the most prevalent and extensive values in the federal bureaucracy.</p>
</sec>
<sec id="section6-0091026013484411">
<title>Policy Mission as the Context of Performance and Effectiveness</title>
<p>Based on the observations above, we wish to speculate whether the agencies’ policy mission suggests anything about the performance and effectiveness values prevailing in the federal bureaucracy, as suggested by the assessment and survey scores. Typologies of policies have figured in numerous studies since Lowi came up with scheme, all of them finding some application for it, if not completely accurate. Lowi had argued that pure policy types predominate more than mixed types, as <xref ref-type="bibr" rid="bibr33-0091026013484411">Spitzer (1983)</xref> confirmed empirically. However, others thought the typology itself was inadequate. <xref ref-type="bibr" rid="bibr30-0091026013484411">Sanders (1990)</xref> thought it would have been better had Lowi generated a set of testable hypotheses for researchers using his scheme. <xref ref-type="bibr" rid="bibr14-0091026013484411">Heckathorn and Maser (1990)</xref> found that Lowi’s scheme fit some but not all policies well when they tried to explain different types of government intervention to solve collective problems.</p>
<p>Typologies have their own weaknesses where performance information is concerned. Another typology is also popular among researchers who link agency attributes to performance. <xref ref-type="bibr" rid="bibr37-0091026013484411">James Wilson’s (1989)</xref> typology distinguished agencies regarding the extent to which their outputs and outcomes could be observed. A recent article used Wilson’s concept to examine how PART and the GPRA of 1993 differed in treating agencies with varying degrees of observability in outputs and outcomes (<xref ref-type="bibr" rid="bibr12-0091026013484411">Gueorguieva et al., 2009</xref>). Although not delineating by policy typologies, Wilson’s classification of agencies as far as performance observability is concerned points to the challenges of performance and effectiveness evaluation in the federal bureaucracy.</p>
<p>The same may be said of Lowi’s typology. His typology does not completely eliminate the problem of goal ambiguity in federal agencies. Radin recognized the usefulness of Lowi’s typology but observed that most government programs use multiple types of approaches in policy implementation such that information on outputs or outcomes for performance measurement may not always be easy to agree upon (<xref ref-type="bibr" rid="bibr26-0091026013484411">Radin, 2006</xref>). Our preferred scheme, Lowi’s, also entails some kind of specification in generating performance data. Radin noted that the Lowi’s typology creates particular demands on information used in performance measurement (<xref ref-type="bibr" rid="bibr26-0091026013484411">Radin, 2006</xref>). For example, redistributive policies, by virtue of creating clear winners and losers in the process, entail a scenario wherein each camp could present information about the effectiveness or ineffectiveness of policy. Thus, a good score for a redistributive agency would indicate its policy effectiveness. Regulatory policies often must show not only that compliance is achieved but at a minimal cost. A good score therefore says that the agency minimized costs in enforcing rules. Distributive policies appear to create only winners, and therefore expected performance information in this area should indicate that funds would continue or are limitless. Thus, distributive agencies, for a good score, would have to be unencumbered in funding. In all these arenas, Lowi claimed that politics is a function of policy as much as the reverse, different policies engender different political dynamics, and consequences and performance information are central to this political dynamics (<xref ref-type="bibr" rid="bibr9-0091026013484411">Ginsberg &amp; Sanders, 1990</xref>).</p>
<p>On top of this, one would have to consider when evaluation takes place. Is it under a conservative or a liberal regime? Our study tries to see whether any of these assumptions using Lowi’s typology might be borne by the results of the cluster analysis. Because the score came under a Republican presidency, we speculate whether the clusters indicate a relationship between policy typology and the assessment tools of a conservative administration. In <xref ref-type="table" rid="table6-0091026013484411">Table 6</xref>, we see a synthesis of the clusters in <xref ref-type="table" rid="table4-0091026013484411">Tables 4</xref> and <xref ref-type="table" rid="table5-0091026013484411">5</xref> and segmented according to policy types, showing the agencies clustering under the <italic>Performance Topography</italic> and the <italic>Effectiveness Topography</italic> models. In terms of policy typology, the ones that generated more observation under <italic>both</italic> models evidently were <italic>constituent and regulatory</italic> types, while <italic>distributive</italic> agencies had the least number of observations.</p>
<table-wrap id="table6-0091026013484411" position="float">
<label>Table 6.</label>
<caption>
<p>Agency Clusters Classified by Policy Typology.</p>
</caption>
<graphic alternate-form-of="table6-0091026013484411" xlink:href="10.1177_0091026013484411-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">Model 1: Performance topography clusters<hr/></th>
<th align="center" colspan="3">Model 2: Effectiveness topography clusters<hr/></th>
</tr>
<tr>
<th/>
<th align="center">HR/ Finance</th>
<th align="center">E-government/ A-76</th>
<th align="center">PART</th>
<th align="center">Performance management</th>
<th align="center">WLB</th>
<th align="center">Team/Leadership</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Distributive agencies</td>
</tr>
<tr>
<td> Army Corps</td>
<td/>
<td/>
<td/>
<td/>
<td>❖</td>
<td/>
</tr>
<tr>
<td> Interior</td>
<td/>
<td>❖</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> USDA</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Transportation</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td colspan="7">Redistributive agencies</td>
</tr>
<tr>
<td> SSA</td>
<td>❖</td>
<td/>
<td/>
<td/>
<td>❖</td>
<td/>
</tr>
<tr>
<td> EPA</td>
<td>❖</td>
<td/>
<td/>
<td>❖</td>
<td/>
<td/>
</tr>
<tr>
<td> Education</td>
<td/>
<td>❖</td>
<td/>
<td/>
<td>❖</td>
<td/>
</tr>
<tr>
<td> HHS</td>
<td/>
<td>❖</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> SBA</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> HUD</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td colspan="7">Regulatory agencies</td>
</tr>
<tr>
<td> Energy</td>
<td>❖</td>
<td/>
<td/>
<td/>
<td>❖</td>
<td>❖</td>
</tr>
<tr>
<td> Justice</td>
<td/>
<td>❖</td>
<td/>
<td/>
<td>❖</td>
<td>❖</td>
</tr>
<tr>
<td> Labor</td>
<td>❖</td>
<td/>
<td/>
<td/>
<td/>
<td>❖</td>
</tr>
<tr>
<td> Commerce</td>
<td>❖</td>
<td/>
<td/>
<td>❖</td>
<td/>
<td/>
</tr>
<tr>
<td> Treasury</td>
<td/>
<td/>
<td>❖</td>
<td/>
<td>❖</td>
<td>❖</td>
</tr>
<tr>
<td colspan="7">Constituent agencies</td>
</tr>
<tr>
<td> GSA</td>
<td>❖</td>
<td/>
<td/>
<td/>
<td/>
<td>❖</td>
</tr>
<tr>
<td> NASA</td>
<td/>
<td>❖</td>
<td/>
<td>❖</td>
<td/>
<td/>
</tr>
<tr>
<td> OMB</td>
<td/>
<td/>
<td/>
<td>❖</td>
<td/>
<td/>
</tr>
<tr>
<td> State</td>
<td/>
<td/>
<td>❖</td>
<td>❖</td>
<td/>
<td/>
</tr>
<tr>
<td> DOD</td>
<td/>
<td/>
<td>❖</td>
<td/>
<td>❖</td>
<td/>
</tr>
<tr>
<td> NSF</td>
<td/>
<td/>
<td/>
<td>❖</td>
<td/>
<td/>
</tr>
<tr>
<td> OPM</td>
<td>❖</td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Smithsonian</td>
<td/>
<td/>
<td>❖</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> USAID</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>❖</td>
</tr>
<tr>
<td> VA</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> DHS</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td><italic>n</italic> = 7</td>
<td><italic>n</italic> = 5</td>
<td><italic>n</italic> = 4</td>
<td><italic>n</italic> = 6</td>
<td><italic>n</italic> = 7</td>
<td><italic>n</italic> = 6</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0091026013484411"><p>Note: PART = Performance Assessment Rating Tool; WLB = work–life balance; USDA = U.S. Department of Agriculture; SSA = Social Security Administration; EPA = Environmental Protection Agency; HHS = Health and Human Services; SBA = Small Business Administration; HUD = Housing and Urban Development; GSA = General Services Administration; NASA = National Aeronautics and Space Administration; OMB = Office of Management and Budget; DOD = Department of Defense; NSF = National Science Foundation; OPM = Office of Personnel Management; USAID = U.S. Agency For International Development; VA = Veterans Affairs; DHS = Department of Homeland Security.</p></fn>
</table-wrap-foot></table-wrap>
<p>Focusing on the <italic>Performance Topography</italic>, the first three columns (Model 1), constituent agencies had the biggest presence, followed closely by regulatory agencies. Because constituent agencies dominated the pool of agencies analyzed in this study (11 out of 26), this is not surprising, but this leads us to some intriguing questions where regulatory types (5 out of 26) are concerned. Does this indicate that for PART and the OMB Scorecard, regulatory agencies possessed attributes that made them more suited to performance assessment criteria obtained under a Republican regime? Does this also suggest that distributive-type agencies might have had a difficult time satisfying performance assessment criteria during the Bush administration? The two types appear at opposite ends of a continuum: Distributive agencies are almost the opposite of regulatory agencies in mission, and distributive agencies, by reputation, strongly resist change (that may be due to a management agenda of a new administration, for example) because of their established relationship with their clientele (<xref ref-type="bibr" rid="bibr20-0091026013484411">Lowi, 1981</xref>; <xref ref-type="bibr" rid="bibr23-0091026013484411">Newman, 1994</xref>). Could regulatory agencies be more amenable to changing political leadership mandates and therefore obtain better assessment scores? This may in part rest in the fact that during these years fiscal constraints hampered the growth of distributive agencies. As our study focuses on agency performance during a Republican presidency, we could speculate that distributive agencies might have had a harder time satisfying the performance criteria under the PMA, or Scorecard and PART, due to fiscal stress. As performance scholar Beryl Radin noted, the demand on distributive agencies is for performance information that shows limitless or continued funding, and conservative fiscal policies could preempt that. Distributive agencies might have had smaller chances of meeting Republican White House performance criteria than other agencies. In an earlier study, <xref ref-type="bibr" rid="bibr8-0091026013484411">Gilmour and Lewis (2006)</xref> noted that OMB appeared to have used performance data generated from PART to justify cuts to liberal agencies/programs, but not conservative ones.</p>
<p>In the same manner that Democratic administrations might focus on the activities of distributive agencies, regulatory agencies, or even redistributive agencies were a target for policy control under the Bush administration. It can be argued that political responsiveness to the White House by regulatory agencies was especially sought during the last several years, and so these agencies might have been more aggressive than others to meet performance standards and please the White House, in one way or another. A preliminary study by <xref ref-type="bibr" rid="bibr11-0091026013484411">Greitens and Joaquin (2008)</xref> might be revealing that kind of pressure on regulatory agencies. They found that PART scores appeared to be systematically lower for regulatory agencies that did not comply with the most controversial component of the PMA, the competitive sourcing initiative, based on the Scorecard. Compliance with the presidential priorities, in effect, might be more highly visible and rewarded in PART. The table confirms this, that regulatory agencies’ PART scores were not significant at all, and that their presence is more noticeable when performance in human capital, finance, and electronic government is assessed. These are the scores that made the regulatory types noticeable in Performance Topography.</p>
<p>A third explanation for regulatory agencies achieving slightly larger clustering in performance is something our data and analysis cannot truly support, that is, to keep regulatory agencies from becoming better at what they do, and so grading their performance not too highly, but favorably enough. When the goal is to improve weak performance, budgets may be increased; but a desire to limit an agency’s regulatory muscle over the regulated entities necessitates keeping the agency budget from going up, which a good enough performance score can do. This political aspect to performance measurement is visible whenever vested interests attack the performance of those agencies responsible for oversight of social and economic activities (<xref ref-type="bibr" rid="bibr34-0091026013484411">Stanton, 2008</xref>).</p>
</sec>
<sec id="section7-0091026013484411">
<title>External Versus Internal, Top-Down Versus Bottom-Up Perspectives</title>
<p>The results allow us to check if the agencies that clustered in terms of performance also clustered in terms of the values that employees perceived to be important according to the BPTW survey (Model 2). <xref ref-type="table" rid="table6-0091026013484411">Table 6</xref> shows that regulatory agencies populated the <italic>Effectiveness Topography</italic> followed closely by constituent agency types. Centralization, formal structures, and selective hiring policies characteristic of regulatory agencies might be fostering a closer, more tightly knit organization conducive to the performance values, teamwork and leadership, WLB that employees perceived to be the BPTW.</p>
<p>On the other hand, <xref ref-type="table" rid="table6-0091026013484411">Table 6</xref> shows that distributive agencies were almost nonexistent under the second model. Newman’s test (1994) of Lowi’s typology found that the work environment of each agency type breeds distinctive managerial and organizational opportunities for employee experiences, advancement, and “fit” with the agency (<xref ref-type="bibr" rid="bibr23-0091026013484411">Newman, 1994</xref>). Newman’s study did not indicate that regulatory, more than distributive, agency types would be “better” places to work, but her study cements a distinction between regulatory and distributive agencies. Regulatory agencies are more rule-bound, formal, characterized by lateral-entry, overhead controls, and discriminating in hiring and promotion more than others. On the other hand, distributive agencies are conducive to collegial, informal networks, have flat, decentralized organizational structures that encourage specialization, long service to the agency and interpersonal ties among employees. Does this indicate that formal, hierarchical structures with strong control are simply easier to evaluate in the sense that measures are tight and easily ascertained? And that, overall, lower scores on effectiveness or performance only suggest the difficulty of measuring performance, not real ineffectiveness or failure of performance?</p>
</sec>
<sec id="section8-0091026013484411" sec-type="conclusions">
<title>Conclusions and Implications</title>
<p>At the close of the Bush era, some believed the administration’s performance assessment efforts significantly improved the way federal bureaucracy worked (<xref ref-type="bibr" rid="bibr5-0091026013484411">Breul, 2007</xref>). The 2008 change in the presidency focused our attention to the systematic use of scorecards in evaluating organizational efficiency and effectiveness. Scorecards or report cards are supposedly powerful instruments in monitoring organizational performance, whether they include ranking of agencies, benchmarking, scorekeeping, or the balanced scorecard (<xref ref-type="bibr" rid="bibr10-0091026013484411">Gormley, 1999</xref>). With the mounting obstacles to effective government today one author recommends building on the current tools of performance management like the GPRA, PART, and the Scorecard (<xref ref-type="bibr" rid="bibr34-0091026013484411">Stanton, 2008</xref>) to help prioritize addressing agency management weaknesses. Other authors note that the periodic public reports used in Clinton’s NPR and the Bush administration PMA helped to keep agencies focused on new priorities and should therefore be maintained (<xref ref-type="bibr" rid="bibr6-0091026013484411">Breul &amp; Kamensky, 2008</xref>).</p>
<p>Before embarking on new assessment systems, we need to understand as much as we can from recent agency experiences with performance assessments and their results. The results of this study indicate certain weaknesses in current systems but the results should be regarded with some caution due to our limited data and statistical methodology. Further studies need to be done with a more robust data set and theoretical framework. We see our attempt as primarily exploratory, and we offer two conclusions.</p>
<sec id="section9-0091026013484411">
<title>On Policy Typology and Performance</title>
<p>In this study, scores from two performance assessment measures were juxtaposed with the survey results of what employees regard as best agencies for employment. The initial results seem to show that agencies rated as better performers and better agencies to work for are regulatory and constituent agencies. As the new administration charts a new course in various military, economic and political fronts, performance management might not get the same attention it has received in recent decades. However, some of the arguments made here might become relevant to the extent that as of this writing the bureaucracy is about to undergo some major redirection with the intended overhaul of financial industry regulation, health care financing, and the massive stimulus spending. Again, the policy missions, handed down to agencies by design, might influence how agencies are perceived by the top and external actors, and the bottom and internal organizational members.</p>
</sec>
<sec id="section10-0091026013484411">
<title>On Top and Bottom Perspectives of Performance and Effectiveness</title>
<p>Measuring performance will remain a staple of management regardless of how the instrument is designed and implemented. Searching for a “one best way” to measure, however, may be futile (<xref ref-type="bibr" rid="bibr2-0091026013484411">Behn, 2003</xref>). Rather, a collection of performance measures may be more useful to help agencies that are established to achieve essentially different purposes. Combining the best analytical tools from the government and private sectors and incorporating top-down and bottom-up perspectives in performance measurement should be a step in the right direction but may be difficult to design and implement. The PART and Scorecard were designed for accountability to the Executive and to external stakeholders while the BPTW employee survey gauged an agency from the inside and below the bottom of the hierarchy. As multidimensional organizations, agency performance and effectiveness could be viewed from different angles. How to make policy and program decisions based on potentially varying results of those analyses is the real challenge of political management.</p>
<p>This study did not look closely at a fundamental aspect of agency performance, strategic management, leadership and teamwork. Future studies could examine their impact. Did leader-driven performance metrics from PART and OMB Scorecard have some interaction with employee-based assessment of organizational effectiveness? When some agencies started asking OMB for flexible scoring criteria to take their uniqueness into consideration in grading them on the Scorecard, OMB’s Deputy Director replied publicly that they at OMB were “controlling what the definition of success is, but shame on (them) if that’s a bad idea” (<xref ref-type="bibr" rid="bibr32-0091026013484411">Singer, 2005</xref>, p. 4). This demonstrates how control emanated from the top, and political appointees were made to communicate presidential priorities or risk Cabinet-level discussion of their failing marks (<xref ref-type="bibr" rid="bibr16-0091026013484411">Johnson, 2003</xref>). A study is needed to see how political appointments influenced scores for PART and the Scorecard. How much did these leaders value the direction from above, compared with their opinions of employees’ views? An internal assessment like BPTW, in contrast, relies on perceptions from below using a survey instrument, which fosters honest opinions of performance that some scholars have found to be important (e.g., <xref ref-type="bibr" rid="bibr24-0091026013484411">Park, 2010</xref>; <xref ref-type="bibr" rid="bibr38-0091026013484411">Yang &amp; Holzer, 2006</xref>). The limited glimpse we have from these three performance assessment measures provides some evidence that performance measures from above, to which leaders would pay attention, do align to some extent with employee perceptions of organizational effectiveness, as the consistently high presence of constituent and regulatory agencies in both models showed.</p>
<p>What could managers take away from this study? External and internal stakeholders command different views of the agency, so that measuring what is “good performance” or “best place to work for” becomes relative to the beholder. An agency that scores well at implementing a presidential initiative might not be the best employer in the eyes of workers. An agency that satisfies its workers might even get a poor score in human resource management when evaluated by the White House. At best, performance evaluation and satisfaction surveys can identify the top performers and the lagging ones, but for most agencies that fall in between, there is no one “best” way of characterizing how effective an agency is, overall, compared with other agencies. Using multiple assessment tools can limit the influence of any single design or implementation variable while enhancing the chances of getting a more complete picture of agency performance and effectiveness.</p>
</sec></sec>
</body>
<back>
<ack><p>We thank the four anonymous reviewers of <italic>Public Personnel Management</italic> for their suggestions and comments.</p></ack>
<fn-group>
<fn fn-type="other">
<label>Authors’ Note</label>
<p>An earlier version of this manuscript was presented at the Midwest Political Science Association (MPSA) 67th Annual Conference, Chicago, Illinois, held April 2 to 5, 2009.</p></fn>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by the National Research Foundation of Korea Grant funded by the Korean Government (NRF-2010-330-B00254).</p></fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0091026013484411">
<label>1.</label>
<p>Under the rubric of “building a high-performing government,” the Obama’s administration is currently developing a new management and performance agenda (called Obama’s Management Agenda), which overhauls the former President’s Management Agenda (PMA) initiatives. The guiding themes include (a) replacing Performance Assessment Rating Tool (PART) with a new performance improvement and analysis framework, (b) ensuring responsible spending of recovery act funds, (c) transforming the federal workforce, (d) managing across sectors, (e) reforming federal contracting and acquisition, (f) transparency, technology, and participatory democracy. For the detailed and updated information, please refer to Office of Management and Budget (OMB’s) Analytical Perspective for FY2010 (<ext-link ext-link-type="uri" xlink:href="http://www.whitehouse.gov/omb/budget/Analytical_Perspectives/">http://www.whitehouse.gov/omb/budget/Analytical_Perspectives/</ext-link>).</p></fn>
<fn fn-type="other" id="fn2-0091026013484411">
<label>2.</label>
<p>In agglomerative clustering analysis, once a cluster is made, it cannot be split; it can only be combined with other clusters. <italic>Agglomerative hierarchical clustering</italic> does not allow cases to separate from previous clusters that they have joined. On the other hand, divisive clustering analysis starts with every case in one cluster and ends up with the cases in independent clusters. <italic>Icicle plot</italic> shows what is happening at each step of the cluster analysis when average linkage between groups is used to link the clusters. Each column represents one of the objects which are being clustered. Each row indicates a cluster solution with different numbers of clusters. <italic>Dendrogram</italic> visually represents the distance at which clusters are combined; while vertical lines show joined clusters, the position of the line on the scale shows the distance at which clusters are joined.</p></fn>
<fn fn-type="other" id="fn3-0091026013484411">
<label>3.</label>
<p>In <italic>Ward’s method</italic>, the means for all variables are calculated for each cluster. Then, for each case, the squared Euclidean distance to the cluster means is used. These distances are summed and calculated for all of the cases included in the analysis. At each step, the two clusters that merge are those that result in the smallest increase in the overall sum of the squared within-cluster distances.</p></fn>
</fn-group>
</notes>
<bio>
<title>Author Biographies</title>
<p><bold>M. Ernita Joaquin</bold> is an assistant professor of public administration at San Francisco State University. Her works on public management reform, bureaucratic adaptation, and local government capacity have appeared in Public Administration Review, Administration &amp; Society, Public Performance &amp; Management Review, and The American Review of Public Administration.</p>
<p><bold>Sung Min Park</bold> is an associate professor of public administration and graduate school of governance at Sungkyunkwan University. His primary research interests are public management, public human resources management, organizational behavior, and quantitative research methods. His works have appeared in The American Review of Public Administration, Review of Public Personnel Administration, International Public Management Journal, International Journal of Human Resource Management, Personnel Review, and Public Management Review.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Barzelay</surname><given-names>M.</given-names></name>
</person-group> (<year>1992</year>). <source>Breaking through bureaucracy: A new vision for managing in government</source>. <publisher-loc>Berkeley</publisher-loc>: <publisher-name>University of California Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Behn</surname><given-names>R. D.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Why measure performance? Different purposes require different measures</article-title>. <source>Public Administration Review</source>, <volume>63</volume>, <fpage>587</fpage>-<lpage>606</lpage>.</citation>
</ref>
<ref id="bibr3-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Berman</surname><given-names>E. M.</given-names></name>
</person-group> (<year>2006</year>). <source>Performance and productivity in public and nonprofit organizations</source>. <publisher-loc>Armonk, NY</publisher-loc>: <publisher-name>M.E. Sharpe</publisher-name>.</citation>
</ref>
<ref id="bibr4-0091026013484411">
<citation citation-type="web"><collab>The Best Places to Work</collab> (<year>2007</year>). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://bestplacestowork.org/BPTW/about/">http://bestplacestowork.org/BPTW/about/</ext-link></citation>
</ref>
<ref id="bibr5-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Breul</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Three Bush administration management reform initiatives: The President’s management agenda, freedom to manage legislative proposals, and the Program Assessment Rating Tool</article-title>. <source>Public Administration Review</source>, <volume>67</volume>, <fpage>21</fpage>-<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr6-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Breul</surname><given-names>J. D.</given-names></name>
<name><surname>Kamensky</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Federal government reform: Lessons from Clinton’s “reinventing government” and Bush’s “management agenda” initiatives</article-title>. <source>Public Administration Review</source>, <volume>68</volume>, <fpage>1009</fpage>-<lpage>1026</lpage>.</citation>
</ref>
<ref id="bibr7-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Daniels</surname><given-names>M.</given-names></name>
</person-group> (<year>2002</year>, <month>July</month>). <source>Memorandum for heads of executive departments and agencies: Program performance assessments for the FY 2004 budget</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Executive Office of the President</publisher-name>.</citation>
</ref>
<ref id="bibr8-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gilmour</surname><given-names>J. B.</given-names></name>
<name><surname>Lewis</surname><given-names>D. E.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Does performance budgeting work? An examination of the office of management and budget’s PART scores</article-title>. <source>Public Administration Review</source>, <volume>66</volume>, <fpage>742</fpage>-<lpage>752</lpage>.</citation>
</ref>
<ref id="bibr9-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ginsberg</surname><given-names>B.</given-names></name>
<name><surname>Sanders</surname><given-names>E.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Theodore J. Lowi and juridical democracy</article-title>. <source>PS: Political Science &amp; Politics</source>, <volume>23</volume>, <fpage>563</fpage>-<lpage>566</lpage>.</citation>
</ref>
<ref id="bibr10-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gormley</surname><given-names>W. T.</given-names></name>
</person-group> (<year>1999</year>). <source>Organizational report cards</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0091026013484411">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Greitens</surname><given-names>T.</given-names></name>
<name><surname>Joaquin</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2008</year>, <month>April</month>). <source>Determinants of program performance: Results from OMB’s PART analysis</source>. <conf-name>Paper presented at the Midwest Political Science Association Annual Conference</conf-name>, <conf-loc>Chicago, IL</conf-loc>.</citation>
</ref>
<ref id="bibr12-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gueorguieva</surname><given-names>V.</given-names></name>
<name><surname>Accius</surname><given-names>J.</given-names></name>
<name><surname>Apaza</surname><given-names>C.</given-names></name>
<name><surname>Bennett</surname><given-names>L.</given-names></name>
<name><surname>Brownley</surname><given-names>C.</given-names></name>
<name><surname>Cronin</surname><given-names>S.</given-names></name>
<name><surname>Preechyanud</surname><given-names>P.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The Program Assessment Rating Tool and the Government Performance and Results Act: Evaluating conflicts and disconnections</article-title>, <source>American Review of Public Administration</source>, <volume>39</volume>, <fpage>225</fpage>-<lpage>245</lpage>.</citation>
</ref>
<ref id="bibr13-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hair</surname><given-names>J. E.</given-names></name>
<name><surname>Anderson</surname><given-names>R. E.</given-names></name>
<name><surname>Tatham</surname><given-names>R. L.</given-names></name>
<name><surname>Black</surname><given-names>W.C.</given-names></name>
</person-group> (<year>1998</year>). <source>Multivariate data analysis</source> (<edition>5th ed.</edition>). <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr14-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Heckathorn</surname><given-names>D. D.</given-names></name>
<name><surname>Maser</surname><given-names>S. M.</given-names></name>
</person-group> (<year>1990</year>). <article-title>The contractual architecture of public policy: A critical reconstruction of Lowi’s typology</article-title>. <source>Journal of Politics</source>, <volume>52</volume>, <fpage>1101</fpage>-<lpage>1123</lpage></citation>
</ref>
<ref id="bibr15-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Joaquin</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Bureaucratic adaptation and the politics of multiple principals in policy implementation</article-title>. <source>American Review of Public Administration</source>, <volume>39</volume>, <fpage>246</fpage>-<lpage>268</lpage>.</citation>
</ref>
<ref id="bibr16-0091026013484411">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>C.</given-names></name>
</person-group> (<year>2003</year>). <source>Ask the White House</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Office of Management and Budget</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://georgewbush-whitehouse.archives.gov/ask/print/20030722.html">http://georgewbush-whitehouse.archives.gov/ask/print/20030722.html</ext-link></citation>
</ref>
<ref id="bibr17-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kaplan</surname><given-names>R.</given-names></name>
<name><surname>Norton</surname><given-names>D.</given-names></name>
</person-group> (<year>1996</year>). <source>The balanced scorecard</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Harvard Business School</publisher-name>.</citation>
</ref>
<ref id="bibr18-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kerr</surname><given-names>D. L.</given-names></name>
</person-group> (<year>2002</year>). <article-title>The balanced scorecard in the public sector</article-title>. <source>Performance Magazine</source>, <volume>1</volume>(<issue>8</issue>), <fpage>4</fpage>-<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr19-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lowi</surname><given-names>T.</given-names></name>
</person-group> (<year>1964</year>). <article-title>American business, public policy, case-studies, and political theory</article-title>. <source>World Politics</source>, <volume>16</volume>, <fpage>677</fpage>-<lpage>715</lpage>.</citation>
</ref>
<ref id="bibr20-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lowi</surname><given-names>T.</given-names></name>
</person-group> (<year>1981</year>). <source>Incomplete conquest: Governing America</source> (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Holt, Rinehart &amp; Winston</publisher-name>.</citation>
</ref>
<ref id="bibr21-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Moynihan</surname><given-names>D. P.</given-names></name>
</person-group> (<year>2008</year>). <source>The dynamics of performance management: Constructing information and reform</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Georgetown University Press</publisher-name>.</citation>
</ref>
<ref id="bibr22-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nathan</surname><given-names>R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Presidential address: Complexifying government oversight in America’s government</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>4</volume>, <fpage>207</fpage>-<lpage>215</lpage>.</citation>
</ref>
<ref id="bibr23-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Newman</surname><given-names>M.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Gender and Lowi’s thesis: Implications for career advancement</article-title>. <source>Public Administration Review</source>, <volume>54</volume>, <fpage>277</fpage>-<lpage>284</lpage>.</citation>
</ref>
<ref id="bibr24-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Park</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The effects of personnel reform systems on Georgia State employees’ attitudes: An empirical analysis from a principal-agent theoretical perspective</article-title>. <source>Public Management Review</source>, <volume>12</volume>, <fpage>403</fpage>-<lpage>437</lpage>.</citation>
</ref>
<ref id="bibr25-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Park</surname><given-names>S. M.</given-names></name>
<name><surname>Rainey</surname><given-names>H. G.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Leadership and public service motivation in U.S. federal agencies</article-title>. <source>International Public Management Journal</source>, <volume>11</volume>, <fpage>109</fpage>-<lpage>142</lpage>.</citation>
</ref>
<ref id="bibr26-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Radin</surname><given-names>B.</given-names></name>
</person-group> (<year>2006</year>). <source>Challenging the performance movement: Accountability, complexity, and democratic values</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Georgetown University Press</publisher-name>.</citation>
</ref>
<ref id="bibr27-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rainey</surname><given-names>H. G.</given-names></name>
<name><surname>Steinbauer</surname><given-names>P.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Galloping elephants: Developing elements of a theory of effective government organizations</article-title>. <source>Journal of Public Administration Research and Theory</source>, <volume>1</volume>, <fpage>1</fpage>-<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr28-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rosenbloom</surname><given-names>D. H.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Reinventing administrative prescriptions: The case for democratic-constitutional impact statements and scorecards</article-title>. <source>Public Administration Review</source>, <volume>67</volume>, <fpage>1027</fpage>-<lpage>1036</lpage>.</citation>
</ref>
<ref id="bibr29-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rubin</surname><given-names>I. S.</given-names></name>
</person-group> (<year>1998</year>). <source>Class, tax, and power: Municipal budgeting in the United States</source>. <publisher-loc>Chatham, NJ</publisher-loc>: <publisher-name>Chatham House</publisher-name>.</citation>
</ref>
<ref id="bibr30-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sanders</surname><given-names>E.</given-names></name>
</person-group> (<year>1990</year>). <article-title>The contributions of Theodore Lowi to political analysis and democratic theory</article-title>. <source>PS: Political Science &amp; Politics</source>, <volume>23</volume>, <fpage>574</fpage>-<lpage>576</lpage>.</citation>
</ref>
<ref id="bibr31-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Selden</surname><given-names>S. C.</given-names></name>
<name><surname>Sowa</surname><given-names>J. E.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Testing a multi-dimensional model of organizational performance: Prospects and problems</article-title>. <source>Journal of Public Administration Research and Theory</source>, <volume>14</volume>, <fpage>395</fpage>-<lpage>416</lpage>.</citation>
</ref>
<ref id="bibr32-0091026013484411">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Singer</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Bush and the bureaucracy: A crusade for control</article-title>. <ext-link ext-link-type="uri" xlink:href="http://Govexec.com">Govexec.com</ext-link>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.govexec.com/dailyfed/0305/032505nj1.htm">http://www.govexec.com/dailyfed/0305/032505nj1.htm</ext-link></citation>
</ref>
<ref id="bibr33-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spitzer</surname><given-names>R.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Presidential policy determinism: How policies frame congressional responses to the president’s legislative program</article-title>. <source>Presidential Studies Quarterly</source>, <volume>13</volume>, <fpage>556</fpage>-<lpage>574</lpage>.</citation>
</ref>
<ref id="bibr34-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stanton</surname><given-names>T. H.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Improving the managerial capacity of the federal government: A public administration agenda for the next president</article-title>. <source>Public Administration Review</source>, <volume>68</volume>, <fpage>1027</fpage>-<lpage>1036</lpage>.</citation>
</ref>
<ref id="bibr35-0091026013484411">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Styles</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <source>The process isn’t pretty, But savings can be dramatic</source> (Updates on the president’s management agenda). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.whitehouse.gov/results/agenda/fiveinitatives02.html">http://www.whitehouse.gov/results/agenda/fiveinitatives02.html</ext-link></citation>
</ref>
<ref id="bibr36-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Whittaker</surname><given-names>J. B.</given-names></name>
</person-group> (<year>2003</year>). <source>President’s management agenda: A balanced scorecard approach</source>. <publisher-loc>Vienna, VA</publisher-loc>: <publisher-name>Management Concepts</publisher-name>.</citation>
</ref>
<ref id="bibr37-0091026013484411">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wilson</surname><given-names>J.</given-names></name>
</person-group> (<year>1989</year>). <source>Bureaucracy: What government agencies do and why they do it</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Basic Books</publisher-name>.</citation>
</ref>
<ref id="bibr38-0091026013484411">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yang</surname><given-names>K.</given-names></name>
<name><surname>Holzer</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The performance—Trust link: Implications for performance measurement</article-title>. <source>Public Administration Review</source>, <volume>66</volume>, <fpage>114</fpage>-<lpage>126</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>