<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JIS</journal-id>
<journal-id journal-id-type="hwp">spjis</journal-id>
<journal-title>Journal of Information Science</journal-title>
<issn pub-type="ppub">0165-5515</issn>
<issn pub-type="epub">1741-6485</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0165551511429418</article-id>
<article-id pub-id-type="publisher-id">10.1177_0165551511429418</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Ocropodium: open source OCR for small-scale historical archives</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Blanke</surname><given-names>Tobias</given-names></name>
</contrib>
<aff id="aff1-0165551511429418">King’s College London, UK</aff>
</contrib-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Bryant</surname><given-names>Michael</given-names></name>
</contrib>
<aff id="aff2-0165551511429418">King’s College London, UK</aff>
</contrib-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Hedges</surname><given-names>Mark</given-names></name>
</contrib>
<aff id="aff3-0165551511429418">King’s College London, UK</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0165551511429418">Tobias Blanke, Centre for e-Research, King’s College London, 26–29 Drury Lane, London WC2B 5RL, UK. Email: <email>tobias.blanke@kcl.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>38</volume>
<issue>1</issue>
<fpage>76</fpage>
<lpage>86</lpage>
<permissions>
<copyright-statement>© Chartered Institute of Library and Information Professionals 2011</copyright-statement>
<copyright-year>2011</copyright-year>
<copyright-holder content-type="sage">Chartered Institute of Library and Information Professionals</copyright-holder>
</permissions>
<abstract>
<p>Large-scale digitization projects dealing with text-based historical material face challenges that are not well catered for by commercial software. This article discusses the results of a project to build a scalable OCR workflow for historical collections based on open source tools that is particularly tailored towards use in small-scale historical archives. It argues that open source tools allow for better customization to match these requirements, particularly with regard to character model training and per-project language modelling. We offer insights into our accuracy evaluation results of various open source OCR tools, as well as a case study about the challenges and opportunities of open source OCR in historical archives.</p>
</abstract>
<kwd-group>
<kwd>historical archives</kwd>
<kwd>open source</kwd>
<kwd>optical character recognition</kwd>
<kwd>workflow</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0165551511429418" sec-type="intro">
<title>1. Introduction: the problem</title>
<p>When digitizing text-based resources such as documents, periodicals and books, a key part of the process is the conversion of paper-based analogue sources into their machine-readable counterparts using optical character recognition (OCR) software [<xref ref-type="bibr" rid="bibr1-0165551511429418">1</xref>]. In essence, this involves obtaining a scanned image of a printed page, using software to distinguish pixel patterns within the image and translating these into alphanumeric characters. Since the early innovations in OCR technology [<xref ref-type="bibr" rid="bibr2-0165551511429418">2</xref>], both the software and methodologies have improved and OCR has been applied successfully to a wide range of material using a variety of software.</p>
<p>This article discusses open source OCRing for historical collections. We argue that open source technology has many advantages for historical collections that are difficult to OCR. Historical documents often present a very different set of challenges to contemporary sources when it comes to digitization and the extraction of textual data. Even in relatively recent (e.g. early twentieth century) documents, typography, printing and language can differ widely from modern usage [<xref ref-type="bibr" rid="bibr3-0165551511429418">3</xref>].</p>
<p>In comparison to the business market, which most commercial OCR vendors aim at, historical OCR is a relatively niche concern [<xref ref-type="bibr" rid="bibr4-0165551511429418">4</xref>]. Commercial OCR engines are designed to work best with contemporary end-user documents with their primary application being the scanning of company notes and documentation. The requirements for such applications are very different from the ones we have for digitizing historical collections. The ‘black box’ nature of commercial OCR software means it is not easily customizable to suit varied historical collections. Staff involved in digitization projects have limited access to what goes on inside the software and limited control over its behaviour. They are, of course, able to modify various parameters under which an OCR system operates, but there is much less scope for understanding why the software performs as it does in particular cases, and thus for tailoring it to different historical collections by, for example, modifying the code, using specific training sets, or integrating different components.</p>
<p>In a commercial environment, often the scanned pages (or older, analogue surrogates such as microfilms) have to be sent for OCR processing to the commercial companies themselves, as they are the only ones with sufficient knowledge of the software; however, they lack the domain-specific knowledge, which if used could feed back into the process and result in more accurate outputs. A frequently quoted example of problems due to insufficient domain knowledge is Google’s attempt to scan and make available <italic>Tristram Shandy</italic> [<xref ref-type="bibr" rid="bibr5-0165551511429418">5</xref>]. Here, whole pages were left out because they were considered to be misprints, despite forming part of the original composition of the novel. Google did not consider it necessary to ask for more input from researchers and scholars in the field. In order to rectify such problems and improve OCRing for historical collection, in 2009 the EU funded ‘IMPACT – improving access to text’ [<xref ref-type="bibr" rid="bibr6-0165551511429418">6</xref>], which is a community project of many large European libraries and archives. However, IMPACT has yet to show its value for the many smaller historical archives for scholarly material. IMPACT concentrates on working with and customizing commercial software and large-scale distributed processing, both of which are often out of reach for these smaller archives.</p>
<p>King’s College London’s Ocropodium project was an attempt to evaluate the potential of open source OCR tools for providing customizable and cost-effective solutions to digital repositories of historical material. It was prompted by the thought that, between commercial pre-packaged products and expensive software development kits (SDKs), there was space for more flexible solutions that could be adapted by knowledgeable digitization staff to the particular demands of historical OCR.</p>
<p>This article covers the results of the Ocropodium project and discusses how to embed open source OCRing in small-scale historical archives. To this end, Section 2 introduces the background of the various commercial and open source OCR tools we have experimented with. Section 3 offers the architecture and some of the technology solutions we have developed, before in Section 4 we detail our evaluation results. Along with a standard evaluation of the accuracy levels obtained with various historical collections, we also present one of our case studies, in which we developed an OCR workflow incorporating both open source and commercial tools to support the digitization of the Serving Soldier archives at King’s College London.</p>
</sec>
<sec id="section2-0165551511429418">
<title>2. Background: OCR tools</title>
<p>In this section we present the OCR tools we evaluated, starting with those which are open source. The evaluation itself took the form of both performance benchmarks, run on a set of historical collections, and the development of prototype systems incorporating the various open source tools. Through this approach it was hoped that worthwhile lessons could be learned without placing too much emphasis on static measures of performance where, for many cases, it was a moving target and a poor indicator of future potential.</p>
<sec id="section3-0165551511429418">
<title>2.1. Open source tools</title>
<p>For our envisaged end-user, the small- or medium-sized digital library, ‘open source’ would typically mean that the software is free to both download and modify. It also means that there is no official support as typically offered by commercial companies, although, to varying degrees, this can be sought on various online forums and mailing lists. A total of five open source OCR tools were analysed. Two of these – Tesseract 3.0 [<xref ref-type="bibr" rid="bibr7-0165551511429418">7</xref>, <xref ref-type="bibr" rid="bibr8-0165551511429418">8</xref>] and Cuneiform Linux 1.1 – derived from relatively mature, formerly commercial products. The remaining three – Ocropus 0.4.4 [<xref ref-type="bibr" rid="bibr9-0165551511429418">9</xref>], GOCR 0.46 and Ocrad 0.17 – have a more typical open source pedigree.</p>
<p>Tesseract was originally developed by Hewlett Packard from the mid-1980s, but saw little or no development until the code was released under an open source licence in 2005. It is currently being worked on at Google, with the latest version, 3.0, having been released in late 2010. Up until version 2.0, Tesseract was more of a pure character-recognition tool, since it contained no facilities for handling non-binary (colour or greyscale) images, or for page layout analysis. These features have since been added in version 3.0 via the integration of the third-party Leptonica library, which improves Tesseract’s general-purpose usefulness as a standalone tool.</p>
<p>Like Tesseract, Cuneiform started life as a commercial product before its makers – Russian company Cognitive Technologies – open-sourced it under the BSD software licence in 2007. The core engine has since been ported to Linux by volunteers and further developed as a command-line tool. Cuneiform for Linux is a general-purpose OCR capable of page layout analysis and outputting transcript text in several formats, although scope for configuring ‘under-the-hood’ settings is quite limited at the time of writing.</p>
<p>Of the five open source OCR tools we looked at, Ocropus stood out as being the most accessible in terms of facilitating active modification by third parties. A set of OCR tools currently being developed by the Image Understanding and Pattern Recognition group (IUPR) at the University of Kaiserslautern, Ocropus comprises a large number of discrete components, which are intended to make adding missing functionality or customizing behaviour a more straightforward task. It is also, to our knowledge, unique in that large amounts of the ‘glue’ code holding the components together is written in the high-level (and programming-friendly) Python scripting language. This makes changing sections of code and testing those changes much quicker and easier than would be the case using lower-level compiled languages (C or C++, typically).</p>
<p>GOCR and Ocrad are both relatively small applications available under the GNU software licence and written to operate in the traditional Unix manner. With far fewer external dependencies and a much smaller code-base, they provide a good starting point for programmers looking to understand the fundamentals of OCR. This relative simplicity does translate to reduced functionality, however, and neither GOCR nor Ocrad are really intended for highly variable or age-degraded source material. Their inclusion in our tests is for completeness reasons only.</p>
</sec>
<sec id="section4-0165551511429418">
<title>2.2. Commercial tools</title>
<p>For our benchmarking, we also tested two commercial products that ran on Linux and featured a similar command-line interface (CLI) to the various open source offerings. Finereader 8.0 CLI is essentially a pre-packaged Linux version of Abbyy’s developer-focused (and comparatively expensive) OCR engine software development kit (SDK). Several page-limited licences are available, with the cheapest offering 12,000 pages on a single computer for around £100. Finereader 8.0 CLI provides a good range of output formats and a limited range of options for tuning its engine to different material.</p>
<p>In addition to Abbyy Finereader, we chose Vividata’s OCR Shop XTR (version 5.6) as a commercial benchmark since it uses the same OCR engine as the OmniPage Pro family of products sold by Nuance. The product costs around £1500 for a single machine licence but is not page-limited, although customer support over 30 days is subject to an annual maintenance fee. Like Finereader 8.0 CLI, OCR Shop XTR is a command-line distillation of a more expensive SDK product, albeit one with a large range of configuration options and format support.</p>
<p>Next, we describe the architecture and workflow environment we developed to bring together the advantages of open source tools (especially Ocropus) with the robustness of commercial OCR engines.</p>
</sec>
</sec>
<sec id="section5-0165551511429418">
<title>3. Architecture: customizing Ocropus for historical OCR</title>
<p>In order to test Ocropus and other tools in the institutional environment of small-scale historical archives, we developed an architecture that abstracted, above the level of specific applications, the broader storage and data management concerns. <xref ref-type="fig" rid="fig1-0165551511429418">Figure 1</xref> summarizes our architecture for scalable historical OCR, utilizing a task scheduler for running conversion jobs in parallel, a digital repository for storing transcripts and associated metadata and a web-based front-end for administration. After scanning, the images are sent to a repository and from there they are sent to several servers on our institutional grid to be OCRed in parallel. The OCR results are stored in the same repository as the scanned images for future further improvements.</p>
<fig id="fig1-0165551511429418" position="float">
<label>Figure 1.</label>
<caption>
<p>Ocropodium general architecture.</p>
</caption>
<graphic xlink:href="10.1177_0165551511429418-fig1.tif"/>
</fig>
<p>One conclusion from our evaluation was that the best results were often delivered by using several of Ocropus’s features in conjunction with another tool for character recognition. These hybrid systems looked particularly suitable for use in small-scale archives, where the potential advantages included a better range of configuration options, more scope to diagnose and address the <italic>causes</italic> of poor OCR results, and better scalability for enabling more flexible automated workflows.</p>
<p>This general architecture allows adjustment of our OCR process to the individual requirements of specific historical collections. It lends itself to dividing the OCRing into several well-defined modular steps that can be reassembled in workflows to address specific issues common to many historical collections. In order to allow users to interact directly with the modular OCR steps, we began developing the Ocropodium Web Processing (OWP) tool, which, over time and in response to our need for comparing various applications, evolved into a generalized front-end for command-line OCR tools. We describe OWP in detail in [<xref ref-type="bibr" rid="bibr10-0165551511429418">10</xref>].</p>
<p>Our OWP workflow tool takes advantage of Ocropus’s modular nature and provides users with a graphical interface to its constituent components. The standard OCR process can broadly be broken down into four areas: pre-processing, page layout analysis, line recognition and post-correction. The first of these areas – pre-processing – is the act of rendering a raw page scan into a state where the OCR recognition engine is able to achieve optimal accuracy rates. This first stage itself consists of numerous discrete steps, which can vary greatly depending on the source material. While the most important single step is probably ‘binarization’, described below, it can also include various physical transformations to the image geometry, removal of artefacts and general clean-up. For this reason it benefits greatly from our flexible, modular approach.</p>
<p>Binarization – converting a colour or grey-scale image to one containing only black (text) and white (background) – is a good example of a process that can benefit from opening up its black box behaviour. While some self-contained commercial OCR tools (e.g. OCR Shop XTR) grant access to a few of the underlying parameters, most of them take a fully automated approach which aims to cover common commercially relevant cases. This approach inevitably involves some compromises and trade-offs, however, and will not be optimal for all inputs, particularly those with highly variable or unusual tinting – both common characteristics in historical documents.</p>
<p>In addition to binarization, steps such as cleaning up undesirable artefacts, or removing non-text elements, are often applicable in diverse ways to different document collections and do not lend themselves to generalized automation. For this reason, we would argue that a highly flexible approach to pre-processing is crucial to an effective workflow for historical OCR.</p>
<p>The second stage in a typical OCR workflow is page layout analysis, which attempts to discern the semantic content of regions within the page and to ultimately determine a set of text lines in reading order. Since the layout of textual content within documents can vary greatly, with single or multiple columns, images and other non-text elements obscuring the overall text flow, layout analysis (or ‘page segmentation’) is an area fraught with complexities. As with the pre-processing stage, it too can benefit from opening up the black box and introducing greater flexibility to the OCR process. Ocropus provided an excellent platform on which we could build here, since it includes a number of different algorithms for performing page layout analysis, each packaged as a separate component. By incorporating an interactive interface for visualizing the page segmentation results on selected images, OWP provides users with a fast and easy way to make informed decisions about the algorithm most suitable for analysing their particular set of documents.</p>
<p>We have also written a proof-of-concept Ocropus component that accepts extra user input in the form of ‘hints’, which help the segmentation algorithm to reach the correct hypothesis via foreknowledge of the typical ‘expected’ page layout for a given document set. Whilst the ideal OCR system should be able to correctly analyse a wide variety of page formats without user intervention, the variable quality and more frequent artefacts found in historical material obviously make it more challenging.</p>
<p>This ‘hint’ approach is best illustrated by taking one of our test collections (described below in Section 4.1.1) as an example. Our Stormont Papers test corpus [<xref ref-type="bibr" rid="bibr11-0165551511429418">11</xref>] consists of 400 images, all of which have the same page layout: a single date-line at the top and two columns. Despite the high rates of variability in the font spacing and quite high relative column skew, the default Ocropus page segmentation component (at the time of writing, SegmentPageByRAST [<xref ref-type="bibr" rid="bibr12-0165551511429418">12</xref>]) will generally handle this layout quite well. Invariably, though, errors creep in. The header line, for example, will sometimes not be discerned as distinct from the two following columns, with the result that some or all of it will appear transcribed midway through the output text.</p>
<p>Occasionally, more pernicious problems occur, such as that highlighted in <xref ref-type="fig" rid="fig2-0165551511429418">Figure 2</xref>, which shows what happens when peculiarities of the source material make it difficult for the layout analyser to get it right. In this case, the unusually leftward indent of the heading titles in the second column of text has appeared to break the two-column layout, with the result that the lines following in <italic>both</italic> columns have been interpreted as a single block of text. The ability of the algorithm to detect very complicated layouts has in this case worked against us, resulting in a mis-segmentation of the page.</p>
<fig id="fig2-0165551511429418" position="float">
<label>Figure 2.</label>
<caption>
<p>Incorrect page segmentation caused by unusually protruding section headings.</p>
</caption>
<graphic xlink:href="10.1177_0165551511429418-fig2.tif"/>
</fig>
<p>To mitigate such issues, we rely on user hints to support the layout analysis. Our ‘hint’ system allows the user to constrain the segmentation of a particular document set with certain simple rules, e.g. header line, two columns and footer. These rules do not specify up-front the whole of the page layout, but rather inform the underlying layout analysis algorithm and reduce the ambiguity it has to overcome to reach the correct hypothesis. From the user’s point of view, the system works like any other Ocropus page-segmentation component and can be managed graphically using the OWP interface. It allows the user to process a large batch of scans (with typographical variability but the same basic high-level layout) and reduce the possibility of layout analysis errors afflicting individual pages through just a relatively small amount of work up-front.</p>
<p>Once an image has been pre-processed and its layout analysed, the actual text recognition takes place, with the typical unit of operation being a single line of text. This is the most opaque part of the OCR process, and one that depends on the use of both character and language models to determine first the optimal segmentation of individual characters within the line, and then their likely textual counterparts. Because of the complexity of this process and its dependence on character modelling, linguistic context and numerous character-set-specific heuristics, it is difficult to break it down in a procedural manner. For that reason we approach the text recognition step as a single discrete process, albeit one where some OCR engines allow far greater scope for user input in choosing parameters than others.</p>
<p>Although a great deal of active research is currently being conducted into ways to automatically improve the output of OCR engines [<xref ref-type="bibr" rid="bibr13-0165551511429418">13</xref>], post-correction of OCR material is still predominantly a manual activity (and invariably the most time-consuming part of the whole process). Discussions with research archive staff did raise one area where automation was employed in the correction stage, however, with use of project-specific word-processing macros employed to correct commonly occurring transcription errors. This kind of text manipulation, typically involving find-and-replace types of operations, lends itself greatly to the procedural workflow approach we use in OWP.</p>
<p>For the manual portion of the OCR workflow our aim, with OWP, was to improve the user experience by making the positional metadata of semantic page elements – i.e. columns, paragraphs and lines – a first-class part of the transcript correction process, with individual lines of text serving as separate editing ‘units’. Too often, with existing OCR correction processes, editing the text results in this metadata being lost, since the word processing tools often employed for such tasks have no notion of semantic elements relating to fixed positions on a physical document.</p>
<p>While this line-centric approach to document editing relies on the page layout hypothesis always being correct, the primary benefits are two-fold. First, it facilitates the creation of a user interface that visually links any part of the transcript being corrected to the portion of the source image from which it was derived. Providing these contextual cues eases correction effort considerably, by reducing the cognitive load required to constantly re-scan for the relevant part of the input image. Secondly, it results in a finished, corrected transcript that still retains the most relevant positional metadata describing its layout and structure. This information can be valuable for many reasons, not least to allow further post-processing and analysis, as well as informing methods of visualizing the text in its original format.</p>
<p>The next section will present our evaluation results. We will see how open source tools compare to commercial ones in terms of accuracy and usability in a historical archive.</p>
</sec>
<sec id="section6-0165551511429418">
<title>4. Evaluation</title>
<p>We had a two-fold approach to evaluation. The first evaluation was concerned with the effectiveness of our open source OCR. We used a wide range of historical collections as well as standard test collections to find out how open source tools compare to commercial ones. Section 4.1 analyses the results. Secondly, we wanted to verify our assumption that OCRing can be embedded in historical digitization environments and localized. We evaluated two case studies. For space reasons, we concentrate here on the Serving Soldier case study, with which we could show that our modular approach to OCRing can be used to improve standard OCRing. Section 4.2 will cover this case study.</p>
<sec id="section7-0165551511429418">
<title>4.1. Accuracy evaluation</title>
<p>The evaluation datasets were chosen because they were part of successfully completed digitization projects undertaken by the Centre for Data Digitisation and Analysis (CDDA) in Belfast. This provided us with access to not only the scans and final transcripts but also useful information regarding the original OCR process and, in some cases, the original machine OCR outputs.</p>
<p>Our evaluation method attempted to establish a raw character recognition rate with each tool deployed in more-or-less its default configuration (as described in Section 2.1), averaged over multiple source pages. To measure the quality of each tool’s output we employed the University of Nevada Information Science Research Institute (ISRI) OCR toolkit, a suite of CLI tools for assessing various metrics of OCR accuracy.<sup><xref ref-type="fn" rid="fn1-0165551511429418">1</xref></sup> For the results shown below, we opted to use raw (uncorrected) per-character correspondences for each image’s output text against an accurate so-called ‘ground-truth’ transcript, or the percentage of characters in the output that match a correctly transcribed source.</p>
<sec id="section8-0165551511429418">
<title>4.1.1. Historical collections</title>
<p>We collaborated with CDDA to analyse a wide range of historical collections, each representing specific OCR challenges in small-scale historical archives:</p>
<list id="list1-0165551511429418" list-type="bullet">
<list-item>
<p>The Stormont papers are a record of Irish parliamentary debates between 1921 and 1976. The collection shows characteristics found in many historical collections: the scans were fairly challenging to OCR, with a variety of printing and scanning artefacts in evidence. Each page comprises two long columns of text, with large amounts of intermixed italics and a variety of heading sizes. The standard text is fairly uneven in weighting, resulting in a lot of broken glyphs and joined serifs, the tightly spaced characters frequently overlapping their neighbours. Additionally, some scans contained a large amount of small non-text artefacts and reverse-sheet bleed-through, and the warping between columns is often inconsistent.</p>
</list-item>
<list-item>
<p>The Dictionary of Older Scottish Tongue scans were of very high quality with few non-text artefacts, minimal warping and even print of a generally constant size. The historical text is, however, unusual and specific in terms of its layout. It is extremely dense and made up of two tightly spaced columns between top and bottom header lines. Linguistically, the material consists of old Scots and archaic English, so the role of language modelling is considerably reduced.</p>
</list-item>
<list-item>
<p>The Act of Union scans were included in our tests as an example of a very low-quality sample set of the kind that might serve as an enrichment of other primary data. These kinds of collections are often the result if small-scale archives do not have the resources for professional support or the images cannot be taken with a high-resolution camera, as the objects are fragile and cannot be moved. The images are much lower resolution than would realistically be expected of anything primarily intended for OCR, with the dimensions of the already binarized scans averaging around only 450 × 800 pixels. The text is visually pixelated with frequent broken characters and heavy bleed-through artefacts.</p>
</list-item>
<list-item>
<p>The Corpus of Electronic Texts (CELT) test material, sourced from the Book of Leinster, consisted principally of one narrow column of Gaelic verse. Since none of the tools under test had out-of-the-box support for medieval Irish, the role of language modelling was again minimized.</p>
</list-item>
</list>
<p>According to <xref ref-type="table" rid="table1-0165551511429418">Table 1</xref> with respect to the standard Stormont papers, Abbyy Finereader 8.0 and XTR Shop lead the rankings, but neither outdoes the best performing open source tool (Cuneiform) by very much. Tesseract’s performance also compares fairly well with the commercial alternatives. Ocropus, using its default multi-layer perceptron (MLP) character model, performs quite poorly by comparison, but is still well ahead of the remaining open source tools. One should, however, add here that this character model is currently being reworked and we are in the process of undertaking new tests with the new character model. As a project at an early stage of its development, Ocropus MLP deserves a large caveat attached when considering the performance results. The relatively rapid rate at which major changes are integrated into the code base means that comparison of the performance of particular releases against much more mature alternatives is difficult.</p>
<table-wrap id="table1-0165551511429418" position="float">
<label>Table 1.</label>
<caption>
<p>Digitization accuracy results of the benchmark historical collections</p>
</caption>
<graphic alternate-form-of="table1-0165551511429418" xlink:href="10.1177_0165551511429418-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Stormont</th>
<th align="left">DoOST</th>
<th align="left">Act of Union</th>
<th align="left">CELT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Finereader</td>
<td>96.0%</td>
<td>94.3%</td>
<td>79.5%</td>
<td>87.0%</td>
</tr>
<tr>
<td>OCR Shop</td>
<td>95.5%</td>
<td>95.4%</td>
<td>62.0%</td>
<td>86.4%</td>
</tr>
<tr>
<td>Cuneiform</td>
<td>95.0%</td>
<td>91.3%</td>
<td>72.3%</td>
<td>70.4%</td>
</tr>
<tr>
<td>GOCR</td>
<td>56.0%</td>
<td>78.4%</td>
<td>40.8%</td>
<td>76.8%</td>
</tr>
<tr>
<td>Ocrad</td>
<td>65.5%</td>
<td>20.5%</td>
<td>38.3%</td>
<td>81.6%</td>
</tr>
<tr>
<td>Ocropus MLP</td>
<td>82.5%</td>
<td>86.8%</td>
<td>20.4%</td>
<td>82.6%</td>
</tr>
<tr>
<td>Tesseract</td>
<td>93.0%</td>
<td>95.5%</td>
<td>64.0%</td>
<td>79.3%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>For the Older Scottish Tongue collection, Tesseract gave the best results on these high-quality scans, with OCR Shop XTR a close second. The Act of the Union test, although quite far from a typical-use case due to the very low quality of the input images, nonetheless illustrates the highly variable output that can result when a particular tool is not optimized for a given source. Although the output is poor across the board in this test, the developers of Finereader 8.0 and Cuneiform have obviously expended more effort on making their products perform as well as possible on low-resolution input images. One should note that an earlier test with Abbyy Finereader 8.0 CLI on the Act of Union material had resulted in a character accuracy result of just 33.2 per cent. We later discovered that this was due to Finereader making incorrect assumptions about the scan’s pixel density from information in the image header. The result given here is as derived from feeding Finereader images with artificially ‘adjusted’ header data.</p>
<p>The CELT collection was added because we were interested in how the OCR would work with the specific Gaelic verse. None of the OCR engines performed very well, but overall the accuracy spread is less extensive. With this specific character set the two open source engines that derive from commercial products perform less well. Of all the open source tools, Ocropus MLP performed best, which implies to us that it has good potential for improvement if more time is spent on its (currently quite immature) character model.</p>
<p>Overall, our accuracy evaluation has shown that the commercial tools are not necessarily the best but they deliver the most stable results. Open source tools, however, are capable of performing competitively on a reasonable range of input material. Next, we investigate the results from a reference collection.</p>
</sec>
<sec id="section9-0165551511429418">
<title>4.1.2. Reference UNLV evaluation</title>
<p>As a complement to our benchmark historical collections we also attempted to test each tool on a subset of the University of Nevada Las Vegas (UNLV) OCR test collection, consisting of a large variety of business, legal and technical documents, along with samples from newspapers and magazines (see <xref ref-type="table" rid="table2-0165551511429418">Table 2</xref>). We restricted our tests to 300 dpi binary images, running a total of 2400 page conversions.</p>
<table-wrap id="table2-0165551511429418" position="float">
<label>Table 2.</label>
<caption>
<p>UNLV accuracy results</p>
</caption>
<graphic alternate-form-of="table2-0165551511429418" xlink:href="10.1177_0165551511429418-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Business</th>
<th align="left">Dept. Env.</th>
<th align="left">Legal</th>
<th align="left">Magazine</th>
<th align="left">Newspaper</th>
<th align="left">Reports</th>
<th align="left">Spanish</th>
</tr>
</thead>
<tbody>
<tr>
<td>Finereader</td>
<td>97.2%</td>
<td>88.8%</td>
<td>92.2%</td>
<td>76.6%</td>
<td>93.3%</td>
<td>85.2%</td>
<td>90.4%</td>
</tr>
<tr>
<td>OCR Shop</td>
<td>95.8%</td>
<td>87.2%</td>
<td>91.2%</td>
<td>74.6%</td>
<td>93.2%</td>
<td>90.6%</td>
<td>83.9%</td>
</tr>
<tr>
<td>Cuneiform</td>
<td>85.2%</td>
<td>78.2%</td>
<td>82.5%</td>
<td>67.2%</td>
<td>86.8%</td>
<td>65.4%</td>
<td>78.6%</td>
</tr>
<tr>
<td>Gocr</td>
<td>70.2%</td>
<td>45.2%</td>
<td>81.6%</td>
<td>0.9%</td>
<td>27.2%</td>
<td>44.2%</td>
<td>25.0%</td>
</tr>
<tr>
<td>Ocrad</td>
<td>83.3%</td>
<td>51.0%</td>
<td>85.4%</td>
<td>13.5%</td>
<td>29.0%</td>
<td>46.1%</td>
<td>21.4%</td>
</tr>
<tr>
<td>Ocropus MLP</td>
<td>85.5%</td>
<td>62.2%</td>
<td>87.8%</td>
<td>12.7%</td>
<td>66.7%</td>
<td>71.6%</td>
<td>60.3%</td>
</tr>
<tr>
<td>Tesseract</td>
<td>93.9%</td>
<td>84.2%</td>
<td>92.4%</td>
<td>74.6%</td>
<td>91.8%</td>
<td>83.3%</td>
<td>87.0%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In absolute terms, the two commercial engines Abbyy Finereader 8.0 CLI and OCR Shop XTR were more or less equally matched and averaged about 5 per cent greater accuracy over the whole dataset than Tesseract 3.0, the best of the open source tools. A nuanced look at the results, though, paints an interesting picture of where the commercial offerings had the edge.</p>
<p>Perhaps surprisingly, raw character recognition rates on this fairly standard English-language material are, for Cuneiform, Tesseract 3.0 and also Ocropus, in some cases very comparable with Finereader and OCR Shop XTR. Generally speaking, for the better open source tools, the main factor drawing the average accuracy down was a lack of robustness in several areas. For the business material, usually standard-format business letters and mail-outs, the difference could generally be attributed to poorer handling of non-textual formatting and other stylistic elements, such as logos, bullet points and, in particular, signatures. Whereas the commercial tools detected and ignored non-text components, Tesseract and Cuneiform generally attempted to OCR them, introducing junk output into the transcript. In contrast, Tesseract 3.0 achieves the highest raw accuracy score on the much more stylistically reserved legal document set.</p>
<p>Handling of non-text elements also had a major bearing on the results for the magazine and newspaper samples, although in these cases page segmentation and layout analysis were also a considerable factor. The often challenging layouts used in these scans produced a very high variability in error rates in all the applications we tested; even the two commercial tools were frequently flummoxed (often by images which the other handled correctly).</p>
<p>In summary, the commercial tools perform considerably better with out-of-the-box tasks, without the benefit of user input. Although the best open source ones do not lag too far behind when it comes to recognizing standard character sets, they are relatively weak when dealing with non-text components and difficult layouts. Whilst its MLP character model text-recognition components are not yet competitive with commercial offerings, Ocropus’s pre-processing and layout analysis features are quite effective, at least when compared to other open source tools.</p>
<p>For our case of small archives and historical collections, we concluded that, for the moment at least, the most promising approach is to combine the commercial engines’ ability to read a wider range of character sets with the flexibility of open source tools in terms of customizable pre-processing and layout analysis. As mentioned above, our OWP provides a platform where command-line OCR tools (open source or commercial) can be integrated with minimal effort. As well as just leveraging each particular tool’s native feature set, we can also take a hybrid approach whereby, for instance, Ocropus’s flexible and effective pre-processing and segmentation features are combined with another application’s character-recognition engine. By integrating Ocropus and Tesseract in this manner, for example, we can obtain some of the best of both worlds: flexible pre-processing with mature recognition features.</p>
<p>Integrating several tools into a broader framework also allows us to run automated evaluation testing, whereby we compare the performance of different combinations of parameters on the same input material, with the ISRI OCR evaluation suite providing a final accuracy ‘score’. Users can quickly and easily test multiple combinations of pre-processing, page segmentation and character recognition tools against one another on any sample material and use the results to inform the method of proceeding with OCR on the full document collection.</p>
<p>The next section describes one of the case studies we have run to verify our approach of mixing open source with off-the-shelf tools in order to employ the best of all worlds.</p>
</sec>
</sec>
<sec id="section10-0165551511429418">
<title>4.2. Case study: Serving Soldier newsletters</title>
<p>We ran two case studies to demonstrate how Ocropodium technology can be embedded in the workflows of small-scale historical archives. The first one covered the parliamentary papers of Northern Ireland discussed earlier, but in this article we would like to concentrate on the second case study, which concerned a project at the King’s College archives. The Serving Soldier digitizes items from the Liddell Hart Centre for Military Archives, which contains 800 collections of material created by senior defence personnel throughout the twentieth century. In parallel to the official digitization programme we experimented with the OWP procedural workflow to compare digitization workflows and analyse advantages and drawbacks of our open source approach. Section 4.2.1 describes the general workflow before in Section 4.2.2 we describe institutional issues.</p>
<sec id="section11-0165551511429418">
<title>4.2.1. The digitization workflow</title>
<p>The Serving Soldier HMS Kelly newsletters provide a good example of specific problems with historical collections. The single-block type-written source documents, dating from 1939, have been fairly well captured as 300 DPI colour scans, albeit with some quite significant warping and skew present on some pages. As one can see in <xref ref-type="fig" rid="fig3-0165551511429418">Figure 3</xref>, however, the newsletters have a heavy yellow tint and quite low-contrast, slightly purple, text. There are also a number of non-text elements present in the page heading and footer, along with significant border noise. Because of these artefacts, even a high-quality commercial OCR such as Abbyy Finereader 8.0 CLI will struggle to produce good results without the aid of external pre-processing. Even with single-column mode activated to reduce layout-analysis errors, Finereader produces transcripts which average about 80 per cent accuracy, as measured by the UNLV ISRI tools.</p>
<fig id="fig3-0165551511429418" position="float">
<label>Figure 3.</label>
<caption>
<p>Various steps in a typical pre-processing workflow: binarization, edge clean-up and de-skew.</p>
</caption>
<graphic xlink:href="10.1177_0165551511429418-fig3.tif"/>
</fig>
<p>The OWP web interface allows us to create a pre-processing workflow (comprising, in this case, components from the Ocropus tool set) which closely matches the requirements of the source scans, and then to feed the pre-processed images to Finereader for recognition. Moreover, if we have ground-truth transcripts for a sample of the source documents, we can instrument the effectiveness of each stage of the workflow, to see if it contributes positively or negatively to the final result.</p>
<p>We were particularly interested in analysing how we could improve the current use of commercial tools in the project – here, Abbyy’s Finereader. In the case of the HMS Kelly newsletters, we found that binarizing the source images using Ocropus’s implementation of the Sauvola algorithm for adaptive document image binarization [<xref ref-type="bibr" rid="bibr14-0165551511429418">14</xref>], where the parameters were interactively adjusted for the highest-contrast text, increased Abbyy Finereader’s accuracy from 80 to 91 per cent. Adding a document clean-up component contributes another 1 per cent to the accuracy score and a RAST de-skew component a further 6 per cent, bringing the average accuracy of the Ocropus/Finereader workflow to 97.8 per cent over the 20 source images – a 17.8 per cent increase in total on using Abbyy Finereader 8.0 CLI alone. Building OCR workflows in this manner gives us a high degree of flexibility. If we later decide, for example, to use an open source tool for text recognition, we can simple substitute Ocropus, Tesseract or Cuneiform in place of Finereader and employ the same tailored pre-processing stage.</p>
<p>In the next section we discuss operational issues we encountered in the case study. These are mainly linked to the fact that users are generally only familiar with Windows-based environments, while open source tools often require a deeper understanding of computing even if embedded in web-based workflows.</p>
</sec>
<sec id="section12-0165551511429418">
<title>4.2.2. Operational issues</title>
<p>Our case study has shown many advantages of our approach but also operational disadvantages towards the existing commercial OCRing. In limiting our evaluation to tools with command-line interfaces (CLIs) common in the open source world, rather than those featuring a graphical user interface (GUI), we made certain assumptions about the level of skills expected at the small- and medium-sized digital archives we consider the principal end-users of the software. These skills could be broadly divided into three areas, relating to operational use, integration and source-level modification, respectively. In this section we discuss these with a particular perspective on the considerable advantages of open source tools and offer solutions to the operational issues based on our OWP work. But, discussions with research staff at the Liddell Hart archives also delivered considerable disadvantages.</p>
<p>The most commonly quoted disadvantage of our open source solutions was ease of use. Most open source tools are mainly command-line based and do not come with advanced graphical user interfaces. Command-line tools invariably require a higher degree of technical proficiency on behalf of the user than typical OCR desktop GUI applications that run on Microsoft Windows and Apple’s Mac OS X. In practice, we would not expect an institution to deploy a command-line tool to end-users without some kind of additional user interface. To this end we developed OWP, but there are also others available that come with the various open source tools. It is a realistic assumption, however, that the use of open source OCR tools will require more technical skills from staff than the commercial GUI alternatives.</p>
<p>The major strength of command-line tools (and indeed the reason commercial vendors like Abbyy provide CLI versions of their OCR engines) is to facilitate integration with a broader IT infrastructure. This is possible at various levels: a basic use-case might be a system administrator installing a service to automatically OCR material ingested into a digital repository. A more advanced case might be a programmer writing a page layout analyser targeted at specific types of material. This kind of extension and integration with off-the-shelf tools is relatively accessible to generalist programmers.</p>
<p>At the most advanced level, open source provides the opportunity to actively modify the OCR tools themselves, although, because of the complexity of the software, we do not anticipate this to be a common occurrence. It is important to note that programmers with the required skill-set and domain-specific knowledge to undertake active enhancement of OCR tools are fairly rare and are therefore likely to be quite difficult and expensive to hire.</p>
<p>At an end-user level, the more transparent, diagnosable behaviour of the componentized workflows made possible by integrating the Ocropus with other commercial and open source character recognition engines was considered a major advantage in our case studies. It led to quite a few discussions with the research staff about the functionalities at the various stages of the workflow and a more advanced understanding of why and where OCR fails when historical collections are concerned.</p>
<p>In a broader practical sense, open source tools are undoubtedly easier to manage than their commercial counterparts in some important respects, namely flexibility of use and ease of administration. Both of the commercial applications tested here (Finereader 8.0 CLI and OCR Shop XTR) need a licence for each machine they are installed on, as well as requiring an internet connection in order to function. This can cause serious productivity issues when problems arise due to machine upgrades or network unavailability. In contrast, all of the open source applications tested here can be installed without restriction on as many computers as necessary and do not require access to the internet in order to run.</p>
<p>Since access to heavyweight institutional infrastructure such as that architected by the IMPACT project is unlikely to be viable for smaller research archiving, we believe the ideal environment for our workflow tools would be on locally administered internal networks. We recognize, however, that there is also scope for sharing such services among several smaller institutions. In general, the future of Ocropodium services will be linked to the joint European arts and humanities research infrastructure DARIAH,<sup><xref ref-type="fn" rid="fn2-0165551511429418">2</xref></sup> which is funded until 2016 and in which KCL is a partner.</p>
</sec>
</sec>
</sec>
<sec id="section13-0165551511429418" sec-type="conclusions">
<title>5. Conclusion</title>
<p>In this article we have tried to offer a pragmatic approach to OCR of historical collections for small archives and digital libraries. Open source tools have unquestionable advantages: besides being effectively free to use, they are sometimes more flexible and have lower administrative overheads compared to comparable commercial offerings. Moreover, the best of them can deliver competitive results on many types of source material. The commercial products we have evaluated here, however, undoubtedly have a significant edge with a broad range of inputs. For the test-case document collections we chose to evaluate, the commercial tools most clearly demonstrate their advantage on a wide range of standard material. When it comes to pure text-recognition capabilities, the performance gap between the commercial tools and the best available open source ones still exists, but is somewhat smaller.</p>
<p>We do, however, believe that, for a large range of historical material that could potentially be digitized via OCR, the flexibility offered by standalone commercial products is insufficiently broad. There is a significant opportunity, therefore, to combine both open source and commercial tools in a manner that provides a mix-and-match approach and allows skilled digitization staff to build custom workflows tailored to specific historical collections. Our OWP tool demonstrates one way that such custom workflows can be built in a user-friendly and flexible way. Our Serving Soldier case study describes one such possible workflow – combining open source pre-processing and page-segmentation components with a commercial character-recognition engine – but the possibilities are much more extensive.</p>
<p>Using the approach outlined above, we believe that it is possible to leverage open source software to produce a robust, scalable system for historical digitization projects. Lowering the technical barriers to effective use of modular open source tools like Ocropus will empower institutions to share experience and build on successive projects. Extensive automation does not have to mean ‘black-box’ tools that no-one who uses them can understand. Our digitization tools need to be as flexible as our source collections are diverse: workflows that can better match the varied needs of the input material will allow staff to better deploy their skills and knowledge in contributing to our digital heritage.</p>
</sec>
</body>
<back>
<ack>
<p>The Ocropodium project was funded by the Joint Information Systems Committee (JISC).</p>
</ack>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0165551511429418">
<label>1.</label>
<p><ext-link ext-link-type="uri" xlink:href="http://code.google.com/p/isri-ocr-evaluation-tools/">http://code.google.com/p/isri-ocr-evaluation-tools/</ext-link></p>
</fn>
<fn fn-type="other" id="fn2-0165551511429418">
<label>2.</label>
<p><ext-link ext-link-type="uri" xlink:href="http://www.dariah.eu">http://www.dariah.eu</ext-link></p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0165551511429418">
<label>[1]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Holley</surname><given-names>R.</given-names></name>
</person-group> <article-title>How good can it get? Analysing and improving OCR accuracy in large scale historic newspaper digitisation programs</article-title>. <source>D-Lib Magazine</source> <year>2009</year>; <volume>15</volume>(3/4).</citation>
</ref>
<ref id="bibr2-0165551511429418">
<label>[2]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mori</surname><given-names>S</given-names></name>
<name><surname>Suen</surname><given-names>CY</given-names></name>
<name><surname>Yamamoto</surname><given-names>K.</given-names></name>
</person-group> <article-title>Historical review of OCR research and development</article-title>. <source>Proceedings of the IEEE 1992</source>; <volume>80</volume>: <fpage>1029</fpage>–<lpage>1058</lpage>.</citation>
</ref>
<ref id="bibr3-0165551511429418">
<label>[3]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Stamatopoulos</surname><given-names>N</given-names></name>
<name><surname>Louloudis</surname><given-names>G</given-names></name>
<name><surname>Gatos</surname><given-names>B.</given-names></name>
</person-group> <article-title>A comprehensive evaluation methodology for noisy historical document recognition techniques</article-title>. <conf-name>Proceedings of the third workshop on analytics for noisy unstructured text data</conf-name>. <conf-loc>Barcelona, Spain: ACM</conf-loc>, <year>2009</year>, pp. <fpage>47</fpage>–<lpage>54</lpage>.</citation>
</ref>
<ref id="bibr4-0165551511429418">
<label>[4]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Bryant</surname><given-names>M</given-names></name>
<name><surname>Blanke</surname><given-names>T</given-names></name>
<name><surname>Hedges</surname><given-names>M</given-names></name>
<name><surname>Palmer</surname><given-names>R.</given-names></name>
</person-group> <article-title>Open source historical OCR: the OCRopodium project research and advanced technology for digital libraries</article-title>. In: <person-group person-group-type="editor">
<name><surname>Lalmas</surname><given-names>M</given-names></name>
<name><surname>Jose</surname><given-names>J</given-names></name>
<name><surname>Rauber</surname><given-names>A</given-names></name>
<name><surname>Sebastiani</surname><given-names>F</given-names></name>
<name><surname>Frommholz</surname><given-names>I</given-names></name>
</person-group> (eds), <conf-name>Research and advanced technology for digital libraries, Proceedings of 14th European conference, ECDL 2010</conf-name>. <conf-loc>Berlin: Springer</conf-loc>, <year>2010</year>, pp. <fpage>522</fpage>–<lpage>525</lpage>.</citation>
</ref>
<ref id="bibr5-0165551511429418">
<label>[5]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Duguid</surname><given-names>P.</given-names></name>
</person-group> <article-title>Inheritance and loss? A brief survey of Google Books</article-title>. <source>First Monday</source> <year>2007</year>; <volume>12</volume>(<issue>8</issue>).</citation>
</ref>
<ref id="bibr6-0165551511429418">
<label>[6]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ploeger</surname><given-names>L.</given-names></name>
</person-group> <article-title>In brief: IMPACT</article-title>. <source>D-LIB Magazine</source> <year>2009</year>; <volume>15</volume>(<issue>1/2</issue>).</citation>
</ref>
<ref id="bibr7-0165551511429418">
<label>[7]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Smith</surname><given-names>R.</given-names></name>
</person-group> <article-title>An overview of the Tesseract OCR engine</article-title>. <conf-name>2007 ICDAR 2007 ninth international conference on document analysis and recognition</conf-name>, <year>2007</year>, pp. <fpage>629</fpage>–<lpage>633</lpage>.</citation>
</ref>
<ref id="bibr8-0165551511429418">
<label>[8]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Unnikrishnan</surname><given-names>R</given-names></name>
<name><surname>Smith</surname><given-names>R.</given-names></name>
</person-group> <article-title>Combined script and page orientation estimation using the Tesseract OCR engine</article-title>. <conf-name>Proceedings of the international workshop on multilingual OCR</conf-name>. <conf-loc>Barcelona, Spain: ACM</conf-loc>, <year>2009</year>, article 6, pp. <fpage>1</fpage>–<lpage>7</lpage>.</citation>
</ref>
<ref id="bibr9-0165551511429418">
<label>[9]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Breuel</surname><given-names>T.</given-names></name>
</person-group> <article-title>Recent progress on the OCRopus OCR system</article-title>. <conf-name>Proceedings of the international workshop on multilingual OCR</conf-name>. <conf-loc>Barcelona, Spain</conf-loc>: <publisher-name>ACM</publisher-name>, <year>2009</year>, article 2, pp. <fpage>1</fpage>–<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr10-0165551511429418">
<label>[10]</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Bryant</surname><given-names>M.</given-names></name>
</person-group> <article-title>The Ocropodium Project [Online]</article-title>. <comment>Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ocropodium.cerch.kcl.ac.uk">http://www.ocropodium.cerch.kcl.ac.uk</ext-link></comment> (accessed <day>16</day> <month>June</month> <year>2011</year>).</citation>
</ref>
<ref id="bibr11-0165551511429418">
<label>[11]</label>
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Dunning</surname><given-names>A</given-names></name>
<name><surname>Anderson</surname><given-names>S</given-names></name>
<name><surname>Polfreman</surname><given-names>M</given-names></name>
<name><surname>Albuquerque</surname><given-names>V</given-names></name>
<name><surname>Hedges</surname><given-names>M</given-names></name>
</person-group> (eds). <article-title>The Stormont papers: from partition to direct rule – 50 years of Northern Ireland Parliamentary papers online</article-title>. <source>Database</source>, <year>2007</year>.</citation>
</ref>
<ref id="bibr12-0165551511429418">
<label>[12]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Breuel</surname><given-names>T.</given-names></name>
</person-group> <article-title>Fast recognition using adaptive subdivisions of transformation space</article-title>. In: <conf-name>Proceedings of IEEE conference on computer vision and pattern recognition, CVPR ’92</conf-name>. <conf-loc>Champaign, IL, USA</conf-loc>, <year>1992</year>, pp. <fpage>445</fpage>–<lpage>451</lpage>.</citation>
</ref>
<ref id="bibr13-0165551511429418">
<label>[13]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Neudecker</surname><given-names>C</given-names></name>
<name><surname>Tzadok</surname><given-names>A.</given-names></name>
</person-group> <article-title>User collaboration for improving access to historical texts</article-title>. <source>LIBER Quarterly</source> <year>2010</year>; <volume>20</volume>(<issue>1</issue>): <fpage>119</fpage>–<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr14-0165551511429418">
<label>[14]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sauvola</surname><given-names>J</given-names></name>
<name><surname>Pietikäinen</surname><given-names>M.</given-names></name>
</person-group> <article-title>Adaptive document image binarization</article-title>. <source>Pattern Recognition</source> <year>2000</year>; <volume>33</volume>(<issue>2</issue>): <fpage>225</fpage>–<lpage>236</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>