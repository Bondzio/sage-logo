<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TIA</journal-id>
<journal-id journal-id-type="hwp">sptia</journal-id>
<journal-id journal-id-type="nlm-ta">Trends Amplif</journal-id>
<journal-title>Trends in Amplification</journal-title>
<issn pub-type="ppub">1084-7138</issn>
<issn pub-type="epub">1940-5588</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1084713812468512</article-id>
<article-id pub-id-type="publisher-id">10.1177_1084713812468512</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Music and Hearing Aids—An Introduction</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Chasin</surname><given-names>Marshall</given-names></name>
<degrees>AuD</degrees>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Hockley</surname><given-names>Neil</given-names></name>
<degrees>MSc</degrees>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Chasin</surname><given-names>Marshall</given-names></name>
<degrees>AuD</degrees>
<xref ref-type="aff" rid="aff1-1084713812468512">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1084713812468512"><label>1</label>Musicians’ Clinics of Canada, Toronto, ON, Canada</aff>
<author-notes>
<corresp id="corresp1-1084713812468512">Marshall Chasin, AuD, Reg. CASLPO, Musicians’ Clinics of Canada, No. 340-340 College Street, Toronto, ON M5T3A9, Canada Email: <email>Marshall.Chasin@rogers.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>9</month>
<year>2012</year>
</pub-date>
<volume>16</volume>
<issue>3</issue>
<issue-title>Special Issue on Music and Hearing Loss: Preventative and Rehabilitative options</issue-title>
<fpage>136</fpage>
<lpage>139</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Modern digital hearing aids have provided improved fidelity over those of earlier decades for speech. The same however cannot be said for music. Most modern hearing aids have a limitation of their “front end,” which comprises the analog-to-digital (A/D) converter. For a number of reasons, the spectral nature of music as an input to a hearing aid is beyond the optimal operating conditions of the “front end” components. Amplified music tends to be of rather poor fidelity. Once the music signal is distorted, no amount of software manipulation that occurs later in the circuitry can improve things. The solution is not a software issue. Some characteristics of music that make it difficult to be transduced without significant distortion include an increased sound level relative to that of speech, and the crest factor- the difference in dB between the instantaneous peak of a signal and its RMS value. Clinical strategies and technical innovations have helped to improve the fidelity of amplified music and these include a reduction of the level of the input that is presented to the A/D converter.</p>
</abstract>
<kwd-group>
<kwd>music</kwd>
<kwd>hearing aids</kwd>
<kwd>distortion</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1084713812468512" sec-type="intro">
<title>Introduction</title>
<p>The subject area of music, as an input to a hearing aid, is relatively new. Understandably, hearing aid design engineers and researchers have long been interested in optimizing a hearing aid response and technology for speech as an input. Speech and music have many similarities; however, there are some important differences that have direct implications on how a hearing aid should be designed (rather than simply programmed) for music.</p>
</sec>
<sec id="section2-1084713812468512">
<title>Similarities and Differences Between Speech and Music</title>
<sec id="section3-1084713812468512">
<title>The Low-Frequency Limit</title>
<p>Both speech and music occupy similar, albeit slightly different frequency ranges. The lowest frequency element of speech is the fundamental frequency, which for a male voice is on the order of 100-125 Hz. (<xref ref-type="bibr" rid="bibr6-1084713812468512">Johnson, 2003</xref>). In the human vocal tract, the vibration of the vocal cords for voiced sounds defines the fundamental frequency and its higher-frequency harmonic content and this limits the low-frequency end of the vocal output. There is simply no speech energy below the frequency of the fundamental. In contrast, musical instruments can generate significantly lower frequency with fundamental energy on the order of 40-50 Hz for bass instruments. <xref ref-type="fig" rid="fig1-1084713812468512">Figure 1</xref> shows a treble clef showing several musical notes and the frequency of their fundamentals. Middle C (just below the treble clef) is 262 Hz, and a typical male’s fundamental frequency is an octave below that (approximately 125 Hz).</p>
<fig id="fig1-1084713812468512" position="float">
<label>Figure 1.</label>
<caption>
<p>A treble clef showing several musical notes and their fundamental frequencies</p>
</caption>
<graphic xlink:href="10.1177_1084713812468512-fig1.tif"/></fig>
<p>However music, like speech, is slightly more complicated than just a rendition of any frequency components available or audible. In turns out that in music, like speech, it is not the fundamental frequency that defines the “pitch” of the note but the difference between any two successive harmonics. This is called the missing fundamental and explains why one only needs to hear the higher-frequency harmonics to define the pitch, which in some cases is below the bandwidth of the transmitter.</p>
<p>In the case of a telephone for example, the bandwidth is typically between 340 and 3400 Hz. A fundamental frequency of 125 Hz which is typical of a male’s voice is below the transmitting frequency response of the telephone, yet we can identify the voice as male. It is the difference between two adjacent harmonics, where the harmonics are within the bandpass of a transmitting device, that is important. In the human vocal tract, the vocal cords function as a one half wavelength resonator with the result of integer multiple harmonics being generated. Harmonics of 250 Hz, 375 Hz, 500 Hz, 625 Hz, and so on are generated by the 125 Hz fundamental and it is these harmonics that are within the bandpass of the telephone that are audible, namely, 375 Hz and higher. The difference between any two adjacent harmonics is exactly 125 Hz and this is what our brains replace.</p>
<p>The take-home message about the lower end of the frequency response of a hearing aid for any stimulus is that the harmonic structure is more important than the lowest frequency component or fundamental. Extending the amplified frequency response, especially in a noisy environment, down to 125 Hz for a male voice is simply not necessary. The same is true of music. Large, low-frequency emphasis instruments, such as the bass and cello, do generate significant energy in the lower-frequency region, but it is typically the mid- and higher-frequency harmonic structure that define its quality.</p>
<p>Despite the presence of some low-frequency elements in music, because of noise reduction circuits in hearing aids that frequently cannot be disabled, amplified music needs to be high-pass-filtered above 100 Hz in order to not confuse the hearing aid into rejecting an important signal. This may not be required in the future, but is certainly the case with current technology.</p>
</sec>
<sec id="section4-1084713812468512">
<title>The High-Frequency Limit</title>
<p>In contrast to the above discussion, both speech and music have similar higher-frequency limits in their spectra. Speech of all languages has higher-frequency sibilants (“s”), affricates (“ch”), and fricatives (“th”) that have significant energy above 10,000 Hz. The same is true of musical instruments although the harmonic structure of bass instruments and some stringed instruments may have minimal high-frequency harmonic energy, especially above their bridge resonance.</p>
<p>The main restriction as discussed by Moore et al. (<xref ref-type="bibr" rid="bibr7-1084713812468512">Moore, Fullgrabe, &amp; Stone, 2011</xref>) and Ricketts et al. (<xref ref-type="bibr" rid="bibr8-1084713812468512">Ricketts, Dittberner, &amp; Johnson, 2008</xref>) is the degree and slope of the audiometric configuration and not the music per se. In general, the more severe the hearing loss and/or the greater its audiometric slope, the more limited will be the higher-frequency amplification that can be applied with minimal distortion.</p>
</sec>
</sec>
<sec id="section5-1084713812468512">
<title>Spectral Intensities</title>
<p>A major difference between speech and music lies in their intensities. The most intense elements of average conversational speech are on the order of 85 dB SPL. This usually is related to the low back vowel [a] as in “f<underline>a</underline>ther” and is common to all human languages. There is minimal damping of the vocal tract with this vowel and subsequently its output at a relative maximum. In contrast with music, even quiet music has energy that is in excess of 80 dB SPL with prolonged levels in excess of 100 dB SPL in many forms of music. <xref ref-type="table" rid="table1-1084713812468512">Table 1</xref> is from <xref ref-type="bibr" rid="bibr2-1084713812468512">Chasin (2006)</xref> and shows the sound level of some musical instruments measured on the horizontal plane from a distance of 3 meters. Higher levels would be measured if the assessment was measured at a different location, such as the violin player’s left ear.</p>
<table-wrap id="table1-1084713812468512" position="float">
<label>Table 1.</label>
<caption>
<p>Sound Levels of Some Musical Instruments Measured on the Horizontal Plane From a Distance of 3 Meters</p>
</caption>
<graphic alternate-form-of="table1-1084713812468512" xlink:href="10.1177_1084713812468512-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Musical instrument</th>
<th align="center">dBA ranges measured from 3 meters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cello</td>
<td>80-104</td>
</tr>
<tr>
<td>Clarinet</td>
<td>68-82</td>
</tr>
<tr>
<td>Flute</td>
<td>92-105</td>
</tr>
<tr>
<td>Trombone</td>
<td>90-106</td>
</tr>
<tr>
<td>Violin</td>
<td>80-90</td>
</tr>
<tr>
<td>Violin (near left ear)</td>
<td>85-105</td>
</tr>
<tr>
<td>Trumpet</td>
<td>88-108</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1084713812468512">
<p>Note: The table shows that higher levels would be measured if the assessment was measured at a different location, such as the violin player’s left ear.</p></fn>
<fn id="table-fn2-1084713812468512">
<p>Source: Adapted from <xref ref-type="bibr" rid="bibr2-1084713812468512">Chasin (2006)</xref>; courtesy of <italic>Hearing Review</italic>, used with permission.</p></fn>
</table-wrap-foot></table-wrap>
</sec>
<sec id="section6-1084713812468512">
<title>Crest Factors</title>
<p>Crest factors are used in every clinical audiology setting whenever a hearing aid is assessed in a test box (according to ANSI S3.22-2003; <xref ref-type="bibr" rid="bibr1-1084713812468512">American National Standards Institute [ANSI], 2003</xref>). Initially, the hearing aid is set to full on volume and the OSPL90 (output sound pressure level with 90 dB SPL input) is measured. Depending on the compression characteristics of the hearing aid, the volume control is then reduced to the reference test gain for testing of the frequency response and some other measures. The reference test gain is given as the OSPL90-77 dB. This 77 dB figure is derived from 65 dB SPL (average conversational level of speech at 1 meter) plus 12 dB. The 12 dB figure is the crest factor for speech using a 125-msec analysis window. The crest factor is the difference in decibels between the instantaneous peak of a signal and its average (or root mean square [RMS]) value. For speech signals of all languages the crest factor is given as 12 dB.</p>
<p>The human vocal tract is a highly damped structure with soft buccal walls, a narrow opening to a highly damped nasal cavity, a highly damped tongue and soft palate, and highly damped nostrils and lips (<xref ref-type="bibr" rid="bibr6-1084713812468512">Johnson, 2003</xref>). The difference of 12 dB between the instantaneous peak and its RMS value is a reflection of its highly damped structure. In contrast, a horn or a violin has less overall damping so its instantaneous peaks are much higher than the RMS of the generated sound—its crest factor is on the order of 18 to 20 dB. Adapted from <xref ref-type="bibr" rid="bibr3-1084713812468512">Chasin (2012a)</xref>, <xref ref-type="table" rid="table2-1084713812468512">Table 2</xref> shows some typical crest factors for speech and music using a 125-msec analyzing window.</p>
<table-wrap id="table2-1084713812468512" position="float">
<label>Table 2.</label>
<caption>
<p>Typical Crest Factors for Speech and Music Using a 125-msec Analyzing Window</p>
</caption>
<graphic alternate-form-of="table2-1084713812468512" xlink:href="10.1177_1084713812468512-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Stimulus</th>
<th align="center">Peak amplitude-total RMS power</th>
<th align="center">Crest factor</th>
</tr>
</thead>
<tbody>
<tr>
<td>Speech 1</td>
<td>–0.92-21.97</td>
<td>21.05</td>
</tr>
<tr>
<td>Speech 2</td>
<td>–5.53-17.99</td>
<td>12.46</td>
</tr>
<tr>
<td>Speech 3</td>
<td>–3.65-17.6</td>
<td>13.95</td>
</tr>
<tr>
<td>Music 1</td>
<td>–8.62-19.35</td>
<td>10.73</td>
</tr>
<tr>
<td>Music 2</td>
<td>–5.0-15.28</td>
<td>10.28</td>
</tr>
<tr>
<td>Music 3</td>
<td>–0.98-22.65</td>
<td>21.67</td>
</tr>
<tr>
<td>Music 4</td>
<td>–2.45-21.88</td>
<td>19.43</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-1084713812468512">
<p>Note: The table shows some typical crest factors for speech and music using a 125-msec analyzing window.</p></fn>
<fn id="table-fn4-1084713812468512">
<p>Source: Adapted from <xref ref-type="bibr" rid="bibr3-1084713812468512">Chasin (2012a)</xref>, courtesy of <italic>Audiology Practices</italic>. Reprinted with permission of the Academy of Doctors of Audiology.</p></fn>
</table-wrap-foot></table-wrap>
<p>Taking the values of <xref ref-type="table" rid="table1-1084713812468512">Table 1</xref> and <xref ref-type="table" rid="table2-1084713812468512">Table 2</xref> together (Average values + 18 dB crest factor), it is apparent that a musical input to a hearing aid is typically in excess of 100 dB SPL.</p>
<p>What can “music and hearing aids,” teach us about “speech and hearing aids”?</p>
<p>Traditional studies of crest factors of speech and hearing aids utilize a 125-msec analyzing window (<xref ref-type="bibr" rid="bibr5-1084713812468512">Cox, Mateisch, &amp; Moore, 1988</xref>; <xref ref-type="bibr" rid="bibr9-1084713812468512">Sivian &amp; White, 1933</xref>). This value of 125 msec was historically chosen since it is close to the temporal resolution of the human auditory system. However, when it comes to hearing aids we should be examining the input to the hearing aid rather than the output to the human auditory system. Hearing aid components are not limited to a 125-msec resolution and as such different limiting time constants should be used. <xref ref-type="table" rid="table3-1084713812468512">Table 3</xref> (<xref ref-type="bibr" rid="bibr4-1084713812468512">Chasin, 2012b</xref>) shows the crest factor of speech (from Sample 2 in <xref ref-type="table" rid="table2-1084713812468512">Table 2</xref>) but with different analyzing windows. For shorter windows the instantaneous peaks are greater with higher calculated crest factors. If we are looking at the input to a hearing aid, the crest factor of even speech is greater than 12 dB. The resulting speech signal, despite only having a maximum level of 85 dB SPL for the average spoken level of the vowel [a], when the crest factor is taken into consideration, the instantaneous peaks of speech can be in excess of 100 dB. When we examine a hard of hearing person’s own speech levels at the location of their own microphone, the input to the hearing aid can be even greater.</p>
<table-wrap id="table3-1084713812468512" position="float">
<label>Table 3.</label>
<caption>
<p>Crest Factor of Speech (From Sample 2 in <xref ref-type="table" rid="table2-1084713812468512">Table 2</xref>) but With Different Analyzing Windows</p>
</caption>
<graphic alternate-form-of="table3-1084713812468512" xlink:href="10.1177_1084713812468512-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Analysis window (msec)</th>
<th align="center">500</th>
<th align="center">400</th>
<th align="center">300</th>
<th align="center">200</th>
<th align="center">125</th>
<th align="center">100</th>
<th align="center">50</th>
<th align="center">25</th>
</tr>
</thead>
<tbody>
<tr>
<td>Crest factor (dB)</td>
<td>12.46</td>
<td>12.48</td>
<td>12.46</td>
<td>12.45</td>
<td>12.46</td>
<td>13.22</td>
<td>16.68</td>
<td>16.68</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-1084713812468512">
<p>Note: The table shows the crest factor of speech (from Sample 2 in <xref ref-type="table" rid="table2-1084713812468512">Table 2</xref>) but with different analyzing windows. For shorter windows, the instantaneous peaks are greater with higher calculated crest factors.</p></fn>
<fn id="table-fn6-1084713812468512">
<p>Source: Adapted Chasing (2012b); table courtesy of <italic>Hearing Review</italic>, used with permission.</p></fn>
</table-wrap-foot></table-wrap>
</sec>
<sec id="section7-1084713812468512">
<title>Hearing Aid Technology and Its Limitation for Music</title>
<p>The technical question arises whether inputs to a microphone of 110 dB SPL can be handled as well as inputs of 75 dB SPL. Since the late 1980s modern hearing aid microphones have been able to handle inputs of 115 dB SPL with minimal distortion. Modern digital-to-analog (D/A) converters also can handle mathematical values that are quite high. The limitation appears to be in the input compressor associated with the analog to digital (A/D) conversion process at the “front end of the hearing aid.”</p>
<p>Modern 16-bit hearing aids have a theoretical limitation of a dynamic range of its input of 96 dB. Because of many engineering and design compromises, the effective dynamic range is typically less than this. While a 96 dB dynamic range (e.g., 0 dB SPL to 96 dB SPL) is adequate for most speech (with the possible exception of a person’s own voice at the level of their own hearing aids), the upper limit of this range tends to be a limiting factor when it comes to music as an input.</p>
<p>If the input is distorted at the level of the A/D converter, then no amount of software reprogramming that occurs later in the circuit will improve the situation. This front-end distortion is perpetuated through the hearing aid. The best approach is to adjust the front-end elements of the hearing aid such that the more intense components of the music are within the operating range of the A/D converter.</p>
<p>In this issue, several articles are dedicated to just this approach. Some ingenious technologies already exist in the marketplace. These include compressing and then expanding the signal on either side of the A/D converter (in a similar vein as ducking under a low hanging door way), altering the input by the selection of a hearing aid microphone that has a different characteristic, and altering the 96 dB dynamic range from 0 dB SPL to 96 dB SPL, to 15 dB SPL to 111 dB SPL. Other approaches exist where an input to a direct audio input jack or an inductive input has been reduced by 10 to 12 dB by the use of a resistive circuit. Each of these technologies is based on the understanding of the limitations of modern A/D converters as well as the actual levels of the musical input to a hearing aid. And each of these technologies can be used with cochlear implants as well as hearing aids. This is an input-related issue and not an output or programming related issue. Only when the front end has been configured to be distortion free can a hearing aid be optimized for listening to and the playing of music.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author received no financial support for the research, authorship, and/or publication of this article.</p></fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1084713812468512">
<citation citation-type="book"><collab>American National Standards Institute</collab>. (<year>2003</year>). <source>American National Standard specification of hearing aid characteristics</source> (ANSI S3.22-2003). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr2-1084713812468512">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chasin</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Sound levels for musical instruments</article-title>. <source>Hearing Review</source>, <volume>13</volume>, <fpage>3</fpage>.</citation>
</ref>
<ref id="bibr3-1084713812468512">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chasin</surname><given-names>M.</given-names></name>
</person-group> (<year>2012a</year>). <article-title>Music and hearing aids</article-title>. <source>Audiology Practices</source>, <volume>4</volume>, <fpage>2</fpage>.</citation>
</ref>
<ref id="bibr4-1084713812468512">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chasin</surname><given-names>M.</given-names></name>
</person-group> (<year>2012b</year>). <article-title>Should all hearing aids have a -6 dB/octave microphone?</article-title> <source>Hearing Review</source>, <volume>19</volume>, <fpage>10</fpage>.</citation>
</ref>
<ref id="bibr5-1084713812468512">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cox</surname><given-names>R. M.</given-names></name>
<name><surname>Mateisch</surname><given-names>J. S.</given-names></name>
<name><surname>Moore</surname><given-names>J. N.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Distribution of short-term RMS levels in conversational speech</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>84</volume>, <fpage>1100</fpage>-<lpage>1104</lpage>.</citation>
</ref>
<ref id="bibr6-1084713812468512">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>K.</given-names></name>
</person-group> (<year>2003</year>). <source>Acoustic and auditory phonetics</source> (<edition>2nd ed.</edition>). <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Blackwell</publisher-name>.</citation>
</ref>
<ref id="bibr7-1084713812468512">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moore</surname><given-names>B. J.</given-names></name>
<name><surname>Fullgrabe</surname><given-names>C.</given-names></name>
<name><surname>Stone</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Determination of preferred parameters for multiband compression using individually fitted simulated hearing aids and paired comparisons</article-title>. <source>Ear and Hearing</source>, <volume>32</volume>(<issue>5</issue>), <fpage>556</fpage>-<lpage>568</lpage>.</citation>
</ref>
<ref id="bibr8-1084713812468512">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ricketts</surname><given-names>T. A.</given-names></name>
<name><surname>Dittberner</surname><given-names>A. B.</given-names></name>
<name><surname>Johnson</surname><given-names>E. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>High frequency amplification and sound quality in listeners with normal through moderate hearing loss</article-title>. <source>Journal of Speech-Language-Hearing Research</source>, <volume>51</volume>, <fpage>160</fpage>-<lpage>172</lpage>.</citation>
</ref>
<ref id="bibr9-1084713812468512">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sivian</surname><given-names>L. J.</given-names></name>
<name><surname>White</surname><given-names>S. D.</given-names></name>
</person-group> (<year>1933</year>). <article-title>On minimum audible sound fields</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>4</volume>, <fpage>288</fpage>-<lpage>321</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>