<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TAP</journal-id>
<journal-id journal-id-type="hwp">sptap</journal-id>
<journal-title>Theory &amp; Psychology</journal-title>
<issn pub-type="ppub">0959-3543</issn>
<issn pub-type="epub">1461-7447</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0959354311429854</article-id>
<article-id pub-id-type="publisher-id">10.1177_0959354311429854</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Significance tests as sorcery: Science is empirical—significance tests are not</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lambdin</surname><given-names>Charles</given-names></name>
</contrib>
<aff id="aff1-0959354311429854">Intel Corporation</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0959354311429854">Charles Lambdin, Intel Corporation-Ronler Acres, 2501 Northwest 229th Avenue, Hillsboro, OR 97124-5506, Mailstop RA1-222, USA. Email: <email>charles.g.lambdin@intel.com</email></corresp>
<fn fn-type="other" id="bio1-0959354311429854">
<p>Charles Lambdin is a human factors engineer at Intel. Address: Intel Corporation-Ronler Acres, 2501 Northwest 229th Avenue, Hillsboro, OR 97124-5506, Mailstop RA1-222, USA. Email: <email>charles.g.lambdin@intel.com</email></p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>22</volume>
<issue>1</issue>
<fpage>67</fpage>
<lpage>90</lpage>
<permissions>
<copyright-statement>© SAGE Publications 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Since the 1930s, many of our top methodologists have argued that significance tests are not conducive to science. Bakan (1966) believed that “everyone knows this” and that we slavishly lean on the crutch of significance testing because, if we didn’t, much of psychology would simply fall apart. If he was right, then significance testing is tantamount to psychology’s “dirty little secret.” This paper will revisit and summarize the arguments of those who have been trying to tell us—for more than 70 years—that <italic>p</italic> values are not empirical. If these arguments are sound, then the continuing popularity of significance tests in our peer-reviewed journals is at best embarrassing and at worst intellectually dishonest.</p>
</abstract>
<kwd-group>
<kwd>controversy</kwd>
<kwd>effect size</kwd>
<kwd>meta-analysis</kwd>
<kwd>null hypothesis</kwd>
<kwd>practical significance</kwd>
<kwd>replication</kwd>
<kwd>science</kwd>
<kwd>significance</kwd>
<kwd>statistics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>In 1972, Polish-British sociologist Stanislav Andreski published <italic>Social Sciences as Sorcery</italic>, a luminous work that is still one of the most vitriolic diagnoses ever assembled of everything wrong with the social sciences. One of Andreski’s key messages to the social science world is simple: real science is empirical, pseudo-science is not. Many social scientists make the claims they do, Andreski states, not because they have corroborated, diverse evidence supporting them as accurate descriptors of reality, but rather because they desire their opinions to <italic>become reality</italic>. This, Andreski argues, is shamanistic, not scientific. In this paper, the case is made that the social sciences (and particularly psychology) are rife with another kind of sorcery, a form of <italic>statistical</italic> shamanism—the test of “significance.”</p>
<p>According to Andreski, many social science publications constitute little more than a translating of platitudes into jargon, where sophisticated statistical lingo lends a specious scientific air to one’s pet hypotheses. I will here suggest that consequent to our generations-long obsession with <italic>p</italic> values and the statistical buffoonery which as a result passes for empirical research in our peer-reviewed journals, many psychologists are in fact guilty of what Andreski charges, and typically without even knowing it. Indeed, in the social sciences, the mindless ritual significance test is applied by researchers with little appreciation of its history and virtually no understanding of its actual meaning, and then—despite this alarming dearth of statistical insight—is held up as the hallmark of confirmatory evidence.</p>
<p>Methodologists have attempted to draw our attention to the foibles of significance tests for generations—indeed since well before our obsession with them even developed—and yet the fad persists, much to the severe detriment of the social sciences. This article chronicles many of the criticisms that have been leveled against “significance” testing and then comments on what the author feels is the most regrettable outcome of our observance of this null ritual, which is a vast and confused body of literature consisting largely of idiosyncratic results.</p>
<sec id="section1-0959354311429854">
<title>Psychology, <italic>p</italic> values, and science</title>
<p>These remarks are not intended to imply that the social sciences and science proper never overlap. Psychologists, for instance, certainly strive to be empirical, though whether psychology is a science is and has long been hotly debated (for an excellent discussion, see <xref ref-type="bibr" rid="bibr77-0959354311429854">Rosenberg, 1988</xref>). Within psychology, the debate often boils down to a bar fight between experimental and clinical psychologists, with the former assuming scientific status while denying it to the latter (e.g., <xref ref-type="bibr" rid="bibr23-0959354311429854">Dawes, 1994</xref>).</p>
<p>One of psychology’s greatest thinkers, Paul Meehl, was an outspoken clinician, researcher, and prolific author. In 1978, Meehl (in)famously noted that theories in psychology, like General MacArthur’s description of old generals, never really die; they just slowly fade away. After looking over 30 years of research, Meehl observed that theories in psychology do not accumulate as in the hard sciences; they are rather more akin to fads or trends that come into and go out of style. <xref ref-type="bibr" rid="bibr78-0959354311429854">Rosenthal (1993)</xref> echoes this sentiment, noting that in psychology “we seem to start anew with each succeeding volume of the psychological journals” (p. 519).</p>
<p>Despite such arguments, experimental psychologists typically maintain that their work is science <italic>because it is experimental</italic>. But are the methods they typically employ actually scientific? Scientific research, after all, is scientific because it is empirical, not because it is experimental. <xref ref-type="bibr" rid="bibr7-0959354311429854">Bakan (1974)</xref>, for instance, argues that experimentation in no way guarantees empiricism, adding that much of the research in psychology is not empirical precisely because of the experimentation employed.Thus, experimental ≠ empirical.</p>
<p>Indeed, there seems to be a lack of appreciation among some researchers (not to mention the media and the public) that the results of any study can be preordained by the selection of stimuli, how variables are operationally defined, the construction of the experimental protocol, the level at which the data are aggregated, or the particular analysis the researcher chooses to utilize. Psychologist Mary P. Koss, for instance, once famously claimed that 27.5% of college women are victims of rape or attempted rape. What Koss didn’t tell us is that her operational definition of “rape” was at odds with what the women surveyed actually believed. Going off the latter, only 4% of college women considered themselves rape victims (<xref ref-type="bibr" rid="bibr71-0959354311429854">Murray, Schwartz, &amp; Lichter, 2001</xref>). To take another example, whenever variables are controlled in a way that ignores their real-world interrelatedness, the spurious findings that emerge are not generalizable beyond the meretricious method used to artificially control the variables in question. <xref ref-type="bibr" rid="bibr12-0959354311429854">Brunswik (1952</xref>, <xref ref-type="bibr" rid="bibr13-0959354311429854">1956</xref>) and <xref ref-type="bibr" rid="bibr74-0959354311429854">Petrinovich (1979)</xref> famously argue that this fact alone casts doubt on a great deal of the research in psychology.</p>
<p>When <xref ref-type="bibr" rid="bibr7-0959354311429854">Bakan (1974)</xref>, however, warned us that the nature of our experimentation may actually be precluding psychological research from being truly empirical, he was referring to one practice in particular. In psychology we typically analyze our data in a way that often creates the impression of a finding that simply is not there (or that misses a finding that is). This ritualistic statistical observance is often called “null hypothesis significance testing” (NHST). There is nothing empirical, <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan (1966)</xref> argues, about formulating a hypothesis and then collecting data until a significant result is obtained. Indeed, using this as the sole, shoddy litmus of evidential discovery, one can easily ferret out “support” for almost any bias.</p>
<p>In 1972, Andreski observed that in the social sciences, “[i]n comparison with half a century ago, the average quality of the publications (apart from those which deal with techniques rather than substance) has declined” (p. 11). I would argue that today the situation has grown even worse. Social scientists, he argues, all too commonly employ a jargonized statistical analysis to smokescreen the seriously flawed reasoning underpinning their conclusions. He calls this practice “quantification as camouflage.”<sup><xref ref-type="fn" rid="fn1-0959354311429854">1</xref></sup>This had gotten so bad, Andreski thought, that the “quantophrenics” in psychology should take a break from learning statistical technique to study some elementary logic and analytic philosophy.<sup><xref ref-type="fn" rid="fn2-0959354311429854">2</xref></sup></p>
<p>Twenty-eight years after Andreski published his book, <xref ref-type="bibr" rid="bibr43-0959354311429854">Gigerenzer (2000)</xref> wrote that he spent an entire day and night in a library reading issues of the <italic>Journal of Experimental Psychology</italic> from the 1920s and 1930s. He became depressed because these 70-some-year-old articles made today’s efforts pale in comparison in regards to experimental design, methodology, and statistical thinking.</p>
</sec>
<sec id="section2-0959354311429854">
<title>Ranting to the wind?</title>
<p>In a recent article, <xref ref-type="bibr" rid="bibr5-0959354311429854">Armstrong (2007)</xref> points out that, contrary to popular belief, “there is no empirical evidence supporting the use of statistical significance tests. Despite repeated calls for evidence, no one has shown that the applications of tests of statistical significance improve decision making or advance scientific knowledge” (p. 335). He is by no means alone in arguing this. Many prominent researchers have now for decades protested NHST, arguing that it often results in the publication of peer-reviewed and journal endorsed pseudo-science. Indeed, this history of criticism now extends back more than 90 years (e.g., <xref ref-type="bibr" rid="bibr5-0959354311429854">Armstrong, 2007</xref>; <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan, 1966</xref>, <xref ref-type="bibr" rid="bibr7-0959354311429854">1974</xref>; <xref ref-type="bibr" rid="bibr8-0959354311429854">Berkson, 1938</xref>; <xref ref-type="bibr" rid="bibr10-0959354311429854">Boring, 1919</xref>; <xref ref-type="bibr" rid="bibr14-0959354311429854">Campbell, 1982</xref>; <xref ref-type="bibr" rid="bibr16-0959354311429854">Carver, 1978</xref>, <xref ref-type="bibr" rid="bibr17-0959354311429854">1993</xref>; <xref ref-type="bibr" rid="bibr20-0959354311429854">Cohen, 1990</xref>, <xref ref-type="bibr" rid="bibr22-0959354311429854">1994</xref>; <xref ref-type="bibr" rid="bibr27-0959354311429854">Edwards, 1965</xref>; <xref ref-type="bibr" rid="bibr29-0959354311429854">Falk, 1998</xref>; <xref ref-type="bibr" rid="bibr33-0959354311429854">Fidler, Thomason, Cumming, Finch, &amp; Leeman, 2004</xref>; <xref ref-type="bibr" rid="bibr38-0959354311429854">Fisher, 1955</xref>, <xref ref-type="bibr" rid="bibr39-0959354311429854">1956</xref>; <xref ref-type="bibr" rid="bibr41-0959354311429854">Gigerenzer, 1987</xref>, <xref ref-type="bibr" rid="bibr42-0959354311429854">1993</xref>, <xref ref-type="bibr" rid="bibr44-0959354311429854">2004</xref>; <xref ref-type="bibr" rid="bibr45-0959354311429854">Gigerenzer et al., 1989</xref>; <xref ref-type="bibr" rid="bibr46-0959354311429854">Gill, 1999</xref>; <xref ref-type="bibr" rid="bibr47-0959354311429854">Granaas, 2002</xref>; <xref ref-type="bibr" rid="bibr48-0959354311429854">Greenwald, 1975</xref>; <xref ref-type="bibr" rid="bibr54-0959354311429854">Hubbard &amp; Armstrong, 2006</xref>; <xref ref-type="bibr" rid="bibr55-0959354311429854">Hubbard &amp; Lindsay, 2008</xref>; <xref ref-type="bibr" rid="bibr56-0959354311429854">Hubbard &amp; Ryan, 2000</xref>; <xref ref-type="bibr" rid="bibr58-0959354311429854">Jones &amp; Tukey, 2000</xref>; <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>, <xref ref-type="bibr" rid="bibr60-0959354311429854">2003</xref>; <xref ref-type="bibr" rid="bibr62-0959354311429854">Lindsay, 1995</xref>; <xref ref-type="bibr" rid="bibr64-0959354311429854">Lykken, 1968</xref>, <xref ref-type="bibr" rid="bibr65-0959354311429854">1991</xref>; <xref ref-type="bibr" rid="bibr67-0959354311429854">Meehl, 1967</xref>, <xref ref-type="bibr" rid="bibr68-0959354311429854">1978</xref>, <xref ref-type="bibr" rid="bibr69-0959354311429854">1990</xref>; <xref ref-type="bibr" rid="bibr72-0959354311429854">Nunnally, 1960</xref>; <xref ref-type="bibr" rid="bibr79-0959354311429854">Rosnow &amp; Rosenthal, 1989</xref>; <xref ref-type="bibr" rid="bibr80-0959354311429854">Rozeboom, 1960</xref>; <xref ref-type="bibr" rid="bibr84-0959354311429854">Schmidt, 1992</xref>, <xref ref-type="bibr" rid="bibr85-0959354311429854">1996</xref>; <xref ref-type="bibr" rid="bibr86-0959354311429854">Schmidt &amp; Hunter, 1997</xref>; <xref ref-type="bibr" rid="bibr88-0959354311429854">Sedlmeier &amp; Gigerenzer, 1989</xref>; <xref ref-type="bibr" rid="bibr90-0959354311429854">Skinner, 1972</xref>; <xref ref-type="bibr" rid="bibr94-0959354311429854">Thompson, 1996</xref>, <xref ref-type="bibr" rid="bibr95-0959354311429854">1999</xref>, <xref ref-type="bibr" rid="bibr96-0959354311429854">2002</xref>, <xref ref-type="bibr" rid="bibr98-0959354311429854">2006</xref>, <xref ref-type="bibr" rid="bibr99-0959354311429854">2007</xref>; <xref ref-type="bibr" rid="bibr102-0959354311429854">Tukey, 1991</xref>).</p>
<p>Despite a long line of top researchers making such points (e.g., <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>, <xref ref-type="bibr" rid="bibr60-0959354311429854">2003</xref>), despite the fact that many methodologists will admit such things in conversation, the use of NHST is still standard practice in the social sciences. In his outgoing comments as editor of the <italic>Journal of Applied Psychology</italic>, <xref ref-type="bibr" rid="bibr14-0959354311429854">Campbell (1982)</xref> wrote:</p>
<p><disp-quote>
<p>It is almost impossible to drag authors away from their <italic>p</italic> values, and the more zeros after the decimal point, the harder people cling to them. It is almost as if all the statistics courses in the world stopped after introducing Type I error. …Perhaps <italic>p</italic> values are like mosquitos. They have an evolutionary niche somewhere and no amount of scratching, swatting, or spraying will dislodge them. Whereas it may be necessary to discount a sampling error explanation for results of a study, investigators must learn to argue for the significance of their results without reference to inferential statistics. (p. 698)</p>
</disp-quote></p>
<p>Nineteen years later, <xref ref-type="bibr" rid="bibr34-0959354311429854">Finch, Cumming, and Thomason (2001)</xref> noted that little had changed in almost two decades.</p>
<p><disp-quote>
<p>Why has [statistical] reform proceeded further in some other disciplines, including medicine, than in psychology? … What happened in psychology was not inevitable. We leave to historians and sociologists of science the fascinating and important question of why psychology has persisted for so long with poor statistical practice. (pp. 205–206)</p>
</disp-quote></p>
<p>Thus, to make the same points now that have been made repeatedly for decades is not, by any means, to beat a dead horse. A horse that is winning the race is not dead. The fact that this horse is still in the lead unfortunately suggests that the long history of authors making such observations have largely been ranting to the wind.</p>
<p>So what exactly is wrong with significance testing? It certainly has its defenders after all (e.g., <xref ref-type="bibr" rid="bibr25-0959354311429854">Dixon, 1998</xref>; <xref ref-type="bibr" rid="bibr50-0959354311429854">Hagen, 1997</xref>; <xref ref-type="bibr" rid="bibr70-0959354311429854">Mulaik, Raju, &amp; Harshman, 1997</xref>), but its defenders fail to address all of the complaints chronicled herein. Furthermore, their supportive arguments in no way get us around that fact that NHST does not lend objectivity to the process of making inferences (<xref ref-type="bibr" rid="bibr5-0959354311429854">Armstrong, 2007</xref>; <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan, 1966</xref>; <xref ref-type="bibr" rid="bibr16-0959354311429854">Carver, 1978</xref>; <xref ref-type="bibr" rid="bibr22-0959354311429854">Cohen, 1994</xref>; <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>; <xref ref-type="bibr" rid="bibr102-0959354311429854">Tukey, 1991</xref>), a fact which led <xref ref-type="bibr" rid="bibr95-0959354311429854">Thompson (1999)</xref> to dub significance testing “pseudo-objectivity.”</p>
<p>Even more troubling, significance testing creates in the minds of researchers the impression of automating the difficult process of thinking through inferences, seemingly reducing the complex notion of scientific support to the mindless task of an assembly line inspector, stamping “accept” or “reject” on every good that is rolled along. This</p>
<p><disp-quote>
<p>practice of focusing exclusively on a dichotomous reject–non reject decision strategy of null hypothesis testing can actually impede scientific progress. … [F]ocusing on <italic>p</italic> values and rejecting null hypotheses actually distracts us from our real goals: deciding whether data support our scientific hypotheses and are practically significant. The focus of research should be on our scientific hypotheses, what data tell us about the magnitude of effects, the practical significance of effects, and the steady accumulation of knowledge. (<xref ref-type="bibr" rid="bibr60-0959354311429854">Kirk, 2003</xref>, p. 100)</p>
</disp-quote></p>
<p>Journal editors, however, probably like this use of significance tests because it doubtlessly makes their jobs easier (<xref ref-type="bibr" rid="bibr63-0959354311429854">Loftus, 1991</xref>). It makes the jobs of researchers easier as well. I would go so far as to say that many who today call themselves “scientists” would be unable to do so without the crutch of NHST to lean on. Just as most would rather take a weight-loss pill than have to diet and exercise, researchers seem all too readily lulled into a “false sense of science” by the convenience of clicking a few buttons and checking whether <italic>p</italic> &lt; .05. The alternative is work—a lot of hard thinking and critical reasoning.</p>
<p>It is certainly a “significant” problem for the social sciences that significance tests do not actually tell researchers what the overwhelming majority of them think they do (<xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan, 1966</xref>). Bakan thought that “everybody knows this” and that to say it out loud is to be like the child who pointed out that the emperor is wearing no clothes. He argues that if we pull out the strand of NHST much of the tapestry of psychology would fall apart. Indeed, NHST, <xref ref-type="bibr" rid="bibr40-0959354311429854">Gerrig and Zimbardo (2002)</xref> state, is the “backbone of psychological research” (p. 46). So, instead of abandoning it, which could be very embarrassing, we make the adjustment of simply misinterpreting what it actually tells us, which is … not much.</p>
</sec>
<sec id="section3-0959354311429854">
<title>Significance testing and the infertility of psychology</title>
<p><xref ref-type="bibr" rid="bibr67-0959354311429854">Meehl (1967)</xref> likens NHST to a sterile intellectual rake who ravishes maidens but creates no viable scientific offspring. <xref ref-type="bibr" rid="bibr22-0959354311429854">Cohen (1994)</xref> jokingly states he had to resist the temptation to call NHST “statistical hypothesis inference testing,” which presumably would have yielded a more appropriate acronym. Countless methodologists tell us we should be focusing on effect sizes and confidence intervals, which—though both built on the foundation of significance testing—are together far more informative and meaningful. Even <xref ref-type="bibr" rid="bibr35-0959354311429854">Fisher (1925)</xref> himself stated that ANOVA <italic>p</italic> values should be supplemented with η (see also <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>). If so many of our best methodologists tell us we should be focusing on confidence intervals and effect sizes, why don’t we listen?</p>
<p>Much of the blame lies with our journals, our statistics texts, and our graduate school statistics courses. The NHST orthodoxy is not only required for publication in most journals but its tenets continue to be proselytized from the pulpits of graduate school classrooms across the fruited plain. This trend has continued unabated since the 1960s. In 1972, B.F. Skinner complained that graduate schools teach “statistics in lieu of scientific method” (p. 319). This curriculum, he argued, is</p>
<p><disp-quote>
<p>incompatible with some major features of laboratory research. As now taught, statistics plays down the direct manipulation of variables and emphasizes the treatment of variables after the fact. If the graduate student’s first result is not significant, statistics tells him to increase his sample size. (p. 319)</p>
</disp-quote></p>
<p>And,</p>
<p><disp-quote>
<p>What statisticians call experimental design (I have pointed out elsewhere that this means design which yields data to which the methods of statistics are appropriate) usually generates a much more intimate acquaintance with a calculating machine than with a behaving organism. (p. 320)</p>
</disp-quote></p>
<p>There are, of course, exceptions to this. A few journals have certainly striven to turn the tide. Bruce Thompson, for instance, has long campaigned for journals to require the reporting of effect sizes. A famous exception is found in the remarks of the great statistician William Kruskal, who, in response to an author using <italic>p</italic> values to assess the importance of differences, once wrote:</p>
<p><disp-quote>
<p>So I’m sorry that this ubiquitous practice received the accolade of use by you and your distinguished coauthors. I am thinking these days about the many senses in which relative importance gets considered. Of these senses, some seem reasonable and others not so. Statistical significance is low on my ordering. Do forgive my bluntness. (as cited in <xref ref-type="bibr" rid="bibr11-0959354311429854">Bradburn, 2007</xref>, p. 263)</p>
</disp-quote></p>
<p>Such exceptions to the norm are important, but are still too few and far between. Social science journals must require the use of effect sizes and the comparison of confidence intervals. Statistics texts must quit teaching NHST and ignorantly misrepresenting it as a happy compromise between Fisher, Neyman, and Pearson. Researchers must quit exploiting public trust in the social sciences by employing the <italic>p</italic> &lt; α pseudo-litmus of empirical support to gain publications and continued paychecks.</p>
<p>The reason many researchers are so hesitant to comply is, <xref ref-type="bibr" rid="bibr22-0959354311429854">Cohen (1994)</xref> suggests, that in psychology most confidence intervals are embarrassingly large. By not reporting miniscule effect sizes or hiding the girth of our confidence intervals, we can present our simple, nonsensical <italic>p</italic> &lt; .05 results while keeping our uninformed readers wholly in the dark regarding the actual size, nature, and practical significance (or lack thereof) of the effects in question (<xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>). To give an example, it may seem impressive (but unsurprising) to state there is a significant correlation between being religious and having religious parents, but it is less impressive (and more surprising) if it is pointed out that for this statistic, <italic>N</italic> = 2084 and <italic>r</italic><sup>2</sup> = .01 (<xref ref-type="bibr" rid="bibr89-0959354311429854">Shermer, 2000</xref>)!</p>
<p>This example brings to mind <xref ref-type="bibr" rid="bibr64-0959354311429854">Lykken (1968)</xref>, who argues that many correlations in psychology have effect sizes so small that it is questionable whether they constitute actual relationships above the “ambient correlation noise” that is always present in the real world. <xref ref-type="bibr" rid="bibr9-0959354311429854">Blinkhorn and Johnson (1990)</xref> persuasively argue, for instance, that a shift away from “culling tabular asterisks” in psychology would likely cause the entire field of personality testing to disappear altogether. Looking at a table of results and highlighting which ones are significant is, after all, akin to throwing quarters in the air and noting which ones land heads.</p>
<p>As a slight aside, some might here object to the author’s use of correlations as examples, claiming that correlational research is not experimental and therefore not scientific. Because of this (and because of a reviewer for another journal making just this argument), the author would like to point out that this claim is patently false (see <xref ref-type="bibr" rid="bibr18-0959354311429854">Cattell, 1978</xref>). Experimental design ≠ statistical analysis.What is usually meant by this argument is that <italic>passive observational studies</italic> are not experimental, not that “correlational” studies are not experimental. To misstate this is to misinform.</p>
<p>A true experiment must have three things: (a) random assignment of cases to levels; (b) manipulation of the levels of at least one independent variable (if you do not manipulate a variable, you cannot randomly assign cases to levels); and (c) control of extraneous variables (<xref ref-type="bibr" rid="bibr92-0959354311429854">Tabachnick &amp; Fidell, 2001</xref>). You <italic>can</italic> do these three things and then follow the design with a correlational analysis. And you might not manipulate a variable and then analyze your data using ANOVA. In the former case, a true experiment has been conducted (despite the use of correlation), and in the latter, the design is not experimental (despite the fact that an ANOVA was run). (This confusion becomes even more comical when one realizes that regression and ANOVA are basically the same thing anyway.)</p>
</sec>
<sec id="section4-0959354311429854">
<title>Clearing the weeds so that something healthy might grow</title>
<p>It has been stated that NHST does not tell us what most researchers think it does. So what are the misconceptions? <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan (1966)</xref> and <xref ref-type="bibr" rid="bibr94-0959354311429854">Thompson (1996</xref>, <xref ref-type="bibr" rid="bibr95-0959354311429854">1999</xref>) catalogue some of the most common:</p>
<list id="list1-0959354311429854" list-type="order">
<list-item><p>A <italic>p</italic> value is the probability the results will replicate if the study is conducted again (false).</p></list-item>
<list-item><p>We should have more confidence in <italic>p</italic> values obtained with larger <italic>N</italic>s than smaller <italic>N</italic>s (this is not only false but backwards).</p></list-item>
<list-item><p>A <italic>p</italic> value is a measure of the degree of confidence in the obtained result (false).</p></list-item>
<list-item><p>A <italic>p</italic> value automates the process of making an inductive inference (false, you still have to do that yourself—and most don’t bother).</p></list-item>
<list-item><p>Significance testing lends objectivity to the inferential process (it really doesn’t).</p></list-item>
<list-item><p>A <italic>p</italic> value is an inference from population parameters to our research hypothesis (false, it is only an inference from sample statistics to population parameters).</p></list-item>
<list-item><p>A <italic>p</italic> value is a measure of the confidence we should have in the veracity of our research hypothesis (false).</p></list-item>
<list-item><p>A <italic>p</italic> value tells you something about the members of your sample (no it doesn’t).</p></list-item>
<list-item><p>A <italic>p</italic> value is a measure of the validity of the inductions made based on the results (false).</p></list-item>
<list-item><p>A <italic>p</italic> value is the probability the null is true (or false) given the data (it is not).</p></list-item>
<list-item><p>A <italic>p</italic> value is the probability the alternative hypothesis is true (or false; this is false).</p></list-item>
<list-item><p>A <italic>p</italic> value is the probability that the results obtained occurred due to chance (very popular but nevertheless false).</p></list-item>
</list>
<p>In their defense of significance testing, <xref ref-type="bibr" rid="bibr70-0959354311429854">Mulaik et al. (1997)</xref> argue that such misconceptions about NHST are irrelevant as to whether we should continue its use. This is an awkward position. The point that Mulaik et al. seem to be missing is that methodologists are not trying to refute NHST when they write of how it is commonly misconstrued. The case made in this context is that the very concept of a <italic>p</italic> value is so seldom accurately grasped that the deleterious impact on the quality of research of which the social sciences are comprised is undoubtedly great—so much so that it should be quite uncontroversial to state that most published research is in fact nonsense. The great philosopher of science Imre Lakatos considered most published research in the social sciences to be little more than “intellectual pollution” (reported by <xref ref-type="bibr" rid="bibr69-0959354311429854">Meehl, 1990</xref>) and <xref ref-type="bibr" rid="bibr57-0959354311429854">Ioannidis (2005)</xref> argues that the spread of the <italic>p</italic> value from psychology to other fields has resulted in a world where, even in medicine, most published findings are probably wrong.</p>
<p><xref ref-type="bibr" rid="bibr70-0959354311429854">Mulaik et al.’s (1997)</xref> point is still a sound observation. A proper response, however, is not to continue with our blind use of NHST, but to do something about it. Before proceeding, let us state just what exactly a <italic>p</italic> value tells us. <italic>A p value is the probability of obtaining the results in hand, assuming that the statistical null hypothesis is true in the population</italic>. That is all and nothing more. As <xref ref-type="bibr" rid="bibr87-0959354311429854">Schopenhauer (1851/2004)</xref> reminds us, nothing more is implied by a premise than what is already contained in it, and this, it is time we admit, does not imply much.</p>
</sec>
<sec id="section5-0959354311429854">
<title>Faddish falsities</title>
<p>Let us now attempt to extirpate some of these statistical delusions so that our attention may then be focused on NHST’s actual flaws. This is no easy feat. <xref ref-type="bibr" rid="bibr54-0959354311429854">Hubbard and Armstrong (2006)</xref> argue that the misconceptions regarding significance testing among researchers are wider and deeper than even the critics appreciate. Indeed, there is undeniably something incredibly amiss when the APA’s own Task Force on Statistical Inference, formed to address this very problem, itself embarrassingly misses the fact that Fisher’s evidential statistic (<italic>p</italic> value) and Neyman–Pearson’s error estimate (α) are not in any meaningful way combined when stating that “<italic>p</italic> &lt; .05” (<xref ref-type="bibr" rid="bibr53-0959354311429854">Hubbard, 2004</xref>; <xref ref-type="bibr" rid="bibr104-0959354311429854">Wilkinson &amp; The TSFI, 1999</xref>).</p>
<p>The most common and destructive delusions are, in my opinion, that <italic>p</italic> values somehow tell you (a) the odds your data are due to chance, (b) the odds your research hypothesis is correct, (c) the odds your result will replicate, and (d) the odds the null is true. Let us now take a closer look at each of these falsities.</p>
<sec id="section6-0959354311429854">
<title>The odds your data are due to chance</title>
<p>Even after reading articles such as this one, most researchers simply file it away as “interesting” and then go right on treating NHST as though it somehow tells them the odds of their results. This is likely the most common misconception regarding <italic>p</italic> values. The statement that if a result is significant, its odds of occurring due to chance are only 1 out of 20, or 5 out of 100, seems omnipresent and yet is wholly false. <xref ref-type="bibr" rid="bibr16-0959354311429854">Carver (1978)</xref> calls this the “odds-against-chance fantasy.” Of this misconception, <xref ref-type="bibr" rid="bibr70-0959354311429854">Mulaik et al. (1997)</xref> comment that “[i]t is hard to imagine how someone would arrive at this conclusion” (p. 74) and then go on to speculate what process of reasoning might lead one there. For my own part, I doubt that many reason their way to this conclusion. Many were simply taught this interpretation and then—it itself sounding so plausible on its face—few likely go on to question it.</p>
<p>Indeed, this misconception is widely taught in graduate school classes and can easily be found asserted by prominent thinkers (e.g., <xref ref-type="bibr" rid="bibr3-0959354311429854">Anastasi, 1976</xref>; <xref ref-type="bibr" rid="bibr51-0959354311429854">Hebb, 1966</xref>; <xref ref-type="bibr" rid="bibr73-0959354311429854">Paulos, 2003</xref>). What matters, however, is the fact that it is so widely held, regardless of why. <xref ref-type="bibr" rid="bibr16-0959354311429854">Carver (1978</xref>, 1993) believed this misconception to be widespread in education and <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan (1966)</xref> believed it predominant in psychology. I would argue that this is still true and would also submit that I have seen this misconception prevalent in both academia and industry. Quite embarrassingly, even a reviewer of this article for a leading methodology journal (though not the one you are currently reading) objected to this very discussion, insisting that <italic>p</italic> values do indeed tell one the odds of one’s data occurring due to chance.</p>
<p>As <xref ref-type="bibr" rid="bibr29-0959354311429854">Falk (1998)</xref> and <xref ref-type="bibr" rid="bibr31-0959354311429854">Falk and Greenbaum (1995)</xref> observe, this misconception is derived from a legitimate concern. If your research results are based on a random sample (or participants have been randomly assigned to levels), then the worry can and should arise that the results obtained may in fact be a fluke. NHST attempts to address this concern—the problem is that it fails. And fail it must, as <xref ref-type="bibr" rid="bibr16-0959354311429854">Carver (1978)</xref> succinctly points out.</p>
<p>A <italic>p</italic> value does not and cannot tell you the odds your results are due to chance, because it is calculated based on the assumption that this probability is already 100%. In other words, a <italic>p</italic> value tells you the odds of your results given that you assume they are in fact due to chance. This brings back to mind the Andreskian admonition that less statistics classes and more courses in simple analytic philosophy are in order. It certainly seems that one of the most robust empirical conclusions collectively reached by the social sciences is that the overwhelming majority of social scientists do not know what a conditional probability is.</p>
<p>The proper use of a <italic>p</italic> value is to assist in deciding whether the probability is in fact 100% that your results are due to chance. Perhaps then the best wording for a low <italic>p</italic> value is simply to state: “Assuming my results are due to chance, my obtained mean difference is very unlikely. Therefore chance may not be the culprit. Now it is up to me to employ <italic>other methods</italic> to determine what that culprit might be.”</p>
</sec>
<sec id="section7-0959354311429854">
<title>The odds your research hypothesis is correct</title>
<p>We have seen that experimental ≠ empirical and that experimental design ≠ statistical analysis. To properly interpret <italic>p</italic> values, one must also keep in mind both that statistical hypotheses ≠ research hypotheses and that P(D|H<sub>0</sub>) ≠ P(H<sub>1</sub>|D). This seems obvious when read as an explicit statement, but not when reading the discussion sections of psychology papers. The equation, P(D|H<sub>0</sub>) ≠ P(H<sub>1</sub>|D), implies that a low probability of a result (or data) given the truth of the null does not indicate the probability of the alternative given the data. Such a misstatement all too readily lends itself to the erroneous belief that rejecting the null indicates that your treatment works. That the treatment does not work is not your null hypothesis and that the treatment works is not the statistical alternative. Your null is likely µ<sub>A</sub> - µ<sub>B</sub> = 0 and your alternative is likely µ<sub>A</sub> - µ<sub>B</sub> ≠ 0. Rejecting H<sub>0</sub> implies only that µ<sub>A</sub> - µ<sub>B</sub> ≠ 0, not that your treatment works. There are virtually an infinite number of reasons why µ<sub>A</sub> - µ<sub>B</sub> ≠ 0.</p>
<p>With a null of no difference and an α of .05, what a significant result indicates is that you would find the difference obtained less than 5% of the time if in reality µ<sub>A</sub> - µ<sub>B</sub> = 0. The null refers only to the statistical hypothesis. That the treatment works is the research hypothesis. As <xref ref-type="bibr" rid="bibr47-0959354311429854">Granaas (2002)</xref> reminds us, the rejection of any nil hypothesis (a null of no difference) supports <italic>all</italic> research hypotheses predicting an effect, <italic>not just yours</italic>—and there may be an infinite number of explanations for the effect in question. In fact, all significance here tells you is that you are justified in proceeding to test your research hypothesis, not that your research hypothesis is supported. And yet this fantasy leads to many articles being accepted for publication whenever <italic>p</italic> values are erroneously taken to suggest the research hypothesis is likely correct when the methodology and hypotheses involved are themselves doubtful (<xref ref-type="bibr" rid="bibr15-0959354311429854">Carver, 1976</xref>, <xref ref-type="bibr" rid="bibr16-0959354311429854">1978</xref>; <xref ref-type="bibr" rid="bibr64-0959354311429854">Lykken, 1968</xref>).</p>
<p>A related error, and a comical one at that, occurs whenever one sees talk of <italic>p</italic> values measuring “degrees of significance.” If a <italic>p</italic> &lt; .05 result is “significant,” then a <italic>p</italic> = .067 result is not “marginally significant.” Similarly, if α is fixed in advanced at .05 (as it typically is), then it is nonsensical to say that a <italic>p</italic> &lt; .001 result is “highly significant.” It is either significant or it is not. <xref ref-type="bibr" rid="bibr54-0959354311429854">Hubbard and Armstrong (2006)</xref> conducted a survey of marketing journals and found that 54.9% of articles examined committed this error.</p>
<p>Supporting a research hypothesis against all competing rival hypotheses which explain a given effect is not something significance testing can help with. Such support (a.k.a., scientific support) is gained only after meticulous theorizing, sound methodology, and numerous replications lead to diverse, corroborating evidence demonstrating the effect in a variety of situations (<xref ref-type="bibr" rid="bibr16-0959354311429854">Carver, 1978</xref>).</p>
</sec>
<sec id="section8-0959354311429854">
<title>The odds your result will replicate</title>
<p>It is an illusion to think one can learn anything about the replicability of a finding from a <italic>p</italic> value. Fisher himself was well aware of this fact (e.g., <xref ref-type="bibr" rid="bibr36-0959354311429854">Fisher, 1929</xref>; see also <xref ref-type="bibr" rid="bibr82-0959354311429854">Salsburg, 2001</xref>; <xref ref-type="bibr" rid="bibr102-0959354311429854">Tukey, 1991</xref>), though many of those who have adopted the idea of significance testing seem to have altogether forgotten it. As <xref ref-type="bibr" rid="bibr97-0959354311429854">Thompson (2003)</xref> states</p>
<p><disp-quote>
<p>If <italic>p</italic> calculated informed the researcher about the truth of the null in the population, then this information would directly test the replicability of results. … Unfortunately, this is not what statistical significance tests, and not what the associated <italic>p</italic> calculated values evaluate. (p. 96)</p>
</disp-quote></p>
<p>Consequently, it is entirely false to state or imply that 1 – <italic>p</italic> = the probability that the results are replicable/reliable (<xref ref-type="bibr" rid="bibr16-0959354311429854">Carver, 1978</xref>).</p>
<p>As <xref ref-type="bibr" rid="bibr78-0959354311429854">Rosenthal (1993)</xref> observes, if there is a real effect in nature with a <italic>d</italic> of .5 (<italic>r</italic> = .24) and a researcher conducts a study with an <italic>N</italic> of 64 (and so the power of the study is .5—a typical power level in psychology), and then someone else replicates this study, there is only a one in four chance that both researchers will find that <italic>p</italic> &lt; .05, even though the effect is real. If three more researchers replicate the study, there is only a 50/50 chance that three or more of the studies will find a significant result. This is obviously not conducive to the accumulation of knowledge. This is unfortunate in that replication is greatly needed and of the utmost importance in psychology. As <xref ref-type="bibr" rid="bibr65-0959354311429854">Lykken (1991)</xref> points out, when we bother to look, many of psychology’s findings actually do not replicate.</p>
<p>What matters in psychology is preponderance of evidence. And if one takes Meehl’s neo-Popperian/neo-Lakatosian view (and there is much to be said for this view), then—stemming both from its rejection of strict falsificationism and from the observation that in psychology all theories are technically false (in that they are incomplete)—what also matters is (a) “money in the bank” and (b) “damn strange coincidences” (<xref ref-type="bibr" rid="bibr69-0959354311429854">Meehl, 1990</xref>). In other words, since no psychological theory can ever be “the whole truth,” they should seek to earn a sort of “good enough verisimilitude” to warrant our continuing to entertain them. This is done, <xref ref-type="bibr" rid="bibr69-0959354311429854">Meehl (1990)</xref> argues, by their accruing “money in the bank” via Wesley Salmon’s notion of “damn strange coincidences” (<xref ref-type="bibr" rid="bibr81-0959354311429854">Salmon, 1984</xref>). As <xref ref-type="bibr" rid="bibr69-0959354311429854">Meehl (1990)</xref> argues, “<italic>The main way a theory gets money in the bank is by predicting facts that, absent the theory, would be antecedently improbable</italic>” (p. 115). The role of significance tests in this process is “minor and misleading” (p. 115). Indeed, psychology’s track record at making predictions is extremely embarrassing. As many know, predictions made by expert psychologists typically do not outperform those made by laypersons (see, e.g., <xref ref-type="bibr" rid="bibr32-0959354311429854">Faust &amp; Ziskin, 1988</xref>).</p>
<p>If replication had been stressed by our journal editors over and above the specious requirement of small <italic>p</italic> values, things might today be different. Sadly, journals typically want “original” ideas, not replications, even though it is likely that the impracticable expectation that every researcher be “original” only encourages intellectual dishonesty and dilutes the overall quality of published research as a whole (<xref ref-type="bibr" rid="bibr4-0959354311429854">Andreski, 1972</xref>).<sup><xref ref-type="fn" rid="fn3-0959354311429854">3</xref></sup> Further, a sound replication that does not achieve statistical significance is not likely to be published anyway, even though the results of a study—so long as its methodology is sound—should be irrelevant as to whether it gets published (<xref ref-type="bibr" rid="bibr66-0959354311429854">Mahoney, 1977</xref>).</p>
</sec>
<sec id="section9-0959354311429854">
<title>The odds the null is true</title>
<p>It should be remembered that science and significance testing do not ask the same question. A <italic>p</italic> value is the probability of the results in hand assuming that the null is true in the population. Many researchers get this definition backwards: a <italic>p</italic> value does not tell us the probability that the null is true in the population given the results (which would be science). Thus, the results of significance tests are typically lent an entirely inappropriate Bayesian interpretation.Those who defend NHST are typically guilty of this error in elementary logic (<xref ref-type="bibr" rid="bibr22-0959354311429854">Cohen, 1994</xref>).</p>
<p>This is likely due to wishful thinking. As scientists, what we should be interested in is the Bayesian probability the null hypothesis is correct given the evidence or data, P(H<sub>0</sub>|D), not the odds of obtaining the data we did (or more extreme data) assuming the null is true in the population, P(D|H<sub>0</sub>). Significance tests can only tell us the latter. Unfortunately, P(D|H<sub>0</sub>) ≠ P(H<sub>0</sub>|D).</p>
<p>To illustrate, take the probability of an abused child having nightmares, P(N|A) and the probability that a child who has nightmares is abused, P(A|N). Clearly P(N|A) ≠ P(A|N): that is, knowing that abused children are likely to have nightmares does not imply that a child who has nightmares is likely abused (<xref ref-type="bibr" rid="bibr24-0959354311429854">Dawes, 2001</xref>). Knowing the odds are low of the mean difference obtained given the assumed truth of the statistical null does not imply that the null is likely false given the evidence. Ignoring this is described by <xref ref-type="bibr" rid="bibr31-0959354311429854">Falk and Greenbaum (1995)</xref> as the “illusion of probabilistic proof by contradiction” and by <xref ref-type="bibr" rid="bibr42-0959354311429854">Gigerenzer (1993)</xref> as the “permanent illusion.”</p>
<p>It is true, however, that knowing P(D|H<sub>0</sub>) can <italic>influence</italic> P(H<sub>0</sub>|D). Though A→B ≠ B→A is a true statement, so is the following: If A↑B denotes the situation P(B|A) &gt; P(B), then A↑B→B↑A. In terms of significance testing, if P(D|H<sub>0</sub>) &lt; P(D), then showing that the probability of our data is low given the assumed truth of the null (i.e., a low <italic>p</italic> value) <italic>does reduce</italic> the conditional probability of the null given the data, P(H<sub>0</sub>|D). The problem is that it does not indicate P(H<sub>0</sub>|D) as a precise probability (<xref ref-type="bibr" rid="bibr30-0959354311429854">Falk, 2008</xref>). In other words, though it is true that if P(D|H<sub>0</sub>) &lt; P(D), then D↓H<sub>0</sub> → H<sub>0</sub>↓D, this in no way indicates P(D|H<sub>0</sub>) = P(H<sub>0</sub>|D) (<xref ref-type="bibr" rid="bibr30-0959354311429854">Falk, 2008</xref>). As noted above, NHST tries to address a legitimate concern. The problem is that it fails to address it. Demonstrating that P(D|H<sub>0</sub>) is low may indeed reduce P(H<sub>0</sub>|D), but it does not demonstrate that P(H<sub>0</sub>|D) <italic>is also low</italic>, which is what (as scientists) we would be interested in seeing (<xref ref-type="bibr" rid="bibr16-0959354311429854">Carver, 1978</xref>; <xref ref-type="bibr" rid="bibr22-0959354311429854">Cohen, 1994</xref>; <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>).</p>
<p>Returning to the example above, if the probability that an abused child will have nightmares is greater than the unconditional probability of children having nightmares, and if the probability that an abused child will have nightmares is high, this does increase the probability that a child who has nightmares has also been abused, but it in no way indicates that it is in any way likely. It is still doubtlessly very unlikely that a child who has nightmares is also an abuse victim. This can easily be seen by inspecting Venn diagrams (see <xref ref-type="fig" rid="fig1-0959354311429854">Figure 1</xref>).</p>
<fig id="fig1-0959354311429854" position="float">
<label>Figure 1.</label>
<caption>
<p>Venn diagram: Abuse and nightmares.</p>
</caption>
<graphic xlink:href="10.1177_0959354311429854-fig1.tif"/>
</fig>
<p>In A, abuse and nightmares barely overlap, which means fewer children who have nightmares are also abused. In B, the overlap is more pronounced, which means, compared to A, more children who have nightmares are necessarily also abuse victims, but the overwhelming majority of children who have nightmares are still abuse-free. In short, just because P(D|H<sub>0</sub>) is low does not mean that P(H<sub>0</sub>|D) is also low.</p>
<p>Here is an illustration from mathematician <xref ref-type="bibr" rid="bibr73-0959354311429854">John Allen Paulos (2003)</xref>: We want to know if a suspect is the DC sniper. He owns a white van, rifles, and sniper manuals. We think he’s innocent. Does the probability that a man who owns these items is innocent = the probability that an innocent man would own these items? Let us assume there are about 4 million innocent people in the DC area, one guilty person (this is before we knew there were two) and that 10 people own all the items in question. Thus, the first probability, the probability that a man who has these items is innocent, is 9/10, or 90%. The latter probability, the probability that an innocent man has all these items, is 9/4,000,000, or .0002%.</p>
<p>Having examined some common misconceptions, let us now move on to NHST’s actual problems.</p>
</sec>
</sec>
<sec id="section10-0959354311429854">
<title>Etiology and evaluation of the NHST virus</title>
<p>Owing chiefly to the inaccuracies in many of our college textbooks, the origins of NHST have practically become cloaked in myth (<xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>). The NHST ritual is a hybrid of Fisher’s <italic>p</italic> value and null hypothesis test and Neyman and Pearson’s alternative hypothesis and Type I error rate. It bears mentioning that the man who started the practice of significance testing and then introduced such methods to the social sciences was not Fisher, but Francis Ysidro Edgeworth, a distant cousin of Francis Galton (<xref ref-type="bibr" rid="bibr26-0959354311429854">Edgeworth, 1886</xref>; <xref ref-type="bibr" rid="bibr91-0959354311429854">Stigler, 1986</xref>). Further, it should be made plain when discussing NHST that its pioneers (e.g., Fisher, Neyman, and Pearson) are not to blame for our current ill-considered misapplication of their ideas. As <xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer (2004)</xref> states, “Each of these eminent statisticians would have rejected the null ritual as bad statistics” (p. 589).</p>
<p>The two models differ not only in statistical method but in philosophic views. Neyman and Pearson rejected null hypothesis testing and emphasized error detection and the determination of β, “which is not a part of the null ritual” (<xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>, p. 589). Indeed, the appropriate use of the Neyman–Pearson model is far narrower than current statistical (mal)practice would suggest (a typical legitimate application would be quality control: <xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>; <xref ref-type="bibr" rid="bibr54-0959354311429854">Hubbard &amp; Armstrong, 2006</xref>). Though Fisher’s early work (e.g., <xref ref-type="bibr" rid="bibr35-0959354311429854">Fisher, 1925</xref>, <xref ref-type="bibr" rid="bibr37-0959354311429854">1935</xref>) advocated the use of cutoffs for significance, and though it has been argued that the force of his early rhetoric likely prompted our adoption of significance testing (<xref ref-type="bibr" rid="bibr106-0959354311429854">Yates, 1951</xref>), it should be noted that <xref ref-type="bibr" rid="bibr38-0959354311429854">Fisher (1955</xref>, <xref ref-type="bibr" rid="bibr39-0959354311429854">1956</xref>) himself later objected both to the use of significance tests for accept–reject decisions and to the idea of fixed alpha levels. He eventually considered it naïve to assume that scientists actually conduct the same test repeatedly. Instead, he argued, one should report the exact <italic>p</italic> value without making a dichotomous accept–reject decision (<xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>).</p>
<p>Sadly, the social sciences took no notice and the influence of Fisher’s early work is still ubiquitous (<xref ref-type="bibr" rid="bibr53-0959354311429854">Hubbard, 2004</xref>). The typical social science researcher wishing to conduct a statistical analysis decides on α, calculates <italic>p</italic>, and then compares <italic>p</italic> to α without appreciating that they do not actually have anything to do with one another. In our obsession with this “<italic>p</italic> &lt; α” gobbledygook, we collectively fantasize that our <italic>p</italic> value is akin to a temperature (call it <italic>t</italic>), that α is akin to knowing that water freezes at 32 degrees Fahrenheit, and that if <italic>t</italic> &lt; 32 degrees, then the water is “statistically frozen.” This is nonsense.</p>
<p>Fisher’s <italic>p</italic> value is the probability of seeing results ≥ your own given the truth of the null. Neyman and Pearson’s α is the probability of committing the error of falsely rejecting the null. Thus, α is an error probability; <italic>p</italic> is not (<xref ref-type="bibr" rid="bibr54-0959354311429854">Hubbard &amp; Armstrong, 2006</xref>). A commonly missed point is that α is not concerned with inductive inference at all. It is concerned with minimizing error in the long run. Fisher’s <italic>p</italic> value, by contrast, has nothing to do with Type I error. It is an evidential probability intended to assist one in making a scientific inductive inference based on the results of experimentation.</p>
<p>Neyman and Pearson’s α is the probability a rejected null is actually true. Since <italic>p</italic> already assumes the truth of the null, the probability of falsely rejecting the null is irrelevant. The calculated value for <italic>p</italic> is conditional on the probability of the null being true = 1. Therefore, to know the exact value of <italic>p</italic> and the probability of one’s results, one must first assume the probability of falsely rejecting the null is zero. <italic>Thus, stating that “</italic>p <italic>&lt; α” is utterly meaningless</italic>. Despite this, this easily achievable demonstration of baloney is all our peer-reviewed journals seem interested in. This state of affairs might seem more fitting if the presenters of such a tortured logic were Lewis Carroll or Groucho Marx and not the majority of social science researchers, textbook authors, and journal editors.</p>
<p>The etiology of this error is difficult to trace. What is known is that in psychology, the Fisher and Neyman–Pearson models somehow confusedly evolved into the amalgamated slapdash hodgepodge of NHST, which in the 1950s haphazardly became enshrined as the be-all and end-all approach to statistical analysis, institutionalized by professional associations and curricula alike (<xref ref-type="bibr" rid="bibr41-0959354311429854">Gigerenzer, 1987</xref>, <xref ref-type="bibr" rid="bibr42-0959354311429854">1993</xref>, <xref ref-type="bibr" rid="bibr44-0959354311429854">2004</xref>), much to the dismay of Fisher himself (<xref ref-type="bibr" rid="bibr38-0959354311429854">Fisher, 1955</xref>, <xref ref-type="bibr" rid="bibr39-0959354311429854">1956</xref>). This mindless, hybrid approach to statistical analysis quickly spread like a virus from psychology to other fields (including the medical and biological sciences), much to their loss (<xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>; <xref ref-type="bibr" rid="bibr57-0959354311429854">Ioannidis, 2005</xref>).</p>
<p>Much of the blame for NHST’s popularity likely rests squarely with the APA. As <xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer (2004)</xref> observes, the <xref ref-type="bibr" rid="bibr1-0959354311429854">1952</xref> first edition of the <italic>Publication Manual of the American Psychological Association</italic> stressed significance testing and even went so far as to ignorantly dissuade authors from reporting nonsignificant results. The 1974 second edition famously states: “Caution: Do not infer trends from data that fail by a small margin to meet the usual levels of significance. Such results are best interpreted as caused by chance and are best reported as such” (<xref ref-type="bibr" rid="bibr2-0959354311429854">APA, 1974</xref>, p. 19). Thus, the second edition of the <italic>Manual</italic> simultaneously achieves two embarrassing things: (a) it gives advice based on Fisher’s early work while ignoring his later arguments; and (b) it encouraged belief in <xref ref-type="bibr" rid="bibr16-0959354311429854">Carver’s (1978)</xref> “odds-against-chance fantasy.” Though the third edition of the <italic>Manual</italic> omitted this passage, your chances of finding a social science statistics text that points such things out is virtually zero (<xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>).</p>
<sec id="section11-0959354311429854">
<title>The nil hypothesis</title>
<p>If your null hypothesis is a mean difference of zero, then your null is what is commonly referred to as “the nil hypothesis.” This is the most common hypothesis used in psychology, and many respected methodologists have observed the imprudence of basing our inferences on such a dubious assumption (e.g., <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>; <xref ref-type="bibr" rid="bibr68-0959354311429854">Meehl, 1978</xref>). As <xref ref-type="bibr" rid="bibr102-0959354311429854">Tukey (1991)</xref> states, “[T]he effects of A and B are always different, in some decimal place, and so to ask if they are different is foolish” (p. 100); and <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan (1966)</xref>, “There really is no good reason to expect the null hypothesis to be true in any population” (p. 426). <xref ref-type="bibr" rid="bibr27-0959354311429854">Edwards (1965)</xref> makes clear why it should matter whether the null is plausible or scientifically preposterous: “If a hypothesis is preposterous to start with, no amount of bias against it can be too great. On the other hand, if it is preposterous to start with, why test it?” (p. 402)</p>
<p>Not everyone agrees with this assessment (e.g., <xref ref-type="bibr" rid="bibr50-0959354311429854">Hagen, 1997</xref>; <xref ref-type="bibr" rid="bibr70-0959354311429854">Mulaik et al., 1997</xref>). <xref ref-type="bibr" rid="bibr50-0959354311429854">Hagen (1997)</xref>, for instance, argues that though there is likely always some effect present that could theoretically be measured on some variable, there is no reason to expect this to be the dependent variable in question. But how can one expect it <italic>not</italic> to be the dependent variable? Further, how can one demonstrate a justification for this assumed immunity of the variable of interest? Hagen seems to be begging the question. Furthermore, as <xref ref-type="bibr" rid="bibr58-0959354311429854">Jones and Tukey (2000)</xref> state in response to Hagen, “We simply do not accept that view” (p. 412).</p>
<p><disp-quote>
<p>For large finite treatment populations, a total census is at least conceivable, and we cannot imagine an outcome for which µ<sub>A</sub> - µ<sub>B</sub> = 0 when the dependent variable (or any other variable) is measured to an indefinitely large number of decimal places. … For hypothetical treatment populations, µ<sub>A</sub> - µ<sub>B</sub> may approach zero as a limit, but as for the approach of population sizes to infinity, the limit never is reached. The population mean difference may be trivially small, but will always be positive or negative. (p. 412)</p>
</disp-quote></p>
<p>Thus, according to Jones and Tukey, µ<sub>A</sub> - µ<sub>B</sub> is either &gt; 0, &lt; 0 or not (yet) determined. It never = 0.</p>
<p>If <xref ref-type="bibr" rid="bibr58-0959354311429854">Jones and Tukey (2000</xref>; among others, e.g., <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>) are correct, then regardless of your null hypothesis, µ<sub>A</sub> - µ<sub>B</sub> ≠ 0, which is why the nil hypothesis can always be rejected with a large enough sample size. <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan (1966)</xref> empirically supported this by running statistical tests on 60,000 subjects. He writes that it didn’t matter whether subjects were divided into North vs. South, East vs. West, or Maine vs. the rest of the US—everything was significant. If this sounds to any reader like a demonstration of a technical point that is unrepresentative of “actual research,” I would then point out that Bakan’s demonstration is highly representative of much of what now passes for science in public health.</p>
<p>Such a discovery is also discussed in <xref ref-type="bibr" rid="bibr72-0959354311429854">Nunnally (1960)</xref>. <xref ref-type="bibr" rid="bibr70-0959354311429854">Mulaik et al. (1997)</xref> object that it is contradictory to use a significance test to empirically support the proposition that significance tests are unempirical. I would suggest that, again, Mulaik et al. are missing the point. The point here is that it most certainly does matter whether the null hypothesis is empirically plausible or not. What both Bakan and Nunnally have done is shown us the circularity of NHST, of running lots of subjects and then testing to see whether we ran lots of subjects. This point was also made almost three-quarters of a century ago by <xref ref-type="bibr" rid="bibr8-0959354311429854">Berkson (1938)</xref>, who astutely observes:</p>
<p><disp-quote>
<p>It would be agreed by statisticians that a large sample is always better than a small sample. If, then, we know in advance the <italic>P</italic> that will result from an application of the Chi-square test to a large sample, there would seem to be no use in doing it on a smaller one. But since the result of the former test is known, it is no test at all. (p. 526)</p>
</disp-quote></p>
<p>This brings up another point. If using a nil hypothesis you have no evidential reason to suspect is in any way plausible, why set up a statistical test to control for phantom Type I error? Why focus on the probability of falsely rejecting the implausible (<xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>)? Indeed, researchers often allow the real threat of Type II errors to remain extraordinarily high, typically as high as 50 to 80% (<xref ref-type="bibr" rid="bibr19-0959354311429854">Cohen, 1962</xref>, <xref ref-type="bibr" rid="bibr22-0959354311429854">1994</xref>; <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk, 1996</xref>; <xref ref-type="bibr" rid="bibr88-0959354311429854">Sedlmeier &amp; Gigerenzer, 1989</xref>). This implies that correcting for “alpha inflation” is often a particularly dubious procedure. Adjusting for alpha error across multiple tests typically only leads to an overestimation of the population effect size and reduces power (<xref ref-type="bibr" rid="bibr22-0959354311429854">Cohen, 1994</xref>; <xref ref-type="bibr" rid="bibr88-0959354311429854">Sedlmeier &amp; Gigerenzer, 1989</xref>). But if we are using a nil hypothesis, then power is simply a calculation of whether we have run enough subjects to detect what we empirically already suspect to be the case. <xref ref-type="bibr" rid="bibr64-0959354311429854">Lykken (1968)</xref> observed that the odds of supporting a directional prediction through NHST—<italic>even if the research hypothesis in question is false</italic>—are typically 50–50. Why not just flip a coin instead? After all, all rejecting a nil hypothesis tells us is the direction of an effect (<xref ref-type="bibr" rid="bibr47-0959354311429854">Granaas, 2002</xref>).</p>
<p><xref ref-type="bibr" rid="bibr5-0959354311429854">Armstrong (2007)</xref> argues that it only makes sense to test a null if it is a reasonable and likely conclusion. Science and knowledge, after all, advance more rapidly through the rejection of theories (<xref ref-type="bibr" rid="bibr48-0959354311429854">Greenwald, 1975</xref>; <xref ref-type="bibr" rid="bibr76-0959354311429854">Popper, 1935/1959</xref>). The hypothesis of interest, this implies, should be set up as the null. This is what <xref ref-type="bibr" rid="bibr75-0959354311429854">Platt (1964)</xref>, in his classic paper, calls “strong inference.” Armstrong adds that even if the null is a reasonable hypothesis, effect sizes and confidence intervals should be the focus, not significance tests. He concludes that significance tests are unnecessary even when conducted and interpreted correctly and that ultimately all they really do is “take up space in journals” (<xref ref-type="bibr" rid="bibr5-0959354311429854">Armstrong, 2007</xref>, p. 336). He is not alone in thinking this. <xref ref-type="bibr" rid="bibr22-0959354311429854">Cohen (1994)</xref> states, “Even a correct interpretation of <italic>p</italic> values does not achieve much, and has not for a long time” (p. 1001); and <xref ref-type="bibr" rid="bibr59-0959354311429854">Kirk (1996)</xref>, “I believe that even when a significance test is interpreted correctly, the business of science does not progress at it should” (pp. 753–754).</p>
<p>We have seen that even when applied without laboring under common misconceptions, NHST still does not achieve much. I would now like to turn our attention to what I feel is the severest outcome of our abject failure to prevent NHST from running rampant for decades in the social sciences.</p>
</sec>
</sec>
<sec id="section12-0959354311429854">
<title>Cherry picking from the sea of happenstance</title>
<p>It was stated in the Introduction that the most unfortunate consequence of psychology’s obsession with NHST is nothing less than the sad state of our entire body of literature. Our morbid overreliance on significance testing has left in its wake a body of literature so rife with contradictions that peer-reviewed “findings” can quite easily be culled to back almost any position, no matter how absurd or fantastic. Such positions, which all taken together are contradictory, typically yield embarrassingly little predictive power, and fail to gel into any sort of cohesive picture of reality, are nevertheless separately propped up by their own individual lists of supportive references. All this is foolhardily done while blissfully ignoring the fact that the tallying of supportive references—a practice which <xref ref-type="bibr" rid="bibr93-0959354311429854">Taleb (2007)</xref> calls “naïve empiricism”—is not actually scientific. It is the quality of the evidence and the validity and soundness of the arguments that matters, not how many authors are in agreement. Science is not a democracy.</p>
<p>It would be difficult to overstress this point. Card sharps can stack decks so that arranged sequences of cards appear randomly shuffled. Researchers can stack data so that random numbers seem to be convincing patterns of evidence, and often end up doing just that wholly without intention. The bitter irony of it all is that our peer-reviewed journals, our hallmark of what counts as scientific writing, are partly to blame. They do, after all, help keep the tyranny of NHST alive, and “[t]he end result is that our literature is comprised mainly of uncorroborated, one-shot studies whose value is questionable for academics and practitioners alike” (<xref ref-type="bibr" rid="bibr54-0959354311429854">Hubbard &amp; Armstrong, 2006</xref>, p. 115).</p>
<p>In many fields in the social sciences, cherry picking is powerfully reinforced by the strong contingencies within which researchers operate. The incentives are strong. Many academics would not get funding or tenure without selectively supporting their body of work, even if it’s nonsense (<xref ref-type="bibr" rid="bibr4-0959354311429854">Andreski, 1972</xref>). Many doing research for companies are understandably reluctant to bite the hand that feeds them. Many corporate researchers are paid to find something specific and are typically statistically savvy enough to paint the picture they are hired to paint. A great many nonprofits could simply not survive without cherry-picked supportive findings. Public health initiatives often employ statisticians whose sole function is to sift through datasets fishing for numbers that support the funded program in question, ignoring the fact that the same spreadsheet also contains data that annihilates the culled finding. This practice is both fraudulent and unethical.</p>
<p>In clinical psychology, there is so much data available that evidence can be assembled to support almost any hypothesis. Published studies making competing claims recurrently have little overlap in their references. To take a now famous example, <xref ref-type="bibr" rid="bibr83-0959354311429854">Sawyer’s (1966)</xref> summary of the literature on clinical vs. mechanical (actuarial) prediction concludes that mechanical prediction is superior. <xref ref-type="bibr" rid="bibr61-0959354311429854">Korman’s (1968)</xref> review argues that clinical prediction is superior. <xref ref-type="bibr" rid="bibr52-0959354311429854">Holt (1970)</xref> points out that though Sawyer and Korman’s “reviews” of the scientific literature are only two years apart, there is zero overlap in their references.</p>
<p>Such cherry picking is not the type of synthesis scientific theories are supposed to represent. The marshalling of cherry-picked evidence to support pet claims is not science but an agenda. It replaces science with a parlor game that few will take seriously. We are supposed to be detectives after all, not just “advocates.”</p>
<p>Because of this, perhaps the most important type of study that can presently be done in the social sciences is the meta-analysis of large and representative collections of studies (<xref ref-type="bibr" rid="bibr84-0959354311429854">Schmidt, 1992</xref>). Sticking with the above example, the famous <xref ref-type="bibr" rid="bibr49-0959354311429854">Grove, Zald, Lebow, Snitz, and Nelson (2000)</xref> meta-analysis quite convincingly demonstrates that mechanical prediction does indeed outperform clinical prediction.</p>
<p>As <xref ref-type="bibr" rid="bibr5-0959354311429854">Armstrong (2007)</xref> points out, there are those who argue this is why we still need significance tests: meta-analyses require them, they say. This claim is false (<xref ref-type="bibr" rid="bibr5-0959354311429854">Armstrong, 2007</xref>; <xref ref-type="bibr" rid="bibr78-0959354311429854">Rosenthal, 1993</xref>; <xref ref-type="bibr" rid="bibr86-0959354311429854">Schmidt &amp; Hunter, 1997</xref>). Not only are significance tests unnecessary for meta-analyses, but as Schmidt and Hunter demonstrate, meta-analyses employing effect sizes and confidence intervals are superior. Broad meta-analyses of the studies conducted on a topic could perhaps aid in salvaging something empirical from more than half a century of fraudulent statistical assumptions. Given this fact, effect sizes should always be reported, if only so that they can be included in future meta-analyses, which need to be conducted in attempt to sift the gold from the humbug (<xref ref-type="bibr" rid="bibr95-0959354311429854">Thompson, 1999</xref>).</p>
<p>The sheer enormity of the literature itself makes this task daunting. Twenty years ago it was estimated that in peer-reviewed psychology journals alone an article is published every 15 minutes. That’s 35,040 articles a year (<xref ref-type="bibr" rid="bibr100-0959354311429854">Thorngate, 1990</xref>)! That is ridiculous. It is perhaps unsurprising then that most published papers are likely never read, that one of the biggest predictors of whether an author will cite a paper is whether he has seen it cited by others, and that, because of such factors, papers that become well known do so largely owing to chance (<xref ref-type="bibr" rid="bibr93-0959354311429854">Taleb, 2007</xref>). A partial solution to some of the problems noted above would simply be to quit requiring academics to publish in order to work and teach. I would much rather require that they stop teaching factually incorrect statistics than that they publish pseudo-original work. The unfortunate publish-or-perish reality in academia has led some to argue that much unscientific research is likely conducted by academics simply to ensure their own professional survival (<xref ref-type="bibr" rid="bibr4-0959354311429854">Andreski, 1972</xref>).</p>
</sec>
<sec id="section13-0959354311429854">
<title>Conclusion</title>
<p>This paper was meant not as a condemnation of psychology but rather as a call for reform. Our obsession with statistical tests of significance has made much of our research blatantly unscientific. <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan (1966)</xref> reminds readers of <xref ref-type="bibr" rid="bibr101-0959354311429854">Tukey’s (1962)</xref> point that statistical tests often pull our attention away from the data themselves, which are ultimately what we are supposed to base inferences on. As <xref ref-type="bibr" rid="bibr6-0959354311429854">Bakan (1966)</xref> notes, “When we reach a point where our statistical procedures are substitutes instead of aids to thought, and we are led to absurdities, then we must return to the common sense basis” (p. 436); and, “We must overcome the myth that if our treatment of our subject matter is mathematical it is therefore precise and valid. Mathematics can serve to obscure as well as reveal” (p. 436).</p>
<p>This is what <xref ref-type="bibr" rid="bibr4-0959354311429854">Andreski (1972)</xref> referred to as “quantification as camouflage.” The point of statistics is to clear the fog and help us identify patterns and relationships in cumbersome data that the naked eye cannot detect unaided. All too often, however, statistics instead serves—indeed, is all too often intentionally employed—to smokescreen what the naked eye does indeed see unaided: that there’s nothing there. If a <italic>p</italic> value is significant but confidence intervals are wide, effect sizes are minute, and corroborating, diverse evidence simply is not there, the <italic>p</italic> value is misleading you. <italic>Ignore it</italic>. To fail to do so is to mislead.</p>
<p>There is no substitute for eyeballing your data. Some surveyors of psychological research have concluded that .5 is the average effect size in some fields (e.g., <xref ref-type="bibr" rid="bibr88-0959354311429854">Sedlmeier &amp; Gigerenzer, 1989</xref>). <xref ref-type="bibr" rid="bibr21-0959354311429854">Cohen (1992)</xref> argues that such an effect should be visible to the naked eye. There is no substitute for what <xref ref-type="bibr" rid="bibr28-0959354311429854">Edwards, Lindman, and Savage (1963)</xref>, in their classic article, call the “interocular traumatic test,” which is when a relationship hits you right between the eyes. When your data show a clear relationship, <italic>you typically do not need a significance test to see it</italic>. How many times have researchers known an effect was present but failed to publish because of the tyranny of the <italic>p</italic> value? How many times have useless effects gained acceptance because <italic>N</italic> was sufficiently large to keep <italic>p</italic> sufficiently small, slipping shoddy results through the door utilizing the standard “<italic>p</italic> &lt; α” gobbledygook?</p>
<p>The making of scientific inferences is always a qualitative process. It is something that we must do ourselves. It can be helped by mathematics, but it cannot be <italic>replaced by</italic> mathematics. Math does not do the reasoning for us. The hard work will always take place between our ears. If we feel that our results, without being dressed up with excessive quantification, are not fancy enough, then we should remind ourselves that almost all great discoveries in science were qualitative in nature (<xref ref-type="bibr" rid="bibr103-0959354311429854">Uttal, 2005</xref>). Furthermore, many of the genuine great discoveries in psychology were made by researchers who were not using significance tests (<xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>).</p>
<p>Despite the (paltry and counterproductive) efforts of the APA to right its wrongs, the tyranny of significance testing continues to reign. And reign it will until the incentives in place begin to change. The ritual observance of this statistical folly will continue unabated so long as it serves as the near-exclusive rite of passage into the undeservingly revered halls of social science peer-reviewed journals. It should by now not sound overly cynical to observe that the business of most social scientists is to stay in business. Most psychologists are not sycophantic myrmidons of the church of NHST. As many economists well realize, process and incentives are what matter most, and the fact of the matter is that social scientists must make a living and they must publish to do so. It should in no way surprise us then that most social scientists are far more interested in publishing articles than they are in statistical thinking or producing science (<xref ref-type="bibr" rid="bibr44-0959354311429854">Gigerenzer, 2004</xref>).</p>
<p>As Roseanne Conner’s father says in the sitcom <italic>Roseanne</italic>, “You can lead a horse to water, but you can’t make him think.” Paraphrasing <xref ref-type="bibr" rid="bibr33-0959354311429854">Fidler et al., (2004)</xref>, “You can lead a social scientist to a confidence interval, but you can’t make him think about statistics.” The recommendations of the APA’s “task force” (<xref ref-type="bibr" rid="bibr104-0959354311429854">Wilkinson &amp; The TFSI, 1999</xref>) will undoubtedly go entirely unheeded until our peer-reviewed journals lead the way and advocate change by themselves changing the requirements for publishing.</p>
<p>I do not harbor the vain illusion that the present article will change current practice. This article is only a reminder of the far better ones referenced herein. I merely hope it to further raise awareness. In conclusion, it is time that we put our <italic>p</italic> values away and get around to the business of science.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<p>This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0959354311429854">
<label>1.</label>
<p><xref ref-type="bibr" rid="bibr68-0959354311429854">Meehl (1978)</xref> found Andreski’s “hatchet job” of the social sciences so effective that he argued it should be required reading for all social science Ph.D. candidates.</p>
</fn>
<fn fn-type="other" id="fn2-0959354311429854">
<label>2.</label>
<p><xref ref-type="bibr" rid="bibr4-0959354311429854">Andreski (1972)</xref> observes that in the social sciences, “[p]retentious and nebulous verbosity, interminable repetition of platitudes and disguised propaganda are the order of the day” (p. 11). As noted above, Andreski warns us that the disguising of propaganda as research is shamanism, not science. And today things are worse, not better. Indeed, in this day and age, it would behoove all social scientists to bear in mind that the wrapping of advocacy in the cloak of the unfettered pursuit of truth is, as Andreski calls it, sorcery. The more political the social sciences become, the more fraught with propaganda (and shamanism) are its journals. This disturbing practice can today be seen taking over pockets of the social sciences, such as community psychology, where inherently anti-scientific causes such as “social activism” threaten to suffocate any attempt at intellectually honest and open-minded scientific inquiry (for an interesting discussion, see <xref ref-type="bibr" rid="bibr105-0959354311429854">Wright &amp; Cummings, 2003</xref>).</p>
</fn>
<fn fn-type="other" id="fn3-0959354311429854">
<label>3.</label>
<p>Indeed, many will likely dismiss this paper by flippantly observing that I have said nothing “original.” That is irrelevant. What matters is this: Am I right or wrong?</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0959354311429854">
<citation citation-type="book">
<collab>American Psychological Association</collab>. (<year>1952</year>). <source>Publication manual</source> (<edition>1st ed.</edition>). <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Garamond/Pridemark Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0959354311429854">
<citation citation-type="book">
<collab>American Psychological Association</collab>. (<year>1974</year>). <source>Publication manual</source> (<edition>2nd ed.</edition>). <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Garamond/Pridemark Press</publisher-name>.</citation>
</ref>
<ref id="bibr3-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Anastasi</surname><given-names>A.</given-names></name>
</person-group> (<year>1976</year>). <source>Psychological testing</source> (<edition>4th ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Macmillan</publisher-name>.</citation>
</ref>
<ref id="bibr4-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Andreski</surname><given-names>S.</given-names></name>
</person-group> (<year>1972</year>). <source>Social sciences as sorcery</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>André Deutsch Limited</publisher-name>.</citation>
</ref>
<ref id="bibr5-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Armstrong</surname><given-names>J.S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Statistical significance tests are unnecessary even when properly done and properly interpreted: Reply to commentaries</article-title>. <source>International Journal of Forecasting</source>, <volume>23</volume>, <fpage>335</fpage>–<lpage>336</lpage>.</citation>
</ref>
<ref id="bibr6-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bakan</surname><given-names>D.</given-names></name>
</person-group> (<year>1966</year>). <article-title>The test of significance in psychological research</article-title>. <source>Psychological Bulletin</source>, <volume>66</volume>, <fpage>423</fpage>–<lpage>437</lpage>.</citation>
</ref>
<ref id="bibr7-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bakan</surname><given-names>D.</given-names></name>
</person-group> (<year>1974</year>). <source>On method: Toward a reconstruction of psychological investigation</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr8-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Berkson</surname><given-names>J.</given-names></name>
</person-group> (<year>1938</year>). <article-title>Confidence curves: An omnibus technique for estimation and testing statistical hypotheses</article-title>. <source>Journal of the American Statistical Association</source>, <volume>33</volume>, <fpage>526</fpage>–<lpage>542</lpage>.</citation>
</ref>
<ref id="bibr9-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Blinkhorn</surname><given-names>S.</given-names></name>
<name><surname>Johnson</surname><given-names>C.</given-names></name>
</person-group> (<year>1990</year>, <month>December</month> <day>27</day>). <article-title>The insignificance of personality testing</article-title>. <source>Nature</source>, <volume>348</volume>, <fpage>671</fpage>–<lpage>672</lpage>. doi: 10.1038/348671a0<pub-id pub-id-type="doi">10.1038/348671a0</pub-id></citation>
</ref>
<ref id="bibr10-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boring</surname><given-names>E.G.</given-names></name>
</person-group> (<year>1919</year>). <article-title>Mathematical versus scientific significance</article-title>. <source>Psychological Bulletin</source>, <volume>16</volume>, <fpage>335</fpage>–<lpage>338</lpage>.</citation>
</ref>
<ref id="bibr11-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bradburn</surname><given-names>N.M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>A tribute to Bill Kruskal</article-title>. <source>Statistical Science</source>, <volume>22</volume>, <fpage>262</fpage>–<lpage>263</lpage>.</citation>
</ref>
<ref id="bibr12-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brunswik</surname><given-names>E.</given-names></name>
</person-group> (<year>1952</year>). <article-title>The conceptual framework of psychology</article-title>. In <person-group person-group-type="editor">
<name><surname>Neurath</surname><given-names>O.</given-names></name>
</person-group> (Ed.), <source>International encyclopedia of unified science</source> (<volume>Vol. 1</volume>, pp. <fpage>655</fpage>–<lpage>760</lpage>). <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr13-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brunswik</surname><given-names>E.</given-names></name>
</person-group> (<year>1956</year>). <source>Perception and the representative design of psychological experiments</source>. <publisher-loc>Berkeley</publisher-loc>: <publisher-name>University of California Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Campbell</surname><given-names>J.P.</given-names></name>
</person-group> (<year>1982</year>). <article-title>Some remarks from the outgoing editor</article-title>. <source>Journal of Applied Psychology</source>, <volume>67</volume>, <fpage>691</fpage>–<lpage>700</lpage>.</citation>
</ref>
<ref id="bibr15-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carver</surname><given-names>R.P.</given-names></name>
</person-group> (<year>1976</year>). <article-title>Letter to the editor</article-title>. <source>Educational Psychologist</source>, <volume>12</volume>, <fpage>96</fpage>–<lpage>97</lpage>.</citation>
</ref>
<ref id="bibr16-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carver</surname><given-names>R.P.</given-names></name>
</person-group> (<year>1978</year>). <article-title>The case against statistical significance testing</article-title>. <source>Harvard Educational Review</source>, <volume>48</volume>, <fpage>378</fpage>–<lpage>399</lpage>.</citation>
</ref>
<ref id="bibr17-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carver</surname><given-names>R.P.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The case against statistical significance testing, revisited</article-title>. <source>The Journal of Experimental Education</source>, <volume>61</volume>, <fpage>287</fpage>–<lpage>292</lpage>.</citation>
</ref>
<ref id="bibr18-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cattell</surname><given-names>R.B.</given-names></name>
</person-group> (<year>1978</year>). <source>The scientific use of factor analysis in behavioral and life sciences</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Plenum</publisher-name>.</citation>
</ref>
<ref id="bibr19-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1962</year>). <article-title>The statistical power of abnormal-social psychological research: A review</article-title>. <source>Journal of Abnormal and Social Psychology</source>, <volume>69</volume>, <fpage>145</fpage>–<lpage>153</lpage>.</citation>
</ref>
<ref id="bibr20-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Things I have learned so far</article-title>. <source>American Psychologist</source>, <volume>12</volume>, <fpage>1304</fpage>–<lpage>1312</lpage>.</citation>
</ref>
<ref id="bibr21-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1992</year>). <article-title>A power primer</article-title>. <source>Psychological Bulletin</source>, <volume>112</volume>, <fpage>155</fpage>–<lpage>159</lpage>.</citation>
</ref>
<ref id="bibr22-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1994</year>). <article-title>The earth is round (<italic>p</italic> &lt; .05)</article-title>. <source>American Psychologist</source>, <volume>12</volume>, <fpage>997</fpage>–<lpage>1003</lpage>.</citation>
</ref>
<ref id="bibr23-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dawes</surname><given-names>R.M.</given-names></name>
</person-group> (<year>1994</year>). <source>House of cards: Psychology and psychotherapy built on myth</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Free Press</publisher-name>.</citation>
</ref>
<ref id="bibr24-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dawes</surname><given-names>R.M.</given-names></name>
</person-group> (<year>2001</year>). <source>Everyday irrationality: How pseudo-scientists, lunatics, and the rest of us systematically fail to think rationally</source>. <publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>Westview</publisher-name>.</citation>
</ref>
<ref id="bibr25-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dixon</surname><given-names>P.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Why scientists value <italic>p</italic> values</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>5</volume>, <fpage>390</fpage>–<lpage>396</lpage>.</citation>
</ref>
<ref id="bibr26-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Edgeworth</surname><given-names>F.Y.</given-names></name>
</person-group> (<year>1886</year>). <article-title>Progressive means</article-title>. <source>Journal of the Royal Statistical Society</source>, <volume>49</volume>, <fpage>469</fpage>–<lpage>475</lpage>.</citation>
</ref>
<ref id="bibr27-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Edwards</surname><given-names>W.</given-names></name>
</person-group> (<year>1965</year>). <article-title>Tactical note on the relation between scientific and statistical hypotheses</article-title>. <source>Psychological Bulletin</source>, <volume>63</volume>, <fpage>400</fpage>–<lpage>402</lpage>.</citation>
</ref>
<ref id="bibr28-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Edwards</surname><given-names>W.</given-names></name>
<name><surname>Lindman</surname><given-names>H.</given-names></name>
<name><surname>Savage</surname><given-names>L.J.</given-names></name>
</person-group> (<year>1963</year>). <article-title>Bayesian statistical inference for psychological research</article-title>. <source>Psychological Review</source>, <volume>70</volume>, <fpage>193</fpage>–<lpage>242</lpage>.</citation>
</ref>
<ref id="bibr29-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Falk</surname><given-names>R.</given-names></name>
</person-group> (<year>1998</year>). <article-title>In criticism of the null hypothesis statistical test</article-title>. <source>American Psychologist</source>, <volume>53</volume>, <fpage>798</fpage>–<lpage>799</lpage>.</citation>
</ref>
<ref id="bibr30-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Falk</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Probabilistic reasoning is not logical</article-title>. <source>Mathematics Magazine</source>, <volume>81</volume>, <fpage>268</fpage>–<lpage>275</lpage>.</citation>
</ref>
<ref id="bibr31-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Falk</surname><given-names>R.</given-names></name>
<name><surname>Greenbaum</surname><given-names>C.W.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Significance tests die hard: The amazing persistence of a probabilistic misconception</article-title>. <source>Theory &amp; Psychology</source>, <volume>5</volume>, <fpage>75</fpage>–<lpage>98</lpage>.</citation>
</ref>
<ref id="bibr32-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Faust</surname><given-names>D.</given-names></name>
<name><surname>Ziskin</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>, <month>July</month> <day>1</day>). <article-title>The expert witness in psychology and psychiatry</article-title>. <source>Science</source>, <volume>241</volume>(<issue>4861</issue>), <fpage>31</fpage>–<lpage>35</lpage>.</citation>
</ref>
<ref id="bibr33-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fidler</surname><given-names>F.</given-names></name>
<name><surname>Thomason</surname><given-names>N.</given-names></name>
<name><surname>Cumming</surname><given-names>G.</given-names></name>
<name><surname>Finch</surname><given-names>S.</given-names></name>
<name><surname>Leeman</surname><given-names>J.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Editors can lead researchers to confidence intervals, but can’t make them think</article-title>. <source>Psychological Science</source>, <volume>15</volume>, <fpage>119</fpage>–<lpage>126</lpage>.</citation>
</ref>
<ref id="bibr34-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Finch</surname><given-names>S.</given-names></name>
<name><surname>Cumming</surname><given-names>G.</given-names></name>
<name><surname>Thomason</surname><given-names>N.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Reporting of statistical inference in the “Journal of Applied Psychology”: Little evidence of reform</article-title>. <source>Educational and Psychological Measurement</source>, <volume>61</volume>, <fpage>181</fpage>–<lpage>210</lpage>.</citation>
</ref>
<ref id="bibr35-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fisher</surname><given-names>R.A.</given-names></name>
</person-group> (<year>1925</year>). <source>Statistical methods for research workers</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Oliver &amp; Boyd</publisher-name>.</citation>
</ref>
<ref id="bibr36-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fisher</surname><given-names>R.A.</given-names></name>
</person-group> (<year>1929</year>). <article-title>The statistical method in psychical research</article-title>. <source>Proceedings of the Society for Psychical Research</source>, <volume>39</volume>, <fpage>185</fpage>–<lpage>189</lpage>.</citation>
</ref>
<ref id="bibr37-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fisher</surname><given-names>R.A.</given-names></name>
</person-group> (<year>1935</year>). <source>The design of experiments</source>. <publisher-loc>Edinburgh, UK</publisher-loc>: <publisher-name>Oliver &amp; Boyd</publisher-name>.</citation>
</ref>
<ref id="bibr38-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fisher</surname><given-names>R.A.</given-names></name>
</person-group> (<year>1955</year>). <article-title>Statistical methods and scientific induction</article-title>. <source>Journal of the Royal Statistical Society. Series B (Methodological)</source>, <volume>17</volume>, <fpage>69</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr39-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fisher</surname><given-names>R.A.</given-names></name>
</person-group> (<year>1956</year>). <source>Statistical methods and scientific inference</source>. <publisher-loc>Edinburgh, UK</publisher-loc>: <publisher-name>Oliver &amp; Boyd</publisher-name>.</citation>
</ref>
<ref id="bibr40-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gerrig</surname><given-names>R.J.</given-names></name>
<name><surname>Zimbardo</surname><given-names>P.G.</given-names></name>
</person-group> (<year>2002</year>). <source>Psychology and life</source> (<edition>16th ed.</edition>). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Allyn &amp; Bacon</publisher-name>.</citation>
</ref>
<ref id="bibr41-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Probabilistic thinking and the fight against subjectivity</article-title>. In <person-group person-group-type="editor">
<name><surname>Krüger</surname><given-names>L.</given-names></name>
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
<name><surname>Morgan</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <source>The probabilistic revolution: Vol. II. Ideas in the sciences</source> (pp. <fpage>11</fpage>–<lpage>33</lpage>). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr42-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The superego, the ego, and the id in statistical reasoning</article-title>. In <person-group person-group-type="editor">
<name><surname>Keren</surname><given-names>G.</given-names></name>
<name><surname>Lewis</surname><given-names>C.</given-names></name>
</person-group> (Eds.), <source>A handbook for data analysis in the behavioral sciences: Methodological issues</source> (pp. <fpage>311</fpage>–<lpage>339</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr43-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
</person-group> (<year>2000</year>). <source>Adaptive thinking: Rationality in the real world</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr44-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Mindless statistics</article-title>. <source>The Journal of Socio-Economics</source>, <volume>33</volume>, <fpage>587</fpage>–<lpage>606</lpage>.</citation>
</ref>
<ref id="bibr45-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
<name><surname>Swijtink</surname><given-names>Z.</given-names></name>
<name><surname>Porter</surname><given-names>T.</given-names></name>
<name><surname>Daston</surname><given-names>L.</given-names></name>
<name><surname>Beatty</surname><given-names>J.</given-names></name>
<name><surname>Kruger</surname><given-names>L.</given-names></name>
</person-group> (<year>1989</year>). <source>The empire of chance</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr46-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gill</surname><given-names>J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>The insignificance of null hypothesis significance testing</article-title>. <source>Political Research Quarterly</source>, <volume>52</volume>, <fpage>647</fpage>–<lpage>674</lpage>.</citation>
</ref>
<ref id="bibr47-0959354311429854">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Granaas</surname><given-names>M.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Hypothesis testing in psychology: Throwing the baby out with the bathwater</article-title>. <source>ICOTS 6 Proceedings</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.stat.auckland.ac.nz/~iase/publications/1/3m1_gran.pdf">http://www.stat.auckland.ac.nz/~iase/publications/1/3m1_gran.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr48-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Greenwald</surname><given-names>A.G.</given-names></name>
</person-group> (<year>1975</year>). <article-title>Consequences of prejudice against the null hypothesis</article-title>. <source>Psychological Bulletin</source>, <volume>82</volume>, <fpage>1</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr49-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grove</surname><given-names>W.M.</given-names></name>
<name><surname>Zald</surname><given-names>D.H.</given-names></name>
<name><surname>Lebow</surname><given-names>B.S.</given-names></name>
<name><surname>Snitz</surname><given-names>B.E.</given-names></name>
<name><surname>Nelson</surname><given-names>C.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Clinical vs. mechanical prediction: A meta-analysis</article-title>. <source>Psychological Assessment</source>, <volume>12</volume>, <fpage>19</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr50-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hagen</surname><given-names>R.L.</given-names></name>
</person-group> (<year>1997</year>). <article-title>In praise of the null hypothesis test</article-title>. <source>American Psychologist</source>, <volume>52</volume>, <fpage>15</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr51-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hebb</surname><given-names>D.O.</given-names></name>
</person-group> (<year>1966</year>). <source>A textbook of psychology</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>Saunders</publisher-name>.</citation>
</ref>
<ref id="bibr52-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Holt</surname><given-names>R.R.</given-names></name>
</person-group> (<year>1970</year>). <article-title>Yet another look at clinical and statistical prediction: Or, is clinical psychology worthwhile?</article-title> <source>American Psychologist</source>, <volume>25</volume>, <fpage>337</fpage>–<lpage>349</lpage>.</citation>
</ref>
<ref id="bibr53-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hubbard</surname><given-names>R.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Alphabet soup: Blurring the distinctions between <italic>p</italic>’s and α’s in psychological research</article-title>. <source>Theory &amp; Psychology</source>, <volume>14</volume>, <fpage>295</fpage>–<lpage>327</lpage>.</citation>
</ref>
<ref id="bibr54-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hubbard</surname><given-names>R.</given-names></name>
<name><surname>Armstrong</surname><given-names>J.S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Why we don’t really know what statistical significance means: Implications for educators</article-title>. <source>Journal of Marketing Education</source>, <volume>28</volume>, <fpage>114</fpage>–<lpage>120</lpage>.</citation>
</ref>
<ref id="bibr55-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hubbard</surname><given-names>R.</given-names></name>
<name><surname>Lindsay</surname><given-names>R.M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Why <italic>p</italic> values are not a useful measure of evidence in statistical significance testing</article-title>. <source>Theory &amp; Psychology</source>, <volume>18</volume>, <fpage>69</fpage>–<lpage>88</lpage>.</citation>
</ref>
<ref id="bibr56-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hubbard</surname><given-names>R.</given-names></name>
<name><surname>Ryan</surname><given-names>P.A.</given-names></name>
</person-group> (<year>2000</year>).<article-title>The historical growth of statistical significance testing in psychology—and its future prospects</article-title>. <source>Educational and Psychological Measurement</source>, <volume>60</volume>, <fpage>661</fpage>–<lpage>681</lpage>.</citation>
</ref>
<ref id="bibr57-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ioannidis</surname><given-names>J.P.A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Why most published research findings are false</article-title>. <source>PLoS Medicine</source>, <volume>2</volume>(<issue>8</issue>), <fpage>e124</fpage>.</citation>
</ref>
<ref id="bibr58-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jones</surname><given-names>L.V.</given-names></name>
<name><surname>Tukey</surname><given-names>J.W.</given-names></name>
</person-group> (<year>2000</year>). <article-title>A sensible formulation of the significance test</article-title>. <source>Psychological Methods</source>, <volume>5</volume>, <fpage>411</fpage>–<lpage>414</lpage>.</citation>
</ref>
<ref id="bibr59-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kirk</surname><given-names>R.E.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Practical significance: A concept whose time has come</article-title>. <source>Educational and Psychological Measurement</source>, <volume>56</volume>, <fpage>746</fpage>–<lpage>759</lpage>.</citation>
</ref>
<ref id="bibr60-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kirk</surname><given-names>R.E.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The importance of effect magnitude</article-title>. In <person-group person-group-type="editor">
<name><surname>Davis</surname><given-names>S.F.</given-names></name>
</person-group> (Ed.), <source>Handbook of research methods in experimental psychology</source> (pp. <fpage>83</fpage>–<lpage>105</lpage>). <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Blackwell</publisher-name>.</citation>
</ref>
<ref id="bibr61-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Korman</surname><given-names>A.K.</given-names></name>
</person-group> (<year>1968</year>). <article-title>The prediction of managerial performance</article-title>. <source>Personnel Psychology</source>, <volume>21</volume>, <fpage>295</fpage>–<lpage>322</lpage>.</citation>
</ref>
<ref id="bibr62-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lindsay</surname><given-names>R.M.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Reconsidering the status of tests of significance: An alternative criterion of adequacy</article-title>. <source>Accounting, Organizations and Society</source>, <volume>20</volume>, <fpage>35</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr63-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Loftus</surname><given-names>G.R.</given-names></name>
</person-group> (<year>1991</year>). <article-title>On the tyranny of hypothesis testing in the social sciences</article-title>. <source>Contemporary Psychology</source>, <volume>36</volume>, <fpage>102</fpage>–<lpage>105</lpage>.</citation>
</ref>
<ref id="bibr64-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lykken</surname><given-names>D.T.</given-names></name>
</person-group> (<year>1968</year>). <article-title>Statistical significance in psychological research</article-title>. <source>Psychological Bulletin</source>, <volume>70</volume>, <fpage>151</fpage>–<lpage>159</lpage>.</citation>
</ref>
<ref id="bibr65-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lykken</surname><given-names>D.T.</given-names></name>
</person-group> (<year>1991</year>). <article-title>What’s wrong with psychology anyway?</article-title> In <person-group person-group-type="editor">
<name><surname>Cicchetti</surname><given-names>D.</given-names></name>
<name><surname>Grove</surname><given-names>W. M.</given-names></name>
</person-group> (Eds.), <source>Thinking clearly about psychology: Matters of public interest</source> (<volume>Vol. 1</volume>, pp. <fpage>3</fpage>–<lpage>39</lpage>). <publisher-loc>Minneapolis</publisher-loc>: <publisher-name>University of Minnesota Press</publisher-name>.</citation>
</ref>
<ref id="bibr66-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mahoney</surname><given-names>M.</given-names></name>
</person-group> (<year>1977</year>). <article-title>Publication prejudices: An experimental study of confirmatory bias in the peer review system</article-title>. <source>Cognitive Therapy and Research</source>, <volume>1</volume>, <fpage>161</fpage>–<lpage>175</lpage>.</citation>
</ref>
<ref id="bibr67-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Meehl</surname><given-names>P.E.</given-names></name>
</person-group> (<year>1967</year>). <article-title>Theory testing in psychology and physics: A methodological paradox</article-title>. <source>Philosophy of Science</source>, <volume>34</volume>, <fpage>103</fpage>–<lpage>115</lpage>.</citation>
</ref>
<ref id="bibr68-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Meehl</surname><given-names>P.E.</given-names></name>
</person-group> (<year>1978</year>). <article-title>Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology</article-title>. <source>Journal of Consulting and Clinical Psychology</source>, <volume>46</volume>, <fpage>103</fpage>–<lpage>115</lpage>.</citation>
</ref>
<ref id="bibr69-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Meehl</surname><given-names>P.E.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Appraising and amending theories: The strategy of Lakatosian defense and two principles that warrant it</article-title>. <source>Psychological Inquiry</source>, <volume>1</volume>, <fpage>108</fpage>–<lpage>141</lpage>.</citation>
</ref>
<ref id="bibr70-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mulaik</surname><given-names>S.A.</given-names></name>
<name><surname>Raju</surname><given-names>N.S.</given-names></name>
<name><surname>Harshman</surname><given-names>R.A.</given-names></name>
</person-group> (<year>1997</year>). <article-title>There is a time and a place for significance testing</article-title>. In <person-group person-group-type="editor">
<name><surname>Harlow</surname><given-names>L.L.</given-names></name>
<name><surname>Mulaik</surname><given-names>S.A.</given-names></name>
<name><surname>Steiger</surname><given-names>J.H.</given-names></name>
</person-group> (Eds.), <source>What if there were no significance tests?</source> (pp. <fpage>65</fpage>–<lpage>115</lpage>). <publisher-loc>London, UK</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr71-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Murray</surname><given-names>D.</given-names></name>
<name><surname>Schwartz</surname><given-names>J.</given-names></name>
<name><surname>Lichter</surname><given-names>R.</given-names></name>
</person-group> (<year>2001</year>). <source>It ain’t necessarily so: How the media make and unmake the scientific picture of reality</source>. <publisher-loc>Lanham, MD</publisher-loc>: <publisher-name>Rowman &amp; Littlefield</publisher-name>.</citation>
</ref>
<ref id="bibr72-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nunnally</surname><given-names>J.</given-names></name>
</person-group> (<year>1960</year>). <article-title>The place of statistics in psychology</article-title>. <source>Educational and Psychological Measurement</source>, <volume>20</volume>, <fpage>641</fpage>–<lpage>650</lpage>.</citation>
</ref>
<ref id="bibr73-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Paulos</surname><given-names>J.A.</given-names></name>
</person-group> (<year>2003</year>). <source>A mathematician plays the stock market</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Basic Books</publisher-name>.</citation>
</ref>
<ref id="bibr74-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Petrinovich</surname><given-names>L.</given-names></name>
</person-group> (<year>1979</year>). <article-title>Probabilistic functionalism: A conception of research method</article-title>. <source>American Psychologist</source>, <volume>34</volume>, <fpage>373</fpage>–<lpage>390</lpage>.</citation>
</ref>
<ref id="bibr75-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Platt</surname><given-names>J.R.</given-names></name>
</person-group> (<year>1964</year>, <month>October</month> <day>16</day>). <article-title>Strong inference: Certain methods of scientific thinking may produce much more rapid progress than others</article-title>. <source>Science</source>, <volume>146</volume>, <fpage>347</fpage>–<lpage>353</lpage>.</citation>
</ref>
<ref id="bibr76-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Popper</surname><given-names>K.R.</given-names></name>
</person-group> (<year>1959</year>). <source>The logic of scientific discovery</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Hutchinson</publisher-name>. (<comment>Original work published 1935</comment>)</citation>
</ref>
<ref id="bibr77-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rosenberg</surname><given-names>A.</given-names></name>
</person-group> (<year>1988</year>). <source>Philosophy of social science</source>. <publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>Westview</publisher-name>.</citation>
</ref>
<ref id="bibr78-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rosenthal</surname><given-names>R.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Cumulating evidence</article-title>. In <person-group person-group-type="editor">
<name><surname>Keren</surname><given-names>G.</given-names></name>
<name><surname>Lewis</surname><given-names>C.</given-names></name>
</person-group> (Eds.), <source>A handbook for data analysis in the behavioral sciences: Methodological issues</source> (pp. <fpage>519</fpage>–<lpage>559</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr79-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rosnow</surname><given-names>R.L.</given-names></name>
<name><surname>Rosenthal</surname><given-names>R.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Statistical procedures and the justification of knowledge in psychological science</article-title>. <source>American Psychologist</source>, <volume>44</volume>, <fpage>1276</fpage>–<lpage>1284</lpage>.</citation>
</ref>
<ref id="bibr80-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rozeboom</surname><given-names>W.W.</given-names></name>
</person-group> (<year>1960</year>). <article-title>The fallacy of the null-hypothesis significance test</article-title>. <source>Psychological Bulletin</source>, <volume>57</volume>, <fpage>416</fpage>–<lpage>428</lpage>.</citation>
</ref>
<ref id="bibr81-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Salmon</surname><given-names>W.C.</given-names></name>
</person-group> (<year>1984</year>). <source>Scientific explanation and the causal structure of the world</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr82-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Salsburg</surname><given-names>D.</given-names></name>
</person-group> (<year>2001</year>). <source>The lady tasting tea: How statistics revolutionized science in the twentieth century</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Holt</publisher-name>.</citation>
</ref>
<ref id="bibr83-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sawyer</surname><given-names>J.</given-names></name>
</person-group> (<year>1966</year>). <article-title>Measurement and prediction, clinical and statistical</article-title>. <source>Psychological Bulletin</source>, <volume>66</volume>, <fpage>178</fpage>–<lpage>200</lpage>.</citation>
</ref>
<ref id="bibr84-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>F.</given-names></name>
</person-group> (<year>1992</year>). <article-title>What do data really mean? Research findings, meta-analysis, and cumulative knowledge in psychology</article-title>. <source>American Psychologist</source>, <volume>47</volume>, <fpage>1173</fpage>–<lpage>1181</lpage>.</citation>
</ref>
<ref id="bibr85-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>F.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Statistical significance testing and cumulative knowledge in psychology: Implications for training of researchers</article-title>. <source>Psychological Methods</source>, <volume>1</volume>, <fpage>115</fpage>–<lpage>129</lpage>.</citation>
</ref>
<ref id="bibr86-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>F.</given-names></name>
<name><surname>Hunter</surname><given-names>J.E.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Eight common but false objections to the discontinuation of significance testing in the analysis of research data</article-title>. In <person-group person-group-type="editor">
<name><surname>Harlow</surname><given-names>L.L.</given-names></name>
<name><surname>Mulaik</surname><given-names>S.A.</given-names></name>
<name><surname>Steiger</surname><given-names>J.H.</given-names></name>
</person-group> (Eds.), <source>What if there were no significance tests?</source> (pp. <fpage>37</fpage>–<lpage>64</lpage>). <publisher-loc>London, UK</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr87-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schopenhauer</surname><given-names>A.</given-names></name>
</person-group> (<year>2004</year>). <source>Essays and aphorisms</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Penguin</publisher-name>. (<comment>Original work published 1851</comment>)</citation>
</ref>
<ref id="bibr88-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sedlmeier</surname><given-names>P.</given-names></name>
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Do studies of statistical power have an effect on the power of studies?</article-title> <source>Psychological Bulletin</source>, <volume>105</volume>, <fpage>309</fpage>–<lpage>316</lpage>.</citation>
</ref>
<ref id="bibr89-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shermer</surname><given-names>M.</given-names></name>
</person-group> (<year>2000</year>). <source>How we believe: Science, skepticism and the search for God</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Holt</publisher-name>.</citation>
</ref>
<ref id="bibr90-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Skinner</surname><given-names>B.F.</given-names></name>
</person-group> (<year>1972</year>). <source>Cumulative record: A selection of papers</source> (<edition>3rd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Appleton-Century-Crofts</publisher-name>.</citation>
</ref>
<ref id="bibr91-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stigler</surname><given-names>S.</given-names></name>
</person-group> (<year>1986</year>). <source>The history of statistics: The measurement of uncertainty before 1900</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr92-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tabachnick</surname><given-names>B.</given-names></name>
<name><surname>Fidell</surname><given-names>L.</given-names></name>
</person-group> (<year>2001</year>). <source>Computer-assisted research design and analysis</source>. <publisher-loc>Needham Heights, MA</publisher-loc>: <publisher-name>Allyn &amp; Bacon</publisher-name>.</citation>
</ref>
<ref id="bibr93-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Taleb</surname><given-names>N.N.</given-names></name>
</person-group> (<year>2007</year>). <source>Black swan: The impact of the highly improbable</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Random House</publisher-name>.</citation>
</ref>
<ref id="bibr94-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (<year>1996</year>). <article-title>AERA editorial policies regarding statistical significance testing: Three suggested reforms</article-title>. <source>Educational Researcher</source>, <volume>25</volume>, <fpage>26</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr95-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Statistical significance tests, effect size reporting and the vain pursuit of pseudo-objectivity</article-title>. <source>Theory &amp; Psychology</source>, <volume>9</volume>, <fpage>191</fpage>–<lpage>196</lpage>.</citation>
</ref>
<ref id="bibr96-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (<year>2002</year>). <article-title>“Statistical,” “practical,” and “clinical”: How many kinds of significance do counselors need to consider?</article-title> <source>Journal of Counseling and Development</source>, <volume>80</volume>, <fpage>64</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr97-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Guidelines for authors reporting score reliability estimates</article-title>. In <person-group person-group-type="editor">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (Ed.), <source>Score reliability: Contemporary thinking on reliability issues</source> (pp. <fpage>91</fpage>–<lpage>102</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr98-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Research synthesis: Effect sizes</article-title>. In <person-group person-group-type="editor">
<name><surname>Green</surname><given-names>J.</given-names></name>
<name><surname>Camilli</surname><given-names>G.</given-names></name>
<name><surname>Elmore</surname><given-names>P.B.</given-names></name>
</person-group> (Eds.), <source>Handbook of complementary methods in education research</source> (pp. <fpage>583</fpage>–<lpage>603</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr99-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Effect sizes, confidence intervals, and confidence intervals for effect sizes</article-title>. <source>Psychology in the Schools</source>, <volume>44</volume>, <fpage>423</fpage>–<lpage>432</lpage>.</citation>
</ref>
<ref id="bibr100-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thorngate</surname><given-names>W.</given-names></name>
</person-group> (<year>1990</year>). <article-title>The economy of attention and the development of psychology</article-title>. <source>Canadian Psychology</source>, <volume>31</volume>, <fpage>262</fpage>–<lpage>271</lpage>.</citation>
</ref>
<ref id="bibr101-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tukey</surname><given-names>J.W.</given-names></name>
</person-group> (<year>1962</year>). <article-title>The future of data analysis</article-title>. <source>Annals of Mathematical Statistics</source>, <volume>33</volume>, <fpage>1</fpage>–<lpage>67</lpage>.</citation>
</ref>
<ref id="bibr102-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tukey</surname><given-names>J.W.</given-names></name>
</person-group> (<year>1991</year>). <article-title>The philosophy of multiple comparison</article-title>. <source>Statistical Science</source>, <volume>6</volume>, <fpage>100</fpage>–<lpage>116</lpage>.</citation>
</ref>
<ref id="bibr103-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Uttal</surname><given-names>W.R.</given-names></name>
</person-group> (<year>2005</year>). <source>Neural theories of mind: Why the mind–brain problem may never be solved</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr104-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wilkinson</surname><given-names>L.</given-names></name>
</person-group>, &amp; <collab>The Task Force on Statistical Inference</collab>. (<year>1999</year>). <article-title>Statistical methods in psychology journals: Guidelines and explanations</article-title>. <source>American Psychologist</source>, <volume>54</volume>, <fpage>594</fpage>–<lpage>604</lpage>.</citation>
</ref>
<ref id="bibr105-0959354311429854">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Wright</surname><given-names>R.H.</given-names></name>
<name><surname>Cummings</surname><given-names>N.A.</given-names></name>
</person-group> (Eds.). (<year>2003</year>). <source>Destructive trends in mental health: The well-intentioned path to harm</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Taylor &amp; Francis</publisher-name>.</citation>
</ref>
<ref id="bibr106-0959354311429854">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yates</surname><given-names>F.</given-names></name>
</person-group> (<year>1951</year>).<article-title>The influence of statistical methods for research workers on the development of the science of statistics</article-title>. <source>Journal of the American Statistical Association</source>, <volume>46</volume>, <fpage>19</fpage>–<lpage>34</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>