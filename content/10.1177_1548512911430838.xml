<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">DMS</journal-id>
<journal-id journal-id-type="hwp">spdms</journal-id>
<journal-title>The Journal of Defense Modeling and Simulation: Applications, Methodology, Technology</journal-title>
<issn pub-type="ppub">1548-5129</issn>
<issn pub-type="epub">1557-380X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1548512911430838</article-id>
<article-id pub-id-type="publisher-id">10.1177_1548512911430838</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Special Issue Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Case study analysis of Defense Experimentation to support Network Enabled Capability development</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Henshaw</surname><given-names>Michael</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Brathen</surname><given-names>Karsten</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Isler</surname><given-names>Veysi</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Arnott</surname><given-names>Shane D</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Lindsay</surname><given-names>Peter A</given-names></name>
</contrib>
<aff id="aff1-1548512911430838">School of Information Technology &amp; Electrical Engineering, The University of Queensland, Brisbane, Australia</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-1548512911430838">Shane D Arnott, School of Information Technology and Electrical Engineering, 363 Adelaide Street, Brisbane, 4001, Australia Email: <email>shane.d.arnott@boeing.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2013</year>
</pub-date>
<volume>10</volume>
<issue>2</issue>
<issue-title>Special Issue: Modelling and Simulation for NEC</issue-title>
<fpage>91</fpage>
<lpage>104</lpage>
<permissions>
<copyright-statement>© 2012 The Society for Modeling and Simulation International</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">The Society for Modeling and Simulation International</copyright-holder>
</permissions>
<abstract>
<p>Defense Experimentation (DE) using modeling and simulation (M&amp;S) is increasingly being adopted as a means to better understand complex defense capability problems. This is being given added impetus by the amplified focus on Network Enabled Capability (NEC) and the rising use of advanced information and communications technology within military operations. This paper presents analysis of observed data trends from a broad range of DE experiments performed within capability development programs for the United Kingdom and Australian Governments over the period of 2001–2010. A range of variables were tracked concerning the experiment’s nature, the DE method employed, M&amp;S technology utilized and human resources used across the experiment life cycle. Time and effort results are presented here, broken down by DE method and life-cycle phase. The paper also analyses where reuse took place in the experiment life cycle, and how time and effort were affected by the number of problem-owner and provider stakeholders involved. The insights yielded are expected to help DE planners improve estimation and scheduling of human resources. In turn, this is intended to facilitate delivery of more effective NEC concept development and experimentation.</p>
</abstract>
<kwd-group>
<kwd>decision making</kwd>
<kwd>military applications</kwd>
<kwd>simulation-based acquisition</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1548512911430838" sec-type="intro">
<title>1. Introduction</title>
<p>Two of the most prolific publishers on the use of the ‘network’ to improve operations have been the United States of America (US) on the topic of ‘Network Centric Warfare’ (NCW)<sup><xref ref-type="bibr" rid="bibr1-1548512911430838">1</xref><xref ref-type="bibr" rid="bibr2-1548512911430838"/><xref ref-type="bibr" rid="bibr3-1548512911430838"/>–<xref ref-type="bibr" rid="bibr4-1548512911430838">4</xref></sup> and the United Kingdom (UK) on the topic of ‘Network Enabled Capability’ (NEC).<sup><xref ref-type="bibr" rid="bibr5-1548512911430838">5</xref></sup> Despite similar names, NCW and NEC have proven to have conceptual differences.<sup><xref ref-type="bibr" rid="bibr6-1548512911430838">6</xref></sup> Regardless of these differences, one point that is unanimous is the application of experimentation using modeling and simulation (M&amp;S) to help understand NCW and NEC. The UK Ministry of Defence’s (MOD’s) <italic>Simulation Strategy</italic><sup><xref ref-type="bibr" rid="bibr7-1548512911430838">7</xref></sup> notes:<disp-quote>
<p>Simulation has become a key enabler for Defence. The increased complexity of the NEC battlespace and environmental limitations on live activities are also key factors in simulation being increasingly regarded as the most practical way of reproducing the modern battlespace. Within the analysis and experimentation communities, it is a core tool in high-level planning, force development, the acquisition cycle and warfare development.</p>
</disp-quote></p>
<p>US publications<sup><xref ref-type="bibr" rid="bibr1-1548512911430838">1</xref>,<xref ref-type="bibr" rid="bibr3-1548512911430838">3</xref>,<xref ref-type="bibr" rid="bibr8-1548512911430838">8</xref>,<xref ref-type="bibr" rid="bibr9-1548512911430838">9</xref></sup> focus less directly on M&amp;S in isolation and more broadly on the act of ‘experimentation’ and how it supports capability development. That is:<sup><xref ref-type="bibr" rid="bibr3-1548512911430838">3</xref></sup><disp-quote>
<p>Experimentation is the lynch pin in the DoD’s strategy for transformation. Without a properly focused, well-balanced, rigorously designed, and expertly conducted program of experimentation the DoD will not be able to take full advantage of the opportunities that Information Age concepts and technologies offer.</p>
</disp-quote></p>
<p>Defense Experimentation (DE) is defined as<sup><xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref></sup><disp-quote>
<p>The application of the experimental method to the solution of complex defense capability development problems, potentially across the full spectrum of conflict types, such as warfighting, peace-enforcement, humanitarian relief and peace-keeping.</p>
</disp-quote></p>
<p>Although DE has a relatively short history, it has been codified in a series of publications. Two early seminal publications appeared in the Command and Control Research Program – Information Age Transformation series: <italic>Code of Best Practice for Experimentation</italic> (COBP-E)<sup><xref ref-type="bibr" rid="bibr8-1548512911430838">8</xref></sup> and <italic>Campaigns of Experimentation.</italic><sup><xref ref-type="bibr" rid="bibr11-1548512911430838">11</xref></sup> Following these US publications, an international effort proposed a set of guiding principles for practitioners and those more closely related with the experimentation process. This document was authored by The Technical Cooperation Program (TTCP) Joint Systems Analysis Action Group 12 with contributions from country-leading experts in the area of experimentation from the US, UK, Canada and Australia, and was titled the <italic>Guide for Understanding and Implementing Defense Experimentation</italic> (GUIDEx).<sup><xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref></sup></p>
<p>Despite these publications covering the theory and practice of DE, very little empirical data is available regarding the application of DE. That is, there is a distinct lack of published knowledge to answer key questions of any Project Manager or Chief Engineer such as: ‘How much time needs to be allocated to better understand this complex problem?’ and ‘How many resources will I need to apply and for how long?’. Our research attempts to address this deficiency.</p>
<p>This paper reports the results of data collected from a set of 50 DE experiments performed on capability development programs for the UK MOD and the Australian Defence Force over the period of 2001–2010. (US Government sources could not be included due to export control issues surrounding the release of data.) The experiments were conducted by a range of industry-based, government-based, and consortia-based organizations. In some cases the experiments were part of a larger experimental campaign and in others they were stand-alone activities. They had a range of purposes, from concept development to project requirements definition and tactics development. Most (74%) were ‘human-in-the-loop’ experiments, but ‘constructive’ experiments (16%) and ‘analytical wargame’ experiments (10%) were also represented (see below for definitions of these concepts). Of note, no data for ‘live’ experiments was collected and it is therefore not addressed by this analysis.</p>
<p>Selection of collected data was based on the recognized definitions and processes for experimentation<sup><xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref></sup>, M&amp;S<sup><xref ref-type="bibr" rid="bibr12-1548512911430838">12</xref></sup> and personnel data typically collected for software development estimation methods, such as the Constructive Cost Model (COCOMO),<sup><xref ref-type="bibr" rid="bibr13-1548512911430838">13</xref>,<xref ref-type="bibr" rid="bibr14-1548512911430838">14</xref></sup> as well as the subsequent adaptation to systems engineering.<sup><xref ref-type="bibr" rid="bibr15-1548512911430838">15</xref>,<xref ref-type="bibr" rid="bibr16-1548512911430838">16</xref></sup> The rationale is explained in further detail in our previous work.<sup><xref ref-type="bibr" rid="bibr17-1548512911430838">17</xref></sup></p>
<p>This paper is the second in a planned series of three publications, detailing the motivation and development of a DE effort estimation method. The structure of this paper is as follows.</p>
<list id="list1-1548512911430838" list-type="bullet">
<list-item><p>Section 2 provides an overview of DE including uses, methods and experiment life-cycle process.</p></list-item>
<list-item><p>Section 3 describes the variables collected, subjects, collection method and a discussion on validity. The scope of the recorded data per experiment is explained, including the DE experimental design, employment of M&amp;S technology and a profile of the involved personnel, as well as the resulting amount of human resources consumed (‘effort’) across the phases of the experiment life cycle, the phases’ duration (‘calendar time’) and the amount of reuse employed.</p></list-item>
<list-item><p>Section 4 summarizes the results of analysis of the data. Time and effort are broken down by life-cycle phase and DE type. Results of preliminary analysis of the case studies are offered in regard to typical effort expended for experiments, reuse by life-cycle phase, and how time and effort was affected by the number of stakeholders involved.</p></list-item>
<list-item><p>Section 5 summarizes the insights gained and describes future work.</p></list-item>
</list>
</sec>
<sec id="section2-1548512911430838">
<title>2. Defense Experimentation overview</title>
<p>This section defines the DE terms and concepts that are used in this paper.</p>
<sec id="section3-1548512911430838">
<title>2.1 Employment of Defense Experimentation</title>
<p>The COBP-E<sup>8</sup> defines three major uses of experimen-tation:</p>
<p><bold>discovery</bold>– to determine the efficacy of something previously untried;</p>
<p><bold>hypothesis testing</bold>– to examine the validity of an hypothesis; and</p>
<p><bold>demonstration</bold>– to examine and demonstrate a known truth.</p>
<p>Discovery experimentation involves introducing novel systems, concepts, organizational structures or technologies into a setting where the innovation can be employed and established if it has military utility. In a scientific sense these are ‘hypothesis generation’ efforts that will typically be employed early in the development cycle. A disadvantage of discovery experimentation is that it will not ordinarily provide enough information or evidence to reach a valid conclusion (correct understandings of the cause-and-effect or temporal relationships) or be reliable (re-creatable in another experimentation setting).</p>
<p>Hypothesis testing is the classic experimentation used by scholars to advance knowledge by seeking to falsify specific hypotheses (specific if–then statements) or discover their limiting conditions. A disadvantage of this type of experimentation is that since the number of independent, dependent and control variables relevant in the military arena is very large, considerable thought and care is often needed to design and conduct valid hypothesis tests.</p>
<p>Demonstration experiments are not intended to generate new information, but to display existing knowledge to people unfamiliar with it. In such demonstrations, the technologies being demonstrated are well known and the settings (scenario, participants, etc.) are orchestrated to show that the technologies can be employed as expected under the specified conditions.</p>
</sec>
<sec id="section4-1548512911430838">
<title>2.2 Defense Experimentation methods and process</title>
<p>The GUIDEx classifies DE activities through the use of technology (primarily simulation) into one of the following four <italic>DE methods</italic>: human in the loop, constructive, analytic wargames and live.<sup><xref ref-type="bibr" rid="bibr9-1548512911430838">9</xref>,<xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref></sup></p>
<p><bold>Human-in-the-loop</bold> methods represent a category of real-time activities within which humans can interact, typically through the use of simulation. That is, military subjects receive real-time inputs from the simulation, make real-time decisions and direct simulated forces within a simulated scenario.</p>
<p><bold>Constructive</bold> methods are those in which no human intervention occurs in the execution after the analysts choose the initial parameters and then conduct analysis via ‘closed-loop’ simulation.</p>
<p><bold>Analytic wargames</bold> cover a broad range of techniques in which the setting is typically seminar style, whereby the use of technology (such as simulation) is used to stimulate discussion rather than provide the basis for the experiment.</p>
<p><bold>Live</bold> methods are defined by conduct within the real world environment, with actual military units and equipment supplemented with operational prototypes under test (often provided, at least in part, by simulation).</p>
<p>The GUIDEx additionally defines the <italic>DE process</italic>, as applied to all the DE methods, through the following life-cycle phases.</p>
<p><bold>Problem formulation</bold> is the first phase of an experiment where the primary output is a list of experimental questions or hypotheses. The objectives are to identify the warfighting problem, proposed solutions and relevant conditions, and to determine the appropriate metrics, measures and key performance parameters to be placed under test.</p>
<p><bold>Design</bold> decomposes the problem further, such that the questions and hypotheses are selected, treatments and trials are defined, scenarios designed and technical requirements determined.</p>
<p><bold>Development</bold> derives the technical requirements and scenarios to determine the requisite fidelity of the experimental representations. Experimental representations are then selected, or developed, and data collection occurs to populate the representations with scenario-specific information. In addition to the technical details, other ancillary elements, such as organizing participants, ethical issues, security provisions, safety procedures and communication plans, are developed.</p>
<p><bold>Execution</bold> is the undertaking of the experiment, with human resources, utilizing the technical environment across the defined treatments and trials in order to gain information, as defined by the measures of effectiveness and performance.</p>
<p><bold>Analysis</bold> is the final phase whereby the data collected in the execution phase is reviewed with the intent to determine cause-and-effect relationships to provide insights and results towards answering the experiment hypotheses.</p>
</sec>
<sec id="section5-1548512911430838">
<title>2.3 Campaigns of Defense Experimentation</title>
<p>The concept of performing a sequence of related experiments is described as a ‘Campaign of Experimentation’.<sup><xref ref-type="bibr" rid="bibr11-1548512911430838">11</xref></sup> The reason for campaigns, as declared by the COBP-E, is that<disp-quote>
<p>Military operations are too complex and the process of change is too expensive for [any country] to rely on a single experiment to prove that a particular innovation should be adopted.</p>
</disp-quote></p>
<p>The GUIDEx notes a driving reason for conducting campaigns is that<disp-quote>
<p>Using a variety of techniques ensures that weaknesses in one technique can be mitigated by others. Where the results (inferences) correlate between activities, it increases confidence and where they diverge, it provides guidance for further investigation. It is only when all activities are brought together in a coherent manner and the insights synthesised, that the overall problem under investigation is advanced as a whole.</p>
</disp-quote></p>
<p>An illustration of how an idealized Campaign of Experimentation would be sequenced is along a ‘campaign vector’, as presented in <xref ref-type="fig" rid="fig1-1548512911430838">Figure 1</xref>.</p>
<fig id="fig1-1548512911430838" position="float">
<label>Figure 1.</label>
<caption>
<p>Defense Experimentation campaign vector.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig1.tif"/>
</fig>
<p>The three dimensions (<italic>x, y, z</italic>) represent the complexity of the experiment, level of ‘use’ and method of experimentation, respectively. The notion is to start with simple discovery-style wargames and head towards complex demonstrations within live military exercises to explore and understand a new capability.</p>
</sec>
</sec>
<sec id="section6-1548512911430838">
<title>3. Data collection method</title>
<p>This section describes the data collected, the method of collection, introduces the case study subjects and discusses validity.</p>
<sec id="section7-1548512911430838">
<title>3.1 Variables</title>
<p>The selection of data collected was based on the recognized definitions and process for experimentation,<sup><xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref></sup> M&amp;S technology<sup><xref ref-type="bibr" rid="bibr12-1548512911430838">12</xref></sup> and personnel data typically collected for software development estimation methods. At a macro level data was collected across three dimensions of DE planning input variables.</p>
<list id="list2-1548512911430838" list-type="bullet">
<list-item><p><bold>Experiment</bold>– capturing the design and process followed. Examples are COPB-E experiment type, GUIDEx method utilized, number of treatments, number of problem owners and number of DE providers.</p></list-item>
<list-item><p><bold>Technology</bold>– capturing the utilization and employment of M&amp;S technology. Examples are the number of simulation federates used and number and type of objects simulated.</p></list-item>
<list-item><p><bold>Personnel</bold>– capturing the size and quality of the team using the technology in order to implement the design and deliver the experimentation service.</p></list-item>
</list>
<p>See the <xref ref-type="app" rid="app1-1548512911430838">Appendix</xref> for a definition of each input variable terms.</p>
<p>The primary dimension of DE output variables collected relate to <italic>cost</italic>, capturing the effort and time associated with performance of an experiment. Specifically:</p>
<list id="list3-1548512911430838" list-type="bullet">
<list-item><p><bold>effort</bold> (variable) – the measurement of the accumulated person hours applied to a particular phase;</p></list-item>
<list-item><p><bold>calendar time</bold> (variable) – the time expenditure calculated in business days elapsed from a phase’s start to finish;</p></list-item>
<list-item><p><bold>reuse</bold> (modifier) – a percentage factor of effort saved due to work spent on other experiments that was reused as part of the experiment under review.</p></list-item>
</list>
<p>A full listing of the collected variables per case can be found in the <xref ref-type="app" rid="app1-1548512911430838">Appendix</xref>.</p>
</sec>
<sec id="section8-1548512911430838">
<title>3.2 Subjects</title>
<p>Three major organizations were supportive of the case study effort, contributing 50 complete case studies for our research. These organizations resided in Australia and the UK with the primary customers of their experimentation service being the Australian Defence Force and the UK Ministry of Defence, respectively.</p>
<p>The subjects’ organization types are characterized as:</p>
<list id="list4-1548512911430838" list-type="order">
<list-item><p><bold>industry</bold>– a large multinational organization that conducts DE for its own purposes in aid of market research and customer engagement activities and as a contractor to the Government in support of future concepts development;</p></list-item>
<list-item><p><bold>Government</bold>– a Government scientific organization that conducts DE for the purposes of project requirements definition and tactics development.</p></list-item>
<list-item><p><bold>Government–industry consortium</bold>– a Government-sponsored industry consortium that provides DE as a service to the Government on a range of issues from concept development to project requirements definition and tactics development.</p></list-item>
</list>
<p>The contribution of different subject organization types (<xref ref-type="table" rid="table1-1548512911430838">Table 1</xref>) was reasonably uniform providing a good cross section of cases and a solid basis from which to examine the perspectives of different organizational constructs on the DE process.</p>
<table-wrap id="table1-1548512911430838" position="float">
<label>Table 1.</label>
<caption>
<p>Case collection by subject.</p>
</caption>
<graphic alternate-form-of="table1-1548512911430838" xlink:href="10.1177_1548512911430838-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Subject organization type</th>
<th align="left">N</th>
<th align="left">%</th>
</tr>
</thead>
<tbody>
<tr>
<td>Industry</td>
<td>20</td>
<td>40%</td>
</tr>
<tr>
<td>Government</td>
<td>18</td>
<td>36%</td>
</tr>
<tr>
<td>Government–industry consortium</td>
<td>12</td>
<td>24%</td>
</tr>
<tr>
<td>Total</td>
<td>50</td>
<td>100%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In regard to the mix of collected methods (<xref ref-type="table" rid="table2-1548512911430838">Table 2</xref>) we note a complete lack of live experiments in the dataset; as such, all subsequent analysis is done without respect to the live method. In addition, there was a bias of human-in-the-loop experiments due to the popularity of this method with the contributing subjects. As a result the human-in-the-loop results are likely to have a higher statistical reliability than the data collected for the other methods, due to the larger sample size.</p>
<table-wrap id="table2-1548512911430838" position="float">
<label>Table 2.</label>
<caption>
<p>Case collection by method.</p>
</caption>
<graphic alternate-form-of="table2-1548512911430838" xlink:href="10.1177_1548512911430838-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">N</th>
<th align="left">%</th>
</tr>
</thead>
<tbody>
<tr>
<td>Human-in-the-loop</td>
<td>38</td>
<td>74%</td>
</tr>
<tr>
<td>Constructive</td>
<td>8</td>
<td>16%</td>
</tr>
<tr>
<td>Analytic wargames</td>
<td>5</td>
<td>10%</td>
</tr>
<tr>
<td>Live</td>
<td>0</td>
<td>0%</td>
</tr>
<tr>
<td>Total</td>
<td>50</td>
<td>100%</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section9-1548512911430838">
<title>3.3 Data collection</title>
<p>The collection method involved interviews and analysis of post-experiment design and execution documentation.</p>
<p>The majority of the collection points across the experiment and technology variables were quantitative values that were directly recorded from experiment design and experiment development artefacts.</p>
<p>Collection of personnel attributes required analysis of records of project resourcing levels (such as timesheet or expense systems) followed by interviews with the experiment leaders in order to make qualitative assessments of each teams’ experience, as per the determination factors outlined in the <xref ref-type="app" rid="app1-1548512911430838">Appendix</xref> –<italic>Personnel variables</italic> section.</p>
<p>Cost, in regard to calendar time, was taken from a variety of sources that detailed the timeline of a project and phase-passing events. This analysis involved review of project schedules, joining instructions and analysis reports. The phases often started or concluded with formal activities, such as workshops, design reviews, experiment execution or report deliveries, which eased this data field collection.</p>
<p>Effort was the most challenging data point. For the industry and Government–industry consortium organizations, timesheet data gave exact values for effort through the enforced adherence to timesheet policies that recorded only productive time on experiment project codes (i.e. other codes were utilized for administrative functions, such as reading email or attending team meetings). Unfortunately, within the Government organizations, such rigorous time keeping was not present. As such, the Government effort allocation plan, which determines the major tasking an individual is approved to work on throughout the year, was analysed. This information, coupled with a utilization factor (i.e. the percentage of ‘productive’ time in a working week), gave an effort estimate per individual to a particular task. Utilization factors were determined subjectively by the Government project managers and set per case study recorded.</p>
</sec>
<sec id="section10-1548512911430838">
<title>3.4 Validity</title>
<p>Of note is that the case study sample size falls below typically acceptable statistical standards for analysis purposes. As a consequence, many of the results present large variations, making it difficult to form firm conclusions. Therefore, the resulting analysis and discussion should be viewed in this light, with the intent to present initial trends to better understand the dynamics of the DE process. That said, the approach of this research is in accordance with previous engineering estimation model developments. The software engineering COCOMO<sup><xref ref-type="bibr" rid="bibr13-1548512911430838">13</xref>,<xref ref-type="bibr" rid="bibr14-1548512911430838">14</xref></sup> was developed from a sample size of 64 case studies. In addition, the derived systems engineering variant COSYSMO<sup><xref ref-type="bibr" rid="bibr15-1548512911430838">15</xref>,<xref ref-type="bibr" rid="bibr16-1548512911430838">16</xref></sup> was similarly based on 42 case studies. The main reason for such small sample sizes is that estimation methods are developed to assist as a discipline matures, and whilst it is in a maturing phase the ability to access case studies is limited. In this instance the sample size (50 case studies), along with the resulting analysis, is in line with similar recognized efforts that have furthered other emerging engineering-based disciplines.</p>
</sec>
</sec>
<sec id="section11-1548512911430838">
<title>4. Analysis and discussion</title>
<p>Preliminary results of the case study analysis provide a number of valuable insights that help us better understand the dynamics of DE through the lens of our subjects.</p>
<sec id="section12-1548512911430838">
<title>4.1 How much effort and time does a Defense Experimentation project require?</title>
<p>This question is difficult to answer without further context and is the focus of our planned Defense Experimentation Cost Estimation Model (DECOMO) to parametrically formulate a context-inclusive answer. That said, examination of the raw case study data does produce a useful gauge against which to benchmark future activities and set expectations for new users of DE.</p>
<p>The following histograms provide the aggregated effort and calendar time values of the case study set. Regarding the total effort, we see from the histogram (<xref ref-type="fig" rid="fig2-1548512911430838">Figure 2(a)</xref>) that the bulk of total effort results resided around the 2000 hours mark, with two small groups of outliers around 9000 and 13,000 hours.</p>
<fig id="fig2-1548512911430838" position="float">
<label>Figure 2.</label>
<caption>
<p>Total effort and calendar time histograms.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig2.tif"/>
</fig>
<p>In <xref ref-type="table" rid="table3-1548512911430838">Table 3</xref> we see the median was approximately 2500 hours (rounded from 2456 hours), with the lower and upper quartiles falling at 1114 and 3610 hours. The large spread of results demonstrates that effort can vary substantially, depending on the context.</p>
<table-wrap id="table3-1548512911430838" position="float">
<label>Table 3.</label>
<caption>
<p>Overall total costs.</p>
</caption>
<graphic alternate-form-of="table3-1548512911430838" xlink:href="10.1177_1548512911430838-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Mean</th>
<th align="left">σ</th>
<th align="left">Min</th>
<th align="left">Q1</th>
<th align="left">Median</th>
<th align="left">Q3</th>
<th align="left">Max</th>
<th align="left"><italic>Sum</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Total effort (person hours)</td>
<td>3559.37</td>
<td>3769.30</td>
<td>332</td>
<td>1114</td>
<td>2456</td>
<td>3610</td>
<td>14,906</td>
<td>177,969 hours (110 person-years)</td>
</tr>
<tr>
<td>Total calendar time (elapsed days)</td>
<td>167.04</td>
<td>87.034</td>
<td>46</td>
<td>90</td>
<td>159</td>
<td>231</td>
<td>422</td>
<td>8352 days (33.1 elapsed years)</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In regard to the total calendar time histogram (<xref ref-type="fig" rid="fig2-1548512911430838">Figure 2(b)</xref>), we found median and mean calendar time expended to be approximately 7 months (167 days out of 252 total working days in a year), with quartile values between 4 and 11 months.</p>
<p>These aggregate results lead to our first benchmark: a ‘typical’ experiment takes approximately 2500 hours, with a duration of about 7 months.</p>
<p>To better understand the wide variation in results we continued to decompose cost across each phase (<xref ref-type="table" rid="table4-1548512911430838">Table 4</xref>).</p>
<table-wrap id="table4-1548512911430838" position="float">
<label>Table 4.</label>
<caption>
<p>Overall phase costs.</p>
</caption>
<graphic alternate-form-of="table4-1548512911430838" xlink:href="10.1177_1548512911430838-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Mean</th>
<th align="left">σ</th>
<th align="left">Min</th>
<th align="left">Q1</th>
<th align="left">Median</th>
<th align="left">Q3</th>
<th align="left">Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>Problem formulation effort</td>
<td>308.07</td>
<td>618.61</td>
<td>10</td>
<td>21.22</td>
<td>44.9</td>
<td>123.24</td>
<td>2812.5</td>
</tr>
<tr>
<td>Problem formulation calendar time</td>
<td>31.5</td>
<td>29.54</td>
<td>2</td>
<td>10</td>
<td>31.5</td>
<td>33</td>
<td>156</td>
</tr>
<tr>
<td>Design effort</td>
<td>586.77</td>
<td>1033.67</td>
<td>11.2</td>
<td>58.05</td>
<td>136.03</td>
<td>371.25</td>
<td>4860</td>
</tr>
<tr>
<td>Design calendar time</td>
<td>33.59</td>
<td>28.70</td>
<td>5</td>
<td>12.95</td>
<td>22</td>
<td>44</td>
<td>141</td>
</tr>
<tr>
<td>Development effort</td>
<td>1155.63</td>
<td>1293.14</td>
<td>82.71</td>
<td>275.48</td>
<td>646.88</td>
<td>1587.44</td>
<td>5568.75</td>
</tr>
<tr>
<td>Development calendar time</td>
<td>43.12</td>
<td>26.03</td>
<td>12</td>
<td>22.25</td>
<td>34.5</td>
<td>55</td>
<td>129</td>
</tr>
<tr>
<td>Execution effort</td>
<td>918.89</td>
<td>693.89</td>
<td>8</td>
<td>333.3</td>
<td>738.25</td>
<td>1446.88</td>
<td>2868.75</td>
</tr>
<tr>
<td>Execution calendar time</td>
<td>6.86</td>
<td>2.95</td>
<td>1</td>
<td>5</td>
<td>5</td>
<td>10</td>
<td>15</td>
</tr>
<tr>
<td>Analysis effort</td>
<td>590.01</td>
<td>689.78</td>
<td>15.4</td>
<td>76.63</td>
<td>291.25</td>
<td>770.34</td>
<td>2700</td>
</tr>
<tr>
<td>Analysis calendar time</td>
<td>51.97</td>
<td>43.24</td>
<td>10</td>
<td>17.15</td>
<td>28</td>
<td>77</td>
<td>185</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Even at phase-level granularity we could not conclude too much, as the range of results continued to exhibit significant variation. One notable exception was the execution calendar time values, with the majority of experiments requiring between 5 and 10 days (i.e. 1–2 weeks) to execute. That is, independent of all other factors, including the ‘size’ of the effort or ‘length’ of the calendar time duration, the execution phase had a median value of one week.</p>
</sec>
<sec id="section13-1548512911430838">
<title>4.2 Effort and calendar time distributions across the DE life cycle</title>
<p>Poor effort allocation is listed as amongst the major root causes of rework due to insufficiently resourced early-phase activities.<sup><xref ref-type="bibr" rid="bibr18-1548512911430838">18</xref></sup> Understanding distribution of effort in other engineering disciplines, such as software engineering, has been claimed to be the basis for facilitating more reasonable project planning, and is also one of the major underlying functionalities (or assumptions) in most estimating or planning methods.<sup><xref ref-type="bibr" rid="bibr14-1548512911430838">14</xref></sup></p>
<p>As such, as part of our work to date on the planned DECOMO, we were required to build effort distributions for DE to underpin our estimation method. A criticism of some previously published parametric estimation methods has been the use of mathematical approximations for effort distribution, such as the Rayleigh curve used by early versions of the COCOMO and SEER-SEM, that were subsequently proved to be unrepresentative of true effort distribution within engineering projects.<sup><xref ref-type="bibr" rid="bibr18-1548512911430838">18</xref></sup> We sought to avoid this pitfall by utilizing the collected case study information of cost per phase in order to create an empirically based set of distributions (albeit based on a small sample size) by assessing patterns in the cost results of the case studies.</p>
<p>As mentioned, the DE process defines five sequential phases: problem formulation, design, development, execution and analysis. Cost is defined by two separate measures, effort (measured in person hours) and calendar time (measured in working calendar days).</p>
<p>Whilst developing the effort and calendar time distributions, we examined the cost values as a per-phase percentage of the overall activity broken out per method. It should be noted that the resulting trends required ‘optimistic’ trend analysis for the reasons noted in the discussion on validity. The distributions are discussed in detail below.</p>
<sec id="section14-1548512911430838">
<title>4.2.1 Human in the loop</title>
<p><xref ref-type="fig" rid="fig3-1548512911430838">Figure 3</xref> provides two graphical representations of the same data within each figure ((a) and (b)). The first representation is a box and whisker showing the summarized quartile values per phase, and second representation is overlaid spline-fit curves passing through the per phase median and quartile values, with the view of providing the distribution ‘pattern’ across the phases.</p>
<fig id="fig3-1548512911430838" position="float">
<label>Figure 3.</label>
<caption>
<p>Human in the loop – effort and calendar time distribution per phase.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig3.tif"/>
</fig>
<p>In examining the effort distribution for the human-in-the-loop method (<xref ref-type="fig" rid="fig3-1548512911430838">Figure 3(a)</xref>), the data indicates a significant effort growth from the problem formulation and design phases (which are relatively small) through to a high percentage of effort in the development and execution phases, easing off to a moderate level in the analysis phase. The pattern is visible, with the lower quartile, median and upper quartile figures all tracking reasonably consistently with the trend line(s) across the phases.</p>
<p>Human-in-the-loop calendar time profiles (<xref ref-type="fig" rid="fig3-1548512911430838">Figure 3(b)</xref>) exhibit a constant calendar time expense in the problem formulation and design phases, followed by a very slight increase in the development phase, and a sharp decrease in the execution phase followed by a relatively lengthy analysis phase.</p>
<p>Analysis of the effort and calendar time distributions together provides assessment of the ‘intensity of effort’ by comparing percentage values per phase. Considering the intensity of effort for human-in-the-loop, the problem formulation and design phases have comparatively less effort to calendar time, indicating a low concentration of activity in the early phases. This apparently slow start changes dramatically through development and execution, with development revealing a high amount of effort compared to calendar time, implying a high concentration of activity escalating to the execution phase with extreme concentration of activity (the highest effort percentage corresponding with lowest amount of calendar time). Finally, the analysis phase shows similar percentages of calendar time and effort, indicating a consistent intensity of effort.</p>
<p>Worthy of mention is that the human-in-the-loop median patterns track well with our experience in conducting DE projects. That is, the effort pattern presents a similar curve to a traditional software engineering project but with the uniquely high intensity (significant effort during relatively short calendar time) in the development and execution phases.</p>
</sec>
<sec id="section15-1548512911430838">
<title>4.2.2 Constructive</title>
<p>Analysis of the constructive effort data presented an interesting distribution pattern as shown in <xref ref-type="fig" rid="fig4-1548512911430838">Figure 4</xref>. In particular, the design and execution phases had relatively small percentages of effort and calendar time, which differs from the gradual growth patterns observed in analysis of the other two methods.</p>
<fig id="fig4-1548512911430838" position="float">
<label>Figure 4.</label>
<caption>
<p>Constructive – effort and calendar time distribution per phase.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig4.tif"/>
</fig>
<p>Assessing the ‘intensity of effort’, we found similar distributions for both effort and calendar time, signifying that concentration of effort was more consistent across constructive experiments than was evident in the two other techniques. Investigation of the reasons behind this consistency was attributed to the relatively lower numbers of personnel within the constructive experiments surveyed, providing for less dramatic surges in recorded effort than the other methods, equating to more constant distribution of effort over time.</p>
<p>The median effort pattern results align with both our experience and discussions with case study respondents. Specifically during data collection, a topic of re-occurring debate with a number of subjects was that the GUIDEx process did not lend itself to all methods, in particular constructive, where many did not see a difference between the problem formulation and design phases (i.e. for the constructive method they saw it as one phase), nor with the development and execution phases. The proposition was that the constructive method was actually a three-phase process (i.e. problem formulation and design; development and execution; analysis), with the aforementioned combined steps being tightly intertwined iterations and not sequential. This is probably the case for the other methods to some degree (and many similar sequential processes that in reality occur as variations through iteration back-and-forth), but this seems to hold true in particular for the constructive method, with the results supporting this inference.</p>
</sec>
<sec id="section16-1548512911430838">
<title>4.2.3 Analytic wargames</title>
<p>Regarding effort for analytic wargames (<xref ref-type="fig" rid="fig5-1548512911430838">Figure 5(a)</xref>), we found a pattern that is initially small in the problem formulation and design phases, with a moderate ramp up in the development phase and a receding plateau in the execution and analysis phases.</p>
<fig id="fig5-1548512911430838" position="float">
<label>Figure 5.</label>
<caption>
<p>Analytic wargames – effort and calendar time distribution per phase.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig5.tif"/>
</fig>
<p>The calendar time distribution (<xref ref-type="fig" rid="fig5-1548512911430838">Figure 5(b)</xref>) pattern has a similar percentage of time spent in the problem formulation and design phases, with development being the most significant phase, followed by a very short execution and a mid-level analysis phase duration.</p>
<p>Assessing ‘intensity of effort’ for analytic wargames, we see an effort and calendar time set of approximation curves that are similar, with the exception of the development and execution phases. That is, the execution phase exhibits a higher relative median effort in a shorter amount of relative calendar time, indicating a higher period of effort intensity during this phase, on average.</p>
<p>These results as they relate to our experience are less strongly aligned than for the other methods. We attributed this to the low number of samples collected for analytic wargames.</p>
</sec>
<sec id="section17-1548512911430838">
<title>4.2.4 Summary effort distribution by the DE method</title>
<p>In summary, each of the DE methods exhibited dissimilar median effort profiles, which has implications on estimating resource levels over the experiment life cycle.</p>
<p><xref ref-type="fig" rid="fig6-1548512911430838">Figure 6</xref> summarizes the median effort distributions per phase across the three DE methods from the case study dataset. The shading indicates the observed ‘intensity of effort’– light being low intensity to dark being high intensity. Understanding the patterns of effort distribution that emerge is expected to assist with individual or concurrent DE resource planning.</p>
<fig id="fig6-1548512911430838" position="float">
<label>Figure 6.</label>
<caption>
<p>Median phased effort distributions per method.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig6.tif"/>
</fig>
</sec>
</sec>
<sec id="section18-1548512911430838">
<title>4.3 Maximizing reuse to minimize per experiment costs</title>
<p>The reuse factor was qualitatively assigned by experiment leaders during the collection interviews to assess how much effort was saved through reuse of experiment elements from previous activities. This factor enabled the normalization of the cost figures in order to assess each case, as if it were a stand-alone case. It also gave us the ability to establish during which phases reuse was exhibited and more broadly understand the contextual situations that lead to increased reuse (and thus decreased cost).</p>
<p>Examination of the reuse per phase within our case studies is presented in <xref ref-type="table" rid="table5-1548512911430838">Table 5</xref>.</p>
<table-wrap id="table5-1548512911430838" position="float">
<label>Table 5.</label>
<caption>
<p>Overall phase reuse.</p>
</caption>
<graphic alternate-form-of="table5-1548512911430838" xlink:href="10.1177_1548512911430838-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Mean</th>
<th align="left">σ</th>
<th align="left">Min</th>
<th align="left">Q1</th>
<th align="left">Median</th>
<th align="left">Q3</th>
<th align="left">Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>Problem formulation reuse</td>
<td>11.6%</td>
<td>22.9%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>10%</td>
<td>90%</td>
</tr>
<tr>
<td>Design reuse</td>
<td>15.6%</td>
<td>22.8%</td>
<td>0%</td>
<td>0%</td>
<td>5%</td>
<td>20%</td>
<td>90%</td>
</tr>
<tr>
<td>Development reuse</td>
<td>25.6%</td>
<td>29.3%</td>
<td>0%</td>
<td>0%</td>
<td>15%</td>
<td>50%</td>
<td>90%</td>
</tr>
<tr>
<td>Execution reuse</td>
<td>6.2%</td>
<td>9.4%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>10.2%</td>
<td>30%</td>
</tr>
<tr>
<td>Analysis reuse</td>
<td>5.7%</td>
<td>13%</td>
<td>0%</td>
<td>0%</td>
<td>0%</td>
<td>7.5%</td>
<td>70%</td>
</tr>
<tr>
<td>Total reuse</td>
<td><bold>10.8%</bold></td>
<td><bold>12.8%</bold></td>
<td><bold>0%</bold></td>
<td><bold>0%</bold></td>
<td><bold>7.5%</bold></td>
<td><bold>18.7%</bold></td>
<td><bold>45%</bold></td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Reviewing the mean figures, we found mean total reuse to be quite low at 10.8%. Given that each of the subjects’ primary purpose was experimentation provision, we expected much higher reuse values. In assessing which phases gave the greatest opportunity for reuse, we found an increasing level of mean reuse within the problem formulation (11.6%), design (15.6%) and development (25.6%) phases. This was followed by a rapid deterioration in the execution and analysis phases (5.7% and 6.2%, respectively). It was evident there is less likelihood for reuse in the late phases of the process.</p>
<p>Of particular note in <xref ref-type="table" rid="table5-1548512911430838">Table 5</xref> are the relatively high standard deviation values (compared to the mean) and the 0% reuse values for the minimum, lower quartile and median across the majority of the phases. These unusual results indicated there were certain conditions that lead to a much higher amount of reuse over others. Investigations into this issue lead us to further examine Campaigns of Experimentation.</p>
<sec id="section19-1548512911430838">
<title>4.3.1 Campaigns of Experimentation</title>
<p>Campaigns of Experimentation are consciously planned efforts to bring multiple experiments to bear on a particular focused problem or set of problems. As explained in our DE overview, ‘Campaigns’ build evidence from varying sources to increase the validity of the experimentation process.<sup><xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref>,<xref ref-type="bibr" rid="bibr11-1548512911430838">11</xref></sup> As part of our aforementioned investigation our interest was to see if conducting Campaigns of Experimentation also had an impact on reuse rates.</p>
<p><xref ref-type="fig" rid="fig7-1548512911430838">Figure 7</xref> shows the mean reuse per phase of stand-alone (‘single’) experiments versus experiments done within a Campaign of Experimentation. A little over half of the sampled experiments were performed as part of a campaign. We found the overall (total) reuse was three times higher within campaigns than it was for single experiments (5.9% versus 15.3%). Again we see the most significant reuse in the first three phases of problem formulation (20% for ‘campaign’ versus 2.5% for ‘single’), design (22.7% versus 7.9%) and development (33.5% versus 17.1%).</p>
<fig id="fig7-1548512911430838" position="float">
<label>Figure 7.</label>
<caption>
<p>Phase reuse single versus campaign experiment.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig7.tif"/>
</fig>
<p>Our analysis has shown a strong link between Campaigns of Experimentation and effort reuse. Experiments within a campaign provide savings of approximately one-fifth to one-third in the developing phases (problem formulation through development) compared to ‘single’ experiments.</p>
</sec>
</sec>
<sec id="section20-1548512911430838">
<title>4.4 Are there problem-owner and provider economies of scale in Defense Experimentation?</title>
<p>We sought to understand the cost implications of the number of separate organizations on either side of the problem-owner–provider divide. That is, we wanted to answer questions such as ‘Does the involvement of multiple providers improve or degrade the costliness of service delivery?’ and ‘Can we do more with less by utilizing one experiment for multiple problem owners?’.</p>
<p>The relationship between providers to total effort is presented in <xref ref-type="fig" rid="fig8-1548512911430838">Figure 8</xref>. The data provided the following linear prediction formula:</p>
<fig id="fig8-1548512911430838" position="float">
<label>Figure 8.</label>
<caption>
<p>Impact of number of providers on total effort.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig8.tif"/>
</fig>
<p>
<disp-formula id="disp-formula1-1548512911430838">
<mml:math display="block" id="math1-1548512911430838">
<mml:mrow>
<mml:mi>y</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1895</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>1</mml:mn>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>7703</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-1548512911430838" xlink:href="10.1177_1548512911430838-eq1.tif"/>
</disp-formula>
</p>
<p>where <italic>y</italic> is total effort, <italic>x</italic> is the number of providers and <italic>R</italic><sup>2</sup>represents variance.</p>
<p>Taking into consideration the previously stated median experiment effort value of 2500 hours, the rule of thumb consequence is that for each extra provider, the effort cost is approximately a 1.3 multiplier (i.e. the average cost of an experiment is 2500 hours and each extra provider incurs a penalty of 1895.1 hours). The initial assumption was if there were more providers, then the experiment must be more ‘complex’ and therefore explain the higher effort values. On further examination of the technology and experiment variables there was no such correlation to validate this assumption. We postulate the reason is attributed to the act of ‘collaboration’ between different organizations. That is, there is an inherent cost incurred by ‘friction’ between the provider organizations working together (owing to their relatively different cultures, processes, etc.).</p>
<p>Despite not concluding the reasons behind this ‘collaboration cost’, the experiments examined by our research indicate that, for the most economical performance, introducing multiple providers is not a good idea.</p>
<p>We note that the most economical performance may not always be the main goal of a DE activity and often multi-organization collaboration is of greater importance. That is, the experimental process provides a cooperative learning opportunity to bring parties together and increase the mutual understanding of the problems ahead. The increased experiment cost, when viewed in terms of complete costs of the whole NEC programme the experiment is supporting, may provide overall cost savings through risk reduction from working together and developing a shared vision of future challenges (and options available) to the programme.</p>
<p>We next examined the question ‘Are there economies of scale by serving multiple problem owners with the same experiment?’.</p>
<p>The impact of the number of problem owners on total effort presents another increasing trend (<xref ref-type="fig" rid="fig9-1548512911430838">Figure 9</xref>), rendering the following formula:</p>
<fig id="fig9-1548512911430838" position="float">
<label>Figure 9.</label>
<caption>
<p>Impact of the number of problem owners on total effort.</p>
</caption>
<graphic xlink:href="10.1177_1548512911430838-fig9.tif"/>
</fig>
<p>
<disp-formula id="disp-formula2-1548512911430838">
<mml:math display="block" id="math2-1548512911430838">
<mml:mrow>
<mml:mi>y</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>2725</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>4</mml:mn>
<mml:mi>x</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>541</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-1548512911430838" xlink:href="10.1177_1548512911430838-eq2.tif"/>
</disp-formula>
</p>
<p>where <italic>y</italic> is total effort and <italic>x</italic> is the number of problem owners.</p>
<p>Again, utilizing the median experiment effort rule of 2500 hours, the gross impact of increasing the number of problem owners suggests a doubling effect on effort for a given experiment. However, this linear relationship is weaker than our previous example (i.e. has lower variance <italic>R</italic><sup>2</sup>), with a number of notable outliers.</p>
<p>Whilst investigating the impact additional problem owners have on experiment and technology variables, we observed some correlation of increasing ‘complexity’ through rising treatment, object and federate counts. This infers that the extra problem owners had differing needs, driving increased requirements on the technology environment, which in turn incurred increased costs. Our initial assessment is for the purposes of economic experiment execution that the addition of extra problem owners is not a good idea and that each problem owner would be better served by their own (customized) experiment.</p>
</sec>
</sec>
<sec id="section21-1548512911430838">
<title>5. Future work</title>
<p>As previously stated, this paper is the second in a planned series of three publications. The first paper detailed the motivation and rationale, while this paper explained the collection process and investigated preliminary trends. It is clear from the single variable analysis that, although providing useful insights into trends, it has limited value to draw strong conclusions on causal relationships. Therefore, multi-variable analysis is the next logical step to further understanding of our case study database and provide a stronger statistical basis of the impact that the predictor variables experiment, technology and personnel have on our response variable, cost. This analysis will form the basis of our last paper that will describe the DECOMO – providing a series of newly developed formula to help providers and problem owners calculate the cost of proposed DE plans. They are intended to be simple enough to support rapid planning options analysis with a level of confidence not currently available to DE planners, who are currently forced to rely on estimation methods based on expert judgement.</p>
</sec>
<sec id="section22-1548512911430838">
<title>6. Summary and conclusions</title>
<p>In summary, we offer the following conclusions drawn from trends examined in our collected dataset on DE that suggest:</p>
<list id="list5-1548512911430838" list-type="bullet">
<list-item><p>in the absence of further context, the ‘typical’ experiment takes approximately 2500 person hours distributed over 7 months to complete;</p></list-item>
<list-item><p>the execution phase tends to be performed within one week of elapsed time, regardless of the ‘size’ of the experiment;</p></list-item>
<list-item><p>each of the DE methods exhibited dissimilar effort profiles across the experiment life cycle (<xref ref-type="fig" rid="fig6-1548512911430838">Figure 6</xref>), which is a notable consideration for individual or concurrent DE resource planning;</p></list-item>
<list-item><p>conducting experiments with more than one provider incurs an increasing penalty on effort per extra provider (assessed as a 1.3 multiplier) – thus it appears that minimizing providers per experiment is advisable if economical experiment performance is a goal;</p></list-item>
<list-item><p>servicing more than one problem owner per experiment incurs a doubling of effort per extra problem owner – thus the analysis suggests that minimizing the number of problem owners per experiment is advisable if economical performance is a goal; and</p></list-item>
<list-item><p>experiments performed within Campaigns of Experimentation increase the likelihood of effort reuse, providing potential savings of between one-fifth to one-third in the developing phases (problem formulation through development).</p></list-item>
</list>
<p>In conclusion, it is hoped these insights provide useful heuristics and rules of thumb for the planning and execution of DE, ahead of development of the formal estimation techniques through the DECOMO – the final outcome of this line of research. The results are a first step towards helping DE planners optimize use of time and resources in assessing complex defense capability problems, such as NEC concept development.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-1548512911430838">
<title>Appendix:</title>
<sec id="section23-1548512911430838">
<title>collected case study variables for development of the Defense Experimentation Cost Estimation Model</title>
<sec id="section24-1548512911430838">
<title>Predictor variables: experiment, technology and personnel</title>
<p>The following are the definitions of each collected ‘candidate’ predictor variable.</p>
</sec>
<sec id="section25-1548512911430838">
<title>Experiment variables</title>
<list id="list6-1548512911430838" list-type="bullet">
<list-item><p><bold>Number of treatments</bold>– where a treatment is a representation of a capability under test in order to compare against another treatment to determine the change in effectiveness.<sup><xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref></sup></p></list-item>
<list-item><p><bold>Number of conditions</bold>– where a condition is a subset of treatments signifying common environmental settings under which a treatment (or series of treatments) is performed. Each condition defines a significant new set of simulated settings contributing to increased development and testing of the simulated objects within each unique condition.</p></list-item>
<list-item><p><bold>Method</bold>– as defined by the GUIDEx: analytic wargame, constructive, human in the loop or live.<sup><xref ref-type="bibr" rid="bibr10-1548512911430838">10</xref></sup></p></list-item>
<list-item><p><bold>Problem type</bold>– as defined by the <italic>Code of best practise – experimentation</italic>: discovery, hypothesis test or demonstration.<sup><xref ref-type="bibr" rid="bibr8-1548512911430838">8</xref></sup></p></list-item>
<list-item><p><bold>Experimentation campaign</bold>– does the experiment reside as part of a wider Campaign of Experimentation: yes or no.</p></list-item>
<list-item><p><bold>Number of problem owners</bold>– where a problem owner is the individual or organization seeking support.<sup><xref ref-type="bibr" rid="bibr19-1548512911430838">19</xref></sup> A problem owner has explicitly participated in the problem formulation process through contribution of questions with the expectation that the experimentation service will provide a contribution to solving their problem.</p></list-item>
<list-item><p><bold>Number of providers</bold>– where a provider is any organization that has a significant provision role in the problem formulation, design, development, execution or analysis phases of an experiment. This includes regulators, security or scrutiny-based organizations that are integral to the facilitation of service delivery.</p></list-item>
<list-item><p><bold>Scrutiny</bold>– the level of scrutiny is defined by the process of determining ‘fitness for purpose’<sup><xref ref-type="bibr" rid="bibr20-1548512911430838">20</xref></sup> of the experimentation environment. The premise is that an increasing level of formality of this activity increases the procedural overhead of an experiment.</p></list-item>
<list-item><p><bold>Classification</bold>– the information security level required for the experiment, and subsequent handling procedures, as dictated by national security or company information security guidelines.</p></list-item>
</list>
</sec>
<sec id="section26-1548512911430838">
<title>Technology variables</title>
<list id="list7-1548512911430838" list-type="bullet">
<list-item><p><bold>Number of federates</bold>– where a federate is the term applied to an individual model and/or simulation that is part of a federation of models.<sup><xref ref-type="bibr" rid="bibr12-1548512911430838">12</xref></sup></p></list-item>
<list-item><p><bold>Percentage of federate/s reuse/development</bold>– the determination of the amount of development of the overall federation software or systems elements versus the use of the software and systems as is.</p></list-item>
<list-item><p><bold>Object scale</bold>– derived from Hughes et al.’s<sup><xref ref-type="bibr" rid="bibr19-1548512911430838">19</xref></sup> definition of model aggregation of each represented object: sub-system, entity or aggregate.</p></list-item>
<list-item><p><bold>Number of live objects</bold>– whereby the experimental object is represented by real people operating real systems.<sup><xref ref-type="bibr" rid="bibr21-1548512911430838">21</xref></sup></p></list-item>
<list-item><p><bold>Number of virtual objects</bold>– whereby the experimental object is represented by a real person or people operating a simulated system or systems.<sup><xref ref-type="bibr" rid="bibr21-1548512911430838">21</xref></sup></p></list-item>
<list-item><p><bold>Number of constructive objects</bold>– whereby the experimental object is represented by simulated people operating simulated systems.<sup><xref ref-type="bibr" rid="bibr21-1548512911430838">21</xref></sup></p></list-item>
<list-item><p><bold>Unique types of live objects</bold>– the count of unique live objects.</p></list-item>
<list-item><p><bold>Unique types of virtual objects</bold>– the count of unique virtual objects.</p></list-item>
<list-item><p><bold>Unique types of constructive objects</bold>– the count of unique constructive objects.</p></list-item>
</list>
</sec>
<sec id="section27-1548512911430838">
<title>Personnel variables</title>
<list id="list8-1548512911430838" list-type="bullet">
<list-item><p><bold>Number of analysts</bold>– whereby an analyst performs a provisioning role in the conduct of the experiment, responsible for the experiment design, execution and interpreting the outputs through to delivering the results to the client.</p></list-item>
<list-item><p><bold>Number of technicians</bold>– whereby a technical staff member is someone who is responsible for provision of a particular technical asset (such as a simulation or experimental system) required to conduct the experiment.</p></list-item>
<list-item><p><bold>Number of subject matter experts</bold>– whereby a subject matter expert is an individual who provides specific guidance or performance in a role of required expertise for the experiment, as defined by the analyst, such as warfighters for human-in-the-loop wargaming.</p></list-item>
<list-item><p><bold>Analyst capability</bold>– whereby capability is defined as a function of years of experience and formal qualifications in the area of operations research.</p></list-item>
<list-item><p><bold>Analyst experimentation experience</bold>– whereby experimentation experience is defined by a function of years of DE experience and number of experiments performed as an analyst.</p></list-item>
<list-item><p><bold>Technical capability</bold>– whereby capability is defined as a function of years of experience and formal qualifications in the area of software, systems or information technology (IT) engineering.</p></list-item>
<list-item><p><bold>Technical experimentation experience</bold>– whereby experimentation experience is defined by a function of years of DE experience and number of experiments performed as a technical team member.</p></list-item>
<list-item><p><bold>Subject matter expert capability</bold>– whereby capability is defined as a function of years of experience and formal qualifications in the area of represented subject matter expertise.</p></list-item>
<list-item><p><bold>Subject matter expert experimentation experience</bold>– whereby experimentation experience is defined by a function of years of DE experience and number of experiments performed as a subject matter expert in any current or former role.</p></list-item>
</list>
<p>Originally it was intended to record the ‘quality’ of an experiment. Despite best efforts it was clear that determining an objective and repeatable measurement for ‘quality’ was not possible and would be a separate line of research within itself.</p>
</sec>
<sec id="section28-1548512911430838">
<title>Response variable – cost</title>
<p>The eventual response variable is cost defined by two variables and one modifier. Each case included collection of the following:</p>
<list id="list9-1548512911430838" list-type="bullet">
<list-item><p><bold>effort</bold> (variable) – the measurement of the accumulated person hours applied to a particular phase;</p></list-item>
<list-item><p><bold>calendar time</bold> (variable) – the time expenditure calculated in business days elapsed from the phase start to finish;</p></list-item>
<list-item><p><bold>reuse</bold> (modifier) – a percentage factor of effort saved due to work spent on other experiments that was reused as part of the experiment under review.</p></list-item>
</list>
</sec>
</sec>
</app>
</app-group>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors.</p>
</fn>
</fn-group>
<bio>
<title>Author Biographies</title>
<p><bold>Shane D Arnott</bold> is a PhD student at the University of Queensland, Australia. In this effort he wishes to progress the emergent field of DE through collection and analysis of industry data to inform and improve the application of the art in support of systems engineering programs. In his professional life he is chief engineer of the Boeing Phantom Works International Experimentation organization, with technical responsibility over all of Boeing’s DE facilities outside of the United States.</p>
<p><bold>Peter A Lindsay</bold> is Boeing Professor of Systems Engineering at the University of Queensland, Australia. He has held academic and research positions at the University of New South Wales (NSW), the University of Manchester and the University of Illinois at Urbana-Champaign. He is co-author of two books on formal specification and verification of software systems. He has been involved with safety and security critical applications in areas such as air traffic control, embedded medical devices, ship-board defense, emergency service dispatch systems and an international diplomatic network. His current research interests include engineering of complex systems, safety-critical systems, air traffic management and formal methods of system development.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1548512911430838">
<label>1.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alberts</surname><given-names>DS</given-names></name>
</person-group>. <source>Information age transformation</source>. <publisher-loc>Washington DC</publisher-loc>: <publisher-name>Command and Control Research Program</publisher-name>, <year>1998</year>.</citation>
</ref>
<ref id="bibr2-1548512911430838">
<label>2.</label>
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Garstka</surname><given-names>J</given-names></name>
</person-group>. <source>Network centric warfare department of defence</source>. Report to Congress, <year>2001</year>.</citation>
</ref>
<ref id="bibr3-1548512911430838">
<label>3.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alberts</surname><given-names>DS</given-names></name>
<name><surname>Garstka</surname><given-names>J</given-names></name>
<name><surname>Stain</surname><given-names>FP</given-names></name>
</person-group>. <source>Network centric warfare: developing and leveraging information superiority</source>. <edition>2nd ed.</edition> <publisher-loc>Washington DC</publisher-loc>: <publisher-name>Command and Control Research Program</publisher-name>, <year>1999</year>.</citation>
</ref>
<ref id="bibr4-1548512911430838">
<label>4.</label>
<citation citation-type="book">
<collab>NATO</collab>. <source>North Atlantic Treaty Organisation (NATO) code of best practise for command and control assessment</source>. Command and Control Research Program. <publisher-loc>Washington DC</publisher-loc>: <publisher-name>NATO Research and Technology Organisation SAS-026</publisher-name>, <year>2002</year>.</citation>
</ref>
<ref id="bibr5-1548512911430838">
<label>5.</label>
<citation citation-type="book">
<collab>MoD</collab>. <source>JSP 777 network enabled capability</source>. <publisher-loc>London</publisher-loc>: <publisher-name>UK Ministry of Defence</publisher-name>, <year>2003</year>.</citation>
</ref>
<ref id="bibr6-1548512911430838">
<label>6.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Boyd</surname><given-names>C</given-names></name>
<name><surname>Williams</surname><given-names>W</given-names></name>
<name><surname>Skinner</surname><given-names>D</given-names></name>
<name><surname>Wilson</surname><given-names>S</given-names></name>
</person-group>. <article-title>A comparison of approaches to assessing network centric warfare (NCW) concept implementation</article-title>. In: <conf-name>Proceedings of the Systems Engineering Test and Evaluation Conference</conf-name>, <year>2005</year>.</citation>
</ref>
<ref id="bibr7-1548512911430838">
<label>7.</label>
<citation citation-type="book">
<collab>MoD</collab>. <source>MoD strategy for simulation</source>. <publisher-loc>London</publisher-loc>: <publisher-name>United Kingdom Ministry of Defence</publisher-name>, <year>2008</year>.</citation>
</ref>
<ref id="bibr8-1548512911430838">
<label>8.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alberts</surname><given-names>DS</given-names></name>
<name><surname>Hayes</surname><given-names>RE</given-names></name>
</person-group>. <source>Code Of Best Practice for Experimentation (COBP-E)</source>. <publisher-loc>Washington DC</publisher-loc>: <publisher-name>Command and Control Research Program</publisher-name>, <year>2002</year>, p.<fpage>420</fpage>.</citation>
</ref>
<ref id="bibr9-1548512911430838">
<label>9.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kass</surname><given-names>R</given-names></name>
</person-group>. <source>The logic of warfighting experiments</source>. <publisher-loc>Washington DC</publisher-loc>: <publisher-name>Command and Control Research Program</publisher-name>, <year>2006</year>.</citation>
</ref>
<ref id="bibr10-1548512911430838">
<label>10.</label>
<citation citation-type="book">
<collab>TTCP</collab>. <source>Guide for understanding and implementing defense experimentation (GUIDEx)</source>. <edition>1.0 ed.</edition> <publisher-loc>Washington DC</publisher-loc>: <publisher-name>Technical Cooperation Program (TTCP), Joint Systems Analysis (JSA) Group, Methods and Approaches for Warfighting Experimentation Action Group 12 (AG-12)</publisher-name>, <year>2005</year>.</citation>
</ref>
<ref id="bibr11-1548512911430838">
<label>11.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alberts</surname><given-names>DS</given-names></name>
<name><surname>Hayes</surname><given-names>RE</given-names></name>
</person-group>. <source>Campaigns of experimentation: pathways to innovation and transformation</source>. <publisher-loc>Washington DC</publisher-loc>: <publisher-name>Command and Control Research Program</publisher-name>, <year>2005</year>.</citation>
</ref>
<ref id="bibr12-1548512911430838">
<label>12.</label>
<citation citation-type="book">
<collab>DoD</collab>. <source>DoD modeling and simulation glossary</source>. <publisher-name>Under Secretary of Defence Acquisition Technology</publisher-name>, <publisher-loc>US DoD</publisher-loc>, <year>1998</year>.</citation>
</ref>
<ref id="bibr13-1548512911430838">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boehm</surname><given-names>BW</given-names></name>
</person-group>. <article-title>Software engineering economics</article-title>. <source>IEEE Trans Software Eng</source> <year>1984</year>; <volume>10</volume>: <fpage>4</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr14-1548512911430838">
<label>14.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Boehm</surname><given-names>BW</given-names></name>
<name><surname>Abts</surname><given-names>C</given-names></name>
<name><surname>Winsor-Brown</surname><given-names>A</given-names></name><etal/>
</person-group>. <source>Software cost estimation with COCOMO II</source>. <publisher-name>Prentice Hall</publisher-name>, <year>2000</year>. <publisher-loc>New Jersey, USA</publisher-loc>.</citation>
</ref>
<ref id="bibr15-1548512911430838">
<label>15.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Lane</surname><given-names>J</given-names></name>
</person-group>. <article-title>Cost model extensions to support systems engineering cost estimation for complex systems and systems of systems</article-title>. In: <conf-name>Proceedings of the Conference on Systems Engineering Research</conf-name>, <year>2009</year>.</citation>
</ref>
<ref id="bibr16-1548512911430838">
<label>16.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Valerdi</surname><given-names>B</given-names></name>
<name><surname>Boehm</surname><given-names>BW</given-names></name>
</person-group>. <article-title>COSYSMO: a constructive systems engineering cost model coming of age</article-title>. In: <conf-name>Proceedings of the INCOSE conference</conf-name>, <year>2003</year>.</citation>
</ref>
<ref id="bibr17-1548512911430838">
<label>17.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Arnott</surname><given-names>S</given-names></name>
<name><surname>Lindsay</surname><given-names>P</given-names></name>
</person-group>. <article-title>Reducing uncertainty in systems engineering through defence experimentation</article-title>. In: <conf-name>Proceedings of the Improving Systems and Software Engineering conference</conf-name>, <conf-loc>Brisbane</conf-loc>, <year>2010</year>.</citation>
</ref>
<ref id="bibr18-1548512911430838">
<label>18.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Ye</surname><given-names>Y</given-names></name>
<name><surname>Mei</surname><given-names>H</given-names></name>
<name><surname>Mingshu</surname><given-names>L</given-names></name>
<name><surname>Qing</surname><given-names>W</given-names></name>
<name><surname>Boehm</surname><given-names>B</given-names></name>
</person-group>. <year>2008</year>. <article-title>Phase distribution of software development effort</article-title>. In <conf-name>Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement (ESEM ’08)</conf-name>. <conf-loc>ACM, New York, NY, USA</conf-loc>, <fpage>61</fpage>–<lpage>69</lpage>. DOI: <pub-id pub-id-type="doi">10.1145/1414004.1414016</pub-id> <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/1414%2000%204.141401">http://doi.acm.org/10.1145/1414 00 4.141401</ext-link></citation>
</ref>
<ref id="bibr19-1548512911430838">
<label>19.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hughes</surname><given-names>W</given-names></name><etal/>
</person-group>., <source>Military Modeling for Decision Making</source>. <edition>Third ed</edition>, ed. <person-group person-group-type="editor">
<name><surname>Hughes</surname><given-names>W.</given-names></name>
</person-group>. <year>1997</year>, <publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>Military Operations Research Society</publisher-name>.</citation>
</ref>
<ref id="bibr20-1548512911430838">
<label>20.</label>
<citation citation-type="book">
<collab>DMSO</collab>. <source>VV&amp;A recommended practices guide - glossary</source>. <publisher-name>Defence Modelling and Simulation Office</publisher-name>, <publisher-loc>US DoD</publisher-loc>, <year>2001</year>.</citation>
</ref>
<ref id="bibr21-1548512911430838">
<label>21.</label>
<citation citation-type="other">
<collab>DoD</collab>. <source>A glossary of modeling and simulation terms for distributed interactive simulation (DIS)</source>. <year>1995</year>.</citation>
</ref>
</ref-list>
</back>
</article>