<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LTJ</journal-id>
<journal-id journal-id-type="hwp">spltj</journal-id>
<journal-title>Language Testing</journal-title>
<issn pub-type="ppub">0265-5322</issn>
<issn pub-type="epub">1477-0946</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0265532212451235</article-id>
<article-id pub-id-type="publisher-id">10.1177_0265532212451235</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The differences among three-, four-, and five-option-item formats in the context of a high-stakes English-language listening test</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Lee</surname><given-names>HyeSun</given-names></name>
<aff id="aff1-0265532212451235">University of Nebraska-Lincoln, USA</aff>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Winke</surname><given-names>Paula</given-names></name>
<aff id="aff2-0265532212451235">Michigan State University, USA</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0265532212451235">Paula Winke, Department of Linguistics and Languages, Michigan State University, East Lansing, MI 48824-1027, USA. Email: <email>winke@msu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>1</issue>
<fpage>99</fpage>
<lpage>123</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>We adapted three practice College Scholastic Ability Tests (CSAT) of English listening, each with five-option items, to create four- and three-option versions by asking 73 Korean speakers or learners of English to eliminate the least plausible options in two rounds. Two hundred and sixty-four Korean high school English-language learners formed three groups. Each took three of the nine tests, one with five-option items, one with four-, and one with three-, with administrations counterbalanced to control for order and practice effects. Mean test scores of the three-option tests were significantly higher than those of four- and five-option tests. While no difference was found in mean item discriminations across the three different test formats, reliability coefficients showed inconsistent patterns depending on the number of options and test versions. One possible interpretation of the low correlations among the scores of three test formats is that items with different numbers of options tap into skills other than listening. The findings suggest that statistically, three options may or may not be optimal depending on the point of view taken – from that of the test score users, or from that of the test stakeholders. Test developers must consider multiple statistical, affective, and contextual factors in determining the optimal number of options.</p>
</abstract>
<kwd-group>
<kwd>Educational measurement</kwd>
<kwd>high-stakes testing</kwd>
<kwd>item formatting</kwd>
<kwd>multiple-choice testing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Multiple-choice items are a popular test format in language testing because they are objective, reliable, easy to score, and efficient to administer. But good multiple-choice items are hard to write. A particular challenge is writing <italic>plausible</italic> distractors – that is, distractors that look like a possible correct answer to test takers who have little knowledge of or skill in the underlying construct the test is intended to measure (<xref ref-type="bibr" rid="bibr28-0265532212451235">Haladyna, 2004</xref>). In theory, plausible distractors should be chosen by low performers and eliminated by high performers who disregard them by knowing and choosing the correct answer. Non-plausible distractors are statistically worthless because they do not get selected by anyone – they are quickly eliminated and have no inherent value to the test, unless, of course, the test designer added them for affective reasons, such as to lighten test takers’ moods or to satisfy test takers’ psychological need to be able to eliminate options quickly. But in a high-stakes or standardized testing situation, such <italic>affective distractors</italic> are not the norm. All distractors are to be plausible. Thus, researchers have, remarkably, spent more than 80 years trying to determine the best number of options in a multiple-choice item (<xref ref-type="bibr" rid="bibr49-0265532212451235">Rodriguez, 2005</xref>). The basic question commonly posed is, should it be three, four, or five?</p>
<p>Operationally, standardized and high-stakes second language (L2) tests vary in the number of options their items have. For example, the College Scholastic Ability Test (CSAT), an extremely high-stakes test in Korea used to determine who can go to college and where, contains a five-option, multiple-choice, English-language-listening test section. Not only is the test high-stakes, it is also administered on a large scale – 648,946 college applicants took the test in 2011 (see the Korea Institute for Curriculum and Evaluation website <ext-link ext-link-type="uri" xlink:href="http://www.kice.re.kr/ko/">www.kice.re.kr/ko/</ext-link>). But what if the test had four- or three-option items? Would the test’s psychometric characteristics such as reliability or power in discriminating among test takers differ? In this paper we investigate these questions and use the CSAT-English-listening test section as the context. Before proceeding, we discuss prior theoretical and empirical research that has investigated the number of options multiple-choice items should have. We explain why this topic is in need of further investigation within the context of high-stakes, L2 testing, and we then introduce the specific research questions and the mixed-methods design we employed.</p>
<sec id="section1-0265532212451235">
<title>Literature review</title>
<p>The optimal number of options for multiple-choice items has long been debated in educational research (<xref ref-type="bibr" rid="bibr49-0265532212451235">Rodriguez, 2005</xref>), with most educational measurement researchers arguing that three-option items are best (e.g. <xref ref-type="bibr" rid="bibr8-0265532212451235">Bruno &amp; Dirkzwagber, 1995</xref>; <xref ref-type="bibr" rid="bibr17-0265532212451235">Crehan, Haladyna, &amp; Brewer, 1993</xref>; <xref ref-type="bibr" rid="bibr19-0265532212451235">Delgado &amp; Prieto, 1998</xref>; <xref ref-type="bibr" rid="bibr29-0265532212451235">Haladyna &amp; Downing, 1993</xref>; <xref ref-type="bibr" rid="bibr36-0265532212451235">Landrum, Cashin, &amp; Theis, 1993</xref>; <xref ref-type="bibr" rid="bibr46-0265532212451235">Owen &amp; Froman, 1987</xref>; <xref ref-type="bibr" rid="bibr49-0265532212451235">Rodriguez, 2005</xref>; <xref ref-type="bibr" rid="bibr63-0265532212451235">Trevisan, Sax, &amp; Michael, 1991</xref>, <xref ref-type="bibr" rid="bibr64-0265532212451235">1994</xref>) because, as summed up by <xref ref-type="bibr" rid="bibr17-0265532212451235">Crehan et al. (1993)</xref>, they are easier to write, have more effective distractors, and take less time to administer. Such arguments are not new. In his meta-analysis of research on the optional number of options in multiple-choice tests, <xref ref-type="bibr" rid="bibr49-0265532212451235">Rodriguez (2005)</xref> noted that even in the early 20th century, researchers in education claimed that three-option items have fewer detrimental effects on test outcomes when compared to four- or five-option items (e.g. <xref ref-type="bibr" rid="bibr52-0265532212451235">Ruch &amp; Stoddard, 1927</xref>). For example, <xref ref-type="bibr" rid="bibr62-0265532212451235">Tversky (1964)</xref>, following <xref ref-type="bibr" rid="bibr41-0265532212451235">Lord (1944)</xref>’s theoretical research on reliability, claimed that the optimal number of options is three because three maximizes a test’s discrimination, power, and information.<sup><xref ref-type="fn" rid="fn1-0265532212451235">1</xref></sup> <xref ref-type="bibr" rid="bibr23-0265532212451235">Ebel (1969)</xref> did similar research and provided a chart regarding the reliability estimates of the optimal number of items in relation to the total number of items and the number of options per item. According to Ebel, even though reliability increases with more options per item, the incremental change successively decreases after three. That is, the reliability increase from three to four options is smaller than in the change from two to three. <xref ref-type="bibr" rid="bibr27-0265532212451235">Grier (1975)</xref> came to the same conclusion that the use of three options could maximize test reliability when 18 or more items were used. <xref ref-type="bibr" rid="bibr42-0265532212451235">Lord (1977)</xref> also concluded that three options were optimal for test takers, especially at the medium level of ability.</p>
<p>But according to <xref ref-type="bibr" rid="bibr10-0265532212451235">Budescu and Nevo (1985)</xref>, some of the assumptions that underlie such theorizing – the assumption of proportionality and the assumption of equally attractive options (which is related to the assumption that random guessing exists when the correct answer is not known and that an item’s distractors are all equally plausible) – are rarely true in actual testing contexts. Budescu and Nevo debated the optimality of three options and emphasized that researchers must contextualize the question in relation to test content, type of test, and test takers’ abilities.</p>
<p>Furthermore, educational measurement research results and practice in the field conflict. For example, in a review of multiple-choice, item-writing guidelines for classroom assessment, <xref ref-type="bibr" rid="bibr30-0265532212451235">Haladyna, Downing, and Rodriguez (2002)</xref> reported that 70% of textbooks and research studies cited the rule ‘write as many plausible distractors as you can.’ Only 4% suggested limiting the number of options. In language testing, basic textbooks for future language testers provide guidelines in how to write four-option, multiple-choice items (<xref ref-type="bibr" rid="bibr2-0265532212451235">Alderson, 2000</xref>; <xref ref-type="bibr" rid="bibr3-0265532212451235">Alderson, Clapham, &amp; Wall, 1995</xref>; <xref ref-type="bibr" rid="bibr5-0265532212451235">H. D. Brown, 2004</xref>; <xref ref-type="bibr" rid="bibr7-0265532212451235">J. D. Brown, 2005</xref>; <xref ref-type="bibr" rid="bibr9-0265532212451235">Buck, 2001</xref>; <xref ref-type="bibr" rid="bibr31-0265532212451235">Hughes, 2003</xref>; <xref ref-type="bibr" rid="bibr44-0265532212451235">McNamara, 2000</xref>; <xref ref-type="bibr" rid="bibr47-0265532212451235">Purpura, 2004</xref>; <xref ref-type="bibr" rid="bibr48-0265532212451235">Read, 2000</xref>). Little attention is given to the writing of three-option or five-option items as alternatives, and motives for doing so are not discussed. <xref ref-type="bibr" rid="bibr5-0265532212451235">Brown (2004)</xref>, for instance, simply explains that multiple-choice items can have three to five options and provides examples. The only discussion on the differences among three-, four-, and five-option items in language testing textbooks appears to be that three-option items provide a greater chance of random guessing over four- or five-option items (see <xref ref-type="bibr" rid="bibr31-0265532212451235">Hughes, 2003</xref>, in particular). The textbooks explain that providing more options reduces the chances of correct or <italic>blind</italic>, random guessing. Such an argument may suggest to novice language testers that multiple-choice questions with more options are fundamentally better, even as the same textbooks unanimously forewarn that writing plausible distractors is extremely time consuming and difficult. Teachers using these textbooks in their language testing classes may need to discuss more fully the balancing act test developers need to perform in estimating the right number of options. The decision may be based in part on the following: the item writers’ item-development skills: the resources for item-writer training; the finances and time allowed for item development; and the specific test context, such as the construct being measured, the age and ability of the test takers, and the testing platform (online, computer-adaptive, or on paper).</p>
<p>The question language testers might pose is the following: Why have educational measurement researchers claimed that three-option multiple-choice items are better than four- or five-option multiple-choice items? First, the notion that fewer options increase the chances that a test’s score variance is influenced by blind guessing is debated in the educational measurement literature (<xref ref-type="bibr" rid="bibr36-0265532212451235">Landrum et al., 1993</xref>; <xref ref-type="bibr" rid="bibr49-0265532212451235">Rodriguez, 2005</xref>; <xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers &amp; Harley, 1999</xref>). For example, Rodriguez deliberated on the argument that blind guessing may only be problematic when lower-ability test takers run out of time in a timed test and randomly answer remaining items. Most test takers, he concluded, do <italic>not</italic> blindly guess (<xref ref-type="bibr" rid="bibr22-0265532212451235">Ebel, 1968</xref>, reported a blind guessing rate of 3 to 8%, while <xref ref-type="bibr" rid="bibr18-0265532212451235">Currie and Chiramanee, 2010</xref>, reported a rate of 6.7%); rather, they make educated guesses, or as <xref ref-type="bibr" rid="bibr53-0265532212451235">Rupp, Ferne, and Choi (2006</xref>, p. 464) wrote, test takers employ a process of “conditional informed guessing.” Rupp et al. explained that test takers guess only as a last resort. They first eliminate some options based on their knowledge and logic. Blind guessing may therefore be a statistically negligible factor.</p>
<p>Second, many education researchers have found few differences between tests with three-option-items and tests with more than three options in terms of the tests’ average item discrimination indices (<xref ref-type="bibr" rid="bibr17-0265532212451235">Crehan et al., 1993</xref>; <xref ref-type="bibr" rid="bibr30-0265532212451235">Haladyna et al., 2002</xref>) and overall estimates of test reliability (<xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers &amp; Harley, 1999</xref>). Third, researchers have predominately found that items with more options are more likely to have distractors that are not plausible (<xref ref-type="bibr" rid="bibr14-0265532212451235">Cizek, Robinson, &amp; O’Day, 1998</xref>). And fourth, researchers have concluded that fewer options result in less need for test takers to have <italic>testwiseness</italic> to succeed (<xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers &amp; Harley, 1999</xref>), that is, talent in being able to apply appropriate and effective test-taking strategies that relate directly to the test format (<xref ref-type="bibr" rid="bibr54-0265532212451235">Sarnaki, 1979</xref>). <xref ref-type="bibr" rid="bibr45-0265532212451235">Millman, Bishop and Ebel (1965)</xref> defined testwiseness specifically as a test taker’s “capacity to utilize the characteristics and formats of the test and/or the test taking situation to receive a high score” (p. 707). Testwiseness can be considered something that helps test takers maximize their observed test scores (<xref ref-type="bibr" rid="bibr51-0265532212451235">Rogers &amp; Yang, 1996</xref>), but it is also something that is independent of the test takers’ knowledge of the subject matter being tested (<xref ref-type="bibr" rid="bibr45-0265532212451235">Millman et al., 1965</xref>). More options make multiple-choice items more susceptible to testwiseness, and this may be responsible for the common finding that item difficulty increases with an increase in the number of options (<xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers &amp; Harley, 1999</xref>). Fewer options, researchers conclude, provide for a better test of the skill being measured (<xref ref-type="bibr" rid="bibr16-0265532212451235">Costin, 1972</xref>; <xref ref-type="bibr" rid="bibr17-0265532212451235">Crehan et al., 1993</xref>; <xref ref-type="bibr" rid="bibr19-0265532212451235">Delgado &amp; Prieto, 1998</xref>; <xref ref-type="bibr" rid="bibr26-0265532212451235">Green, Sax, &amp; Michael, 1982</xref>; <xref ref-type="bibr" rid="bibr46-0265532212451235">Owen &amp; Froman, 1987</xref>; <xref ref-type="bibr" rid="bibr63-0265532212451235">Trevisan et al., 1991</xref>) and are, ultimately, easier and take less time for teachers to write (<xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers &amp; Harley, 1999</xref>).</p>
<p>What is particularly interesting for language testing researchers is that none of the studies reviewed above (except <xref ref-type="bibr" rid="bibr53-0265532212451235">Rupp et al., 2006</xref>, which did not investigate the optimal number of options – it was a qualitative study on the cognitive response processes that L2 learners engage in when they choose multiple-choice options on an L2 reading test) were conducted in the context of L2 testing. One of the few studies to do so was conducted by <xref ref-type="bibr" rid="bibr55-0265532212451235">Shizuka, Takeuchi, Yashima, and Yoshizawa (2006)</xref>. They investigated the effects of three- and four-option items on test performance within the context of an L2 English reading test used as a university entrance exam in Japan. They changed an original four-option reading test to a three-option test by discarding the least chosen option from a previous administration of the test. One hundred and ninety-two Japanese English-language learners who had not taken the original test took the revised test. In common with outcomes from educational measurement research, their results indicated that the average item facility and average item discrimination between the four-option-item test and the three-option-item test were not significantly different. Also, test reliability was not significantly different across test formats. Furthermore, in their analysis of distractors, they found that the average number of actual plausible distractors was less than two, regardless of the number of options the items had. Thus, the researchers claimed that items with three options are optimal, considering three- and four-option items had relatively equal item facility and item discrimination. Furthermore, they noted that by having items with three options, test takers may be able to add items and thus improve test reliability. More recently, <xref ref-type="bibr" rid="bibr18-0265532212451235">Currie and Chiramanee (2010)</xref> conducted a study in the context of L2 testing that investigated how multiple-choice items differ from open-ended items in measuring L2 English grammar. Relevant to research investigating the optional number of options in multiple-choice items, they included three versions of the multiple-choice test in their investigation – three-, four-, and five-option versions. They found three-option items were easier for the learners in their study (L2 English learners in Thailand), but there were no significant differences in item facility between the four- and five-option items. They noted that multiple-choice testing is widespread in ESL and EFL contexts worldwide; thus it is important, they wrote, that researchers come to understand how L2 learning outcomes are shaped by the type of L2 tests learners take (2010, p. 488).</p>
</sec>
<sec id="section2-0265532212451235">
<title>The need for further research</title>
<p>This study investigates the claims that the number of multiple-choice options matters in L2 testing contexts. Only one of the studies reviewed above was primarily concerned with the optional number of options and was conducted in a high-stakes, L2-test situation (<xref ref-type="bibr" rid="bibr55-0265532212451235">Shizuka et al., 2006</xref>), but it only compared three versus four options. <xref ref-type="bibr" rid="bibr18-0265532212451235">Currie and Chiramanee (2010)</xref> reported data on whether L2 grammar tests with three-, four-, and five-option items assessed the same underlying construct by estimating correlation coefficients among the scores from the three-, four-, and five-option tests, but the correlation coefficients (which ranged from .860 to .991) were run <italic>between groups</italic> (and not within groups), meaning that different groups of learners took the different versions of the tests, making direct comparisons of scores problematic. Thus, the empirical studies described above did not employ research designs that would allow for investigations into how the scores from the tests with various option numbers correlated in terms of the construct measured – either the tests with differing numbers of options did not overlap in test specifications and content, or the test takers were divided into distinct groups (as in <xref ref-type="bibr" rid="bibr18-0265532212451235">Currie &amp; Chiramanee, 2010</xref>; <xref ref-type="bibr" rid="bibr55-0265532212451235">Shizuka et al., 2006</xref>) with participants taking test versions that differed only in the number of options. Furthermore, test takers’ opinions regarding items with three, four, or five options in connection with the specific testing context are relatively unexplored (cf. <xref ref-type="bibr" rid="bibr46-0265532212451235">Owen &amp; Froman, 1987</xref>). Test purposes, score uses, and qualitative investigations into test impacts may be particularly enlightening for understanding why tests have items with the number of options they do. Such information might also clarify what might result from implementing changes suggested by quantitative investigations into the optimal number of multiple-choice item options. The question might not be how many options are optimal universally, but rather how many options are optimal in a given testing context.</p>
<p>Additionally, research is needed that explores what test takers’ preferences are regarding the number of options. Language testers in particular should understand test takers’ opinions regarding potential changes to the number of options in the high-stakes tests that they must take. Thus, we present four research questions to investigate the psychometric and psychological outcomes from varying the number of options in CSAT prep-tests. The research questions are as follows:</p>
<list id="list1-0265532212451235" list-type="order">
<list-item><p>Does L2-listening-test performance vary depending on whether the test has three-, four-, or five-option multiple-choice items? Specifically, does the number of options affect the average level of test difficulty and item discrimination indices?</p></list-item>
<list-item><p>Do L2 listening tests with three-, four-, and five-option items consistently measure test takers’ listening skills? Additionally, do L2 listening tests with three different numbers of options assess the same skills?</p></list-item>
<list-item><p>Does the time it takes test takers to respond to multiple-choice items differ in relation to the number of options the items have?</p></list-item>
<list-item><p>What are test takers’ opinions regarding three-, four-, and five-option-item formats on the high-stakes, CSAT English listening test?</p></list-item></list>
</sec>
<sec id="section3-0265532212451235">
<title>The context of the current study</title>
<p>The College Scholastic Ability Test (CSAT) is used in Korea to screen applicants for entrance into universities. One portion of the test, the CSAT English-language test, is a 70-minute test that includes a 20-minute listening section. All of the CSAT, including the English-language listening test, consists of multiple-choice items with five options. The Korea Institute for Curriculum and Evaluation (KICE) is responsible for developing test items, administering the CSAT, reporting the CSAT results, and analyzing the CSAT data. The KICE has used five-option items in the CSAT since 1993 – before that, all items on the test had four options. Also, the KICE, Seoul metropolitan office of education, and Kyounggi and Incheon provincial offices of education develop CSAT prep-tests (as large-scale nationwide prep-tests) and administer them seven times a year to future test takers. Most high school students take more than five prep-tests a year as a part of school curricula. Total test takers for the CSAT in 2011 was 648,946 (see the KICE website <ext-link ext-link-type="uri" xlink:href="http://www.kice.re.kr/ko/">www.kice.re.kr/ko/</ext-link>) and the number of test takers of the prep-test administered in November, 2011 was 587,446 (see the website of Gyeonggi provincial office of education <ext-link ext-link-type="uri" xlink:href="http://www.goedu.kr/index.jsp">www.goedu.kr/index.jsp</ext-link>). The test development team is composed of content experts and professional psychometricians who also work on test evaluation. Like the CSAT development procedure, the prep-test development is conducted with high security. As <xref ref-type="bibr" rid="bibr11-0265532212451235">Card (2005)</xref> and <xref ref-type="bibr" rid="bibr13-0265532212451235">Choi (2008)</xref> described,<sup><xref ref-type="fn" rid="fn2-0265532212451235">2</xref></sup> the consequential impact of the CSAT results on each test taker is not to be underestimated. The test affects an individual’s future social status in Korea and that of his or her family. The CSAT prep-tests are important because they provide opportunities for practice and help diagnose test takers’ problems and difficulties in taking the CSAT.</p>
<p>When it comes to the development of tests, writing items with more options rather than less takes more effort, time, and money (<xref ref-type="bibr" rid="bibr1-0265532212451235">Aamodt &amp; McShane, 1992</xref>; <xref ref-type="bibr" rid="bibr10-0265532212451235">Budescu &amp; Nevo, 1985</xref>; <xref ref-type="bibr" rid="bibr19-0265532212451235">Delgado &amp; Prieto, 1998</xref>; <xref ref-type="bibr" rid="bibr28-0265532212451235">Haladyna, 2004</xref>; <xref ref-type="bibr" rid="bibr29-0265532212451235">Haladyna &amp; Downing, 1993</xref>; <xref ref-type="bibr" rid="bibr46-0265532212451235">Owen &amp; Froman, 1987</xref>; <xref ref-type="bibr" rid="bibr49-0265532212451235">Rodriguez, 2005</xref>; <xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers &amp; Harley, 1999</xref>; <xref ref-type="bibr" rid="bibr55-0265532212451235">Shizuka et al., 2006</xref>; <xref ref-type="bibr" rid="bibr56-0265532212451235">Sidick, Barrett, &amp; Doverspike, 1994</xref>; <xref ref-type="bibr" rid="bibr58-0265532212451235">Straton &amp; Catts, 1980</xref>). This is especially true of the CSAT and its prep-tests. For example, in developing CSAT items, more than 650 personnel, including content experts and psychometricians, are isolated in a secure place for 33 days (<xref ref-type="bibr" rid="bibr38-0265532212451235">Lee, 2005</xref>). As proposed by <xref ref-type="bibr" rid="bibr55-0265532212451235">Shizuka et al. (2006)</xref>, it would be beneficial if test developers could spend fewer resources developing options with enhanced measurement accuracy. Thus investigating the impact of the number of options items have on test performance is warranted. But in this study we are not solely interested in the statistical comparisons among test formats to figure out the optimal number of options; we are also interested in exploring contextual factors related to the test and stakeholders’ opinions in regard to the optional number of options for this test. Thus, in this study we employ both quantitative and qualitative research methods (mixed method), described below.</p>
</sec>
<sec id="section4-0265532212451235" sec-type="methods">
<title>Methods</title>
<sec id="section5-0265532212451235">
<title>Participants</title>
<p>The study participants were 264 Korean high school students (40 males and 224 females) from six 10th-grade, English language classes in a private high school in Seoul. The participants were taught English for four hours per week in Korean by Korean English teachers. They had been learning English for seven years through classroom-oriented instruction mainly focused on reading comprehension and grammar. They rarely had opportunities to use English outside the classroom like in other EFL learning contexts.</p>
</sec>
<sec id="section6-0265532212451235">
<title>Materials</title>
<sec id="section7-0265532212451235">
<title>The CSAT English listening tests</title>
<p>We used three authentic CSAT English listening prep-tests for this research. The three tests we used were developed by the Gyeonggi provincial office of education and administered nationwide in November 2007 (version I), November 2008 (version II), and November 2009 (version III). We downloaded the three tests from a website hosted by the Gyeonggi provincial office of education (<ext-link ext-link-type="uri" xlink:href="http://www.goedu.kr/index.jsp">www.goedu.kr/index.jsp</ext-link>). All three versions had 17 five-option listening items with the same test format. The tests differed in content but had similar item stems. For example, the first item in all three versions assessed whether test takers can understand a description of a picture (a baby’s toy in version I, a table clock in version II, and a humidifier in version III); test takers had to choose the correct picture after listening to the audio file. That is, each version asked similar questions (had similar prompts or stems) but contained different audio content. This is because prep-test developers try to keep test characteristics as similar as possible regardless of the test administered.</p>
</sec>
<sec id="section8-0265532212451235">
<title>Option deletion method</title>
<p>We adapted each of the three original five-option multiple-choice tests (I5, II5, and III5) into two additional forms – ones with four-option items (I4, II4, and III4) and ones with three-option items (I3, II3, and III3) – by deleting the least plausible option for each item. <xref ref-type="bibr" rid="bibr10-0265532212451235">Budescu and Nevo (1985)</xref> had two procedures to reduce the number of options in a multiple-choice test: They (based on the previous test administration data) deleted the most attractive option to create an easier version and deleted the least attractive option to create a more difficult version. We decided the latter option was more representative of what real test administrators would do, thus, we sought to delete the least plausible distractors to create items with fewer options. Unlike the previous studies, we deleted options based on many different item evaluators’ judgments. A total of 73 Koreans (13 graduate students matriculated in various universities in the United States, 15 Korean English instructors teaching at English institutes in Korea, and 45 Korean 9th-grade students in schools located in Korea) participated by selecting the least plausible options while taking the listening tests (they circled what they thought was the correct answer, and then crossed out the least plausible option from the remaining options). In contrast to previous studies relying on item analysis statistics or judgments from (at most) several evaluators, the deletion method in this study reflected a more balanced perspective (from test developers and test takers) on the least plausible options. The various perspectives (from advanced-level test takers represented by graduate students, from lower-level test takers represented by 9th-grade EFL students, and from content experts represented by the instructors) helped to produce what we believed would be highly parallel tests through the elimination of the least plausible options. These new forms were labeled as tests I4, II4, and III4 (all with four-option items), and I3, II3, and III3 (all with three-option items). As a result, we had nine different tests to investigate the research questions.</p>
</sec>
<sec id="section9-0265532212451235">
<title>Survey</title>
<p>We designed a survey to provide additional information beyond the quantitative results. This survey asked all participants the number of options they preferred (three, four, or five) and their opinions about changing the option format in the CSAT. As recommended by <xref ref-type="bibr" rid="bibr43-0265532212451235">Mackey and Gass (2005)</xref> and <xref ref-type="bibr" rid="bibr20-0265532212451235">Dörnyei and Taguchi (2009)</xref>, the survey was written in the native language of the participants, Korean, so that their English proficiency would not affect the quality of their responses. The survey, which had seven items, is in the <xref ref-type="app" rid="app1-0265532212451235">Appendix</xref>.</p>
</sec>
</sec>
<sec id="section10-0265532212451235">
<title>Procedure</title>
<p>The data collection sessions were in May 2010. Six intact English classes were randomly assigned into three groups to take the nine different tests, administered to the three groups based on Latin Squares to control for order and practice effects. There were three test sessions and one survey session. That is, all participants took three tests, one with three-option items, one with four-, and one with five-, each with different versions (versions I, II, and III) and with the order of presentation counterbalanced. The three different groups took their three tests with a one-week interval between each administration. The test administration schedule demonstrating the complete-block design of this study (illustrating that every participant took each of the three test forms under investigation) is presented in <xref ref-type="table" rid="table1-0265532212451235">Table 1</xref>.</p>
<table-wrap id="table1-0265532212451235" position="float">
<label>Table 1.</label>
<caption><p>Test administration schedule</p></caption>
<graphic alternate-form-of="table1-0265532212451235" xlink:href="10.1177_0265532212451235-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Group</th>
<th align="left">Male/Female</th>
<th align="left" colspan="4">Session<hr/></th>
</tr>
<tr>
<th/>
<th/>
<th align="left">1</th>
<th align="left">2</th>
<th align="left">3</th>
<th align="left">4</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 (<italic>n</italic> = 86)</td>
<td>11/75</td>
<td><bold>II4</bold></td>
<td><bold>I3</bold></td>
<td><bold>III5</bold></td>
<td>Survey</td>
</tr>
<tr>
<td/>
<td/>
<td>(2008 content, four options)</td>
<td>(2007 content, three options)</td>
<td>(2009 content, five options)</td>
<td/>
</tr>
<tr>
<td>2 (<italic>n</italic> = 89)</td>
<td>15/74</td>
<td><bold>I5</bold></td>
<td><bold>III4</bold></td>
<td><bold>II3</bold></td>
<td>Survey</td>
</tr>
<tr>
<td/>
<td/>
<td>(2007 content, five options)</td>
<td>(2009 content, four options)</td>
<td>(2008 content, three options)</td>
<td/>
</tr>
<tr>
<td>3 (<italic>n</italic> = 89)</td>
<td>14/75</td>
<td><bold>III3</bold></td>
<td><bold>II5</bold></td>
<td><bold>I4</bold></td>
<td>Survey</td>
</tr>
<tr>
<td/>
<td/>
<td>(2009 content, three options)</td>
<td>(2008 content, five options)</td>
<td>(2007 content, four options)</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0265532212451235">
<p><italic>Note: n</italic> = number of test takers.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>As in the operational test, the 17 listening items were presented in a paper-based CSAT test booklet to the test takers. The test takers were allowed to look over the listening items before listening to the audio files, which were played through speakers at the test site. Each item comprised a brief direction (e.g. <italic>listen to the conversation and choose what the man will do next</italic>). There was a 12–15-second pause between the playing of audio files, and during the pause test takers chose their answers. Each test administration took approximately 20 minutes.</p>

<p>To investigate the claim that multiple-choice questions with fewer options can lead to shorter exam times, at least four participants in each test session (37 participants in total) recorded their response time per item by using a stop watch. Finally, to explore the test takers’ opinions about the three, option-item formats, we asked all the participants to fill out an anonymous, paper-based survey directly after they completed the third session. They took from 10 to 15 minutes to write responses to the seven questions.</p>
</sec>
<sec id="section11-0265532212451235">
<title>Scoring and coding</title>
<sec id="section12-0265532212451235">
<title>Listening tests</title>
<p>To increase item discrimination, the CSAT developers use a scoring system that assigns items more or fewer points based on a pre-expected level of difficulty. The most difficult item out of the 17 is given three points and the easiest item is given one point. The other 15 items are given two points. In this study, however, each item was equally scored right (1 point) or wrong (0 points), with the rationale that the pre-expected item difficulty may be different from the actual item difficulty that can be identified only after the administration of a test.</p>
</sec>
<sec id="section13-0265532212451235">
<title>Survey</title>
<p>The responses from the open-ended items on the survey were analyzed through open coding, using an inductive-analytic approach (<xref ref-type="bibr" rid="bibr60-0265532212451235">Thomas, 2006</xref>). The first researcher, Lee, a native speaker of Korean, devised initial categories by first reading all responses. She segmented the data into detailed categories by investigating connected patterns and removing overlapping themes, as suggested by <xref ref-type="bibr" rid="bibr4-0265532212451235">Berg (2008)</xref>. Once the categories were drafted, the researcher coded 10% of the data, as advised by <xref ref-type="bibr" rid="bibr6-0265532212451235">J. D. Brown (2001)</xref> and <xref ref-type="bibr" rid="bibr12-0265532212451235">Chandler (2003)</xref>. Then, to improve the validity of the coding categories and the consistency of the coding procedure, a second native-Korean-speaking rater coded the same 10% of the data. By comparing results, inter-coder agreement was 87%. The two researchers discussed the coding categories and reasons why some data were coded differently. Discrepancies in the coding scheme and in the process of applying that scheme were settled through discussion. Fine-grained changes to the categories and the coding process were made. Lee then used the revised coding categories to code the entire data set.</p>
</sec>
</sec>
</sec>
<sec id="section14-0265532212451235" sec-type="results">
<title>Results</title>
<sec id="section15-0265532212451235">
<title>Quantitative</title>
<sec id="section16-0265532212451235">
<title>Preliminary data analysis</title>
<p>Because we used intact classes, we needed to ensure that the three groups were comparable. We obtained the mean score of each class from a norm-referenced, school-based, English-language achievement test comprosed of reading and listening sections they all took as part of their regular curriculum one month before the first research session. The mean scores of three groups were equivalent: 70.74 (out of 100) for group 1, 69.98 for group 2, and 70.12 for group 3. The groups’ assignments to the three different study procedure paths were at random.</p>
<p>To ensure that the three original tests with five-option items (I5, II5, and III5) were equally difficult, we analyzed the total scores of the three five-option-item tests using one-way ANOVA. Even though the normality assumption seemed to be untenable due to skewness (the Kolmogorov-Smirnov statistic was significant), we could employ ANOVA because (a) ANOVA is quite robust against the violation of normality with equal group sizes (<xref ref-type="bibr" rid="bibr32-0265532212451235">Keppel &amp; Wickens, 2007</xref>; <xref ref-type="bibr" rid="bibr34-0265532212451235">Kirk, 1995</xref>), and (b) the sample sizes of the three groups were almost equal (<italic>n</italic><sub><italic>I5</italic></sub> = 89, <italic>n</italic><sub><italic>II5</italic></sub> = 89, <italic>n</italic><sub><italic>III5</italic></sub> = 86); <xref ref-type="bibr" rid="bibr59-0265532212451235">Tabachnick and Fidell (2007)</xref> stated that sample sizes can be regarded as relatively equal if the ratio of the largest to smallest group size is four to one or less. The homogeneity of variance assumption was met. The result revealed that test I5 (<italic>M</italic> = 12.65, <italic>SD</italic> = 3.21) was relatively more difficult than test II5 (<italic>M</italic> = 14.31, <italic>SD</italic> = 3.04) and test III5 (<italic>M</italic> = 14.71, <italic>SD</italic> = 3.07). <italic>F</italic> statistic indicated a significant difference across the three tests, <italic>F</italic> (2, 261) = 10.9, <italic>p</italic> = 000, <italic>ω</italic><sup><italic>2</italic></sup> = 0.07. Test I5 differed significantly from test II5 and test III5 (<italic>p</italic> = .001, <italic>d</italic> = 0.53, <italic>p</italic> = .000, <italic>d</italic> = 0.21, respectively).</p>
<p>To control the level of difficulty, we adopted equipercentile equating. Equating is a statistical procedure to adjust differences in scores from the test forms with the different levels of difficulty for the interchangeable use of test scores (<xref ref-type="bibr" rid="bibr35-0265532212451235">Kolen &amp; Brennan, 2004</xref>). According to <xref ref-type="bibr" rid="bibr40-0265532212451235">Livingston (2004)</xref>, equipercentile equating is transforming scores on a new test form to scores on a reference form by matching the percentile rank in the new test group to that in the reference test group. As a result, the mean, standard deviation, and skewness of the scores in the new form will be quite the same as those in the reference test (Livingston). We used the version I tests (I3, I4, and I5, the hardest tests) as reference tests and equated the total scores of test version II (II3, II4, and II5) and version III (III3, III4, and III5). Thus, we were able control two nuisance variables that were not of interest in the study: group effect and the effect of the difficulty of the underlying test forms. This equating process made all nine tests sufficiently comparable so we could investigate whether the number of options affected the tests’ outcomes (see <xref ref-type="table" rid="table2-0265532212451235">Table 2</xref>).</p>
<table-wrap id="table2-0265532212451235" position="float">
<label>Table 2.</label>
<caption><p>Descriptive statistics of all nine tests (before vs. after equating)</p></caption>
<graphic alternate-form-of="table2-0265532212451235" xlink:href="10.1177_0265532212451235-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Option</th>
<th align="left">Version</th>
<th align="left"><italic>N</italic></th>
<th align="left">Mean</th>
<th align="left"><italic>SD</italic></th>
<th align="left">Skewness</th>
<th align="left">Kurtosis</th>
</tr>
</thead>
<tbody>
<tr>
<td>3</td>
<td>I3</td>
<td>86</td>
<td>14.03</td>
<td>2.43</td>
<td>−1.02</td>
<td>0.83</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(14.03)</td>
<td>(2.43)</td>
<td>(−1.02)</td>
<td>(0.83)</td>
</tr>
<tr>
<td/>
<td>II3</td>
<td>89</td>
<td>13.87</td>
<td>3.44</td>
<td>−1.70</td>
<td>2.73</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(14.02)</td>
<td>(2.46)</td>
<td>(−1.06)</td>
<td>(0.95)</td>
</tr>
<tr>
<td/>
<td>III3</td>
<td>89</td>
<td>15.46</td>
<td>1.78</td>
<td>−1.44</td>
<td>2.52</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(14.02)</td>
<td>(2.39)</td>
<td>(−1.16)</td>
<td>(1.11)</td>
</tr>
<tr>
<td>4</td>
<td>I4</td>
<td>89</td>
<td>13.25</td>
<td>3.25</td>
<td>−0.95</td>
<td>0.27</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(13.25)</td>
<td>(3.25)</td>
<td>(−0.95)</td>
<td>(0.27)</td>
</tr>
<tr>
<td/>
<td>II4</td>
<td>86</td>
<td>14.53</td>
<td>2.42</td>
<td>−1.29</td>
<td>2.81</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(13.26)</td>
<td>(3.17)</td>
<td>(−0.90)</td>
<td>(−0.08)</td>
</tr>
<tr>
<td/>
<td>III4</td>
<td>89</td>
<td>14.34</td>
<td>2.8</td>
<td>−1.11</td>
<td>0.85</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(13.23)</td>
<td>(3.25)</td>
<td>(−1.00)</td>
<td>(0.33)</td>
</tr>
<tr>
<td>5</td>
<td>I5</td>
<td>89</td>
<td>12.65</td>
<td>3.20</td>
<td>−0.74</td>
<td>0.25</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(12.65)</td>
<td>(3.20)</td>
<td>(−0.74)</td>
<td>(0.25)</td>
</tr>
<tr>
<td/>
<td>II5</td>
<td>89</td>
<td>14.31</td>
<td>3.04</td>
<td>3.79</td>
<td>0.51</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(12.64)</td>
<td>(3.20)</td>
<td>(−0.78)</td>
<td>(0.20)</td>
</tr>
<tr>
<td/>
<td>III5</td>
<td>86</td>
<td>14.71</td>
<td>3.07</td>
<td>−2.01</td>
<td>4.66</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(12.62)</td>
<td>(3.15)</td>
<td>(−0.85)</td>
<td>(0.23)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0265532212451235">
<p><italic>Note:</italic> Maximum score = 17. The equated value is in the parenthesis.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section17-0265532212451235">
<title>Mean test scores</title>
<p>To examine research question one (‘Does the number of options affect the average level of test difficulty?’), we used Latin Square analysis with the assumption that there would be no interaction among testing groups, test versions, and the number of options. The five-option tests were hardest (<italic>M</italic> = 12.64, <italic>SD</italic> = 3.17), the four-option tests were in-between (<italic>M</italic> = 13.25, <italic>SD</italic> = 3.21), and the three-option tests were the easiest (<italic>M</italic> = 14.02, <italic>SD</italic> = 2.42). The result indicated that the number of options significantly affected the mean test scores, <italic>F</italic> (2, 875) = 14.40, <italic>p</italic> = .000, <italic>η</italic><sup><italic>2</italic></sup> = 0.04, while the effects of test versions and group were not significant. Tukey <italic>HSD</italic> post hoc tests indicated that the mean score in three-option tests was significantly different from that in four-option tests (<italic>p</italic> = .008. <italic>d</italic> = 0.27) and that in five-option tests (<italic>p</italic> = .000, <italic>d</italic> = 0.49). No significant difference was found between four- and five-option listening test scores.</p>
</sec>
<sec id="section18-0265532212451235">
<title>Item discrimination</title>
<p>To explore the second part of research question one, that is, how listening tests with different numbers of options function in terms of discriminating among applicants, we derived point-biserial correlations (item to total score correlations). Because we used the original (unequated) test scores to compute point-biserial correlations, we separated the analyses according to each test version (I, II, and III). We used Fisher’s <italic>z</italic> transformation (transformed the coefficient <italic>r</italic> to <italic>z</italic> value) to compute the mean item discrimination indices and to conduct significance tests of the mean item discrimination indices among three-, four-, and five-option tests. In version I, the four-option test had slightly higher mean item discrimination (<italic>r</italic><sub><italic>pb</italic></sub> = 0.49) than in the five-option test (by 0.03). In version II, the mean item discrimination in the three-option test (<italic>r</italic><sub><italic>pb</italic></sub> = 0.57) was as large as that in the five-option test (<italic>r</italic><sub><italic>pb</italic></sub> = 0.57). In version III, the item discrimination of the five-option test was the largest (<italic>r</italic><sub><italic>pb</italic></sub> = 0.57). But no statistically significant differences were detected, corroborating other research that also found no significant difference in discrimination indices between multiple-choice tests with three versus four options (<xref ref-type="bibr" rid="bibr16-0265532212451235">Costin, 1972</xref>; <xref ref-type="bibr" rid="bibr17-0265532212451235">Crehan et al., 1993</xref>; <xref ref-type="bibr" rid="bibr19-0265532212451235">Delgado &amp; Prieto, 1998</xref>; <xref ref-type="bibr" rid="bibr46-0265532212451235">Owen &amp; Froman, 1987</xref>) or between tests with three versus five options (<xref ref-type="bibr" rid="bibr46-0265532212451235">Owen &amp; Froman, 1987</xref>). (See <xref ref-type="table" rid="table3-0265532212451235">Table 3</xref>.)</p>
<table-wrap id="table3-0265532212451235" position="float">
<label>Table 3.</label>
<caption><p>Item discrimination index (point-biserial correlation)</p></caption>
<graphic alternate-form-of="table3-0265532212451235" xlink:href="10.1177_0265532212451235-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Version</th>
<th align="left">Option</th>
<th align="left">Mean item discrimination (<italic>r</italic><sub><italic>pb</italic></sub>)</th>
<th align="left">Comparison</th>
<th align="left"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>I</td>
<td>3</td>
<td>0.42</td>
<td>I3 × I4</td>
<td>.80</td>
</tr>
<tr>
<td/>
<td>4</td>
<td>0.49</td>
<td>I4 × I5</td>
<td>.87</td>
</tr>
<tr>
<td/>
<td>5</td>
<td>0.46</td>
<td>I5 × I3</td>
<td>.92</td>
</tr>
<tr>
<td>II</td>
<td>3</td>
<td>0.57</td>
<td>II3 × II4</td>
<td>.68</td>
</tr>
<tr>
<td/>
<td>4</td>
<td>0.46</td>
<td>II4 × II5</td>
<td>.99</td>
</tr>
<tr>
<td/>
<td>5</td>
<td>0.57</td>
<td>II5 × II3</td>
<td>.67</td>
</tr>
<tr>
<td>III</td>
<td>3</td>
<td>0.39</td>
<td>III3 × III4</td>
<td>.80</td>
</tr>
<tr>
<td/>
<td>4</td>
<td>0.47</td>
<td>III4 × III5</td>
<td>.53</td>
</tr>
<tr>
<td/>
<td>5</td>
<td>0.57</td>
<td>III5 × III3</td>
<td>.70</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0265532212451235">
<p><italic>Note: N</italic> = 17 items. Bonferroni adjustment. <italic>p</italic>  .017.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section19-0265532212451235">
<title>Test reliability</title>
<p>To investigate research question two (‘Do L2 listening tests with three-, four-, and five-option items consistently measure test takers’ listening skills?’), Cronbach’s alpha coefficients were examined. The reliability coefficients in three-option tests, especially in version I and III, were relatively low (<italic>α</italic><sub><italic>I3</italic></sub> = 0.66 and <italic>α</italic><sub><italic>III3</italic></sub> = 0.63) compared with that in version II (<italic>α</italic><sub><italic>II3</italic></sub> = 0.85). In contrast to the claim in previous research, each version revealed different patterns of reliability coefficients: Among version III tests, the five-option test (III5) showed highest reliability (<italic>α</italic><sub><italic>II5</italic></sub> = 0.86), whereas the reliability of the three-option test (II3) was highest among version II tests (<italic>α</italic><sub><italic>II3</italic></sub> = 0.85). With four-option tests, version I had the highest reliability (<italic>α</italic><sub><italic>I4</italic></sub> = 0.79) (see <xref ref-type="table" rid="table4-0265532212451235">Table 4</xref>). Since reliability coefficients were computed using variances of original (unequated) test scores, we separately conducted overall significance tests according to test versions and explored where the differences existed (if statistical significance was detected). We used the <italic>AlphaTest</italic> (<xref ref-type="bibr" rid="bibr37-0265532212451235">Lautenschlager &amp; Meade, 2008</xref>) program based on the <italic>UX</italic><sub><italic>1</italic></sub> statistic as used in <xref ref-type="bibr" rid="bibr24-0265532212451235">Feldt, Woodruff, and Salih (1987)</xref>. According to Feldt et al., the <italic>UX</italic><sub><italic>1</italic></sub> statistic works well to control the Type I error rate even with negatively skewed and heterogeneous variance data when the sample size approaches 100. Each group’s size in the current data was close to 100, so we could employ this test statistic without concern about the violation of assumptions. While in version I the coefficients (alphas) of the three-, four-, and five-option tests did not differ significantly, in version II and III, those of the three-, four-, and five-option tests indicated significant differences (<italic>χ</italic><sup>2</sup> (2) = 7.02, <italic>p</italic> = .03, <italic>χ</italic><sup>2</sup> (2) = 16.53, <italic>p</italic> = .000, respectively). To reveal where the significant differences existed, we conducted pairwise comparisons with Bonferroni adjustments (<italic>p</italic> equals or is less than .017). Significant differences were found between the three- and four-option test pair (<italic>α</italic><sub><italic>II3</italic></sub> vs. <italic>α</italic><sub><italic>II4</italic></sub>) in version II and the three- and five-option test pair (<italic>α</italic><sub><italic>III3</italic></sub> vs. <italic>α</italic><sub><italic>III5</italic></sub>) in version III (<italic>χ</italic><sup><italic>2</italic></sup> (1) = 5.74, <italic>p</italic> = .017, <italic>χ</italic><sup><italic>2</italic></sup> (1) = 16.27, <italic>p</italic> = .000, respectively). The easier versions (version II and III) revealed significant differences in terms of test score reliability between the three-option tests and the four- and five-option tests.</p>
<table-wrap id="table4-0265532212451235" position="float">
<label>Table 4.</label>
<caption><p>Test score reliability coefficients (Cronbach’s alpha)</p></caption>
<graphic alternate-form-of="table4-0265532212451235" xlink:href="10.1177_0265532212451235-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Version</th>
<th align="left">3-option</th>
<th align="left">4-option</th>
<th align="left">5-option</th>
<th align="left"><italic>χ</italic><sup><italic>2</italic></sup></th>
<th align="left"><italic>df</italic></th>
<th align="left"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>I</td>
<td>0.66</td>
<td>0.79</td>
<td>0.76</td>
<td>4.46</td>
<td>2</td>
<td>.11</td>
</tr>
<tr>
<td>II</td>
<td><bold>0.85</bold></td>
<td><bold>0.74</bold></td>
<td>0.85</td>
<td>7.02<xref ref-type="table-fn" rid="table-fn3-0265532212451235">*</xref></td>
<td>2</td>
<td>.03</td>
</tr>
<tr>
<td>III</td>
<td><bold>0.63</bold></td>
<td>0.78</td>
<td><bold>0.86</bold></td>
<td>16.53<xref ref-type="table-fn" rid="table-fn3-0265532212451235">*</xref></td>
<td>2</td>
<td>.000</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0265532212451235">
<p><italic>Note:</italic> The version I tests were harder than the other two versions. *Overall comparison, <italic>p</italic> &lt; .05. and pairwise comparison, <italic>p</italic>  .017. Bolded data in a row indicates where the statistical differences between test reliabilities are.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section20-0265532212451235">
<title>Test score correlation</title>
<p>To further investigate research question two (‘Do L2 listening tests with three different numbers of options assess the same skills?’), we derived correlations between the test scores. Correlations are regarded as one type of construct validity evidence that can be used to assess whether two tests taken by the same group of test takers overlap in construct (<xref ref-type="bibr" rid="bibr39-0265532212451235">Lissitz, 2009</xref>; <xref ref-type="bibr" rid="bibr61-0265532212451235">Thorndike, 1982</xref>). We separately derived correlations on each group (group 1, 2, and 3), based on the equated total scores of each group of test takers. We used the equated scores to control for the levels of difficulty in tests. The coefficients revealed that the three test scores did not correlate highly in groups 1 and 2. However, in test group 3 the magnitude of the correlation was relatively larger than that of group 1 and 2 (see <xref ref-type="table" rid="table5-0265532212451235">Table 5</xref>). The correlation coefficients are as follows: 0.42 ≤ <italic>r</italic> ≤ 0.54 (group 1), 0.52 ≤ <italic>r ≤</italic> 0.53 (group 2), and 0.63 ≤ <italic>r ≤</italic> 0.83 (group 3). Rather than interpreting the results solely by relying on the statistical outcome, we considered the four potential factors that might affect the low correlation coefficients in this current study: (a) restriction of range (lack of score variability in groups); (b) different shapes of distributions; (c) existence of influential outliers; and (d) measurement error (low reliability) (<xref ref-type="bibr" rid="bibr25-0265532212451235">Goodwin &amp; Leech, 2006</xref>). First, the minimum and maximum values in each test group did not cast doubts on the restriction of range or lack of score variability, even though the distributions of data were skewed. Also, standard deviation, skewness, and kurtosis values did not indicate that the distributions of test scores were different from one another in each group (see <xref ref-type="table" rid="table2-0265532212451235">Table 2</xref> for descriptive statistics and <xref ref-type="fig" rid="fig1-0265532212451235">Figure 1</xref> for the histograms related to the shape of the score distributions). To reveal whether outliers prompted the low correlation coefficients, we removed 11 cases among the 264 participants by using Tukey’s boxplot method (<xref ref-type="bibr" rid="bibr65-0265532212451235">Tukey, 1977</xref>). Among the detected outliers by the boxplot method, we removed only the cases that showed one or two extremely low score(s) among the three test scores with the rationale that the unusual low score(s) among the three test scores might not be indicators of the test taker’s true ability. In other words, unexpected factors (e.g. motivation, fatigue), not test takers’ ability, might have been the cause of the extremely low score(s). If all three scores were low, we did not regard this case as an outlier even though the boxplot method detected the case as an outlier. However, the correlation coefficients, with the elimination of outliers, were almost the same: 0.45 ≤ <italic>r</italic> ≤ 0.52 (group 1), 0.45 ≤ <italic>r ≤</italic> 0.60 (group 2), and 0.65 ≤ <italic>r ≤</italic> 0.81 (group 3). Then, we investigated the possibility that measurement error affected the coefficients. As indicated, the reliability coefficients for the three-option tests in version I and III were relatively lower than for the other tests (<italic>α</italic><sub><italic>I3</italic></sub> = 0.66, <italic>α</italic><sub><italic>III3</italic></sub> = 0.63, respectively), thus we used correction for attenuation to estimate the expected correlation coefficient if no measurement error was assumed (<xref ref-type="bibr" rid="bibr57-0265532212451235">Spearman, 1904</xref>). However, the disattenuated values in group 1 and 2 may not be regarded as relatively strong correlations (<xref ref-type="bibr" rid="bibr7-0265532212451235">J. D. Brown, 2005</xref>): 0.6 ≤ <italic>r</italic> ≤ 0.68 (group 1), 0.65 ≤ <italic>r ≤</italic> 0.7 (group 2), as against 0.81 ≤ <italic>r ≤</italic> 1.0 (group 3). It is possible that the four factors set out above, either separately or compositely, might have contributed to the low coefficients. Possibly four factors, either separately or compositely, might affect low coefficients. However, based on the data it cannot be said that the low correlations among the three different test formats were due to outliers or measurement error (unreliability).</p>
<table-wrap id="table5-0265532212451235" position="float">
<label>Table 5.</label>
<caption><p>Test score correlation coefficients</p></caption>
<graphic alternate-form-of="table5-0265532212451235" xlink:href="10.1177_0265532212451235-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Test group</th>
<th/>
<th align="left">3-option</th>
<th align="left">4-option</th>
<th align="left">5-option</th>
</tr>
</thead>
<tbody>
<tr>
<td>Group 1</td>
<td>3-option</td>
<td>1</td>
<td>0.42</td>
<td>0.47</td>
</tr>
<tr>
<td>(<italic>n</italic> = 86)</td>
<td/>
<td/>
<td>0.45</td>
<td>0.48</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>0.60</td>
<td>0.62</td>
</tr>
<tr>
<td/>
<td colspan="4"><hr/></td>
</tr>
<tr>
<td/>
<td>4-option</td>
<td/>
<td>1</td>
<td>0.54</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>0.52</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>0.68</td>
</tr>
<tr>
<td/>
<td colspan="4"><hr/></td>
</tr>
<tr>
<td/>
<td>5-option</td>
<td/>
<td/>
<td>1</td>
</tr>
<tr>
<td>Group 2</td>
<td>3-option</td>
<td>1</td>
<td>0.53</td>
<td>0.52</td>
</tr>
<tr>
<td><italic>(n</italic> = 89)</td>
<td/>
<td/>
<td>0.45</td>
<td>0.60</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>0.66</td>
<td>0.65</td>
</tr>
<tr>
<td/>
<td colspan="4"><hr/></td>
</tr>
<tr>
<td/>
<td>4-option</td>
<td/>
<td>1</td>
<td>0.53</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>0.55</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>0.70</td>
</tr>
<tr>
<td/>
<td colspan="4"><hr/></td>
</tr>
<tr>
<td/>
<td>5-option</td>
<td/>
<td/>
<td>1</td>
</tr>
<tr>
<td>Group 3</td>
<td>3-option</td>
<td>1</td>
<td>0.63</td>
<td>0.83</td>
</tr>
<tr>
<td><italic>(n</italic> = 89)</td>
<td/>
<td/>
<td>0.65</td>
<td>0.81</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>0.90</td>
<td>1.00</td>
</tr>
<tr>
<td/>
<td>4-option</td>
<td/>
<td>1</td>
<td>0.66</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>0.69</td>
</tr>
<tr>
<td/>
<td colspan="4"><hr/></td>
</tr>
<tr>
<td/>
<td>5-option</td>
<td/>
<td/>
<td>1</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0265532212451235">
<p><italic>Note:</italic> The second value in each cell is without outliers and the third value is the disattenuated coefficient.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<fig id="fig1-0265532212451235" position="float">
<label>Figure 1.</label>
<caption><p>Standardized measures of difficulty, discrimination, and reliability across the three test versions with three-, four-, and five-options. Version I = solid line; version II = dot/dash/dot; version III = single dash. In the first graph of item difficulty, lower mean scores indicate a more difficult test (difficulty level is the inverse of mean test scores); not all three lines are visible because they overlap.</p></caption>
<graphic xlink:href="10.1177_0265532212451235-fig1.tif"/>
</fig>
</sec>
<sec id="section21-0265532212451235">
<title>Response time</title>
<p>Research question three asked, ‘Does the time it takes test takers to respond to multiple-choice items differ according to the number of options the items have?’ In line with previous research (e.g. <xref ref-type="bibr" rid="bibr46-0265532212451235">Owen &amp; Froman, 1987</xref>), the answer is yes. A total of 37 participants recorded the time it took them to answer test questions: 12 participants recorded their time for three-option items, 13 for four-option items, and 12 for five-option items, with the participants balanced almost equally among all nine individual testing sessions. The average item response time for the 17 items (recorded from the time the audio file began to when the item response was selected) was 668 seconds for three-option items (mean response time per item was 39 seconds), 696 seconds for four-option items (41 seconds per item), and 736 seconds for the five-option item tests (43 seconds per item). The differences in response time between the different versions of the 17-item test are thus as follows: between the three- and four-option tests, 28 seconds; between the four- and five-option tests, 40 seconds; and between the three- and five-option tests, 68 seconds. This indicates that if the administration is held constant at 20 minutes, changing the CSAT listening test from a five-option item test to a four- or three-option item one would allow for the inclusion of one more item.</p>
</sec></sec>
<sec id="section22-0265532212451235">
<title>Qualitative</title>
<p>We addressed research question four (‘What are test takers’ opinions regarding three-, four-, and five-option-item formats on a high-stakes CSAT English listening test?’) by investigating the 264 test takers’ written responses to the survey questions. (The survey is in the <xref ref-type="app" rid="app1-0265532212451235">Appendix</xref>.)</p>
<p>Regarding the first question as to which item format test takers prefer, 54% of the participants (<italic>n</italic> = 143) responded that they preferred three-option items, 28% (<italic>n</italic> = 75) preferred four-option items, and 17% (<italic>n</italic> = 45) chose five-option items. (To retain as much data as possible, we used pairwise deletion.) We also asked everyone how much they liked the three-option items (using a seven-point Likert-scale, with 7 being ‘like most’ and 1 being ‘dislike most’). The average score on this item was 5.10, with those choosing three options on the first question liking three-option items the most (<italic>M</italic> = 5.94); those choosing four options liking them less (<italic>M</italic> = 4.36); and those preferring five options liking them the least (<italic>M</italic> = 3.60).</p>
<p>Their written responses in relation to the first question were revealing. Among the 143 respondents who preferred the three-option items, 48% wrote that they did so because it was easier to select the correct answer. For example, one test taker who preferred three-option items wrote, ‘I can easily choose an answer.’ Another wrote, ‘I feel less anxiety with three-option items.’ Thirteen percent of the 143 respondents wrote that they liked the three-option format because it was user friendly and saved time. Among the 75 who preferred items with four options, 62% wrote that they thought four was the proper number of options, which was also a common theme with those liking five-option items (e.g. ‘I think that the number of five options is proper’). On the other hand, ones preferring three options indicated that they thought three was the proper number of options for a multiple-choice question. Another common refrain was liking four- or five-option items because that was what they were used to: 11% of those who preferred four options indicated they were used to four-option items; of those preferring five options, 62% made comments like ‘It is a familiar format because I have been taking the five-option items for so long.’ Additionally, 16% of those who preferred the five-option format suggested such a test would better discriminate between less-able and more-able test takers, or those who studied and those who did not; that is, they thought five options made for a test whose results they could, in some ways, trust more, which they indicated was important because this test determines their future.</p>
<p>Another discrete (closed) question we asked on the survey was whether the test takers agree or disagree with the CSAT having items with three options – that is, did they think having three-option items on the CSAT would be fair? A small number (16) of test takers were neutral about this question, but of those who expressed an opinion, 164 (66%) said they would agree that such a situation would be fair and 84 (34%) said they would not agree that such a situation would be fair. The reasons for liking or disliking the notion of a three-option-item CSAT were similar to those reported above: a three-option CSAT would be more ‘user friendly,’ or a CSAT with three options would ‘be so unfamiliar.’ Interestingly, most of the ‘yes’ responses on this question were followed by statements that started with ‘I’ – ‘I can save time,’ ‘I can concentrate more on the test,’ ‘I feel less anxiety,’ or ‘I can easily choose a correct answer’ – while the ‘no’ responses centered more on the test as a precise measurement instrument – ‘There is a higher possibility to have a correct answer by guessing because of reduced number of options,’ or ‘The test will not discriminate as well among good and bad students.’ Many of those who wrote in response to the first question on the survey that they preferred three-option items also wrote that they thought a CSAT with three-option items would be unfair. The main reason for their disagreement was a perceived increased possibility of correctly guessing the answer, which they indicated would unfairly benefit those who did not study as hard as they did.</p>
</sec>
</sec>
<sec id="section23-0265532212451235" sec-type="discussion">
<title>Discussion</title>
<p>Considering that many empirical studies in educational measurement have indicated that multiple-choice items with three options are optimal, the question arises as to why, in that case, so many high-stakes L2 tests have four- or even five-option items. Applying educational measurement researchers’ methods in investigating the differential effects of differing numbers of item options is therefore warranted in the context of high-stakes, L2 listening tests. Conducting this type of research may help test the developers of such tests to understand better the choices they make in regard to the number of options on their tests, and may help them foresee the consequences that may ensue if they make changes to the number of options the items on their tests have.</p>
<p>This study found that the mean scores (the level of test difficulty) were significantly different between three-, and four-option tests, and between three- and five-option tests, but not between the four- and five-option tests, with three-option tests being easiest overall. This is not surprising given that prior research has found similar conclusions (e.g. <xref ref-type="bibr" rid="bibr17-0265532212451235">Crehan et al., 1993</xref>; <xref ref-type="bibr" rid="bibr18-0265532212451235">Currie &amp; Chiramanee, 2010</xref>; <xref ref-type="bibr" rid="bibr36-0265532212451235">Landrum et al., 1993</xref>; <xref ref-type="bibr" rid="bibr64-0265532212451235">Trevisan et al., 1994</xref>) – fewer options results in a relatively easier test because, we speculate, such a test may rely less on construct irrelevant skills such as testwiseness, L1 reading speed, and test anxiety. A longstanding argument is that items with fewer options are easier because if a test taker guesses the answer, with fewer options he or she has a higher chance of selecting the correct response. But as we explained in the literature review, few test takers blindly guess (<xref ref-type="bibr" rid="bibr18-0265532212451235">Currie &amp; Chiramanee, 2010</xref>; <xref ref-type="bibr" rid="bibr22-0265532212451235">Ebel, 1968</xref>; <xref ref-type="bibr" rid="bibr36-0265532212451235">Landrum et al., 1993</xref>; <xref ref-type="bibr" rid="bibr49-0265532212451235">Rodriguez, 2005</xref>; <xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers &amp; Harley, 1999</xref>; <xref ref-type="bibr" rid="bibr53-0265532212451235">Rupp et al., 2006</xref>); rather, in most cases they use educated guesses first of all to reduce the number of options. In effect, having fewer options reduces steps in the process of educated guessing, but the ultimate outcome (a random or partially educated guess from the remaining options under consideration) most likely remains the same regardless of the number of options the test taker had to consider in the first place. Thus, we suspect that the ever-present cognitive burdens associated with more options account for item facility decreases more than rarely occurring blind guessing does.</p>
<p>Second, the mean item discrimination was not significantly different among the three different test formats, corroborating results from <xref ref-type="bibr" rid="bibr55-0265532212451235">Shizuka et al. (2006)</xref> and research that has been conducted in educational measurement (<xref ref-type="bibr" rid="bibr17-0265532212451235">Crehan et al., 1993</xref>; <xref ref-type="bibr" rid="bibr30-0265532212451235">Haladyna et al., 2002</xref>). This could be because fundamentally, the items are the same across the different formats – the stems and the correct answer choices remain the same at the core of each item, regardless of the number of options. Only the process of selecting the correct answer differs, depending on the number of options, with the added burden (with more options) being that test takers must eliminate more non-plausible distractors (<xref ref-type="bibr" rid="bibr14-0265532212451235">Cizek et al., 1998</xref>), which takes more time (as can be seen from the response time data in this study). Thus, with more options, the test is more difficult across the board (for every student), but altering the number of options does not change the fundamental ability of the test to discriminate among test takers. Or, viewing this from a completely different approach, it could be that statistical significance was difficult to reach due to the low number of items (17 on each test). No matter which viewpoint one takes, it would be interesting to see if a test with more items (e.g. 50 or 100) would produce similar results, or if larger, significant differences in discrimination indices would be detected with a larger number of test items.</p>
<p>Third, in this study, test score reliability coefficients were statistically different in the easier tests (versions II and III); that is, significant differences in reliability were found between three- and four-option tests in version II (<italic>α</italic><sub><italic>II3</italic></sub> vs. <italic>α</italic><sub><italic>II4</italic></sub>) (three options were more reliable than four), and between the three- and five-option tests in version III (<italic>α</italic><sub><italic>III3</italic></sub> vs. <italic>α</italic><sub><italic>III5</italic></sub>) (five options were more reliable than three). These data are thus not straightforward, except that in most cases, there were no differences in reliability between test forms within a test version (see <xref ref-type="table" rid="table4-0265532212451235">Table 4</xref>). These findings partially refute those from <xref ref-type="bibr" rid="bibr56-0265532212451235">Sidick et al. (1994)</xref> and <xref ref-type="bibr" rid="bibr16-0265532212451235">Costin (1972)</xref>, who found in their studies that more options resulted in higher test reliability. This study’s results are therefore somewhat unique in that the data suggest that adding distractors to items does not guarantee higher test reliability, even if doing so makes a test more difficult. Again, the argument could be that adding or subtracting distractors does not change the core item, thus discrimination and reliability are not consistently affected by changes in the number of options, even though item difficulty is. This may be because reliability and discrimination are closely related (<xref ref-type="bibr" rid="bibr21-0265532212451235">Ebel, 1967</xref>) – increases in one result in increases in the other. The relationship between discrimination and reliability can be seen directly by observing the patterns in the data from <xref ref-type="table" rid="table3-0265532212451235">Tables 3</xref> (discrimination indices) and 4 (reliability estimates), presented in <xref ref-type="fig" rid="fig1-0265532212451235">Figure 1</xref>. Taken together, reducing the number of options made most of the items in this study easier to answer, but it did not (in all cases) consistently lower discrimination and/or reliability.</p>
<p>There is evidence in this data, however, that five-option items may be preferable to CSAT administrators. In this study, five-option items spread out examinees’ scores more closely along a normal distribution than the three-option-item tests (see <xref ref-type="fig" rid="fig2-0265532212451235">Figure 2</xref>). This result would seem to favor the use of a five-option test in this particular context because in a large-scale, high-stakes test like the CSAT, a more normal curve (one with a more symmetric distribution, or at least one that is not as negatively skewed) is critical for screening and segregating hundreds of thousands of test takers. In fact, CSAT test developers increased the number of options (from four to five options) in 1993 and adopted a weighted point system and increased the number of items in 1995 to better ensure a normal distribution of scores – that is, to prevent negative skewness (which would indicate the test is perhaps too easy and would also, in this context, make segregating students based on scores difficult) and to have fewer students within the upper-most range of scores (<xref ref-type="bibr" rid="bibr33-0265532212451235">KICE, 2009</xref>). Having five options on the CSAT may result in fewer disputes concerning the ultimate interpretation of test results, especially among the upper-level scorers who are above the go-to-college cut-off. Their scores are used to determine which college they go to, and these decisions are based on a very small range of scores. Thus, in these data the benefit of more options is statistically seen in the skew – information that might please the current end users of the real five-option item test scores – the colleges which have to make hardline decisions concerning who receives entrance to universities in Korea.</p>
<fig id="fig2-0265532212451235" position="float">
<label>Figure 2.</label>
<caption><p>Histograms of the tests with three-, four-, and five-option items.</p></caption>
<graphic xlink:href="10.1177_0265532212451235-fig2.tif"/>
</fig>
<p>Here we consider the quantitative results more in light of the context of the CSAT. Going back to the level of test difficulty (or mean item difficulty), the significant difference among the three different test formats (three-, four-, and five-option items – with the three-option items being the easiest) may not matter in medium to low-stakes tests. In such a case it would be wise, as recommended by educational researchers for over 80 years, to have three-option items even if the three-option format produces an easier test because such a test may more validly measure the construct the test is intended to measure (<xref ref-type="bibr" rid="bibr14-0265532212451235">Cizek et al., 1998</xref>). Even though it was statistically significant, the effect size of the difference between test scores’ difficulties (<italic>η</italic><sup><italic>2</italic></sup> = 0.04) was small (<xref ref-type="bibr" rid="bibr15-0265532212451235">Cohen, 1988</xref>). However, in a large-scale, norm-referenced test such as the CSAT, even a minimally easier test may produce too many perfect scorers (see <xref ref-type="fig" rid="fig1-0265532212451235">Figure 2</xref>). In fact, the KICE reported that the CSAT English test administered in 2011 resulted in quite a high proportion of perfect scorers (2.67%, 17,326 test takers), compared with the other three subtests in the CSAT (between 0.02% and 0.55%; see the KICE website <ext-link ext-link-type="uri" xlink:href="http://www.kice.re.kr/ko/">www.kice.re.kr/ko/</ext-link>). In other words, the 2011 CSAT English test failed to function well in segregating upper-level applicants. Thus, as <xref ref-type="bibr" rid="bibr32-0265532212451235">Keppel and Wickens (2007)</xref> stated, the small effect size could be meaningful depending on the context and how scores are interpreted and used.</p>
<p>Many prior studies investigating the optional number of multiple-choice items suggested that the saved testing time, due to the administration of three-option items instead of five-option items, would contribute to increased reliability because it would allow test developers to make longer tests. However, reducing the number of options and increasing the number of items may not be effective in the case of the CSAT L2 English listening test. With only 20 minutes allowed for the test, reducing the number of options to three would allow for one more item. That change would increase reliability by only 0.013 using the Brown-Spearman Prophecy formula. In the current context of the CSAT, test developers would not gain much in terms of reliability by adopting three-option items and increasing the number of items to 18.</p>
<p>Nonetheless, perhaps CSAT administrators should not ignore the low correlations among the multiple-choice item formats. We carefully considered statistical factors that can affect low correlations, and in this case measurement error is a potential reason for the low correlations (because correction for attenuation showed increases in the reliability coefficient), but measurement error is not solely responsible for the low correlations. This implies that there could be other factors that affect the low correlations, and one of the possible reasons could be that tests tapped into different skills. Although debated (<xref ref-type="bibr" rid="bibr18-0265532212451235">Currie &amp; Chiramanee, 2010</xref>), a high correlation coefficient can indicate that two tests, taken by the same group of people, are measuring the same underlying skills or abilities (<xref ref-type="bibr" rid="bibr39-0265532212451235">Lissitz, 2009</xref>; <xref ref-type="bibr" rid="bibr61-0265532212451235">Thorndike, 1982</xref>). Thus, there is some evidence that these tests with differing numbers of options tapped into different underlying skills and abilities, as foreshadowed by <xref ref-type="bibr" rid="bibr50-0265532212451235">Rogers and Harley (1999)</xref>.</p>
<p>This brings us back to the question we started with: Within more formal and high-stakes testing contexts, what is the best number of options in a multiple-choice item? Three, four, or five? From a statistical perspective and considering the impossible situation of the CSAT, one would have to say five. However, researchers also need to consider the optimality from the perspective of the most important stakeholder, the test taker. A total of 54% of the respondents preferred the three-option-item format: they indicated that the three-option items were less difficult; thus they experienced less test anxiety with them, saved crucial time in responding to the items (leaving more time to listen or prepare to listen), and viewed the three-option items as more reader-friendly. Also, 66% of the respondents indicated that they would prefer a CSAT L2 English listening test with three-option items, mostly for similar reasons. As test developers and educators, the emphasis should always be on the test takers, the main stakeholders in the test. Test developers and education decision makers should consider the affective and social impacts when deciding on the optimal number of options in multiple-choice items.</p>
<p>Despite the psychological advantages of a three-option-item format, the developers of the CSAT may be reluctant to adopt it for two reasons: (1) the CSAT is such a high-stakes test that even minute, non-statistically significant differences in scores may be needed to finely differentiate test takers; and (2) considering Koreans’ unique obsession with the CSAT scores and the larger CSAT testing system currently in place, it might be hard to convince the CSAT administrators to adopt a three-option-item format, and even if they did, stakeholders may be opposed. There may <italic>not</italic> be a universal, easy answer to the optimal number of multiple-choice options. Test developers and researchers have to consider the statistical, affective, and contextual factors when determining the optimal number of options for their test. We recommend, however, that any testing agency, like the KICE, that employs more rather than fewer options within their multiple-choice items be overt in acknowledging that more options most likely increases the possibility that the items measure additional skills, something that the CSAT’s stakeholders, nonetheless, most likely intuitively recognize.</p>
<p>Finally, several limitations should be noted. The limited representativeness of the participants should be considered in interpreting the results. One of the threats to the wider validity of the results is that the participants’ motivation in an experimental setting may be different from that of actual CSAT examinees, and that could have affected the study’s results. Another limitation could be the relatively large standard errors in the equated scores because of the small sample size. More studies with longer tests (tests with more items) for comparison are needed.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-0265532212451235">
<title>Appendix:</title>
<sec id="section24-0265532212451235">
<title>Survey</title>
<list id="list2-0265532212451235" list-type="simple">
<list-item><p>1. Which multiple-choice item format do you prefer? Circle your answer.</p>
<list id="list3-0265532212451235" list-type="alpha-lower">
<list-item><p>Three-option-item format</p></list-item>
<list-item><p>Four-option-item format</p></list-item> <list-item><p>(c) Five-option-item format</p></list-item>
</list></list-item>
<list-item><p>2. Why?</p>
<p>Indicate your preference level regarding the three-option-item format by circling one of the numbers below, with 1 being ‘strongly dislike’ and 7 being ‘like very much.’</p>
<p><graphic id="img1-0265532212451235" position="anchor" xlink:href="10.1177_0265532212451235-img1.tif"/></p></list-item>
<list-item><p>4. Can you think of any other reasons why a three-option-item format would be good?</p></list-item>
<list-item><p>5. Can you think of any other reasons why the three-option-item format would be bad?</p></list-item>
<list-item><p>6. If the CSAT were to be administered with a three-option-item format, do you think it would be fair?</p></list-item>
<list-item><p>7. Why do agree or disagree with the CSAT having a three-option-item format?</p></list-item></list>
</sec>
</app>
</app-group>
<ack><p>We would like to thank the principal, teachers, and students at the high school in Korea where the data were collected. Without them this research would not have been possible. We would like to thank Charlene Polio and the anonymous <italic>Language Testing</italic> reviewers for their valuable insights that helped us improve upon earlier versions of this manuscript. Any mistakes, however, are our own.</p></ack>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0265532212451235">
<label>1.</label>
<p>In this vein, discrimination is the number of the possible unique sets of responses. A test’s power is an index (up to 1) that remains after subtracting the probability of getting a perfect score by chance, and test information is the expected amount of uncertainty regarding all the possible sets of responses (<xref ref-type="bibr" rid="bibr10-0265532212451235">Budescu &amp; Nevo, 1985</xref>).</p></fn>
<fn fn-type="other" id="fn2-0265532212451235">
<label>2.</label>
<p>According to <xref ref-type="bibr" rid="bibr13-0265532212451235">Choi (2008)</xref>, the CSAT results determine the future social status of each student in Korea. Thus, the national obsession over the CSAT in Korea is extraordinary, as <xref ref-type="bibr" rid="bibr11-0265532212451235">Card (2005)</xref> described. On the test day, under government policy, employees in government and companies arrive to work one hour later than usual to avoid traffic jams. The Korean stock market also opens one hour later than usual. During the listening test, domestic and international flights are not allowed to take off and land, as the noise may influence the test results. Cars are not allowed to honk horns near testing sites on that day (<xref ref-type="bibr" rid="bibr11-0265532212451235">Card, 2005</xref>).</p></fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Aamodt</surname><given-names>M. G.</given-names></name>
<name><surname>McShane</surname><given-names>T.</given-names></name>
</person-group> (<year>1992</year>). <article-title>A metaanalytic investigation of the effect of various test item characteristics on test scores and test completion times</article-title>. <source>Public Personnel Management</source>, <volume>21</volume>(<issue>2</issue>), <fpage>151</fpage>–<lpage>160</lpage>.</citation>
</ref>
<ref id="bibr2-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alderson</surname><given-names>J. C.</given-names></name>
</person-group> (<year>2000</year>). <source>Assessing reading</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr3-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alderson</surname><given-names>J. C.</given-names></name>
<name><surname>Clapham</surname><given-names>C.</given-names></name>
<name><surname>Wall</surname><given-names>D.</given-names></name>
</person-group> (<year>1995</year>). <source>Language test construction and evaluation</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Berg</surname><given-names>B. L.</given-names></name>
</person-group> (<year>2008</year>). <source>Qualitative research methods for the social sciences</source> (<edition>7th ed.</edition>). <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Allyn &amp; Bacon, Pearson Education</publisher-name>.</citation>
</ref>
<ref id="bibr5-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>H. D.</given-names></name>
</person-group> (<year>2004</year>). <source>Language assessment: Principles and classroom practice</source>. <publisher-loc>White Plains, NY</publisher-loc>: <publisher-name>Longman, Pearson Education</publisher-name>.</citation>
</ref>
<ref id="bibr6-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2001</year>). <source>Using surveys in language programs</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr7-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2005</year>). <source>Testing in language programs: A comprehensive guide to English language assessment</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr8-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bruno</surname><given-names>J. E.</given-names></name>
<name><surname>Dirkzwagber</surname><given-names>A.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Determining the optimal number of alternatives to a multiple choice test item: An information theoretic perspective</article-title>. <source>Educational and Psychological Measurement</source>, <volume>55</volume>(<issue>6</issue>), <fpage>959</fpage>–<lpage>966</lpage>.</citation>
</ref>
<ref id="bibr9-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Buck</surname><given-names>G.</given-names></name>
</person-group> (<year>2001</year>). <source>Assessing listening</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Budescu</surname><given-names>D. V.</given-names></name>
<name><surname>Nevo</surname><given-names>B.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Optimal number of options: An investigation of the assumption of proportionality</article-title>. <source>Journal of Educational Measurement</source>, <volume>22</volume>(<issue>3</issue>), <fpage>183</fpage>–<lpage>196</lpage>.</citation>
</ref>
<ref id="bibr11-0265532212451235">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Card</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>, <month>November</month> <day>30</day>). <article-title>Life and death exams in South Korea</article-title>, <source>Asia Times</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.atimes.com/atimes/Korea/GK30Dg01.html">www.atimes.com/atimes/Korea/GK30Dg01.html</ext-link>.</citation>
</ref>
<ref id="bibr12-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chandler</surname><given-names>J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The efficacy of various kinds of error feedback for improvement in the accuracy and fluency of L2 student writing</article-title>. <source>Journal of Second Language Writing</source>, <volume>12</volume>(<issue>3</issue>), <fpage>267</fpage>–<lpage>296</lpage>.</citation>
</ref>
<ref id="bibr13-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Choi</surname><given-names>I.-C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The impact of EFL testing on EFL education in Korea</article-title>. <source>Language Testing</source>, <volume>25</volume>(<issue>1</issue>), <fpage>39</fpage>–<lpage>62</lpage>.</citation>
</ref>
<ref id="bibr14-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cizek</surname><given-names>G. J.</given-names></name>
<name><surname>Robinson</surname><given-names>L. K.</given-names></name>
<name><surname>O’Day</surname><given-names>D. M.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Nonfunctioning options: A closer look</article-title>. <source>Educational &amp; Psychological Measurement</source>, <volume>58</volume>(<issue>4</issue>), <fpage>605</fpage>–<lpage>611</lpage>.</citation>
</ref>
<ref id="bibr15-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>). <source>Statistical power analysis for the behavioral sciences</source> (<edition>2nd ed.</edition>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr16-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Costin</surname><given-names>F.</given-names></name>
</person-group> (<year>1972</year>). <article-title>Three-choice versus four-choice items: Implications for reliability and validity of objective achievement tests</article-title>. <source>Educational and Psychological Measurement</source>, <volume>32</volume>(<issue>4</issue>), <fpage>1035</fpage>–<lpage>1038</lpage>.</citation>
</ref>
<ref id="bibr17-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Crehan</surname><given-names>K. D.</given-names></name>
<name><surname>Haladyna</surname><given-names>T. M.</given-names></name>
<name><surname>Brewer</surname><given-names>B. W.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Use of an inclusive option and the optimal number of options for multiple-choice items</article-title>. <source>Educational and Psychological Measurement</source>, <volume>53</volume>(<issue>1</issue>), <fpage>241</fpage>–<lpage>247</lpage>.</citation>
</ref>
<ref id="bibr18-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Currie</surname><given-names>M.</given-names></name>
<name><surname>Chiramanee</surname><given-names>T.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The effect of the multiple-choice item format on the measurement of knowledge of language structure</article-title>. <source>Language Testing</source>, <volume>27</volume>(<issue>4</issue>), <fpage>471</fpage>–<lpage>491</lpage>.</citation>
</ref>
<ref id="bibr19-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Delgado</surname><given-names>A. R.</given-names></name>
<name><surname>Prieto</surname><given-names>G.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Further evidence favoring three-option items in multiple-choice tests</article-title>. <source>European Journal of Psychological Assessment</source>, <volume>14</volume>(<issue>3</issue>), <fpage>197</fpage>–<lpage>201</lpage>.</citation>
</ref>
<ref id="bibr20-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dörnyei</surname><given-names>Z.</given-names></name>
<name><surname>Taguchi</surname><given-names>T.</given-names></name>
</person-group> (<year>2009</year>). <source>Questionnaires in second language Research: Construction, administration, and processing</source> (<edition>2nd ed.</edition>). <publisher-loc>New York</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr21-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ebel</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1967</year>). <article-title>The relation of item discrimination to test reliability</article-title>. <source>Journal of Educational Measurement</source>, <volume>4</volume>(<issue>3</issue>), <fpage>125</fpage>–<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr22-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ebel</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1968</year>). <article-title>Blind guessing on objective achievement tests</article-title>. <source>Journal of Educational Measurement</source> <volume>5</volume>(<issue>4</issue>), <fpage>321</fpage>–<lpage>325</lpage>.</citation>
</ref>
<ref id="bibr23-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ebel</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1969</year>). <article-title>Expected reliability as a function of choices per item</article-title>. <source>Educational and Psychological Measurement</source> <volume>29</volume>(<issue>3</issue>), <fpage>565</fpage>–<lpage>570</lpage>.</citation>
</ref>
<ref id="bibr24-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Feldt</surname><given-names>L. S.</given-names></name>
<name><surname>Woodruff</surname><given-names>D. J.</given-names></name>
<name><surname>Salih</surname><given-names>F. A.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Statistical inference for coefficient alpha</article-title>. <source>Applied Psychological Measurement</source>, <volume>11</volume>(<issue>1</issue>), <fpage>93</fpage>–<lpage>103</lpage>.</citation>
</ref>
<ref id="bibr25-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goodwin</surname><given-names>L. D.</given-names></name>
<name><surname>Leech</surname><given-names>N. L.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Understanding correlation: Factors that affect the size of</article-title> <source>r. The Journal of Experimental Education</source>, <volume>74</volume>(<issue>3</issue>), <fpage>251</fpage>–<lpage>266</lpage>.</citation>
</ref>
<ref id="bibr26-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Green</surname><given-names>K.</given-names></name>
<name><surname>Sax</surname><given-names>G.</given-names></name>
<name><surname>Michael</surname><given-names>W. B.</given-names></name>
</person-group> (<year>1982</year>). <article-title>Validity and reliability of tests having different numbers of options for students of differing levels of ability</article-title>. <source>Educational and Psychological Measurement</source>, <volume>42</volume>(<issue>1</issue>), <fpage>239</fpage>–<lpage>245</lpage>.</citation>
</ref>
<ref id="bibr27-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grier</surname><given-names>J. B.</given-names></name>
</person-group> (<year>1975</year>). <article-title>The number of alternatives for optimum test reliability</article-title>. <source>Journal of Educational Measurement</source>, <volume>12</volume>(<issue>2</issue>), <fpage>109</fpage>–<lpage>112</lpage>.</citation>
</ref>
<ref id="bibr28-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T. M.</given-names></name>
</person-group> (<year>2004</year>). <source>Developing and validating multiple-choice test items</source> (<edition>3rd ed.</edition>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>.</citation>
</ref>
<ref id="bibr29-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T. M.</given-names></name>
<name><surname>Downing</surname><given-names>S. M.</given-names></name>
</person-group> (<year>1993</year>). <article-title>How many options is enough for a multiple-choice test item?</article-title> <source>Educational &amp; Psychology Measurement</source>, <volume>53</volume>(<issue>4</issue>), <fpage>999</fpage>–<lpage>1010</lpage>.</citation>
</ref>
<ref id="bibr30-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T. M.</given-names></name>
<name><surname>Downing</surname><given-names>S. M.</given-names></name>
<name><surname>Rodriguez</surname><given-names>M. C.</given-names></name>
</person-group> (<year>2002</year>). <article-title>A review of multiple-choice item-writing guidelines for classroom assessment</article-title>. <source>Applied Measurement in Education</source>, <volume>15</volume>(<issue>3</issue>), <fpage>309</fpage>–<lpage>334</lpage>.</citation>
</ref>
<ref id="bibr31-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hughes</surname><given-names>A.</given-names></name>
</person-group> (<year>2003</year>). <source>Testing for language teachers</source> (<edition>2nd ed.</edition>). <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr32-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Keppel</surname><given-names>G.</given-names></name>
<name><surname>Wickens</surname><given-names>T. D.</given-names></name>
</person-group> (<year>2007</year>). <source>Design and analysis: A researcher’s handbook</source> (<edition>5th ed.</edition>). <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr33-0265532212451235">
<citation citation-type="web">
<collab>KICE</collab>. (<year>2009</year>). <source>Current issues and prospects in CSAT</source>. <conf-name>Paper presented at the Forum conducted at the second Korea Institute for Curriculum and Evaluation (KICE) educational curriculum and policy symposium</conf-name>, <conf-loc>Seoul, South Korea</conf-loc>. Available from <ext-link ext-link-type="uri" xlink:href="http://www.kice.re.kr/ko">www.kice.re.kr/ko</ext-link>.</citation>
</ref>
<ref id="bibr34-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kirk</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1995</year>). <source>Experimental design: Procedures for the behavioral sciences</source> (<edition>3rd ed.</edition>). <publisher-loc>Pacific Grove, CA</publisher-loc>: <publisher-name>Brooks/Cole</publisher-name>.</citation>
</ref>
<ref id="bibr35-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kolen</surname><given-names>M. J.</given-names></name>
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2004</year>). <source>Test equating, scaling, and linking: Methods and practices</source> (<edition>2nd ed.</edition>). <publisher-loc>New York</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr36-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Landrum</surname><given-names>R. E.</given-names></name>
<name><surname>Cashin</surname><given-names>J. R.</given-names></name>
<name><surname>Theis</surname><given-names>K. S.</given-names></name>
</person-group> (<year>1993</year>). <article-title>More evidence in favor of three-option multiple-choice tests</article-title>. <source>Educational and Psychological Measurement</source>, <volume>53</volume>(<issue>1</issue>), <fpage>771</fpage>–<lpage>778</lpage>.</citation>
</ref>
<ref id="bibr37-0265532212451235">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lautenschlager</surname><given-names>G. J.</given-names></name>
<name><surname>Meade</surname><given-names>A. W.</given-names></name>
</person-group> (<year>2008</year>). <article-title>AlphaTest: A Windows program for tests of hypotheses about coefficient alpha</article-title>. [Computer software available from <ext-link ext-link-type="uri" xlink:href="http://www4.ncsu.edu/~awmeade/Links/AlphaTest.htm">www4.ncsu.edu/~awmeade/Links/AlphaTest.htm</ext-link>]. <source>Applied Psychological Measurement</source>, <volume>32</volume>(<issue>6</issue>), <fpage>502</fpage>–<lpage>503</lpage>.</citation>
</ref>
<ref id="bibr38-0265532212451235">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>, <month>October</month> <month>23</month>). <article-title>Samsip samilganeui gamgeun suneung chujewuiwoneui bimil</article-title>. <source>The Hankyoreh</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.hani.co.kr/arti/society/schooling/73560.html">www.hani.co.kr/arti/society/schooling/73560.html</ext-link>.</citation>
</ref>
<ref id="bibr39-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Lissitz</surname><given-names>R. W.</given-names></name>
</person-group> (Ed.). (<year>2009</year>). <source>The concept of validity: Revisions, new directions, and applications</source>. <publisher-loc>Charlotte, NC</publisher-loc>: <publisher-name>Information Age Publishing</publisher-name>.</citation>
</ref>
<ref id="bibr40-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Livingston</surname><given-names>S. A.</given-names></name>
</person-group> (<year>2004</year>). <source>Equating test scores (without IRT)</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr41-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lord</surname><given-names>F. M.</given-names></name>
</person-group> (<year>1944</year>). <article-title>Reliability of multiple-choice tests as a function of number of choices per item</article-title>. <source>Journal of Educational Psychology</source>, <volume>35</volume>(<issue>3</issue>), <fpage>175</fpage>–<lpage>180</lpage>.</citation>
</ref>
<ref id="bibr42-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lord</surname><given-names>F. M.</given-names></name>
</person-group> (<year>1977</year>). <article-title>Optimal number of choices per item–a comparison of four approaches</article-title>. <source>Journal of Educational Measurement</source>, <volume>14</volume>(<issue>1</issue>), <fpage>33</fpage>–<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr43-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mackey</surname><given-names>A.</given-names></name>
<name><surname>Gass</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2005</year>). <source>Second language research: Methodology and design</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum Associates</publisher-name>.</citation>
</ref>
<ref id="bibr44-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McNamara</surname><given-names>T.</given-names></name>
</person-group> (<year>2000</year>). <source>Language testing</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr45-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Millman</surname><given-names>J.</given-names></name>
<name><surname>Bishop</surname><given-names>C. H.</given-names></name>
<name><surname>Ebel</surname><given-names>R.</given-names></name>
</person-group> (<year>1965</year>). <article-title>An analysis of test-wiseness</article-title>. <source>Educational and Psychological Measurement</source>, <volume>25</volume>(<issue>3</issue>), <fpage>707</fpage>–<lpage>726</lpage>.</citation>
</ref>
<ref id="bibr46-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Owen</surname><given-names>S. V.</given-names></name>
<name><surname>Froman</surname><given-names>R. D.</given-names></name>
</person-group> (<year>1987</year>). <article-title>What’s wrong with three-option multiple choice items?</article-title> <source>Educational and Psychological Measurement</source>, <volume>47</volume>(<issue>2</issue>), <fpage>513</fpage>–<lpage>522</lpage>.</citation>
</ref>
<ref id="bibr47-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Purpura</surname><given-names>J. E.</given-names></name>
</person-group> (<year>2004</year>). <source>Assessing grammar</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr48-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Read</surname><given-names>J.</given-names></name>
</person-group> (<year>2000</year>). <source>Assessing vocabulary</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr49-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rodriguez</surname><given-names>M. C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Three options are optimal for multiple-choice items: A meta-analysis of 80 years of research</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>24</volume>(<issue>2</issue>), <fpage>3</fpage>–<lpage>13</lpage>.</citation>
</ref>
<ref id="bibr50-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>W. T.</given-names></name>
<name><surname>Harley</surname><given-names>D.</given-names></name>
</person-group> (<year>1999</year>). <article-title>An empirical comparison of three- and four-choice items and tests: Susceptibility to testwiseness and internal consistency reliability</article-title>. <source>Educational and Psychological Measurement</source>, <volume>59</volume>(<issue>2</issue>), <fpage>234</fpage>–<lpage>247</lpage>.</citation>
</ref>
<ref id="bibr51-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>W. T.</given-names></name>
<name><surname>Yang</surname><given-names>P.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Test-wiseness: Its nature and application</article-title>. <source>European Journal of Psychological Assessment</source>, <volume>12</volume>(<issue>3</issue>), <fpage>247</fpage>–<lpage>259</lpage>.</citation>
</ref>
<ref id="bibr52-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ruch</surname><given-names>G. M.</given-names></name>
<name><surname>Stoddard</surname><given-names>G. D.</given-names></name>
</person-group> (<year>1927</year>). <source>Tests and measurements in high school instruction</source>. <publisher-loc>Yonkers-on-Hudson</publisher-loc>: <publisher-name>World Book Company</publisher-name>.</citation>
</ref>
<ref id="bibr53-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rupp</surname><given-names>A. A.</given-names></name>
<name><surname>Ferne</surname><given-names>T.</given-names></name>
<name><surname>Choi</surname><given-names>H.</given-names></name>
</person-group> (<year>2006</year>). <article-title>How assessing reading comprehension with multiple-choice questions shapes the construct: A cognitive processing perspective</article-title>. <source>Language Testing</source>, <volume>23</volume>(<issue>4</issue>), <fpage>441</fpage>–<lpage>474</lpage>.</citation>
</ref>
<ref id="bibr54-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sarnaki</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1979</year>). <article-title>An examination of test-wiseness in the cognitive domain</article-title>. <source>Review of Educational Research</source>, <volume>49</volume>(<issue>2</issue>), <fpage>252</fpage>–<lpage>279</lpage>.</citation>
</ref>
<ref id="bibr55-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shizuka</surname><given-names>T.</given-names></name>
<name><surname>Takeuchi</surname><given-names>O.</given-names></name>
<name><surname>Yashima</surname><given-names>T.</given-names></name>
<name><surname>Yoshizawa</surname><given-names>K.</given-names></name>
</person-group> (<year>2006</year>). <article-title>A comparison of three- and four-option English tests for university entrance selection purposes in Japan</article-title>. <source>Language Testing</source>, <volume>23</volume>(<issue>1</issue>), <fpage>35</fpage>–<lpage>57</lpage>.</citation>
</ref>
<ref id="bibr56-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sidick</surname><given-names>J. T.</given-names></name>
<name><surname>Barrett</surname><given-names>G. V.</given-names></name>
<name><surname>Doverspike</surname><given-names>D.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Three alternative multiple choice tests: An attractive option</article-title>. <source>Personal Psychology</source>, <volume>47</volume>(<issue>4</issue>), <fpage>829</fpage>–<lpage>835</lpage>.</citation>
</ref>
<ref id="bibr57-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spearman</surname><given-names>C.</given-names></name>
</person-group> (<year>1904</year>). <article-title>‘General intelligence,’ objectively determined and measured</article-title>. <source>The American Journal of Psychology</source>, <volume>15</volume>(<issue>2</issue>), <fpage>201</fpage>–<lpage>292</lpage>.</citation>
</ref>
<ref id="bibr58-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Straton</surname><given-names>R. G.</given-names></name>
<name><surname>Catts</surname><given-names>R. M.</given-names></name>
</person-group> (<year>1980</year>). <article-title>A comparison of two, three and four-choice item tests given a fixed total number of choices</article-title>. <source>Educational and Psychological Measurement</source>, <volume>40</volume>(<issue>2</issue>), <fpage>357</fpage>–<lpage>365</lpage>.</citation>
</ref>
<ref id="bibr59-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tabachnick</surname><given-names>B. G.</given-names></name>
<name><surname>Fidell</surname><given-names>L. S.</given-names></name>
</person-group> (<year>2007</year>). <source>Using multivariate statistics</source> (<edition>5th ed.</edition>). <publisher-loc>Boston</publisher-loc>: <publisher-name>Allyn &amp; Bacon</publisher-name>.</citation>
</ref>
<ref id="bibr60-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thomas</surname><given-names>D. R.</given-names></name>
</person-group> (<year>2006</year>). <article-title>A general inductive approach for analyzing qualitative evaluation data</article-title>. <source>American Journal of Evaluation</source>, <volume>27</volume>(<issue>2</issue>), <fpage>237</fpage>–<lpage>246</lpage>.</citation>
</ref>
<ref id="bibr61-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thorndike</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1982</year>). <source>Applied psychometrics</source>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Houghton Mifflin</publisher-name>.</citation>
</ref>
<ref id="bibr62-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tversky</surname><given-names>A.</given-names></name>
</person-group> (<year>1964</year>). <article-title>On the optimal number of alternatives at a choice point</article-title>. <source>Journal of Mathematical Psychology</source>, <volume>1</volume>(<issue>1</issue>), <fpage>386</fpage>–<lpage>391</lpage>.</citation>
</ref>
<ref id="bibr63-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Trevisan</surname><given-names>M. S.</given-names></name>
<name><surname>Sax</surname><given-names>G.</given-names></name>
<name><surname>Michael</surname><given-names>W. B.</given-names></name>
</person-group> (<year>1991</year>). <article-title>The effects of the number of options per item and student ability on test validity and reliability</article-title>. <source>Educational and Psychological Measurement</source>, <volume>51</volume>(<issue>4</issue>), <fpage>829</fpage>–<lpage>837</lpage>.</citation>
</ref>
<ref id="bibr64-0265532212451235">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Trevisan</surname><given-names>M. S.</given-names></name>
<name><surname>Sax</surname><given-names>G.</given-names></name>
<name><surname>Michael</surname><given-names>W. B.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Estimating the optimum number of options per item using an incremental option paradigm</article-title>. <source>Educational and Psychological Measurement</source>, <volume>54</volume>(<issue>1</issue>), <fpage>86</fpage>–<lpage>91</lpage>.</citation>
</ref>
<ref id="bibr65-0265532212451235">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tukey</surname><given-names>J. W.</given-names></name>
</person-group> (<year>1977</year>). <source>Exploratory data analysis</source>. <publisher-loc>Reading, MA</publisher-loc>: <publisher-name>Addison-Wesley</publisher-name>.</citation>
</ref>
<ref id="bibr66-0265532212451235">
<citation citation-type="journal">
<collab>Tversky</collab> (<year>1964</year>). [to come].</citation>
</ref>
</ref-list>
</back>
</article>