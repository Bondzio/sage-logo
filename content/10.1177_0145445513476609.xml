<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BMO</journal-id>
<journal-id journal-id-type="hwp">spbmo</journal-id>
<journal-id journal-id-type="nlm-ta">Behav Modif</journal-id>
<journal-title>Behavior Modification</journal-title>
<issn pub-type="ppub">0145-4455</issn>
<issn pub-type="epub">1552-4167</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0145445513476609</article-id>
<article-id pub-id-type="publisher-id">10.1177_0145445513476609</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Reconsidering Overlap-Based Measures for Quantitative Synthesis of Single-Subject Data</article-title>
<subtitle>What They Tell Us and What They Don’t</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Carter</surname><given-names>Mark</given-names></name>
<xref ref-type="aff" rid="aff1-0145445513476609">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0145445513476609"><label>1</label>Macquarie University, New South Wales, Australia</aff>
<author-notes>
<corresp id="corresp1-0145445513476609">Mark Carter, Macquarie University Special Education Centre, Macquarie University, NSW 2109, Australia. Email: <email>mark.carter.mq@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>37</volume>
<issue>3</issue>
<fpage>378</fpage>
<lpage>390</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Overlap-based measures are increasingly applied in the synthesis of single-subject research. This article considers two criticisms of overlap-based metrics, specifically that they do not measure magnitude of effect and do not adequately correspond with visual analysis. It is argued that these criticisms are based on fundamental misconceptions regarding the nature of effect sizes and their appropriate interpretation in single-subject research. Suggestions for considerations in evaluating single-subject research studies are offered, including the need to separately consider experimental control and magnitude of effect.</p>
</abstract>
<kwd-group>
<kwd>single subject</kwd>
<kwd>meta-analysis</kwd>
<kwd>overlap metrics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The meta-analysis of traditional group studies has broad acceptance, and increasingly sophisticated statistical methods have been developed (<xref ref-type="bibr" rid="bibr3-0145445513476609">Borenstein, Hedges, Higgins, &amp; Rothstein, 2009</xref>). This approach has formed the methodological cornerstone of the movement toward evidence-based medicine (see <xref ref-type="bibr" rid="bibr16-0145445513476609">Higgins &amp; Green, 2009</xref>) and has been widely used in special education (<xref ref-type="bibr" rid="bibr19-0145445513476609">Kavale &amp; Forness, 1999</xref>) and education more generally (<xref ref-type="bibr" rid="bibr15-0145445513476609">Hattie, 2009</xref>). Nevertheless, much research in special education relies on single-subject research designs. Although methodology for the quantitative synthesis of such research dates back more than 20 years and some procedures, such as percentage of overlapping data (PND; <xref ref-type="bibr" rid="bibr39-0145445513476609">Scruggs, Mastropieri, &amp; Casto, 1987b</xref>), are in reasonably wide use, there remains considerable controversy over their application (e.g., <xref ref-type="bibr" rid="bibr13-0145445513476609">Haardorfer, 2010</xref>; <xref ref-type="bibr" rid="bibr22-0145445513476609">Kratochwill et al., 2010</xref>; <xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery, Busick, Reichow, &amp; Barton, 2010</xref>). Quantitative summaries of single-subject research offers several possible advantages over traditional narrative reviews, including the ability to identify effects that may be too subtle to detect in a single study, estimation of the robustness of an intervention, reducing subjectivity in summarizing research, and examining moderator variables (<xref ref-type="bibr" rid="bibr8-0145445513476609">Critchfield, Newland, &amp; Kollins, 2000</xref>).</p>
<p>An effect size is “a value that reflects the magnitude of the treatment effect or (more generally) the strength of a relationship between two variables” (<xref ref-type="bibr" rid="bibr3-0145445513476609">Borenstein et al., 2009</xref>, p. 3). Central to the process of meta-analysis is the calculation of a standardized effect, which can be used to provide a measurement-independent metric to estimate a summary effect, evaluate consistency across studies, and allow examination of moderator variables that may affect outcomes (e.g., participant age or intervention intensity). Several approaches to the calculation of effect size in single-subject studies have been proposed.</p>
<p>Perhaps the most obvious approach is to use a corollary of the standardized mean difference metrics that are often used in analyzing continuous variables in traditional group designs (see <xref ref-type="bibr" rid="bibr4-0145445513476609">Busk &amp; Serlin, 1992</xref>). Within group comparison designs, this family of metrics typically standardizes the difference between the means of two groups by a measure of dispersion, often the pooled standard deviation across both groups. There are, however, a number of significant problems with this approach when applied to single-subject research. First, in traditional group designs, differences are standardized by the between-individual variation, whereas in single-subject research, differences are standardized by the within-individual variation. It is reasonable to expect that within-individual variation would typically be more constrained than variation across individuals, resulting in relatively inflated effect sizes (<xref ref-type="bibr" rid="bibr23-0145445513476609">Leong &amp; Carter, 2008</xref>). That is, metrics comparing groups at one point in time will not have the same scale as those describing change in one individual over time (<xref ref-type="bibr" rid="bibr2-0145445513476609">Beretvas &amp; Chung, 2008</xref>). In addition, where change is standardized by baseline data variation, problems arise with zero baselines (division by zero) and baselines with very limited variability. Importantly, these measures are based on assumptions such as independence and normality of distribution, which are often violated in single-subject data series (<xref ref-type="bibr" rid="bibr31-0145445513476609">Parker, Vannest, &amp; Brown, 2009</xref>). Finally, means often fail to adequately characterize data sets in single-subject research (<xref ref-type="bibr" rid="bibr32-0145445513476609">Parker, Vannest, &amp; Davis, 2011</xref>).</p>
<p>Another approach to meta-analysis of single-subject research has been to use regression-based procedures (e.g., <xref ref-type="bibr" rid="bibr1-0145445513476609">Allison &amp; Gorman, 1993</xref>; <xref ref-type="bibr" rid="bibr7-0145445513476609">Center, Skiba, &amp; Casey, 1985</xref>) but there are problems with such methodologies. One particular issue is that single-subject baseline data series are short on average and regression estimates that are based on limited numbers of data points are highly suspect (<xref ref-type="bibr" rid="bibr1-0145445513476609">Allison &amp; Gorman, 1993</xref>). The fundamental obstacle to these approaches, however, is their reliance on assumptions, most specifically that of independence of data (<xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al., 2010</xref>) and regression-based approaches have not been extensively used in single-subject meta-analysis.</p>
<p>One approach that potentially overcomes the difficulties associated with assumptions underlying parametric tests involves overlap-based measures. The progenitor of these approaches is PND, first described by <xref ref-type="bibr" rid="bibr39-0145445513476609">Scruggs et al. (1987b)</xref>, but a number of other metrics have been proposed such as percentage exceeding the median (PEM; <xref ref-type="bibr" rid="bibr26-0145445513476609">Ma, 2009</xref>), improvement rate difference (IRD; <xref ref-type="bibr" rid="bibr31-0145445513476609">Parker et al., 2009</xref>), and percentage of all nonoverlapping data (PAND; <xref ref-type="bibr" rid="bibr30-0145445513476609">Parker, Hagan-Burke, &amp; Vannest, 2006</xref>). In essence, these measures dichotomize data and examine the degree of overlap between baseline and intervention phases. For example, the PND measure counts the percentage of data points in the treatment phase that exceed the most extreme baseline point in the expected direction of change. PND is the most widely used metric (<xref ref-type="bibr" rid="bibr2-0145445513476609">Beretvas &amp; Chung, 2008</xref>) although applications of more recently developed measures such as PEM (<xref ref-type="bibr" rid="bibr26-0145445513476609">Ma, 2009</xref>; <xref ref-type="bibr" rid="bibr34-0145445513476609">Preston &amp; Carter, 2009</xref>), IRD (<xref ref-type="bibr" rid="bibr11-0145445513476609">Ganz et al., 2011</xref>; <xref ref-type="bibr" rid="bibr35-0145445513476609">Reynhout &amp; Carter, 2011</xref>), and PAND (<xref ref-type="bibr" rid="bibr37-0145445513476609">Schneider, Goldstein, &amp; Parker, 2008</xref>) are appearing in the literature. Overlap measures have the additional advantages that they are relatively simple to calculate (<xref ref-type="bibr" rid="bibr32-0145445513476609">Parker, Vannest, &amp; Davis, 2011</xref>) and are reasonably conceptually transparent.</p>
<p>Despite these advantages, overlap measures have been subject to criticism on a number of grounds (<xref ref-type="bibr" rid="bibr13-0145445513476609">Haardorfer, 2010</xref>; <xref ref-type="bibr" rid="bibr36-0145445513476609">Salzberg, Strain, &amp; Baer, 1987</xref>; <xref ref-type="bibr" rid="bibr41-0145445513476609">White, 1987</xref>) with <xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al. (2010)</xref> recently arguing that these measures should be abandoned. Two particular criticisms are that these types of metrics cannot be considered effect sizes as they fail to measure magnitude (<xref ref-type="bibr" rid="bibr13-0145445513476609">Haardorfer, 2010</xref>; <xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al., 2010</xref>) and that they do not correspond sufficiently with visual judgments to offer a useful approach to synthesizing single-subject research (<xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al., 2010</xref>). These criticisms will now be examined and it will be argued that they are based on fundamental misconceptions regarding the nature of effect sizes and their appropriate interpretation in single-subject research. Some general suggestions for an approach to the evaluation of single-subject research will then be offered.</p>
<sec id="section1-0145445513476609">
<title>Measurement of Magnitude</title>
<p><xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al. (2010)</xref> argued that overlap-based measures “are not an estimate of the magnitude of the effects between conditions” (p. 24). This claim is echoed by <xref ref-type="bibr" rid="bibr13-0145445513476609">Haardorfer (2010)</xref> in reference to the PND statistic in noting that “it is not an effect size measure; it does not measure the magnitude of an effect” (p. 127). Furthermore, Haardorfer pointed out that <xref ref-type="bibr" rid="bibr38-0145445513476609">Scruggs, Mastropieri, and Casto (1987a)</xref> explicitly stated that they viewed PND as a measure of convincingness of effect, rather than magnitude. While Scruggs et al. may not have considered PND an effect size metric, many other certainly have regarded it as such (e.g., <xref ref-type="bibr" rid="bibr2-0145445513476609">Beretvas &amp; Chung, 2008</xref>; <xref ref-type="bibr" rid="bibr32-0145445513476609">Parker, Vannest, &amp; Davis, 2011</xref>).</p>
<p>The basis of the argument forwarded by <xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al. (2010)</xref> and <xref ref-type="bibr" rid="bibr13-0145445513476609">Haardorfer (2010)</xref> appears to be based on a circumscribed view of the terms <italic>magnitude</italic> and <italic>effect size</italic>. As previously noted, effect sizes are by definition values that reflect the magnitude of a relationship between two variables. There are a wide variety of different measures of effect size. In traditional group meta-analysis, a variety of effect sizes may be used, including unstandardized differences and standardized mean differences (e.g., Cohen’s <italic>d</italic>, Hedges’ <italic>g</italic>) for continuous data. In the latter case, magnitude is reflected in the standardized difference between group means. Correlational metrics may also be used (<xref ref-type="bibr" rid="bibr24-0145445513476609">Lipsy &amp; Wilson, 2001</xref>) reflecting the magnitude of association between paired data. Furthermore, when dichotomous data are examined, measures such as the odds ratio, risk difference, and intervention rate difference may be used (<xref ref-type="bibr" rid="bibr3-0145445513476609">Borenstein et al., 2009</xref>). In the example of the odds ratio, the magnitude of difference between groups is reflected in the ratio of the odds of an event, such as passing a test, occurring in one group to the odds of that event occurring in a second group. Not only are all these measures considered effect sizes that reflect magnitude of difference but formulas are also available to allow conversion between metrics when studies use different types of endpoints (e.g., dichotomous and continuous; <xref ref-type="bibr" rid="bibr3-0145445513476609">Borenstein et al., 2009</xref>).</p>
<p>Overlap-based effect size measures may be considered similar to dichotomous metrics (e.g., odds ratio) in the sense that data are treated as binary (overlapping or nonoverlapping) with the criterion for overlap defined by the specific measure (e.g., most extreme baseline value, median value). In fact, IRD is a direct extension of the IRD metric used in group research (<xref ref-type="bibr" rid="bibr29-0145445513476609">Parker &amp; Hagan-Burke, 2007</xref>). It would certainly be legitimate to argue that overlap-based measures may be insensitive to underlying variability under some circumstances, such as when there is no overlap between baseline and intervention phases or where baseline data reaches a floor or ceiling when using the PND metric. Nevertheless, it appears to be incorrect to suggest that magnitude is not measured and consequently that these metrics do not constitute effect size measures.</p>
</sec>
<sec id="section2-0145445513476609">
<title>Correspondence With Visual Judgment</title>
<p>The efficacy of single-subject research has traditionally been judged by visual inspection, despite long-standing concern regarding lack of reliability between judges, even experts (<xref ref-type="bibr" rid="bibr10-0145445513476609">Fisch, 1998</xref>). <xref ref-type="bibr" rid="bibr2-0145445513476609">Beretvas and Chung (2008)</xref> have argued that effect size summaries in single-subject research “provide an alternative to visual inspection” (p. 129). A persistent criticism of nonoverlap effect size metrics is that they do not necessarily correspond with visual judgments regarding treatment effects for some data sets. For example, <xref ref-type="bibr" rid="bibr41-0145445513476609">White (1987)</xref> raised the issue of the failure of PND to address obvious baseline trends in his commentary on the initial description of the technique. Recently, in considering overlap-based effect size measures, <xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al. (2010)</xref> argued that “a major criterion for judging the utility and rigor of these methods is to determine the extent to which they agree with judgments of visual analysts” (p. 19). Wolery et al. compared correspondence of several overlap measures with visual judgments about whether change was present in AB sequences. Using the criteria for PND, which may not be appropriate to other measures, Wolery et al. reported total errors between 13.2% and 22.3% when compared with data series identified as having treatment effects or no treatment effects by visual judgment. Based on this analysis, they held that overlap-based metrics are inadequate as they fail to detect all the relevant characteristics of time series data, specifically trend and variability. The question that arises is whether this is an appropriate or reasonable standard for the judgment of overlap measures? To address this question, it is helpful to consider the distinction between experimental control and effect size in traditional group research.</p>
<p>In traditional group research, the ability of a study to adequately demonstrate experimental control is substantially a product of the <italic>a priori</italic> features of research design, such as randomization and blinding. That is, a well-designed study offers a good probability of concluding that any observed effects are likely to be a result of the intervention rather than extraneous uncontrolled variables. Nevertheless, some threats to internal validity may arise after the commencement of research. For example, differential attrition in groups may compromise the researchers’ ability to draw causal inference about intervention effects (<xref ref-type="bibr" rid="bibr5-0145445513476609">Campbell &amp; Stanley, 1963</xref>). In addition, despite randomization, pretest differences between groups may compromise interpretation, particularly when participants are highly heterogeneous (<xref ref-type="bibr" rid="bibr12-0145445513476609">Gersten et al., 2005</xref>) or when group size is small. Once data collection is complete, researchers typically use inferential statistical tests to determine whether results as extreme as those observed are probable if the null hypothesis is true. It should be stressed that at this point, we know nothing about the magnitude of any effect, just whether our design will allow strong causal inference about observed effects (or lack there of) and whether any observed effect is sufficiently large to be unlikely due to chance on the assumption that the null hypothesis is true. In contrast, effect sizes do provide us with an index of the magnitude of a treatment effect. The term <italic>effect size</italic> does not imply causation and effect sizes are independent of demonstrations of experimental control (<xref ref-type="bibr" rid="bibr1-0145445513476609">Allison &amp; Gorman, 1993</xref>). That is, a large effect size may be estimated even when convincing demonstration of experimental control can be excluded on the basis of fatal design flaws. Conversely, demonstration of experimental control may be unequivocal, but the calculated effect size may be very small.</p>
<p>The same basic issues may be seen to apply with single-subject research. That is, demonstration of experimental control is conceptually discrete from that of the magnitude of treatment effects, although in practice, the issues become somewhat more blurred. Careful attention to experimental design can reduce threats to internal validity and significantly enhance the ability to demonstrate experimental control in single-subject research. Nevertheless, even with due diligence to planning the design of a study, there remains the possibility that characteristics of data, such as baseline trends or instability, may still compromise ability to draw causal inferences. Ideally, baseline data should not exhibit trend or substantial variability (<xref ref-type="bibr" rid="bibr20-0145445513476609">Kazdin, 1982</xref>) and researchers should attempt to establish a steady behavior state before intervention (<xref ref-type="bibr" rid="bibr21-0145445513476609">Kennedy, 2005</xref>). In applied research, however, it may be undesirable to delay intervention for extended periods (<xref ref-type="bibr" rid="bibr21-0145445513476609">Kennedy, 2005</xref>). Furthermore, in some cases, the issue becomes moot as intervention effects may be so powerful that they overwhelm baseline trends and variability. Thus, researchers ultimately need to make a subjective judgment about the degree of acceptable baseline trend and variability, given the anticipated magnitude of treatment effects (<xref ref-type="bibr" rid="bibr20-0145445513476609">Kazdin, 1982</xref>).</p>
<p><xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al. (2010)</xref> asserted that lack of correspondence between overlap measures and visual judgment undermines the validity of these metrics. The key issue in assessing this claim is what the judges were asked to evaluate—experimental control, magnitude of treatment effects, or both? Wolery et al. specifically asked, “Did a change exist in the data from Condition 1 to Condition 2?” This clearly addressed the question of experimental control rather than magnitude of effect and is characteristic of the type of judgments examined in many studies addressing visual analysis of single-subject research (e.g., <xref ref-type="bibr" rid="bibr6-0145445513476609">Carter, 2009</xref>; <xref ref-type="bibr" rid="bibr18-0145445513476609">Kahng et al., 2010</xref>; <xref ref-type="bibr" rid="bibr27-0145445513476609">Matyas &amp; Greenwood, 1990</xref>; <xref ref-type="bibr" rid="bibr28-0145445513476609">Ottenbacher, 1986</xref>). Obviously, it would be expected that on average, larger magnitude effects would tend to lead to more confident judgments regarding control, so the issues are related but are not inextricably linked. Large effect sizes are possible without experimental control, and conversely, small effect sizes are possible where control is unambiguous.</p>
<p>The criticism of <xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al. (2010)</xref> that overlap-based measures do not sufficiently correspond to visual judgments conflates two separate but correlated issues, experimental control, and magnitude of effects. In fact, the findings of Wolery et al. may be seen as somewhat predictable. That is, a substantial degree of correspondence between visual judgment of experimental control and measured magnitude of effect is expected, but that correspondence is less than perfect. Nevertheless, depreciating the value of overlap metrics on the basis of their comparison with visual judgment focusing primarily on demonstrated experimental control (in contrast to magnitude) is fundamentally inappropriate. More appropriate comparisons between visual judgment of <italic>magnitude</italic> of effect and overlap measures (<xref ref-type="bibr" rid="bibr25-0145445513476609">Ma, 2006</xref>; <xref ref-type="bibr" rid="bibr29-0145445513476609">Parker &amp; Hagan-Burke, 2007</xref>) have yielded degrees of correlation that are arguably of less concern than the levels of correspondence between visual judges with regard to experimental control (e.g., <xref ref-type="bibr" rid="bibr9-0145445513476609">DeProspero &amp; Cohen, 1979</xref>; <xref ref-type="bibr" rid="bibr28-0145445513476609">Ottenbacher, 1986</xref>). It should, however, be noted that more recent research has suggested that judgments regarding control may be more consistent than previously thought (<xref ref-type="bibr" rid="bibr6-0145445513476609">Carter, 2009</xref>; <xref ref-type="bibr" rid="bibr18-0145445513476609">Kahng et al., 2010</xref>).</p>
</sec>
<sec id="section3-0145445513476609">
<title>Considerations in Evaluating Efficacy in Single-Subject Research</title>
<p>In evaluating single-subject studies, two sets of judgments need to be made. First a judgment needs to be made about experimental control. This includes an assessment of whether the research design used can allow the possibility of adequate demonstrations of control and then whether such control has actually been observed. <xref ref-type="bibr" rid="bibr17-0145445513476609">Horner et al. (2005)</xref> suggested that at least three clear demonstrations of experimental control at different time points should be the minimum standard in single-subject research studies. Assuming the basic design features are such that reasonable inferences regarding causation are possible, appraisal needs to be made regarding whether presented data do demonstrate experimental control. This analysis will need to consider immediacy of treatment effects, change in level, change in trend, and variability in data. Furthermore, complex judgments of multiple phases in a study may need to be made to make an overall assessment regarding experimental control (<xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al., 2010</xref>), and visual inspection remains the accepted method to make such judgments (<xref ref-type="bibr" rid="bibr22-0145445513476609">Kratochwill et al., 2010</xref>). Second, a judgment needs to be made about the magnitude of an effect and overlap-based effect size metrics appear to offer a viable option. These measures, however, need to be interpreted in the context of experimental control. Where such control cannot be demonstrated or is not demonstrated, measures of magnitude of effect are not meaningful.</p>
<p>One approach might be to only proceed to data aggregation when experimental control is clearly demonstrated, but such a strategy is inherently problematic as it effectively means that only the “winners” are counted. In effect, more nuanced use and interpretation of effect sizes may be appropriate with consideration of whether experimental control has been demonstrated in the first instance and the magnitude of the effect in the second. For example, one approach to data aggregation might involve partialing or removal of studies with experimental designs that prohibit adequate demonstration of experimental control, consistent with the “best evidence” approach advocated by <xref ref-type="bibr" rid="bibr40-0145445513476609">Slavin (1987)</xref>. The calculated effect size for the remaining studies needs to be interpreted in relation to the proportion of data series in which clear experimental control has been demonstrated. Impressive effect sizes in the absence of high levels of demonstrated experimental control should be depreciated. Given effect sizes and experimental control need to be considered in quantitative synthesis of single-subject research, a possible direction for future research might be to examine how these two factors can be incorporated into a single metric.</p>
<p><xref ref-type="bibr" rid="bibr14-0145445513476609">Harvey et al. (2009)</xref> noted that choice of effect size metrics is an extensively debated issue and that “each time a new metric is proposed, its advocate typically emphasises its virtue by pointing out flaws in other metrics” (p. 71). Taking a more positive viewpoint, it could be argued there has been rapid development of metrics as well as increasing debate regarding their statistical properties and relative merits. This has included the development of overlap metrics that may address baseline trend (<xref ref-type="bibr" rid="bibr32-0145445513476609">Parker, Vannest, &amp; Davis, 2011</xref>; <xref ref-type="bibr" rid="bibr33-0145445513476609">Parker, Vannest, Davis, &amp; Sauber, 2011</xref>) as well as extension of existing measures to examine reversals in addition to the conventional AB phase comparisons (<xref ref-type="bibr" rid="bibr31-0145445513476609">Parker et al., 2009</xref>). Issues still remain to be adequately addressed, such as possible inflation of some effect sizes by the extended collection of treatment data (<xref ref-type="bibr" rid="bibr13-0145445513476609">Haardorfer, 2010</xref>; <xref ref-type="bibr" rid="bibr42-0145445513476609">Wolery et al., 2010</xref>) and the synthesis of alternating treatment designs, which do not primarily rely on baseline comparisons. It is currently uncertain as to which metric or combination of metrics will ultimately provide the most appropriate outcome measure in single-subject research. Until such time as the picture becomes clearer, researchers may be well advised to consider the use of multiple measures to triangulate findings (<xref ref-type="bibr" rid="bibr2-0145445513476609">Beretvas &amp; Chung, 2008</xref>). Convergence of findings regarding the magnitude of effects will increase confidence in conclusions drawn by reviewers (<xref ref-type="bibr" rid="bibr35-0145445513476609">Reynhout &amp; Carter, 2011</xref>).</p>
<p>Aggregation of data presents inherent risks, in particular the risk of loss of information on idiosyncratic response to intervention (<xref ref-type="bibr" rid="bibr8-0145445513476609">Critchfield et al., 2000</xref>; <xref ref-type="bibr" rid="bibr36-0145445513476609">Salzberg et al., 1987</xref>). Nevertheless, generalities are important in science, no single finding is authoritative, and narrative and quantitative reviews can make converging contributions to understanding (<xref ref-type="bibr" rid="bibr8-0145445513476609">Critchfield et al., 2000</xref>).</p>
</sec>
<sec id="section4-0145445513476609">
<title>Conclusion</title>
<p>Two criticisms of overlap-based effect size metrics are addressed in this article and it is argued that both arise from misconceptions regarding the nature of effect sizes and what they tell us. Overlap-based measures do reflect the magnitude of changes between baseline and treatment, but they do so by dichotomizing data. Furthermore, comparison of visual judgment regarding experimental control and effect sizes inappropriately conflates the issues of control and magnitude, which are to some extent independent. Overlap-based measures can describe the magnitude of effects but cannot tell us whether effects are functionally related to interventions. This requires assessment of the basic features of experimental design as well as the analysis of data to evaluate experimental control, typically involving visual inspection.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<bio>
<title>Author Biography</title>
<p><bold>Mark Carter</bold> is an associate professor at the Macquarie University Special Education Centre, Sydney, Australia. He has interests in evidence-based practice in special education, single-subject research and autism spectrum disorders.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Allison</surname><given-names>D. B.</given-names></name>
<name><surname>Gorman</surname><given-names>B. S.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Calculating effect sizes for meta-analysis: The case of the single case</article-title>. <source>Behavior Research and Therapy</source>, <volume>31</volume>, <fpage>621</fpage>-<lpage>631</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0005-7967(93)90115-B</pub-id></citation>
</ref>
<ref id="bibr2-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Beretvas</surname><given-names>S. N.</given-names></name>
<name><surname>Chung</surname><given-names>H.</given-names></name>
</person-group> (<year>2008</year>). <article-title>A review of meta-analyses of single-subject experimental designs: Methodological issues and practice</article-title>. <source>Evidence-Based Communication Assessment and Intervention</source>, <volume>2</volume>, <fpage>129</fpage>-<lpage>141</lpage>. doi:<pub-id pub-id-type="doi">10.1080/17489530802446302</pub-id></citation>
</ref>
<ref id="bibr3-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Borenstein</surname><given-names>M.</given-names></name>
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
<name><surname>Higgins</surname><given-names>J. P. T.</given-names></name>
<name><surname>Rothstein</surname><given-names>H. R.</given-names></name>
</person-group> (<year>2009</year>). <source>Introduction to meta-analysis</source>. <publisher-loc>Chichester, UK</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr4-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Busk</surname><given-names>P. L.</given-names></name>
<name><surname>Serlin</surname><given-names>R. C.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Meta-analysis for single-case research</article-title>. In <person-group person-group-type="editor">
<name><surname>Kratochwill</surname><given-names>T. R.</given-names></name>
<name><surname>Levin</surname><given-names>J. R.</given-names></name>
</person-group> (Eds.), <source>Single-case research design and analysis: New directions for psychology and education</source>. <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr5-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Campbell</surname><given-names>D. T.</given-names></name>
<name><surname>Stanley</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1963</year>). <source>Experimental and quasi-experimental designs for research</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>Rand McNally</publisher-name>.</citation>
</ref>
<ref id="bibr6-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carter</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Effects of graphing conventions and response options on interpretation of small n graphs</article-title>. <source>Educational Psychology</source>, <volume>29</volume>, <fpage>643</fpage>-<lpage>658</lpage>. doi:<pub-id pub-id-type="doi">10.1080/ 01443410903204315</pub-id></citation>
</ref>
<ref id="bibr7-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Center</surname><given-names>B. J.</given-names></name>
<name><surname>Skiba</surname><given-names>R. J.</given-names></name>
<name><surname>Casey</surname><given-names>A.</given-names></name>
</person-group> (<year>1985</year>). <article-title>A methodology for the quantitative synthesis of intra-subject design research</article-title>. <source>Journal of Special Education</source>, <volume>19</volume>, <fpage>387</fpage>-<lpage>400</lpage>. doi:<pub-id pub-id-type="doi">10.1177/002246698501900404</pub-id></citation>
</ref>
<ref id="bibr8-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Critchfield</surname><given-names>T. S.</given-names></name>
<name><surname>Newland</surname><given-names>M. C.</given-names></name>
<name><surname>Kollins</surname><given-names>S. H.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The good, the bad, and the aggregate</article-title>. <source>Behavior Analyst</source>, <volume>23</volume>, <fpage>107</fpage>-<lpage>115</lpage>.</citation>
</ref>
<ref id="bibr9-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>DeProspero</surname><given-names>A.</given-names></name>
<name><surname>Cohen</surname><given-names>S.</given-names></name>
</person-group> (<year>1979</year>). <article-title>Inconsistent visual analyses of intrasubject data</article-title>. <source>Journal of Applied Behavior Analysis</source>, <volume>12</volume>, <fpage>573</fpage>-<lpage>579</lpage>. doi:<pub-id pub-id-type="doi">10.1901/jaba.1979.12-573</pub-id></citation>
</ref>
<ref id="bibr10-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fisch</surname><given-names>G. S.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Visual inspection of data revisited: Do the eyes still have it?</article-title> <source>Behavior Analyst</source>, <volume>21</volume>, <fpage>111</fpage>-<lpage>123</lpage>.</citation>
</ref>
<ref id="bibr11-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ganz</surname><given-names>J. B.</given-names></name>
<name><surname>Earles-Vollrath</surname><given-names>T. L.</given-names></name>
<name><surname>Heath</surname><given-names>A. K.</given-names></name>
<name><surname>Parker</surname><given-names>R. I.</given-names></name>
<name><surname>Rispoli</surname><given-names>M. J.</given-names></name>
<name><surname>Duran</surname><given-names>J. B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A meta-analysis of single case research studies on aided augmentative and alternative communication systems with individuals with autism spectrum disorders</article-title>. <source>Journal of Autism and Developmental Disorders</source>, <volume>42</volume>, <fpage>60</fpage>-<lpage>74</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10803-011-1212-2</pub-id></citation>
</ref>
<ref id="bibr12-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gersten</surname><given-names>R.</given-names></name>
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Compton</surname><given-names>D.</given-names></name>
<name><surname>Coyne</surname><given-names>M.</given-names></name>
<name><surname>Greenwood</surname><given-names>C.</given-names></name>
<name><surname>Innocenti</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Quality indicators for group experimental and quasi-experimental research in special education</article-title>. <source>Exceptional Children</source>, <volume>71</volume>, <fpage>149</fpage>-<lpage>164</lpage>.</citation>
</ref>
<ref id="bibr13-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haardorfer</surname><given-names>R.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Concerns with using Cohen’s d and PND in single-case data analysis</article-title>. <source>Focus on Autism and Other Developmental Disabilities</source>, <volume>25</volume>, <fpage>125</fpage>-<lpage>127</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1088357610371274</pub-id></citation>
</ref>
<ref id="bibr14-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Harvey</surname><given-names>S.</given-names></name>
<name><surname>Boer</surname><given-names>D.</given-names></name>
<name><surname>Meyer</surname><given-names>L.</given-names></name>
<name><surname>Evans</surname><given-names>I.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Updating a meta-analysis of intervention research with challenging behaviour: Treatment validity and standards of practice</article-title>. <source>Journal of Intellectual &amp; Developmental Disability</source>, <volume>34</volume>, <fpage>67</fpage>-<lpage>80</lpage>. doi:<pub-id pub-id-type="doi">10.1080/13668250802690922</pub-id></citation>
</ref>
<ref id="bibr15-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hattie</surname><given-names>J. A. C.</given-names></name>
</person-group> (<year>2009</year>). <source>Visible learning: A synthesis of over 800 meta-analyses relating to achievement</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr16-0145445513476609">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Higgins</surname><given-names>J. P. T.</given-names></name>
<name><surname>Green</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <source>Cochrane handbook for systematic reviews of interventions</source> (Version 5.0.2). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.cochrane-handbook.org/">http://www.cochrane-handbook.org/</ext-link></citation>
</ref>
<ref id="bibr17-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Horner</surname><given-names>R. H.</given-names></name>
<name><surname>Carr</surname><given-names>E. G.</given-names></name>
<name><surname>Halle</surname><given-names>J.</given-names></name>
<name><surname>McGee</surname><given-names>G.</given-names></name>
<name><surname>Odom</surname><given-names>S.</given-names></name>
<name><surname>Wolery</surname><given-names>M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The use of single-subject research to identify evidence-based practice in special education</article-title>. <source>Exceptional Children</source>, <volume>71</volume>, <fpage>165</fpage>-<lpage>179</lpage>.</citation>
</ref>
<ref id="bibr18-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kahng</surname><given-names>S.</given-names></name>
<name><surname>Chung</surname><given-names>K. M.</given-names></name>
<name><surname>Gutshall</surname><given-names>K.</given-names></name>
<name><surname>Pitts</surname><given-names>S. C.</given-names></name>
<name><surname>Kao</surname><given-names>J.</given-names></name>
<name><surname>Girolami</surname><given-names>K.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Consistent visual analyses of intrasubject data</article-title>. <source>Journal of Applied Behavior Analysis</source>, <volume>43</volume>, <fpage>35</fpage>-<lpage>45</lpage>. doi:<pub-id pub-id-type="doi">10.1901/jaba.2010.43-35</pub-id></citation>
</ref>
<ref id="bibr19-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kavale</surname><given-names>K. A.</given-names></name>
<name><surname>Forness</surname><given-names>S. R.</given-names></name>
</person-group> (<year>1999</year>). <source>Efficacy of special education and related services</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>AAMR</publisher-name>.</citation>
</ref>
<ref id="bibr20-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kazdin</surname><given-names>A. E.</given-names></name>
</person-group> (<year>1982</year>). <source>Single-case research designs: Methods for clinical and applied settings</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr21-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kennedy</surname><given-names>C. H.</given-names></name>
</person-group> (<year>2005</year>). <source>Single-case designs for education research</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Allyn &amp; Bacon</publisher-name>.</citation>
</ref>
<ref id="bibr22-0145445513476609">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Kratochwill</surname><given-names>T. R.</given-names></name>
<name><surname>Hitchcock</surname><given-names>J.</given-names></name>
<name><surname>Horner</surname><given-names>R. H.</given-names></name>
<name><surname>Levin</surname><given-names>J. R.</given-names></name>
<name><surname>Odom</surname><given-names>S. L.</given-names></name>
<name><surname>Rindskopf</surname><given-names>D. M.</given-names></name>
<name><surname>Shadish</surname><given-names>W. R.</given-names></name>
</person-group> (<year>2010</year>). <source>Single-case designs technical documentation</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/wwc/pdf/wwc_scd.pdf">http://ies.ed.gov/ncee/wwc/pdf/wwc_scd.pdf</ext-link></citation>
</ref>
<ref id="bibr23-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Leong</surname><given-names>H. M.</given-names></name>
<name><surname>Carter</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Research on the efficacy of sensory integration therapy: Past, present and future</article-title>. <source>Australasian Journal of Special Education</source>, <volume>32</volume>, <fpage>83</fpage>-<lpage>99</lpage>. doi:<pub-id pub-id-type="doi">10.1080/10300110701842653</pub-id></citation>
</ref>
<ref id="bibr24-0145445513476609">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lipsy</surname><given-names>M. W.</given-names></name>
<name><surname>Wilson</surname><given-names>D. B.</given-names></name>
</person-group> (<year>2001</year>). <source>Practical meta-analysis</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr25-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ma</surname><given-names>H. H.</given-names></name>
</person-group> (<year>2006</year>). <article-title>An alternative method for quantitative synthesis of single-subject researches: Percentage of data points exceeding the median</article-title>. <source>Behavior Modification</source>, <volume>30</volume>, <fpage>598</fpage>-<lpage>617</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0145445504272974</pub-id></citation>
</ref>
<ref id="bibr26-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ma</surname><given-names>H. H.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The effectiveness of intervention on the behavior of individuals with autism: A meta-analysis using percentage of data points exceeding the median of baseline phase (PEM)</article-title>. <source>Behavior Modification</source>, <volume>33</volume>, <fpage>339</fpage>-<lpage>359</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0145445509333173</pub-id></citation>
</ref>
<ref id="bibr27-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Matyas</surname><given-names>T. A.</given-names></name>
<name><surname>Greenwood</surname><given-names>K. M.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Visual analysis of single-case time series: Effects of variability, serial dependence and magnitude of intervention effects</article-title>. <source>Journal of Applied Behavior Analysis</source>, <volume>23</volume>, <fpage>341</fpage>-<lpage>351</lpage>. doi:<pub-id pub-id-type="doi">10.1901/jaba.1990.23-341</pub-id></citation>
</ref>
<ref id="bibr28-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ottenbacher</surname><given-names>K.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Reliability and accuracy of visually analyzing graphed data from single-subject designs</article-title>. <source>American Journal of Occupational Therapy</source>, <volume>40</volume>, <fpage>464</fpage>-<lpage>469</lpage>.</citation>
</ref>
<ref id="bibr29-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parker</surname><given-names>R. I.</given-names></name>
<name><surname>Hagan-Burke</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Median-based overlap analysis for single case data: A second study</article-title>. <source>Behavior Modification</source>, <volume>31</volume>, <fpage>919</fpage>-<lpage>936</lpage>. doi:<pub-id pub-id-type="doi">10.1177/ 0145445507303452</pub-id></citation>
</ref>
<ref id="bibr30-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parker</surname><given-names>R. I.</given-names></name>
<name><surname>Hagan-Burke</surname><given-names>S.</given-names></name>
<name><surname>Vannest</surname><given-names>K.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Percentage of all non-overlapping data (PAND): An alternative to PND</article-title>. <source>Journal of Special Education</source>, <volume>40</volume>, <fpage>194</fpage>-<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr31-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parker</surname><given-names>R. I.</given-names></name>
<name><surname>Vannest</surname><given-names>K. J.</given-names></name>
<name><surname>Brown</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The improvement rate difference for single-case research</article-title>. <source>Exceptional Children</source>, <volume>75</volume>, <fpage>135</fpage>-<lpage>150</lpage>.</citation>
</ref>
<ref id="bibr32-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parker</surname><given-names>R. I.</given-names></name>
<name><surname>Vannest</surname><given-names>K. J.</given-names></name>
<name><surname>Davis</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Effect size in single-case research: A review of nine nonoverlap techniques</article-title>. <source>Behavior Modification</source>, <volume>35</volume>, <fpage>303</fpage>-<lpage>322</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0145445511399147</pub-id></citation>
</ref>
<ref id="bibr33-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parker</surname><given-names>R. I.</given-names></name>
<name><surname>Vannest</surname><given-names>K. J.</given-names></name>
<name><surname>Davis</surname><given-names>J. L.</given-names></name>
<name><surname>Sauber</surname><given-names>S. B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Combining nonoverlap and trend for single-case research: Tau-U</article-title>. <source>Behavior Therapy</source>, <volume>42</volume>, <fpage>284</fpage>-<lpage>299</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.beth.2010.08.006</pub-id></citation>
</ref>
<ref id="bibr34-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Preston</surname><given-names>D.</given-names></name>
<name><surname>Carter</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A review of the efficacy of the picture exchange communication system intervention</article-title>. <source>Journal of Autism and Developmental Disorders</source>, <volume>39</volume>, <fpage>1471</fpage>-<lpage>1486</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10803-009-0763-y</pub-id></citation>
</ref>
<ref id="bibr35-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reynhout</surname><given-names>G.</given-names></name>
<name><surname>Carter</surname><given-names>M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Evaluation of the efficacy of social stories (TM) using three single subject metrics</article-title>. <source>Research in Autism Spectrum Disorders</source>, <volume>5</volume>, <fpage>885</fpage>-<lpage>900</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.rasd.2010.10.003</pub-id></citation>
</ref>
<ref id="bibr36-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Salzberg</surname><given-names>C. L.</given-names></name>
<name><surname>Strain</surname><given-names>P. S.</given-names></name>
<name><surname>Baer</surname><given-names>D. M.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Meta-analysis for single-subject research: When does it clarify, when does it obscure?</article-title> <source>Remedial and Special Education</source>, <volume>8</volume>(<issue>2</issue>), <fpage>43</fpage>-<lpage>48</lpage>.</citation>
</ref>
<ref id="bibr37-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schneider</surname><given-names>N.</given-names></name>
<name><surname>Goldstein</surname><given-names>H.</given-names></name>
<name><surname>Parker</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Social skills interventions for children with autism: A meta-analytic application of percentage of all non-overlapping data (PAND)</article-title>. <source>Evidence-Based Communication Assessment and Intervention</source>, <volume>2</volume>, <fpage>152</fpage>-<lpage>168</lpage>. doi:<pub-id pub-id-type="doi">10.1080/17489530802505396</pub-id></citation>
</ref>
<ref id="bibr38-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scruggs</surname><given-names>T. E.</given-names></name>
<name><surname>Mastropieri</surname><given-names>M. A.</given-names></name>
<name><surname>Casto</surname><given-names>G.</given-names></name>
</person-group> (<year>1987a</year>). <article-title>Reply to Owen White</article-title>. <source>Remedial and Special Education</source>, <volume>8</volume>(<issue>2</issue>), <fpage>40</fpage>-<lpage>42</lpage>. doi:<pub-id pub-id-type="doi">10.1177/074193258700800208</pub-id></citation>
</ref>
<ref id="bibr39-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scruggs</surname><given-names>T. E.</given-names></name>
<name><surname>Mastropieri</surname><given-names>M. A.</given-names></name>
<name><surname>Casto</surname><given-names>G.</given-names></name>
</person-group> (<year>1987b</year>). <article-title>The quantitative synthesis of single-subject research: Methodology and validation</article-title>. <source>Remedial and Special Education</source>, <volume>8</volume>(<issue>2</issue>), <fpage>24</fpage>-<lpage>33</lpage>. doi:<pub-id pub-id-type="doi">10.1177/074193258700800206</pub-id></citation>
</ref>
<ref id="bibr40-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Mastery learning reconsidered</article-title>. <source>Review of Educational Research</source>, <volume>57</volume>, <fpage>175</fpage>-<lpage>213</lpage>.</citation>
</ref>
<ref id="bibr41-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>White</surname><given-names>O. R.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Some comments concerning “The quantitative synthesis of single-subject research.”</article-title> <source>Remedial and Special Education</source>, <volume>8</volume>(<issue>2</issue>), <fpage>34</fpage>-<lpage>39</lpage>. doi:<pub-id pub-id-type="doi">10.1177/074193258700800207</pub-id></citation>
</ref>
<ref id="bibr42-0145445513476609">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolery</surname><given-names>M.</given-names></name>
<name><surname>Busick</surname><given-names>M.</given-names></name>
<name><surname>Reichow</surname><given-names>B.</given-names></name>
<name><surname>Barton</surname><given-names>E. E.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Comparison of overlap methods for quantitatively synthesizing single-subject data</article-title>. <source>Journal of Special Education</source>, <volume>44</volume>, <fpage>18</fpage>-<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0022466908328009</pub-id></citation>
</ref>
</ref-list>
</back>
</article>