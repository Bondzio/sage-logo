<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPM</journal-id>
<journal-id journal-id-type="hwp">spepm</journal-id>
<journal-title>Educational and Psychological Measurement</journal-title>
<issn pub-type="ppub">0013-1644</issn>
<issn pub-type="epub">1552-3888</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0013164411422903</article-id>
<article-id pub-id-type="publisher-id">10.1177_0013164411422903</article-id>
<title-group>
<article-title>Comparison Between Dichotomous and Polytomous Scoring of Innovative Items in a Large-Scale Computerized Adaptive Test</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Jiao</surname><given-names>Hong</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411422903">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Liu</surname><given-names>Junhui</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411422903">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Haynie</surname><given-names>Kathleen</given-names></name>
<xref ref-type="aff" rid="aff2-0013164411422903">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Woo</surname><given-names>Ada</given-names></name>
<xref ref-type="aff" rid="aff3-0013164411422903">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Gorham</surname><given-names>Jerry</given-names></name>
<xref ref-type="aff" rid="aff4-0013164411422903">4</xref>
</contrib>
</contrib-group>
<aff id="aff1-0013164411422903"><label>1</label>University of Maryland, College Park, MD, USA</aff>
<aff id="aff2-0013164411422903"><label>2</label>Haynie Research and Evaluation, Skillman, NJ, USA</aff>
<aff id="aff3-0013164411422903"><label>3</label>National Council of State Boards of Nursing, Chicago, IL, USA</aff>
<aff id="aff4-0013164411422903"><label>4</label>Pearson, Cerrillos, NM, USA</aff>
<author-notes>
<corresp id="corresp1-0013164411422903">Hong Jiao, Department of Measurement, Statistics and Evaluation, University of Maryland, 1230B Benjamin Building, College Park, MD 20742, USA Email: <email>hjiao@umd.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2012</year>
</pub-date>
<volume>72</volume>
<issue>3</issue>
<fpage>493</fpage>
<lpage>509</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This study explored the impact of partial credit scoring of one type of innovative items (multiple-response items) in a computerized adaptive version of a large-scale licensure pretest and operational test settings. The impacts of partial credit scoring on the estimation of the ability parameters and classification decisions in operational test settings were explored in one real data analysis and two simulation studies when two different polytomous scoring algorithms, automated polytomous scoring and rater-generated polytomous scoring, were applied. For the real data analyses, the ability estimates from dichotomous and polytomous scoring were highly correlated; the classification consistency between different scoring algorithms was nearly perfect. Information distribution changed slightly in the operational item bank. In the two simulation studies comparing each polytomous scoring with dichotomous scoring, the ability estimates resulting from polytomous scoring had slightly higher measurement precision than those resulting from dichotomous scoring. The practical impact related to classification decision was minor because of the extremely small number of items that could be scored polytomously in this current study.</p>
</abstract>
<kwd-group>
<kwd>polytomous scoring</kwd>
<kwd>innovative items</kwd>
<kwd>computerized adaptive test</kwd>
<kwd>Rasch model</kwd>
<kwd>partial credit model</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Computer-based testing has the potential to assess higher order thinking skills and abilities that are difficult or expensive to assess using traditional testing formats (<xref ref-type="bibr" rid="bibr3-0013164411422903">Harmes, 1999</xref>) such as multiple-choice items. Broader and more complex knowledge, skills, and abilities can be measured by innovative types of items (<xref ref-type="bibr" rid="bibr8-0013164411422903">Parshall, Spray, Kalohn, &amp; Davey, 2002</xref>; <xref ref-type="bibr" rid="bibr12-0013164411422903">Wendt, 2008</xref>). Innovative items may include the features of sound, graphics, animation, video, and simulation in the item stem, the response options, or both (<xref ref-type="bibr" rid="bibr1-0013164411422903">Bennett et al., 1999</xref>; <xref ref-type="bibr" rid="bibr2-0013164411422903">Chung &amp; Baker, 1997</xref>; <xref ref-type="bibr" rid="bibr8-0013164411422903">Parshall et al., 2002</xref>; <xref ref-type="bibr" rid="bibr9-0013164411422903">Pellegrino, Chudowsky, &amp; Glaser, 2001</xref>; <xref ref-type="bibr" rid="bibr12-0013164411422903">Wendt, 2008</xref>). As part of a properly constructed assessment, the use of innovative items should increase content coverage (<xref ref-type="bibr" rid="bibr11-0013164411422903">Scalise, 2007</xref>); the enlarged scope of content and increased complexity tapped by innovative items can provide better evidence of the representativeness of the population content domains. The increased information resulting from innovative items contributes to higher measurement accuracy, thus yielding more accurate estimation of examinees’ latent abilities. Overall, the use of innovative items should enhance content-related and construct-related evidence of validity and should increase measurement accuracy by providing more information.</p>
<p>Although the use of innovative items shows promise in terms of content coverage and measurement precision, there are still issues associated with developing and using these new item types in operational test settings. The major issues related to the use of innovative items are the costs and expenses associated with developing and using quality innovative items. These include designing and developing item delivery software (using a variety of media), developing examinee tutorials, conducting training for examinees’ computer skills, building automated scoring systems (<xref ref-type="bibr" rid="bibr6-0013164411422903">Parshall, Davey, &amp; Pashley, 2000</xref>; <xref ref-type="bibr" rid="bibr7-0013164411422903">Parshall &amp; Harmes, 2008</xref>), employing human scoring procedures, and conducting psychometric research.</p>
<p>The scoring of innovative items polytomously can be costly in terms of time and expense. If innovative items are to be scored using a polytomous measurement model, human raters typically rate each examinee’s response. Most often, two human raters per response are needed to guarantee the quality of ratings. In addition, human rating cannot be accomplished in real time during the test administration—a period of rating time between the administration of the examination and the availability of score results is necessary.</p>
<p>Automated scoring has great potential for scoring innovative items by overcoming these disadvantages of human rater scoring. When comparing automated scoring with rater’s rating, 100% consistency usually cannot be achieved (<xref ref-type="bibr" rid="bibr6-0013164411422903">Parshall et al., 2000</xref>) though rater’s rating is not the gold standard. In practice, some computer-based tests have used dichotomous scoring of innovative items, which may impede the full realization of the potential contribution of innovative items to measurement efficiency and accuracy.</p>
<p>The studied large-scale licensure test is a computerized adaptive test (CAT). It contains a number of innovative item types that include fill-in-the-blank, ordered response, multiple-response, and hot-spot items. In current operational administrations, these items are scored dichotomously, and calibrated using the dichotomous Rasch measurement model (<xref ref-type="bibr" rid="bibr10-0013164411422903">Rasch, 1960</xref>). Although some studies (e.g., <xref ref-type="bibr" rid="bibr12-0013164411422903">Wendt, 2008</xref>) found that the measurement properties of dichotomously scored innovative items are comparable with, or at times better than, traditional multiple-choice type of items; scoring innovative items polytomously is likely to provide even more useful information and greater measurement accuracy. Since the creation and the implementation of innovative items are costly, it is worth exploring whether polytomous scoring procedures for innovative items compare favorably with dichotomous scoring procedures. Thus, this study explored the calibration of one type of innovative items using a polytomous item response theory model, the partial credit model (<xref ref-type="bibr" rid="bibr5-0013164411422903">Master, 1982</xref>). The measurement accuracy based on dichotomous and polytomous scoring procedures are compared and evaluated.</p>
<sec id="section1-0013164411422903" sec-type="methods">
<title>Method</title>
<p>This study investigated the differences in measurement precision in person ability estimates and classification decisions when one type of innovative items (multiple-response) in a large-scale licensure CAT was scored both dichotomously and polytomously. It consisted of one real data analysis and two simulation studies. The real data analysis provided both the polytomous model parameter estimates for the simulation studies, as well as the results for making the comparison of differences in ability estimation and classification decisions between dichotomous and polytomous scorings of innovative items. In the first simulation study, the real test data structure was simulated based on a rater-generated polytomous scoring algorithm. In the second simulation study, the real test structure was simulated based on an automated polytomous scoring algorithm. For the simulated data sets, estimation errors in person parameter estimates and classification accuracy were compared based on both the dichotomous and the polytomous scoring of innovative items for the test under each of the polytomous scoring algorithms.</p>
<sec id="section2-0013164411422903">
<title>Real Data Analysis</title>
<p>Three data sets were obtained for the studied test from recent administrations: One contained dichotomous scores, and the other two contained polytomous scores for innovative items. One set of polytomous scores was obtained from human raters and the other set was obtained from an automated scoring algorithm. The analyses of real data followed the steps as listed in <xref ref-type="app" rid="app1-0013164411422903">Appendix A</xref>.</p>
</sec>
<sec id="section3-0013164411422903">
<title>Simulation of Item Response Data and Data Analyses</title>
<p>When innovative items were scored polytomously, it was in the context of mixed-format pretest and operational tests in which noninnovative items were scored dichotomously. True item parameters for the simulation studies were obtained from the concurrent calibration of the polytomously scored innovative items (with item location and step parameter estimates) and dichotomously scored noninnovative items (with item difficulty parameter estimates) in real data for the pretest and operational test. These were used for generating item responses (polytomous responses for innovative items and dichotomous responses for noninnovative items) for both simulated pretest and the operational test. True person ability parameters for the pretest and the operational test were the estimated values from real data analyses. The item responses for the pretest with random missing patterns and the operational test with conditional missing patterns were simulated based on the methodology of <xref ref-type="bibr" rid="bibr13-0013164411422903">Wolfe and McGill (2006)</xref>. The generated polytomous item responses with the missing pattern for the pretests and the operational test were recoded into dichotomous item responses for innovative items. The detailed steps for item response data generation and calibration are listed in <xref ref-type="app" rid="app2-0013164411422903">Appendix B</xref>.</p>
</sec>
<sec id="section4-0013164411422903">
<title>Evaluation Criteria</title>
<p>Simulation for the pretest and operational tests was each replicated 50 times to compute ability parameter estimation errors for both dichotomous and polytomous scoring in terms of average bias, standard error (<italic>SE</italic>), and root mean square error (RMSE). The bias, <italic>SE</italic>, and RMSE were computed based on Equations 1, 2, and 3, respectively.</p>
<p><disp-formula id="disp-formula1-0013164411422903">
<mml:math display="block" id="math1-0013164411422903">
<mml:mrow>
<mml:mtext>Bias</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mi>r</mml:mi>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mi>β</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:mfrac>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0013164411422903" xlink:href="10.1177_0013164411422903-eq1.tif"/>
</disp-formula></p>
<p><disp-formula id="disp-formula2-0013164411422903">
<mml:math display="block" id="math2-0013164411422903">
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mi>N</mml:mi>
</mml:mfrac>
<mml:msup>
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:msub>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mi>r</mml:mi>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>t</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:msub>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mi>t</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:mfrac>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:msqrt>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0013164411422903" xlink:href="10.1177_0013164411422903-eq2.tif"/>
</disp-formula></p>
<p><disp-formula id="disp-formula3-0013164411422903">
<mml:math display="block" id="math3-0013164411422903">
<mml:mrow>
<mml:mtext>RMSE</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mi>N</mml:mi>
</mml:mfrac>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mi>r</mml:mi>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mi>β</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
</mml:msqrt>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-0013164411422903" xlink:href="10.1177_0013164411422903-eq3.tif"/>
</disp-formula></p>
<p>where β is the true ability parameter, <inline-formula id="inline-formula1-0013164411422903">
<mml:math display="inline" id="math4-0013164411422903">
<mml:mrow>
<mml:msub>
<mml:mover accent="true">
<mml:mi>β</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mi>r</mml:mi>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is the estimated ability parameters for the <italic>r</italic>th replication, and <italic>N</italic> is the number of replications. The average bias, <italic>SE</italic>, and RMSE are computed by averaging each of the values over all ability parameters.</p>
</sec>
</sec>
<sec id="section5-0013164411422903" sec-type="results">
<title>Results</title>
<p>First presented are results for real data analyses comparing the polytomous and dichotomous scoring algorithms, followed by the results from simulation studies. For both real data analyses and simulation studies, results related to the rater-generated polytomous scoring are first reported followed by the results related to the automated polytomous scoring.</p>
<sec id="section6-0013164411422903">
<title>Real Data Analyses</title>
<sec id="section7-0013164411422903">
<title>Rater-generated polytomous scoring algorithm</title>
<p>The cut score for the operational test is −0.37. Under the dichotomous scoring algorithm, 68.7% of examinees were classified as pass, whereas under the polytomous scoring method, 68.6% of examinees were classified as pass; therefore, the passing rates were virtually equivalent. The classification consistency between these two scoring algorithms was 99.5%. The correlation between the ability estimates was .999. The estimated ability distributions for dichotomous and polytomous scoring were comparable (a summary table is available on request).</p>
<p>Information distribution in the operational item bank, including all items, was compared between the dichotomous and polytomous scoring (see <xref ref-type="fig" rid="fig1-0013164411422903">Figure 1</xref>). The information from the polytomous scoring was higher than that from the dichotomous scoring in the middle of the ability scale. The information was slightly different at the two ends of the ability scale. This indicates that measurement precision may vary slightly from one scoring method to the other.</p>
<fig id="fig1-0013164411422903" position="float">
<label>Figure 1.</label>
<caption>
<p>The information distribution in the item bank between dichotomous (MC) and polytomous (CR) scoring</p>
</caption>
<graphic xlink:href="10.1177_0013164411422903-fig1.tif"/>
</fig>
</sec>
<sec id="section8-0013164411422903">
<title>Automated polytomous scoring algorithm</title>
<p>For the automated polytomous scores, 68.4% of examinees were classified as passing. The passing rate was about the same for the dichotomous scoring. The consistent classification decision rate was 99.6%. The correlation between the ability estimates from dichotomous and polytomous scores was .999. Again, the estimated ability distributions for dichotomous and polytomous scoring were comparable, a similar pattern as observed for the comparison between rater-generated polytomous scores and dichotomous scores. Information distribution between the automated polytomous and dichotomous scoring methods followed the same pattern as that observed in <xref ref-type="fig" rid="fig1-0013164411422903">Figure 1</xref>.</p>
<p>The correlation between the ability estimates from automated polytomous scoring and rater-generated polytomous scores was .999. The classification consistency between two polytomous scoring algorithms was 99.9%. There was virtually no difference in the information distribution in the item bank between the two polytomous scoring algorithms.</p>
<p>The high classification consistency and the minor differences in the passing rates and the information distribution in the item bank between the dichotomous and polytomous scoring are because of the small number of polytomously scored items in both the pretest and the operational test, which were 40 and 10 innovative items out of 1,142 and 1,683 total items, respectively, when human raters generated the polytomous scores. Given the minimum number of items of 60, the polytomous scoring only changed a maximum of about 16% of the item responses. The numbers of innovative items scored polytomously based on the automated polytomous scoring method were 64 and 16 out of 1,142 and 1,683 items for the pretest and the operational test, respectively. There was an increase in the number of innovative items that can be scored polytomously based on the automated scoring algorithm. However, the impact on the ability parameter estimation and the classification decisions was not large. In general, based on the real data, scoring innovative items polytomously did not have much impact on ability estimation and classification decision.</p>
</sec>
</sec>
<sec id="section9-0013164411422903">
<title>Simulation Data Analyses</title>
<sec id="section10-0013164411422903">
<title>Rater-generated polytomous scoring algorithm</title>
<p>Item responses were simulated following the steps described in <xref ref-type="app" rid="app2-0013164411422903">Appendix B</xref>. To get a general idea of the conditional missing responses simulated for the operational test related to the rater-generated polytomous scoring algorithm, the number of items answered by each examinee is compared with that in the real data. The distribution of the number of items simulated for each examinee was very close to the values as obtained from the real data analyses. For example, the largest difference in the number of items simulated for examinees in the operational test in Replication 1 was three items. This discrepancy was because of the allocation of items based on the bin percentage information as displayed in <xref ref-type="table" rid="table4-0013164411422903">Table B2</xref> in <xref ref-type="app" rid="app2-0013164411422903">Appendix B</xref>. In general, examinees with true ability levels at the two ends of the scale responded to far fewer items compared with examinees in the middle of the scale who responded to more items (see <xref ref-type="fig" rid="fig2-0013164411422903">Figure 2</xref>). More items were especially needed to make pass/fail decisions around the cut scores.</p>
<fig id="fig2-0013164411422903" position="float">
<label>Figure 2.</label>
<caption>
<p>The total number of items simulated for examinees at different ability levels</p>
</caption>
<graphic xlink:href="10.1177_0013164411422903-fig2.tif"/>
</fig>
<p>In the simulation studies, the true ability parameters were known. The correlation between the estimated and the true ability was greater than .94 for both polytomous and dichotomous scoring. The estimation errors for ability parameters are summarized in <xref ref-type="table" rid="table1-0013164411422903">Table 1</xref>. In general, all three types of errors in ability estimation were slightly smaller for the polytomous partial credit model than those for the dichotomous Rasch measurement model. The effect sizes for the observed differences in bias, <italic>SE</italic>, and RMSE were large (1.32, 0.60, and 0.86, respectively). Both scoring methods slightly overestimated the true ability parameters along the ability scale.</p>
<table-wrap id="table1-0013164411422903" position="float">
<label>Table 1.</label>
<caption>
<p>Descriptive Statistics for the Ability Estimation Errors (Rater-Generated Polytomous Scores)</p>
</caption>
<graphic alternate-form-of="table1-0013164411422903" xlink:href="10.1177_0013164411422903-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center"><italic>N</italic></th>
<th align="center">Minimum</th>
<th align="center">Maximum</th>
<th align="center">Mean</th>
<th align="center">Standard deviation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Polytomous scoring</td>
<td>Bias</td>
<td>16,870</td>
<td>−.1710</td>
<td>.2979</td>
<td>.0346</td>
<td>.0374</td>
</tr>
<tr>
<td/>
<td><italic>SE</italic></td>
<td>16,870</td>
<td>.0978</td>
<td>.6895</td>
<td>.2374</td>
<td>.0619</td>
</tr>
<tr>
<td/>
<td>RMSE</td>
<td>16,870</td>
<td>.1086</td>
<td>.7059</td>
<td>.2429</td>
<td>.0614</td>
</tr>
<tr>
<td>Dichotomous scoring</td>
<td>Bias</td>
<td>16,870</td>
<td>−.1688</td>
<td>.3186</td>
<td>.0438</td>
<td>.0385</td>
</tr>
<tr>
<td/>
<td><italic>SE</italic></td>
<td>16,870</td>
<td>.1006</td>
<td>.6874</td>
<td>.2401</td>
<td>.0629</td>
</tr>
<tr>
<td/>
<td>RMSE</td>
<td>16,870</td>
<td>.1066</td>
<td>.7028</td>
<td>.2471</td>
<td>.0628</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013164411422903">
<p>Note: <italic>SE</italic> = standard error; RMSE = root mean square error.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The bias, <italic>SE</italic>, and RMSE for ability estimates from polytomous and dichotomous scoring are compared in <xref ref-type="fig" rid="fig3-0013164411422903">Figure 3</xref>. To make the plots easy to read, ability was categorized into intervals with a width of 0.1. Then the estimation errors were averaged across all examinees with ability within that interval. In general, over the ability scale, the bias in ability estimates was higher with dichotomous scoring than with polytomous scoring. The discrepancy was more evident at the higher end of the ability scale. The differences in <italic>SE</italic> between ability estimates from dichotomous and polytomous scoring were not evident at the middle of the scale; but slightly more discernable, at the two ends of the scale (as shown in <xref ref-type="fig" rid="fig3-0013164411422903">Figure 3</xref>), the ability estimates from dichotomous scoring had slightly higher <italic>SE</italic> than those from polytomous scoring. Similar patterns were observed for RMSE in ability estimates.</p>
<fig id="fig3-0013164411422903" position="float">
<label>Figure 3.</label>
<caption><p>Errors in ability estimates in dichotomous and polytomous scoring (rater)</p>
<p>Note: <italic>SE</italic> = standard error; RMSE = root mean square error.</p></caption>
<graphic xlink:href="10.1177_0013164411422903-fig3.tif"/>
</fig>
<p>The classification accuracy was obtained by comparing the percentage of correct classifications, given that the true class was known. The average classification accuracy over 50 replications was slightly higher for polytomous scoring (0.935) than for dichotomous scoring (0.934). The percentages of examinees classified as passing for both polytomous and dichotomous scoring were higher than the true passing rate. This is consistent with the overestimation of ability parameters. In this analysis, classification errors (false positives and false negatives) were further examined. Using Replication 1 results as an example, the classification errors for both polytomous and dichotomous scoring centered on the cut score. The classification discrepancy between the polytomous and dichotomous scoring algorithms centered on the cut scores as well. The classification inconsistencies between the polytomous and dichotomous scoring were only about 0.5%—roughly 92 out of 16,870 examinees would be affected by using different scoring methods in the operational test.</p>
<p>In summary, this section reports the results related to the simulated rater-generated polytomous scoring algorithm in comparison with the dichotomous scoring method used in the pretest and the operational test, where missing patterns were simulated to reflect these actual tests. Both scoring algorithms overestimated true ability parameters. The bias due to the polytomous scoring was relatively smaller than that from the dichotomous scoring, especially at the higher end of the ability scale. There was not much difference in the random error and the total error along the ability scale. The polytomous scoring had slightly higher classification accuracy; however, the difference in classification decision between the two scoring algorithms was not large—0.5% of the examinees (92 out of 16,870) would be affected. This minor impact could be attributed to the extremely small number of polytomously scored items in the tests. About 0.5% of the total items in the operational test (10 out of 1,683 items) were scored polytomously by human raters.</p>
</sec>
<sec id="section11-0013164411422903">
<title>Automated polytomous scoring algorithm</title>
<p>The simulated numbers of items answered by individual examinees for automated polytomous scoring were very close to those obtained from the real data analyses. The largest difference in the number of items simulated for examinees in the operational test were three more or two fewer items. For the same reason articulated with respect to simulating rater-generated polytomous scores, this discrepancy was because of the allocation of items based on the bin percentage information. In general, for the simulated operational test, examinees with abilities at the two ends of the ability distribution answered fewer items, and examinees with abilities near the cut score area responded to more items, a similar pattern as observed in <xref ref-type="fig" rid="fig2-0013164411422903">Figure 2</xref>.</p>
<p>The ability estimates from real data calibrations using the automated polytomous scoring were treated as true parameters in this simulation study. Item response data for innovative items were generated using a polytomous partial credit model and then recoded as dichotomous data. The generated data were calibrated through the same procedure as the real data analysis. The estimated abilities and true abilities were highly correlated—.94 for both polytomous and dichotomous scoring. The descriptive statistics for the estimation errors for person ability parameters are listed in <xref ref-type="table" rid="table2-0013164411422903">Table 2</xref>. Overall, the estimates from the polytomous partial credit model were slightly closer to the true parameters than the estimates from the dichotomous Rasch model. The effect sizes for the observed differences in bias, <italic>SE</italic>, and RMSE were large (1.09, 0.79, and 0.93, respectively). Both scoring methods slightly overestimated the true ability parameters.</p>
<table-wrap id="table2-0013164411422903" position="float">
<label>Table 2.</label>
<caption><p>Descriptive Statistics for the Ability Estimation Errors (Automated Polytomous Scoring)</p></caption>
<graphic alternate-form-of="table2-0013164411422903" xlink:href="10.1177_0013164411422903-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center"><italic>N</italic></th>
<th align="center">Minimum</th>
<th align="center">Maximum</th>
<th align="center">Mean</th>
<th align="center">Standard deviation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Polytomous scoring</td>
<td>Bias</td>
<td>16,870</td>
<td>−.1930</td>
<td>.1561</td>
<td>−.0257</td>
<td>.0372</td>
</tr>
<tr>
<td/>
<td><italic>SE</italic></td>
<td>16,870</td>
<td>.0997</td>
<td>.5914</td>
<td>.2370</td>
<td>.0613</td>
</tr>
<tr>
<td/>
<td>RMSE</td>
<td>16,870</td>
<td>.1027</td>
<td>.5956</td>
<td>.2414</td>
<td>.0611</td>
</tr>
<tr>
<td>Dichotomous scoring</td>
<td>Bias</td>
<td>16,870</td>
<td>−.2209</td>
<td>.1580</td>
<td>−.0350</td>
<td>.0391</td>
</tr>
<tr>
<td/>
<td><italic>SE</italic></td>
<td>16,870</td>
<td>.1015</td>
<td>.5932</td>
<td>.2414</td>
<td>.0631</td>
</tr>
<tr>
<td/>
<td>RMSE</td>
<td>16,870</td>
<td>.1051</td>
<td>.6015</td>
<td>.2470</td>
<td>.0634</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013164411422903">
<p>Note: <italic>SE</italic> = standard error; RMSE = root mean square error.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p><xref ref-type="fig" rid="fig4-0013164411422903">Figure 4</xref> shows the comparison between bias, <italic>SE</italic>, and RMSE for ability estimates from polytomous and dichotomous scoring. As in the simulation study based on human rater–generated polytomous responses, ability was regrouped into categories with an interval width of 0.1, and the estimation errors were averaged across all examinees with abilities within the interval. In general, the bias in the ability estimates was higher in dichotomous scoring than in polytomous scoring, when ability is larger than zero. The <italic>SE</italic> and RMSE statistics showed similar differences in ability estimates between the dichotomous and polytomous scoring. As demonstrated in <xref ref-type="fig" rid="fig4-0013164411422903">Figure 4</xref>, the difference was not evident at the lower end of the scale; however, at the upper end of the scale, the ability estimates from the dichotomous scoring method had higher <italic>SE</italic> and RMSE values than those from the polytomous scoring. For both polytomous and dichotomous scoring, the ability parameters were slightly overestimated.</p>
<fig id="fig4-0013164411422903" position="float">
<label>Figure 4.</label>
<caption><p>Errors in ability estimates in dichotomous and polytomous scoring (automated)</p>
<p>Note: <italic>SE</italic> = standard error; RMSE = root mean square error.</p></caption>
<graphic xlink:href="10.1177_0013164411422903-fig4.tif"/>
</fig>
<p>To compare classification accuracy, the percentages of correct classification of subjects were obtained for each scoring method in the simulation. Similar to the results from rater-generated polytomous scoring, the average classification accuracy over 50 replications was a little higher for the polytomous scoring (0.935) than for the dichotomous scoring (0.934). Classification errors (false positives and false negatives) were also obtained. For example, in Replication 1, the misclassified cases for both polytomous and dichotomous scoring clustered around the cut scores. The classification discrepancy was also near the cut scores. The classification error distribution is similar to that observed in rater-generated polytomous scores. A very small percentage of examinees, 0.7%, were not classified consistently for polytomous and dichotomous scoring; in other words, about 118 out of 16,870 examines would be affected by using these different scoring methods in a similar operational setting. The passing rates were higher than the true passing rate. This is consistent with the fact that the ability was overestimated.</p>
<p>In summary, both scoring algorithms overestimated person abilities. Bias resulting from the polytomous scoring method was smaller than that resulting from the dichotomous scoring method at the upper end of ability scale. Classification accuracy was slightly higher with polytomous scoring than with dichotomous scoring; however, the difference was minor since only small percentages (0.7%) of examinees would be affected if different scoring methods were to be used. The minor differences in the classification accuracy between different scoring methods may be due to the fact that only a small portion (about 1%) of items were scored polytomously under the automated polytomous scoring algorithms. However, there were more items that could be scored polytomously based on the automated scoring algorithm than those scored by human raters; therefore, the differences in classification accuracy were slightly larger than those obtained from the simulation of human rater–generated polytomous item responses.</p>
</sec>
</sec>
</sec>
<sec id="section12-0013164411422903">
<title>Summary and Discussions</title>
<p>This article investigated the impact of polytomously scoring one type of innovative items on ability estimation and pass/fail classification decision in a large-scale licensure CAT. Two types of polytomous scoring algorithms were studied: one was generated by subject experts (rater-generated polytomous scoring algorithm) and the other was an automated polytomous scoring algorithm. Both real data analysis and simulation studies based on the real data analyses results were conducted.</p>
<p>We expected that scoring innovative items polytomously (as opposed to dichotomously) would affect the accuracy in ability estimation. Specifically, assuming valid rubrics and accurate scoring processes, the item information provided by polytomously scoring innovative items is expected to be higher since increased (and possibly more refined) score categories will provide more information over the entire ability range. In the real data analyses, the ability estimation and classification decisions were not affected. This is due to the fact that only about 1% of the total items were scored polytomously in either the rater-generated polytomous scoring algorithm or the automated polytomous scoring algorithm. The simulation study did reveal a slightly higher measurement precision in polytomous scoring compared with dichotomous scoring. The ability levels were overestimated for both dichotomous and polytomous scoring methods under each of the polytomous scoring algorithms. When the rater-generated polytomous scoring rubrics were applied, the impact was smaller in terms of ability estimation errors and classification accuracy and consistency, compared with those under the automated polytomous scoring algorithm. This is because of the smaller number of innovative items that can be scored polytomously under the rater-generated rubrics compared with the number of polytomously scored items associated with the automated polytomous scoring method.</p>
<p>This study simulated the random missing patterns observed in the CAT pretest and the conditional missing patterns in the operational test. The test length for each examinee in the simulation study was identical to that observed in the real operational test. Although test length was simulated to vary from examinee to examinee, it may not accurately reflect the required test length if some innovative items were to be scored polytomously in the operational setting. In general, this study compared ability estimates from polytomous and dichotomous scoring given the same test length and the results did indicate that estimation error of ability parameters in polytomous scoring was smaller or at most the same as that in dichotomous scoring. Future research studies might explore whether a shorter test length would be sufficient to achieve the same measurement accuracy when the innovative items are scored polytomously, given the item pool information and other practical constraints associated with administering the available operational test.</p>
<p>Even though only a small number of innovative items were scored polytomously in this current study, the slight gain in measurement precision was still detectable although there was minor practical impact on classification decision. It is recommended that future research studies explore the impact of using more polytomously scored innovative items. Related to the polytomous scoring algorithms, we recommend future exploration of the use of the automated polytomous scoring algorithm, due to the ease of implementation (assuming that validity is maintained in the process of generating automated scoring rubrics).</p>
<p>If polytomous scoring of the innovative items (compared with dichotomous scoring) results in more accurate estimation of person ability given the same test length, polytomously scoring these items for future operational administrations of the test should be considered. This study serves as a promising first step in the exploration of such future operational scoring by indicating a slight gain in the ability estimation accuracy based on the Rasch partial credit modeling of one type of innovative items in a large-scale CAT of a licensure examination.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-0013164411422903">
<title>Appendix A</title>
<sec id="section13-0013164411422903">
<title>Procedures for Real Data Analyses</title>
<list id="list1-0013164411422903" list-type="order">
<list-item><p>Pretest items were calibrated by anchoring the person ability parameter estimates from previous operational test administrations using WINSTEPS (<xref ref-type="bibr" rid="bibr4-0013164411422903">Linacre, 2005</xref>).</p></list-item>
<list-item><p>Examinee’s operational test responses were scored by anchoring common item parameter estimates obtained from the pretest calibration. All common items including innovative and noninnovative items used on pretest and operational tests served as anchor items in calibrating the operational test and scoring.</p></list-item>
<list-item><p>Classifications decisions were made based on the cut score for the test, after obtaining the ability estimates from the operational test calibration.</p></list-item>
<list-item><p>Steps 1 to 3 were repeated for both dichotomously and polytomously scored item response data.</p></list-item>
<list-item><p>Ability estimates, classification decisions, and information distribution in the item bank were compared between the two scoring methods.</p></list-item>
</list>
<p>Steps 1 to 5 were repeated for the rater-generated polytomous scoring algorithm and the automated polytomous scoring algorithm.</p>
</sec>
</app>
<app id="app2-0013164411422903">
<title>Appendix B</title>
<sec id="section14-0013164411422903">
<title>Procedures for Generating Item Responses With Conditional Missing</title>
<list id="list2-0013164411422903" list-type="order">
<list-item><p>In generating pretest item response data, all item and person parameter estimates from concurrent calibration of polytomously scored innovative items and dichotomously scored noninnovative items in the pretest from the real data analyses were used as the true model parameters to generate polytomous and dichotomous item responses for innovative and noninnovative items, respectively, in simulation studies. Item responses with missing at random patterns were simulated for the pretest using a program written in R by the authors. Every examinee was simulated to answer 25 items for the pretest.</p></list-item>
<list-item><p>The simulated polytomous item responses for innovative items were recoded into dichotomous item responses for the pretest by setting the highest score category as 1 and all other score categories as 0.</p></list-item>
<list-item><p>For operational item response data generation, true item and person parameters were those from the concurrent calibration of polytomously scored innovative items and dichotomously scored noninnovative items in the data analysis of the real operational test. Since in the real data analyses some pretest items (both innovative and noninnovative) were administered in the operational test, this structure was maintained in the simulation studies to guarantee a common scale across the pretest and the operational test via common item anchoring. The number of items for the pretest and the operational test was 1,142 and 1,683, respectively; the number of examinees for the pretest and the operational test was 17,173 and 16,870, respectively.</p></list-item>
<list-item><p>Conditional missing patterns were simulated for mixed format item responses for the operational test using a program written in R by the authors. The conditional missing responses as described in <xref ref-type="bibr" rid="bibr13-0013164411422903">Wolfe and McGill (2006)</xref> in the operational test were simulated based on the information for the bin (with 10 intervals for item difficulty and person ability parameters) and the percentage of missing within each bin. Bins are produced based on the categorization of item difficulty and person ability estimates from the operational tests (see <xref ref-type="table" rid="table3-0013164411422903">Table B1</xref> for the cut values for item difficulty and person ability). The percentage of missing within each bin was based on the real test data (see <xref ref-type="table" rid="table4-0013164411422903">Table B2</xref> for the percentage of examinees answering questions in a bin). The total number of items answered by each examinee in the operational test was simulated to be the same as those observed in the operational tests from the real data analyses. This is consistent with the design of the simulated operational test; the minimum and maximum test lengths were 60 and 205 for the operational test, respectively.</p></list-item>
<list-item><p>After their generation, polytomous item responses were recoded into dichotomous responses by converting the highest score to 1 and all others to 0.</p></list-item>
<list-item><p>In calibrating the simulated pretest, anchored person parameters were the true ability values used to generate item responses. Ability parameters were anchored for calibrating the pretest, as was done in real data analyses.</p></list-item>
<list-item><p>In scoring the simulated operational tests, items in common with the pretests were anchored to score examinees, as was done in real data analyses.</p></list-item>
<list-item><p>Steps 6 and 7 were repeated when innovative items were scored dichotomously and polytomously, respectively.</p></list-item>
<list-item><p>Fifty replications were implemented.</p></list-item>
<list-item><p>Estimated ability parameters from the operational test were converted into pass/fail categories. Classification accuracy and the estimation error (the computation of the estimation errors is illustrated in the “Method” section) of ability parameters were compared between dichotomous and polytomous scoring methods.</p></list-item>
</list>
<table-wrap id="table3-0013164411422903" position="float">
<label>Table B1.</label>
<caption><p>Cut Values for Bins Based on Item and Person Categorization</p></caption>
<graphic alternate-form-of="table3-0013164411422903" xlink:href="10.1177_0013164411422903-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Item</th>
<th align="center">Person</th>
</tr>
</thead>
<tbody>
<tr>
<td>−0.979</td>
<td>−0.835</td>
</tr>
<tr>
<td>−0.808</td>
<td>−0.495</td>
</tr>
<tr>
<td>−0.656</td>
<td>−0.290</td>
</tr>
<tr>
<td>−0.520</td>
<td>−0.082</td>
</tr>
<tr>
<td>−0.375</td>
<td>0.014</td>
</tr>
<tr>
<td>−0.212</td>
<td>0.111</td>
</tr>
<tr>
<td>−0.038</td>
<td>0.253</td>
</tr>
<tr>
<td>0.221</td>
<td>0.419</td>
</tr>
<tr>
<td>0.605</td>
<td>0.622</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table4-0013164411422903" position="float">
<label>Table B2.</label>
<caption><p>Percentage of Examinees Answering Questions by Bin for the Operational Test</p></caption>
<graphic alternate-form-of="table4-0013164411422903" xlink:href="10.1177_0013164411422903-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Theta bins</th>
<th align="center" colspan="10">Item difficulty intervals (%)</th>
</tr>
<tr>
<th/>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>19.2</td>
<td>6.3</td>
<td>1.6</td>
<td>2.7</td>
<td>5.5</td>
<td>15.2</td>
<td>29.9</td>
<td>14.7</td>
<td>3.5</td>
<td>1.4%</td>
</tr>
<tr>
<td>2</td>
<td>19.7</td>
<td>26.2</td>
<td>15.9</td>
<td>16.2</td>
<td>8.6</td>
<td>4.7</td>
<td>3.1</td>
<td>2.8</td>
<td>1.6</td>
<td>1.1%</td>
</tr>
<tr>
<td>3</td>
<td>3.2</td>
<td>3.2</td>
<td>3.9</td>
<td>20.8</td>
<td>36.3</td>
<td>20.0</td>
<td>6.4</td>
<td>3.6</td>
<td>1.7</td>
<td>0.9%</td>
</tr>
<tr>
<td>4</td>
<td>2.3</td>
<td>2.0</td>
<td>1.9</td>
<td>5.8</td>
<td>22.4</td>
<td>39.6</td>
<td>17.8</td>
<td>5.3</td>
<td>1.9</td>
<td>0.9%</td>
</tr>
<tr>
<td>5</td>
<td>2.4</td>
<td>1.8</td>
<td>1.4</td>
<td>3.3</td>
<td>8.9</td>
<td>26.4</td>
<td>30.0</td>
<td>15.8</td>
<td>7.6</td>
<td>2.3%</td>
</tr>
<tr>
<td>6</td>
<td>3.0</td>
<td>1.3</td>
<td>0.8</td>
<td>1.3</td>
<td>2.1</td>
<td>3.6</td>
<td>6.9</td>
<td>30.4</td>
<td>39.8</td>
<td>10.7%</td>
</tr>
<tr>
<td>7</td>
<td>2.7</td>
<td>1.1</td>
<td>0.6</td>
<td>0.9</td>
<td>1.4</td>
<td>2.5</td>
<td>4.4</td>
<td>18.4</td>
<td>51.2</td>
<td>16.9%</td>
</tr>
<tr>
<td>8</td>
<td>2.4</td>
<td>0.8</td>
<td>0.4</td>
<td>0.6</td>
<td>1.0</td>
<td>1.7</td>
<td>2.7</td>
<td>9.1</td>
<td>46.7</td>
<td>34.5%</td>
</tr>
<tr>
<td>9</td>
<td>2.3</td>
<td>0.9</td>
<td>0.5</td>
<td>0.9</td>
<td>1.5</td>
<td>3.3</td>
<td>7.5</td>
<td>9.6</td>
<td>20.1</td>
<td>53.3%</td>
</tr>
<tr>
<td>10</td>
<td>3.6</td>
<td>2.1</td>
<td>1.4</td>
<td>2.4</td>
<td>4.2</td>
<td>8.6</td>
<td>19.0</td>
<td>38.0</td>
<td>15.9</td>
<td>4.9%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Steps 1 to 10 were repeated for rater-generated polytomous scoring and automated polytomous scoring.</p>
</sec>
</app>
</app-group>
<ack>
<p>Thanks are due to the Joint Research Committee for their insightful advice and Dr. Edward Wolfe and Dr. Shu-chuan Kao for getting and cleaning the real data.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The authors would like to thank Pearson VUE and NCSBN for the joint funding support of this research.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0013164411422903">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bennett</surname><given-names>R. E.</given-names></name>
<name><surname>Goodman</surname><given-names>M.</given-names></name>
<name><surname>Hessinger</surname><given-names>J.</given-names></name>
<name><surname>Ligget</surname><given-names>J.</given-names></name>
<name><surname>Marshall</surname><given-names>G.</given-names></name>
<name><surname>Kahn</surname><given-names>H.</given-names></name>
<name><surname>Zack</surname><given-names>J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Using multimedia in large-scale computer-based testing programs</article-title>. <source>Computers in Human Behavior</source>, <volume>15</volume>, <fpage>283</fpage>-<lpage>294</lpage>.</citation>
</ref>
<ref id="bibr2-0013164411422903">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chung</surname><given-names>G.</given-names></name>
<name><surname>Baker</surname><given-names>E.</given-names></name>
</person-group> (<year>1997</year>). <source>Year 1 technology studies: Implications for technology in assessment</source> (<comment>CSE Technical Report No. 459</comment>). <publisher-loc>Los Angeles</publisher-loc>: <publisher-name>Center for the Study of Evaluation, National Center for Research on Evaluation, Standards, and Student Testing, University of California</publisher-name>.</citation>
</ref>
<ref id="bibr3-0013164411422903">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Harmes</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1999</year>). <source>Computer-based testing: Toward the design and use of innovative items</source>. <publisher-loc>Tampa</publisher-loc>: <publisher-name>University of South Florida</publisher-name>.</citation>
</ref>
<ref id="bibr4-0013164411422903">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Linacre</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2005</year>). <source>WINSTEPS: Rasch measurement program</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.winsteps.com">www.winsteps.com</ext-link></comment></citation>
</ref>
<ref id="bibr5-0013164411422903">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Masters</surname><given-names>G. N.</given-names></name>
</person-group> (<year>1982</year>). <article-title>A Rasch model for partial credit scoring</article-title>. <source>Psychometrika</source>, <volume>47</volume>, <fpage>149</fpage>-<lpage>174</lpage>.</citation>
</ref>
<ref id="bibr6-0013164411422903">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Parshall</surname><given-names>C. G.</given-names></name>
<name><surname>Davey</surname><given-names>T.</given-names></name>
<name><surname>Pashley</surname><given-names>P.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Innovative items for computerized testing</article-title>. In <person-group person-group-type="editor">
<name><surname>van der Linden</surname><given-names>W. J.</given-names></name>
<name><surname>Glas</surname><given-names>C. A. W.</given-names></name>
</person-group> (Eds.), <source>Computerized adaptive testing: Theory and practice</source> (pp. <fpage>129</fpage>-<lpage>148</lpage>). <publisher-loc>Norwell, MA</publisher-loc>: <publisher-name>Kluwer Academic</publisher-name>.</citation>
</ref>
<ref id="bibr7-0013164411422903">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parshall</surname><given-names>C. G.</given-names></name>
<name><surname>Harmes</surname><given-names>J. C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The design of innovative item types: Targeting constructs, selecting innovations, and refining prototypes</article-title>. <source>Clear Exam Review</source>, <volume>XIX</volume>(<issue>2</issue>), <fpage>18</fpage>-<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr8-0013164411422903">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Parshall</surname><given-names>C. G.</given-names></name>
<name><surname>Spray</surname><given-names>J.</given-names></name>
<name><surname>Kalohn</surname><given-names>J.</given-names></name>
<name><surname>Davey</surname><given-names>T.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Issues in innovative item types</article-title>. In <source>Practical considerations in computer-based testing</source> (pp. <fpage>70</fpage>-<lpage>91</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr9-0013164411422903">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Pellegrino</surname><given-names>J. W.</given-names></name>
<name><surname>Chudowsky</surname><given-names>N.</given-names></name>
<name><surname>Glaser</surname><given-names>R.</given-names></name>
</person-group> (Eds.). (<year>2001</year>). <source>Knowing what students know: The science and design of educational assessment</source> (<comment>National Research Council Center for Education Report</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academies Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-0013164411422903">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rasch</surname><given-names>G.</given-names></name>
</person-group> (<year>1960</year>). <source>Probabilistic models for some intelligence and attainment tests</source>. <publisher-loc>Copenhagen, Denmark</publisher-loc>: <publisher-name>Danmarks Paedogogische Institut</publisher-name>.</citation>
</ref>
<ref id="bibr11-0013164411422903">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Scalise</surname><given-names>K.</given-names></name>
</person-group> (<year>2007</year>). <source>Using innovative items in computer-based testing: Automated scoring and measurement models</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.lamark.com/downloads/Kathleen_Scalise.pdf">http://www.lamark.com/downloads/Kathleen_Scalise.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr12-0013164411422903">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wendt</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Investigation of the item characteristics of innovative item formats</article-title>. <source>Clear Exam Review</source>, <volume>19</volume>(<issue>1</issue>), <fpage>22</fpage>-<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr13-0013164411422903">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>E. W.</given-names></name>
<name><surname>McGill</surname><given-names>M. T.</given-names></name>
</person-group> (<year>2006</year>). <source>The comparability of item quality indices from sparse data matrices</source>. <comment>Research report submitted to the NCSBN Joint Research Committee</comment>.</citation>
</ref>
</ref-list>
</back>
</article>