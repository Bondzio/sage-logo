<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPM</journal-id>
<journal-id journal-id-type="hwp">spepm</journal-id>
<journal-title>Educational and Psychological Measurement</journal-title>
<issn pub-type="ppub">0013-1644</issn>
<issn pub-type="epub">1552-3888</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0013164412442376</article-id>
<article-id pub-id-type="publisher-id">10.1177_0013164412442376</article-id>
<title-group>
<article-title>Investigating ESL Students’ Performance on Outcomes Assessments in Higher Education</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lakin</surname><given-names>Joni M.</given-names></name>
<xref ref-type="aff" rid="aff1-0013164412442376">1</xref></contrib>
<contrib contrib-type="author">
<name><surname>Elliott</surname><given-names>Diane Cardenas</given-names></name>
<xref ref-type="aff" rid="aff2-0013164412442376">2</xref></contrib>
<contrib contrib-type="author">
<name><surname>Liu</surname><given-names>Ou Lydia</given-names></name>
<xref ref-type="aff" rid="aff2-0013164412442376">2</xref></contrib>
</contrib-group>
<aff id="aff1-0013164412442376"><label>1</label>Auburn University, Auburn, AL, USA</aff>
<aff id="aff2-0013164412442376"><label>2</label>Educational Testing Service, Princeton, NJ, USA</aff>
<author-notes>
<corresp id="corresp1-0013164412442376">Joni Lakin, Department of Educational Foundations, Leadership, and Technology, Auburn University, Auburn, AL 36849, USA Email: <email>joni.lakin@auburn.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>72</volume>
<issue>5</issue>
<fpage>734</fpage>
<lpage>753</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Outcomes assessments are gaining great attention in higher education because of increased demand for accountability. These assessments are widely used by U.S. higher education institutions to measure students’ college-level knowledge and skills, including students who speak English as a second language (ESL). For the past decade, the increasing number of ESL students has changed the landscape of U.S. higher education. However, little research exists documenting how ESL students perform on outcomes assessments. In this study, the authors investigated ESL students’ performance on the Educational Testing Service Proficiency Profile in terms of factor structure, criterion validity, and differential item functioning. The test showed partial measurement invariance between ESL and non-ESL students, consistent criterion validity, and few examples of differential item functioning. The results suggest the critical need for consideration of language background in outcomes assessment research in higher education.</p>
</abstract>
<kwd-group>
<kwd>English learners</kwd>
<kwd>higher education</kwd>
<kwd>outcomes assessment</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The educational quality and outcomes of U.S. higher education have received heightened levels of interest as a result of rising college costs, public dissatisfaction with higher education, and President <xref ref-type="bibr" rid="bibr38-0013164412442376">Obama’s (2010)</xref> recent call to produce 8 million graduates by 2020 (<xref ref-type="bibr" rid="bibr3-0013164412442376">Alexander, 2000</xref>; <xref ref-type="bibr" rid="bibr26-0013164412442376">Kuh &amp; Ikenberry, 2009</xref>). Although the outcomes of interest vary by institutional characteristics and discipline, there appears to be agreement on core common outcomes for undergraduate education. These key outcomes, agreed on by accreditors and educators alike, include knowledge of academic content in science, social studies, mathematics, humanities, and arts as well as practical skills, such as written and oral communication, critical thinking, teamwork, quantitative and information literacy, ethical reasoning, and intercultural knowledge (<xref ref-type="bibr" rid="bibr27-0013164412442376">Leskes, Shoenberg, Gaff, Tritelli, &amp; Nichols, 2004</xref>; <xref ref-type="bibr" rid="bibr31-0013164412442376">Lumina Foundation for Education, 2010</xref>). Although institutions use a variety of mechanisms and tools for assessing outcomes, there has been increasing reliance on standardized assessments to measure students’ general college-level skills.</p>
<p>In evaluating the adequacy of standardized outcomes assessments for this role, special attention should be given to the rising population of students who speak English as a second language (ESL<sup><xref ref-type="fn" rid="fn1-0013164412442376">1</xref></sup>). Over the past half century, rapidly increasing numbers of international and domestic students whose primary language is not English have contributed to the growing number of ESL students in higher education (<xref ref-type="bibr" rid="bibr19-0013164412442376">Institute of International Education, 2010</xref>; <xref ref-type="bibr" rid="bibr37-0013164412442376">National Center for Educational Statistics [NCES], 2008</xref>). In 2008, 3.5% of the total undergraduate population self-reported being ESL students, and nearly 15% did not consider English as their primary language. However, both figures may underestimate the number of nonnative English speakers in higher education because of students who may speak English and another language equally well or who choose (for various reasons) not to identify themselves as nonnative English speakers (<xref ref-type="bibr" rid="bibr5-0013164412442376">Bers, 1994</xref>; <xref ref-type="bibr" rid="bibr18-0013164412442376">ICAS ESL Task Force, 2006</xref>).</p>
<p>Evaluation of the quality of education provided to these students is critical because of their increasing numbers and their risk of dropout in college (<xref ref-type="bibr" rid="bibr12-0013164412442376">Erisman &amp; Looney, 2007</xref>; <xref ref-type="bibr" rid="bibr15-0013164412442376">Gray, Rolph, &amp; Melamid, 1996</xref>; <xref ref-type="bibr" rid="bibr22-0013164412442376">Kanno &amp; Cromley, 2010</xref>). Although a range of studies have considered the validity and fairness of K-12 achievement tests (e.g., <xref ref-type="bibr" rid="bibr2-0013164412442376">Abedi &amp; Lord, 2001</xref>), there has been little work on fairness for higher education outcomes assessments. To address this issue, this study investigates validity issues related to ESL students’ performance on a general outcomes assessment in higher education.</p>
<sec id="section1-0013164412442376">
<title>The Importance and Prevalence of Outcomes Assessments</title>
<p>The movement toward accountability was propelled with the Commission on the Future of Higher Education’s report, <italic>A Test for Leadership: Charting the Future of U.S. Higher Education</italic> (<xref ref-type="bibr" rid="bibr45-0013164412442376">U.S. Department of Education, 2006</xref>). Traditionally, institutions have used peer review, market choice, standardized admissions test scores, graduation rate, student/faculty ratio, and racial and ethnic student body composition as means of demonstrating efficacy (<xref ref-type="bibr" rid="bibr3-0013164412442376">Alexander, 2000</xref>; <xref ref-type="bibr" rid="bibr14-0013164412442376">Gates et al., 2001</xref>; <xref ref-type="bibr" rid="bibr23-0013164412442376">Klein, Kuh, Chun, Hamilton, &amp; Shavelson, 2005</xref>). However, the report urged that improvements to quality would be achievable if higher education institutions embraced and implemented rigorous accountability measures, including assessments. Consequently, final recommendations included the creation of a robust “culture of accountability and transparency throughout higher education” (<xref ref-type="bibr" rid="bibr45-0013164412442376">U.S. Department of Education, 2006</xref>, p. 20).</p>
<p>One response to the accountability call to action was the establishment of the Voluntary System of Accountability (VSA) by the American Association of State Colleges and Universities (AASCU) and the Association of Public and Land-Grant Universities (APLU; formerly NASULGC). VSA was developed, in part, to help postsecondary institutions demonstrate accountability through the measurement of educational outcomes (Pascarella, Seifert, &amp; Blaich, 2008). To date, 64% of AASCU and APLU members, or 334 institutions, have joined the VSA initiative.</p>
<p>To foster institutional evaluation efforts, <xref ref-type="bibr" rid="bibr47-0013164412442376">VSA (2008)</xref> has sanctioned the use of three standardized assessments that measure student knowledge of content areas as well as abilities in critical thinking, analytical reasoning, and written communication. These assessments are the Educational Testing Service (ETS) Proficiency Profile,<sup><xref ref-type="fn" rid="fn2-0013164412442376">2</xref></sup> the Collegiate Assessment of Academic Proficiency, and the Collegiate Learning Assessment. These instruments were specifically chosen because of their ability to reflect both students’ initial ability and the educational value added by the institution (<xref ref-type="bibr" rid="bibr33-0013164412442376">McPherson &amp; Shulenburger, 2006</xref>). However, research on the measurement of educational outcomes using such standardized instruments is ongoing and should continue as student populations and test uses change (<xref ref-type="bibr" rid="bibr29-0013164412442376">Liu, 2011a</xref>; <xref ref-type="bibr" rid="bibr33-0013164412442376">McPherson &amp; Shulenburger, 2006</xref>).</p>
</sec>
<sec id="section2-0013164412442376">
<title>Validity Research on ESL Students</title>
<p>A validity argument is based on converging evidence supporting proposed interpretations, uses, or inferences of test scores (<xref ref-type="bibr" rid="bibr20-0013164412442376">Kane, 2006</xref>, <xref ref-type="bibr" rid="bibr21-0013164412442376">2010</xref>). Validity is not just a property of the test but an interaction of the properties of the test, the purposes for which it is being used, and the characteristics of the examinees (<xref ref-type="bibr" rid="bibr4-0013164412442376">American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education, 1999</xref>). Therefore, validity arguments must be reconstructed for each test, each purpose, and for each group of examinees to whom the test is administered.</p>
<p>Validity and fairness investigations for ESL students are critical because of their increasing numbers in both K-12 and higher education institutions (<xref ref-type="bibr" rid="bibr12-0013164412442376">Erisman &amp; Looney, 2007</xref>; <xref ref-type="bibr" rid="bibr37-0013164412442376">NCES, 2008</xref>; <xref ref-type="bibr" rid="bibr44-0013164412442376">Striplin, 2000</xref>), their particular need for high quality instruction to close achievement gaps (<xref ref-type="bibr" rid="bibr13-0013164412442376">Fry, 2008</xref>), and the unique ways in which their English proficiency interacts with the constructs measured by many content assessments (<xref ref-type="bibr" rid="bibr1-0013164412442376">Abedi, 2002</xref>). The primary concern with assessing ESL students is that their language proficiency acts as a source of construct-irrelevant variance that influences their performance on tests intended to measure other knowledge or skills besides English proficiency (<xref ref-type="bibr" rid="bibr41-0013164412442376">Pitoniak et al., 2009</xref>). One issue that pervades research on test fairness regarding ESL students is the fact that limited English proficiency may affect both true knowledge acquisition and test performance. The confounding influences are difficult to disentangle when only dealing with mean differences. However, the validity of the <italic>uses</italic> of the tests based on correlational data (e.g., prediction of later performance) can still be evaluated for ESL students without needing to precisely pinpoint to origins of mean score differences.</p>
<p>Thus, many validity investigations concerning ESL students in higher education focused on the prediction of college performance by either admission test scores or high school grade point average (HSGPA) or both. For example, <xref ref-type="bibr" rid="bibr51-0013164412442376">Zwick and Sklar (2005)</xref> examined how SAT and HSGPA predict first-year college GPA and college degree attainment for students in different ethnicity and language groups using the High School and Beyond data. They found that the SAT scores showed overprediction<sup><xref ref-type="fn" rid="fn3-0013164412442376">3</xref></sup> for Hispanic students who speak English as their first language, but underprediction for Hispanic students who speak Spanish as their first language. The finding suggests that it is important to consider language background in research on educational attainment. In another study, <xref ref-type="bibr" rid="bibr50-0013164412442376">Zwick and Schlemer (2004)</xref> found significant overprediction of freshman year GPA by high school GPA for language minority students and pronounced overprediction for Latino students. Including SAT scores in the regression significantly improved the accuracy of the prediction. The results also varied among different language groups (e.g., Latino, Asian). Likewise, <xref ref-type="bibr" rid="bibr40-0013164412442376">Patterson, Mattern, and Kobrin’s (2007)</xref> analysis of the SAT highlights the importance of evaluating the validity of tests for ESL students because they found that SAT total scores led to significant underprediction of ESL students’ freshman GPA (.28 GPA points lower). In summary, as <xref ref-type="bibr" rid="bibr49-0013164412442376">Zwick (2007)</xref> pointed out, it becomes critical to examine the impact of language status in educational research, including research at the postsecondary level, when the number of students who are immigrants or children of immigrants increases substantially.</p>
<p>Previous research outlined the most salient sources of validity evidence for the assessment of educational outcomes for ESL students (<xref ref-type="bibr" rid="bibr41-0013164412442376">Pitoniak et al., 2009</xref>; <xref ref-type="bibr" rid="bibr48-0013164412442376">Young, 2009</xref>). These include (a) demonstrating that the test has adequate psychometric qualities including test reliability for ESL students, (b) showing that the factor structure underlying test scores are consistent between ESL and non-ESL students, (c) investigating items for differential functioning, and (d) verifying that the correlation of test scores with related measures is the same across ESL and non-ESL students. These types of evidence should be evaluated relative to the educational decisions being made on the basis of test scores—that is, the reliability, structure, and criterion correlations of the test should be adequate to make consistently accurate and valid inferences for all examinees regardless of their language backgrounds. As reviewed earlier, these aspects of validity for ESL students have been evaluated for higher education admissions tests. However, the published literature on outcomes assessments is sparse, particularly with respect to ESL students. Thus, one aim of this study was to demonstrate the importance of such research.</p>
</sec>
<sec id="section3-0013164412442376">
<title>Focus of the Study</title>
<p>In this study, we aimed to explore a general model for assessing validity of higher education outcomes assessment using the ETS Proficiency Profile as an example. Given the increasing attention that outcomes assessment has received in higher education and the concomitant growth in the number of ESL college students, it is critical to examine the validity and fairness issues of ESL students’ performance on outcomes assessment. We focused our investigations on descriptive statistics, instrument factor structure, differential item functioning (DIF) analysis, and criterion validity. We asked four specific research questions:</p>
<list id="list1-0013164412442376" list-type="order">
<list-item><p>Do the descriptive and psychometric characteristics of the test differ between ESL and non-ESL students across skill areas?</p></list-item>
<list-item><p>Is the factor structure underlying test scores consistent between ESL and non-ESL students?</p></list-item>
<list-item><p>Are any items identified as showing DIF between ESL and non-ESL students?</p></list-item>
<list-item><p>Do Proficiency Profile scores show consistent criterion validity between ESL and non-ESL students?</p></list-item>
</list>
</sec>
<sec id="section4-0013164412442376" sec-type="methods">
<title>Methods</title>
<sec id="section5-0013164412442376">
<title>Instrument</title>
<p>The ETS Proficiency Profile, formerly known as the Measure of Academic Proficiency and Progress (MAPP), measures four skill areas that assess different aspects of academic achievement in higher education: reading, critical thinking, writing, and mathematics. The stated purposes of the Proficiency Profile include the evaluation of program and school effectiveness in promoting achievement in these skill areas (<xref ref-type="bibr" rid="bibr11-0013164412442376">ETS, 2010</xref>). As such, the test should provide fair and valid assessments of students’ achievement in these four areas regardless of the students’ cultural or linguistic background. Previous analyses of the Proficiency Profile (<xref ref-type="bibr" rid="bibr24-0013164412442376">Klein et al., 2009</xref>; <xref ref-type="bibr" rid="bibr32-0013164412442376">Marr, 1995</xref>; <xref ref-type="bibr" rid="bibr28-0013164412442376">Liu, 2008</xref>, <xref ref-type="bibr" rid="bibr30-0013164412442376">2011b</xref>) have provided validity evidence for the use of the Proficiency Profile for measuring higher education outcomes for the general population of students in higher education. In this article, we sought to expand the range of validity evidence to students whose native language is not English.</p>
<p>The Proficiency Profile standard form consists of 27 questions in each of the four skill areas for a total of 108 multiple-choice questions. The full test can be administered in one or two testing sessions. Most of the reading and critical thinking questions are grouped in clusters of two to four items related to a common reading passage, figure, or graph. To represent the domain of college content, the reading and critical thinking questions sample content from the humanities, social sciences, and natural sciences. The reading and critical thinking questions were designed to create a common proficiency scale with critical thinking representing the highest level of reading proficiency (<xref ref-type="bibr" rid="bibr11-0013164412442376">ETS, 2010</xref>; <xref ref-type="bibr" rid="bibr24-0013164412442376">Klein et al., 2009</xref>). As a result of the overlap in reading passages and definitions of the two constructs, these two tests are highly correlated (<italic>r</italic> = .96 at the school level; <xref ref-type="bibr" rid="bibr24-0013164412442376">Klein et al., 2009</xref>). The writing questions measure sentence-level skills, including grammatical agreement, sentence organization, and sentence revision strategies. The mathematics test consists of a range of skills from recognizing mathematics terms to interpreting scientific measurement scales (<xref ref-type="bibr" rid="bibr11-0013164412442376">ETS, 2010</xref>). Scaled scores for total score (range 400-500) and four skill scores (each scaled to range from 100 to 130) are reported.</p>
<p>As part of the Proficiency Profile, students also respond to demographic survey questions. The survey question asking for the student’s “best language” could be answered “English,” “other language,” or “both equal.” For the purposes of this study, students who answered “other language” were classified as ESL, and students who answered “English” were considered non-ESL. The few students answering “both equal” were not considered in the study. Students were also asked about the number of class credits they had accrued. Students could indicate one of five levels: None (freshman), &lt;30 hours, 30 to 60 hours, 61 to 90 hours, &gt;90 hours.</p>
</sec>
<sec id="section6-0013164412442376">
<title>Participants</title>
<p>The 65,651 participants analyzed in this study were students who took the Proficiency Profile at their home institution as part of an accountability program. We excluded 2,850 students who indicated that they were ESL but reported Caucasian ethnicity. Preliminary analyses indicated that these students performed as well or better than the non-ESL students and dissimilarly from ethnic-minority ESL students, who are of primary interest in this study because their language differences are more likely to be a source of concern for test fairness. The participants came from 30 institutions (5 community colleges, 15 public 4-year institutions, and 10 private 4-year institutions) in 20 states. <xref ref-type="table" rid="table1-0013164412442376">Table 1</xref> presents a breakdown of the sample in terms of gender, race/ethnicity, number of credit hours (class standing), and school sector.</p>
<table-wrap id="table1-0013164412442376" position="float">
<label>Table 1.</label>
<caption><p>Demographics of Sample in Percentages</p></caption>
<graphic alternate-form-of="table1-0013164412442376" xlink:href="10.1177_0013164412442376-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Non-ESL (<italic>N</italic> = 63,020)</th>
<th align="center">ESL (<italic>N</italic> = 2,631)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Female</td>
<td>57.7</td>
<td>51.7</td>
</tr>
<tr>
<td>Ethnicity</td>
<td/>
<td/>
</tr>
<tr>
<td> Caucasian</td>
<td>57.3</td>
<td align="center">—</td>
</tr>
<tr>
<td> Black</td>
<td>33.0</td>
<td>54.9</td>
</tr>
<tr>
<td> Asian/Pacific Islander</td>
<td>1.6</td>
<td>17.5</td>
</tr>
<tr>
<td> Latino</td>
<td>3.4</td>
<td>11.9</td>
</tr>
<tr>
<td> Native American</td>
<td>0.6</td>
<td>0.9</td>
</tr>
<tr>
<td> Other</td>
<td>4.1</td>
<td>14.7</td>
</tr>
<tr>
<td>Credit hours</td>
<td/>
<td/>
</tr>
<tr>
<td> None, freshman</td>
<td>22.6</td>
<td>31.8</td>
</tr>
<tr>
<td> &lt;30 hours</td>
<td>4.5</td>
<td>6.5</td>
</tr>
<tr>
<td> 30-60 hours</td>
<td>26.0</td>
<td>19.7</td>
</tr>
<tr>
<td> 61-90 hours</td>
<td>27.9</td>
<td>23.6</td>
</tr>
<tr>
<td> &gt;90 hours</td>
<td>19.0</td>
<td>18.4</td>
</tr>
<tr>
<td>School sector</td>
<td/>
<td/>
</tr>
<tr>
<td> Community college</td>
<td>18.8</td>
<td>6.3</td>
</tr>
<tr>
<td> 4-year public</td>
<td>55.8</td>
<td>57.2</td>
</tr>
<tr>
<td> 4 year private</td>
<td>25.3</td>
<td>36.5</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013164412442376">
<p>Note: ESL = students who speak English as a second language.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section7-0013164412442376">
<title>Analyses</title>
<sec id="section8-0013164412442376">
<title>Descriptive characteristic differences</title>
<p>To understand basic score differences between ESL and non-ESL students, descriptive statistics, including means, <italic>SD</italic>s, and Cronbach’s alpha internal consistency coefficients, were calculated. To support the practice of reporting four skill area scores, correlations between skill areas (reading, writing, critical thinking, and mathematics) were also examined. These correlations should be moderate and indicate that distinct, though related, skills are being measured.</p>
</sec>
<sec id="section9-0013164412442376">
<title>Consistency of factor structure underlying test scores</title>
<p>To investigate whether the internal structure of the test differs for ESL and non-ESL students, a multigroup confirmatory factor analysis (MGCFA) was used to investigate the invariance of the measurement model in terms of factor structure, item loadings on factors, factor variance, or factor covariance. Rather than analyzing a theoretical model with four broad factors, a model with three factors was studied. This decision was made for a number of reasons. First, although critical thinking and reading scores can be reported separately for institutions to have detailed information about each, they are designed to represent a single proficiency continuum with critical thinking skills representing the highest level of reading proficiency (<xref ref-type="bibr" rid="bibr11-0013164412442376">ETS, 2010</xref>). Second, the assessment design was supported by the empirical finding that the corrected correlation between the reading and critical thinking scales were nearly 1.0 (see <xref ref-type="table" rid="table2-0013164412442376">Table 2</xref>). Last, our preliminary exploratory factor analyses revealed that the reading and critical thinking items did not define two distinct factors and that the item loadings of a four-factor model were not consistent with the theoretical model. Thus, a single underlying factor was hypothesized for the reading and critical thinking items in the MGCFA analyses. All 108 dichotomously scored items were included in the analyses. Because the ESL group (<italic>n</italic> = 2,631) and the non-ESL group (<italic>n</italic> = 63,020) differed greatly in size, the MGCFA was conducted using a randomly selected subsample of 2,631 non-ESL students to balance the contributions that each group made to the model fit results.</p>
<table-wrap id="table2-0013164412442376" position="float">
<label>Table 2.</label>
<caption><p>Descriptive Statistics of Proficiency Profile Total Score, Four Skill Areas</p></caption>
<graphic alternate-form-of="table2-0013164412442376" xlink:href="10.1177_0013164412442376-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">Non-ESL (<italic>N</italic> = 63,020)<hr/></th>
<th align="center" colspan="3">ESL (<italic>N</italic> = 2,631)<hr/></th>
<th/>
<th align="center" rowspan="2">Cohen’s <italic>d</italic> effect sizes<sup><xref ref-type="table-fn" rid="table-fn3-0013164412442376">a</xref></sup></th>
</tr>
<tr>
<th/>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center">α</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center">α</th>
<th align="center"><italic>t</italic> test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Total score</td>
<td>439.1</td>
<td>20.0</td>
<td>.94</td>
<td>430.8</td>
<td>17.1</td>
<td>.92</td>
<td>20.9<xref ref-type="table-fn" rid="table-fn4-0013164412442376">**</xref></td>
<td>0.44</td>
</tr>
<tr>
<td>Reading</td>
<td>116.3</td>
<td>7.1</td>
<td>.83</td>
<td>113.0</td>
<td>6.7</td>
<td>.79</td>
<td>22.9<xref ref-type="table-fn" rid="table-fn4-0013164412442376">**</xref></td>
<td>0.47</td>
</tr>
<tr>
<td>Critical thinking</td>
<td>110.9</td>
<td>6.4</td>
<td>.79</td>
<td>108.2</td>
<td>5.7</td>
<td>.75</td>
<td>20.7<xref ref-type="table-fn" rid="table-fn4-0013164412442376">**</xref></td>
<td>0.44</td>
</tr>
<tr>
<td>Writing</td>
<td>113.4</td>
<td>5.0</td>
<td>.78</td>
<td>111.2</td>
<td>5.0</td>
<td>.76</td>
<td>22.2<xref ref-type="table-fn" rid="table-fn4-0013164412442376">**</xref></td>
<td>0.44</td>
</tr>
<tr>
<td>Math</td>
<td>111.8</td>
<td>6.4</td>
<td>.84</td>
<td>110.7</td>
<td>6.3</td>
<td>.82</td>
<td>9.1<xref ref-type="table-fn" rid="table-fn4-0013164412442376">**</xref></td>
<td>0.18</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013164412442376">
<p>Note: ESL = students who speak English as a second language.</p>
</fn>
<fn id="table-fn3-0013164412442376">
<label>a.</label>
<p>Positive values indicate that non-ESL group has higher mean.</p>
</fn>
<fn id="table-fn4-0013164412442376">
<label>**</label>
<p><italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>In the MGCFA procedure, we used an iterative multigroup comparison of measurement models to compare models that were increasingly constrained across the groups (<xref ref-type="bibr" rid="bibr6-0013164412442376">Bollen, 1989</xref>; <xref ref-type="bibr" rid="bibr7-0013164412442376">Brown, 2006</xref>). The procedure consisted of the following steps: (1) Fit a common model in each group (ESL and non-ESL) separately; (2) fit a common model to both groups simultaneously with all parameters freely estimated; (3) constrain factor loadings of items on factors; (4) constrain item intercepts (thresholds for categorical variables); (5) constrain factor variances; (6) constrain factor covariances; and (7) constrain latent means.</p>
<p>The analyses were conducted using MPlus 6 (<xref ref-type="bibr" rid="bibr36-0013164412442376">Muthén &amp; Muthén, 1998-2009</xref>) and its WLSMV (mean- and variance-adjusted weighted least square) estimator option for categorical indicators, which allows for the use of dichotomous item variables as indicators. To have an identified model, we fixed factor means to zero in all groups and fixed scale factors (equivalent to item error variances for categorical variables) to 1 for all groups (<xref ref-type="bibr" rid="bibr36-0013164412442376">Muthén &amp; Muthén, 1998-2009</xref>). Because of these constraints, Step 7 entailed releasing the constraints on the latent means rather than constraining.</p>
<p>The fit of individual models were assessed using the comparative fit index (CFI) and root mean square error of approximation (RMSEA). For the CFI, a value greater than .90 is acceptable and a value above .95 is good. For RMSEA, estimates less than .05 are optimal (<xref ref-type="bibr" rid="bibr25-0013164412442376">Kline, 2004</xref>).<sup><xref ref-type="fn" rid="fn4-0013164412442376">4</xref></sup> Improvements in fit for nested models were tested using a change in χ<sup>2</sup> test (using the DIFFTEST option for WLSMV estimators; <xref ref-type="bibr" rid="bibr36-0013164412442376">Muthén &amp; Muthén, 1998-2009</xref>).</p>
</sec>
<sec id="section10-0013164412442376">
<title>Differential item and bundle functioning</title>
<p>When considering the assessment of a group of examinees who form a minority in the testing population, DIF is a crucial tool for helping test developers identify items that behave differently when administered to different groups of examinees. To explore the existence of DIF in the Proficiency Profile, we used the SIBTEST procedure to detect DIF for each factor domain informed by the above factor analysis (<xref ref-type="bibr" rid="bibr8-0013164412442376">Clauser &amp; Mazor, 1998</xref>; <xref ref-type="bibr" rid="bibr43-0013164412442376">Shealy &amp; Stout, 1993</xref>). SIBTEST detects DIF by assessing individual items for multidimensionality when the cluster of items used for matching is assumed to be unidimensional. Thus, separate SIBTEST analyses were conducted for math, writing, and reading/critical thinking items.</p>
<p>SIBTEST has an additional benefit in that it can detect differential bundle functioning (DBF). DBF (also known as DIF amplification) is observed when clusters of items behave differently for two groups of students (the items in the bundle may or may not individually show DIF; <xref ref-type="bibr" rid="bibr10-0013164412442376">Douglas, Roussos, &amp; Stout, 1996</xref>). Because the Reading and Critical Thinking subtests consist of clusters of items associated with a common reading passage, we used SIBTEST to evaluate those combined subtests for DBF.</p>
<p><xref ref-type="bibr" rid="bibr42-0013164412442376">Roussos and Stout (1996)</xref> defined cutoff points for increasingly problematic levels of DIF based on the SIBTEST beta metric: Negligible or A-level DIF is defined as beta values below 0.059; moderate or B-level DIF is defined as beta values between 0.060 and 0.088; and large or C-level DIF is defined as beta values in excess of 0.088. As with the MGCFA, the same random sample of non-ESL students was used to create samples of equal size for consistency across analyses.</p>
</sec>
<sec id="section11-0013164412442376">
<title>Relationship between Proficiency Profile scores and credit hours</title>
<p>For a test of academic outcomes (rather than academic inputs), significant relationships should be found between test performance and students’ exposure to college coursework. That is, the skills measured by the test are expected to improve accumulatively from taking a combination of courses belonging to different disciplinary subjects. Thus, regression analyses were used to investigate the relationship between students’ Proficiency Profile scores and number of credit hours taken. Because this question addresses a practical issue for users of such tests, namely, whether they can expect growth with additional credit hours for ESL and non-ESL students on each reported scale, the four battery-level scores rather than three latent-factor scores were used in these analyses. Separate regression analyses were conducted for the four skill scores. Credit hours, ESL status, and an interaction of the two variables were added to the predictive model in successive steps to determine whether Proficiency Profiles varied by credit hours and whether the trend varied by ESL status.</p>
</sec>
</sec>
</sec>
<sec id="section12-0013164412442376" sec-type="results">
<title>Results</title>
<p>Our first research question addressed whether ESL and non-ESL students show mean differences across skill areas. <xref ref-type="table" rid="table2-0013164412442376">Table 2</xref> presents descriptive and psychometric statistics, including means, <italic>SD</italic>s, and Cronbach’s alpha internal consistency for ESL and non-ESL students. Internal consistency indices were similarly strong for ESL and non-ESL students on the total score and in all four skill areas. Mean effect sizes and Student’s <italic>t</italic>-test results reveal that there were significant differences between ESL and non-ESL students in the total Proficiency Profile score and four skill areas. Differences were smallest for math, where only a small effect size was found (less than .2; <xref ref-type="bibr" rid="bibr9-0013164412442376">Cohen, 1988</xref>). For the other domains, differences were medium in size.</p>
<sec id="section13-0013164412442376">
<title>Subtest Correlations</title>
<p>To support the practice of reporting four skill scores, correlations between skill areas (reading, writing, critical thinking, and mathematics) should be moderate and indicate that distinct, though related, skills are being measured (see <xref ref-type="table" rid="table3-0013164412442376">Table 3</xref>). The correlations are quite strong between skill areas for ESL and non-ESL students. Because of the large sample sizes, all of the differences in corrected correlations were significant between ESL and non-ESL students. However, most differences in correlations were small. One interesting finding was that the correlations between math and the other three skill areas were noticeably lower for ESL compared with non-ESL students (even considering that all of the correlations were somewhat lower for ESL students). Thus, it appears that for ESL students, there is less overlap of math skills with other skills than for non-ESL students.</p>
<table-wrap id="table3-0013164412442376" position="float">
<label>Table 3.</label>
<caption><p>Correlations Between Skill Areas</p></caption>
<graphic alternate-form-of="table3-0013164412442376" xlink:href="10.1177_0013164412442376-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="4">Uncorrected<hr/></th>
<th align="center" colspan="4">Corrected<sup><xref ref-type="table-fn" rid="table-fn6-0013164412442376">a</xref></sup><hr/></th>
</tr>
<tr>
<th/>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. Reading</td>
<td/>
<td>0.74</td>
<td>0.64</td>
<td>0.80</td>
<td/>
<td>0.92</td>
<td>0.76</td>
<td>0.99</td>
</tr>
<tr>
<td>2. Writing</td>
<td>0.69</td>
<td/>
<td>0.61</td>
<td>0.69</td>
<td>0.89</td>
<td/>
<td>0.75</td>
<td>0.88</td>
</tr>
<tr>
<td>3. Math</td>
<td>0.46</td>
<td>0.51</td>
<td/>
<td>0.64</td>
<td>0.56</td>
<td>0.64</td>
<td/>
<td>0.79</td>
</tr>
<tr>
<td>4. Critical thinking</td>
<td>0.74</td>
<td>0.63</td>
<td>0.48</td>
<td/>
<td>0.96</td>
<td>0.84</td>
<td>0.61</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0013164412442376">
<p>Note: ESL = students who speak English as a second language. Non-ESL above the diagonal; ESL below the diagonal. All Fisher’s <italic>z</italic>-transformation tests of correlations were significant, <italic>p</italic> &lt; .001 (<xref ref-type="bibr" rid="bibr16-0013164412442376">Hays, 1994</xref>).</p>
</fn>
<fn id="table-fn6-0013164412442376">
<label>a.</label>
<p>Correlations disattenuated for unreliability (<xref ref-type="bibr" rid="bibr16-0013164412442376">Hays, 1994</xref>).</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section14-0013164412442376">
<title>Consistency of Factor Structure Underlying Test Scores</title>
<p>To better understand the underlying measurement model of the Proficiency Profile for ESL and non-ESL students, an MGCFA was conducted. To begin the measurement invariance procedure, the theoretical model with three factors (including a combined reading and critical thinking factor) was applied to the ESL and non-ESL samples in separate analyses. The model fit indices were acceptable for both groups (see <xref ref-type="table" rid="table4-0013164412442376">Table 4</xref>). The RMSEA estimate was well below the recommended .05 threshold, and CFI estimates indicated that model fit was acceptable (recommended threshold of .90). In Step 2, we fit a common model to both groups with all parameters freely estimated, which was used for baseline comparison with subsequent models. Model fit indices were acceptable with a CFI of .94 and RMSEA estimate of .019.</p>
<table-wrap id="table4-0013164412442376" position="float">
<label>Table 4.</label>
<caption><p>Fit of Increasingly Constrained Models for the ESL-Minority and Non-ESL Sample</p></caption>
<graphic alternate-form-of="table4-0013164412442376" xlink:href="10.1177_0013164412442376-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="3">χ<sup>2</sup> test of model fit<hr/></th>
<th align="center" colspan="2">χ<sup>2</sup> contributions by group<hr/></th>
<th align="center" colspan="3">χ<sup>2</sup> difference test<sup><xref ref-type="table-fn" rid="table-fn8-0013164412442376">a</xref></sup><hr/></th>
<th/>
<th align="center" rowspan="2">No. of free parameters</th>
<th/>
</tr>
<tr>
<th/>
<th/>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center"><italic>p</italic></th>
<th align="center">Non-ESL</th>
<th align="center">ESL</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center"><italic>p</italic></th>
<th align="center">CFI</th>
<th align="center">RMSEA<sup><xref ref-type="table-fn" rid="table-fn9-0013164412442376">b</xref></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>Step 1</td>
<td>Non-ESL</td>
<td>9,046</td>
<td>5,667</td>
<td>.00</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>.962</td>
<td>219</td>
<td>.016</td>
</tr>
<tr>
<td/>
<td>ESL</td>
<td>11,227</td>
<td>5,667</td>
<td>.00</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>.906</td>
<td>219</td>
<td>.021</td>
</tr>
<tr>
<td>Step 2</td>
<td>Freely fitting model</td>
<td>20,331</td>
<td>11,334</td>
<td>.00</td>
<td>8,381</td>
<td>11,950</td>
<td/>
<td/>
<td/>
<td>.939</td>
<td>438</td>
<td>.019</td>
</tr>
<tr>
<td>Step 3</td>
<td>Constrain factor loading</td>
<td>18,905</td>
<td>11,439</td>
<td>.00</td>
<td>7,772</td>
<td>11,133</td>
<td>139</td>
<td>105</td>
<td>.02</td>
<td>.949</td>
<td>333</td>
<td>.017</td>
</tr>
<tr>
<td>Step 4</td>
<td>Constrain item intercepts</td>
<td>21,402</td>
<td>11,547</td>
<td>.00</td>
<td>9,139</td>
<td>12,263</td>
<td>3,072</td>
<td>108</td>
<td>.00</td>
<td>.933</td>
<td>225</td>
<td>.019</td>
</tr>
<tr>
<td>Step 5</td>
<td>Constrain factor variance</td>
<td>21,758</td>
<td>11,550</td>
<td>.00</td>
<td>9,378</td>
<td>12,380</td>
<td>379</td>
<td>111</td>
<td>.00</td>
<td>.931</td>
<td>222</td>
<td>.020</td>
</tr>
<tr>
<td>Step 5b</td>
<td>Constrain factor variance<sup><xref ref-type="table-fn" rid="table-fn10-0013164412442376">c</xref></sup></td>
<td>21,375</td>
<td>11,549</td>
<td>.00</td>
<td>9,142</td>
<td>12,234</td>
<td>11</td>
<td>2</td>
<td>.00</td>
<td>.933</td>
<td>223</td>
<td>.019</td>
</tr>
<tr>
<td>Step 7</td>
<td>Free factor means</td>
<td>19,148</td>
<td>11,546</td>
<td>.00</td>
<td>7,917</td>
<td>11,230</td>
<td>N/A<sup><xref ref-type="table-fn" rid="table-fn9-0013164412442376">d</xref></sup></td>
<td/>
<td/>
<td>.948</td>
<td>226</td>
<td>.017</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-0013164412442376">
<p>Note: ESL = students who speak English as a second language; CFI = comparative fit index; RMSEA = root mean square error of approximation; N/A = not applicable. <italic>N</italic> = 2,631 for both ESL and subsample of non-ESL.</p>
</fn>
<fn id="table-fn8-0013164412442376">
<label>a.</label>
<p>For the WLSMV (mean- and variance-adjusted weighted least square) estimator, the χ<sup>2</sup> value of model fit cannot be used directly in the χ<sup>2</sup> difference test for nested models. Instead, the DIFFTEST option in MPlus was used to calculate appropriate <italic>df</italic>s for this test (<xref ref-type="bibr" rid="bibr36-0013164412442376">Muthén &amp; Muthén, 1998-2009</xref>). All χ<sup>2</sup> differences represent a decrease in model fit.</p>
</fn>
<fn id="table-fn9-0013164412442376">
<label>b.</label>
<p>RMSEA confidence intervals cannot be calculated for the WLSMV estimator.</p>
</fn>
<fn id="table-fn10-0013164412442376">
<label>c.</label>
<p>Factor variance for critical thinking/reading freed.</p>
</fn>
<fn id="table-fn11-0013164412442376">
<label>d.</label>
<p>Freeing parameters yielded a model that was not nested within the previous model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The constraints on factor loadings (Step 3) led to minimal decreases in model fit according to the χ<sup>2</sup> difference test.<sup><xref ref-type="fn" rid="fn5-0013164412442376">5</xref></sup> However, Step 4, constraining item intercepts, led to a noticeable decline in model fit, Δχ<sup>2</sup>108 = 3,072, <italic>p</italic> &lt; .001. An inspection of modification indices localized the issue to the factor means, particularly for critical thinking/reading (Δχ<sup>2</sup> = 1,408) but also for writing (Δχ<sup>2</sup> = 632) and math (Δχ<sup>2</sup> = 253). The finding that constraining item intercepts led to factor level misfit indicates that overall differences at the factor level likely influences most of the test items in terms of apparent item difficulty. The strain at the factor mean parameters persisted to Step 7, where they were freed, which led to appreciable improvements in fit.</p>
<p>In Step 5, where factor variances were constrained, the model fit decreased modestly. The source of model strain was clear as releasing the variance constraint for Critical Thinking/Reading would significantly increase model fit. By inspecting the unconstrained factor variances, we found that the ESL group was considerably <italic>less</italic> variable than the non-ESL group on this factor. In <xref ref-type="table" rid="table4-0013164412442376">Table 4</xref>, Step 5b shows the modest increase in overall model fit when the constraint on Critical Thinking/Reading variance was released. Constraining factor covariance is predicated on invariant factor variances, so Step 6 of the MGCFA procedure was skipped and the covariances of the factors are assumed to be variant because of differences in factor variances.</p>
<p>Finally, releasing the factor means in Step 7 led to the largest improvement in model fit, indicating that the two groups differed significantly on latent factor means. Compared with the non-ESL sample, whose latent factor means were fixed to zero, ESL students scored −0.24 <italic>SD</italic> lower on Critical Thinking/Reading, −0.19 <italic>SD</italic> lower in Mathematics, and −0.25 <italic>SD</italic> lower in Writing.<sup><xref ref-type="fn" rid="fn6-0013164412442376">6</xref></sup> The final model, with the Critical Thinking/Reading variance and factor mean constraints freed, had good overall fit.</p>
</sec>
<sec id="section15-0013164412442376">
<title>Items Showing Differential Item Functioning</title>
<p>SIBTEST was used to investigate items for DIF for each of the three factors (Writing, Math, and Reading/Critical Thinking) and DBF for Reading/Critical Thinking. The results indicated that one writing item favored the ESL group with B-level DIF (β = −.08), whereas three math items were found to favor non-ESL students with B-level DIF (βs = .06-.07). Because the four items flagged showed only moderate DIF, it is unlikely that the items are truly problematic or have significant impacts on student performance at the test level. In most operational settings, only items with C-level, large DIF are considered to be problematic and are either removed from the item bank or further studied.</p>
<p>We were able to inspect the mathematics subtest and found that the Proficiency Profile used math items that varied in the reading load required. Some items were situated in simple contexts and required relatively low reading levels, whereas other items had more complex contexts where the mathematical problem had to be extracted from the context and resulted in higher reading demands. The three math items we identified as showing moderate DIF seemed to have more complex contexts, which would explain their differential difficulty for ESL students.</p>
<p>Although the differential difficulty may be traced to linguistic demands, the complex language of the items may be considered relevant to the construct of interest. The user’s guide for the Proficiency Profile (<xref ref-type="bibr" rid="bibr11-0013164412442376">ETS, 2010</xref>) defines one of the proficiency levels in mathematics as including the ability to “solve arithmetic problems with some complications, including complex wording” (p. 10). If this is a skill valued by Proficiency Profile score users, then these differences at the item level may accurately reflect differences in the reading skills of the ESL students, which impede their ability to solve math problems with complex contexts. In fact, to solve mathematical problems in real life, one also needs to understand the complex contexts where the math problems are situated.</p>
<p>SIBTEST was also used to look for differential function of item bundles created by common reading passages. Each reading passage was associated with 2 to 4 reading and/or critical thinking test questions. For each bundle of items inspected, the matching subtest was composed of all other critical thinking and reading items (50-52 items). The results indicated that moderate DBF was present for three bundles, but the bundles varied in whether they favored ESL or non-ESL students. Thus, there did not appear to be strong DBF effects by reading passage.</p>
</sec>
<sec id="section16-0013164412442376">
<title>Relationship Between Proficiency Profile Scores and Credit Hours</title>
<p>Student credit hours were used to predict Proficiency Profile scores in a test of criterion validity. Linear regressions with credit hours and language group as independent variables predicting the skill scores indicated that, for all four skill areas, there was a significant main effect of credit hours (see <xref ref-type="table" rid="table5-0013164412442376">Table 5</xref>). The ESL effects and the interaction terms in the regression model were significant and negative, indicating that the ESL students’ trajectories across credit hours differed from non-ESL students. However, the coefficients associated with those effects were quite small and did not add appreciably to the variance accounted for.</p>
<table-wrap id="table5-0013164412442376" position="float">
<label>Table 5.</label>
<caption><p>Regression Results for Credit Hours and ESL Status</p></caption>
<graphic alternate-form-of="table5-0013164412442376" xlink:href="10.1177_0013164412442376-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">Reading<hr/></th>
<th align="center" colspan="3">Writing<hr/></th>
<th align="center" colspan="3">Math<hr/></th>
<th align="center" colspan="3">Critical thinking<hr/></th>
</tr>
<tr>
<th/>
<th align="center"><italic>R</italic></th>
<th align="center">Δ<italic>R</italic><sup>2</sup></th>
<th align="center">β</th>
<th align="center"><italic>R</italic></th>
<th align="center">Δ<italic>R</italic><sup>2</sup></th>
<th align="center">β</th>
<th align="center"><italic>R</italic></th>
<th align="center">Δ<italic>R</italic><sup>2</sup></th>
<th align="center">β</th>
<th align="center"><italic>R</italic></th>
<th align="center">Δ<italic>R</italic><sup>2</sup></th>
<th align="center">β</th>
</tr>
</thead>
<tbody>
<tr>
<td>Credit hours</td>
<td>.38</td>
<td>.15</td>
<td>.39</td>
<td>.31</td>
<td>.10</td>
<td>.31</td>
<td>.26</td>
<td>.07</td>
<td>.27</td>
<td>.35</td>
<td>.12</td>
<td>.36</td>
</tr>
<tr>
<td>ESL</td>
<td>.39</td>
<td>.01</td>
<td>−.01</td>
<td>.32</td>
<td>.01</td>
<td>−.02</td>
<td>.27</td>
<td>.00</td>
<td>.00</td>
<td>.36</td>
<td>.01</td>
<td>−.01</td>
</tr>
<tr>
<td>Hours × ESL</td>
<td>.39</td>
<td>.00</td>
<td>−.08</td>
<td>.32</td>
<td>.00</td>
<td>−.06</td>
<td>.27</td>
<td>.00</td>
<td>−.03</td>
<td>.36</td>
<td>.00</td>
<td>−.07</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn12-0013164412442376">
<p>Note: All regression coefficients were significant to <italic>p</italic> &lt; .001. β is the standardized regression coefficient in the final model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Inspection of the score means by credit hours confirmed that there was a positive trend of test scores across credit hours. Across skill areas, gains from freshman to senior cohorts ranged from 4.0 to 7.2 points (<italic>d</italic> = 0.83-1.21) for non-ESL students and from 2.5 to 4.0 points (<italic>d</italic> = 0.52-0.73) for ESL students. These results suggest that ESL students showed smaller gains in test scores across credit hours than non-ESL students across all four skill areas. <xref ref-type="fig" rid="fig1-0013164412442376">Figure 1</xref> illustrates the relationship between number of credit hours and Proficiency Profile scores for both ESL and non-ESL students. In contrast to the regression results, which indicated small ESL and interaction effects, it appeared that several of the significant differences might have practical importance. For example, the greatest difference was found for students with 30 to 60 credit hours, where the difference between ESL and non-ESL students in critical thinking skills was 3.6 scale points (<italic>d</italic> = −0.57). Reading scores showed the greatest differences in growth across credit hours, with ESL students showing considerably smaller learning gains than non-ESL students. Math showed the most consistent growth between the two groups of students.</p>
<fig id="fig1-0013164412442376" position="float">
<label>Figure 1.</label>
<caption><p>Relationship between number of credit hours and Proficiency Profile scores for ESL (English as a second language) and non-ESL students</p>
<p>Note: Significant mean differences within credit hours between ESL and non-ESL groups are starred.</p></caption>
<graphic xlink:href="10.1177_0013164412442376-fig1.tif"/>
</fig>
</sec>
</sec>
<sec id="section17-0013164412442376" sec-type="discussion">
<title>Discussion</title>
<p>In this study, we investigated ESL students’ performance on the ETS Proficiency Profile with regard to factor structure, differential item and bundle functioning criterion and validity. Overall, the test showed similar descriptive and psychometric qualities for ESL and non-ESL students. In looking at subtest correlations, we found that although correlations between skill areas were consistently lower for ESL students, the drop in the correlations between the math subtest and the language-focused subtests was more pronounced.</p>
<p>In the multigroup CFA analysis, partial measurement invariance was found when comparing the ESL and non-ESL students. We found that the test items behaved similarly across ESL and non-ESL students. However, the model required freeing the factor-level Critical Thinking/Reading parameter to allow for the ESL sample to have smaller variance (which precluded constraining factor covariances) and freeing factor means for the two groups. Lower means for ESL students on all three factors contributed to model misfit, though the strain was greatest for Critical Thinking/Reading followed by Writing. This finding was consistent with the observed scores, which indicated moderate mean differences (around .45) between ESL and non-ESL students on those three skill scores.</p>
<p>Invariant factor loadings support the conclusion of <italic>metric invariance</italic> by <xref ref-type="bibr" rid="bibr17-0013164412442376">Horn and McArdle’s (1992)</xref> definition and <italic>measurement unit equivalence</italic> by <xref ref-type="bibr" rid="bibr46-0013164412442376">van de Vijver and Poortinga’s (2005)</xref> definition. However, because of the differences in factor variance identified, <italic>full score equivalence</italic> (<xref ref-type="bibr" rid="bibr46-0013164412442376">van de Vijver &amp; Poortinga, 2005</xref>) was not met. Metric invariance supports the use of test scores <italic>within</italic> examinee groups (such as comparing gains over time) but indicates that comparisons across groups should be made with caution.</p>
<p>It was unclear whether differences in observed and latent means and latent factor variances should be attributed to the effects of construct-irrelevant language proficiency or true differences in academic skills for ESL and non-ESL students. Additional research that can disentangle the effect of language proficiency from the effect of opportunity to learn is needed to understand the reasons for the differences that were found.</p>
<p>In the DIF and DBF analyses, three items and two item bundles with moderate DIF favoring non-ESL students were identified. That level of DIF is usually not considered problematic in operational test development. Overall, the results indicate that the behavior of items was quite consistent across ESL and non-ESL students even at the bundle level.</p>
<p>An analysis of the relationship between Proficiency Profile scores and credit hours indicated that, in general, students who had completed more course credits had higher Proficiency Profile scores, which provides criterion validity evidence for the Proficiency Profile. ESL and non-ESL students showed similar criterion validity. Math showed the most overlap in growth trends for ESL and non-ESL groups, whereas reading showed somewhat smaller differences for ESL students with increasing credit hours. This finding warrants further investigation to determine whether ESL students have sufficient opportunity and support to develop their reading skills in college.</p>
<sec id="section18-0013164412442376">
<title>Implications</title>
<p>As increasing numbers of immigrants alter the demographic characteristics of the U.S. college population, it becomes important to study how language background interacts with college performance. Sensitivity to the unique needs and challenges faced by the ESL population is necessary for institutions of higher education, especially given recent political pressure to produce more graduates ready to meet the challenges of the 21st century economy. Such pressure has prompted the development of tools to help higher education institutions ensure that graduates at all levels have mastered the skills and knowledge necessary for demanding and complex job responsibilities. One such tool gaining prominence in higher education circles is the Degree Profile framework developed by the <xref ref-type="bibr" rid="bibr31-0013164412442376">Lumina Foundation for Education (2010)</xref>.</p>
<p>The Degree Profile benchmarks specific learning outcomes for associate, bachelor’s, and master’s degrees through the provision of clear reference points that demonstrate the incremental learning that should accompany advanced degrees. Most relevant to our study are Intellectual Skills, which emphasize competencies that transcend disciplinary boundaries, including quantitative fluency (e.g., the ability to present and interpret mathematical computations), use of information resources (e.g., the ability to synthesize and evaluate information from various sources), and communication fluency (e.g., abilities related to grammatical competency and prose). Such skills and knowledge are closely aligned with the constructs measured by many standardized learning outcomes assessments, including the Proficiency Profile. Thus, as the Degree Profile framework is implemented in higher education, learning outcomes assessments may find continued or increasing use. Thus, the validity and fairness of these tests for intended purposes should be examined for all major student subgroups.</p>
<p>Additional research is needed to better understand the academic development of ESL students in higher education. Future validity studies should consider using a more nuanced determination of ESL status rather than the self-reported “best language” used in this study, which results in considerable heterogeneity of the ESL group. Using measures that better represent the continuum of student language ability could provide more detailed results and clear implications of level of English proficiency on test performance. Similarly, the present study relied on a sample of students recruited by participating colleges and universities for their accountability programs. Although institutions are encouraged to recruit a representative sample, they often rely on a convenience sample, which may be influenced by volunteer and drop-out effects. Accordingly, future validity studies could collect data that more accurately reflect the larger student population by using random sampling.</p>
<p>Overall, this study made clear the importance of exploring validity and fairness for ESL students taking outcomes assessments in higher education. The approach adopted in this study also applies to other outcomes assessment involving the ESL population. As the landscape of higher education continues to evolve, such work will play an important role in promoting the valid use of assessments for evaluating the learning of U.S. college students.</p>
</sec>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="other">
<label>Authors’ Note</label>
<p>Any opinions expressed in this article are those of the authors and not necessarily of Educational Testing Service. Joni Lakin was a postdoctoral fellow at Educational Testing Service at the time this work was completed.</p>
</fn>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared a potential conflict of interest (e.g. a financial relationship with the commercial organizations or products discussed in this article) as follows: The authors are employees or former employees of Educational Testing Service.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0013164412442376">
<label>1.</label>
<p>In the K-12 research literature, the preferred terms for students who are acquiring English are English-language learners (ELL) or English learners (EL). In the higher education literature, English as a second language (ESL) is widely used.</p>
</fn>
<fn fn-type="other" id="fn2-0013164412442376">
<label>2.</label>
<p>VSA refers to the Proficiency Profile by its previous acronym, MAPP.</p>
</fn>
<fn fn-type="other" id="fn3-0013164412442376">
<label>3.</label>
<p>Over- and underprediction was defined in these studies by using a common regression model for all students. Overprediction was the observation of negative regression residuals for a group of students (meaning the model predicted higher scores than students received on average). Underprediction reflects positive residuals.</p>
</fn>
<fn fn-type="other" id="fn4-0013164412442376">
<label>4.</label>
<p>Note that for the WLSMV estimator used for these data, RMSEA confidence intervals have not yet been developed for MPlus (<xref ref-type="bibr" rid="bibr35-0013164412442376">Muthén, 2009b</xref>).</p>
</fn>
<fn fn-type="other" id="fn5-0013164412442376">
<label>5.</label>
<p>The other model fit indices seem to indicate that the constrained model in Step 3 fits better than Step 2. In general, constraining parameters would be expected to decrease fit. However, fit can appear to improve because of differences in the way the indices are calculated. <xref ref-type="bibr" rid="bibr34-0013164412442376">Muthén (2009a)</xref> recommended relying solely on the χ<sup>2</sup> difference test for comparing the fit of nested models for the WLSMV estimator.</p>
</fn>
<fn fn-type="other" id="fn6-0013164412442376">
<label>6.</label>
<p>Factor <italic>SD</italic>s were fixed to unit loading on one item. Item scales (error variances) were fixed to 1, so factor <italic>SD</italic>s were also 1.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abedi</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Standardized achievement tests and English language learners: Psychometrics issues</article-title>. <source>Educational Assessment</source>, <volume>8</volume>, <fpage>231</fpage>-<lpage>257</lpage>.</citation>
</ref>
<ref id="bibr2-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abedi</surname><given-names>J.</given-names></name>
<name><surname>Lord</surname><given-names>C.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The language factor in mathematics tests</article-title>. <source>Applied Measurement in Education</source>, <volume>14</volume>, <fpage>219</fpage>-<lpage>234</lpage>.</citation>
</ref>
<ref id="bibr3-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Alexander</surname><given-names>FK</given-names></name>
</person-group>. (<year>2000</year>). <article-title>The changing face of accountability</article-title>. <source>Journal of Higher Education</source>, <volume>71</volume>, <fpage>411</fpage>-<lpage>431</lpage>.</citation>
</ref>
<ref id="bibr4-0013164412442376">
<citation citation-type="book">
<collab>American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education</collab>. (<year>1999</year>). <source>Standards for educational and psychological testing</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr5-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bers</surname><given-names>T.</given-names></name>
</person-group> (<year>1994</year>). <article-title>English proficiency, course patterns, and academic achievements of limited-English-proficient community college students</article-title>. <source>Research in Higher Education</source>, <volume>35</volume>, <fpage>209</fpage>-<lpage>234</lpage>.</citation>
</ref>
<ref id="bibr6-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bollen</surname><given-names>K. A.</given-names></name>
</person-group> (<year>1989</year>). <source>Structural equations with latent variables</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr7-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>T.A.</given-names></name>
</person-group> (<year>2006</year>). <source>Confirmatory factor analysis for applied research</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford Press</publisher-name>.</citation>
</ref>
<ref id="bibr8-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clauser</surname><given-names>B. E.</given-names></name>
<name><surname>Mazor</surname><given-names>K. M.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Using statistical procedures to identify differentially functioning test items</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>17</volume>, <fpage>31</fpage>-<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr9-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>). <source>Statistical power analysis for the behavioral sciences</source> (<edition>2nd ed.</edition>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr10-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Douglas</surname><given-names>J. A.</given-names></name>
<name><surname>Roussos</surname><given-names>L. A.</given-names></name>
<name><surname>Stout</surname><given-names>W.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Item-bundle DIF hypothesis testing: Identifying suspect bundles and assessing their differential functioning</article-title>. <source>Journal of Educational Measurement</source>, <volume>33</volume>, <fpage>465</fpage>-<lpage>484</lpage>.</citation>
</ref>
<ref id="bibr11-0013164412442376">
<citation citation-type="book">
<collab>Educational Testing Service</collab>. (<year>2010</year>). <source>ETS Proficiency Profile user’s guide</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr12-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Erisman</surname><given-names>W.</given-names></name>
<name><surname>Looney</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <source>Opening the door to the American Dream: Increasing higher education access and success for immigrants</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Institute for Higher Education Policy</publisher-name>.</citation>
</ref>
<ref id="bibr13-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fry</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <source>The role of schools in the English language learner achievement gap</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Pew Hispanic Center</publisher-name>.</citation>
</ref>
<ref id="bibr14-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gates</surname><given-names>S. M.</given-names></name>
<name><surname>Augustine</surname><given-names>C. H.</given-names></name>
<name><surname>Benjamin</surname><given-names>R.</given-names></name>
<name><surname>Bikson</surname><given-names>T. K.</given-names></name>
<name><surname>Derghazarian</surname><given-names>E.</given-names></name>
<name><surname>Kaganoff</surname><given-names>T.</given-names></name>
<name><surname>. . . Zimmer</surname><given-names>R. W.</given-names></name>
</person-group> (<year>2001</year>). <source>Ensuring the quality and productivity of education and professional development activities: A review of approaches and lessons for DoD</source>. <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>National Defense Research Institute, RAND</publisher-name>.</citation>
</ref>
<ref id="bibr15-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gray</surname><given-names>M. J.</given-names></name>
<name><surname>Rolph</surname><given-names>E.</given-names></name>
<name><surname>Melamid</surname><given-names>E.</given-names></name>
</person-group> (<year>1996</year>). <source>Immigration and higher education: Institutional responses to changing demographics</source>. <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>RAND Center for Research on Immigration Policy</publisher-name>.</citation>
</ref>
<ref id="bibr16-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hays</surname><given-names>W.L.</given-names></name>
</person-group> (<year>1994</year>). <source>Statistics</source> (<edition>5th edition</edition>). <publisher-loc>Forth Worth, TX</publisher-loc>: <publisher-name>Harcourt Brace &amp; Co</publisher-name>.</citation>
</ref>
<ref id="bibr17-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Horn</surname><given-names>J. L.</given-names></name>
<name><surname>McArdle</surname><given-names>J. J.</given-names></name>
</person-group> (<year>1992</year>). <article-title>A practical and theoretical guide to measurement invariance in aging research</article-title>. <source>Experimental Aging Research</source>, <volume>18</volume>, <fpage>117</fpage>-<lpage>144</lpage>.</citation>
</ref>
<ref id="bibr18-0013164412442376">
<citation citation-type="book">
<collab>ICAS ESL Task Force</collab>. (<year>2006</year>). <source>ESL students in California public higher education</source>. <publisher-loc>Sacramento, CA</publisher-loc>: <publisher-name>Intersegmental Committee of the Academic Senates</publisher-name>.</citation>
</ref>
<ref id="bibr19-0013164412442376">
<citation citation-type="web">
<collab>Institute of International Education</collab>. (<year>2010</year>). <source>Open doors 2010 fast facts</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.iie.org/en/Research-and-Publications/Open-Doors.aspx">http://www.iie.org/en/Research-and-Publications/Open-Doors.aspx</ext-link></comment></citation>
</ref>
<ref id="bibr20-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kane</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Validation</article-title>. In <person-group person-group-type="editor">
<name><surname>Brennan</surname><given-names>R.</given-names></name>
</person-group> (Ed.), <source>Educational measurement</source> (<edition>4th ed.</edition>, pp. <fpage>17</fpage>-<lpage>64</lpage>). <publisher-loc>Westport, CT</publisher-loc>: <publisher-name>American Council on Education and Praeger</publisher-name>.</citation>
</ref>
<ref id="bibr21-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kane</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Validity and fairness</article-title>. <source>Language Testing</source>, <volume>27</volume>(<issue>2</issue>), <fpage>177</fpage>-<lpage>182</lpage>.</citation>
</ref>
<ref id="bibr22-0013164412442376">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Kanno</surname><given-names>Y.</given-names></name>
<name><surname>Cromley</surname><given-names>J. G.</given-names></name>
</person-group> (<year>2010</year>). <source>English Language learners’ access to and attainment in postsecondary education</source> <comment>(Research Report No. RG 09-141) [AIR grant report]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.airweb.org/images/Grants2009/Kanno_Final.pdf">www.airweb.org/images/Grants2009/Kanno_Final.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr23-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Klein</surname><given-names>S. P.</given-names></name>
<name><surname>Kuh</surname><given-names>G. D.</given-names></name>
<name><surname>Chun</surname><given-names>M.</given-names></name>
<name><surname>Hamilton</surname><given-names>L.</given-names></name>
<name><surname>Shavelson</surname><given-names>R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>An approach to measuring cognitive outcomes across higher education institutions</article-title>. <source>Research in Higher Education</source>, <volume>46</volume>, <fpage>251</fpage>-<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr24-0013164412442376">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Klein</surname><given-names>S.</given-names></name>
<name><surname>Liu</surname><given-names>O. L.</given-names></name>
<name><surname>Sconing</surname><given-names>J.</given-names></name>
<name><surname>Bolus</surname><given-names>R.</given-names></name>
<name><surname>Bridgeman</surname><given-names>B.</given-names></name>
<name><surname>Kugelmass</surname><given-names>H.</given-names></name>
<name><surname>. . . Steedle</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>, <month>September</month>). <source>Test Validity Study (TVS) report</source> <comment>(Supported by the Fund for Improvement of Postsecondary Education [FIPSE]). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.voluntarysystem.org/index.cfm?page=research">http://www.voluntarysystem.org/index.cfm?page=research</ext-link></comment></citation>
</ref>
<ref id="bibr25-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kline</surname><given-names>R. B.</given-names></name>
</person-group> (<year>2004</year>). <source>Principles and practice of structural equation modeling</source> (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford Press</publisher-name>.</citation>
</ref>
<ref id="bibr26-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kuh</surname><given-names>G.</given-names></name>
<name><surname>Ikenberry</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <source>More than you think, less than we need: Learning outcomes assessment in American higher education</source>. <publisher-loc>Champaign, IL</publisher-loc>: <publisher-name>National Institution for Learning Outcomes</publisher-name>.</citation>
</ref>
<ref id="bibr27-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Leskes</surname><given-names>A.</given-names></name>
<name><surname>Shoenberg</surname><given-names>R.</given-names></name>
<name><surname>Gaff</surname><given-names>R.</given-names></name>
<name><surname>Tritelli</surname><given-names>D.</given-names></name>
<name><surname>Nichols</surname><given-names>J.</given-names></name>
</person-group> (<year>2004</year>). <source>Taking responsibility for the quality of the baccalaureate degree</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Association of American Colleges and Universities</publisher-name>.</citation>
</ref>
<ref id="bibr28-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>O. L.</given-names></name>
</person-group> (<year>2008</year>). <source>Measuring learning outcomes in higher education using the Measure of Academic Proficiency and Progress (MAPP)</source> (<comment>ETS Research Report Series No. RR-08-047</comment>). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr29-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>O. L.</given-names></name>
</person-group> (<year>2011a</year>). <article-title>Outcomes assessment in higher education: Challenges and future research in the context of Voluntary System of Accountability</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>30</volume>(<issue>3</issue>), <fpage>2</fpage>-<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr30-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>O. L.</given-names></name>
</person-group> (<year>2011b</year>). <article-title>Value-added assessment in higher education: A comparison of two methods</article-title>. <source>Higher Education</source>, <volume>61</volume>, <fpage>445</fpage>-<lpage>461</lpage>.</citation>
</ref>
<ref id="bibr31-0013164412442376">
<citation citation-type="web">
<collab>Lumina Foundation for Education</collab>. (<year>2010</year>). <source>The degree qualifications profile</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.luminafoundation.org/publications/The_Degree_Qualifications_Profile.pdf">http://www.luminafoundation.org/publications/The_Degree_Qualifications_Profile.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr32-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Marr</surname><given-names>D.</given-names></name>
</person-group> (<year>1995</year>). <source>Validity of the academic profile</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr33-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McPherson</surname><given-names>P.</given-names></name>
<name><surname>Shulenburger</surname><given-names>D.</given-names></name>
</person-group> (<year>2006</year>). <source>Toward a voluntary system of accountability program (VSA) for public colleges universities and colleges</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Association of State Universities and Land-Grant Colleges</publisher-name>.</citation>
</ref>
<ref id="bibr34-0013164412442376">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Muthén</surname><given-names>L. K.</given-names></name>
</person-group> (<year>2009a</year>, <month>April</month> <day>16</day>). <article-title>Re: DIFFTEST [Online forum comment]</article-title>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.statmodel.com/discussion/messages/9/1865.html">http://www.statmodel.com/discussion/messages/9/1865.html</ext-link></comment></citation>
</ref>
<ref id="bibr35-0013164412442376">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Muthén</surname><given-names>L. K.</given-names></name>
</person-group> (<year>2009b</year>, <month>June</month> <day>22</day>). <article-title>Re: WLSMV and RMSEA [Online forum comment]</article-title>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.statmodel.com/discussion/messages/9/3287.html">http://www.statmodel.com/discussion/messages/9/3287.html</ext-link></comment></citation>
</ref>
<ref id="bibr36-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Muthén</surname><given-names>L. K.</given-names></name>
<name><surname>Muthén</surname><given-names>B. O.</given-names></name>
</person-group> (<year>1998-2009</year>). <source>Mplus user’s guide</source> (<edition>5th ed.</edition>). <publisher-loc>Los Angeles, CA</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr37-0013164412442376">
<citation citation-type="gov">
<collab>National Center for Educational Statistics</collab>. (<year>2008</year>). <source>National Postsecondary Student Aid Study</source> <comment>(NPSAS) [Data file]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://nces.ed.gov/datalab/">http://nces.ed.gov/datalab/</ext-link></comment></citation>
</ref>
<ref id="bibr38-0013164412442376">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Obama</surname><given-names>B. H.</given-names></name>
</person-group> (<year>2010</year>, <month>August</month> <day>9</day>). <source>Remarks by the president on higher education and the economy</source>. <publisher-loc>Austin</publisher-loc>: <publisher-name>University of Texas</publisher-name>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.whitehouse.gov/the-press-office/2010/08/09/remarks-president-higher-education-and-economy-university-texas-austin">http://www.whitehouse.gov/the-press-office/2010/08/09/remarks-president-higher-education-and-economy-university-texas-austin</ext-link></comment></citation>
</ref>
<ref id="bibr39-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pascarella</surname><given-names>E. T.</given-names></name>
<name><surname>Seifert</surname><given-names>T. A.</given-names></name>
<name><surname>Blaich</surname><given-names>C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>How effective are the NSSE benchmarks in predicting important educational outcomes?</article-title> <source>Change</source>, <volume>42</volume>, <fpage>16</fpage>-<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr40-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Patterson</surname><given-names>B. F.</given-names></name>
<name><surname>Mattern</surname><given-names>K. D.</given-names></name>
<name><surname>Kobrin</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2007</year>). <source>Validity of the SAT for predicting FYGPA: 2007 SAT validity sample</source> [<comment>Statistical report</comment>]. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr41-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pitoniak</surname><given-names>M. J.</given-names></name>
<name><surname>Young</surname><given-names>J. W.</given-names></name>
<name><surname>Martiniello</surname><given-names>M.</given-names></name>
<name><surname>King</surname><given-names>T. C.</given-names></name>
<name><surname>Buteux</surname><given-names>A.</given-names></name>
<name><surname>Ginsburgh</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <source>Guidelines for the assessment of English-language learners</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr42-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Roussos</surname><given-names>L. A.</given-names></name>
<name><surname>Stout</surname><given-names>W. F.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Simulation studies of the effects of small sample size and studied item parameters on SIBTEST and Mantel-Haenszel type I error performance</article-title>. <source>Journal of Educational Measurement</source>, <volume>33</volume>, <fpage>215</fpage>-<lpage>230</lpage>.</citation>
</ref>
<ref id="bibr43-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shealy</surname><given-names>R.</given-names></name>
<name><surname>Stout</surname><given-names>W. F.</given-names></name>
</person-group> (<year>1993</year>). <article-title>A model-based standardization approach that separates true bias/DIF from group ability differences and detects test bias/DTF as well as item bias/DIF</article-title>. <source>Psychometrika</source>, <volume>58</volume>, <fpage>159</fpage>-<lpage>194</lpage>.</citation>
</ref>
<ref id="bibr44-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Striplin</surname><given-names>J. C.</given-names></name>
</person-group> (<year>2000</year>). <source>A review of community college curriculum trends</source>. <comment>Retrieved from ERIC database. (ED438011)</comment></citation>
</ref>
<ref id="bibr45-0013164412442376">
<citation citation-type="book">
<collab>U.S. Department of Education</collab>. <year>2006</year>. <source>A test of leadership: Charting the future of U.S. higher education</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr46-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>van de Vijver</surname><given-names>F. J. R.</given-names></name>
<name><surname>Poortinga</surname><given-names>Y. H.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Conceptual and methodological issues in adapting tests</article-title>. In <person-group person-group-type="editor">
<name><surname>Hambleton</surname><given-names>R. K.</given-names></name>
<name><surname>Merenda</surname><given-names>P. F.</given-names></name>
<name><surname>Spielberger</surname><given-names>C. D.</given-names></name>
</person-group> (Eds.), <source>Adapting educational and psychological tests for cross-cultural assessment</source> (pp. <fpage>39</fpage>-<lpage>63</lpage>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr47-0013164412442376">
<citation citation-type="web">
<collab>Voluntary System of Accountability</collab>. (<year>2008</year>). <source>Background on learning outcomes measures</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.voluntarysystem.org/index.cfm">http://www.voluntarysystem.org/index.cfm</ext-link></comment></citation>
</ref>
<ref id="bibr48-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Young</surname><given-names>J. W.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A framework for test validity research on content assessments taken by English language learners</article-title>. <source>Educational Assessment</source>, <volume>14</volume>, <fpage>122</fpage>-<lpage>138</lpage>.</citation>
</ref>
<ref id="bibr49-0013164412442376">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Zwick</surname><given-names>R.</given-names></name>
</person-group> (<year>2007</year>). <source>College admission tests</source>. <publisher-loc>Arlington, VA</publisher-loc>: <publisher-name>National Association for College Admission Counseling</publisher-name>.</citation>
</ref>
<ref id="bibr50-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zwick</surname><given-names>R.</given-names></name>
<name><surname>Schlemer</surname><given-names>L.</given-names></name>
</person-group> (<year>2004</year>). <article-title>SAT validity for linguistic minorities at the University of California, Santa Barbara</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>25</volume>, <fpage>6</fpage>-<lpage>16</lpage>.</citation>
</ref>
<ref id="bibr51-0013164412442376">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zwick</surname><given-names>R.</given-names></name>
<name><surname>Sklar</surname><given-names>J. C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Predicting college grades and degree completion using high school grades and SAT scores: The role of student ethnicity and first language</article-title>. <source>American Educational Research Journal</source>, <volume>42</volume>, <fpage>439</fpage>-<lpage>464</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>