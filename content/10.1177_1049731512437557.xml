<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">RSW</journal-id>
<journal-id journal-id-type="hwp">sprsw</journal-id>
<journal-title>Research on Social Work Practice</journal-title>
<issn pub-type="ppub">1049-7315</issn>
<issn pub-type="epub">1552-7581</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1049731512437557</article-id>
<article-id pub-id-type="publisher-id">10.1177_1049731512437557</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Evaluating an Objective Structured Clinical Examination (OSCE) Adapted for Social Work</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Bogo</surname>
<given-names>Marion</given-names>
</name>
<xref ref-type="aff" rid="aff1-1049731512437557">1</xref>
<xref ref-type="corresp" rid="corresp1-1049731512437557"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Regehr</surname>
<given-names>Cheryl</given-names>
</name>
<xref ref-type="aff" rid="aff1-1049731512437557">1</xref>
<xref ref-type="aff" rid="aff2-1049731512437557">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Katz</surname>
<given-names>Ellen</given-names>
</name>
<xref ref-type="aff" rid="aff1-1049731512437557">1</xref>
<xref ref-type="aff" rid="aff3-1049731512437557">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Logie</surname>
<given-names>Carmen</given-names>
</name>
<xref ref-type="aff" rid="aff4-1049731512437557">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tufford</surname>
<given-names>Lea</given-names>
</name>
<xref ref-type="aff" rid="aff1-1049731512437557">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Litvack</surname>
<given-names>Andrea</given-names>
</name>
<xref ref-type="aff" rid="aff1-1049731512437557">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1049731512437557">
<label>1</label>Factor-Inwentash Faculty of Social Work, University of Toronto, Toronto, Ontario, Canada</aff>
<aff id="aff2-1049731512437557">
<label>2</label>Office of the Vice-President and Provost, Academic Programs, University of Toronto, Toronto, Ontario, Canada</aff>
<aff id="aff3-1049731512437557">
<label>3</label>Hincks-Dellcrest Centre, Toronto, Ontario, Canada</aff>
<aff id="aff4-1049731512437557">
<label>4</label>Canadian Institutes of Health Research, Women’s College Research Institute, University of Toronto, Toronto, Ontario, Canada</aff>
<author-notes>
<corresp id="corresp1-1049731512437557">Marion Bogo, Factor-Inwentash Faculty of Social Work, University of Toronto, 246 Bloor St. West, Toronto, Ontario M5S 1V4, Canada Email: <email>marion.bogo@utoronto.ca</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2012</year>
</pub-date>
<volume>22</volume>
<issue>4</issue>
<fpage>428</fpage>
<lpage>436</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Objectives: To evaluate an objective structured clinical examination (OSCE) adapted for social work in a lab course and examine the degree to which it predicts competence in the practicum. Method: 125 Masters students participated in a one-scenario OSCE and wrote responses to standardized reflection questions. OSCE performance and reflections were rated on previously standardized scales, competence in field was evaluated using the online practice-based evaluation. Results: A wide range of scores on OSCE scales indicate that differences in student competencies were captured. Correlational analyses revealed an association between OSCE scales and field final evaluations. Nevertheless, a number of students who performed poorly on the OSCE did well in the practicum. Conclusions: The OSCE method of evaluation warrants cautious optimism and requires further replication and adaptation for social work educational outcomes assessment.</p>
</abstract>
<kwd-group>
<kwd>objective structured clinical examination (OSCE)</kwd>
<kwd>social work competence</kwd>
<kwd>educational outcomes assessment</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1049731512437557">
<title>Introduction</title>
<p>The competency-based assessment framework provided by Educational Policy and Accreditation Standards (EPAS, 2008) guides social work educators in identifying and assessing the educational outcomes of their programs. This new paradigm is based on the premise that the expression of the knowledge, values, and skills learned are evident in complex practice behaviors. While in the past, the assessment of performance has been delegated to the field practicum, this new approach challenges all components of the curriculum to find methods to assess competence—not only the knowledge students have gained but also how knowledge is evident in practice. Data obtained will provide feedback to students to promote learning as well as information to programs to “continuously inform and promote change in the explicit and implicit curriculum to enhance attainment of program competencies” (Council on Social Work Education [<xref ref-type="bibr" rid="bibr7-1049731512437557">CSWE], 2008</xref>, p. 16). Accordingly, educators are eager to find reliable, valid, and authentic assessment methods.</p>
<p>In a multiproject program of research designed to develop effective evaluation approaches for social work education, the Objective Structured Clinical Examination (OSCE) used in medicine and other health professions was adapted for social work. Testing of the adapted model demonstrated reliability and validity for both the adapted OSCE method and the evaluative tools developed to measure competency (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>). The current study aimed to evaluate this newly developed approach as a method for assessing student performance at the completion of the first semester in a Master in social work (MSW) graduate program, prior to entry into the field practicum and to determine the degree to which this method of evaluation predicted performance in the practicum.</p>
<sec id="section2-1049731512437557">
<title>Competence</title>
<p>In the United Kingdom and Australia, national governments embraced competency models for defining national occupational standards in a wide range of employment groups. Educators however were critical of this approach suggesting that when work tasks are broken down into ever growing lists of discrete, concrete, behavioral skills, the art of practice is lost (<xref ref-type="bibr" rid="bibr10-1049731512437557">Hackett, 2001</xref>; <xref ref-type="bibr" rid="bibr15-1049731512437557">Kelly &amp; Horder, 2001</xref>; <xref ref-type="bibr" rid="bibr16-1049731512437557">Leung, 2002</xref>). Consequently, in the past decade, American educators in professional programs have designed more complex and nuanced competence models. The American Accreditation Council for Graduate Medical Education developed six general competencies to guide the design and assessment of learning in all medical residencies. These include: patient care, medical knowledge, practice-based learning and improvement, interpersonal and communication skills, professionalism, and systems-based practice (<xref ref-type="bibr" rid="bibr1-1049731512437557">Accreditation Council for Graduate Medical Education [ACGME, 2010</xref>]). The American Psychological Association uses a cube model to define intellectual, adaptive, and creative capacities for clinical psychologists (<xref ref-type="bibr" rid="bibr13-1049731512437557">Kaslow et al., 2004</xref>). This model integrates foundational, functional, and developmental competencies (<xref ref-type="bibr" rid="bibr8-1049731512437557">Fouad et al., 2009</xref>). And, through EPAS 2008, social work has developed a complex model of competence with its articulation of 10 key competencies and their related-practice behaviors.</p>
<p>In a multiproject program of research, we developed a holistic model of competence which identified two interrelated levels, metacompetencies, and procedural competencies (<xref ref-type="bibr" rid="bibr6-1049731512437557">Bogo et al., 2006</xref>). Metacompetencies are overarching qualities and abilities of a conceptual, interpersonal, and personal/professional nature. Metacompetencies include cognitive capacities, specifically, the ability to conceptualize practice through the lens of the discipline’s knowledge and values. Metacompetencies also include interpersonal dimensions that affect engagement with others—capacities such as self-awareness, reflection, and self-regulation. Procedural competencies refer to skills and techniques such as interviewing, conducting assessments, offering interventions, and communicating professional information through verbal and written methods. This model emphasizes the link between the cognitive processes involved in conceptualizing practice, the subjective experience of the practitioner, and the performance of skillful behavior. The challenge then is to design assessment tools which incorporate both of these levels of competence.</p>
<p>Assessment of competence has received considerable attention as educators in many disciplines conceptualize and define the components of competence and design reliable, valid, and authentic assessment tools. Examples of such tools can be found for instance, in medicine (<xref ref-type="bibr" rid="bibr19-1049731512437557">Lurie, Mooney, &amp; Lyness, 2009</xref>; <xref ref-type="bibr" rid="bibr27-1049731512437557">Snell &amp; Frank, 2010</xref>) and in psychology (<xref ref-type="bibr" rid="bibr14-1049731512437557">Kaslow et al., 2007</xref>, 2009). The holistic model of competence developed by the Toronto team guided development of reliable field evaluation tools for students in direct practice (<xref ref-type="bibr" rid="bibr24-1049731512437557">Regehr, Bogo, &amp; Regehr, 2011</xref>; <xref ref-type="bibr" rid="bibr25-1049731512437557">Regehr, Bogo, Regehr, &amp; Power, 2007</xref>) and in macro practice (Regehr, Bogo, &amp; Regehr, 2011). Testing of online versions of these tools demonstrated high acceptability from students and faculty, high internal consistency and clearly reduced ceiling effect when compared with a traditional competency-based evaluation tool. In our studies however, field instructors were uniformly vocal about the burden they experienced in serving as gatekeepers for the profession. They argued that the university program should take greater responsibility for assessing students’ actual performance rather than continue to rely on field instructors or proxy measures used in courses, such as written essays and tests (<xref ref-type="bibr" rid="bibr5-1049731512437557">Bogo, Regehr, Power, &amp; Regehr, 2007</xref>).</p>
</sec>
<sec id="section3-1049731512437557">
<title>The OSCE Adapted for Social Work</title>
<p>The OSCE was designed in medical education to provide an examination that would assess not only knowledge, but also clinical skills, have high reliability, and was standardized and hence fair for students (<xref ref-type="bibr" rid="bibr11-1049731512437557">Harden &amp; Gleeson, 1979</xref>). It is now used in a range of health professions, for example, in nursing (<xref ref-type="bibr" rid="bibr29-1049731512437557">Watson, Stimpson, Topping, &amp; Porock, 2002</xref>), pharmacy (<xref ref-type="bibr" rid="bibr2-1049731512437557">Austin, O'Byrne, Pugsley, &amp; Quero Munoz, 2003</xref>), and physical therapy (<xref ref-type="bibr" rid="bibr30-1049731512437557">Wessel, Williams, Finch, &amp; Gemus, 2003</xref>). In an OSCE, students interact with a trained actor, called a standardized patient (SP)/client, in a scenario, and perform professional tasks. The student is observed and her performance is rated on a checklist or Global Process Rating scale. In a review of the OSCE method over the past three decades, <xref ref-type="bibr" rid="bibr12-1049731512437557">Hodges (2006)</xref> notes extensive empirical studies which provide support that the method has acceptable validity and reliability. Also, it is viewed positively by students and faculty members and is used frequently for periodic performance-based assessment in undergraduate medical education, specialty areas, and in licensing examinations in some countries.</p>
<p>Social work educators have used client simulations in teaching and evaluation for many decades. While systematic study is limited (Logie, Bogo, Regehr, &amp; Regehr, in press), recently there is a growing interest in testing the use of OSCE in social work as seen in three studies (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>; <xref ref-type="bibr" rid="bibr18-1049731512437557">Lu et al., 2011</xref>; <xref ref-type="bibr" rid="bibr22-1049731512437557">Rawlings, 2008</xref>). <xref ref-type="bibr" rid="bibr22-1049731512437557">Rawlings (2008)</xref> compared students entering and exiting an undergraduate program on their performance of a one-scenario OSCE where the client was dealing with grief issues. Standardized instruments with high internal reliability were used to assess direct practice skill and self-efficacy. The study found mean scores on all scales and subscales were significantly higher for students exiting the program than for those entering. <xref ref-type="bibr" rid="bibr18-1049731512437557">Lu and colleagues (2011)</xref> used an OSCE to assess graduate students’ practice skills in clinical competence and cultural sensitivity after completing a one semester course on clinical practice with diverse populations. Each student interviewed one of the six standardized clients and was rated using a checklist developed and tested for the study. The checklist demonstrated high internal reliability.</p>
<p>Based on our previous research, we developed a theoretical model of competence which consists of the interrelationship between procedural competencies and metacompetencies. To assess both these dimensions of competence, we adapted the OSCE for social work to consist of two components: (1) a simulated interview which could assess procedural competence evident in performance of social work tasks and (2) a reflective activity, which could assess metacompetencies which guide the practitioner in the interview (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>). The original OSCE study consisted of five practice scenarios each involving a 15-min interview followed by a 15-min structured reflective dialogue. While the students interviewed the standardized client, a trained rater assessed their performance of procedural competencies using the OSCE Adapted for Social Work Performance Rating scale. Following the interview, the rater used a structured set of questions to elicit students’ reflections and critical thinking about their performance, using the OSCE adapted for social work post-OSCE reflective dialogue probes and rated the students’ responses using the OSCE Adapted for Social Work Post-OSCE Reflective Dialogue Rating scale. Evaluation of this approach demonstrated promising reliability in the OSCE approach and scales and demonstrated construct validity; in that, it differentiated between social workers in training and experienced workers.</p>
<p>The current study aimed to evaluate this newly developed approach, an OSCE adapted for social work, to assess student performance of the procedural competencies at the completion of the first semester in a graduate program, prior to entry into the field practicum. The MSW Program at the University of Toronto has a delayed-entry practicum design. First semester courses include two companion courses, one on social work practice theory and the other a laboratory that emphasizes integrating knowledge and values with skillful interviewing behavior. Students must successfully pass these courses before entering the field practicum in the subsequent term. Formative and summative evaluations include typical written papers, reflections, and interview role-plays with a fellow student accompanied by a written critical reflective analysis. A weakness of the later method is that role-play interviews are not standardized and vary considerably depending on students’ ability to play a client role in an authentic manner and the level of client difficulty they present. These variations in turn affect the complexity of the written critical reflective analysis. Course instructors were seeking a reliable and valid assessment method that reflected the interrelated competencies studied in these courses. At a program level, we were interested in examining whether there was an association between competencies gained in the lab course and the subsequent level of competence gained in the field practicum.</p>
<p>The OSCE method tested in the original study was modified due to resource limitations. Instead of participating in five scenarios, each student participated in one scenario and the reflective dialogue was replaced with a reflective written assignment, however, the questions tested in the original study were used. This article examines the effectiveness of this new evaluation approach.</p>
</sec>
</sec>
<sec id="section4-1049731512437557" sec-type="methods">
<title>Method</title>
<p>In December 2010, all 125 students enrolled in the Masters Program participated in a one-scenario OSCE as their final assessment for the Social Work Practice Laboratory. For each student, the examination consisted of a 2-min period to read a brief written case scenario and then the conduct of a one-to-one individual interview with a standardized client, played by an actor trained for the particular role. While the student conducted the interview, a course instructor sat in the room and observed and rated the student. The rater was a member of the course team instructors other than the student’s own instructor. The OSCE Adapted for Social Work Performance Rating scale was used by the rater. Following the interview, the “client” provided immediate feedback to the student. Immediately following the interview and the feedback, the student spent 25 min in the computer lab providing written responses to a set of standardized questions, designed to foster reflection on the just-completed interview. The rater then evaluated the student reflections using the OSCE Adapted for Social Work Reflective Dialogue Rating scale. The course instructor incorporated information from the scales in written feedback to each student as part of the overall evaluation in the laboratory course which has a credit/no credit grade. Passing students began the field practicum in January.</p>
<p>Students were asked to consent to have their data used as part of an evaluation of this new method. Of the total 125 students, 109 students consented to participate in the study and allow the researchers to use their data. The data consist of student scores on (1) the OSCE Adapted for Social Work Performance Rating scale; (2) the OSCE Adapted for Social Work Reflective Dialogue Rating scale; and (3) the Online Practice-Based Evaluation tool that was completed by field instructors and students at the midterm and end of the field practicum. In addition, participating students completed a feedback form about the OSCE experience. This protocol was approved by the University of Toronto Research Ethics Board.</p>
<sec id="section5-1049731512437557">
<title>OSCE Scenarios</title>
<p>Two case scenarios representing authentic social work client situations were used in this evaluation: an isolated elderly woman needing home care services, and a young distraught mother of an infant where there was possible child neglect. Consistent with social work’s emphasis on integrating diversity in practice (CSWE, 2008; <xref ref-type="bibr" rid="bibr21-1049731512437557">NASW, 2007</xref>) issues regarding ethnicity, religion, age, gender, and ability were integrated in the cases. Case scenarios were those used in our original study and were designed in consultation with practicing social workers in the respective fields of service (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>). Our holistic competence model (<xref ref-type="bibr" rid="bibr4-1049731512437557">Bogo et al., 2004</xref>; <xref ref-type="bibr" rid="bibr6-1049731512437557">2006</xref>) had guided identification of competencies that could be assessed in 15-min scenarios and in reflections, the content of each case scenario, and training of actors from the university’s SP program.</p>
<p>The research team worked with the long established SP Program at The University of Toronto to refine scenarios and train SPs over 2 half-day sessions. SPs/clients are increasingly part of health professionals’ education and trained to demonstrate the personal history, physical symptoms, emotional characteristics, and everyday concerns of an actual person based on the scenario and description provided by the educator. SPs/clients are given a number of verbatim statements with associated affect to offer at specific times during the interview so that all students receive the same factual information and emotional cues. As well SPs are trained to provide constructive feedback from the client’s perspective, and in this examination to focus especially on responding to the student’s efforts to develop a relationship, and to elicit and respond to surface and depth information from the client. Four actors were used to portray each of the two scenarios.</p>
</sec>
<sec id="section6-1049731512437557">
<title>Training Raters</title>
<p>Faculty members who were instructors in the courses were trained as a group to rate the OSCE in a 3-hr session. Training included: review of the process and all materials; watching video recordings made in the pilot stage of the project of various participants interviewing the standardized client in the scenarios to be used in the OSCE; rating the participants on the scales; comparing faculty member ratings of test participants; and discussion about the use of the scales to achieve more similar ratings.</p>
</sec>
</sec>
<sec id="section7-1049731512437557">
<title>Measures</title>
<sec id="section8-1049731512437557">
<title>OSCE Adapted for Social Work Performance Rating Scale</title>
<p>This scale was previously constructed as a set of nine dimensions and a single global rating of overall performance (for 10 items), deemed to be assessable in a 15-minute interview (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>). The dimensions assess procedural competencies such as the ability to develop a collaborative relationship with the client, assessment skills, goal-setting, and cultural competence. Each dimension is evaluated by a rater on a scale of 1 (<italic>poor performance</italic>) to 5 (<italic>excellent performance</italic>) with each level of the scale being anchored with clear performance descriptors. In the study that evaluated the tool across five different OSCE stations, the internal consistency was high at each station with the average Cronbach’s α across the five stations of .92. The reliability of the rating tool across stations was moderate, with an α of .55 for five stations (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>). On the specific scenarios selected for this study, the Performance scale demonstrated moderate internal consistency for Scenario 1 (Cronbach’s α = .69) and high internal consistency for Scenario 2 (Cronbach’s α = .91).</p>
<p>To assess the construct validity of the examination, the total OSCE scores of the examination participants at three experience levels (completed Year 1, completed Year 2, and more than 5 years of experience) were compared. Analysis of variance revealed a significant effect of experience for the performance ratings (<italic>F</italic>
<sub>2,20</sub> = 4.30, <italic>p</italic> &lt; .05) with experience level accounting for 30.1% of the variance in participant scores (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>).</p>
</sec>
<sec id="section9-1049731512437557">
<title>OSCE Adapted for Social Work Post-OSCE Reflective Dialogue Rating Scale</title>
<p>The reflective dialogue scale has nine dimensions that assess metacompetencies such as the ability to conceptualize practice, self-regulation, and professional development (<xref ref-type="bibr" rid="bibr3-1049731512437557">Bogo et al., 2011</xref>). In the previous study, this scale similarly demonstrated internal consistency with a Cronbach’s α of .93. The reliability across stations was moderate, with an α of .48 for five stations. This suggests some evidence for the generalizability of individual scores across the range of scenarios tested. On the specific scenarios selected for this study, the internal consistency of the Reflection scale was .78 for Scenario 1 and  .90 for Scenario 2.</p>
</sec>
<sec id="section10-1049731512437557">
<title>Student Satifaction</title>
<p>Students completed a survey containing seven questions rated on a 5-point Likert-type scale. The questions addressed their perceptions of the effectiveness of this model for evaluation and for learning, whether it should be used in future, and the overall value of the method.</p>
</sec>
<sec id="section11-1049731512437557">
<title>Online Practice-Based Evaluation Tool</title>
<p>The Online Practice-Based Evaluation Tool allows field instructors to assess student performance in the field practicum on six dimensions: learning and growth; behavior in the organization; conceptualizing practice; clinical relationships; assessment and intervention; and professional communication. Students complete the same tool as a form of self-evaluation. For each dimension, the evaluator is required to select from the list of descriptive phrases those that best described the student using a pull down menu. Once all relevant phrases are selected for each dimension, and the evaluation is submitted, a numercial value of 1–5 is automatically assigned to each selected phrase based on level of performance in previous testing of the tool (<xref ref-type="bibr" rid="bibr4-1049731512437557">Bogo et al., 2004</xref>). A mean score is derived for each dimension and a final score is calculated as the unweighted average of the six-dimensional scores. The tool has excellent internal consistency with a Cronbach’s α of .99 for instructors and .98 for students (<xref ref-type="bibr" rid="bibr23-1049731512437557">Regehr, Bogo, &amp; Regehr 2011</xref>). Further, the tool is able to differentiate levels of performance at midterm and final evaluation.</p>
</sec>
</sec>
<sec id="section12-1049731512437557">
<title>Results</title>
<p>Of the 125 students who took the OSCE as a course requirement, 109 students consented to participate in the study and allow the researchers to use their data. Performance in the OSCE was evaluated by eight different raters. There were no significant differences in scores between individual raters on the OSCE Performance scale. However, two raters were significantly different than the others on the OSCE Reflections scale. One rater had significantly lower scores and one rater had significantly higher scores.</p>
<sec id="section13-1049731512437557">
<title>OSCE Scores</title>
<p>There were a wide range of scores on both the Performance scale and the Reflection scale, indicating that differences in student abilities were captured by the scale. Total scores on the Performance scale ranged from 11 to 28.5 with a mean of 20.65 (<italic>SD</italic> = 4.22) out of a possible score of 50. Total scores on the Reflection scale ranged from 10 to 38 with a mean of 27.14 (<italic>SD</italic> = 6.50) out of a total possible score of 45. Reflection and Performance scale scores were correlated with one another (<italic>r</italic> = .57, <italic>p</italic> ≤ .001). There was no significant difference in Performance scale scores between the two different scenarios (the young disraught mother or the elderly woman). However, the case with the young disraught mother resulted in significantly lower scores on the Reflection scale (<italic>T</italic> = 3.92, <italic>p</italic> ≤ .001). This appeared to reflect the fact that students had less life experience with young mothers to guide their reflections.</p>
</sec>
<sec id="section14-1049731512437557">
<title>The Association Between OSCE Scores and Practicum Scores</title>
<p>Correlational analyses revealed an association between both OSCE Reflection scale scores and OSCE Performance scale scores and field instructor final evaluations of student performance in the practicum on the Online Practice-Based Evaluation tool. The association between the final practicum score and the OSCE Performance scale total score and the practicum and the OSCE Reflection Scale total score were <italic>r</italic> = .23 (<italic>p</italic> = .027) and <italic>r</italic> = .38 (<italic>p</italic> = .004), respectively.</p>
<p>Subscales on the Online Practice-Based Evaluation tool were combined to provide two additional scales; one for metacompetencies and one for procedural competencies in the clinical domain. Metacompetencies consisted of learning and growth, behavior in the organization, and conceptualizing practice. Procedural competencies consisted of clinical relationships and assessment and intervention. These combined scales were based on our previous work (<xref ref-type="bibr" rid="bibr6-1049731512437557">Bogo et al., 2006</xref>; <xref ref-type="bibr" rid="bibr23-1049731512437557">Regehr et al., 2011</xref>). The reliability of combined Metacompetencies subscale as determined by a Cronbach’s α was .83. The reliability of the combined procedural subscale was moderate (Cronbach’s α, .70).</p>
<p>Metacompetencies in the field evaluation were associated with both OSCE Reflection scale and OSCE Performance scale scores (<italic>r</italic> = .36, <italic>p</italic> = .001 and <italic>r</italic> = .22, <italic>p</italic> = .029, respectively). Procedural competencies were associated with only OSCE Reflection scale scores (<italic>r</italic> = .27, <italic>p</italic> = .017). Associations between OSCE subscales and practicum evaluation subscales can be found in <xref ref-type="table" rid="table1-1049731512437557">Table 1</xref>. However, the scatterplots (<xref ref-type="fig" rid="fig1-1049731512437557">Figures 1</xref>
<xref ref-type="fig" rid="fig2-1049731512437557"/>
<xref ref-type="fig" rid="fig3-1049731512437557"/>–<xref ref-type="fig" rid="fig4-1049731512437557">4</xref>) reveal that the associations are indicative of the fact that students who struggle in the practicum also tend to do poorly in the OSCEs. In students receiving practicum ratings of 4–5, there is no clear association between OSCE scores and ratings in the practicum.</p>
<table-wrap id="table1-1049731512437557" position="float">
<label>Table 1</label>
<caption>
<p>Association between OSCE Scores and Final Practicum Scores</p>
</caption>
<graphic alternate-form-of="table1-1049731512437557" xlink:href="10.1177_1049731512437557-table1.tif"/>
<table>
<thead>
<tr>
<th/>
<th colspan="7">OSCE Scores</th>
</tr>
<tr>
<th>
</th>
<th>Perform Collab Relation</th>
<th>Perform Assess</th>
<th>Perform Total</th>
<th>Reflect Concept</th>
<th>Reflect Self Aware</th>
<th>Reflect Prof Dev</th>
<th>Reflect Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning &amp; Growth</td>
<td>
</td>
<td>r=.28 	P≤.004</td>
<td>r=.21 	P≤.038</td>
<td>r=.21 	P≤.029</td>
<td>r=.22 	P≤.029</td>
<td>r=.27 	P≤.008</td>
<td>r=.33 	P≤.004</td>
</tr>
<tr>
<td>Behavior in Organization</td>
<td>
</td>
<td>r=.20 	P≤.034</td>
<td>
</td>
<td>
</td>
<td>r=.27 	P≤.007</td>
<td>r=.27 	P≤.007</td>
<td>r=.38 	P≤.003</td>
</tr>
<tr>
<td>Conceptualizing Practice</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>r=.34 	P≤.001</td>
<td>r=.32 	P≤.003</td>
<td>r=.28 	P≤.013</td>
</tr>
<tr>
<td>Clinical Relationships</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>r=.26 	P≤.014</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>Assess&amp; Intervention</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>r=.31 	P≤.003</td>
<td>r=.268 	P≤.012</td>
<td>r=.278 	P≤.015</td>
</tr>
<tr>
<td>Communication</td>
<td>r=.22 	P≤.022</td>
<td>r=.24 	P≤.012</td>
<td>r=.27 	P≤.008</td>
<td>r=.37 	P≤.001</td>
<td>r=.38 	P≤.001</td>
<td>r=.33 	P≤.001</td>
<td>r=.39 	P≤.001</td>
</tr>
<tr>
<td>
<bold>Combined Meta-Competencies</bold>
</td>
<td>
</td>
<td>r=.27 	P≤.008</td>
<td>r=.22 	P≤.029</td>
<td>r=.22 	P≤.029</td>
<td>r=.35 	P≤.001</td>
<td>r=.35 	P≤.001</td>
<td>r=.36 	P≤.001 </td>
</tr>
<tr>
<td>
<bold>Combined Procedural Competencies</bold>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>r=.32 	P≤.002</td>
<td>r=.27 	P≤.013</td>
<td>r=.27 	P≤.017 </td>
</tr>
<tr>
<td>
<bold>Total Competencies</bold>
</td>
<td>
</td>
<td>r=.25 	P≤.013</td>
<td>r=.23 	P≤.027</td>
<td>r=.21 	P≤.029</td>
<td>r=.22 	P≤.029</td>
<td>r=.27 	P≤.008</td>
<td>r=.33 	P≤.004 </td>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="fig1-1049731512437557" position="float">
<label>Figure 1.</label>
<caption>
<p>Relationship between Objective Structured Clinical Examination (OSCE) Reflection scale scores and final practicum evaluation.</p>
</caption>
<graphic xlink:href="10.1177_1049731512437557-fig1.tif"/>
</fig>
<fig id="fig2-1049731512437557" position="float">
<label>Figure 2.</label>
<caption>
<p>Relationship between Objective Structured Clinical Examination (OSCE) Performance scale scores and final practicum evaluation.</p>
</caption>
<graphic xlink:href="10.1177_1049731512437557-fig2.tif"/>
</fig>
<fig id="fig3-1049731512437557" position="float">
<label>Figure 3.</label>
<caption>
<p>Relationship between Objective Structured Clinical Examination (OSCE) Reflection scale scores and final metacompetencies.</p>
</caption>
<graphic xlink:href="10.1177_1049731512437557-fig3.tif"/>
</fig>
<fig id="fig4-1049731512437557" position="float">
<label>Figure 4.</label>
<caption>
<p>Relationship between Objective Structured Clinical Examination (OSCE) Performance scale scores and final metacompetencies.</p>
</caption>
<graphic xlink:href="10.1177_1049731512437557-fig4.tif"/>
</fig>
<p>Significant associations were found between practicum instructor ratings of students at the midterm and OSCE scores on two subscales: the clinical relationship subscale of the practicum evaluation tool and the OSCE reflection professional development subscale (<italic>r</italic> = .26, <italic>p</italic> = .028) and the practicum communication subscale and reflection self-awareness and total scales (<italic>r</italic> = .23, <italic>p</italic> = .041; <italic>r</italic> = .32; <italic>p</italic> = .013). There was no significant association between OSCE scores and student self-appraisals in the practicum at either midterm or final evaluation times.</p>
</sec>
<sec id="section15-1049731512437557">
<title>Student Response to the OSCE</title>
<p>Students reported high levels of satisfaction with the method. The majority of students perceived that it was an effective assessment of their skills and that it was an effective learning tool that they would recommend to other students. These data are presented in <xref ref-type="table" rid="table2-1049731512437557">Table 2</xref>.</p>
<table-wrap id="table2-1049731512437557" position="float">
<label>Table 2.</label>
<caption>
<p>Student Response to OSCE</p>
</caption>
<graphic alternate-form-of="table2-1049731512437557" xlink:href="10.1177_1049731512437557-table2.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Effective Assessment</th>
<th>Integrate Knowledge</th>
<th>Improve Skills</th>
<th>Educational Tool</th>
<th>Continue to Use</th>
<th>Recommend</th>
<th>Overall Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean score</td>
<td>4.23</td>
<td>4.28</td>
<td>4.18 </td>
<td>4.47</td>
<td>4.46</td>
<td>4.50</td>
<td>4.33</td>
</tr>
<tr>
<td>% Agree or strongly agree</td>
<td>80.4</td>
<td>82.6</td>
<td>82.6</td>
<td>92.3</td>
<td>85.9</td>
<td>90.2</td>
<td>85.9</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1049731512437557">
<p>Note. OSCE = objective structured clinical examination.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section16-1049731512437557">
<title>Discussion and Applications to Social Work</title>
<p>Educators using competency-based frameworks across professions are searching for multiple methods to assess educational outcomes; methods that are authentic, reliable and valid, and also feasible to administer. Following pilot testing of the OSCE Adapted for Social Work, the University of Toronto Research Team modified the approach and used a one-station OSCE to assess students’ competence after completion of the first semester prior to entering the field practicum. The study found promising results of this assessment method as well as challenges.</p>
<p>The adapted OSCE format can be used to assess competence in a particular course, at the end of a semester, a program year, or the entire program. Since the holistic competence model we tested maps well onto the EPAS core competencies framework (<xref ref-type="bibr" rid="bibr7-1049731512437557">CSWE, 2008</xref>), social work educators may wish to use the conceptualization of competence and related scales. Specifically, the holistic competence model recognizes and assesses the interrelationships between foundational procedural competencies of engagement and assessment (2.1.10, a, b) and competencies related to professionalism (2.1.1), ethics (2.1.2), critical thinking (2.1.3), diversity (2.1.4), and applying knowledge (2.1.7). Through the assessment of two interrelated dimensions, performance of procedural competencies, and reflections which provide some evidence of metacompetencies, this OSCE format appears able to provide a measure of students’ ability to demonstrate skillful practice behavior and articulate the underlying set of knowledge and values that inform intentional practice. In this manner, it illuminates students’ integration of theory and practice and may provide an assessment method that could form part of a suite of tools.</p>
<p>The strength of the approach was evident in the ability of the method to capture a wide range of student competence on both the scale that assessed performance and that assessed written reflections on that performance. Educators are universally concerned about grade inflation both in academic courses and in the field practicum. For example, studies find limited variability and student scores clustered at the high end in social work field practicum evaluation (<xref ref-type="bibr" rid="bibr4-1049731512437557">Bogo et al., 2004</xref>; <xref ref-type="bibr" rid="bibr28-1049731512437557">Sowbel, 2011</xref>) and field instructors report the difficulty in assessment in the context of an intense dyadic relationship (<xref ref-type="bibr" rid="bibr5-1049731512437557">Bogo et al., 2007</xref>). The variability in student scores captured through these scales may be due to a number of factors; that raters were observing approximately 14 standardized interviews and structured written reflections thus having the chance to see numerous examples of competence (rather than only one as in field evaluations) and that raters did not know and hence were not affected by their relationships with the students.</p>
<p>Regarding student competence, Performance and Reflection scale scores were correlated pointing to the interrelated nature of metacompetencies, the ability to reflect on internal cognitive processes and subjective reactions in conceptualization of practice, with procedural competencies, intentionally using skillful behaviors. This holistic view of practice underscores the importance of classroom teachers and field instructors drawing explicit and aritculated links between theory, research, subjective reactions, and the use of skills. A competency-based educational framework has the potential to focus teaching in this direction.</p>
<p>The association between the OSCE scores and the Field Evaluation Tool was not straightforward. Students who did well on the OSCE also did well in the practicum, students who struggled in practicum also did poorly on OSCE. However, not all students who struggled on the OSCE had trouble in the practicum. Clearly, the simulation in the OSCE was able to identify students who then had difficulty in the actual world of practice. However, the OSCE also identified a number of “false positives.” This could be a result of the fact that some students simply do not perform well when under the pressure of an examination and observation by a rater and this would be a limitation of this assessment method. Or, some students may have been correctly assessed in the OSCE format and were able to learn and progress through the second semester as a result of their development, being in a field setting with actual clients, and/or being taught by a skillful field instructor. Also, it may be that the populations and problems represented in the OSCE (an older woman and a young mother) were not representative of those particular students encountered in their field setting.</p>
<p>Another interpretation of the “false-positive” finding regarding associations between the OSCE and field evaluation scores relates to the nature of field instructor evaluations. While confident in the properties of the field evaluation tool used in the study (<xref ref-type="bibr" rid="bibr23-1049731512437557">Regehr et al., 2011</xref>), we are not certain that field instructors actually observe student performance in their settings. Ratings on the practicum tool may relate more to metacompetencies—the way in which students reflect on, write about, and discuss their practice—than on their actual performance with clients. Support for this explanation comes from studies that found instructors rarely observe students’ practice (<xref ref-type="bibr" rid="bibr20-1049731512437557">Maidment, 2000</xref>; <xref ref-type="bibr" rid="bibr26-1049731512437557">Rogers &amp; McDonald, 1995</xref>). As well the main associations between the OSCE scores and field evaluation scores were between “meta competencies” and not actual skills. Metacompetencies are assessed in the OSCE through the reflection component and, we propose, in the field through the field instructor’s interaction with the student and review of written material. As a result, the two assessment approaches may tap into a stable construct, the student’s ability to reflect on and articulate the underpinnings of practice—both conceptual and subjective domains. Although in the OSCE, the client experience is simulated, the aspect being evaluated is the student’s ability to reflect on the experience. In this respect, it is similar enough to the experience in the field.</p>
<p>A final interpretation of this finding is that a one-station OSCE is not a valid assessment of student competence. Clearly for high stakes evaluations, such as those that determine readiness to graduate or license to practice, multiple scenarios are required and this is the case in related professions. Even in our five-station OSCE, we noted that skills are to some extent context specific and for greater generalizability of scores more stations are needed. And yet, social work programs generally do not have sufficient resources to mount extensive OSCE scenarios. As a result, they have relied on proxy measures such as written essays or tests. Field evaluations should more closely approach Gambrill’s proposal, offered a decade ago: “The best measure of students’ competence however, is their ability to effectively perform the core functions of the profession in practice situations” (<xref ref-type="bibr" rid="bibr9-1049731512437557">Gambrill, 2001</xref>). These evaluations, however, have also been fraught with difficulties as noted. OSCEs can provide a bridge between the authenticity of assessment of practice in real situations and traditional written assignments. Hence, it is with cautious optimism that we suggest the design and the testing of limited station OSCEs for social work.</p>
<p>Finally, it is important to note, the OSCE method adapted for social work described in this study was extremely well received by students. The great majority valued the experience not only as a method for fairly assessing their competence but also as contributing to their learning. This finding is similar to that in other studies where high enthusiasm and support is given by students and instructors (<xref ref-type="bibr" rid="bibr17-1049731512437557">Logie et al., in press</xref>).</p>
<p>Notwithstanding the limitations found, the OSCE method warrants further replication, adaptation, and investigation regarding its potential for social work educational outcomes assessment. Research and scholarship that examine and develop a rich array of reliable, valid, and feasible assessment methods will surely contribute to advancing our knowledge of effective pedagogy in social work.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This research was generously supported by a grant from the Social Sciences and Humanities Research Council of Canada.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn1-1049731512437557">
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn2-1049731512437557">
<p>The authors received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1049731512437557">
<citation citation-type="book">
<collab collab-type="author">ACGME</collab>. (<year>2010</year>). <source>Outcome project: Common program requirements: General competencies</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr2-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Austin</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>O'Byrne</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Pugsley</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Quero Munoz</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Development and validation processes for an objective structured clinical examination (OSCE) for entry-to-practice certification in pharmacy: The Canadian experience</article-title>. <source>American Journal of Pharmaceutical Education</source>, <volume>67</volume>, <fpage>76</fpage>.</citation>
</ref>
<ref id="bibr3-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Logie</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Katz</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Mylopoulos</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Adapting objective structured clinical examinations to assess social work students’ performance and reflections</article-title>. <source>Journal of Social Work Education</source>, <volume>47</volume>, <fpage>5</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr4-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Power</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Hughes</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Woodford</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Toward new approaches for evaluating student field performance: Tapping the implicit criteria used by experienced field instructors</article-title>. <source>Journal of Social Work Education</source>, <volume>40</volume>, <fpage>417</fpage>–<lpage>426</lpage>.</citation>
</ref>
<ref id="bibr5-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Power</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>When values collide: Providing feedback and evaluating competence in social work</article-title>. <source>Clinical Supervisor</source>, <volume>26</volume>, <fpage>99</fpage>–<lpage>117</lpage>.</citation>
</ref>
<ref id="bibr6-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Woodford</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Hughes</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Power</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Beyond competencies: Field instructors’ descriptions of student performance</article-title>. <source>Journal of Social Work Education</source>, <volume>42</volume>, <fpage>191</fpage>–<lpage>205</lpage>.</citation>
</ref>
<ref id="bibr7-1049731512437557">
<citation citation-type="book">
<collab collab-type="author">Council on Social Work Education (CSWE)</collab>. (<year>2008</year>). <source>Educational policy and accreditation standards</source> <edition>(rev. ed.)</edition>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.cswe.org/Accreditation/41865.aspx">http://www.cswe.org/Accreditation/41865.aspx</ext-link>
</citation>
</ref>
<ref id="bibr8-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fouad</surname>
<given-names>N. A.</given-names>
</name>
<name>
<surname>Hatcher</surname>
<given-names>R. L.</given-names>
</name>
<name>
<surname>Hutchings</surname>
<given-names>P. S.</given-names>
</name>
<name>
<surname>Collins</surname>
<given-names>F. L.</given-names>
<suffix>Jr.</suffix>
</name>
<name>
<surname>Grus</surname>
<given-names>C. L.</given-names>
</name>
<name>
<surname>Kaslow</surname>
<given-names>N. J.</given-names>
</name>
<name>
<surname>Crossman</surname>
<given-names>R. E</given-names>
</name>
</person-group>. (<year>2009</year>). <article-title>Competency benchmarks: A model for understanding and measuring competence in professional psychology across training levels</article-title>. <source>Training and Education in Professional Psychology</source>, <volume>3</volume>, <fpage>S5</fpage>–<lpage>S26</lpage>.</citation>
</ref>
<ref id="bibr9-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gambrill</surname>
<given-names>E.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Evaluating the quality of social work education: Options galore</article-title>. <source>Journal of Social Work Education</source>, <volume>37</volume>, <fpage>418</fpage>–<lpage>429</lpage>.</citation>
</ref>
<ref id="bibr10-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hackett</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Educating for competency and reflective practice: Fostering a conjoint approach in education and training</article-title>. <source>Journal of Workplace Learning</source>, <volume>13</volume>, <fpage>103</fpage>–<lpage>112</lpage>.</citation>
</ref>
<ref id="bibr11-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Harden</surname>
<given-names>R. M.</given-names>
</name>
<name>
<surname>Gleeson</surname>
<given-names>F. A.</given-names>
</name>
</person-group> (<year>1979</year>). <article-title>Assessment of clinical competence using an observed structured clinical examination</article-title>. <source>Medical Education</source>, <volume>13</volume>, <fpage>41</fpage>–<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr12-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hodges</surname>
<given-names>B. D.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>The objective structured clinical examination: Three decades of development</article-title>. <source>Journal of Veterinary Medical Education</source>, <volume>33</volume>, <fpage>571</fpage>–<lpage>577</lpage>.</citation>
</ref>
<ref id="bibr13-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kaslow</surname>
<given-names>N. J.</given-names>
</name>
<name>
<surname>Borden</surname>
<given-names>K. A.</given-names>
</name>
<name>
<surname>Collins</surname>
<given-names>F. L.</given-names>
</name>
<name>
<surname>Forrest</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Illfelder-Kaye</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Nelson</surname>
<given-names>P. D.</given-names>
</name>
<name>
<surname>&amp; Rallo</surname>
<given-names>J. S</given-names>
</name>
</person-group>. (<year>2004</year>). <article-title>Competencies conference: Future directions in education and credentialing in professional psychology</article-title>. <source>Journal of Clinical Psychology</source>, <volume>60</volume>, <fpage>699</fpage>–<lpage>712</lpage>.</citation>
</ref>
<ref id="bibr14-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kaslow</surname>
<given-names>N. J.</given-names>
</name>
<name>
<surname>Rubin</surname>
<given-names>N. J.</given-names>
</name>
<name>
<surname>Bebeau</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Leigh</surname>
<given-names>I. W.</given-names>
</name>
<name>
<surname>Lichtenberg</surname>
<given-names>J. W.</given-names>
</name>
<name>
<surname>Nelson</surname>
<given-names>P. D.</given-names>
</name>
<name>
<surname>Smith</surname>
<given-names>I. L</given-names>
</name>
</person-group>. (<year>2007</year>). <article-title>Guiding principles and recommendations for the assessment of competence</article-title>. <source>Professional Psychology: Research and Practice</source>, <volume>38</volume>, <fpage>441</fpage>–<lpage>451</lpage>.</citation>
</ref>
<ref id="bibr15-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kelly</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Horder</surname>
<given-names>W.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>The how and why: Competences and holistic practice</article-title>. <source>Social Work Education</source>, <volume>20</volume>, <fpage>689</fpage>–<lpage>699</lpage>.</citation>
</ref>
<ref id="bibr16-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leung</surname>
<given-names>W. C.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Competency based medical training: Review</article-title>. <source>British Journal of Medicine</source>, <volume>325</volume>, <fpage>693</fpage>–<lpage>695</lpage>.</citation>
</ref>
<ref id="bibr17-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Logie</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>G</given-names>
</name>
</person-group>. (<year>in press</year>). <article-title>The use of standardized clients in social work education: A critical synthetic review</article-title>. <source>Journal of Social Work Education</source>.</citation>
</ref>
<ref id="bibr18-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lu</surname>
<given-names>Y. E.</given-names>
</name>
<name>
<surname>Ain</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Chamorro</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Chang</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Feng</surname>
<given-names>J. Y.</given-names>
</name>
<name>
<surname>Fong</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Yu</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>A new methodology for assessing social work practice: The adaptation of the objective structured clinical evaluation (SW-OSCE)</article-title>. <source>Social Work Education</source>, <volume>30</volume>, <fpage>170</fpage>–<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr19-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lurie</surname>
<given-names>S. J.</given-names>
</name>
<name>
<surname>Mooney</surname>
<given-names>C. J.</given-names>
</name>
<name>
<surname>Lyness</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Measurement of the general competencies of the accreditation council for graduate medical education: A systematic review</article-title>. <source>Academic Medicine</source>, <volume>84</volume>, <fpage>301</fpage>–<lpage>309</lpage>.</citation>
</ref>
<ref id="bibr20-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Maidment</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Methods used to teach social work students in the field: A research report from New Zealand</article-title>. <source>Social Work Education</source>, <volume>19</volume>, <fpage>145</fpage>–<lpage>154</lpage>.</citation>
</ref>
<ref id="bibr21-1049731512437557">
<citation citation-type="book">
<collab collab-type="author">National Association of Social Workers</collab>. (<year>2007</year>). <source>The indicators for the achievement of the NASW standards for cultural competence in the social work profession</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>NASW Press</publisher-name>.</citation>
</ref>
<ref id="bibr22-1049731512437557">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Rawlings</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Assessing direct practice skill performance in undergraduate social work education using standardized clients and self-reported self-efficacy</source>. <publisher-loc>Cleveland, OH</publisher-loc>: <publisher-name>Case Western Reserve</publisher-name>.</citation>
</ref>
<ref id="bibr23-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Donovan</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Lim</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Evaluating a scale to measure student competencies in macro social work practice</article-title>. <source>Journal of Social Service Research</source>, <volume>38</volume>(1), <issue>(1)</issue>
<fpage>100</fpage>–<lpage>109</lpage>.</citation>
</ref>
<ref id="bibr24-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Development of an online practice-based evaluation tool for social work</article-title>. <source>Research on Social Work Practice</source>, <volume>21</volume>, <fpage>469</fpage>–<lpage>475</lpage>.</citation>
</ref>
<ref id="bibr25-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Regehr</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Bogo</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Regehr</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Power</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Can we build a better mousetrap? Improving measures of social work practice performance in the field</article-title>. <source>Journal of Social Work Education</source>, <volume>43</volume>, <fpage>327</fpage>–<lpage>343</lpage>.</citation>
</ref>
<ref id="bibr26-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rogers</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>McDonald</surname>
<given-names>P. L.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Expedience over education: Teaching methods used by field instructors</article-title>. <source>Clinical Supervisor</source>, <volume>13</volume>, <fpage>41</fpage>–<lpage>65</lpage>.</citation>
</ref>
<ref id="bibr27-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Snell</surname>
<given-names>L. S.</given-names>
</name>
<name>
<surname>Frank</surname>
<given-names>J. R.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Competencies, the tea bag model, and the end of time</article-title>. <source>Medical Teacher</source>, <volume>32</volume>, <fpage>629</fpage>–<lpage>630</lpage>.</citation>
</ref>
<ref id="bibr28-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sowbel</surname>
<given-names>L. R.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Gatekeeping in field performance: Is grade inflation a given?</article-title> <source>Journal of Social Work Education</source>, <volume>47</volume>, <fpage>367</fpage>–<lpage>377</lpage>.</citation>
</ref>
<ref id="bibr29-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Watson</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Stimpson</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Topping</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Porock</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Clinical competence assessment in nursing: A systematic review of the literature</article-title>. <source>Journal of Advanced Nursing</source>, <volume>39</volume>, <fpage>421</fpage>–<lpage>431</lpage>.</citation>
</ref>
<ref id="bibr30-1049731512437557">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wessel</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Williams</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Finch</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Gemus</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Reliability and validity of an objective structured clinical examination for physical therapy students</article-title>. <source>Journal of Allied Health</source>, <volume>32</volume>, <fpage>266</fpage>–<lpage>269</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>