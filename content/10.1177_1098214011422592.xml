<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AJE</journal-id>
<journal-id journal-id-type="hwp">spaje</journal-id>
<journal-title>American Journal of Evaluation</journal-title>
<issn pub-type="ppub">1098-2140</issn>
<issn pub-type="epub">1557-0878</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1098214011422592</article-id>
<article-id pub-id-type="publisher-id">10.1177_1098214011422592</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Values Engagement in Evaluation</article-title>
<subtitle>Ideas, Illustrations, and Implications</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Hall</surname>
<given-names>Jori N.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1098214011422592">1</xref>
<xref ref-type="corresp" rid="corresp1-1098214011422592"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ahn</surname>
<given-names>Jeehae</given-names>
</name>
<xref ref-type="aff" rid="aff2-1098214011422592">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Greene</surname>
<given-names>Jennifer C.</given-names>
</name>
<xref ref-type="aff" rid="aff2-1098214011422592">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-1098214011422592"><label>1</label>Department of Lifelong Education, Administration, and Policy, University of Georgia, Athens, GA, USA</aff>
<aff id="aff2-1098214011422592"><label>2</label>University of Illinois at Urbana–Champaign, Urbana, IL, USA</aff>
<author-notes>
<corresp id="corresp1-1098214011422592">Jori N. Hall, Department of Lifelong Education, Administration and Policy, University of Georgia, Athens, GA 30602, USA Email: <email>jorihall@uga.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2012</year>
</pub-date>
<volume>33</volume>
<issue>2</issue>
<fpage>195</fpage>
<lpage>207</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Evaluation Association</copyright-holder>
</permissions>
<abstract>
<p>Values-engagement in evaluation involves both describing stakeholder values and prescribing certain values. Describing stakeholder values is common practice in responsive evaluation traditions. Prescribing or advocating particular values is only <italic>explicitly</italic> part of democratic, culturally responsive, critical, and other openly ideological traditions in evaluation, but we argue that it is <italic>implicit</italic> in all evaluation approaches and practices. In this article, we discuss various conceptualizations of values-engagement in evaluation. We further present a specific form of values-engaged evaluation that is committed to descriptive and prescriptive valuing, with an emphasis on its prescriptive advancement of the values of inclusion and equity. Examples from field experience illustrate these two countenances and underscore the multiple challenges invoked by intentional engagement with the values dimensions of evaluation. The examples come from evaluations of science, technology, engineering, and mathematics (STEM) educational programs.</p>
</abstract>
<kwd-group>
<kwd>values-engagement</kwd>
<kwd>responsive evaluation</kwd>
<kwd>democratic evaluation</kwd>
<kwd>prescriptive valuing</kwd>
<kwd>STEM educational evaluation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Despite a general consensus in the evaluation community that “evaluation is an enterprise that involves valuing” (<xref ref-type="bibr" rid="bibr4-1098214011422592">Christie &amp; Alkin, 2008</xref>, p. 131), evaluators still grapple with acknowledging and actively engaging values in their work. Partly, this struggle involves understanding the nature and role of values in evaluation. In <italic>Foundations of Program Evaluation</italic>, <xref ref-type="bibr" rid="bibr24-1098214011422592">Shadish, Cook, and Leviton (1991)</xref> distinguish between approaches that describe the values that exist in the evaluation program and context, and those that prescribe particular values. Prescriptive theories have been characterized as giving evaluators an “intellectual authority that descriptive theories cannot match” (<xref ref-type="bibr" rid="bibr24-1098214011422592">Shadish, Cook, &amp; Leviton, 1991</xref>, p. 49), and further—though more controversially—giving evaluators some measure of moral authority and power.<sup><xref ref-type="fn" rid="fn1-1098214011422592">
1</xref></sup> Thus, the ways in which evaluators engage diverse stakeholder values and promote selected others represent key challenges for the field. With these concerns in mind, this article relates theoretical notions about values engagement to the practice of values engagement. We first describe how values in evaluation are understood by selected evaluation theorists. Then, we link this discussion to a particular values-engaged approach to evaluation, providing examples of the approach as applied to evaluations of science, technology, engineering, and mathematics (STEM) educational programs.</p>
<p>We call our approach “values-engaged” because it explicitly and intentionally involves both descriptions and prescriptions of values (<xref ref-type="bibr" rid="bibr8-1098214011422592">Greene, 2005</xref>; <xref ref-type="bibr" rid="bibr9-1098214011422592">Greene, DeStefano, Burgon, &amp; Hall, 2006</xref>). Our presentation emphasizes prescriptive engagement, as that is the more distinctive component, and it underscores the challenges inherent in enacting value commitments in practice.</p>
<sec id="section1-1098214011422592">
<title>Values in Evaluation: Theoretical Overview</title>
<p>Within the evaluation community, the role of values in evaluation typically refers to evaluators’ judgmental responsibility for assessing the merit or worth of the object being evaluated. Because the work of evaluation, at its core, emphasizes evaluator judgments about the value of a program based on a certain set of criteria, evaluation is broadly characterized as a values-laden enterprise (<xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>; <xref ref-type="bibr" rid="bibr23-1098214011422592">Scriven, 2003</xref>). <xref ref-type="bibr" rid="bibr24-1098214011422592">Shadish et al. (1991)</xref> suggest that describing values is a common part of evaluation because many evaluators seek out and describe stakeholder values and then use them, in addition to other criteria, to judge the merit of a program. Theoretically, this descriptive perspective on values follows the pluralistic responsive tradition in evaluation (<xref ref-type="bibr" rid="bibr25-1098214011422592">Stake, 2004</xref>). In the responsive tradition, evaluators intentionally surface and include the values and concerns of diverse stakeholders as threaded throughout the information gathered and interpreted as part of the evaluation process. Values are included, for example, in the descriptions of program experiences and in the judgments of its meaningfulness and consequence in the contexts being studied.</p>
<p>Responsive evaluation is grounded in a constructivist paradigm, which recognizes that the meanings of program practices are constructed through participants’ interpretations of interactions and dialogue. Further, those meanings represent values that are continuously being negotiated in practice (<xref ref-type="bibr" rid="bibr2-1098214011422592">Amba, 2006</xref>; <xref ref-type="bibr" rid="bibr5-1098214011422592">Everitt, 1996</xref>). This perspective situates evaluation as a process wherein the evaluator’s role is not only to describe various values and issues but also to create opportunities for diverse stakeholder views to be heard by others (<xref ref-type="bibr" rid="bibr10-1098214011422592">Guba &amp; Lincoln, 1989</xref>). Methodologically, being inclusive of and creating opportunities for varied stakeholders’ views to be heard can be enacted in multiple ways—for example, by gathering stakeholder experiences through participant observations, involving diverse stakeholders in the planning of the evaluation, or creating interim reports that feature descriptive accounts of stakeholder perspectives of the quality of the program (<xref ref-type="bibr" rid="bibr6-1098214011422592">Freeman, 2010</xref>; <xref ref-type="bibr" rid="bibr20-1098214011422592">Renger &amp; Bourdeau, 2004</xref>).</p>
<p>It is important to note that the methodologies employed to enact responsive practices, as <xref ref-type="bibr" rid="bibr7-1098214011422592">Greene (1997)</xref> and others have indicated, themselves advance values, in part vis-à-vis selected regulative ideals, or philosophical assumptions about what counts as knowledge. For example, a postpositivist framework or regulative ideal advances values of objectivity and neutrality, while a constructivist framework privileges particularity and insider perspectives on knowing. It is in this way, among others, that all evaluations advance some values. Specifically, this line of thinking undergirds responsive evaluators’ valuing of active stakeholder involvement in the meaning-making work of evaluation. Stakeholders are viewed as “information givers” and the evaluation process is influenced by the responsive evaluator’s relationships and conversations with stakeholders (<xref ref-type="bibr" rid="bibr2-1098214011422592">Amba, 2006</xref>, p. 33). Because each stakeholder may view the program differently and value it in different ways (<xref ref-type="bibr" rid="bibr5-1098214011422592">Everitt, 1996</xref>), the dialogue generated from conversations with stakeholders can surface prevailing or conflicting values and meanings (<xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>), greater self-awareness of values, and increased understanding of stakeholder values within and across groups. A primary issue related to the evaluator’s work of describing values is how best to manage the potential magnitude and variety of value stances generated. Should all stakeholders in the evaluation context be heard? Should the values generated be prioritized? And if so, how? In response, <xref ref-type="bibr" rid="bibr5-1098214011422592">Everitt (1996)</xref> suggests that a “theory of power and exclusion is needed to guide the evaluator in an effort to ensure that some stakeholders are not systematically excluded from evaluation processes” (p. 179). And <xref ref-type="bibr" rid="bibr12-1098214011422592">House and Howe (1999)</xref> advocate the principles of inclusion, dialogue, and deliberation as guideposts for stakeholder involvement and voice in evaluation. These sensibilities introduce the prescriptive perspective on valuing in evaluation.</p>
<p>Prescriptive valuing is anchored in the democratic evaluation tradition (<xref ref-type="bibr" rid="bibr9-1098214011422592">Greene et al., 2006</xref>) and, as <xref ref-type="bibr" rid="bibr24-1098214011422592">Shadish et al. (1991)</xref> state, “advocates the primacy of particular values” (p. 47). Prescriptive valuing in the democratic tradition includes evaluation models such as democratic evaluation (<xref ref-type="bibr" rid="bibr15-1098214011422592">MacDonald, 1976</xref>, <xref ref-type="bibr" rid="bibr16-1098214011422592">1977</xref>, <xref ref-type="bibr" rid="bibr17-1098214011422592">1978</xref>); deliberative democratic evaluation (<xref ref-type="bibr" rid="bibr11-1098214011422592">House, 1980</xref>; <xref ref-type="bibr" rid="bibr12-1098214011422592">House &amp; Howe, 1999</xref>); and inclusive evaluation (<xref ref-type="bibr" rid="bibr18-1098214011422592">Mertens, 1999</xref>). These models respect the descriptive tradition of responsive evaluation (<xref ref-type="bibr" rid="bibr25-1098214011422592">Stake, 2004</xref>), which legitimizes and respects the plurality of stakeholder values that are present in the evaluation context at hand, and explicitly seeks out these stakeholder values as part of the evaluation process. In addition, democratic models have a prescriptive component that is grounded in the view that evaluation inevitably promotes certain values and that democratic values, such as social justice and equity, are the most defensible values to promote (<xref ref-type="bibr" rid="bibr9-1098214011422592">Greene et al., 2006</xref>). As stated by <xref ref-type="bibr" rid="bibr12-1098214011422592">House and Howe (1999)</xref>, it is the inherent societal inequalities in stakeholder power and voice that justify the inclusion of and particular attention to “insider perspectives and the ‘voices’ of those who have been marginalized or excluded” (p. xviii).</p>
<p>Central to a prescriptive perspective is the critical assessment of values and practices in the evaluation context that promote or create obstacles for a more democratic society (<xref ref-type="bibr" rid="bibr6-1098214011422592">Freeman, 2010</xref>). Such an assessment “turns the evaluator’s attention to power and the powerful processes through which knowledge is constructed” (<xref ref-type="bibr" rid="bibr5-1098214011422592">Everitt, 1996</xref>, p. 180). In this light, the practice of evaluation is considered a political act (House, 2005) focused on answering the question: Whose interests are being served (by the program and by the evaluation) in this context? (<xref ref-type="bibr" rid="bibr5-1098214011422592">Everitt, 1996</xref>; <xref ref-type="bibr" rid="bibr21-1098214011422592">Schwandt, 1997</xref>). The answer to this question and others like it is attained by explicitly embracing democratic ideals through practices that enable all stakeholders to voice their views, that strive to level the playing field of power and influence within the spaces defined by the evaluation, that encourage open dialogue and deliberation among stakeholders from diverse standpoints, that judge program quality on democratic criteria, and more. <xref ref-type="bibr" rid="bibr6-1098214011422592">Freeman (2010)</xref> suggests that practicing evaluation in this way can “help participants unlearn harmful conceptions of self and others, while actively participating in constructing new forms of knowledge, including those of justice” (p. 2).</p>
<p>Although prescriptive models share commitments to stakeholder inclusion and an explicit valuing of some form of democratic ideals, each model does so in a particular way. For example, <xref ref-type="bibr" rid="bibr11-1098214011422592">House (1980)</xref> uses <xref ref-type="bibr" rid="bibr19-1098214011422592">Rawls’s (1971)</xref> justice theory to support prioritizing the interests of stakeholders with the least economic and political power in a given context (<xref ref-type="bibr" rid="bibr1-1098214011422592">Alkin, 2004</xref>; <xref ref-type="bibr" rid="bibr24-1098214011422592">Shadish et al., 1991</xref>). Yet, as <xref ref-type="bibr" rid="bibr24-1098214011422592">Shadish et al. (1991)</xref> point out, even when justified, a prescriptive theory of valuing must acknowledge the difficulties associated with prioritizing certain beliefs above others.</p>
<p>The issues raised by explicit valuing have long been considered and contested within the evaluation community. Some argue that evaluators who promote explicit ideological stances in their practice are endeavoring to advance goals that go beyond or are different from the goals of evaluation (<xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>). Moreover, explicit value commitments do not eliminate the need to make an evaluative judgment about program practices (<xref ref-type="bibr" rid="bibr5-1098214011422592">Everitt, 1996</xref>). In response to this concern, <xref ref-type="bibr" rid="bibr10-1098214011422592">Guba and Lincoln (1989)</xref> argued that explicit critical assessments of program values and practices be guided by a “moral imperative” principle, which assigns the evaluator moral authority for reaching judgments about interpretations of program quality. However, a criticism of this imperative is that it allocates unwarranted authority to the evaluator and becomes a form of domination itself (<xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>). That is, as the “authors behind the evaluation text,” evaluators can be accused of determining and dictating what is equitable and just (<xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>, p. 88), on questionable authoritative grounds.</p>
<p>Some commentators believe that the challenges of prescriptive valuing are offset by what society ultimately gains from it. Notably, these theorists argue that all evaluations play a role in political decision making (House, 2005) and that evaluators who articulate explicit value commitments are perhaps more mindful of their political engagement, role, and power (<xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>). Such evaluators are in a better position to “exercise one’s obligations as a citizen while simultaneously exercising one’s obligations as an evaluator” (<xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>, p. 95).</p>
</sec>
<sec id="section2-1098214011422592">
<title>A Values-Engaged Approach</title>
<p>Our conceptualization of values-engaged evaluation features both descriptive and prescriptive valuing, alongside concerted strategies to make the valuing strands of evaluation an explicit part of the process and the results. Following both the responsive and the democratic traditions, a primary value promoted in our approach is that of inclusion. We aspire to inclusively describe and engage the perspectives, concerns, and values of all legitimate stakeholders in the evaluation, with particular attention to ensuring inclusion of the interests, perspectives, and values of those traditionally unheard, underrepresented, or least well served in that context. Our commitment to inclusion is intended to acknowledge and respect the plurality of stakeholder perspectives and experiences present in the context, rather than to validate them.</p>
<p>More prescriptively and more importantly, we privilege and therefore seek to engage and advance the democratic ideal of equity. We define equity as being concerned with the treatment of program stakeholders, specifically, how well the evaluand affords program access, meaningful participation, and accomplishment for all relevant stakeholders. Our commitment to equity is enacted through generating evaluation questions, data, and dialogues related to the ways in which a program is attending appropriately and with meaningful consequence to all individuals and groups that are present in the context, particularly those least well served.</p>
<p>Embedded in this commitment is the understanding of diversity as multidimensional and contextual. We seek to understand what dimensions of diversity matter most and how those dimensions are defined or understood in the context at hand as a way to foreground equity. Diversity is thus engaged in the service of equity. Our criteria for judging program quality also explicitly include the advancement of the interests of underrepresented and underserved groups. That is, a good program is judged, in part, by how well it promotes the interests and accomplishments of those least well served.</p>
<p>What does engaging with values, especially prescriptively, look like in practice? What follows are stories from our recent work, in the form of case vignettes, on field testing these values-engaged concepts and practice guidelines in various STEM education contexts.</p>
</sec>
<sec id="section3-1098214011422592">
<title>Stories From the Field</title>
<p>Critical reflections on practice constitute a time-honored way in which evaluators examine the connections between evaluation theory and evaluation (<xref ref-type="bibr" rid="bibr3-1098214011422592">Chelimsky, 1998</xref>). Case study vignettes can focus on key events of a study to portray and explore attitudes and actions in context. Narrative vignettes are strongest in their ability to foster critical reflection and depict particular events, and weakest in the applicability of the theoretical insights gained to other contexts. The cases we employ have additional characteristics that both contribute to their usefulness for reflective purposes, as well as constrain the generality of theoretical insights and claims.</p>
<p>The cases come from research-on-evaluation projects, funded specifically by National Science Foundation (NSF) to explore the conceptualization and field testing of ideas related to values engagement. A stance of critical reflection accompanied our field testing in real time, affording closer and more detailed connections between theoretical insights and concrete field experiences. At the same time, the cases all represent small-scale, local evaluations of STEM education programs, thus possibly limiting the applicability of our insights to evaluations of similar scale. Further, some of our field test evaluations were initiated by the evaluators, often in sync with an extant evaluation requirement for that evaluand, but still diffusing somewhat the authority for the evaluation and its possible consequences in that local context.</p>
<p>While our conceptualizations of values engagement and its practical meanings continue to be refined through field testing, the premises of values engagement raise questions and implications for all strands of our evaluation activity. The following three examples<sup><xref ref-type="fn" rid="fn2-1098214011422592">
2</xref></sup> from our field testing experiences illustrate the potentials and challenges inherent in the values-engaged evaluator’s role. The first is an initiative conducted as a way to enact our descriptive value stance. The second involves “a missed opportunity” for promoting values of equity, and the third presents our responses to the challenges invoked by this missed opportunity.</p>
<sec id="section4-1098214011422592">
<title>Personalizing Evaluation: A Strategy for Descriptive Values Engagement</title>
<p>The values engagement strategy at the center of this example was influenced by <xref ref-type="bibr" rid="bibr13-1098214011422592">Kushner (2000)</xref>. His views honor inclusiveness and aim for descriptive valuing in terms of how a program is personally valued by participants, specifically, how it is manifested in meaningful ways in their lives. For example, instead of asking how well students perform in an educational program, Kushner asks how well the program “performs” or shows up in students’ lives. This type of values engagement Kushner refers to as “personal inquiry.” We believed that personal inquiry could be particularly beneficial in bringing voice to underserved youth in our STEM contexts.</p>
<sec id="section5-1098214011422592">
<title>Program and evaluation overview</title>
<p>NanoDARE is an innovative program, centered on active student participation in actual laboratory research in a Nanolab on a university campus. NanoDARE sessions are facilitated by a professor from the Department of Chemical and Biomolecular Engineering at the university, a postdoctoral fellow assigned to work with the professor and two science educators. The program seeks to involve high school juniors and seniors from groups underrepresented in scientific fields who have aptitudes and interests in STEM and potential to pursue these subject areas in higher education and in their careers.</p>
<p>The evaluation team initially conducted an evaluation of the NanoDARE program. This evaluation provided valuable feedback regarding the quality and meaningfulness of the program experience and program accomplishments, and the ways in which students from groups traditionally underrepresented in the STEM disciplines were meaningfully included in the program. Evaluative findings centered on four main aspects of NanoDARE: program instruction, motivation for program participation, peer and instructor relationships, and content (nanoscience) knowledge. The seven youth participants varied in their characterizations of their program experiences. For example, nearly all mentioned that they enjoyed the experience but with different degrees of enthusiasm. Among the most enthusiastic were two youth who highlighted the excitement of the experimental nature of the program. One observed, “I like labs because they are hands-on. They are fun while also stimulating the mind. In the lab, I can actually see it happening, rather than just read or hear about it.” In contrast, another youth found the experimental part of NanoDARE somewhat “boring” and repetitious. This youth was disappointed that the program was not as conceptually “involved,” as he had wished for “more of the theory stuff.” When asked about the contributions of NanoDARE to their college and career ambitions, several students reported that their interest in science, engineering, or nanotechnology was “reaffirmed” by the experience. Ironically, as a result of his participation, one youth was seriously considering a general liberal arts school for his undergraduate work and then concentrating on science in graduate school.</p>
</sec>
<sec id="section7-1098214011422592">
<title>Situational context</title>
<p>To further understand the youths’ program experiences and the ways in which the program connected to their lives, four<sup><xref ref-type="fn" rid="fn3-1098214011422592">3</xref></sup> youth were interviewed a year after their participation in NanoDARE. A primary purpose of the interviews was to assess longer-term program outcomes. To make this personal inquiry activity more interactive, the evaluator created theme cards based on the four aspects that framed the findings of the NanoDARE program evaluation (i.e., instruction, motivation, relationships, and content knowledge). One theme was featured on each card. Each theme was represented by direct quotes taken from the youth participants’ prior evaluative interviews.</p>
<p>The participant was asked to review a theme card and then elaborate about how his or her program experience related to the quotes on the card. Next, to stimulate a more values-engaged dialogue, the evaluator asked such questions as, what was important to you about nanoscience instruction? And, what did you care about most concerning program participation? These questions were followed up by a probe focusing on reasons why the youth thought it was important, valuable, meaningful, and so on. Each interview concluded with questions that asked how the program currently connected with the participant’s life in general and/or other STEM educational experiences. A major assumption undergirding this activity was that the questioning would effectively surface participants’ values related to the program, particularly how the program showed up in their lives.</p>
</sec>
<sec id="section8-1098214011422592">
<title>Reflections</title>
<p>The theme cards with narrative text were a helpful starting point for the interviews, and facilitated students’ recall of their experiences with the program. For example, when asked about program instruction, most of the youth stated that they valued having the autonomy to solve problems in the lab on their own but also liked having the instructors available to provide more guidance when needed. Interviewees also shared that the NanoDARE program informed their decision about which college to attend, and the possibility of pursuing a STEM-related career. One former participant noted that NanoDARE “changed my opinion about going to this university. I was willing to go to this university after having gone through NanoDARE and developing a relationship with [a science educator].” Another youth stated that the program “helped me realize what I don’t want to do. I mean I enjoy it [engineering] a lot, but it’s not something I want to do with the rest of my life.” Reflecting on the program’s influence, one youth mentioned: “I’ve actually adapted one of the methods that we used in the NanoDARE program to assist with my experiments in my chemistry lab.” This same youth further explained that “participating in the program allowed me to have different points of view from others with whom I am now working, [which] has been helpful to solve problems within the lab environment I currently work.”</p>
<p>Because the evaluator used indirect questions to elicit participants’ underlying values, interpreting their responses was challenging. For example, when asked to elaborate on what was important about program instruction, participants used language that signaled what they liked or what they would have preferred, which was interpreted as what they valued. Thus, the comment “I love working by myself on things” was seen as valuing individuality. Similarly, another former participant mentioned that “I did not get a chance to work with the other students,” which was regarded as valuing collaboration. However, these responses were not necessarily straightforward value statements because they were typically made in reference to context-specific situations. That is, the values related to the youths’ experiences could not be separated from explanations about why something was valued and under what circumstances. Consequently, naming values was time-consuming, complex, and messy. Even so, questioning youth about their experiences and probing them about what the program meant to them helped determine the perceived importance of the program. From this vantage point, a personal inquiry approach was useful in going beyond a simple “inventorying of stakeholders’ perspectives” (<xref ref-type="bibr" rid="bibr22-1098214011422592">Schwandt, 2008</xref>, p. 147). In particular, this strategy has the potential to meaningfully interact with stakeholders concerning what matters most to them, and to further understand how their interests are stifled or advanced in a program.</p>
<p>Through this approach (and others), we have learned more about descriptive valuing, specifically the usefulness of asking such questions as, “What is important to you about …?” We have also learned that it is imperative to stay close to stakeholders’ views when practicing descriptive valuing, specifically, by using their own voices through quotes and by attending carefully to context when naming value statements. At the same time, we appreciate the challenges involved in interpreting and representing values, and welcome more practice in this domain.</p>
</sec>
</sec>
<sec id="section9-1098214011422592">
<title>“Missed Opportunities” for Prescriptive Values Engagement: Looking Back and Looking Ahead</title>
<p>Summer Math Academy (SMA) is a summer school mathematics program designed to prepare rising sixth-, seventh-, and eighth-grade African American students for successful participation in advanced mathematics courses the following year. According to school administrators, the aim of the program is to change normative school culture about who is an advanced math student by expanding African American student enrollment in advanced math. Our evaluation team had conducted two previous values-engaged evaluations of the SMA program that focused on assessing the program’s design, implementation, and short-term outcomes. On the whole, our evaluations were well received by the program staff and school administrators. Most notably, our evaluative feedback helped restructure the content of the program to be more intentionally aligned with the state’s learning standards.</p>
<p>The primary purpose of this third (and last) evaluation of the SMA was to provide feedback to the program administrators and staff regarding participant selection and recruitment processes. To this end, the evaluation was guided, in part, by the following questions: How are students identified for the program? and What are the students’ and staff’s understandings and perceptions of the recruitment process used to identify participants for the program?</p>
<sec id="section10-1098214011422592">
<title>Situational context</title>
<p>After completing the evaluation, we were invited by program leaders to present key findings to the school’s math department. We gladly accepted this invitation, as this was in line with our agenda of providing opportunities, through evaluation reporting, for stakeholders to dialogue about the meanings and implications of the findings, as well as related underlying value claims. Attending this meeting were the school’s math teachers, including the two leaders of the summer math program, as well as teachers not involved in the SMA. The latter group was, in the words of a program leader, “in the dark about the program.” We began our presentation with a brief program description, followed by key evaluation findings related to student recruitment. Our presentation included skits offering concrete scenarios that we have learned can encourage stakeholders to engage with key issues and their attendant values more readily and safely than conventional presentations.</p>
<p>In particular, in response to a question asking for their thoughts on the SMA’s current recruitment procedures “What kind of student constitutes a ‘good fit’ for the program, and how should such students be identified?” several teachers, none of whom had been directly involved in the program, raised questions, and concerns about the program’s exclusive focus on African American students. “Having a program based on race is problematic … The goal should be to have more kids in advanced math, not just more African Americans” was one such concern. Another teacher said, “It’s a problem targeting and focusing only on African American students.” Yet another commented, “Maybe we need to draw attention to better math learning for all students, rather than African American students’ math learning in the summer, because there are kids, non-African Americans, who are denied such opportunity ….” We did not venture any response to these comments.</p>
<p>
<italic>Reflections.</italic> This venue provided (in hindsight) an opportunity for us to engage and enact our equity vision and value commitments. However, we were unable to pursue the issues at the time, partly because of the time constraints but mainly because we were uncertain on what grounds and with what authority we could speak up to promote equity issues, especially when doing so might entail our challenging some of the views and values of certain stakeholders. We were also wary of creating disruption in a context in which we neither live nor work on a daily basis. What could have been done in that context, and with what authority could we have done it? Our inaction left us pondering about how we might facilitate conversations among program stakeholders about their different values and assumptions, while calling particular attention to the concept of equity. We also wondered what prior interactions and actions might have been needed for us to gain legitimacy and authority to promote our agenda of values engagement in this situation?</p>
<p>In reflecting on this issue, the team generated several possible strategies. The first was to appeal to the broader issue of fairness in STEM education to gain authority to address equity concerns, even using NSF’s own championship of diversity in support of this effort. Students from groups underrepresented in STEM fields do not always have the same opportunities to study and excel in math as their majority peers. Therefore, it is legitimate, important, and even necessary, given our society’s democratic ideal of equity, to have programs such as SMA to promote minority participation in advanced mathematics. Drawing on these issues, we could have reiterated and reinforced the official mission of the program at the meeting and explain the appropriateness of privileging African American students in program targeting.</p>
<p>Another strategy was to turn to the program leaders appealing to their authority to establish our grounding for promoting equity. We knew from our interactions with them that they were highly supportive of, and even enthusiastic about, the SMA. For example, we could have asked the leaders to counter some of the questions and concerns other teachers raised at the meeting regarding the program’s exclusive focus on African American students. However, this option, while seemingly viable, would have required a degree of contextual understanding on our part—particularly related to stakeholder dynamics—that we simply did not have. In retrospect, we were relieved that we had not turned to program leaders in this fashion, as it might have disrupted extant relationships or unfairly targeted leaders.</p>
<p>A third idea was to build our authority through more transparent statements of our equity commitments throughout the evaluation. We could have openly and explicitly stated our values-engaged agenda from the outset and repeated this agenda in ongoing communications across stakeholder groups throughout the evaluation process.</p>
<p>Responding to the SMA evaluation, we pursued this third idea and focused our subsequent field tests on exploring various methods to convey our values-engaged framework more clearly. In particular, we pursued approaches to stakeholder communications to make our equity-oriented intentions more transparent. In the next section, we share lessons learned from these efforts by examining key practice decisions and actions from a subsequent evaluation of a high school math program.</p>
</sec>
</sec>
</sec>
<sec id="section12-1098214011422592">
<title>Forewarned is Forearmed: Prescriptive-Values Engagement Revisited</title>
<p>Hillside High is a public high school serving a diverse student body that includes a considerable number of underrepresented and underserved students from surrounding rural areas. The Hillside mathematics department offers a wide variety of courses, ranging from pre-algebra to calculus. While our evaluation of the school’s math program was not required by the school district or any outside agency, school administrators and math teachers stated that they wished to learn more about the role of diversity in their math program.</p>
<p>Within this context, we conceptualized our evaluation around issues of diversity and equity in STEM education and tried to be more explicit in maintaining a clear focus on equity throughout most of our key evaluation activities, interactions, and communications. The main stated purpose of the evaluation, for example, was to assess how well the structure and content of the school’s math program engaged and supported diverse students’ math learning, with a special focus on students who were least well served. At Hillside High, these were the students who entered high school lacking the skills and knowledge for high school–level math, and were thus considered “most at risk” for not succeeding in mathematics and thereby for not graduating from high school. Underlying the framing of our evaluation in this way was the understanding that, while success in mathematics is critical for many STEM opportunities and accomplishments, students from groups traditionally underrepresented and underserved in STEM fields do not always experience the same opportunities to study and excel in math as their majority peers.</p>
<p>Within these equity-oriented objectives, we directed our evaluative planning, data gathering, analysis, reporting, and communication in ways that engaged issues of equity—again defined as equal opportunities in program access, experience, and accomplishment. Our evaluation plan stated explicitly our commitment “The evaluation aims to investigate the structure and content of the math pathways at Hillside High, with a focus on equity of access to and learning opportunities in mathematics across the whole diversity of the student body.” The key evaluation questions asked and criteria used for judging program quality addressed how well the school’s math program served to advance the interests and well-being of those students placed at risk: “How and with what rationale are math pathways at Hillside High structured? In what ways does the structure support equitable opportunities to learn across the diversity of the school’s student population?” Our evaluation used a mix of methods, including a student school culture survey, interviews with students and teachers, and document analysis, in order to inclusively capture and represent a wide diversity of perspectives, experiences, and values of key program stakeholders. In particular, we made special efforts to include students placed at risk in this context by making multiple observation visits to pre-algebra classes, where most of these students were placed.</p>
<p>Perhaps, the most visible representation of our value commitments was our final evaluation report. We structured and presented evaluation findings by issues or themes, within which different stakeholder perspectives and experiences were featured. Within the “program content and pedagogy” theme, for example, the views of both teachers and students in different math classes were reported. In addition, different data sets, such as student and teacher interviews, were often presented side by side to facilitate “dialogues” between distinct program experiences and perspectives. We also included detailed and specific descriptions of the structure and routines of lower-level math classes and higher-level/honors math classes in order to convey varied patterns of student participation and engagement (see <xref ref-type="table" rid="table1-1098214011422592">Table 1</xref>
).</p>
<table-wrap id="table1-1098214011422592" position="float">
<label>Table 1.</label>
<caption>
<p>Excerpts From the Two Vignettes Presented Side by Side in the Final Report of the Hillside High Mathematics Evaluation to Convey the Varied Patterns of Student Participation and Engagement in Different Math Classes. Each Vignette Represents a Composite of Student Experiences as Observed and Expressed in the Lower-Level Math Courses, and in the Higher-Level/Honors Math Courses, Respectively
</p>
</caption>
<graphic alternate-form-of="table1-1098214011422592" xlink:href="10.1177_1098214011422592-table1.tif"/>
<table>
<thead>
<tr>
<th>Lower-Level Math Classes</th>
<th>Higher-Level/Honors Math Classes</th>
</tr>
</thead>
<tbody>
<tr>
<td>On the whole, students in these classes, particularly in pre-algebra, were disconnected, disengaged, and/or disruptive. The atmosphere of the classroom was one of getting through the class with as little trouble as possible. Student disruptions were continuous and constant, and as such, a considerable amount of class time was spent on the teacher making repeated and persistent efforts to “discipline” acting-out students, telling them to “sit in [their] own seats, stop talking, focus and/or pay attention….”</td>
<td>In general, the majority of the students in these classes were well behaved, attentive, and fully engaged, which in turn brought about interactive and collaborative–competitive class atmosphere. Most students were highly attentive to the day’s lesson, taking notes intently during the lecture and actively responding to the teacher’s questions and directions. For the most part, the teacher had no problems eliciting students’ participation in the class or getting them to stay on task and complete the assignments….</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Most distinctively, a large portion of the report was dedicated to key value dimensions of the evaluation results. This section featured the multiple and diverse values held by various stakeholders in relation to their particular program experiences and specifically highlighted values related to diversity and equity at Hillside High. For example, our final report included discussions of “school climate at Hillside in relation to diversity”; teachers’ and students’ understandings of the importance of mathematics learning; the ways in which diverse students, especially those placed at risk, are engaged and supported in their math learning; and the challenges of providing such supports at Hillside. The report also included a clearly labeled section on “equity and diversity,” where we reiterated and reemphasized the importance of and need for increased diversity and equity in STEM fields, in light of the key evaluation issues raised.</p>
<p>Finally, in terms of our broader communication and relational strategies, we engaged in ongoing reporting and discussions with math teachers throughout the evaluation process, always inviting their comments, feedback, and questions. We often reserved time during staff meetings and/or lunch periods to further pursue equity issues surrounding the students most underserved and at risk. For example, we shared an excerpt from our observations in pre-algebra classes and elicited teacher conversation about it. At the closing of our evaluation, we presented our evaluation report to the Hillside math department during a math faculty meeting. The intention was to provide a safe space and time for teachers to dialogue about the evaluation findings, their underlying value claims, and possible action implications.</p>
<p>We do not claim extensive use of our evaluation work at Hillside High, in part due to the absence of an external mandate for the project. However, we do believe that our concerted efforts to consistently, transparently, and respectfully communicate our values-engaged agenda—from the very beginning of the evaluation—provided the requisite authority for this agenda. The Hillside math teachers maintained their participation in the evaluation, found some of our results and conversations with them to be thoughtful and meaningful, and initiated their own conversations about alternatives to pre-algebra for those students placed most at risk of not succeeding in their math classrooms.</p>
</sec>
<sec id="section13-1098214011422592">
<title>Opportunities, Challenges, and Future Directions</title>
<p>Although the opportunities and challenges presented in this work are not unique to our values-engaged approach, our extended field testing of these ideas has reinforced the inherent values-laden character of evaluation. More specifically, the work has fostered understanding of what constitutes valuing in evaluation, produced promising pathways to better values-engaged practice, and facilitated progress on justifications for an explicit focus on values in evaluation. In this final section, we explore each of these accomplishments.</p>
<p>First, in the process of thinking about our own emphases on inclusion and equity, and especially in trying to actually advance them in our own evaluation practice, we often found ourselves swimming in values. We became sensitized to how many evaluation practice decisions reflect choices among alternative value stances. To illustrate from the STEM field tests, framing evaluation questions meant emphasizing either teacher values about program quality (e.g., having math content that was engaging and activities that promoted student collaboration) or administrator values about program quality (e.g., having content that meets state standards in mathematics in order to address the accountability demands of the No Child Left Behind policy<sup><xref ref-type="fn" rid="fn4-1098214011422592">4</xref></sup>).</p>
<p>Determining criteria for judging program quality clearly invoked value stances and claims, notably in defining a “good” program experience and “good” outcomes. Our values-engaged approach, for example, partly defined a good program as one that advanced the academic interests and well-being of students least well served in that context. Data collection, interpretation, and especially representation involved multiple decisions about what results and which perspectives were most important in that context. Accordingly, beyond our written evaluation reports (which were constructed to reflect our value commitments), we experimented with more dialogic formats of evaluation reporting. Our intent was to engage diverse stakeholders in conversations about the results and their implications, with a focus on the values involved. The NanoDARE descriptive values engagement strategy (theme cards with former participants) as well as the SMA feedback session with teachers were examples of such experimentation. Through all of these experiences and others, we have come to more clearly understand the saturation of evaluation practice with values and to appreciate the importance of evaluators explicating and justifying the value commitments advanced in their own practice (<xref ref-type="bibr" rid="bibr4-1098214011422592">Christie &amp; Alkin, 2008</xref>; <xref ref-type="bibr" rid="bibr7-1098214011422592">Greene, 1997</xref>; <xref ref-type="bibr" rid="bibr24-1098214011422592">Shadish et al., 1991</xref>).</p>
<p>Second, our field testing identified a number of practical strategies for values engagement, at least with the particular values of inclusion and equity (closely backed by a valuing of diversity). In the NanoDARE evaluation, descriptive values engagement was facilitated by Kushner’s “personalizing evaluation” focus, which asks how the program shows up in the lives of participants and is related to what matters to them. Evaluators can initiate these discussions with questions such as “What’s important to you about this program? In what ways does this program connect to other areas of your life?” We also noted that descriptive-values engagement challenges the evaluator in terms of interpretation and representation of values, a challenge to which we responded by retaining respondents’ own words as support for any values ascriptions we offered. In addition, the field testing of these ideas signaled the importance of going beyond simply gathering stakeholder perspectives. This includes an authentic consideration for and meaningful engagement with stakeholder values and issues as well as creating opportunities for their voices to be heard by others in the evaluation (<xref ref-type="bibr" rid="bibr10-1098214011422592">Guba &amp; Lincoln, 1989</xref>). And from our SMA evaluation and conversations with teachers, our written and dialogic reporting strategies now more clearly identify our commitment to equity and engage stakeholders in interactions surrounding evaluation results from this values-explicit stance.</p>
<p>Third, questions remain regarding the authoritative role of the evaluator to engage particular values. The discomfort we experienced during our SMA feedback conversation with the school’s math teachers well conveys the persistence and the importance of these questions. Yet, as we have noted, evaluators cannot help but advance particular values in the work they do because it is a values-laden endeavor. These values may be those of objectivity and political neutrality, or of accountability, learning, or social critique (<xref ref-type="bibr" rid="bibr6-1098214011422592">Freeman, 2010</xref>). We have offered a glimpse of one set of value commitments in the context of STEM education evaluation. Led by NSF, these commitments are supported by the broader context of STEM education, and such support helped us to justify our particular commitments to inclusion and equity, alongside more general appeals to democratic ideals. Other evaluators may find parallel justifications for their value commitments in the contexts in which they work.</p>
<p>Further field testing and actual use of these ideas are needed in a variety of contexts and programs—in particular, high-stakes contexts—in order to both challenge and refine our current understanding of values engagement and its contributions to the power of evaluation to contribute to constructive social and educational change.</p>
<p>In the end, our experience, both conceptually and practically, highlights the need for attention to the relational and communicative aspects of evaluation, specifically, evaluation’s interpersonal interactions, including the language, attitudes, and procedures drawn upon to develop and sustain trusting relationships, convey commitments, state objectives, share ideas, express interpretations, make decisions, and the like for particular evaluative purposes. These aspects influence the character of values engagement. Such attention positions evaluation as a moral–political practice, rather than a mere methodological undertaking (<xref ref-type="bibr" rid="bibr5-1098214011422592">Everitt, 1996</xref>; House, 2005; <xref ref-type="bibr" rid="bibr12-1098214011422592">House &amp; Howe, 1999</xref>; <xref ref-type="bibr" rid="bibr14-1098214011422592">Mabry, 2010</xref>; <xref ref-type="bibr" rid="bibr22-1098214011422592">Schwandt, 2008</xref>). Focusing on these aspects has reinforced our belief that evaluators must assume responsibility for explicating and justifying the values being advanced in their work, in ways that respect other values and thus other evaluation approaches and evaluators. Ultimately, then, this article is not a promotion of our specific values-engaged approach per se but rather a challenge to evaluators to embrace the valuing anchor of our profession as we engage the pluralistic contexts within which we work.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict" id="fn5-1098214011422592">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn6-1098214011422592">
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: Evaluation Research and Evaluation Capacity grant No.0535793 from the National Science Foundation.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<title>Notes</title>
<fn fn-type="other" id="fn1-1098214011422592">
<label>1.</label>
<p>For example, one invited critic of our values-engaged approach wondered aloud, “Who made you God?” (F. Lawrenz, personal communication, June 12, 2006).</p>
</fn>
<fn fn-type="other" id="fn2-1098214011422592">
<label>2.</label>
<p>All the names used in the examples are pseudonyms.</p>
</fn>
<fn fn-type="other" id="fn3-1098214011422592">
<label>3.</label>
<p>All seven participants were invited to conduct the personal inquiry interviews; however, only four provided consent.</p>
</fn>
<fn fn-type="other" id="fn4-1098214011422592">
<label>4.</label>
<p>The No Child Left Behind Act of 2001 is a U.S. policy designed to hold public schools accountable for increases in student achievement by requiring states to develop academic standards and to assess students’ progress on those standards via annual testing. See the U.S. Department of Education’s website for more details <ext-link ext-link-type="uri" xlink:href="http://www.2.ed.gov/nclb/landing.jhtml">http://www.2.ed.gov/nclb/landing.jhtml</ext-link>.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="editor"><name><surname>Alkin</surname><given-names>M. C. </given-names></name></person-group> (Ed.) (<year>2004</year>). <source>Evaluation roots</source>. <publisher-loc>Thousand Oaks</publisher-loc>: <publisher-name>SAGE</publisher-name>.
</citation>
</ref>
<ref id="bibr2-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Amba</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>The practice of politics of responsive evaluation</article-title>. <source>American Journal of Evaluation</source>, <volume>27</volume>, <fpage>31</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr3-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chelimsky</surname>
<given-names>E.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>The role of experience in formulating theories of evaluation practice</article-title>. <source>American Journal of Evaluation</source>, <volume>19</volume>, <fpage>35</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr4-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Christie</surname>
<given-names>C. A.</given-names>
</name>
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Evaluation theory tree re-examined</article-title>. <source>Studies in Educational Evaluation</source>, <volume>34</volume>, <fpage>131</fpage>–<lpage>135</lpage>.</citation>
</ref>
<ref id="bibr5-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Everitt</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Developing a critical evaluation</article-title>. <source>Evaluation</source>, <volume>2</volume>, <fpage>173</fpage>–<lpage>188</lpage>.</citation>
</ref>
<ref id="bibr6-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="editor"><name><surname>Freeman</surname><given-names>M. </given-names></name></person-group> (Ed.). (<year>2010</year>). <article-title>Critical social theory and evaluation practice</article-title>. <source>New Directions for Evaluation</source>, <volume>127</volume>, <fpage>1</fpage>–<lpage>6</lpage>.
</citation>
</ref>
<ref id="bibr7-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Greene</surname>
<given-names>J. C.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Evaluation as advocacy</article-title>. <source>Evaluation Practice</source>, <volume>18</volume>, <fpage>25</fpage>–<lpage>35</lpage>.</citation>
</ref>
<ref id="bibr8-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Greene</surname>
<given-names>J. C.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>A Value-Engaged Approach for Evaluating the Bunche-Da Vinci Learning Academy</article-title>. <source>New Directions for Evaluation</source>, <volume>106</volume>, <fpage>27</fpage>–<lpage>45</lpage>.</citation>
</ref>
<ref id="bibr9-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Greene</surname>
<given-names>J. C.</given-names>
</name>
<name>
<surname>DeStefano</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Burgon</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Hall</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>An educative, values-engaged approach to evaluating STEM educational programs</article-title>. <source>New Directions for Evaluation</source>, <volume>109</volume>, <fpage>53</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr10-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Guba</surname>
<given-names>E. G.</given-names>
</name>
<name>
<surname>Lincoln</surname>
<given-names>Y. S.</given-names>
</name>
</person-group> (<year>1989</year>). <source>Fourth generation evaluation</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr11-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>House</surname>
<given-names>E. R.</given-names>
</name>
</person-group> (<year>1980</year>). <source>Evaluating with validity</source>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr12-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>House</surname>
<given-names>E. R.</given-names>
</name>
<name>
<surname>Howe</surname>
<given-names>K. R.</given-names>
</name>
</person-group> (<year>1999</year>). <source>Values in evaluation and social research</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr13-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kushner</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2000</year>). <source>Personalizing evaluation</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr14-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mabry</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Critical social theory evaluation: Slaying the dragon</article-title>. <source>New Directions for Evaluation</source>, <volume>127</volume>, <fpage>83</fpage>–<lpage>98</lpage>.</citation>
</ref>
<ref id="bibr15-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>MacDonald</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1976</year>). <article-title>Evaluation and the control of education</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Tawney</surname>
<given-names>D.</given-names>
</name>
</person-group> (Ed.), <source>Curriculum evaluation today: Trends and implications</source> (pp. <fpage>125</fpage>–<lpage>136</lpage>). <publisher-loc>London, England</publisher-loc>: <publisher-name>MacMillan Education</publisher-name>.</citation>
</ref>
<ref id="bibr16-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>MacDonald</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1977</year>). <article-title>The portrayal of persons as evaluation data</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Nigel</surname>
<given-names>N.</given-names>
</name>
</person-group> (Ed.), <source>Safari: Theory in practice</source> (pp. <fpage>50</fpage>–<lpage>67</lpage>). <publisher-loc>Norwich, England</publisher-loc>: <publisher-name>Centre for Applied Research in Education, University of East Anglia</publisher-name>.</citation>
</ref>
<ref id="bibr17-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author"><name><surname>MacDonald</surname><given-names>B. </given-names></name></person-group> (<year>1978</year>). <source>Evaluation and democracy</source>. <publisher-name>Public address at the University of Alberta Faculty of Education</publisher-name>, <publisher-loc>Edmonton, Canada</publisher-loc>.
</citation>
</ref>
<ref id="bibr18-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mertens</surname>
<given-names>D. M.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Inclusive evaluation: Implications of transformative theory for evaluation</article-title>. <source>American Journal of Evaluation</source>, <volume>20</volume>, <fpage>1</fpage>–<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr19-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Rawls</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1971</year>). <source>A theory of justice</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Renger</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Bourdeau</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Strategies for values inquiry: an exploratory case study</article-title>. <source>American Journal of Evaluation</source>, <volume>25</volume>, <fpage>39</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr21-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Schwandt</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>1997</year>). <source>Qualitative inquiry: A dictionary of terms</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr22-1098214011422592">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schwandt</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Educating for intelligent belief in evaluation</article-title>. <source>American Journal of Evaluation</source>, <volume>29</volume>, <fpage>139</fpage>–<lpage>150</lpage>.</citation>
</ref>
<ref id="bibr23-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Scriven</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Evaluation in the new millennium: The transdisciplinary vision</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Donaldson</surname>
<given-names>S. I.</given-names>
</name>
<name>
<surname>Scriven</surname>
<given-names>M.</given-names>
</name>
</person-group> (Eds.), <source>Evaluating social programs and problems Visions for the New Millennium</source> (pp. <fpage>19</fpage>–<lpage>42</lpage>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr24-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Shadish</surname>
<given-names>W. R.</given-names>
</name>
<name>
<surname>Cook</surname>
<given-names>T. D.</given-names>
</name>
<name>
<surname>Leviton</surname>
<given-names>L. C.</given-names>
</name>
</person-group> (<year>1991</year>). <source>Foundations of program evaluation: Theories of practice</source>. <publisher-loc>Newbury Park, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr25-1098214011422592">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Stake</surname>
<given-names>R. E.</given-names>
</name>
</person-group> (<year>2004</year>). <source>Standards-based and responsive evaluation</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr26-1098214011422592">
<citation citation-type="web">
<collab collab-type="author">U.S. Department of Education</collab>. (<year>2010</year>). <source>No Child Left Behind</source>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.2.ed.gov/nclb/landing.jhtml">http://www.2.ed.gov/nclb/landing.jhtml</ext-link>
</citation>
</ref>
</ref-list>
</back>
</article>