<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ALH</journal-id>
<journal-id journal-id-type="hwp">spalh</journal-id>
<journal-title>Active Learning in Higher Education</journal-title>
<issn pub-type="ppub">1469-7874</issn>
<issn pub-type="epub">1741-2625</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1469787412441284</article-id>
<article-id pub-id-type="publisher-id">10.1177_1469787412441284</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>How effective are self- and peer assessment of oral presentation skills compared with teachers’ assessments?</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>De Grez</surname><given-names>Luc</given-names></name>
<aff id="aff1-1469787412441284">University College Brussels, Belgium</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Valcke</surname><given-names>Martin</given-names></name>
<aff id="aff2-1469787412441284">University of Ghent, Belgium</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Roozen</surname><given-names>Irene</given-names></name>
<aff id="aff3-1469787412441284">University College Brussels, Belgium</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-1469787412441284">Luc De Grez, University College Brussels, Stormstraat 2, B1000 Brussels, Belgium Email: <email>luc.degrez@hubrussel.be</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2012</year>
</pub-date>
<volume>13</volume>
<issue>2</issue>
<fpage>129</fpage>
<lpage>142</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Assessment of oral presentation skills is an underexplored area. The study described here focuses on the agreement between professional assessment and self- and peer assessment of oral presentation skills and explores student perceptions about peer assessment. The study has the merit of paying attention to the inter-rater reliability of the teachers. Comparison of the teacher and peer assessment rubric scores points at a positive relationship, but also at critical differences. The lower intra-class correlation suggests that peers and teachers still interpret the criteria and indicators of the rubric in a different way. With regard to the comparison of self-assessment scores and teacher scores, we have to conclude that there are significant differences between these scores. Self-assessment scores are, for the most part, higher than the marks given by teachers. The results also reflect a very positive attitude of students towards peer assessment as a relevant source of external feedback.</p>
</abstract>
<kwd-group>
<kwd>assessment</kwd>
<kwd>oral presentation skills</kwd>
<kwd>peer assessment</kwd>
<kwd>self-assessment</kwd>
<kwd>student perception</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1469787412441284">
<title>Quality of assessment</title>
<p>Recent approaches to assessment stress the learning potential of assessment (<xref ref-type="bibr" rid="bibr47-1469787412441284">Taras, 2008</xref>), particularly formative assessment, which is defined as ‘assessment that is specifically intended to provide feedback on performance to improve and accelerate learning’ (<xref ref-type="bibr" rid="bibr33-1469787412441284">Nicol and Milligan, 2006</xref>: 64). Some consider this a key quality of assessment and regard it as the ‘consequential validity’ of assessment (<xref ref-type="bibr" rid="bibr19-1469787412441284">Gielen et al., 2003</xref>). Consequential validity sits alongside the two other traditional psychometric qualities of an assessment: reliability and validity. According to <xref ref-type="bibr" rid="bibr30-1469787412441284">Messick (1994)</xref> consequential validity is one of the six aspects of his unified concept of validity. Involvement of students in assessment can be organized in two ways: self-assessment and peer assessment. In peer assessment, according to <xref ref-type="bibr" rid="bibr15-1469787412441284">Falchikov (2005</xref>: 27), ‘. . . students use criteria and apply standards to the work of their peers in order to judge that work’. Building on this, we state that in self-assessment students use criteria and apply standards to judge their own work. Both self- and peer assessment are expected to decrease the central role of the teacher in assessment activities. During recent decades, there has been an increase in the implementation of self- and peer assessment in higher education learning environments (<xref ref-type="bibr" rid="bibr40-1469787412441284">Segers et al., 2003</xref>). Despite this increased interest, formative assessment in higher education is still largely controlled by teachers (<xref ref-type="bibr" rid="bibr32-1469787412441284">Nicol and Macfarlane-Dick, 2006</xref>).</p>
</sec>
<sec id="section2-1469787412441284">
<title>The theoretical position of self- and peer assessment in a self-regulated learning process</title>
<p>In this study a socio-cognitive theoretical perspective towards self-regulated learning is adopted as a theoretical basis for oral presentation skills instruction (<xref ref-type="bibr" rid="bibr2-1469787412441284">Bandura, 1997</xref>; <xref ref-type="bibr" rid="bibr39-1469787412441284">Schunk, 2001</xref>). This choice builds on the literature that links the instruction of oral presentation skills to observational learning (<xref ref-type="bibr" rid="bibr2-1469787412441284">Bandura, 1997</xref>). Via observational learning, learners compare their performance or the performance of others with more or less explicit standards of a good oral presentation. The oral presentation skills will evolve by achieving a better match between these standards and the current performance level (<xref ref-type="bibr" rid="bibr38-1469787412441284">Sadler, 1989</xref>). We adopt the term <italic>calibration</italic> to refer to the match between an internal evaluation and a standard (<xref ref-type="bibr" rid="bibr53-1469787412441284">Winne, 2004</xref>). Both internal and external sources of feedback are helpful to foster the calibration process to attain higher performance levels in the context of productive self-regulated learning (<xref ref-type="bibr" rid="bibr53-1469787412441284">Winne, 2004</xref>). The calibration activity can be fostered by providing opportunities for self-assessment.</p>
<p>External feedback from peers can play a comparable role (<xref ref-type="bibr" rid="bibr48-1469787412441284">Topping, 1998</xref>). An accurate calibration of oral presentation performance and the standards suggests that a sufficient level of reliability can be attained when comparable assessment results are reported by a teacher/expert, by peers, or by the learner. Self- and peer assessment result in a more active involvement of students in their own learning process (Ozogul and Sullivan, 2007). A student who always expects teachers to present a judgment will develop to a lesser extent a self-assessment orientation (<xref ref-type="bibr" rid="bibr4-1469787412441284">Boud and Falchikov, 2006</xref>). From a self-regulated learning point of view, it is however critical to develop self-observation skills that help to compare the information gathered via self-observation with a performance goal. Sub-processes related to self-observation and self-judgment are important. They are regarded as the steps in a learning monitoring process that helps learners to bring their behaviour in line with their performance and goals (<xref ref-type="bibr" rid="bibr39-1469787412441284">Schunk, 2001</xref>). Next to self-assessment, peer assessment was also found to have positive effects on domain-specific and on peer assessment skills (<xref ref-type="bibr" rid="bibr52-1469787412441284">Van Zundert et al., 2009</xref>). <xref ref-type="bibr" rid="bibr50-1469787412441284">Topping (2009)</xref> explains this by linking peer assessment to the provision of immediate, individualized and richer feedback. Since this feedback is formative in nature, it has a clear potential of fostering the subsequent learning process (<xref ref-type="bibr" rid="bibr22-1469787412441284">Hattie, 2009</xref>).</p>
<p>Analysis of the assessment of oral presentation skills mainly results in an overview of studies about self- and peer assessment of individual (oral) presentation skills (<xref ref-type="bibr" rid="bibr1-1469787412441284">AlFallay, 2004</xref>; <xref ref-type="bibr" rid="bibr6-1469787412441284">Campbell et al., 2001</xref>; <xref ref-type="bibr" rid="bibr8-1469787412441284">Cheng and Warren, 2005</xref>; <xref ref-type="bibr" rid="bibr20-1469787412441284">Hafner and Hafner, 2003</xref>; <xref ref-type="bibr" rid="bibr23-1469787412441284">Hughes and Large, 1993</xref>; <xref ref-type="bibr" rid="bibr27-1469787412441284">Langan et al., 2005</xref>; <xref ref-type="bibr" rid="bibr28-1469787412441284">Langan et al., 2008</xref>; <xref ref-type="bibr" rid="bibr29-1469787412441284">Magin and Helmore, 2001</xref>; <xref ref-type="bibr" rid="bibr34-1469787412441284">Oldfield and Macalpine, 1995</xref>; <xref ref-type="bibr" rid="bibr35-1469787412441284">Patri, 2002</xref>; <xref ref-type="bibr" rid="bibr41-1469787412441284">Sellnow and Treinen, 2004</xref>). In some studies, the research focuses only partly on self- and peer assessment (<xref ref-type="bibr" rid="bibr16-1469787412441284">Fallows and Chandramohan, 2001</xref>). In a minor number of cases, group presentations have been assessed (<xref ref-type="bibr" rid="bibr25-1469787412441284">Kerby and Romine, 2009</xref>).</p>
<p>In general, research about self- and peer assessment of oral presentation skills reveals under-explored areas and diverging views. Moreover, the use of very different samples and different assessment instruments makes it difficult to compare the findings of those studies. The study by <xref ref-type="bibr" rid="bibr1-1469787412441284">AlFallay (2004)</xref>, for instance, involved students in applied sciences enrolled in an English language programme, while the study by <xref ref-type="bibr" rid="bibr35-1469787412441284">Patri (2002)</xref> involved Chinese students, and <xref ref-type="bibr" rid="bibr6-1469787412441284">Campbell et al. (2001)</xref> American students. These results indicate that more research is needed regarding self- and peer assessment of oral presentation skills.</p>
</sec>
<sec id="section3-1469787412441284">
<title>Benefits of self- and peer assessments</title>
<p><xref ref-type="bibr" rid="bibr15-1469787412441284">Falchikov (2005</xref>: 16) hypothesizes that ‘involving students in the assessment of presentations is extremely beneficial’ for developing self-regulating skills. Students are expected to analyse their own behaviour and develop a better understanding of the nature of quality criteria. <xref ref-type="bibr" rid="bibr8-1469787412441284">Cheng and Warren (2005)</xref> cite several studies that reported improved presentation performance as a result of peer assessment. Other studies adopted videotaped feedback for self-assessments, and also reported the attainment of improved oral presentation skills (<xref ref-type="bibr" rid="bibr5-1469787412441284">Bourhis and Allen, 1998</xref>).</p>
<p><xref ref-type="bibr" rid="bibr48-1469787412441284">Topping (1998)</xref> dedicates part of his review of the literature about peer assessment to the assessment of oral presentation skills. He summarizes improvements in marks, perceived higher learning performance, higher presentation confidence (self-efficacy), and the development of appraisal skills. In addition, <xref ref-type="bibr" rid="bibr49-1469787412441284">Topping (2003)</xref> mentions the economic benefits of adopting self- and peer assessment. Shifting part of the responsibilities for assessment and feedback from the teacher to the student has – next to educational benefits – also benefits in terms of reducing teaching workload.</p>
</sec>
<sec id="section4-1469787412441284">
<title>Inter-rater reliability of self- and peer assessments</title>
<p>There is considerable debate about the inter-rater reliability of self- and peer assessments (<xref ref-type="bibr" rid="bibr50-1469787412441284">Topping, 2009</xref>). <xref ref-type="bibr" rid="bibr49-1469787412441284">Topping (2003)</xref> points to the widespread approach in discussions of the reliability of self- and peer assessments compared with assessment by teachers, stressing that the a priori assumption that assessment by a teacher is more reliable and more valid can be doubted in some contexts. This a priori assumption relates to a positivist epistemological perspective about assessment (<xref ref-type="bibr" rid="bibr14-1469787412441284">Elton and Johnston, 2002</xref>). There is little research that actually tested this assumption, and sometimes disagreement between tutors is mentioned (<xref ref-type="bibr" rid="bibr28-1469787412441284">Langan et al., 2008</xref>).</p>
<p><xref ref-type="bibr" rid="bibr17-1469787412441284">Freeman (1995)</xref> concludes that there is no significant difference in the overall mark averages given by peers and those given by teachers. In contrast, <xref ref-type="bibr" rid="bibr27-1469787412441284">Langan et al. (2005)</xref> report that peer marks are on average 5% higher than marks given by their tutors. Other correlational studies conclude that peer assessment can be a relevant substitute for assessments by teachers (<xref ref-type="bibr" rid="bibr1-1469787412441284">AlFallay, 2004</xref>; <xref ref-type="bibr" rid="bibr6-1469787412441284">Campbell et al., 2001</xref>; <xref ref-type="bibr" rid="bibr23-1469787412441284">Hughes and Large, 1993</xref>; <xref ref-type="bibr" rid="bibr34-1469787412441284">Oldfield and Macalpine, 1995</xref>; <xref ref-type="bibr" rid="bibr35-1469787412441284">Patri, 2002</xref>). Nevertheless, <xref ref-type="bibr" rid="bibr23-1469787412441284">Hughes and Large (1993)</xref> warn that a high correlation between marks of peers and teachers can still hide a considerable variation in the marks. <xref ref-type="bibr" rid="bibr17-1469787412441284">Freeman (1995)</xref> reports only a moderate correlation between peer and teacher scores. He also reports that the standard deviation of marks given by peers was half the value of the standard deviation in scores given by teachers (see also <xref ref-type="bibr" rid="bibr23-1469787412441284">Hughes and Large, 1993</xref>). <xref ref-type="bibr" rid="bibr8-1469787412441284">Cheng and Warren (2005)</xref> add to this that student average marks are within one standard deviation of teacher marks, but they point out that students did not always assess the same elements or criteria as their teachers did. <xref ref-type="bibr" rid="bibr24-1469787412441284">Kappe (2008)</xref> found that students were able to provide a reliable overall assessment but needed additional training to provide reliable marks on specific criteria of oral presentations. <xref ref-type="bibr" rid="bibr20-1469787412441284">Hafner and Hafner (2003)</xref> adopted regression analysis showing a significant positive functional relationship between instructor and mean peer scores and found that students come to a strong agreement in the final ranking of their scores.</p>
<p>Fewer studies are found that compare teacher assessment with self-assessment of oral presentation skills, and it is clear that results are not univocal. Some studies report lower correlations values between self- and teacher assessments than between teacher and peer assessment (<xref ref-type="bibr" rid="bibr6-1469787412441284">Campbell et al., 2001</xref>; <xref ref-type="bibr" rid="bibr28-1469787412441284">Langan et al., 2008</xref>; <xref ref-type="bibr" rid="bibr35-1469787412441284">Patri, 2002</xref>). Nevertheless, others consider self-assessment to be as valid as peer assessment (<xref ref-type="bibr" rid="bibr1-1469787412441284">AlFallay, 2004</xref>; <xref ref-type="bibr" rid="bibr20-1469787412441284">Hafner and Hafner, 2003</xref>).</p>
</sec>
<sec id="section5-1469787412441284">
<title>Variables affecting the quality of self- and peer assessment of oral presentations</title>
<p>A continuous debate is observed about the reliability of self- and peer assessment in relation to the development of presentation skills. In the context of peer assessment, rating errors and the impact of student perceptions of peer assessment is a key topic for discussion. Rating errors are central in the study of <xref ref-type="bibr" rid="bibr45-1469787412441284">Sluijsmans et al. (2001)</xref>, who refer to personal differences in standards and rating styles, and the extent to which peers distribute grades and have different opinions about the rating tasks. Student perceptions are stated to have a considerable influence on student learning (<xref ref-type="bibr" rid="bibr46-1469787412441284">Struyven et al., 2003</xref>). Concerns have been raised about resulting difficulties in peer assessment contexts (<xref ref-type="bibr" rid="bibr21-1469787412441284">Hanrahan and Isaacs, 2001</xref>). Results show that students were concerned about their inexperience in marking, they felt uncomfortable critiquing others’ work and remarked that their marking input was not taken seriously because it was not considered when calculating the final mark. Students also complained about the time-consuming nature of the activity and asked for feedback on their involvement in the assessment.</p>
<p>Only a small number of studies explore the views that students hold about peer assessments of oral presentation skills. The findings of <xref ref-type="bibr" rid="bibr8-1469787412441284">Cheng and Warren (2005)</xref> showed that students reflected a low level of comfort in a peer assessment situation, and a low degree of confidence in their personal peer assessment skills. This suggests that low self-efficacy levels for peer assessment skills can affect the nature and quality of peer assessment. <xref ref-type="bibr" rid="bibr27-1469787412441284">Langan et al. (2005)</xref> point at obvious problems with anonymity when building on peer assessment of oral presentations. Lack of anonymity may lead to assessment bias. These authors also detected gender effects and found that peers rated students from the same university slightly higher than students from other universities. <xref ref-type="bibr" rid="bibr15-1469787412441284">Falchikov (2005</xref>: 154) cites a 1999 study of Lapham and Webster, who mention bias when peers are asked to mark seminar presentations. Lastly, <xref ref-type="bibr" rid="bibr41-1469787412441284">Sellnow and Treinen (2004)</xref> report that neither the gender of the presenter, nor the gender of the assessor, affects overall peer ratings.</p>
<p>As to self-assessment, a meta-analysis of <xref ref-type="bibr" rid="bibr15-1469787412441284">Falchikov (2005)</xref> indicates that some, but clearly not all, students are able to assess in the way that teachers apply assessment criteria. This is confirmed by a study of <xref ref-type="bibr" rid="bibr26-1469787412441284">Kruger and Dunning (1999)</xref> where novices and low performers overestimate their performance level and lack related metacognitive abilities (monitoring, evaluation). <xref ref-type="bibr" rid="bibr37-1469787412441284">Rust et al. (2003)</xref> and <xref ref-type="bibr" rid="bibr28-1469787412441284">Langan et al. (2008)</xref> reported that women are more likely to underestimate their performance, whereas males tend to overestimate the quality of their performance in a self-assessment context.</p>
</sec>
<sec id="section6-1469787412441284">
<title>Improving the quality of self- and peer assessments</title>
<p>There is no unequivocal answer as to how the quality of self- and peer assessments of oral presentation skills can be improved. Earlier research focused on the critical value of assessment training, the feasibility of student-based assessment, the nature of the assessment criteria and the scoring approach. <xref ref-type="bibr" rid="bibr20-1469787412441284">Hafner and Hafner (2003)</xref> state that providing training is not sufficient. In addition, <xref ref-type="bibr" rid="bibr7-1469787412441284">Carlson and Smith-Howell (1995)</xref> found hardly any differences in assessment practices between untrained and trained teachers. Others conclude that peers need training in view of peer assessment (<xref ref-type="bibr" rid="bibr1-1469787412441284">AlFallay, 2004</xref>; <xref ref-type="bibr" rid="bibr6-1469787412441284">Campbell et al., 2001</xref>; <xref ref-type="bibr" rid="bibr17-1469787412441284">Freeman, 1995</xref>; <xref ref-type="bibr" rid="bibr35-1469787412441284">Patri, 2002</xref>; <xref ref-type="bibr" rid="bibr43-1469787412441284">Sluijsmans, 2002</xref>). <xref ref-type="bibr" rid="bibr27-1469787412441284">Langan et al. (2005)</xref> found that marks awarded by students who participated in preliminary discussions about the assessment criteria were significantly lower than the marks of students who were not involved in these initial discussions. Compensating for low self-efficacy related to peer assessment during these discussions was a key point in the study of <xref ref-type="bibr" rid="bibr16-1469787412441284">Fallows and Chandramohan (2001)</xref>. <xref ref-type="bibr" rid="bibr31-1469787412441284">Miller (2003)</xref> concluded that a larger number of items in the evaluation checklist resulted in an increase in variance in scores. This diminishes inter-rater reliability but, on the other hand, provides students with more detailed and thus better feedback. In contrast, <xref ref-type="bibr" rid="bibr17-1469787412441284">Freeman (1995)</xref> suggests reducing the number of criteria in the checklist, diminishing the quality of feedback generated by the assessment. <xref ref-type="bibr" rid="bibr28-1469787412441284">Langan et al. (2008)</xref> recommend adopting short sessions in order to minimize loss of concentration.</p>
<p>To conclude, many questions remain unanswered. The main problem is the lack of empirical research that directs specific practices in the field of self- and peer assessment (<xref ref-type="bibr" rid="bibr44-1469787412441284">Sluijsmans, 2008</xref>). Building on the theoretical and empirical basis outlined above, we put forward the following research questions:</p>
<list id="list1-1469787412441284" list-type="bullet">
<list-item><p>What is the level of agreement between undergraduate students’ peer assessments and the assessments of university teachers in relation to oral presentations?</p></list-item>
<list-item><p>What is the level of agreement between self-assessments and assessments by university teachers?</p></list-item>
<list-item><p>What are the student perceptions about peer assessment?</p></list-item></list>
</sec>
<sec id="section7-1469787412441284">
<title>Research design</title>
<sec id="section8-1469787412441284">
<title>Participants</title>
<p>Oral presentations and answers on a questionnaire were collected from 57 university freshmen students (21 females and 36 males) enrolled for a Business Administration introductory course on psychology. The research was set up during the first semester. Administration of the questionnaire was carried out at the end of the semester (December). Initially, 73 students participated in the study but, owing to illnesses, incompatibility of roster, internships or other reasons among the students, 16 students dropped out. The reasons for drop-out were deemed not to be systematic. The average age among the 57 students who participated was 18 years. Informed consent was obtained from all participants, but they were not informed about the nature of the research questions.</p>
</sec>
<sec id="section9-1469787412441284">
<title>Research instruments</title>
<sec id="section10-1469787412441284">
<title>Assessment instrument for oral presentation performance</title>
<p>In a preliminary study (<xref ref-type="bibr" rid="bibr12-1469787412441284">De Grez et al., 2009b</xref>), six existing assessment scales to judge the quality of an oral presentation were analysed by four experienced higher education teachers acquainted with the knowledge domain. On the basis of the results of semi-structured interviews with these experts, a rubric was developed consisting of nine oral presentation evaluation criteria: three content-related criteria (quality of introduction, structure and conclusion), five criteria about the nature of the delivery (eye contact, vocal delivery, enthusiasm, interaction with the audience and body language), and a general quality criterion (professionalism). Descriptors and indicators were added to support the use of the assessment criteria in the rubric. These were improved after application of the rubric by trained assessors who judged the quality of more than 300 oral presentations (for a detailed description, see <xref ref-type="bibr" rid="bibr12-1469787412441284">De Grez et al., 2009b</xref>).</p>
<p>On the basis of a factor analysis, two evaluation components can be distinguished when applying the nine criteria: content-related criteria and delivery-related criteria (see <xref ref-type="table" rid="table1-1469787412441284">Table 1</xref>). One item, labelled ‘professionalism’, loads on both components (<xref ref-type="bibr" rid="bibr12-1469787412441284">De Grez et al., 2009b</xref>).</p>
<table-wrap id="table1-1469787412441284" position="float">
<label>Table 1.</label>
<caption>
<p>Components found on the basis of the principal components analysis, and loadings</p>
</caption>
<graphic alternate-form-of="table1-1469787412441284" xlink:href="10.1177_1469787412441284-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Component 1: Content</th>
<th align="left">Component 2: Delivery</th>
</tr>
</thead>
<tbody>
<tr>
<td>Introduction (.72)</td>
<td>Contact with audience (.73)</td>
</tr>
<tr>
<td>Structure (.84)</td>
<td>Enthusiasm (.76)</td>
</tr>
<tr>
<td>Conclusion (.77)</td>
<td>Eye contact (.70)</td>
</tr>
<tr>
<td>Professionalism (.59)</td>
<td>Vocal delivery (.54)</td>
</tr>
<tr>
<td/>
<td>Body language (.82)</td>
</tr>
<tr>
<td/>
<td>Professionalism (.71)</td>
</tr>
</tbody>
</table></table-wrap>
<p>University teachers and peer assessors were asked to rate the quality of an oral presentation on the basis of the rubric. For each criterion a five-point Likert scale was used. Descriptors and indicators are provided to support the assessment process.</p>
<p>As an example, we describe the assessment related to the criterion ‘quality of the introduction’. Assessors are invited to consider three indicators to score this criterion:</p>
<list id="list2-1469787412441284" list-type="bullet">
<list-item><p>Grasps the attention of the audience with the first sentences.</p></list-item>
<list-item><p>Gives a goal or central idea of the presentation in the introduction.</p></list-item>
<list-item><p>Gives an idea of the structure of the presentation in the introduction.</p></list-item></list>
<p>The score reflects the extent to which the quality of the introduction meets none, one or more of the three indicators.</p>
</sec>
<sec id="section11-1469787412441284">
<title>Perception of peer assessment and of the learning process</title>
<p>A subscale focusing on ‘perceptions of peer assessment’ was adopted from the seven-item questionnaire (<italic>α</italic> = .74) developed and validated by <xref ref-type="bibr" rid="bibr43-1469787412441284">Sluijsmans (2002)</xref>. One item that was context-specific for the <xref ref-type="bibr" rid="bibr43-1469787412441284">Sluijsmans (2002)</xref> study was omitted from the scale, and some words were changed in order to adapt the subscale to the specific oral presentation situation (for example ‘You can learn from the feedback of peers’ and ‘I think students should be able to assess each other’). The scale, as used in this study, reflects a good reliability (<italic>α =</italic> .80). Respondents were also asked how much they learned from seven instructional components (on a ten-point Likert scale).</p>
</sec>
<sec id="section12-1469787412441284">
<title>Teachers and peer assessors</title>
<p>The oral presentations (recorded) were assessed by five different assessors (two male, three female) on the basis of the assessment rubric as explained above. Four of these assessors were faculty members with at least five years experience of language teaching but who had not taught the students being assessed. The fifth assessor was a junior researcher. These assessors (and their assessments) are labelled as teachers in this article. As well as the teachers, 47 students were involved as peer assessors. These students did not belong to the same group as those being assessed. They were enrolled in the second year of a Business Administration course (32 males) and participated in the study as a formal part of their course on communication skills. Both the teachers and the peer assessors were unaware of the nature of the research questions. All the teachers received some short training (45 minutes on average) about the nature and use of the assessment rubric. Peer assessors received, as part of their formal instruction programme, an introduction to oral presentation skills and the use of the evaluation rubric. They had extensive experience in using the assessment instrument, as part of their communication course.</p>
</sec></sec>
<sec id="section13-1469787412441284">
<title>Procedure</title>
<p>The students in the research sample were, as a formal part of their psychology course, invited to deliver three short (on average three minutes) oral presentations about a prescribed topic. Topics were of similar level of difficulty: my town, my high school, my university. All the presentations were recorded. In the research setting, as well as the presenter, those present included an audience consisting of two other participants (but always a different pair), and the first author. A camera, recording the session, was positioned in an unobtrusive location. As a result of the drop-out of a number of participants for the second or third presentation, the final number of recordings was 209 instead of 219 (73 students, 3 oral presentations each).</p>
<p>After the first presentation, students participated individually in a computer-based multimedia training programme about oral presentations (see <xref ref-type="bibr" rid="bibr11-1469787412441284">De Grez et al. (2009a)</xref> for more details). After the second presentation, students received feedback on their first presentation, based on the scores for the nine assessment criteria (see below). The intervention was spread over nine lesson weeks.</p>
<sec id="section14-1469787412441284">
<title>Assessors and the assessment procedure</title>
<p>The evaluation of the oral presentations by the teachers and peer assessors was based on video recordings. None of the assessors was aware whether they were assessing a recording of a first, a second or a third oral presentation. Recorded presentations were assigned randomly to assessors. <xref ref-type="table" rid="table2-1469787412441284">Table 2</xref> indicates the allocation of specific assessors to specific oral presentations.</p>
<table-wrap id="table2-1469787412441284" position="float">
<label>Table 2.</label>
<caption>
<p>Summary of the assessment procedure</p>
</caption>
<graphic alternate-form-of="table2-1469787412441284" xlink:href="10.1177_1469787412441284-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Assessor</th>
<th align="left">Presentation 1</th>
<th align="left">Presentation 2</th>
<th align="left">Presentation 3</th>
<th align="left">Total number of assessments</th>
<th align="left">Average number of assessed presentations for one assessor</th>
<th align="left">Number of assessors for one presentation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Teachers</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>209</td>
<td>41.8</td>
<td>1</td>
</tr>
<tr>
<td>Peers</td>
<td>X</td>
<td/>
<td/>
<td>174</td>
<td>3.7</td>
<td>6</td>
</tr>
<tr>
<td>Presenters</td>
<td>X</td>
<td>X</td>
<td/>
<td>79</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table></table-wrap>
<p>Each teacher individually evaluated between 34 and 49 of the total number of 209 oral presentations. For each oral presentation, scores were required for the nine rubric criteria. Student peers assessed 29 presentations. Each of these 29 presentations was assessed by six different peers. The choice of this specific number is based on the work of <xref ref-type="bibr" rid="bibr20-1469787412441284">Hafner and Hafner (2003)</xref>, who reported a large improvement in reliability when moving from a single rater to about five raters, and on the work of <xref ref-type="bibr" rid="bibr10-1469787412441284">Dannefer et al. (2005)</xref>, who concluded that at least six peers are needed to achieve a moderate reliability when assessing professional competences. As part of the research design, participants were also asked to rate their own presentation on the basis of the assessment rubric. One third of the participants rated their first presentation and all the participants rated their second presentation. The rubric was introduced as part of the multimedia instruction package. Therefore, we assume that students were sufficiently acquainted with the rubric criteria in view of the self-assessment activity.</p>
<p>The consistency between scores of different raters is central to the concept of reliability. There is much debate about ways to calculate or estimate the inter-correlations between assessors. We calculated intra-class correlation coefficients, as suggested by <xref ref-type="bibr" rid="bibr9-1469787412441284">Cho et al. (2006</xref>: 896), who recommend adopting measures that are not influenced by distribution features, so correlation measures are preferred to percentage agreement measures. However, the use of the Pearson product-moment correlations can also be criticized. Following <xref ref-type="bibr" rid="bibr42-1469787412441284">Shrout and Fleiss (1979)</xref>, we used intra-class correlations (ICC), a common measure of reliability of either different judges or different items on a scale, in this study.</p>
</sec></sec>
</sec>
<sec id="section15-1469787412441284" sec-type="results">
<title>Results</title>
<sec id="section16-1469787412441284">
<title>Initial analyses</title>
<p>Prior to the actual analysis of the research data, a quality control of the assessment process was carried out. This focused on the way teachers applied the assessment rubric. Analysis of variance was applied to test differences. Post hoc comparisons confirm that teachers did not differ significantly in applying the rubric criteria Introduction, Structure and Contact with audience. But significant differences were observed in relation to the other six criteria. Additional analysis revealed that, for five of the six criteria, it was consistently the same teacher adopting a more lenient view compared with the other assessors. Scores from this teacher were removed from the data set.</p>
<p>To detect gender bias, an analysis of variance was carried out to compare whether the gender of the teacher and the gender of the assessed participant resulted in significantly different oral presentation skill sum scores. The results indicate that there was no significant difference between the scores of male and female presenters when assessed by a male or a female teacher.</p>
</sec>
<sec id="section17-1469787412441284">
<title>What is the level of agreement between peer assessments and teacher assessments?</title>
<p>After calculating the sum score of the nine rubric criteria, sum scores of teachers and peer assessors were compared. It is important to keep in mind that, as indicated in <xref ref-type="table" rid="table2-1469787412441284">Table 2</xref>, peers only assessed ‘first’ presentations. <xref ref-type="table" rid="table3-1469787412441284">Table 3</xref> summarizes the analysis results. We can conclude that we have achieved an acceptable but low reliability level considering the value of the intra-class correlation.</p>
<table-wrap id="table3-1469787412441284" position="float">
<label>Table 3.</label>
<caption>
<p>Teacher scores versus peer assessment scores: descriptives and intra-class correlation (<italic>n</italic> = 29)</p>
</caption>
<graphic alternate-form-of="table3-1469787412441284" xlink:href="10.1177_1469787412441284-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Teacher mean</th>
<th align="left">Teacher σ</th>
<th align="left">Peer mean</th>
<th align="left">Peer σ</th>
<th align="left">Intra-class correlation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sum score</td>
<td>2.14</td>
<td>0.37</td>
<td>2.57</td>
<td>0.32</td>
<td>.45</td>
</tr>
</tbody>
</table></table-wrap>
<p>The rubric sum score reported by teachers is significantly lower compared with the peer assessments (<italic>t</italic> = 6.210; <italic>p</italic> &lt; .001). To detect possible gender effects, an analysis of variance was carried out with the gender of the assessor and the gender of the assessed student as independent variables and the oral presentation sum score as the dependent variable. The analysis was repeated for teachers and for peer assessors. Results indicate that gender of the teachers (F<sub>(1,205)</sub> = .03, <italic>p</italic> = .87) and of the peer assessors (F<sub>(1,170)</sub> = .85, <italic>p</italic> = .36) did not result in significant differences in scoring. This implies that male assessors were not more severe or lenient than female assessors, and this was the case for teachers and for peer assessors. The interaction effect between gender of the assessor and gender of the assessed was not significant for teachers but was significant for peers (F<sub>(1,170)</sub> = 4.17, <italic>p</italic> &lt; .05), as can be seen in <xref ref-type="fig" rid="fig1-1469787412441284">Figure 1</xref>. Male peers allocated higher scores to female presenters than female peers did. Female presenters obtained higher scores than male presenters.</p>
<fig id="fig1-1469787412441284" position="float">
<label>Figure 1.</label>
<caption>
<p>Gender interaction effect peer assessors</p>
</caption>
<graphic xlink:href="10.1177_1469787412441284-fig1.tif"/></fig>
</sec>
<sec id="section18-1469787412441284">
<title>What is the level of agreement between self-assessments and teacher assessments?</title>
<p>To answer this question, the scores of teachers were compared with the scoring results obtained via self-assessment. It is important to keep in mind that, as indicated in <xref ref-type="table" rid="table2-1469787412441284">Table 2</xref>, participants self-assessed first and second presentations. <xref ref-type="table" rid="table4-1469787412441284">Table 4</xref> summarizes the analysis results. We can conclude that we have achieved again an acceptable but low level of reliability considering the value of the intra-class correlation.</p>
<table-wrap id="table4-1469787412441284" position="float">
<label>Table 4.</label>
<caption>
<p>Teacher assessment versus self-assessment: descriptives and intra-class correlation (<italic>n</italic> = 79)</p>
</caption>
<graphic alternate-form-of="table4-1469787412441284" xlink:href="10.1177_1469787412441284-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Teacher mean</th>
<th align="left">Teacher σ</th>
<th align="left">Self mean</th>
<th align="left">Self σ</th>
<th align="left">Intra-class correlation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sum score</td>
<td>2.46</td>
<td>0.53</td>
<td>2.70</td>
<td>0.49</td>
<td>.54</td>
</tr>
</tbody>
</table></table-wrap>
<p>The ‘total’ rubric score of teacher assessments is significantly lower compared with self-assessment scores (<italic>t</italic> = 6.19; <italic>p</italic> &lt; .001). The self-assessment scores of male and female participants are not significantly different (F<sub>(1,75)</sub> = .30, <italic>p</italic> = .58).</p>
</sec>
<sec id="section19-1469787412441284">
<title>What are the student perceptions about peer assessment and the learning process?</title>
<p>The average perception score about peer assessment reflects a predominantly positive opinion about peer assessment. Comparison of the first (<italic>m</italic> = 3.67) and second administration (<italic>m</italic> = 4.11) of the scale indicates a significant increase in the positive appreciation of peer assessment (<italic>t</italic> = 4.11; <italic>p</italic> = &lt; .001).</p>
<p>Participants ranked the instructional components, and results showed they believed they learned most from the feedback (<italic>M</italic> = 7.42) and least from their first presentation (<italic>M</italic> = 5.91). They indicated, however, that they learned more from the second (<italic>M</italic> = 6.79) and third (<italic>M</italic> = 7.02) presentations.</p>
</sec></sec>
<sec id="section20-1469787412441284" sec-type="discussion|conclusions">
<title>Discussion and conclusions</title>
<p>In this study, alternative assessment approaches were explored. Self- and peer assessment were positioned within a socio-cognitive perspective on self-regulated learning. The limited, and often contradictory, empirical evidence about self- and peer assessment of oral presentation skills prompted the design of a study in which self- and peer assessment were contrasted with the assessment by teaching staff.</p>
<p>Comparison of the teacher and peer assessment rubric scores points at a positive relationship, but also at critical differences. The lower intra-class correlation described here suggests that peers and teachers still interpret the criteria and indicators of the rubric in a different way. This can be explained by differences in the width and depth of their experience basis. Also, within the group of peers, not all students could have applied the same criteria in a comparable and/or consistent way. Lastly, the finding that peers report higher marks compared with teachers is in agreement with the results of other studies (<xref ref-type="bibr" rid="bibr28-1469787412441284">Langan et al., 2008</xref>).</p>
<p>With regard to the comparison of self-assessment scores and teacher scores, we have to conclude that there is a level of agreement and disagreement when assessing the oral presentations. Also the finding that the self-assessment scores are, mostly, higher than the marks given by teachers is consistent with the results reported in the literature (<xref ref-type="bibr" rid="bibr35-1469787412441284">Patri, 2002</xref>).</p>
<p>As stated above, these scoring differences can be explained by the broader experience of teachers when judging the quality of oral presentations. They can retrieve from their memory a larger set of models that exemplify how oral presentations do or do not meet the criteria. <xref ref-type="bibr" rid="bibr36-1469787412441284">Price and O’Donovan (2006)</xref> mention tacit knowledge that is experience-based and can only be made explicit through the sharing of experiences. This also implies that, in an implicit way, teachers add criteria and/or indicators when judging the quality of an oral presentation. This adds to the unreliable, but often neglected, nature of teacher assessments. This study has the merit of paying attention to the inter-rater reliability of the teachers. As explained above, one of the teachers applied a number of the criteria in a more lenient way. This problem was tackled by removing the particular assessment from the data set. Nevertheless, in a normal instructional setting, teachers have to be aware of bias potentially caused by assessors approaching the criteria in diverse ways. This should also be considered when setting up assessment-related research (<xref ref-type="bibr" rid="bibr50-1469787412441284">Topping, 2009</xref>).</p>
<p>With regard to the research question focusing on student perceptions of peer assessment, it can be concluded that the results reflect a very positive attitude towards the value of peer assessment. In addition, the results indicate that the actual process of carrying out self- and or peer assessment affects this perception in a positive way. This is a promising finding in the light of the impact of perceptions on the outcomes of student learning (<xref ref-type="bibr" rid="bibr46-1469787412441284">Struyven et al., 2003</xref>). We might assume that students’ perceptions of peer assessments will influence their willingness to take into account the feedback generated by peer assessment and to actually do something with that feedback. This positive attitude is probably also reflected in the ranking of the instructional components when participants declared that they learned most from the feedback.</p>
<p>Gender was also studied as a potential source of bias. Neither the gender of the assessor nor that of the student being assessed seems to influence the assessment process or assessment marks. The interaction effect between gender of the assessor and gender of the assessed was not significant for teachers but was significant for peers. Gender effects are also reported by some researchers (<xref ref-type="bibr" rid="bibr13-1469787412441284">Edens et al., 2000</xref>; <xref ref-type="bibr" rid="bibr27-1469787412441284">Langan et al., 2005</xref>) but not by others (<xref ref-type="bibr" rid="bibr41-1469787412441284">Sellnow and Treinen, 2004</xref>). It is possible that the gender effect observed in peer assessments interacted with the lower correlation between peer and teacher assessments. This gender effect can lower the quality of the peer assessments and the correlation with teacher assessments. The gender effect was caused by male assessors who gave higher marks to female presenters than female assessors did. Male assessors might have been somewhat biased towards female presenters and were too generous with their marks. It is also possible that female assessors underestimated the female presenters. Further research should shed more light on this issue.</p>
<p>Although a large number of recorded oral presentation sessions were assessed by peers, teachers and students themselves, the study remains limited when it comes to sample size, duration of the instructional intervention, scope of the skills to be mastered and the complexity level of the competencies. An important limitation is the limited variation in nature and background of the participants. The study has to be replicated involving students from other domains and from other educational levels. Additional research could focus on the impact of assessment training and student collaboration in relation to defining assessment criteria. Future studies should also consider the nature of the target audience, which could vary in knowledge domains and expertise levels. These studies should investigate the short-term, middle-term and long-term effects of self- and peer assessment. It would also be interesting to investigate the impact of individual and interpersonal variables (<xref ref-type="bibr" rid="bibr51-1469787412441284">Van Gennip et al., 2009</xref>). In this study, as in many other studies, teacher assessments were compared with peer and self-assessments. It might be interesting, in future research, to additionally compare self- and peer assessments.</p>
<p>Though a more in-depth and qualitative analysis of differences in scoring behaviour between teachers, peers and the students giving the oral presentations is beyond the scope of the present study, we have to bear in mind that the requirement to attain a high level of reliability was not completely answered (<xref ref-type="bibr" rid="bibr36-1469787412441284">Price and O’Donovan, 2006</xref>; <xref ref-type="bibr" rid="bibr49-1469787412441284">Topping, 2003</xref>). This finding suggests that the training of assessors is very important, and is in line with ideas presented (<xref ref-type="bibr" rid="bibr52-1469787412441284">Van Zundert et al., 2009</xref>). Such training could provide more examples and/or more concrete indicators. The suggestion is also very important for practitioners. They should provide extensive training to learners. Enriching the learning process with an explicit discussion of the assessment approach and criteria between peers and between teacher and learners could enhance the quality of the educational process.</p>
<p>Nevertheless, the results do not question the value of self- and peer assessment of oral presentation skills, and practitioners are advised to use both forms of assessment in order to provide the learner with a sufficient level of formative feedback. Also <xref ref-type="bibr" rid="bibr27-1469787412441284">Langan et al. (2005)</xref> and <xref ref-type="bibr" rid="bibr43-1469787412441284">Sluijsmans (2002)</xref> make clear that the benefits of peer assessment outweigh a certain degree of discrepancy between student marks, tutor marks and peer markings. <xref ref-type="bibr" rid="bibr3-1469787412441284">Boud (2007)</xref> refers in this context to the consequential validity of assessment. This means that the value of self- and peer assessment is also to be found in the impact on the acquisition process of complex oral presentation skills. Some studies, such as <xref ref-type="bibr" rid="bibr53-1469787412441284">Winne (2004)</xref>, stress the importance of the accuracy of feedback in view of future learning outcomes. But others, such as <xref ref-type="bibr" rid="bibr18-1469787412441284">Gibbs (2006)</xref> and <xref ref-type="bibr" rid="bibr54-1469787412441284">Yorke (2003)</xref>, state that it is not only the quality of the feedback evolving from the assessment that is crucial but what a student does with the feedback. In our opinion, a combination of both views is needed. On the one hand we do not want students to take wrong actions based on low-quality feedback but on the other we want students to do something with the feedback.</p>
<p>The question is therefore how to improve the quality of self- and peer assessment approaches. <xref ref-type="bibr" rid="bibr15-1469787412441284">Falchikov (2005)</xref> recommends developing evaluation criteria in close collaboration with students. <xref ref-type="bibr" rid="bibr36-1469787412441284">Price and O’Donovan (2006)</xref> warn that it is insufficient to concentrate on more detailed indicators for assessment criteria or standards because these indicators can become counterproductive if they are too comprehensive. They stress the importance of giving students sufficient practice and discussion to develop a shared understanding of the explicit and tacit assessment criteria. Part of the less positive results of the present study can therefore be explained on the basis of insufficient practice. The students did not get sufficient opportunities to practise with the assessment criteria. This conclusion also challenges the statements of <xref ref-type="bibr" rid="bibr20-1469787412441284">Hafner and Hafner (2003)</xref> and <xref ref-type="bibr" rid="bibr7-1469787412441284">Carlson and Smith-Howell (1995)</xref> that assessment training is not essential.</p>
<p>Our study revealed some interesting results about the, until now, underexplored self- and peer assessment field of oral presentation skills. Additional research could help to clarify the relationship between self- and peer assessment and our theoretical framework about self-regulated learning.</p>
</sec>
</body>
<back>
<bio>
<title>Biographical notes</title>
<p>Luc De Grez is assistant professor in Psychology and Leadership Behaviour at the University College Brussels (PhD Ghent University). His research interests include the learning and teaching of communication skills, and entrepreneurial learning. <italic>Address</italic>: University College Brussels, Stormstraat 2, B1000 Brussel, Belgium. [email: <email>luc.degrez@hubrussel.be</email>]</p>
<p>Martin Valcke is full Professor of Instructional Sciences at Ghent University, Belgium, and head of the Educational Studies department. His main research field is linked to the innovation of higher education and the integrated use of Information and Communication Technologies (ICT). <italic>Address</italic>: University of Ghent, H. Dunantlaan 2, B9000 Gent, Belgium. [email: <email>martin.valcke@ugent.be</email>]</p>
<p>Irene Roozen is assistant professor in Marketing at the University College Brussels. Her research interests include marketing communication, advertising effectiveness and assessment of communication skills. <italic>Address</italic>: University College Brussels, Stormstraat 2, B1000 Brussel, Belgium. [email: <email>Irene.roozen@hubrussel.be</email>]</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>AlFallay</surname><given-names>I</given-names></name>
</person-group> (<year>2004</year>) <article-title>The role of some selected psychological and personality traits of the rater in the accuracy of self- and peer-assessment</article-title>. <source>System</source> <volume>32</volume>: <fpage>407</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr2-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bandura</surname><given-names>A</given-names></name>
</person-group> (<year>1997</year>) <source>Self Efficacy: The Exercise of Control</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Freeman</publisher-name>.</citation>
</ref>
<ref id="bibr3-1469787412441284">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Boud</surname><given-names>D</given-names></name>
</person-group> (<year>2007</year>) <article-title>Assessment design for learner responsibility</article-title>. <comment>Available at: <ext-link ext-link-type="uri" xlink:href="http://ewds.strath.ac.uk/public/reap07/Boud-web/img0.html">http://ewds.strath.ac.uk/public/reap07/Boud-web/img0.html</ext-link></comment> (<access-date>accessed 3 November 2007</access-date>).</citation>
</ref>
<ref id="bibr4-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boud</surname><given-names>D</given-names></name>
<name><surname>Falchikov</surname><given-names>N</given-names></name>
</person-group> (<year>2006</year>) <article-title>Aligning assessment with long-term learning</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>31</volume>(<issue>4</issue>): <fpage>399</fpage>–<lpage>413</lpage>.</citation>
</ref>
<ref id="bibr5-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bourhis</surname><given-names>J</given-names></name>
<name><surname>Allen</surname><given-names>M</given-names></name>
</person-group> (<year>1998</year>) <article-title>The role of videotaped feedback in the instruction of public speaking: A quantitative synthesis of published empirical research</article-title>. <source>Communication Research Reports</source> <volume>15</volume>(<issue>3</issue>): <fpage>256</fpage>–<lpage>61</lpage>.</citation>
</ref>
<ref id="bibr6-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Campbell</surname><given-names>K</given-names></name>
<name><surname>Mothersbaugh</surname><given-names>D</given-names></name>
<name><surname>Brammer</surname><given-names>C</given-names></name>
<name><surname>Taylor</surname><given-names>T</given-names></name>
</person-group> (<year>2001</year>) <article-title>Peer versus self-assessment of oral business presentation performance</article-title>. <source>Business Communication Quarterly</source> <volume>64</volume>(<issue>3</issue>): <fpage>23</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr7-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carlson</surname><given-names>R</given-names></name>
<name><surname>Smith-Howell</surname><given-names>D</given-names></name>
</person-group> (<year>1995</year>) <article-title>Classroom public speaking assessment: Reliability and validity of selected evaluation instruments</article-title>. <source>Communication Education</source> <volume>44</volume>: <fpage>87</fpage>–<lpage>97</lpage>.</citation>
</ref>
<ref id="bibr8-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cheng</surname><given-names>W</given-names></name>
<name><surname>Warren</surname><given-names>M</given-names></name>
</person-group> (<year>2005</year>) <article-title>Peer assessment of language proficiency</article-title>. <source>Language Testing</source> <volume>22</volume>(<issue>1</issue>): <fpage>93</fpage>–<lpage>121</lpage>.</citation>
</ref>
<ref id="bibr9-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cho</surname><given-names>K</given-names></name>
<name><surname>Schunn</surname><given-names>C</given-names></name>
<name><surname>Wilson</surname><given-names>W</given-names></name>
</person-group> (<year>2006</year>) <article-title>Validity and reliability of scaffolded peer assessment of writing from instructor and student perspectives</article-title>. <source>Journal of Educational Studies</source> <volume>95</volume>(<issue>4</issue>): <fpage>891</fpage>–<lpage>901</lpage>.</citation>
</ref>
<ref id="bibr10-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dannefer</surname><given-names>E</given-names></name>
<name><surname>Henson</surname><given-names>L</given-names></name>
<name><surname>Bierer</surname><given-names>S</given-names></name>
<name><surname>Grady-Weliky</surname><given-names>T</given-names></name>
<name><surname>Meldrum</surname><given-names>S</given-names></name>
<name><surname>Nofziger</surname><given-names>A</given-names></name>
<name><surname>Barclay</surname><given-names>C</given-names></name>
<name><surname>Stein</surname><given-names>R</given-names></name>
</person-group> (<year>2005</year>) <article-title>Peer assessment of professional competence</article-title>. <source>Medical Education</source> <volume>39</volume>(<issue>7</issue>): <fpage>713</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr11-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>De Grez</surname><given-names>L</given-names></name>
<name><surname>Valcke</surname><given-names>M</given-names></name>
<name><surname>Roozen</surname><given-names>I</given-names></name>
</person-group> (<year>2009a</year>) <article-title>The impact of an innovative instructional intervention on the acquisition of oral presentation skills in higher education</article-title>. <source>Computers and Education</source> <volume>53</volume>: <fpage>112</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr12-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>De Grez</surname><given-names>L</given-names></name>
<name><surname>Valcke</surname><given-names>M</given-names></name>
<name><surname>Roozen</surname><given-names>I</given-names></name>
</person-group> (<year>2009b</year>) <article-title>The impact of goal orientation, self-reflection and personal characteristics on the acquisition of oral presentations skills</article-title>. <source>European Journal of Psychology of Education</source> <volume>24</volume>(<issue>3</issue>): <fpage>293</fpage>–<lpage>306</lpage>.</citation>
</ref>
<ref id="bibr13-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Edens</surname><given-names>F</given-names></name>
<name><surname>Rink</surname><given-names>F</given-names></name>
<name><surname>Smilde</surname><given-names>M</given-names></name>
</person-group> (<year>2000</year>) <article-title>De studentenrechtbank : een evaluatieonderzoek naar beoordelingslijsten voor presentatievaardigheden (Student court of justice: An evaluation of assessment instruments for presentation skills)</article-title>. <source>Tijdschrift voor Onderwijsresearch (Journal for Educational Research)</source> <volume>24</volume>(<issue>3–4</issue>): <fpage>265</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr14-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Elton</surname><given-names>L</given-names></name>
<name><surname>Johnston</surname><given-names>B</given-names></name>
</person-group> (<year>2002</year>) <source>Assessment in Universities: A Critical Review of Research</source>. <publisher-loc>York</publisher-loc>: <publisher-name>Learning and Teaching Support Network Generic Centre</publisher-name>.</citation>
</ref>
<ref id="bibr15-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Falchikov</surname><given-names>N</given-names></name>
</person-group> (<year>2005</year>) <source>Improving Assessment Through Student Involvement: Practical Solutions for Aiding Learning in Higher and Further Education</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>RoutledgeFalmer</publisher-name>.</citation>
</ref>
<ref id="bibr16-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fallows</surname><given-names>S</given-names></name>
<name><surname>Chandramohan</surname><given-names>B</given-names></name>
</person-group> (<year>2001</year>) <article-title>Multiple approaches to assessment: Reflections on use of tutor, peer and self-assessment</article-title>. <source>Teaching in Higher Education</source> <volume>6</volume>(<issue>2</issue>): <fpage>229</fpage>–<lpage>46</lpage>.</citation>
</ref>
<ref id="bibr17-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Freeman</surname><given-names>M</given-names></name>
</person-group> (<year>1995</year>) <article-title>Peer assessment by groups of group work</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>20</volume>(<issue>3</issue>): <fpage>289</fpage>–<lpage>301</lpage>.</citation>
</ref>
<ref id="bibr18-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gibbs</surname><given-names>G</given-names></name>
</person-group> (<year>2006</year>) <article-title>How assessment frames student learning</article-title>. In: <person-group person-group-type="editor">
<name><surname>Bryan</surname><given-names>C</given-names></name>
<name><surname>Clegg</surname><given-names>K</given-names></name>
</person-group> (eds) <source>Innovative Assessment in Higher Education</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>, pp. <fpage>23</fpage>–<lpage>36</lpage>.</citation>
</ref>
<ref id="bibr19-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gielen</surname><given-names>S</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
<name><surname>Dierick</surname><given-names>S</given-names></name>
</person-group> (<year>2003</year>) <article-title>Evaluating the consequential validity of new modes of assessment: The influence of assessment on learning, including pre-, post-, and true assessment effects</article-title>. In: <person-group person-group-type="editor">
<name><surname>Segers</surname><given-names>M</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
<name><surname>Cacallar</surname><given-names>E</given-names></name>
</person-group> (eds) <source>Optimising New Modes of Assessment: In Search of Qualities and Standards</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic</publisher-name>, pp. <fpage>37</fpage>–<lpage>54</lpage>.</citation>
</ref>
<ref id="bibr20-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hafner</surname><given-names>J</given-names></name>
<name><surname>Hafner</surname><given-names>P</given-names></name>
</person-group> (<year>2003</year>) <article-title>Quantitative analysis of the rubric as an assessment tool: An empirical study of peer-group rating</article-title>. <source>International Journal of Science Education</source> <volume>25</volume>(<issue>12</issue>): <fpage>1509</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr21-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hanrahan</surname><given-names>S</given-names></name>
<name><surname>Isaacs</surname><given-names>G</given-names></name>
</person-group> (<year>2001</year>) <article-title>Assessing self- and peer-assessment: The students’ views</article-title>. <source>Higher Education Research and Development</source> <volume>20</volume>(<issue>1</issue>): <fpage>53</fpage>–<lpage>70</lpage>.</citation>
</ref>
<ref id="bibr22-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hattie</surname><given-names>J</given-names></name>
</person-group> (<year>2009</year>) <source>Visible Learning: A Synthesis of over 800 Meta-analyses Relating to Achievement</source>. <publisher-loc>Abingdon</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr23-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hughes</surname><given-names>I</given-names></name>
<name><surname>Large</surname><given-names>B</given-names></name>
</person-group> (<year>1993</year>) <article-title>Staff and peer-group assessment of oral communication skills</article-title>. <source>Studies in Higher Education</source> <volume>18</volume>(<issue>3</issue>): <fpage>379</fpage>–<lpage>85</lpage>.</citation>
</ref>
<ref id="bibr24-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kappe</surname><given-names>F</given-names></name>
</person-group> (<year>2008</year>) <article-title>Hoe betrouwbaar is peer-assessment? Twee empirische studies naar studentbeoordelingen (How reliable is peer-assessment? Two empirical studies about assessment by students)</article-title>. <source>Tijdschrift voor Hoger Onderwijs (Journal of Higher Education)</source> <volume>26</volume>(<issue>2</issue>).</citation>
</ref>
<ref id="bibr25-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kerby</surname><given-names>D</given-names></name>
<name><surname>Romine</surname><given-names>J</given-names></name>
</person-group> (<year>2009</year>) <article-title>Develop oral presentation skills through accounting curriculum design and course-embedded assessment</article-title>. <source>Journal of Education for Business</source> <volume>85</volume>: <fpage>172</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr26-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kruger</surname><given-names>J</given-names></name>
<name><surname>Dunning</surname><given-names>D</given-names></name>
</person-group> (<year>1999</year>) <article-title>Unskilled and unaware of it: How difficulties in recognizing one’s own incompetence lead to inflated self-assessments</article-title>. <source>Journal of Personality and Social Psychology</source> <volume>77</volume>(<issue>6</issue>): <fpage>1121</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr27-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Langan</surname><given-names>A</given-names></name>
<name><surname>Wheater</surname><given-names>C</given-names></name>
<name><surname>Shaw</surname><given-names>E</given-names></name>
<name><surname>Haines</surname><given-names>B</given-names></name>
<name><surname>Cullen</surname><given-names>W</given-names></name>
<name><surname>Boyle</surname><given-names>J</given-names></name><etal/>
</person-group>. (<year>2005</year>) <article-title>Peer assessment of oral presentations: Effects of student gender, university affiliation and participation in the development of assessment criteria</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>30</volume>(<issue>1</issue>): <fpage>21</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr28-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Langan</surname><given-names>M</given-names></name>
<name><surname>Shuker</surname><given-names>D</given-names></name>
<name><surname>Cullen</surname><given-names>R</given-names></name>
<name><surname>Penney</surname><given-names>D</given-names></name>
<name><surname>Preziosi</surname><given-names>R</given-names></name>
<name><surname>Wheater</surname><given-names>P</given-names></name>
</person-group> (<year>2008</year>) <article-title>Relationships between student characteristics and self-, peer and tutor evaluations of oral presentations</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>33</volume>(<issue>2</issue>): <fpage>179</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr29-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Magin</surname><given-names>D</given-names></name>
<name><surname>Helmore</surname><given-names>P</given-names></name>
</person-group> (<year>2001</year>) <article-title>Peer and teacher assessments of oral presentation skills: How reliable are they?</article-title> <source>Studies in Higher Education</source> <volume>26</volume>(<issue>3</issue>): <fpage>287</fpage>–<lpage>98</lpage>.</citation>
</ref>
<ref id="bibr30-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Messick</surname><given-names>S</given-names></name>
</person-group> (<year>1994</year>) <source>Validity of Psychological Assessment: Validation of Inferences From Persons’ Responses and Performances as Scientific Inquiry into Score Meaning</source>. <comment>Research Report RR 94-95</comment>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr31-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Miller</surname><given-names>P</given-names></name>
</person-group> (<year>2003</year>) <article-title>The effect of scoring criteria specificity on peer and self-assessment</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>28</volume>(<issue>4</issue>): <fpage>383</fpage>–<lpage>94</lpage>.</citation>
</ref>
<ref id="bibr32-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nicol</surname><given-names>D</given-names></name>
<name><surname>Macfarlane-Dick</surname><given-names>D</given-names></name>
</person-group> (<year>2006</year>) <article-title>Formative assessment and self-regulated learning: A model and seven principles of good feedback practice</article-title>. <source>Studies in Higher Education</source> <volume>31</volume>(<issue>2</issue>): <fpage>199</fpage>–<lpage>218</lpage>.</citation>
</ref>
<ref id="bibr33-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Nicol</surname><given-names>D</given-names></name>
<name><surname>Milligan</surname><given-names>C</given-names></name>
</person-group> (<year>2006</year>) <article-title>Rethinking technology-supported assessment practices in relation to the seven principles of good feedback practice</article-title>. In: <person-group person-group-type="editor">
<name><surname>Bryan</surname><given-names>C</given-names></name>
<name><surname>Clegg</surname><given-names>K</given-names></name>
</person-group> (eds) <source>Innovative Assessment in Higher Education</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>, pp. <fpage>64</fpage>–<lpage>77</lpage>.</citation>
</ref>
<ref id="bibr34-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Oldfield</surname><given-names>K</given-names></name>
<name><surname>Macalpine</surname><given-names>J</given-names></name>
</person-group> (<year>1995</year>) <article-title>Peer and self-assessment at tertiary level: An experiential report</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>20</volume>(<issue>1</issue>): <fpage>125</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr35-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Patri</surname><given-names>M</given-names></name>
</person-group> (<year>2002</year>) <article-title>The influence of peer feedback on self- and peer assessment of oral skills</article-title>. <source>Language Testing</source> <volume>19</volume>(<issue>2</issue>): <fpage>109</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr36-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Price</surname><given-names>M</given-names></name>
<name><surname>O’Donovan</surname><given-names>B</given-names></name>
</person-group> (<year>2006</year>) <article-title>Improving performance through enhancing student understanding of criteria and feedback</article-title>. In: <person-group person-group-type="editor">
<name><surname>Bryan</surname><given-names>C</given-names></name>
<name><surname>Clegg</surname><given-names>K</given-names></name>
</person-group> (eds) <source>Innovative Assessment in Higher Education</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>, pp. <fpage>100</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr37-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rust</surname><given-names>C</given-names></name>
<name><surname>Price</surname><given-names>M</given-names></name>
<name><surname>O’Donovan</surname><given-names>B</given-names></name>
</person-group> (<year>2003</year>) <article-title>Improving students’ learning by developing their understanding of assessment criteria and processes</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>28</volume>(<issue>2</issue>): <fpage>147</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr38-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sadler</surname><given-names>D</given-names></name>
</person-group> (<year>1989</year>) <article-title>Formative assessment and the design of instructional systems</article-title>. <source>Instructional Science</source> <volume>18</volume>: <fpage>119</fpage>–<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr39-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schunk</surname><given-names>D</given-names></name>
</person-group> (<year>2001</year>) <article-title>Social cognitive theory and self-regulated learning</article-title>. In: <person-group person-group-type="editor">
<name><surname>Zimmerman</surname><given-names>B</given-names></name>
<name><surname>Schunk</surname><given-names>D</given-names></name>
</person-group> (eds) <source>Self-regulated Learning and Academic Achievement: Theoretical Perspectives</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum, pp</publisher-name>.<fpage>125</fpage>–<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr40-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Segers</surname><given-names>M</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
<name><surname>Cascallar</surname><given-names>E</given-names></name>
</person-group> (<year>2003</year>) <article-title>The era of assessment engineering: Changing perspectives on teaching and learning and the role of new modes of assessment</article-title>. In: <person-group person-group-type="editor">
<name><surname>Segers</surname><given-names>M</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
<name><surname>Cascallar</surname><given-names>E</given-names></name>
</person-group> (eds) <source>Optimising New Modes of Assessment: In Search of Qualities and Standards</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic, pp</publisher-name>.<fpage>1</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr41-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sellnow</surname><given-names>D</given-names></name>
<name><surname>Treinen</surname><given-names>K</given-names></name>
</person-group> (<year>2004</year>) <article-title>The role of gender in perceived speaker competence: An analysis of student critiques</article-title>. <source>Communication Education</source> <volume>53</volume>(<issue>3</issue>): <fpage>286</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr42-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shrout</surname><given-names>P</given-names></name>
<name><surname>Fleiss</surname><given-names>J</given-names></name>
</person-group> (<year>1979</year>) <article-title>Intraclass correlation: Uses in assessing rater reliability</article-title>. <source>Psychological Studies</source> <volume>56</volume>(<issue>2</issue>): <fpage>420</fpage>–<lpage>8</lpage>.</citation>
</ref>
<ref id="bibr43-1469787412441284">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Sluijsmans</surname><given-names>D</given-names></name>
</person-group> (<year>2002</year>) <article-title>Student involvement in assessment: The training of peer assessment skills</article-title>. <comment>Unpublished doctoral thesis</comment>, <publisher-name>Open University of the Netherlands</publisher-name>, <publisher-loc>Heerlen</publisher-loc>.</citation>
</ref>
<ref id="bibr44-1469787412441284">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Sluijsmans</surname><given-names>D</given-names></name>
</person-group> (<year>2008</year>) <article-title>Towards (quasi-) experimental research on the design of peer assessment</article-title>. In: <person-group person-group-type="editor">
<name><surname>van den Heuvel-Panhuizen</surname><given-names>M</given-names></name>
<name><surname>Lacher</surname><given-names>M</given-names></name>
</person-group> (eds) <source>Challenging Assessment: Book of Abstracts of the Fourth Biennial Earli/Northumbria Assessment Conference</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Humboldt-Universität</publisher-name>, p. <fpage>46</fpage>.</citation>
</ref>
<ref id="bibr45-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sluijsmans</surname><given-names>D</given-names></name>
<name><surname>Moerkerke</surname><given-names>G</given-names></name>
<name><surname>Van Merriënboer</surname><given-names>J</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
</person-group> (<year>2001</year>) <article-title>Peer assessment in problem based learning</article-title>. <source>Studies in Educational Evaluation</source> <volume>27</volume>: <fpage>153</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr46-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Struyven</surname><given-names>K</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
<name><surname>Janssens</surname><given-names>S</given-names></name>
</person-group> (<year>2003</year>) <article-title>Students’ perceptions about new modes of assessment in higher education: A review</article-title>. In: <person-group person-group-type="editor">
<name><surname>Segers</surname><given-names>M</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
<name><surname>Cascallar</surname><given-names>E</given-names></name>
</person-group> (eds) <source>Optimising New Modes of Assessment: In Search of Qualities and Standards</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic</publisher-name>, pp. <fpage>171</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr47-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taras</surname><given-names>M</given-names></name>
</person-group> (<year>2008</year>) <article-title>Summative and formative assessment: Perceptions and realities</article-title>. <source>Active Learning in Higher Education</source> <volume>9</volume>(<issue>2</issue>): <fpage>172</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr48-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Topping</surname><given-names>K</given-names></name>
</person-group> (<year>1998</year>) <article-title>Peer assessment between students in colleges and universities</article-title>. <source>Review of Educational Research</source> <volume>68</volume>(<issue>3</issue>): <fpage>249</fpage>–<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr49-1469787412441284">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Topping</surname><given-names>K</given-names></name>
</person-group> (<year>2003</year>) <article-title>Self- and peer assessment in school and university: Reliability, validity and utility</article-title>. In: <person-group person-group-type="editor">
<name><surname>Segers</surname><given-names>M</given-names></name>
<name><surname>Dochy</surname><given-names>F</given-names></name>
<name><surname>Cascallar</surname><given-names>E</given-names></name>
</person-group> (eds) <source>Optimising New Modes of Assessment: In Search of Qualities and Standards</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic</publisher-name>, pp. <fpage>55</fpage>–<lpage>87</lpage>.</citation>
</ref>
<ref id="bibr50-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Topping</surname><given-names>K</given-names></name>
</person-group> (<year>2009</year>) <article-title>Peer assessment</article-title>. <source>Theory Into Practice</source> <volume>48</volume>: <fpage>20</fpage>–<lpage>7</lpage>.</citation>
</ref>
<ref id="bibr51-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van Gennip</surname><given-names>N</given-names></name>
<name><surname>Segers</surname><given-names>M</given-names></name>
<name><surname>Tillema</surname><given-names>H</given-names></name>
</person-group> (<year>2009</year>) <article-title>Peer assessment for learning from a social perspective: The influence of interpersonal variables and structural features</article-title>. <source>Educational Research Review</source> <volume>4</volume>: <fpage>41</fpage>–<lpage>54</lpage>.</citation>
</ref>
<ref id="bibr52-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van Zundert</surname><given-names>M</given-names></name>
<name><surname>Sluijsmans</surname><given-names>D</given-names></name>
<name><surname>Van Merriënboer</surname><given-names>J</given-names></name>
</person-group> (<year>2009</year>) <article-title>Effective peer assessment processes: Research findings and future directions</article-title>. <source>Learning and Instruction</source> <volume>20</volume>(<issue>4</issue>): <fpage>270</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr53-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Winne</surname><given-names>P</given-names></name>
</person-group> (<year>2004</year>) <article-title>Students’ calibration of knowledge and learning processes: Implications for designing powerful software learning environments</article-title>. <source>International Journal of Educational Research</source> <volume>41</volume>: <fpage>466</fpage>–<lpage>88</lpage>.</citation>
</ref>
<ref id="bibr54-1469787412441284">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yorke</surname><given-names>M</given-names></name>
</person-group> (<year>2003</year>) <article-title>Formative assessment in higher education: Moves towards theory and the enhancement of pedagogic practice</article-title>. <source>Higher Education</source> <volume>45</volume>: <fpage>47</fpage>–<lpage>501</lpage>.</citation>
</ref></ref-list>
</back>
</article>