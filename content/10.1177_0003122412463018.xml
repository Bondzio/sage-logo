<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ASR</journal-id>
<journal-id journal-id-type="hwp">spasr</journal-id>
<journal-id journal-id-type="nlm-ta">Am Sociol Rev</journal-id>
<journal-title>American Sociological Review</journal-title>
<issn pub-type="ppub">0003-1224</issn>
<issn pub-type="epub">1939-8271</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0003122412463018</article-id>
<article-id pub-id-type="publisher-id">10.1177_0003122412463018</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Comment and Reply</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Symmetry Is Beautiful</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Shwed</surname><given-names>Uri</given-names></name>
<xref ref-type="aff" rid="aff1-0003122412463018">a</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bearman</surname><given-names>Peter S.</given-names></name>
<xref ref-type="aff" rid="aff2-0003122412463018">b</xref>
</contrib>
</contrib-group>
<aff id="aff1-0003122412463018"><label>a</label>Ben Gurion University of the Negev</aff>
<aff id="aff2-0003122412463018"><label>b</label>Columbia University</aff>
<author-notes>
<corresp id="corresp1-0003122412463018">Uri Shwed, Department of Sociology and Anthropology, Ben Gurion University of the Negev, PO Box 653, Beer-Sheva 84105 Israel E-mail: <email>shwed@bgu.ac.il</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>77</volume>
<issue>6</issue>
<fpage>1064</fpage>
<lpage>1069</lpage>
<permissions>
<copyright-statement>© American Sociological Association 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Sociological Association</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p><italic>The Temporal Structure of Scientific Consensus Formation</italic> (<xref ref-type="bibr" rid="bibr5-0003122412463018">Shwed and Bearman 2010</xref>) developed a procedure—which did not require expert judgment—to evaluate the level of contestation in scientific literatures. Examining different cases of consensus and contestation, we showed that science may progress in a spiral pattern that quickly generates new questions in new domains from recent answers (e.g., climate change research), stagnate around old questions in a cyclical pattern (e.g., smoking research in the 1950s), or entrench in a flat pattern responding to irrelevant external critiques (e.g., research on autism and vaccines). Bruggeman, Traag, and Uitermark (hereafter BTU) argue that without distinguishing between positive and negative citations, our procedure is of little value. They arrive at this conclusion by misunderstanding the role of network communities in our procedure. They then propose as an improvement a self-defeating solution that relies on distinguishing between postive and negative ties, requires expert evaluation, and destroys the possiblity of an unbiased evaluation. Avoiding that trap was the point of our article.</p>
<p>While much of their comment considers instances of contestation in politics that have nothing to do with science, the relevant findings they offer with respect to our analyses support our original findings. Given this, it is not surprising that BTU’s own calculations show that their concern with the valence of citations has no bearing on the evaluation of contestation levels. By accepting the comment for publication, <italic>ASR</italic>’s editors prolong a period of flat argumentation in the sociology of science. In this reply we reiterate the original article’s contribution and show how BTU’s findings strengthen ours. We conclude with remarks on the importance of symmetry, a guiding principle of our analysis that BTU completely abandoned.</p>
<p>If it seems that the original article, the comment, and the reply are talking past each other, it is because they are. In the original article we made arguments about scientific contestation, modularity as a global property of graphs, symmetry, comparability, and empirical results. In the comment, BTU discuss the allocation of nodes to communities, non-scientific discourse, and doing what the 457 previous papers using modularity on topics unrelated to ours did. Although of great interest to many people, these are different foci than those we considered.</p>
<p>Our article exploited the modularity community detection algorithm not for community detection, but as a general property of networks. This enabled us to link black-boxing (an idea) with modularity (a measure) to provide a useful, symmetrical tool for sociologists of science to evaluate the state of contestation around politically laden issues. Specifically, we used maximal directed modularity as a way to quantitatively operationalize the imagery of black-boxing (Latour) or bottled ships (Collins). We showed that the measure fits the contestation and consensus trends in disparate science domains described qualitatively (for validation) by field-specific experts. We developed our procedure by observing a simple homology between what Latour and Collins suggest changes in the process of consensus making, and what modularity quantifies—observed differences in the salience of internal parts. Validation of our procedure, however, is purely empirical: we did not claim or assume that modularity indexes contestation; we tested that idea and found that it did.</p>
<p>The main benefit of our measure is that it allows comparative research into science without field-specific expertise. A main requirement for a useful measure is what Latour defined as generalized symmetry—rejection of any pre-categorization or pre-demarcation of the field of inquiry. We achieved symmetry by offering a tool that requires no parameter specification or analyst’s judgment. To attain symmetry, the analyst must not impose interpretations on the data. Oddly, this is precisely what BTU criticize us for. We have no interest in defending claims that we never made and are wrong. However, in the next section we focus on the analyses provided by the authors, and outline why their critique is irrelevant to the ideas in our article and actually strengthens our results.</p>
<sec id="section1-0003122412463018">
<title>Symmetry and Negative Citations</title>
<p>BTU’s main critique is that we ignore the content of ties and assume or interpret all ties as positive. BTU propose a modularity algorithm for signed networks; that is, their algorithm does not simply maximize modularity, it does so only on ties that are signed positive, while minimizing modularity on ties that are signed negative.</p>
<p>This could be a welcomed addition, for instances where the signing process itself may be taken for granted. Science, of course, is no such instance. This is why BTU fail to provide signed networks that are relevant for the discussion of scientific consensus. In their own words, “To distinguish negative from positive references, we would have to acquaint ourselves with the vernacular of cancer researchers and read thousands of articles, which is beyond our capabilities here” (<xref ref-type="bibr" rid="bibr1-0003122412463018">Bruggeman, Traag, and Uitermark 2012</xref>:1053). Had it been within BTU’s capabilities, all they would have provided us with is yet another expert-based, asymmetrical interpretation of the literature based on content analysis. The idea of our article is that one ought to refuse to do so—regardless of capabilities—because doing so endangers the symmetry and objectivity that our analysis strives for.</p>
<p>But what are negative citations in science? Consider, for example, this exchange. Judging from BTU’s tone, their critique is clearly a negative citation of our article, focusing on what they see as flaws. And yet, they expose our analysis to more readers, prolong its tenure in <italic>ASR</italic>’s pages, and broaden its linkage to the scientific communities interested in community detection algortithms, while substantively strengthening our results. The border between negative and positive citations is fuzzy; negative citations imply recognition and contribute to the progress of science as much as positive citations (<xref ref-type="bibr" rid="bibr2-0003122412463018">Cole and Cole 1981</xref>:24–27). To argue that the symmetry constraint (which prohibits us from signing ties) is flawed, BTU need to demonstrate that when asking the same questions asymmetrically they reach significantly different conclusions regarding scientific consensus. This is not something their comment is oriented toward.</p>
<p>In the original article, we neither argued nor assumed that citations are positive. We noted that two different theories of citations (by Merton and Latour) imply that most—although not all—citations are positive. From this we proposed that changes in contestation levels (which also likely mean changes in the ratio of negative to positive citations) are observable through scaled modularity. Our original article tested this and found it is the case. Scaled modularity distinguishes between periods of contestation and consensus in part because the ratio of negative citations to positive citations is different in such periods and the structural properties of negative citations are different from positive citations. BTU’s own results demonstrate this. However, acknowledging the difference between positive and negative citations at the micro level of article construction does not mean the macro measures we employ must feed on such data. Signing citations and coding them as negative or positive requires domain specific expertise and human judgment—two issues we minimized in favor of a symmetrical analysis. That was the point of our article.</p>
</sec>
<sec id="section2-0003122412463018">
<title>Community Allocation Versus Scaled Modularity</title>
<p>BTU charge us with assuming that negative citations have no implications for the communities detected, and they provide an elegant analysis to demonstrate that this assumption does not hold. We did not make this assumption. BTU are confused between the common use of community detection algorithms—for detecting communities—and our use of the same procedure for an evaluation of a network macro property. That was one of the innovations of the original article.</p>
<p>BTU show that when they introduce even a small ratio of negative citations into the equation they get different results, which is reasonable. But this finding is not really relevant for the argument we made. We made no claims about the actual community structure, which is the only thing they report. Huge dissimilarities in the allocation of specific nodes to specific communities can yield similar (macro) moldularity scores. As a matter of fact, in many cases, they do (Good, Montjoye, and Clauset 2010). In addition, it is not surprising that arbitrarily signing some citations as positive and others as negative—while taking that arbitrary value into the calculation—generates different results.</p>
<p>BTU show (in their <xref ref-type="fig" rid="fig1-0003122412463018">Figure 1</xref>) that as more citations are arbitrarily defined as negative, different draws yield increasingly different community allocations. Our <xref ref-type="fig" rid="fig1-0003122412463018">Figure 1</xref> simply overlays theirs with the corresponding line representing the trend in scaled modularity for the cases and periods from our original article. BTU provide no explanations for the clear drop in the variance of different draws, apparent in 1991 to 1992 in the smoking case and in 1994 to 1995 in the skin cancer case. This trend corresponds closely to our findings;<sup><xref ref-type="fn" rid="fn1-0003122412463018">1</xref></sup> in 1992, the final EPA report on hazards of passive smoking terminated that debate, and in 1994 our analysis of the skin cancer case crosses the .1 value of scaled modularity, which we carefully suggested as a possible threshold for consensus. Thus, it seems that (yes) arbitrarily defined negative citations generate different allocations of nodes to communities (which our procedure does not care about), but that when the literature is consensual, different draws are more coherent among themselves. In other words, when employing an algorithm that is sensitive to some random quality, consensual black-boxed networks are more robust then networks with salient internal divisions.</p>
<fig id="fig1-0003122412463018" position="float">
<label>Figure 1.</label>
<caption>
<p>An Overlay of BTU’s <xref ref-type="fig" rid="fig1-0003122412463018">Figure 1</xref> with Modularity Trends from <xref ref-type="bibr" rid="bibr5-0003122412463018">Shwed and Bearman (2010)</xref></p>
<p><italic>Note</italic>: The top panel refers to the literature about smoking hazards and the bottom panel to the carcinogenicity of solar radiation. In both cases, modularity trends suggested by <xref ref-type="bibr" rid="bibr5-0003122412463018">Shwed and Bearman (2010)</xref> are indicative of a reduction in the variance of results from BTU’s procedure, following an independent random signing of parts of the original networks. This means that as a literature becomes consensual (indexed by the decreasing modularity trend line), even BTU’s random manipulation of 10 percent of the network ceases to have an effect. Consensual literatures, black-boxed, are more robust.</p>
</caption>
<graphic xlink:href="10.1177_0003122412463018-fig1.tif"/></fig>
<p>Their <xref ref-type="fig" rid="fig1-0003122412463018">Figure 1</xref> thus replicates our findings. In fact, BTU (2012:1054) summarize their analysis saying: “During periods of epistemic rivalry, when the percentage of negative ties is higher, the difference will usually be larger. . . . the actual pattern of negative ties is unknown to us and remains an empirical question.” Our article answered this empirical question for seven cases and a century of research, and showed that periods of epistemic rivalry correspond with scaled (unsigned) modularity.</p>
</sec>
<sec id="section3-0003122412463018">
<title>Specialization and the “Serious Flaw” of Scaling Modularity</title>
<p>BTU go to great length to argue that our scaling of the modularity property is “a serious technical flaw” (BTU:1051). They call it renormalization rather than scaling. They argue it is wrong to scale modularity, which is “by definition normalized” (BTU:1052). Modularity (as part of the process of community detection) is normalized for the number of ties (references); we scale it for the (logged) number of nodes (articles). Those are very different things. The normalization predefined in the calculation of modularity pertains to connectivity, the number of ties in the network, and not to network size. It is the required minimal normalization to allocate nodes into communities. At the risk of being didactic, this assignment process was not what our article was about.</p>
<p>We presented scaling to control for benign contestation—the natural process of scientific specialization, as well as the development of invisible colleges. These processes are a function of the literature’s size, regardless of its connectivity. BTU mention that out of 457 expert papers cited by <xref ref-type="bibr" rid="bibr3-0003122412463018">Fortunato (2010)</xref> none used our scaling. This is no coincidence, of course, but it is not condemning evidence either. It establishes that we were the first to focus on the modularity of a graph as a diagnostic concept (rather than on the communities it creates) in the specific context of citation networks where a big part of community creation is a product of size.</p>
<p>Thus we find BTU’s Figure 2 unremarkable: it demonstrates our point that much of what goes on in science is benign contestation. The figure also shows that without our scaling of modularity, it remains fairly constant throughout the period. BTU (2012:1052) manage to support our substantive argument for scaling, while ignoring the scaling itself because we “did not infer renormalization within the theoretical framework on which modularity is built.” They show the growing specialization over size, they show that raw modularity remains stable over that time, and they ignore the empirical fact that throughout this period our scaling tracks important processes such as the end of the dispute about safer cigarettes and the forming of consensus around passive smoking.</p>
</sec>
<sec id="section4-0003122412463018">
<title>Scientific Consensus Versus Dutch Politics</title>
<p>BTU consider as a new case a public debate in Dutch newspapers. They bridge the immense gap between what they comment on—our discussion of scientific consensus formation—and a political debate in the Dutch public sphere by arguing that their method is “very general” (BTU 2012:1051).<sup><xref ref-type="fn" rid="fn2-0003122412463018">2</xref></sup> They provide three colorful figures that reiterate the (undisputed) point that information on the nature of a tie may change community allocation in cross-sectional networks. They show—with data far removed from the original context—that community allocation would have been different if tie valence were taken into account. The analysis is not longitudinal and BTU do not report modularity—so we cannot tell whether this has any bearing on our approach—although the context is so different it is hard to determine what similar or different modularity trends would mean for our model of distinguishing benign contestation from epistemic rivalries in scientific literatures.</p>
<p>And yet, the Dutch newspaper analysis warrants the observation that BTU introduce a radically different context in order to show the utility of their extension of modularity, only to end up analyzing a subset of a subset of a subset of a subset of their original data.</p>
<p>Let us compare the data selection involved in our analysis with theirs. Both analyses define an issue with boolean search keywords. Our analysis selects on journals indexed by the Web of Science, they select on Lexis-Nexus. BTU add four more levels of data selection. First, they rely on only two newspapers, neither of which are among the top Dutch newspapers with respect to circulation. Second, among all the articles they retrieved, BTU select only articles over 1,000 words. From these they manually generated a population of actors and references between them, labeling the references as positive, negative, or neutral. They then discarded the data labeled neutral (which actually means ambigous; BTU:1058). Finally, afer dispensing with an unreported ratio of the data twice, introducing coder bias and discarding more than half of the data, they restrict their analysis to only the largest connected component—discarding an unreported proportion of whatever data they had left. That is not the symmetry we strived for.</p>
</sec>
<sec id="section5-0003122412463018">
<title>Concluding Remarks</title>
<p>BTU present three distinct analyses to challenge our methods and results. The first analysis shows that community allocation changes when tie type is taken into account. This has no relevance for our analysis, which makes no use of community allocation outcomes in the first place. In addition, community allocations may vary while modularity remains robust (<xref ref-type="bibr" rid="bibr4-0003122412463018">Good et al. 2010</xref>). Their second analysis aptly demonstrates our idea (see <xref ref-type="bibr" rid="bibr5-0003122412463018">Shwed and Bearman 2010</xref>:823–24) that a big part of scientific contestation is what we label benign contestation—the normal process of specialization. BTU’s final analysis provides support for all of the claims we made with respect to what is wrong with expert judgment and why symmetry is a goal to strive for.</p>
<p>Our finding is that scaled modularity of citation networks indexes contestation. Scientific consensus sometimes increases and sometimes decreases, on specific issues in specific times. BTU (2012:1058) write that they “believe that [our] claims and findings do not stand up to scrutiny.” And yet, the trends we find in scaled modularity, without any data manipulation, fit broad expert reports on each of our seven cases—two of which (in the cases of autism and non-ionizing radiation) were made after our analysis. Nowhere in their comment do BTU supply any shred of relevant evidence that contradicts any of our findings, theories, or methods. That is the nature of belief.</p>
<p>BTU (2012:1052) also argue that our scaling is not “within the theoretical framework on which modularity is built.” We plead guilty. We started with theoretical puzzles and insights from the sociology of science, found a tool (modularity) that seemed to relate to the theory, and went out to the field to test the utility of our tool. In the process, empirical reality (the fact that benign contestation is clearly size dependent) led us to modify our tool by scaling for size. We never attempted to be “within the theoretical framework” of our tool, because we use tools and theories to help us understand data. Theories and tools are something to work with, not something to be entraped within.</p>
<p>The main benefit of our method is that it enforces a robust symmetry. We leave no room for discretion, coder bias, or manipulation of context and scope in data selection. This is why our method is useful for evaluating the state of contestation on scientific issues laden with political interests. The symmetry constraint actually makes it possible for us to say that we can identify when scientists agree and disagree about important issues. We show that our approach works across such diverse cases as gravitational waves, anthropogenic climate change, and cancer and smoking. The method proposed in the comment is a technical expression of qualitative analyses that people have been doing for years. Our idea in the original article was to show why that analysis strategy is not useful. That our commentators did that for us is very kind indeed.</p>
</sec>
</body>
<back>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0003122412463018">
<label>1.</label>
<p>BTU report they replicated our data collection procedures, but they start their analysis some 27 years later than ours in the smoking case.</p></fn>
<fn fn-type="other" id="fn2-0003122412463018">
<label>2.</label>
<p>It appears not general enough to be applied to the subject of our article, which is why BTU use arbitrary randomization instead of real data, and real data is used in a radically different context.</p></fn>
</fn-group>
</notes>
<bio>
<p><bold>Uri Shwed</bold> is an Assistant Professor of Sociology at Ben Gurion University of the Negev. He chairs the masters program in critical studies of society. His research focuses on the sociology of science, stratification, networks, and methodology.</p>
<p><bold>Peter S. Bearman</bold> is the Cole Professor of Social Science at Columbia University. He is currently working on the autism epidemic, qualitative research design, large-scale civil conflict, and the problem of continuity.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0003122412463018">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bruggeman</surname><given-names>Jeroen</given-names></name>
<name><surname>Traag</surname><given-names>V. A.</given-names></name>
<name><surname>Uitermark</surname><given-names>Justus</given-names></name>
</person-group>. <year>2012</year>. “<article-title>Detecting Communities through Network Data</article-title>.” <source>American Sociological Review</source> <volume>77</volume>:<fpage>1050</fpage>–<lpage>1063</lpage>.</citation>
</ref>
<ref id="bibr2-0003122412463018">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cole</surname><given-names>Jonathan R.</given-names></name>
<name><surname>Cole</surname><given-names>Stephen</given-names></name>
</person-group>. <year>1981</year>. <source>Social Stratification in Science</source>. <publisher-loc>Chicago</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr3-0003122412463018">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fortunato</surname><given-names>Santo</given-names></name>
</person-group>. <year>2010</year>. “<article-title>Community Detection in Graphs</article-title>.” <source>Physics Reports</source> <volume>486</volume>:<fpage>75</fpage>–<lpage>174</lpage>.</citation>
</ref>
<ref id="bibr4-0003122412463018">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Good</surname><given-names>Benjamin H.</given-names></name>
<name><surname>de Montjoye</surname><given-names>Yves-Alexandre</given-names></name>
<name><surname>Clauset</surname><given-names>Aaron</given-names></name>
</person-group>. <year>2010</year>. “<article-title>The Performance of Modularity Maximization in Practical Contexts</article-title>.” <source>Physical Review</source> <volume>E 81</volume>, <fpage>046106</fpage>.</citation>
</ref>
<ref id="bibr5-0003122412463018">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shwed</surname><given-names>Uri</given-names></name>
<name><surname>Bearman</surname><given-names>Peter S.</given-names></name>
</person-group> <year>2010</year>. “<article-title>The Temporal Structure of Scientific Consensus Formation</article-title>.” <source>American Sociological Review</source> <volume>75</volume>:<fpage>817</fpage>–<lpage>40</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>