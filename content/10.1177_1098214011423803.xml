<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AJE</journal-id>
<journal-id journal-id-type="hwp">spaje</journal-id>
<journal-title>American Journal of Evaluation</journal-title>
<issn pub-type="ppub">1098-2140</issn>
<issn pub-type="epub">1557-0878</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1098214011423803</article-id>
<article-id pub-id-type="publisher-id">10.1177_1098214011423803</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Comparison of Participant and Practitioner Beliefs About Evaluation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Whitehall</surname>
<given-names>Anna K.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1098214011423803">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Hill</surname>
<given-names>Laura G.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1098214011423803">1</xref>
<xref ref-type="corresp" rid="corresp1-1098214011423803"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Koehler</surname>
<given-names>Christian R.</given-names>
</name>
<xref ref-type="aff" rid="aff2-1098214011423803">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-1098214011423803"><label>1</label>Department of Human Development, Washington State University, Pullman, WA, USA</aff>
<aff id="aff2-1098214011423803"><label>2</label>Washington State University, Grays Harbor, WA, USA</aff>
<author-notes>
<corresp id="corresp1-1098214011423803">Laura G. Hill, Department of Human Development, Washington State University, PO Box 644852, Pullman, WA 99164, USA Email: <email>laurahill@wsu.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2012</year>
</pub-date>
<volume>33</volume>
<issue>2</issue>
<fpage>208</fpage>
<lpage>220</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Evaluation Association</copyright-holder>
</permissions>
<abstract>
<p>The move to build capacity for internal evaluation is a common organizational theme in social service delivery, and in many settings, the evaluator is also the practitioner who delivers the service. The goal of the present study was to extend our limited knowledge of practitioner evaluation. Specifically, the authors examined practitioner concerns about administering pretest and posttest evaluations within the context of a multisite 7-week family strengthening program and compared those concerns with self-reported attitudes of the parents who completed evaluations. The authors found that program participants (<italic>n</italic> = 105) were significantly less likely to find the evaluation process intrusive, and more likely to hold positive beliefs about the evaluation process, than practitioners (<italic>n</italic> = 140) expected. Results of the study may address a potential barrier to effective practitioner evaluation—the belief that having to administer evaluations interferes with establishing a good relationship with program participants.</p>
</abstract>
<kwd-group>
<kwd>practitioner evaluation</kwd>
<kwd>barriers to evaluation</kwd>
<kwd>evaluation anxiety</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Building capacity for internal evaluation is a common organizational theme in social service delivery, education, health care delivery, community coalition work, and other disseminated services (<xref ref-type="bibr" rid="bibr2-1098214011423803">Cullen, Giles, &amp; Rosenthal, 2006</xref>; <xref ref-type="bibr" rid="bibr5-1098214011423803">Fetterman &amp; Wandersman, 2005</xref>; <xref ref-type="bibr" rid="bibr12-1098214011423803">King, 2002</xref>; <xref ref-type="bibr" rid="bibr27-1098214011423803">Taut, 2007</xref>). One approach to enhancing internal evaluation capacity is to designate and train internal evaluators—that is, individuals whose role within the organization or agency is to evaluate, not to deliver programs or services. A second, and increasingly popular, model is to use “practitioner evaluators”—that is, individuals whose role is both to deliver programming and to conduct evaluation (<xref ref-type="bibr" rid="bibr24-1098214011423803">Shaw &amp; Faulkner, 2006</xref>). Indeed, this practice is a core component of empowerment evaluation (see <xref ref-type="bibr" rid="bibr5-1098214011423803">Fetterman &amp; Wandersman, 2005</xref>). Practitioner evaluation may be comprehensive, including the design and administration of evaluation as well as analysis and reporting of results; it may be self-reflective and systematically integrated with practice; or it may be limited to executing an evaluation protocol designed by others. The skills required for a typical pretest, posttest data-gathering evaluation are likely to be different from those required for program implementation and service delivery, and interaction with clients during evaluation may have a different feel, different goals, and different pitfalls from interaction during program delivery (<xref ref-type="bibr" rid="bibr16-1098214011423803">Mills, 2002</xref>; <xref ref-type="bibr" rid="bibr18-1098214011423803">Myers-Walls, 2000</xref>). The goal of the present study was to extend our limited knowledge of practitioner evaluation by examining, within the context of a multisite dissemination of a family intervention, (a) practitioners’ responses to conducting evaluation, (b) participants’ concerns about completing an evaluation, and (c) practitioners’ predictions of participants’ concerns.</p>
<sec id="section2-1098214011423803">
<title>Benefits and Barriers</title>
<p>There is a substantial literature on the ways in which practitioner evaluation or research may benefit the quality of practice—for example, by encouraging self-evaluation and critical reflection about practice (<xref ref-type="bibr" rid="bibr21-1098214011423803">Riemann, 2005</xref>; <xref ref-type="bibr" rid="bibr23-1098214011423803">Shaw, 2005</xref>) or by helping to systematize service delivery (<xref ref-type="bibr" rid="bibr1-1098214011423803">Cherin, 2004</xref>). Moreover, practitioner evaluation has many potential benefits over outside evaluation (<xref ref-type="bibr" rid="bibr13-1098214011423803">Kinsey, 1981</xref>). First, practitioners can provide information about the context and operations of a program that may not be available to outside evaluators. Second, their program experience, which is deeper and more comprehensive than an outsider’s, can inform both the content and the process of an evaluation and thus improve its validity and utility. Third, practitioner evaluation, if done properly, may be more practical, efficient, and cost-effective than outside evaluation, especially in ongoing or multisite evaluations. Finally, practitioner evaluation may be more collaborative and participatory than outside evaluation, by virtue of the relationship between practitioner and participants, which in turn may result in better data and ultimately improved quality of implementation (<xref ref-type="bibr" rid="bibr4-1098214011423803">Donaldson, Gooler, &amp; Scriven, 2002</xref>; <xref ref-type="bibr" rid="bibr5-1098214011423803">Fetterman &amp; Wandersman, 2005</xref>).</p>
<p>Understanding practitioner-directed evaluation is important because organizations and agencies that dispense funds for evidence-based programming increasingly require that programs conduct outcome evaluation, even when there is little internal capacity to do so and when resources are inadequate to hire independent evaluators. As a result, practitioners who implement programs may conduct evaluation themselves or at least collect the data. Thus, practitioner evaluation constitutes a “hidden and invisible research practice” that we have little understanding or knowledge of (<xref ref-type="bibr" rid="bibr24-1098214011423803">Shaw &amp; Faulkner, 2006</xref>).</p>
<p>At the same time, there is a common perception that practitioners do not like conducting program evaluations and some evidence to support that belief. In a survey of 224 Cooperative Extension practitioners from across the United States, <xref ref-type="bibr" rid="bibr17-1098214011423803">Morford, Kozak, Suvedi, and Innes (2006)</xref> found that whereas 60% did not mind conducting program evaluation, 27% preferred to ignore it or dreaded it. Similarly, <xref ref-type="bibr" rid="bibr11-1098214011423803">Kegeles, Rebchook, and Tebbetts (2005)</xref>, in a qualitative study of community-based organizations (CBOs), reported that 20% of CBO leaders and two thirds of technical assistance providers felt that practitioner evaluators held negative attitudes about conducting evaluation for their programs.</p>
<p>Negative feelings about evaluation may be a function of practical barriers (perceived and real) to evaluation practice. For example, evaluation may feel cumbersome or intimidating to practitioners who know that evaluation should be done “right” but lack training in basic evaluation techniques (<xref ref-type="bibr" rid="bibr11-1098214011423803">Kegeles, Rebchook, &amp; Tebbetts, 2005</xref>; <xref ref-type="bibr" rid="bibr18-1098214011423803">Myers-Walls, 2000</xref>; <xref ref-type="bibr" rid="bibr28-1098214011423803">Taut &amp; Alkin, 2003</xref>). Organizational demands for evaluation in the absence of training, technical support, and resources may engender resistance, resentment, and ultimately a negative attitude toward evaluation generally—for example, <xref ref-type="bibr" rid="bibr10-1098214011423803">Hill and Parker (2005)</xref> found that 44% of extension faculty and staff surveyed felt that they had inadequate resources to conduct evaluation. Even with adequate training and organizational support, practitioners may perceive the time spent conducting an evaluation as time lost to program implementation (<xref ref-type="bibr" rid="bibr13-1098214011423803">Kinsey, 1981</xref>; <xref ref-type="bibr" rid="bibr17-1098214011423803">Morford et al., 2006</xref>; <xref ref-type="bibr" rid="bibr18-1098214011423803">Myers-Walls, 2000</xref>), and infringements on program time are likely to foster negative responses to the process of evaluation.</p>
<p>Negative feelings may also be a function of motivational barriers to evaluation. For example, practitioners may be skeptical about the meaningfulness of evaluation and mistrust the reduction of rich experience to data points (<xref ref-type="bibr" rid="bibr8-1098214011423803">Harvey &amp; Oliver, 2002</xref>; <xref ref-type="bibr" rid="bibr16-1098214011423803">Mills, 2002</xref>; <xref ref-type="bibr" rid="bibr18-1098214011423803">Myers-Walls, 2000</xref>; <xref ref-type="bibr" rid="bibr24-1098214011423803">Shaw &amp; Faulkner, 2006</xref>), and they may be skeptical about the utility of evaluation if they do not see results used actively (<xref ref-type="bibr" rid="bibr13-1098214011423803">Kinsey, 1981</xref>; <xref ref-type="bibr" rid="bibr19-1098214011423803">Patton, 2008</xref>). Conversely, practitioners may fear that program evaluation <italic>will</italic> be used to assess their performance (<xref ref-type="bibr" rid="bibr12-1098214011423803">King, 2002</xref>; <xref ref-type="bibr" rid="bibr17-1098214011423803">Morford et al., 2006</xref>) or to justify discontinuing program funding (<xref ref-type="bibr" rid="bibr11-1098214011423803">Kegeles et al., 2005</xref>). Often, even when such threats are not perceived or do not materialize, for many practitioners the process of evaluation is neither rewarding nor rewarded (<xref ref-type="bibr" rid="bibr16-1098214011423803">Mills, 2002</xref>; <xref ref-type="bibr" rid="bibr18-1098214011423803">Myers-Walls, 2000</xref>); thus, motivation to conduct evaluation is low.</p>
</sec>
<sec id="section3-1098214011423803">
<title>The Present Study</title>
<p>In the current investigation, we examined another motivational barrier to practitioner evaluation: The possibility that evaluation may be perceived as intrusive or annoying by program participants (<xref ref-type="bibr" rid="bibr9-1098214011423803">Hill &amp; Betz, 2005</xref>; <xref ref-type="bibr" rid="bibr18-1098214011423803">Myers-Walls, 2000</xref>). Even if a practitioner evaluator feels confident in her technical skills, has organizational support, and considers evaluation an integral part of program implementation, the belief that evaluation is aversive to participants could pose a significant barrier. In the present study, we explore the degree to which practitioner evaluators hold such a belief.</p>
<sec id="section4-1098214011423803">
<title>The Strengthening Families Program</title>
<p>We conducted this study within the context of an ongoing (2001–present) regional dissemination of an evidence-based family strengthening program (the Strengthening Families Program for Parents and Youth 10–14 [SFP]), delivered in many communities across the U.S. Pacific Northwest. Each SFP cycle meets once a week for 2 hr over the course of 7 weeks. During the first hour, parents and youth meet separately and participate in a combination of group activities, discussions, games, and didactic presentations. Parents learn about adolescent development, practice communicating rules clearly and warmth openly, and learn a variety of methods to provide structure while also supporting their adolescents’ growing need for autonomy. Youth learn peer resistance skills, discuss their goals and dreams, and engage in activities designed to increase empathy for some of their parents’ fears and concerns. During the second hour, families come together for activities designed to increase involvement and strengthen family identity. SFP has a strong research base and is considered a “model” or “exemplary” program by a number of U.S. federal agencies. Youth whose families attended the program as part of a randomized clinical trial were less likely to have initiated substance use or to abuse alcohol, tobacco, and prescription medicines than control students 6 years later; have higher school attachment and school achievement; and are less likely to report depressive symptoms (<xref ref-type="bibr" rid="bibr25-1098214011423803">Spoth, Trudeau, Guyll, Shin, &amp; Redmond, 2009</xref>). The program is popular and is implemented widely throughout the United States as well as in Panama, Sweden, Turkey, Russia, England, Wales, Thailand, and other countries.</p>
</sec>
<sec id="section6-1098214011423803">
<title>SFP evaluation</title>
<p>We provide evaluation services, free of charge, to CBOs across the Pacific Northwest region of the United States. Thus, we have extensive contact with SFP practitioners in over 40 counties. The extension system of Washington’s land-grant university supports dissemination of SFP and regularly conducts trainings for practitioners from community agencies who are interested in implementing the program. Trainers present data on program effectiveness from the original clinical trial and also from the regional evaluation. They include instruction on how and when to conduct the evaluation and orient practitioners to the university’s SFP website and its section on evaluation, which includes downloadable pretest and posttest forms; evaluation protocols with scripts for both parent and youth pretests and posttests; sample reports; video instructions for preparing, administering, and sending in the evaluation; and contact information for anyone who wants technical assistance with the evaluation.</p>
<p>The evaluation is designed to be administered by practitioners on the first night (a quantitative pretest) and last night (a quantitative posttest with several open-ended questions). Practitioners send completed evaluations to a university researcher (the second author) who, with graduate research assistants, processes the data and sends back reports about parent and youth responses to the program. The evaluation is free and in most cases voluntary—though many funders require evidence of program effectiveness, most do not require that practitioners use this particular evaluation. At many sites, evaluation reports have been useful in obtaining continued funding for the program from various agencies, foundations, or local businesses.</p>
<p>Over the course of 10 years, we have processed evaluation data and produced reports for over 400 occurrences of the program. The third author has delivered the program and carried out the evaluation several times, and we have had numerous discussions with other practitioners who have conducted the evaluation throughout the years. Practitioners report that the pretest and posttest each take about 15–20 min on average, though some report much longer times (e.g., 45 min). Thus, the average evaluation uses up a significant portion of the 1-hr parent or youth session on the first night and of the 1-hr family session on the seventh (last) night of the program, unless extra time is added specifically for evaluation. The process, content, and length of the evaluation have been refined over time and in response to practitioner feedback, to minimize the time demand and to make it as easy as possible to administer. Nonetheless, many practitioners are dismayed by the evaluation burden: time taken away from program content and the creation of more paperwork to process.</p>
<p>We continue to look for ways to minimize the practical burden of evaluation. The present study, however, arose in response to anecdotal evidence that some practitioners were reluctant to implement the evaluation not because of practical barriers but because it was perceived as intrusive by participants. Other practitioners reported that they administered the evaluation, but conducted the pretest at the second meeting of the program, after having established rapport with program participants during the first meeting. There was no evidence, however, of participants objecting to the evaluation—that is, we received no reports of participants complaining about or refusing to complete evaluations. Hence, it was unclear whether participants actually found the evaluation intrusive, or whether that was simply a perception of practitioners.</p>
<p>Accordingly, our main objective was to examine the match between participant attitudes and practitioner beliefs about participant attitudes. We surveyed participants who attended SFP at different sites as well as SFP practitioners throughout the state. Because we had no evidence, anecdotal or otherwise, of participants objecting to or refusing to complete the evaluation, we hypothesized that practitioner perceptions of participant attitudes would be significantly more negative than participants’ self-reported attitudes. In our practitioner survey, we also included several items about specific barriers to evaluation in order to explore the applicability of findings from the studies cited above (e.g., <xref ref-type="bibr" rid="bibr10-1098214011423803">Hill &amp; Parker, 2005</xref>; <xref ref-type="bibr" rid="bibr11-1098214011423803">Kegeles et al., 2005</xref>; <xref ref-type="bibr" rid="bibr17-1098214011423803">Morford et al., 2006</xref>) in this different context.</p>
<p>The role of practitioner evaluators in the dissemination of SFP is participatory to some degree (e.g., in providing feedback and suggestions that lead to the ongoing revision of evaluation protocol, materials, and reporting), but the majority of practitioner involvement comes in administering the evaluation. Hence, practitioner evaluators described in the present study are not engaged in a dynamic and fully participatory evaluation process but rather in the execution of a standardized evaluation, with opportunity for input to change that evaluation.</p>
</sec>
</sec>
<sec id="section7-1098214011423803">
<title>Method</title>
<sec id="section8-1098214011423803">
<title>Procedure</title>
<p>We recruited practitioners for the study from a roster of SFP facilitators trained in the state of Washington between 2000 and 2006. After elimination of duplicate entries and those who were missing contact information, the eligible sample comprised 326 practitioners. We conducted the survey using the tailored design method (<xref ref-type="bibr" rid="bibr3-1098214011423803">Dillman, Smyth, &amp; Christian, 2009</xref>), with an initial letter informing potential respondents about the survey, followed 1 week later by a letter that included the survey, consent form, a stamped envelope with the researchers’ address preprinted, and a $2.00 bill. We sent reminder postcards in Week 4 to 151 practitioners from whom we had not heard back and a final mailing in Week 6 to the remaining pool of 132 nonrespondents. Forty-three surveys were returned as undeliverable. Of the remaining 283, 124 practitioners (44% of those with valid contact information, and 38% of the total mailings, including surveys returned for invalid contact information) submitted completed surveys. An additional 16 practitioners were recruited at a practitioner training that occurred during the course of the study, resulting in a final sample size of 140.</p>
<p>We contacted 11 site coordinators who had previously conducted SFP to explain the study and ask if they were willing to recruit participants from upcoming programs for the participant attitude survey. All coordinators agreed to participate. At those sites, practitioners announced the 20-item survey at the beginning of the program; consenting participants completed the survey along with the standard program pretest on the first night of the program. All participants at the 11 sites completed both the pretests and the attitude survey. Surveys (and program evaluation pretests and posttests) were anonymous. Participants were given a $5.00 gift card to a local store as a token of gratitude. The Institutional Review Board of Washington State University deemed procedures for this portion of the study exempt.</p>
</sec>
<sec id="section9-1098214011423803">
<title>Sample Characteristics</title>
<p>Of the 140 practitioners who completed the survey, 76% were female. Practitioner ages ranged from 21 to 72, with an average of 49 years. All practitioners had completed at least high school or general educational development (GED). Of 105 program participants, 68% were female. Participant ages ranged from 21 to 65, with an average age of 38 years. In <xref ref-type="table" rid="table1-1098214011423803">Table 1</xref>
, we report details of practitioner and participant race and ethnicity as well as additional demographic information about practitioners. We did not collect more extensive demographic information from participants because of practitioner concern about the time required to complete surveys.</p>
<table-wrap id="table1-1098214011423803" position="float">
<label>Table 1.</label>
<caption>
<p>Demographic Characteristics of Facilitator and Program Participant Samples From Washington State Strengthening Families Program</p>
</caption>
<graphic alternate-form-of="table1-1098214011423803" xlink:href="10.1177_1098214011423803-table1.tif"/>
<table>
<thead>
<tr>
<th>Demographic Characteristic</th>
<th>Facilitators <italic>n</italic> = 140 (<italic>n</italic>, % of total)</th>
<th>Program Participants <italic>n</italic> = 105 (<italic>n</italic>, % of total)</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Gender</td>
</tr>
<tr>
<td> Male</td>
<td>17 (12)</td>
<td>22 (21)</td>
</tr>
<tr>
<td> Female</td>
<td>107 (76)</td>
<td>71 (68)</td>
</tr>
<tr>
<td> Unreported</td>
<td>16 (11)</td>
<td>12 (11)</td>
</tr>
<tr>
<td colspan="3">Race/ethnicity</td>
</tr>
<tr>
<td> White/European</td>
<td>91 (65)</td>
<td>72 (67)</td>
</tr>
<tr>
<td> Asian/Pacific islander</td>
<td>2 (1)</td>
<td>0</td>
</tr>
<tr>
<td> Black/African American</td>
<td>3 (2)</td>
<td>0</td>
</tr>
<tr>
<td> American Indian/Alaska native</td>
<td>3 (2)</td>
<td>7 (7)</td>
</tr>
<tr>
<td> Latino/Latina</td>
<td>13 (9)</td>
<td>5 (5)</td>
</tr>
<tr>
<td> Other</td>
<td>9 (7)</td>
<td>5 (5)</td>
</tr>
<tr>
<td> Unreported</td>
<td>19 (14)</td>
<td>16 (15)</td>
</tr>
<tr>
<td colspan="3">Age</td>
</tr>
<tr>
<td> 18–30</td>
<td>6 (4)</td>
<td>18 (17)</td>
</tr>
<tr>
<td> 31–40</td>
<td>24 (17)</td>
<td>47 (45)</td>
</tr>
<tr>
<td> 41–50</td>
<td>30 (21)</td>
<td>30 (29)</td>
</tr>
<tr>
<td> 51–60</td>
<td>46 (33)</td>
<td>8 (8)</td>
</tr>
<tr>
<td> 61–70</td>
<td>11 (8)</td>
<td>1 (&lt;1)</td>
</tr>
<tr>
<td> 71–80</td>
<td>1 (&lt;1)</td>
<td>0</td>
</tr>
<tr>
<td> Unreported</td>
<td>16 (16)</td>
<td>1 (&lt;1)</td>
</tr>
<tr>
<td colspan="3">Questions asked of facilitators only</td>
</tr>
<tr>
<td>Education (completed)</td>
<td colspan="2">
</td>
</tr>
<tr>
<td> Sixth grade</td>
<td>0</td>
<td>
</td>
</tr>
<tr>
<td> Ninth grade</td>
<td>0</td>
<td>
</td>
</tr>
<tr>
<td> Eleventh grade</td>
<td>0</td>
<td>
</td>
</tr>
<tr>
<td> GED/Twelfth grade</td>
<td>13 (9)</td>
<td>
</td>
</tr>
<tr>
<td> Associate’s degree</td>
<td>21 (15)</td>
<td>
</td>
</tr>
<tr>
<td> Bachelor’s degree</td>
<td>44 (31)</td>
<td>
</td>
</tr>
<tr>
<td> Master’s degree</td>
<td>42 (30)</td>
<td>
</td>
</tr>
<tr>
<td> PhD/professional training</td>
<td>4 (3)</td>
<td>
</td>
</tr>
<tr>
<td> Unreported</td>
<td>16 (11)</td>
<td>
</td>
</tr>
<tr>
<td colspan="3">Employment</td>
</tr>
<tr>
<td> Currently working</td>
<td>117 (84)</td>
<td>
</td>
</tr>
<tr>
<td> Not currently working</td>
<td>7 (5)</td>
<td>
</td>
</tr>
<tr>
<td> Unreported</td>
<td>16 (11)</td>
<td>
</td>
</tr>
<tr>
<td colspan="3">Types of jobs</td>
</tr>
<tr>
<td> Administrator/program coordinator</td>
<td>40 (29)</td>
<td>
</td>
</tr>
<tr>
<td> Counseling</td>
<td>32 (23)</td>
<td>
</td>
</tr>
<tr>
<td> Parent educator</td>
<td>5 (4)</td>
<td>
</td>
</tr>
<tr>
<td> Teacher/program facilitator</td>
<td>19 (14)</td>
<td>
</td>
</tr>
<tr>
<td> Other</td>
<td>12 (9)</td>
<td>
</td>
</tr>
<tr>
<td> Unreported</td>
<td>29 (21)</td>
<td>
</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1098214011423803">
<p>
<italic>Note:</italic> In some cases percentages do not sum to 100% because of rounding error.</p>
</fn>
<fn id="table-fn2-1098214011423803">
<p>GED = general educational development.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section10-1098214011423803">
<title>Measures</title>
<sec id="section11-1098214011423803">
<title>Beliefs and feelings about evaluation</title>
<p>We created items designed to assess positive feelings, negative feelings, and neutral feelings about evaluation. Practitioners responded to 6 items asking how they felt about evaluation (<italic>The following questions ask you to report your own feelings about conducting program evaluations for the SFP(s) that you have facilitated</italic>) and 10 items asking how they thought participants felt about evaluation (<italic>Please mark the answer that best matches your thoughts about how program participants respond to being evaluated</italic>; see <xref ref-type="table" rid="table2-1098214011423803">Table 2</xref>
 for complete list of items). The latter set of 10 items matched the 10 items that program participants answered about evaluation beliefs (<italic>For the following questions, please mark the answer that best matches your thoughts or feelings</italic>). Response options for both sets of questions on a Likert-type 5-point scale ranged from <italic>strongly agree</italic> to <italic>strongly disagree</italic>.</p>
<table-wrap id="table2-1098214011423803" position="float">
<label>Table 2.</label>
<caption>
<p>Average Scale and Item Scores for Participants and Practitioners</p>
</caption>
<graphic alternate-form-of="table2-1098214011423803" xlink:href="10.1177_1098214011423803-table2.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>SFP Parent Participants (<italic>n</italic> = 105)</th>
<th>Practitioner Sample I (<italic>n</italic> = 16)</th>
<th>Practitioner Sample II (<italic>n</italic> = 124)</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="4">Evaluation belief scales</td>
</tr>
<tr>
<td>Learning</td>
<td>3.56 (0.81)</td>
<td>3.34 (0.77)</td>
<td>3.53 (0.75)</td>
</tr>
<tr>
<td>Negative</td>
<td>2.52 (0.60)<sub>a</sub>
</td>
<td>3.17 (0.55)<sub>b</sub>
</td>
<td>3.04 (0.65)<sub>b</sub>
</td>
</tr>
<tr>
<td>Positive</td>
<td>4.23 (0.45)<sub>a</sub>
</td>
<td>3.56 (0.45)<sub>b</sub>
</td>
<td>3.51 (0.58)<sub>b</sub>
</td>
</tr>
<tr>
<td colspan="4">Evaluation belief scale items—Participants and practitioner respondents<sup>a</sup>
</td>
</tr>
<tr>
<td>I feel that my opinions are a valuable part of the evaluation process [OR: Participants feel that their opinions are a valuable part of the evaluation process <italic>(Practitioner version)</italic>
<sup>a</sup>]</td>
<td>4.07 (0.58)<sub>a</sub>
</td>
<td>3.75 (0.68)</td>
<td>3.56 (0.75)<sub>b</sub>
</td>
</tr>
<tr>
<td>I feel that filling out evaluations takes too much time</td>
<td>2.50 (0.80)<sub>a</sub>
</td>
<td>3.06 (1.00)<sub>b</sub>
</td>
<td>3.04 (0.91)<sub>b</sub>
</td>
</tr>
<tr>
<td>I answer the questions on evaluations honestly</td>
<td>4.56 (0.54)<sub>a</sub>
</td>
<td>3.06 (0.44)<sub>b</sub>
</td>
<td>3.40 (0.76)<sub>b</sub>
</td>
</tr>
<tr>
<td>I believe that evaluations are an invasion of my privacy</td>
<td>2.12 (0.79)<sub>a</sub>
</td>
<td>2.75 (0.77)<sub>b</sub>
</td>
<td>2.73 (0.79)<sub>b</sub>
</td>
</tr>
<tr>
<td>I am happy to give my opinions on evaluations</td>
<td>4.07 (0.77)<sub>a</sub>
</td>
<td>3.63 (0.72)</td>
<td>3.48 (0.71)<sub>b</sub>
</td>
</tr>
<tr>
<td>Some parts of the evaluation process I do not enjoy</td>
<td>3.07 (0.90)<sub>a</sub>
</td>
<td>3.63 (0.72)<sub>b</sub>
</td>
<td>3.55 (0.75)<sub>b</sub>
</td>
</tr>
<tr>
<td>I am uncomfortable filling out evaluations</td>
<td>2.35 (0.81)<sub>a</sub>
</td>
<td>3.25<sub>a</sub> (0.86)<sub>b</sub>
</td>
<td>2.83 (0.81)<sub>c</sub>
</td>
</tr>
<tr>
<td>I learn something about the program by completing evaluations</td>
<td>3.50 (0.90)</td>
<td>3.25 (1.00)</td>
<td>3.47 (0.84)</td>
</tr>
<tr>
<td>I believe that evaluating a program may help to improve it</td>
<td>4.33 (0.61)<sub>a</sub>
</td>
<td>3.81 (0.54)<sub>b</sub>
</td>
<td>3.62 (0.78)<sub>b</sub>
</td>
</tr>
<tr>
<td>I learn something about myself by completing evaluations</td>
<td>3.62 (0.87)</td>
<td>3.44 (0.81)</td>
<td>3.59 (0.3)</td>
</tr>
<tr>
<td colspan="4">
<italic>Individual items—Practitioner respondents only</italic>
</td>
</tr>
<tr>
<td>Conducting a “pretest” evaluation on the first night of a program makes it harder to establish rapport with participants</td>
<td align="center">—</td>
<td>2.93 (1.11)</td>
<td>2.56 (0.96)</td>
</tr>
<tr>
<td>I am uncomfortable administering evaluation surveys to program participants</td>
<td align="center">—</td>
<td>2.12 (0.88)</td>
<td>1.94 (0.85)</td>
</tr>
<tr>
<td>It is easy for me to describe to program participants why we are conducting an evaluation</td>
<td align="center">—</td>
<td>3.81 (0.96)</td>
<td>3.63 (1.31)</td>
</tr>
<tr>
<td>Evaluations invade participants’ personal privacy</td>
<td align="center">—</td>
<td>2.42 (0.82)</td>
<td>2.13 (0.89)</td>
</tr>
<tr>
<td>Evaluating a program may help to improve it</td>
<td align="center">—</td>
<td>4.34 (0.81)</td>
<td>4.69 (0.60)</td>
</tr>
<tr>
<td>The evaluation process takes too much time</td>
<td align="center">—</td>
<td>2.84 (1.01)</td>
<td align="center">—</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-1098214011423803">
<p>
<italic>Note:</italic> Practitioner I = pilot sample of SFP practitioner trainees; Practitioner II = mail survey sample. <sup>a</sup>Participants responded to items about how they felt (<italic>For the following questions, please mark the answer that best matches your thoughts or feelings</italic>); practitioners were asked to rate how they thought program participants felt (<italic>Please mark the answer that best matches your thoughts about how program participants respond to being evaluated</italic>), and items were slightly reworded, as shown in first items. Mean values with different letter subscripts are significantly different at <italic>p</italic> &lt; .05 (Tukey’s correction used for multiple comparisons).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>An initial exploratory factor analysis using a varimax rotation indicated that a three-factor solution best described the data for both practitioners and participants (see <xref ref-type="table" rid="table3-1098214011423803">Table 3</xref>
 for items in each factor). We then used structural equation modeling to test for adequacy of the model. Fit indices for the three-factor model indicated a good fit, χ<sup>2</sup>(26) = 38.99, <italic>p</italic> = .05; comparative fit index (CFI) = .99; root mean square error of approximation (RMSEA) = .04, 90% confidence interval (CI) = [.00, .07], whereas a single-factor solution provided an inadequate fit, χ<sup>2</sup>(29) =178.82, <italic>p</italic> &lt; .00; CFI = .83; RMSEA = .15, 90% CI = [.13, .17]. We averaged items within each of the three factors to create subscales; we called these subscales “positive orientation” (α = .81) “negative orientation” (α = .79), and “learning orientation” (α = .71).</p>
<table-wrap id="table3-1098214011423803" position="float">
<label>Table 3.</label>
<caption>
<p>Factor Loadings for Items on the Survey of Evaluation Beliefs</p>
</caption>
<graphic alternate-form-of="table3-1098214011423803" xlink:href="10.1177_1098214011423803-table3.tif"/>
<table>
<thead>
<tr>
<th rowspan="2">Variables</th>
<th colspan="3">Scales</th>
</tr>
<tr>
<th>Negative</th>
<th>Positive</th>
<th>Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Uncomfortable</td>
<td>
<bold>.80</bold>
</td>
<td>−.31</td>
<td>−.07</td>
</tr>
<tr>
<td>Do not enjoy</td>
<td>
<bold>.77</bold>
</td>
<td>−.02</td>
<td>−.08</td>
</tr>
<tr>
<td>Too much time</td>
<td>
<bold>.76</bold>
</td>
<td>−.08</td>
<td>−.12</td>
</tr>
<tr>
<td>Invasion of privacy</td>
<td>
<bold>.70</bold>
</td>
<td>−.31</td>
<td>−.04</td>
</tr>
<tr>
<td>May improve program</td>
<td>.01</td>
<td>
<bold>.79</bold>
</td>
<td>.14</td>
</tr>
<tr>
<td>Answer honestly</td>
<td>−.19</td>
<td>
<bold>.74</bold>
</td>
<td>.07</td>
</tr>
<tr>
<td>Opinion valuable</td>
<td>−.18</td>
<td>
<bold>.72</bold>
</td>
<td>.24</td>
</tr>
<tr>
<td>Happy to give opinion</td>
<td>−.40</td>
<td>
<bold>.61</bold>
</td>
<td>.23</td>
</tr>
<tr>
<td>Learn about program</td>
<td>−.06</td>
<td>.20</td>
<td>
<bold>.87</bold>
</td>
</tr>
<tr>
<td>Learn about self</td>
<td>−.15</td>
<td>.20</td>
<td>
<bold>.85</bold>
</td>
</tr>
<tr>
<td>Eigenvalues</td>
<td>4.49</td>
<td>1.27</td>
<td>
<bold>1.</bold>1.0404</td>
</tr>
<tr>
<td>Percent of variance explained</td>
<td align="center">45%</td>
<td align="center">13%</td>
<td align="center">10%</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-1098214011423803">
<p>Note: Sample (<italic>n</italic> = 245) includes both practitioners and program participants. Bolded figures represent items with high loadings on the factor represented in that column.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section12-1098214011423803">
<title>Marlowe–Crowne Social Desirability Scale</title>
<p>Program participants completed a 10-item version of the Marlowe–Crowne Social Desirability Scale (<xref ref-type="bibr" rid="bibr26-1098214011423803">Strahan &amp; Gerbasi, 1972</xref>); use of this scale enabled us to rule out the possibility that self-reported positive attitudes about evaluation were a function of socially desirable response bias. Sample items on the Marlowe–Crowne include “When I don’t know something I don’t at all mind” and “I never hesitate to go out of my way to help someone in trouble” with forced-choice response options of “True” or “False.” Scale scores are calculated as the sum of “Yes” answers, so that higher scores indicate a higher likelihood of positive self-presentation. The average score in our sample was 4.54 (<italic>SD</italic> = 2.14, range 0–9), with a median and modal score of 4. The average scores are nearly identical to those reported by <xref ref-type="bibr" rid="bibr26-1098214011423803">Strahan and Gerbasi (1972)</xref> from a sample of college men and women (<italic>M</italic>s 4.5 and 4.6, <italic>SD</italic>s both 2.1), which indicates that our sample did not show an unusually strong motivation to respond in a socially desirable fashion. Reliability in our sample was low (Cronbach’s α = .59). However, this short form of the scale has been recommended by other authors because it is less burdensome and more easily accepted than the longer form, and because it is less sensitive to differences in age and socioeconomic status than other short forms (<xref ref-type="bibr" rid="bibr7-1098214011423803">Fraboni &amp; Cooper, 1989</xref>).</p>
</sec>
</sec>
</sec>
<sec id="section13-1098214011423803">
<title>Results</title>
<sec id="section14-1098214011423803">
<title>Descriptive Statistics</title>
<p>Descriptive statistics for scale variables are shown in <xref ref-type="table" rid="table2-1098214011423803">Table 2</xref>. For both participants and practitioners, scores on the Negative Orientation scale are negatively correlated with those on the Positive and Learning Orientation scales (see <xref ref-type="table" rid="table4-1098214011423803">Table 4</xref>
). Scores on the short version of the Marlowe–Crowne Social Desirability scale, administered only to program participants, were not significantly correlated with any of the Orientation scales, indicating that responses to those scales were not biased by a motivation to provide socially desirable answers.</p>
<table-wrap id="table4-1098214011423803" position="float">
<label>Table 4.</label>
<caption>
<p>Correlations of Evaluation Beliefs Subscales for Practitioners and Participants</p>
</caption>
<graphic alternate-form-of="table4-1098214011423803" xlink:href="10.1177_1098214011423803-table4.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Negative Orientation</th>
<th>Positive Orientation</th>
<th>Learning Orientation</th>
<th>Missingness</th>
</tr>
</thead>
<tbody>
<tr>
<td>Negative orientation</td>
<td>−</td>
<td>−.46<sup><xref ref-type="table-fn" rid="table-fn6-1098214011423803">*</xref></sup></td>
<td>−.27<sup><xref ref-type="table-fn" rid="table-fn6-1098214011423803">*</xref></sup></td>
<td>.06</td>
</tr>
<tr>
<td>Positive orientation</td>
<td>−.61<sup><xref ref-type="table-fn" rid="table-fn6-1098214011423803">*</xref></sup></td>
<td>−</td>
<td>.45<sup><xref ref-type="table-fn" rid="table-fn6-1098214011423803">*</xref></sup></td>
<td>.00</td>
</tr>
<tr>
<td>Learning orientation</td>
<td>−.53<sup><xref ref-type="table-fn" rid="table-fn6-1098214011423803">*</xref></sup></td>
<td>.49<sup><xref ref-type="table-fn" rid="table-fn6-1098214011423803">*</xref></sup></td>
<td>−</td>
<td>−.10</td>
</tr>
<tr>
<td>Missingness</td>
<td>.04</td>
<td>.01</td>
<td>−.02</td>
<td>–</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-1098214011423803">
<p>Note: Intercorrelations of subscales above the diagonal represent the practitioner sample (<italic>n</italic> = 140) and below the diagonal represent the program participant sample (<italic>n</italic> = 105).</p>
</fn>
<fn id="table-fn6-1098214011423803">
<p>*<italic>p</italic> &lt; .05.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section15-1098214011423803">
<title>Practitioner Perceptions of Participant Orientation Toward Evaluation</title>
<p>Results confirmed our hypothesis that practitioners would overestimate participants’ negative feelings and underestimate their positive feelings about evaluation. Independent <italic>t</italic> tests showed that practitioners rated participants lower in their Positive Orientation (<italic>t</italic> = 11.38, <italic>p</italic> &lt; .001) and significantly higher in their Negative Orientation (<italic>t</italic> = 6.70, <italic>p</italic> &lt; .001) toward evaluation than participants rated themselves. We had no specific hypothesis about Learning Orientation, and practitioners and participants did not differ in their response to the Learning Orientation items (<italic>t</italic> = 0.54, <italic>p</italic> = .59).</p>
</sec>
<sec id="section16-1098214011423803">
<title>Practitioners’ Own Beliefs and Feelings About Evaluation</title>
<p>Almost 30% of practitioners felt that administering an evaluation on the first night of a program made it harder to establish rapport with participants, but only 9% felt that evaluation was an invasion of participants’ privacy. Only 8% felt uncomfortable administering an evaluation; 76% said it was easy for them to explain the reasons for conducting an evaluation; and 93% felt that evaluation might help improve a program. Twenty-five percent felt that evaluation took up too much time. In short, practitioner attitudes about evaluation were largely positive, though a minority believed that it interfered with their initial relationship building with participants and were concerned about the time burden.</p>
</sec>
<sec id="section17-1098214011423803">
<title>Post Hoc Analyses</title>
<p>We conducted post hoc regression analyses to determine whether practitioner characteristics, including their own feelings about evaluation, were related to their estimates of participant feelings about evaluation. We controlled for practitioners’ own beliefs about evaluation and examined the relation of practitioner demographics (gender, minority status, education, and age) with their beliefs about participant attitudes. Older practitioners were significantly more likely to believe that participants felt positively about evaluation (<italic>B</italic> = 0.02; <italic>p</italic> &lt; .001); demographic characteristics were unrelated to negative or learning beliefs. Among participants, demographic characteristics were unrelated to scores on any of the belief subscales.</p>
<p>We also examined whether the number of missing values (i.e., items with no response) was associated with beliefs about evaluation. More than 80% of respondents answered all items. The proportion of program participants with complete data (84.8%) was higher than that of practitioners (74.6%), and a <italic>t</italic> test of mean differences in amount of missing data was significant (<italic>t</italic> = 3.13, <italic>p</italic> &lt; .005). The correlation of amount of missing data with positive, negative, and learning belief subscales was nonsignificant among both participants and practitioners, indicating that specific beliefs about evaluation, as measured by our items, did not influence willingness to complete all items on the evaluation.</p>
<p>We also compared the participants in the current study with all other SFP participants who have participated in the regional SFP evaluation between 2003 and 2011 (<italic>n</italic> = 4,911) to see if there were systematic differences between the two groups. We found no significant differences between participants in the current study and SFP participants in age (<italic>t = .</italic>94, <italic>p =</italic> .35) or gender (<italic>χ</italic>
<sup>2</sup> = 1.77, <italic>p</italic> = .18).</p>
</sec>
</sec>
<sec id="section18-1098214011423803">
<title>Discussion</title>
<p>The goal of this study was to examine one possible motivational barrier to conducting program evaluation. Specifically, we hypothesized that practitioners would estimate that participants in a family-strengthening intervention would hold more strongly negative and less strongly positive attitudes about evaluation than they actually did. Results supported this hypothesis: participants were significantly more positive and less negative than practitioners thought they were. Examination of practitioners’ own feelings about evaluation revealed that only a minority believed that evaluation was intrusive, and a strong majority felt confident about administering the evaluation and believed it was useful. Consistent with previous research, about a quarter to a third of practitioner evaluators felt that evaluation took too much time and interfered with initial rapport building. However, to the best of our knowledge, this is the first study to demonstrate that practitioner attitudes may differ from those of the groups they serve (but see <xref ref-type="bibr" rid="bibr22-1098214011423803">Schwandt &amp; Dahler-Larsen, 2006</xref>).</p>
<sec id="section19-1098214011423803">
<title>Limitations</title>
<p>Our findings and their implications for training are likely influenced by the context of the evaluation. The program in question, SFP, is a voluntary intervention with an emphasis on promoting family strengths; this positive focus may explain in part the positive attitudes of participants toward evaluation. It is a universal program and therefore does not target families identified as having problems, nor does the evaluation ask parents to report highly sensitive information (e.g., about physical discipline or substance use). Findings may not generalize to programs with different types of content (e.g., remediation or court-mandated programs), different types of clientele (e.g., participants with legal difficulties or those with physical or emotional vulnerabilities), or different types of evaluation (e.g., those that request sensitive information).</p>
<p>All practitioner evaluators who were asked to have their programs take part in the study of participant attitudes agreed, and all participants in those programs present at pretest completed the survey. However, the practitioner survey mailed to all eligible practitioners from the SFP training roster had a low response rate (44%), which raises the possibility of response bias. Of particular concern is the bias we would encounter if practitioners in our sample were unusually negative about evaluation. In that case, we would have a sample of program participants who were (conceivably) more positive than most about evaluation because of the nature of the program compared to a sample of practitioners who were more negative about evaluation. This would exaggerate the difference between practitioner and participant beliefs, thus limiting the generalizability of our findings and the likelihood of their replication even in similar settings.</p>
<p>Three factors argue against a strong concern about a negativity bias: First, our findings on practitioner reports of perceived barriers to evaluation are congruent with those of other studies (e.g., similar proportions of practitioner evaluators concerned about evaluation taking up too much time, as reported in <xref ref-type="bibr" rid="bibr13-1098214011423803">Kinsey, 1981</xref>; <xref ref-type="bibr" rid="bibr17-1098214011423803">Morford et al., 2006</xref>; <xref ref-type="bibr" rid="bibr18-1098214011423803">Myers-Walls, 2000</xref>). This leads us to believe that concerns about program evaluation interfering with programming or with the establishment of rapport may be common to practitioners across a variety of settings. Second, the subsample of practitioners whose programs participated in the study were not significantly different in their beliefs, or their estimates of participant beliefs, than the overall sample of practitioners who completed the mailed survey. We believe that the former group, who voluntarily committed to their programs’ involvement in the study, were likely to represent the positive end of the spectrum of beliefs about evaluation. Since they were no different from the larger group, it does not appear that practitioner responses were especially negatively biased. And finally, we believe that practitioner evaluators who held especially negative attitudes about evaluation would be less likely to complete and return the survey, and that our sample therefore represents those who are more positive about evaluation generally and presumably also in their estimates of participant attitudes. This assumption is also consistent with the attitudinal match between the smaller and larger practitioner subsamples. Thus, it appears that our findings are in fact conservative: With a more representative sample, there would have been an even greater discrepancy between practitioner estimates and participant self-reports.</p>
<p>However, our inferences are speculative and must be weighed in light of the possibility of selection bias. In addition, the generalizability (and meaning) of these findings can only be established with additional research. Future studies should include representative practitioner and participant samples from different types of programs and with a sample that allows for comparison of respondents with nonrespondents.</p>
</sec>
<sec id="section20-1098214011423803">
<title>Implications for Practice</title>
<p>These findings have implications both for program practice and for the training of practitioners and others, such as teachers, who may be required to conduct evaluations of programs and curricula delivered in community or school settings. An unexpected finding was that both practitioners and program participants saw evaluation as a learning opportunity. This finding is consistent with principles of both empowerment evaluation (<xref ref-type="bibr" rid="bibr5-1098214011423803">Fetterman &amp; Wandersman, 2005</xref>) and organizational development (<xref ref-type="bibr" rid="bibr20-1098214011423803">Preskill &amp; Torres, 1999</xref>)—ownership of the evaluation process enhances credibility of findings and creates a fertile context for learning. When administration of an evaluation is a requirement of program delivery, it may be helpful to emphasize in training for programs such as the one described here the fact that participants generally hold positive attitudes about evaluation, do not find it intrusive, and understand its value as a quality improvement tool. (We reemphasize, however, that this finding may not generalize to other types of programs.) In other words, the data presented here may help dispel practitioner concerns about negative effects of administering pretests and posttests.</p>
<p>In addition, training of practitioner evaluators can include discussion of the utility of evaluation as part of programming itself, aside from quality improvement or documentation of outcomes (<xref ref-type="bibr" rid="bibr27-1098214011423803">Taut, 2007</xref>). For example, pretests may provide an opportunity to preview program content and goals, and posttests provide an opportunity to reflect on what has been learned (<xref ref-type="bibr" rid="bibr9-1098214011423803">Hill &amp; Betz, 2005</xref>). We now use this information when we present the evaluation in practitioner trainings, proposing that the evaluation be approached (and also discussed with participants) as contributing to program goals rather than simply as an external imposition. Indeed, practitioner evaluators are in a much better position to integrate evaluation into program practice than outside evaluators. This provides an additional rationale for the participation of practitioner evaluators in the design of an evaluation since alignment of evaluation content and process with program practice would increase the likelihood of its utility, usability, and subsequent integration into program practice. By extension, and in the tradition of action research, input from participants on how evaluation contributes to their experience of a program would increase the validity and utility of evaluation as a component of practice.</p>
<p>Perhaps more meaningfully, the finding that participants and practitioners both perceived evaluation as an opportunity to learn more about the program and about themselves suggests richer possibilities for practitioner evaluation and research. In the present study, the evaluation was a traditional, standardized pretest/posttest tool used primarily to document quantitative program outcomes for funders, a format often experienced by practitioners as intrusive and external to their service goals. However, knowing that participants appreciate evaluation as a mode of learning might help practitioners think about how to engage participants in a more reflective evaluation process—to pose questions that would facilitate participant learning as well as their own and that would provide information directly useful in improving practice.</p>
</sec>
<sec id="section21-1098214011423803">
<title>Further Research and Conclusions</title>
<p>The present study was designed to answer a single question that arose directly from practice, in the context of an ongoing program evaluation: Did program participants feel as negatively about evaluation as many practitioner evaluators appeared to think? We approached the study, and have presented it, as a straightforward empirical examination of that particular question. However, design of future research on practitioner evaluation in evidence-based program delivery would benefit from attention to the larger theoretical literature on practitioner research. In particular, future studies examining barriers to evaluation and resistance to conducting evaluation, reviewed briefly in the introduction, might be expanded to include questions about whether the content, method, and results of evaluation are perceived as congruent with professional practice and identity; and about how those perceptions vary as a function of the extent to which practitioners experience ownership of the evaluation (<xref ref-type="bibr" rid="bibr6-1098214011423803">Fook, 2002</xref>; <xref ref-type="bibr" rid="bibr14-1098214011423803">Lang, 1994</xref>; <xref ref-type="bibr" rid="bibr15-1098214011423803">Lunt et al., 2008</xref>).</p>
<p>One significant concern, reported by nearly a third of practitioner evaluators, was that evaluation on the first night of a program interferes with their ability to build rapport with participants. Data in the current study cannot tell us the extent to which this concern is warranted from the perspective of participants; an observational study, direct survey of participant engagement, or interviews of participants would be needed to provide information about the relation of pretest evaluation to rapport building. In addition, participant attitudes about evaluation should be explored in other, more sensitive, contexts (e.g., programs for substance abuse treatment or domestic violence), where questions are more likely to be experienced as intrusive.</p>
<p>In conclusion, our results provide information about a specific barrier to practitioner evaluation—the concern that participants do not like to complete program evaluations—within the context of a universal, community-based family intervention. Our hope is that by providing this information to practitioner evaluators and trainees, we will decrease one source of evaluation anxiety (<xref ref-type="bibr" rid="bibr4-1098214011423803">Donaldson et al., 2002</xref>) and stimulate additional research into practitioner evaluation.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgment</title>
<p>The authors would like to thank Brianne Hood for her research assistance, the program facilitators and families who participated in the program evaluation, and the Washington state Department of Social and Health Services for their collaboration.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn1-1098214011423803">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<fn-group>
<fn fn-type="financial-disclosure" id="fn2-1098214011423803">
<p>The author(s) disclosed receipt of the following financial support for the research, authorship and/or publication of this article: The study was supported in part by two grants from the National Institute of Drug Abuse of the U.S. National Institutes of Health (R21 DA025139-01Al and R21 DA19758-01).</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1098214011423803">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cherin</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Collecting and using data for organizational learning</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Austin</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Hopkins</surname>
<given-names>K. M.</given-names>
</name>
</person-group> (Eds.), <source>Supervision as collaboration in the human services: Building a learning culture</source> (pp. <fpage>240</fpage>–<lpage>251</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr2-1098214011423803">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cullen</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Giles</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Rosenthal</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2006</year>). <source>Evaluating community-based child health promotion programs: A snapshot of strategies and methods</source>. <publisher-loc>Portland, ME</publisher-loc>: <publisher-name>National Academy for State Health Policy</publisher-name>.</citation>
</ref>
<ref id="bibr3-1098214011423803">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Dillman</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Smyth</surname>
<given-names>J. D.</given-names>
</name>
<name>
<surname>Christian</surname>
<given-names>L. M.</given-names>
</name>
</person-group> (<year>2009</year>). <source>Internet, mail, and mixed-mode surveys: The tailored design method</source>. (<edition>3rd ed</edition>.). <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr4-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Donaldson</surname>
<given-names>S. I.</given-names>
</name>
<name>
<surname>Gooler</surname>
<given-names>L. E.</given-names>
</name>
<name>
<surname>Scriven</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Strategies for managing evaluation anxiety: Toward a psychology of program evaluation</article-title>. <source>American Journal of Evaluation</source>, <volume>23</volume>, <fpage>261</fpage>–<lpage>273</lpage>. <comment>doi:10.1016/s1098-2140(02)00209-6</comment>
</citation>
</ref>
<ref id="bibr5-1098214011423803">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fetterman</surname>
<given-names>D. M.</given-names>
</name>
<name>
<surname>Wandersman</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2005</year>). <source>Empowerment evaluation principles in practice</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fook</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Theorizing from practice: Towards an inclusive approach for social work research</article-title>. <source>Qualitative Social Work</source>, <volume>1</volume>, <fpage>79</fpage>–<lpage>95</lpage>.</citation>
</ref>
<ref id="bibr7-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fraboni</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Cooper</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1989</year>). <article-title>Further validation of three short forms of the Marlowe-Crowne scale of social desirability</article-title>. <source>Psychological Reports</source>, <volume>65</volume>, <fpage>595</fpage>–<lpage>600</lpage>.</citation>
</ref>
<ref id="bibr8-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Harvey</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Oliver</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Towards effective practitioner evaluation: An exploration of issues relating to skills, motivation and evidence</article-title>. <source>Educational Technology &amp; Society</source>, <volume>5</volume>, <fpage>1</fpage>–<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr9-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hill</surname>
<given-names>L. G.</given-names>
</name>
<name>
<surname>Betz</surname>
<given-names>D. L.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Revisiting the retrospective pretest</article-title>. <source>American Journal of Evaluation</source>, <volume>26</volume>, <fpage>501</fpage>–<lpage>517</lpage>.</citation>
</ref>
<ref id="bibr10-1098214011423803">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Hill</surname>
<given-names>L. G.</given-names>
</name>
<name>
<surname>Parker</surname>
<given-names>L. A.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Extension as a delivery system for prevention programming: Capacity, barriers, and opportunities</article-title>. <source>Journal of Extension, 43</source>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.joe.org/joe/2005february/a1.shtml">http://www.joe.org/joe/2005february/a1.shtml</ext-link>
</citation>
</ref>
<ref id="bibr11-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kegeles</surname>
<given-names>S. M.</given-names>
</name>
<name>
<surname>Rebchook</surname>
<given-names>G. M.</given-names>
</name>
<name>
<surname>Tebbetts</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Challenges and facilitators to building program evaluation capacity among community-based organizations</article-title>. <source>AIDS Education and Prevention. Special Issue: Building HIV Prevention Capacity through Community Collaborative Research</source>, <volume>17</volume>, <fpage>284</fpage>–<lpage>299</lpage>. <comment>doi:10.1521/aeap.2005.17.4.284</comment>
</citation>
</ref>
<ref id="bibr12-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>King</surname>
<given-names>J. A.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Building the evaluation capacity of a school district</article-title>. <source>New Directions for Evaluation</source>, <volume>93</volume>, <fpage>63</fpage>–<lpage>80</lpage>.</citation>
</ref>
<ref id="bibr13-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kinsey</surname>
<given-names>D. C.</given-names>
</name>
</person-group> (<year>1981</year>). <article-title>Participatory evaluation in adult and nonformal education</article-title>. <source>Adult Education Quarterly</source>, <volume>31</volume>, <fpage>155</fpage>–<lpage>168</lpage>.</citation>
</ref>
<ref id="bibr14-1098214011423803">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Lang</surname>
<given-names>N.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Integrating the data processing of qualitative research and social work practice to advance the practitioner as knowledge builder: Tools for knowing and doing</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Sherman</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Reid</surname>
<given-names>W.</given-names>
</name>
</person-group> (Eds.), <source>Qualitative research in social work</source> (pp. <fpage>265</fpage>–<lpage>270</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Columbia University Press</publisher-name>.</citation>
</ref>
<ref id="bibr15-1098214011423803">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Lunt</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Fouché</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Yates</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2008</year>). <source>“Growing research in practice”: Report of an innovative partnership model</source>. <publisher-loc>Wellington, Australia</publisher-loc>: <publisher-name>Families Commission</publisher-name>.</citation>
</ref>
<ref id="bibr16-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mills</surname>
<given-names>D. S.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Examining the structural and philosophical positions of community-based organization practitioners</article-title>. <source>Dissertation Abstracts International</source>, <volume>62</volume>(<issue>9-B</issue>):<fpage>4620</fpage>.</citation>
</ref>
<ref id="bibr17-1098214011423803">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Morford</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Kozak</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Suvedi</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Innes</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Factors affecting program evaluation behaviours of natural resource extension practitioners: Motivation and capacity building</article-title>. <source>Journal of Extension,44</source>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.joe.org/joe/2006june/a7.php">http://www.joe.org/joe/2006june/a7.php</ext-link>
</citation>
</ref>
<ref id="bibr18-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Myers-Walls</surname>
<given-names>J. A.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>An odd couple with promise: Researchers and practitioners in evaluation settings</article-title>. <source>Family Relations</source>, <volume>49</volume>, <fpage>341</fpage>–<lpage>347</lpage>. <comment>doi:10.1111/j.1741-3729.2000.00341.x</comment>
</citation>
</ref>
<ref id="bibr19-1098214011423803">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Patton</surname>
<given-names>M. Q.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Utilization-focused evaluation</source>. (<edition>4th ed</edition>.).
<publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr20-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Preskill</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Torres</surname>
<given-names>R. T.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Building capacity for organizational learning through evaluative inquiry</article-title>. <source>Evaluation</source>, <volume>5</volume>, <fpage>42</fpage>–<lpage>60</lpage>.</citation>
</ref>
<ref id="bibr21-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Riemann</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Ethnographies of practice–Practicing ethnography</article-title>. <source>Journal of Social Work Practice</source>, <volume>19</volume>, <fpage>87</fpage>–<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr22-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schwandt</surname>
<given-names>T. A.</given-names>
</name>
<name>
<surname>Dahler-Larsen</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>When evaluation meets the “rough ground” in communities</article-title>. <source>Evaluation</source>, <volume>12</volume>, <fpage>474</fpage>–<lpage>495</lpage>.</citation>
</ref>
<ref id="bibr23-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shaw</surname>
<given-names>I.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Practitioner research: Evidence or critique?</article-title> <source>British Journal of Social Worsk</source>, <volume>35</volume>, <fpage>1231</fpage>–<lpage>1248</lpage>.</citation>
</ref>
<ref id="bibr24-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shaw</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Faulkner</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Practitioner evaluation at work</article-title>. <source>American Journal of Evaluation</source>, <volume>27</volume>, <fpage>44</fpage>–<lpage>63</lpage>. <comment>doi: 10.1177/1098214005284968</comment>
</citation>
</ref>
<ref id="bibr25-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Spoth</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Trudeau</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Guyll</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Shin</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Redmond</surname>
<given-names>C.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Universal intervention effects on substance use among young adults mediated by delayed adolescent substance initiation</article-title>. <source>Journal of Consulting and Clinical Psychology</source>, <volume>77</volume>, <fpage>620</fpage>–<lpage>632</lpage>.</citation>
</ref>
<ref id="bibr26-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Strahan</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Gerbasi</surname>
<given-names>K. C.</given-names>
</name>
</person-group> (<year>1972</year>). <article-title>Short, homogeneous versions of the Marlowe-Crowne social desirability scale</article-title>. <source>Journal of Clinical Psychology</source>, <volume>28</volume>, <fpage>191</fpage>–<lpage>193</lpage>. <comment>doi:10.1002/1097-4679(197204)28:2&lt;191: AID-JCLP2270280220&gt;3.0.CO;2-G</comment>
</citation>
</ref>
<ref id="bibr27-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Taut</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Studying self-evaluation capacity building in a large international development organization</article-title>. <source>American Journal of Evaluation</source>, <volume>28</volume>, <fpage>45</fpage>.</citation>
</ref>
<ref id="bibr28-1098214011423803">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Taut</surname>
<given-names>S. M.</given-names>
</name>
<name>
<surname>Alkin</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Program staff perceptions of barriers to evaluation implementation</article-title>. <source>American Journal of Evaluation</source>, <volume>24</volume>, <fpage>213</fpage>–<lpage>226</lpage>. <comment>doi:10.1016/s1098-2140(03)00028-6</comment>
</citation>
</ref>
</ref-list>
</back>
</article>