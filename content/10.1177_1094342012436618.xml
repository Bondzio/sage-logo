<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342012436618</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342012436618</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Network-theoretic classification of parallel computation patterns</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Whalen</surname>
<given-names>Sean</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012436618">1</xref>
<xref ref-type="aff" rid="aff2-1094342012436618">2</xref>
<xref ref-type="corresp" rid="corresp1-1094342012436618"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Engle</surname>
<given-names>Sophie</given-names>
</name>
<xref ref-type="aff" rid="aff3-1094342012436618">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Peisert</surname>
<given-names>Sean</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012436618">1</xref>
<xref ref-type="aff" rid="aff2-1094342012436618">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bishop</surname>
<given-names>Matt</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342012436618">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-1094342012436618">
<label>1</label>Lawrence Berkeley National Laboratory, USA</aff>
<aff id="aff2-1094342012436618">
<label>2</label>Department of Computer Science, University of California, USA</aff>
<aff id="aff3-1094342012436618">
<label>3</label>Department of Computer Science, University of San Francisco, USA</aff>
<author-notes>
<corresp id="corresp1-1094342012436618">Sean Whalen, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720 and Department of Computer Science, University of California, Davis, 1 Shields Avene, Davis, CA 95616, USA Email: <email>shwhalen@ucdavis.edu</email></corresp>
<fn fn-type="other" id="fn1-1094342012436618">
<p>
<italic>Sean Whalen</italic> is a postdoctoral researcher at Columbia University. He was previously an I3P postdoctoral fellow at Lawrence Berkeley National Laboratory after finishing his PhD at the University of California, Davis in 2010. His primary research interests include computer security, machine learning, and network theory.</p>
</fn>
<fn fn-type="other" id="fn2-1094342012436618">
<p>
<italic>Sophie Engle</italic> is a professor in the Department of Computer Science at the University of San Francisco. She received her PhD in computer science from the University of California, Davis in 2010. Her primary research focus is on computer security, including topics such as vulnerability analysis, insider threat, and electronic voting.</p>
</fn>
<fn fn-type="other" id="fn3-1094342012436618">
<p>
<italic>Sean Peisert</italic> received his PhD from the University of California, San Diego. He holds a joint appointment as an assistant adjunct professor at the University of California, Davis and as a research scientist at Lawrence Berkeley National Laboratory. He works in the area of computer security and is particularly interested in forensic analysis, electronic voting, the insider threat, and empirical studies of security.</p>
</fn>
<fn fn-type="other" id="fn4-1094342012436618">
<p>
<italic>Matt Bishop</italic> received his PhD in computer science from Purdue University, where he specialized in computer security, in 1984. He is a professor in the Department of Computer Science at the University of California, Davis. His main research area is the analysis of vulnerabilities in computer systems, the insider threat, network security, electronic voting and process models, and data sanitization (among other areas). His textbook, “Computer Security: Art and Science” was published in 2002 by Addison-Wesley Professional.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>26</volume>
<issue>2</issue>
<issue-title>Issues in Large Scale Computing Environments: Heterogeneous Computing and Operating Systems - two subjects, one special issue</issue-title>
<fpage>159</fpage>
<lpage>169</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Parallel computation in a high-performance computing environment can be characterized by the distributed memory access patterns of the underlying algorithm. During execution, networks of compute nodes exchange messages that indirectly exhibit these access patterns. Identifying the algorithm underlying these observable messages is the problem of latent class analysis over information flows in a computational network. Towards this end, our work applies methods from graph and network theory to classify parallel computations solely from network communication patterns. Pattern classification has applications to several areas including anomaly detection, performance analysis, and automated algorithm replacement. We discuss the difficulties encountered by previous efforts, introduce two new approximate matching techniques, and compare these approaches using massive datasets collected at Lawrence Berkeley National Laboratory.</p>
</abstract>
<kwd-group>
<kwd>communication patterns</kwd>
<kwd>computational dwarves</kwd>
<kwd>pattern recognition</kwd>
<kwd>graph theory</kwd>
<kwd>network theory</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342012436618">
<title>1 Introduction</title>
<p>The field of high-performance computing (HPC) is undergoing dramatic change as researchers plan for next-generation exascale systems, and distributed computing in general has seen rapid growth due to the cloud. Understanding the characteristics of distributed computation is essential for improving the efficiency and scalability of software in these environments (<xref ref-type="bibr" rid="bibr3-1094342012436618">Balaji et al., 2009</xref>) as well as growing concerns about security. Static analysis of binaries or source is one (costly) approach; another is to examine computations indirectly by observing their patterns of communication.</p>
<p>Our work uses the characteristics of runtime communication patterns to infer what code or algorithm is actually executing in a distributed environment. Towards this end, we build classifiers using a combination of methods from graph theory, network theory, and hypothesis testing. Classification in this setting is the problem of structural pattern recognition applied to unknown parallel computations that reveal themselves indirectly via messages exchanged by a network of compute nodes. Identifying the underlying algorithm is thus a type of latent class analysis where a “hidden” algorithm must be identified only from observable information flows. This task is non-trivial: compilers, architectures, libraries, datasets, parameters, and software flaws can each potentially influence communication patterns. Additionally, different algorithms can express similar patterns, and different implementations of the same algorithm can express different patterns.</p>
<p>In this paper we first describe how communication patterns are dynamically captured from running applications and the relation of these patterns to abstract computational classes called dwarves. Methods from graph and network theory are introduced and their shortcomings discussed. We then demonstrate multiple approaches to achieve efficient, approximate matching of communication patterns that avoid the computational costs of static analysis.</p>
</sec>
<sec id="section2-1094342012436618">
<title>2 Background</title>
<sec id="section3-1094342012436618">
<title>2.1 Communication logging</title>
<p>Message Passing Interface (MPI) is a communications protocol standard used by many parallel programs to exchange data using a distributed memory model. There are several implementations such as OpenMPI and MPICH, each based on the idea of logical processors with unique labels called ranks placed in groups called communicators. MPI programs have an initialization phase where each processor joins a communicator and is assigned a rank, and a finalization phase to gracefully terminate after computation.</p>
<p>The Integrated Performance Monitoring (IPM) library (<xref ref-type="bibr" rid="bibr7-1094342012436618">Borrill et al., 2005</xref>) provides low-overhead performance and resource profiling for parallel programs. It logs features of MPI calls such as the call name, the source and destination rank, the number of bytes sent, and aggregate performance counters such as the number of integer and floating point operations. The library is enabled either at compile time or run time and uses library interposition to intercept MPI calls at run time.</p>
<p>Consider the following abbreviated IPM log entry:</p>
<p>&lt;hent call= “MPI_Isend” bytes= “599136” orank=“1” count=“26” /&gt;</p>
<p>These entries become rows in a two dimensional feature matrix where rows are individual calls and columns are call features. Call names are mapped to unique integers so the contents of the feature matrix are purely numerical. The above entry then becomes:<disp-formula id="disp-formula1-1094342012436618">
<mml:math id="mml-disp1-1094342012436618">
<mml:mfenced close=")" open="("><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">_</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>599136</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mn>26</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced>
</mml:math>
<graphic alternate-form-of="disp-formula1-1094342012436618" xlink:href="10.1177_1094342012436618-eq1.tif"/>
</disp-formula>
The result is a matrix of features for each run of a parallel program. By varying datasets, parameters, the number of compute nodes, and other factors, we obtain multiple matrices for each program. These matrices are then converted to directed graphs by treating ranks as nodes and calls between ranks as edges. The task at hand, then, is to apply structural and statistical pattern analysis to differentiate patterns of parallel computation.</p>
</sec>
<sec id="section4-1094342012436618">
<title>2.2 Computational dwarves</title>
<p>A computational dwarf is “a pattern of communication and computation common across a set of applications” (<xref ref-type="bibr" rid="bibr2-1094342012436618">Asanovic et al., 2006</xref>). Each dwarf is an equivalence class of computation independent of the programming language or numerical methods used for a particular implementation. The common use of shared libraries such as BLAS and LAPACK provides some evidence of these equivalence classes, though dwarves imply a level of algorithmic equivalence beyond code reuse.</p>
<p>
<xref ref-type="bibr" rid="bibr9-1094342012436618">Colella et al. (2004)</xref> identified seven dwarves in HPC applications: dense linear algebra, sparse linear algebra, spectral methods, <inline-formula id="inline-formula3-1094342012436618">
<mml:math id="mml-inline3-1094342012436618">
<mml:mi>n</mml:mi>
</mml:math>
</inline-formula>-body methods, structured grids, unstructured grids, and monte carlo methods. <xref ref-type="bibr" rid="bibr2-1094342012436618">Asanovich et al. (2006)</xref> asked if these seven also captured patterns from areas outside of HPC. They found six additional dwarves were needed to capture the distinct patterns of computation outside HPC including combinational logic, graph traversal, dynamic programming, backtrack and branch/bound, graphical models, and finite state machines. Originally named in reference to the seven dwarves of Snow White, they are now commonly called computational motifs due to these additional patterns. However, we use the original term to prevent confusion with the network motifs presented in this paper.</p>
<p>Distributed memory parallel programs, then, will fall into one or more of these 13 dwarf classes. If the variance of the expressed patterns is bounded, identification of the dwarf class should be possible solely from observed communications.</p>
</sec>
<sec id="section5-1094342012436618">
<title>2.3 Visualization</title>
<p>Consider a three-node communicator where rank 0 sends messages to ranks 1 and 2, rank 1 sends a message to rank 2, and ranks 1 and 2 send messages back to 0. These messages have the following adjacency matrix representation:<disp-formula id="disp-formula2-1094342012436618">
<mml:math id="mml-disp2-1094342012436618">
<mml:mfenced close=")" open="("><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced>
</mml:math>
<graphic alternate-form-of="disp-formula2-1094342012436618" xlink:href="10.1177_1094342012436618-eq2.tif"/>
</disp-formula>
</p>
<p>Adjacency matrices are commonly visualized as a grid where the axes are rank numbers and filled pixels denote ranks that exchanged one or more messages. Different communication features such as the number of messages exchanged or their total size can be stored in the matrix and color-mapped to provide additional insight. Such visualizations are commonly used to examine communication patterns and have been offered as evidence for the existence of computational dwarves.</p>
<p>The adjacency matrices for single runs of three different parallel programs are shown in <xref ref-type="fig" rid="fig1-1094342012436618">Figure 1</xref>. Another layer of structure can be seen by extending the adjacency matrix into a third dimension, mapping an additional communication feature onto the <inline-formula id="inline-formula4-1094342012436618">
<mml:math id="mml-inline4-1094342012436618">
<mml:mi>z</mml:mi>
</mml:math>
</inline-formula>-axis. <xref ref-type="fig" rid="fig2-1094342012436618">Figure 2</xref> shows such mappings for MPI messages and their counts using general relativity simulator CACTUS.</p>
<fig id="fig1-1094342012436618" position="float">
<label>Figure. 1.</label>
<caption>
<p>Adjacency matrices for individual runs of performance benchmark MADBENCH (256 nodes), atmospheric dynamics simulator FVCAM (64 nodes), and linear equation solver SUPERLU (64 nodes). The number of bytes sent between ranks is linearly mapped from dark blue (lowest) to red (highest), with white indicating an absence of communication.</p>
</caption>
<graphic alternate-form-of="fig1-1094342012436618" xlink:href="10.1177_1094342012436618-fig1.tif"/>
</fig>
<fig id="fig2-1094342012436618" position="float">
<label>Figure. 2.</label>
<caption>
<p>Adjacency matrix of general relativity simulator CACTUS augmented by MPI call (top) and message size (bottom). For purposes of display, MPI call names are mapped to integers; identical MPI calls receive the same color. In the bottom figure, color is a linear mapping of the number of repeats from dark blue (lowest) to red (highest). Such plots show the structure seen in adjacency matrices extends to features other than the source and destination ranks of MPI communications.</p>
</caption>
<graphic alternate-form-of="fig2-1094342012436618" xlink:href="10.1177_1094342012436618-fig2.tif"/>
</fig>
<p>Communication patterns are strongly tied to distributed memory access within a parallel program. To see this, examine the diagonal of <xref ref-type="fig" rid="fig1-1094342012436618">Figure 1</xref>’s center panel and note the communication between a rank and its immediate neighbors. Such a pattern is generated by finite difference equations and is found across many HPC applications. Another type of equation will have a different visual signature unless its pattern of distributed memory access is similar.</p>
<p>The structure seen in <xref ref-type="fig" rid="fig1-1094342012436618">Figures 1</xref> and <xref ref-type="fig" rid="fig2-1094342012436618">2</xref> is typical of the applications we examined and suggests that classification is possible. By the same argument, however, distinguishing applications within the same dwarf class may be difficult due to their topological similarity. Complicating matters, the same program may alter its communications given different parameter values, datasets, or communicator sizes (see <xref ref-type="fig" rid="fig3-1094342012436618">Figure 3</xref>). As a result, we cannot simply compare adjacency matrices to classify the underlying computation.</p>
<fig id="fig3-1094342012436618" position="float">
<label>Figure. 3.</label>
<caption>
<p>Data-dependent topology demonstrated by molecular dynamics simulator NAMD under different molecular arrangements. The number of bytes sent between ranks is linearly mapped from dark blue (lowest) to red (highest), with white indicating an absence of communication.</p>
</caption>
<graphic alternate-form-of="fig3-1094342012436618" xlink:href="10.1177_1094342012436618-fig3.tif"/>
</fig>
</sec>
</sec>
<sec id="section6-1094342012436618">
<title>3 Classification</title>
<p>This section presents both structural (topological) and statistical approaches to recognizing patterns in communication graphs constructed from IPM logs. These patterns are used to predict the unknown computation underlying the observed communications; this is the process of classification. We first introduce several approaches that failed to accurately classify our datasets. Their shortcomings motivate our later approaches.</p>
<p>Communications are represented as directed graphs with ranks as nodes and MPI calls as edges. Call names are stored as edge labels (colors) and multiple edges exist between nodes if more than one type of MPI message is exchanged. Graphs with labeled nodes and/or edges are called attributed relational graphs (ARGs) (<xref ref-type="bibr" rid="bibr30-1094342012436618">Sanfeliu and Fu, 1983</xref>) and this data reduces the search space of some algorithms such as the subgraph isomorphism test introduced in Section 3.2. The full details of our datasets and evaluation are presented in Section 3.6.</p>
<sec id="section7-1094342012436618">
<title>3.1 Node distributions</title>
<p>The first statistical graph measure, the node degree distribution, counts the total number of nodes having a particular number of edges (degree). This analysis is restricted to the out-degree distribution measuring only outbound edges. For example, the adjacency matrix in Section 2.3 has two nodes of degree 2 and a single node of degree 1. The node degree distribution for two individual runs of the <sc>madbench</sc> performance benchmark are shown in <xref ref-type="fig" rid="fig4-1094342012436618">Figures 4</xref>(a) and 4(b).</p>
<fig id="fig4-1094342012436618" position="float">
<label>Figure. 4.</label>
<caption>
<p>Node degree and betweenness centrality distribution for two runs of performance benchmark MADBENCH. These runs exhibit similar degree distributions but different centrality distributions. Classification using these statistics is error prone due to their variance over different runs of the same program.</p>
</caption>
<graphic alternate-form-of="fig4-1094342012436618" xlink:href="10.1177_1094342012436618-fig4.tif"/>
</fig>
<p>Node degree distributions are a summary statistic over the adjacency matrix and the types of messages exchanged, reflecting the layered per-call adjacency matrices in the left panel of <xref ref-type="fig" rid="fig2-1094342012436618">Figure 2</xref>. Though offering additional insight, they summarize a single aspect of the graph that may fail to distinguish different patterns. In these cases, the notion of centrality can be helpful.</p>
<p>Centrality measures the importance of a node in the graph, and this importance can be defined in several ways. We examine the betweenness centrality (<inline-formula id="inline-formula5-1094342012436618">
<mml:math id="mml-inline5-1094342012436618">
<mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub>
</mml:math>
</inline-formula>), measuring the percent of shortest paths passing through a node <inline-formula id="inline-formula6-1094342012436618">
<mml:math id="mml-inline6-1094342012436618">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula> in an undirected graph (<xref ref-type="bibr" rid="bibr14-1094342012436618">Freeman, 1977</xref>):<disp-formula id="disp-formula3-1094342012436618">
<mml:math id="mml-disp3-1094342012436618">
<mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false" stretchy="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo stretchy="false">≠</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">≠</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">∈</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-1094342012436618" xlink:href="10.1177_1094342012436618-eq3.tif"/>
</disp-formula>
where <inline-formula id="inline-formula7-1094342012436618">
<mml:math id="mml-inline7-1094342012436618">
<mml:msub><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is the number of shortest paths between nodes <inline-formula id="inline-formula8-1094342012436618">
<mml:math id="mml-inline8-1094342012436618">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula9-1094342012436618">
<mml:math id="mml-inline9-1094342012436618">
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> that pass through <inline-formula id="inline-formula10-1094342012436618">
<mml:math id="mml-inline10-1094342012436618">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>. This number is normalized by the total number of shortest paths between <inline-formula id="inline-formula11-1094342012436618">
<mml:math id="mml-inline11-1094342012436618">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula12-1094342012436618">
<mml:math id="mml-inline12-1094342012436618">
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula>. The <inline-formula id="inline-formula13-1094342012436618">
<mml:math id="mml-inline13-1094342012436618">
<mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub>
</mml:math>
</inline-formula> of <inline-formula id="inline-formula14-1094342012436618">
<mml:math id="mml-inline14-1094342012436618">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>, then, is the sum of these normalized shortest path counts over all node pairs not containing <inline-formula id="inline-formula15-1094342012436618">
<mml:math id="mml-inline15-1094342012436618">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>.</p>
<p>Intuitively, nodes acting as coordinators of computation such as rank 0 have high <inline-formula id="inline-formula16-1094342012436618">
<mml:math id="mml-inline16-1094342012436618">
<mml:msub><mml:mi>C</mml:mi><mml:mi>B</mml:mi></mml:msub>
</mml:math>
</inline-formula>. As an artifact of IPM, broadcast messages also have high centrality. This can be seen in <xref ref-type="fig" rid="fig4-1094342012436618">Figures 4</xref>(c) and 4(d). Note that despite similar degree distributions, the second run has a very different centrality distribution.</p>
<p>In this example, relying solely on centrality to classify the computation results in a false negative (incorrectly predicting the patterns are from different algorithms). The degree distribution works for this example but will result in a false positive (incorrectly predicting patterns are from the same algorithm) for many others. Thus, multiple such measures are important for differentiating parallel computations.</p>
<p>However, these statistics are based solely on topological properties of the computational network and do not incorporate other attributes of information flow. They are also sensitive to the number of compute nodes. It is vital that any classification is independent of the communicator size, whether the computation is performed with 32, 64, 128, or more nodes.</p>
</sec>
<sec id="section8-1094342012436618">
<title>3.2 Graph isomorphisms</title>
<p>In our setting, two parallel computations using the same algorithm are often isomorphic: given the set of vertices <inline-formula id="inline-formula17-1094342012436618">
<mml:math id="mml-inline17-1094342012436618">
<mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and edges <inline-formula id="inline-formula18-1094342012436618">
<mml:math id="mml-inline18-1094342012436618">
<mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> for some graph <inline-formula id="inline-formula19-1094342012436618">
<mml:math id="mml-inline19-1094342012436618">
<mml:mi>G</mml:mi>
</mml:math>
</inline-formula>, graph isomorphism is a bijection between two graphs <inline-formula id="inline-formula20-1094342012436618">
<mml:math id="mml-inline20-1094342012436618">
<mml:mi>G</mml:mi>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula21-1094342012436618">
<mml:math id="mml-inline21-1094342012436618">
<mml:mi>H</mml:mi>
</mml:math>
</inline-formula>:<disp-formula id="disp-formula4-1094342012436618">
<mml:math id="mml-disp4-1094342012436618">
<mml:mi>f</mml:mi><mml:mo stretchy="false">:</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">→</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula4-1094342012436618" xlink:href="10.1177_1094342012436618-eq4.tif"/>
</disp-formula>
</p>
<p>This mapping preserves edge structure: <inline-formula id="inline-formula22-1094342012436618">
<mml:math id="mml-inline22-1094342012436618">
<mml:mi>u</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">∈</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> if and only if <inline-formula id="inline-formula23-1094342012436618">
<mml:math id="mml-inline23-1094342012436618">
<mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">∈</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> (<xref ref-type="bibr" rid="bibr36-1094342012436618">West, 2001</xref>). Isomorphic communication patterns imply that messages are passed between the same nodes in each graph regardless of the assigned rank number, akin to scrambling the columns of the adjacency matrix.</p>
<p>As two computations performed on different numbers of nodes cannot be isomorphic, we are instead interested in the related problem of subgraph isomorphism. Two graphs are subgraph isomorphic if some subgraph of <inline-formula id="inline-formula24-1094342012436618">
<mml:math id="mml-inline24-1094342012436618">
<mml:mi>G</mml:mi>
</mml:math>
</inline-formula> is isomorphic to <inline-formula id="inline-formula25-1094342012436618">
<mml:math id="mml-inline25-1094342012436618">
<mml:mi>H</mml:mi>
</mml:math>
</inline-formula>: for example, if some subset of the communication graph for atmospheric dynamics simulator FVCAM with 256 nodes is isomorphic to the same program run with 128 nodes.</p>
<p>Graph isomorphism has not been proven <inline-formula id="inline-formula26-1094342012436618">
<mml:math id="mml-inline26-1094342012436618">
<mml:mi>N</mml:mi><mml:mi>P</mml:mi>
</mml:math>
</inline-formula>-complete or a member of <inline-formula id="inline-formula27-1094342012436618">
<mml:math id="mml-inline27-1094342012436618">
<mml:mi>P</mml:mi>
</mml:math>
</inline-formula>, while subgraph isomorphism is <inline-formula id="inline-formula28-1094342012436618">
<mml:math id="mml-inline28-1094342012436618">
<mml:mi>N</mml:mi><mml:mi>P</mml:mi>
</mml:math>
</inline-formula>-complete via reduction to the maximum clique problem (<xref ref-type="bibr" rid="bibr11-1094342012436618">Cook, 1971</xref>). Runtime complexity is of serious concern as HPC networks often contain hundreds or thousands of nodes. Our investigations focus on Ullman’s subgraph isomorphism algorithm (<xref ref-type="bibr" rid="bibr33-1094342012436618">Ullmann, 1976</xref>), the VF2 algorithm (<xref ref-type="bibr" rid="bibr12-1094342012436618">Cordella et al., 2004</xref>), and the NetworkX implementation of VF2 (<xref ref-type="bibr" rid="bibr17-1094342012436618">Hagberg et al., 2008</xref>). The Ullman and VF2 algorithms are implemented in the VFLib library (<xref ref-type="bibr" rid="bibr13-1094342012436618">Foggia, 2001</xref>) and have worst-case time complexity <inline-formula id="inline-formula29-1094342012436618">
<mml:math id="mml-inline29-1094342012436618">
<mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">!</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula30-1094342012436618">
<mml:math id="mml-inline30-1094342012436618">
<mml:mrow><mml:mi mathvariant="script">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">!</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, respectively.</p>
<p>IPM logs are converted to directed graphs with MPI calls encoded as edge attributes. As discussed earlier, these attributed relational graphs prune the state space of the isomorphism test and reduce false positives (<xref ref-type="bibr" rid="bibr30-1094342012436618">Sanfeliu and Fu, 1983</xref>). However, the primary problem with isomorphism-based classification is its exact matching requirements that result in false negatives for all data-dependent topologies (see <xref ref-type="fig" rid="fig3-1094342012436618">Figure 3</xref>). Ideally, such topologies will be classified the same if differences are within some bounded variance. To address this, the next section introduces an approximate matching approach using statistical hypothesis testing.</p>
</sec>
<sec id="section9-1094342012436618">
<title>3.3 Hypothesis testing</title>
<p>A graph isomorphism test requires exact node correspondence between two graphs. This requirement results in many false negatives when applied to communication patterns whose statistics can vary with architecture, communicator size, parameters, and datasets. Current approximate graph matching methods such as graph edit distance (<xref ref-type="bibr" rid="bibr30-1094342012436618">Sanfeliu and Fu, 1983</xref>) are often more computationally expensive than exact graph matching. Instead, we present a matching algorithm based on statistical hypothesis testing with low computational complexity. First we review the relevant concepts and notation then discuss our approach in these terms. An exhaustive introduction to hypothesis testing is outside the scope of this paper and so our discussion makes some simplifications.</p>
<p>A hypothesis test is a statistical method to determine whether a null hypothesis <inline-formula id="inline-formula31-1094342012436618">
<mml:math id="mml-inline31-1094342012436618">
<mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub>
</mml:math>
</inline-formula> or alternative hypothesis <inline-formula id="inline-formula32-1094342012436618">
<mml:math id="mml-inline32-1094342012436618">
<mml:msub><mml:mi>H</mml:mi><mml:mi>a</mml:mi></mml:msub>
</mml:math>
</inline-formula> best explain some data (<xref ref-type="bibr" rid="bibr16-1094342012436618">Gibbons and Chakraborti, 2010</xref>). The null hypothesis commonly theorizes the data is a result of chance and is accepted or rejected at a significance level <inline-formula id="inline-formula33-1094342012436618">
<mml:math id="mml-inline33-1094342012436618">
<mml:mi mathvariant="italic">α</mml:mi>
</mml:math>
</inline-formula> using some statistical test. If rejected, <inline-formula id="inline-formula34-1094342012436618">
<mml:math id="mml-inline34-1094342012436618">
<mml:msub><mml:mi>H</mml:mi><mml:mi>a</mml:mi></mml:msub>
</mml:math>
</inline-formula> is accepted as true with <inline-formula id="inline-formula35-1094342012436618">
<mml:math id="mml-inline35-1094342012436618">
<mml:mi mathvariant="italic">α</mml:mi>
</mml:math>
</inline-formula> probability of a type-I error (false positive).</p>
<p>A type of hypothesis test called a goodness-of-fit test can be used to determine the equality of probability distributions. We use the two-sample Kolmogorov–Smirnov (KS) test (<xref ref-type="bibr" rid="bibr20-1094342012436618">Massey, 1951</xref>), first computing the D-statistic for two empirical cumulative distribution functions <inline-formula id="inline-formula36-1094342012436618">
<mml:math id="mml-inline36-1094342012436618">
<mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula37-1094342012436618">
<mml:math id="mml-inline37-1094342012436618">
<mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>:<disp-formula id="disp-formula5-1094342012436618">
<mml:math id="mml-disp5-1094342012436618">
<mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:munder><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:mrow><mml:mi>x</mml:mi></mml:munder><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>|</mml:mo></mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-1094342012436618" xlink:href="10.1177_1094342012436618-eq5.tif"/>
</disp-formula>
where <inline-formula id="inline-formula38-1094342012436618">
<mml:math id="mml-inline38-1094342012436618">
<mml:mi>m</mml:mi>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula39-1094342012436618">
<mml:math id="mml-inline39-1094342012436618">
<mml:mi>n</mml:mi>
</mml:math>
</inline-formula> are the total event counts of their respective distributions. We then compute the probability that differences in the distributions are due to chance (the <inline-formula id="inline-formula40-1094342012436618">
<mml:math id="mml-inline40-1094342012436618">
<mml:mi>p</mml:mi>
</mml:math>
</inline-formula>
<italic>-value</italic>) and reject <inline-formula id="inline-formula41-1094342012436618">
<mml:math id="mml-inline41-1094342012436618">
<mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub>
</mml:math>
</inline-formula> if this value is less than our threshold <inline-formula id="inline-formula42-1094342012436618">
<mml:math id="mml-inline42-1094342012436618">
<mml:mi mathvariant="italic">α</mml:mi>
</mml:math>
</inline-formula>: <disp-formula id="disp-formula6-1094342012436618">
<mml:math id="mml-disp6-1094342012436618">
<mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">≥</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mi>O</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">&lt;</mml:mo><mml:mi mathvariant="italic">α</mml:mi>
</mml:math>
<graphic alternate-form-of="disp-formula6-1094342012436618" xlink:href="10.1177_1094342012436618-eq6.tif"/>
</disp-formula>
for the observed statistic <inline-formula id="inline-formula45-1094342012436618">
<mml:math id="mml-inline45-1094342012436618">
<mml:msub><mml:mi>D</mml:mi><mml:mi>O</mml:mi></mml:msub>
</mml:math>
</inline-formula> (<xref ref-type="bibr" rid="bibr16-1094342012436618">Gibbons and Chakraborti, 2010</xref>). Though defined theoretically for continuous distributions, a modified KS test can be used with discrete distributions (<xref ref-type="bibr" rid="bibr10-1094342012436618">Conover, 1972</xref>) or an unmodified test can simply provide conservative <inline-formula id="inline-formula46-1094342012436618">
<mml:math id="mml-inline46-1094342012436618">
<mml:mi>p</mml:mi>
</mml:math>
</inline-formula>-values.</p>
<p>We present two applications of the KS test for pattern classification in the following sections.</p>
</sec>
<sec id="section10-1094342012436618">
<title>3.4 Network motifs</title>
<p>One approach to characterizing communication topologies is to describe global communication patterns in terms of their localized subgraphs. Those subgraphs that occur more often than would be expected in randomized networks are called motifs. Network theorists have studied motifs in a wide range of fields including biology (<xref ref-type="bibr" rid="bibr1-1094342012436618">Alon, 2007</xref>), ecology (<xref ref-type="bibr" rid="bibr22-1094342012436618">Milo et al., 2002</xref>), chemistry (<xref ref-type="bibr" rid="bibr18-1094342012436618">Kuramochi and Karypis, 2001</xref>), and neuroscience (<xref ref-type="bibr" rid="bibr26-1094342012436618">Qian et al., 2011</xref>). Motifs in such real-world networks are often described as building blocks with intuitive functional interpretations, and these networks are commonly characterized by their distribution of <inline-formula id="inline-formula47-1094342012436618">
<mml:math id="mml-inline47-1094342012436618">
<mml:mi>n</mml:mi>
</mml:math>
</inline-formula>-node motifs. Such intuitive interpretations may also exist for HPC algorithms, though we leave this to future work and instead focus on their use as classifiers.</p>
<p>Motif discovery can be divided into three subtasks: 1) counting subgraphs, 2) grouping equivalent subgraphs, and 3) determining which subgraphs are over-represented relative to some random graph model. Each of these steps is computationally expensive and so the process is often restricted to three-, four-, or five-node subgraphs.</p>
<p>We use the FANMOD tool (<xref ref-type="bibr" rid="bibr34-1094342012436618">Wernicke, 2006</xref>; <xref ref-type="bibr" rid="bibr35-1094342012436618">Wernicke and Rasche, 2006</xref>), which provides an order of magnitude speedup over its predecessors for steps 1 and 3. Input graphs may contain node and/or edge data referred to as colors. A maximum of seven edge colors may be used and so the MPI calls stored on the edges of our ARGs are grouped by different criteria. Single color graphs assign all MPI calls to the same group, while two-color graphs distinguish between broadcast and point-to-point calls. Lastly, three-color graphs divide point-to-point calls into send and receive groups. The tool is run multiple times to find 1–3 color motifs of size 3 and 1–2 color motifs of size 4. Intuitively, larger motifs and more colors should help distinguish different patterns of communication.</p>
<p>Over-representation of a subgraph is determined by its <inline-formula id="inline-formula48-1094342012436618">
<mml:math id="mml-inline48-1094342012436618">
<mml:mi>z</mml:mi>
</mml:math>
</inline-formula>-score:<disp-formula id="disp-formula7-1094342012436618">
<mml:math id="mml-disp7-1094342012436618">
<mml:mi>z</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>O</mml:mi></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mfrac></mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula7-1094342012436618" xlink:href="10.1177_1094342012436618-eq7.tif"/>
</disp-formula>
where <inline-formula id="inline-formula49-1094342012436618">
<mml:math id="mml-inline49-1094342012436618">
<mml:msub><mml:mi>N</mml:mi><mml:mi>O</mml:mi></mml:msub>
</mml:math>
</inline-formula> is its count in the original network, <inline-formula id="inline-formula50-1094342012436618">
<mml:math id="mml-inline50-1094342012436618">
<mml:msub><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:msub>
</mml:math>
</inline-formula> is its mean count in randomized networks, and <inline-formula id="inline-formula51-1094342012436618">
<mml:math id="mml-inline51-1094342012436618">
<mml:mi mathvariant="italic">σ</mml:mi>
</mml:math>
</inline-formula> is the standard deviation from <inline-formula id="inline-formula52-1094342012436618">
<mml:math id="mml-inline52-1094342012436618">
<mml:msub><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:msub>
</mml:math>
</inline-formula>. For brevity, a random selection of the most common three- and four-node motifs are shown in <xref ref-type="fig" rid="fig5-1094342012436618">Figures 5</xref> and <xref ref-type="fig" rid="fig6-1094342012436618">6</xref>. These figures summarize topologically equivalent motifs that have the same number of colors but potentially different colorings. For example, the motif in the lower right of <xref ref-type="fig" rid="fig5-1094342012436618">Figure 5</xref> could represent two isomorphic motifs where red maps to a “send” call in the first and a “receive” call in the second.</p>
<fig id="fig5-1094342012436618" position="float">
<label>Figure. 5.</label>
<caption>
<p>A random sample of common three-node motifs with different edge colorings. Black represents all calls in single-color motifs, while collective (black) and point-to-point (red) calls are represented separately in two-color motifs. In three-color motifs, point-to-point calls are further divided into send (red) and receive (blue) calls.</p>
</caption>
<graphic alternate-form-of="fig5-1094342012436618" xlink:href="10.1177_1094342012436618-fig5.tif"/>
</fig>
<fig id="fig6-1094342012436618" position="float">
<label>Figure. 6.</label>
<caption>
<p>A random sample of common four-mode motifs with different edge colorings. Black represents all calls in single-color motifs, while collective (black) and point-to-point (red) calls represented separately in two-color motifs.</p>
</caption>
<graphic alternate-form-of="fig6-1094342012436618" xlink:href="10.1177_1094342012436618-fig6.tif"/>
</fig>
<p>We apply a KS test to compare the distribution over all motifs discovered in two communication graphs. If the <inline-formula id="inline-formula53-1094342012436618">
<mml:math id="mml-inline53-1094342012436618">
<mml:mi>p</mml:mi>
</mml:math>
</inline-formula>-value returned by the test is less than the significance level, the graphs and the parallel programs that generated them are considered distinct. Tests are performed for each pair of graphs to evaluate the accuracy of this approach and the results are presented in Section 3.6.</p>
</sec>
<sec id="section11-1094342012436618">
<title>3.5 Call distributions</title>
<p>Hypothesis testing is also used to test the fit of MPI call distributions between each corresponding rank of two parallel computations. The counts of MPI messages sent by each rank are normalized to form a probability distribution and the cumulative sum is taken to produce <inline-formula id="inline-formula54-1094342012436618">
<mml:math id="mml-inline54-1094342012436618">
<mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover>
</mml:math>
</inline-formula>. For example: summed over all destination ranks, source rank 1 of program A may transmit 15 MPI_Send, 20 MPI_Recv, and 5 MPI_Barrier messages. The probability mass function is then 37.5%, 50%, and 12.5% respectively for these calls and 0% for all others. If source rank 1 of program B transmits 18 MPI_Send, 19 MPI_Recv, and 4 MPI Barrier messages, the probability mass function is 43.9%, 46.3%, and 9.7%.</p>
<p>Taking their respective cumulative sums to obtain <inline-formula id="inline-formula55-1094342012436618">
<mml:math id="mml-inline55-1094342012436618">
<mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mn>40</mml:mn></mml:mrow></mml:msub>
</mml:math>
</inline-formula> (<inline-formula id="inline-formula56-1094342012436618">
<mml:math id="mml-inline56-1094342012436618">
<mml:mi>m</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>15</mml:mn><mml:mo stretchy="false">+</mml:mo><mml:mn>20</mml:mn><mml:mo stretchy="false">+</mml:mo><mml:mn>5</mml:mn>
</mml:math>
</inline-formula>) and <inline-formula id="inline-formula57-1094342012436618">
<mml:math id="mml-inline57-1094342012436618">
<mml:msub><mml:mover accent="true"><mml:mi>S</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mn>41</mml:mn></mml:mrow></mml:msub>
</mml:math>
</inline-formula> (<inline-formula id="inline-formula58-1094342012436618">
<mml:math id="mml-inline58-1094342012436618">
<mml:mi>n</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>18</mml:mn><mml:mo stretchy="false">+</mml:mo><mml:mn>19</mml:mn><mml:mo stretchy="false">+</mml:mo><mml:mn>4</mml:mn>
</mml:math>
</inline-formula>), the KS test determines that these distributions are not significantly different at the <inline-formula id="inline-formula59-1094342012436618">
<mml:math id="mml-inline59-1094342012436618">
<mml:mi mathvariant="italic">α</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0.01</mml:mn>
</mml:math>
</inline-formula> level. If the probability mass functions remained the same but instead 2000 calls were logged, the test would determine the distributions are not the same at the <inline-formula id="inline-formula60-1094342012436618">
<mml:math id="mml-inline60-1094342012436618">
<mml:mi mathvariant="italic">α</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0.01</mml:mn>
</mml:math>
</inline-formula> level since there is far more data and the differences are less likely due to chance.</p>
<p>To determine if two communication patterns are generated by the same program, a KS test is applied to corresponding ranks in the communicators. For example: rank 1 of program A is compared to rank 1 of program B, rank 2 compared to rank 2, and so on. If the communicators are of the same size then all ranks are compared; if they are different, comparisons are performed between ranks present only in the smallest communicator. If more than some threshold of ranks are equivalent at significance level <inline-formula id="inline-formula61-1094342012436618">
<mml:math id="mml-inline61-1094342012436618">
<mml:mi mathvariant="italic">α</mml:mi>
</mml:math>
</inline-formula>, the programs are deemed equivalent. Both parameters provide an adjustable tolerance to topological differences. We found half the size of the smallest communicator to be an effective threshold.</p>
</sec>
<sec id="section12-1094342012436618">
<title>3.6 Evaluation</title>
<p>A total of 328 logs (34 gigabytes) were collected for Lawrence Berkeley National Laboratory by the National Energy Research Scientific Computing Center (see <xref ref-type="fig" rid="fig7-1094342012436618">Figure 7</xref>). Multiple logs exist for each program with varying ranks, parameters, architectures, and datasets when possible. Several simpler codes were logged by us; codes requiring significant domain knowledge or private datasets were logged from willing specialists on production systems. As a result, the inputs and parameters for some codes were not under our control. However, this dataset is several orders of magnitude larger than related efforts and we believe contains a representative sample of the dwarves found in scientific computing.</p>
<fig id="fig7-1094342012436618" position="float">
<label>Figure.7.</label>
<caption>
<p>Summary of MPI codes used to generate IPM logs. Some codes may fall under multiple dwarf classes. Monte Carlo dwarves are not represented as they are embarrassingly parallel and thus require minimal communication.</p>
</caption>
<graphic alternate-form-of="fig7-1094342012436618" xlink:href="10.1177_1094342012436618-fig7.tif"/>
</fig>
<p>Classifiers are evaluated by their true positive and false positive rates. A true positive (TP) denotes matching patterns generated by different runs of the same program; a false negative (FN) occurs when these patterns do not match. Similarly, a true negative (TN) occurs when patterns generated by different programs do not match; a false positive (FP) occurs when they do. The true positive rate (TPR) is defined as:<disp-formula id="disp-formula8-1094342012436618">
<mml:math id="mml-disp8-1094342012436618">
<mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula8-1094342012436618" xlink:href="10.1177_1094342012436618-eq8.tif"/>
</disp-formula>
</p>
<p>Similarly, true negatives (<inline-formula id="inline-formula62-1094342012436618">
<mml:math id="mml-inline62-1094342012436618">
<mml:mi>t</mml:mi><mml:mi>n</mml:mi>
</mml:math>
</inline-formula>) and false positives (<inline-formula id="inline-formula63-1094342012436618">
<mml:math id="mml-inline63-1094342012436618">
<mml:mi>f</mml:mi><mml:mi>p</mml:mi>
</mml:math>
</inline-formula>) define the false positive rate (FPR):<disp-formula id="disp-formula9-1094342012436618">
<mml:math id="mml-disp9-1094342012436618">
<mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula9-1094342012436618" xlink:href="10.1177_1094342012436618-eq9.tif"/>
</disp-formula>
</p>
<p>The TPR and FPR for classifiers using subgraph isomorphism testing, motif distributions, and call distributions are shown in <xref ref-type="fig" rid="fig8-1094342012436618">Figure 8</xref>.</p>
<fig id="fig8-1094342012436618" position="float">
<label>Figure. 8.</label>
<caption>
<p>Comparison of true positive rate (TPR) and false positive rate (FPR) for multiple communication pattern classifiers.</p>
</caption>
<graphic alternate-form-of="fig8-1094342012436618" xlink:href="10.1177_1094342012436618-fig8.tif"/>
</fig>
<p>Several results are of note. First, as mentioned, isomorphism testing is ineffective due to variance across multiple runs of the same program. However, hypothesis testing is effective both for four-node motifs as well as call distributions. Given the increase in true positives moving from three to four nodes, a similar increase may be possible with five-node motifs. Unfortunately, mining motifs of this size was computationally prohibitive.</p>
<p>Grouping related calls using finer-grained edge coloring appears to be beneficial. Additional colors could distinguish synchronous and asynchronous calls, or instead assign unique colors to each call. The latter case risks over-fitting the data: algorithms using slightly different calls for the same computation would no longer have the same motif, and <xref ref-type="fig" rid="fig8-1094342012436618">Figure 8</xref> demonstrates that additional colors do not always improve accuracy. The number of colors required for such an evaluation is not currently supported by FANMOD.</p>
<p>Finally, while motifs are nearly as effective as call distributions, the time required for motif discovery is prohibitive for many applications, taking days or even weeks as opposed to minutes. As a result, some graphs (in particular those with 512 or more ranks) were excluded from the motif analysis.</p>
<p>Given its superior true and false positive rates and orders of magnitude less computation time (see <xref ref-type="fig" rid="fig8-1094342012436618">Figure 8</xref>), hypothesis testing of MPI call distributions is our preferred method of classifying the communication patterns of distributed memory parallel programs. It is worth emphasizing the accuracy of the call distribution approach despite its time-independence; the overhead required to collect time-ordered data was prohibitive for our production systems.</p>
<p>It should be noted that these results occur in a more difficult multi-class classification setting: the chance of randomly guessing the correct class is only 7% here as opposed to 50% in a binary setting. Classification difficulty is also increased by the topological variance induced by changes in parameters, datasets, communicator size, and other factors. In particular, many programs exhibit all-to-all communication at some phase of execution, and this rules out the use of simpler classification methods based solely on topology with no notion of call names or other attributes.</p>
</sec>
</sec>
<sec id="section13-1094342012436618">
<title>4 Related work</title>
<p>IPM logs have previously been used to study the performance of MPI applications. <xref ref-type="bibr" rid="bibr15-1094342012436618">Fürlinger et al. (2010</xref>) provide a general introduction to the IPM package and discuss several concepts related to this work including visualization of adjacency matrices and examining the distribution of aggregate MPI calls. <xref ref-type="bibr" rid="bibr31-1094342012436618">Shalf et al. (2005</xref>) perform similar analysis to evaluate the communication requirements of parallel programs for improving processor interconnect designs. The adjacency matrices of several NAS parallel benchmarks, augmented by number of messages and message size, are presented by <xref ref-type="bibr" rid="bibr28-1094342012436618">Riesen (2006</xref>).</p>
<p>
<xref ref-type="bibr" rid="bibr19-1094342012436618">Ma et al. (2009</xref>) introduce a communication correlation coefficient to characterize the similarity of parallel programs using several metrics. The first compares the average transmission rate, message size, and unique neighbor count for each rank, while the second computes the maximum common subgraph. Their evaluation was limited to four programs in the NAS parallel benchmark.</p>
<p>In addition to graph and network theory, we have previously used machine learning to classify parallel computation (<xref ref-type="bibr" rid="bibr24-1094342012436618">Peisert, 2010</xref>; <xref ref-type="bibr" rid="bibr37-1094342012436618">Whalen, 2010</xref>) and discuss related machine learning efforts elsewhere. This approach achieves greater accuracy than those found in Section 3.3 but requires substantially more computation and care to prevent overfit models.</p>
<p>Other parallel programming standards such as OpenMP are based on a shared, as opposed to distributed, memory model. In an effort to increase the portability of parallel software, recent work uses compiler techniques to translate OpenMP into MPI source code (<xref ref-type="bibr" rid="bibr5-1094342012436618">Basumallik and Eigenmann, 2005</xref>; <xref ref-type="bibr" rid="bibr6-1094342012436618">Basumallik et al., 2007</xref>), and our approach should apply when such techniques are used. While we focus on latent analysis using only runtime communications, source code translation has also been used to replace inefficient computations (<xref ref-type="bibr" rid="bibr21-1094342012436618">Metzger and Wen, 2000</xref>; <xref ref-type="bibr" rid="bibr25-1094342012436618">Preissl et al., 2010</xref>). The classification of communication patterns thus has strong ties to compilers, static analysis, and code optimization.</p>
</sec>
<sec id="section14-1094342012436618">
<title>5 Conclusion</title>
<p>This work applies methods from graph and network theory to identify the latent class of a parallel computation from the observable information passed between nodes in a computational network: given logs of MPI messages from an unknown program, the task is to infer the program most likely to have generated those logs. Our original motivation was the detection of anomalous behavior on HPC systems, though we present our work in a general context and suggest additional applications including performance analysis and automated algorithm replacement.</p>
<p>As initially postulated by work on computational dwarves (<xref ref-type="bibr" rid="bibr2-1094342012436618">Asanovic et al., 2006</xref>; <xref ref-type="bibr" rid="bibr9-1094342012436618">Colella, 2004</xref>), communication patterns tend to be highly structured and reflect the distributed memory access patterns of the underlying algorithm. When dealing with algorithm implementations, however, many other factors affect the communication patterns of theoretical algorithms. Different implementations of the same algorithm, shared libraries, compiler optimizations, architecture differences, software flaws, debug flags, and numerous MPI implementations all make this task more difficult. Further, some parallel programs have data-dependent communication topologies, varying both slightly (see <xref ref-type="fig" rid="fig3-1094342012436618">Figure 3</xref>) and greatly as with multi-use (“swiss-army”) libraries or interpreters such as Matlab.</p>
<p>Using gigabytes of data covering over a dozen parallel programs, we constructed directed communication graphs and found network-theoretic measures such as node degree and centrality distributions capture insufficient information on their own to classify parallel computations. We also examined subgraph isomorphism testing for comparing topologies with different numbers of compute nodes and found many topologies exhibit the worst-case factorial runtime of the algorithm. More importantly, isomorphism requires exact matching and thus lacks the error tolerance necessary to correctly classify data-dependent computation.</p>
<p>To perform approximate matching we evaluate two approaches using goodness-of-fit tests; the first using the distribution of over-represented subgraphs called motifs and the second using the distribution of MPI calls relative to each rank. Both approaches succeed in classifying data-dependent topologies and comparisons are extremely fast excluding the motif-discovery phase. A significance level allows tuning false positive and false negative rates. The best method, comparing per-rank distributions of MPI calls, achieved a 92% true positive rate in less than 2 minutes. However, swiss-army programs with extreme pattern variance elude this approach.</p>
<p>Other statistical measures such as Claussen’s offdiagonal complexity (<xref ref-type="bibr" rid="bibr8-1094342012436618">Claussen, 2007</xref>) may also be useful for approximate topology comparison. Graph edit distance (<xref ref-type="bibr" rid="bibr30-1094342012436618">Sanfeliu and Fu, 1983</xref>), Bayesian (<xref ref-type="bibr" rid="bibr23-1094342012436618">Myers et al., 2000</xref>) and spectral (<xref ref-type="bibr" rid="bibr29-1094342012436618">Robles-Kelly and Hancock, 2005</xref>) approaches to edit distance, graph kernels (Shervashidze et al., 2010), and factor graphs (<xref ref-type="bibr" rid="bibr27-1094342012436618">Reichardt et al., 2010</xref>) offer additional approaches to approximate graph matching.</p>
<p>These directions may provide increased generality for topologies not yet observed or performance bounds in adversarial environments (<xref ref-type="bibr" rid="bibr4-1094342012436618">Barreno et al., 2008</xref>). However, the results presented in this paper show that error-tolerant methods for matching parallel communication patterns are practical for inferring latent classes of computation.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>Thanks to Scott Campbell and David Skinner for capturing IPM data at NERSC and to members of the Cyber Security project at LBNL for helpful discussions.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure" id="fn5-1094342012436618">
<p>This work was supported in part by the Director, Office of Computational and Technology Research, Division of Mathematical, Information, and Computational Sciences of the U.S. Department of Energy (contract number DE-AC02-05CH11231), and also by the U.S. Department of Homeland Security (grant number 2006-CS-001-000001) under the auspices of the Institute for Information Infrastructure Protection (I3P) research program. The I3P is managed by Dartmouth College. The views and conclusions contained in this document are those of the authors and not necessarily those of its sponsors.</p>
</fn>
<fn fn-type="conflict" id="fn6-1094342012436618">
<p>None declared.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Alon</surname>
<given-names>U</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Network motifs: theory and experimental approaches</article-title>. <source>Nature Reviews Genetics</source> <volume>8</volume>(<issue>6</issue>): <fpage>450</fpage>–<lpage>461</lpage>.</citation>
</ref>
<ref id="bibr2-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Asanovic</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Bodik</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Catanzaro</surname>
<given-names>B</given-names>
</name>
<etal/>
</person-group> (<year>2006</year>) <article-title>The landscape of parallel computing research: a view from Berkeley</article-title>. <comment>Report no. UCB/EECS-2006-183, University of California, Berkeley, USA</comment>.</citation>
</ref>
<ref id="bibr3-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Balaji</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Buntinas</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Goodell</surname>
<given-names>D</given-names>
</name>
<etal/>
</person-group> (<year>2009</year>) <article-title>MPI on a million processors</article-title>. In: <source>16th European PVM/MPI Users’ Group meeting on recent advances in parallel virtual machine and message passing interface, Espoo</source>, <comment>Finland, 7–10 September 2009</comment>, pp. <fpage>20</fpage>–<lpage>30</lpage>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Spinger-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr4-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Barreno</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Bartlett</surname>
<given-names>PL</given-names>
</name>
<name>
<surname>Chi</surname>
<given-names>FJ</given-names>
</name>
<etal/>
</person-group> (<year>2008</year>) <article-title>Open problems in the security of learning</article-title>. In: <source>1st ACM workshop on AISec, Alexandria</source>, <comment>USA, 27 October 2008</comment>, pp. <fpage>19</fpage>–<lpage>26</lpage>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation>
</ref>
<ref id="bibr5-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Basumallik</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Eigenmann</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>Towards automatic translation of OpenMP to MPI</article-title>. In: <source>19th International Conference on Supercomputing, Cambridge</source>, <comment>USA, 20–22 June 2005</comment>, pp. <fpage>189</fpage>–<lpage>198</lpage>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation>
</ref>
<ref id="bibr6-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Basumallik</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Min</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Eigenmann</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Programming distributed memory systems using OpenMP</article-title>. In: <source>2007 IEEE International Parallel and Distributed Processing Symposium, Long Beach</source>, <comment>USA, 26–30 March 2007</comment>, pp. <fpage>207</fpage>–<lpage>214</lpage>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr7-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Borrill</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Carter</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Oliker</surname>
<given-names>L</given-names>
</name>
<etal/>
</person-group> (<year>2005</year>) <article-title>Integrated performance monitoring of a cosmology application on leading HEC platforms</article-title>. In: <source>2005 International Conference on Parallel Processing, Oslow, Norway</source>, <comment>14–17 June 2005</comment>, pp. <fpage>119</fpage>–<lpage>128</lpage>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr8-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Claussen</surname>
<given-names>JC</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Offdiagonal complexity: a computationally quick complexity measure for graphs and networks</article-title>. <source>Physica A</source> <volume>375</volume>(<issue>1</issue>): <fpage>365</fpage>–<lpage>373</lpage>.</citation>
</ref>
<ref id="bibr9-1094342012436618">
<citation citation-type="other">
<person-group person-group-type="author">
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2004</year>) <comment>Defining software requirements for scientific computing. Report, DARPA<sup>TM</sup> high productivity computing systems program</comment>.</citation>
</ref>
<ref id="bibr10-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Conover</surname>
<given-names>WJ</given-names>
</name>
</person-group> (<year>1972</year>) <article-title>A Kolmogorov goodness-of-fit test for discontinuous distributions</article-title>. <source>Journal of the American Statistical Association</source> <volume>67</volume>(<issue>339</issue>): <fpage>591</fpage>–<lpage>596</lpage>.</citation>
</ref>
<ref id="bibr11-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cook</surname>
<given-names>SA</given-names>
</name>
</person-group> (<year>1971</year>) <article-title>The complexity of theorem-proving procedures</article-title>. In: <source>3rd Annual ACM Symposium on the Theory of Computing, Shaker Heights</source>, <comment>USA, 3–5 May 1971</comment>, pp. <fpage>151</fpage>–<lpage>158</lpage>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation>
</ref>
<ref id="bibr12-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cordella</surname>
<given-names>LP</given-names>
</name>
<name>
<surname>Foggia</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Sansone</surname>
<given-names>C</given-names>
</name>
<etal/>
</person-group> (<year>2004</year>) <article-title>A (sub)graph isomorphism algorithm for matching large graphs</article-title>. <source>IEEE Transactions on pattern analysis and machine intelligence</source> <volume>26</volume>(<issue>10</issue>): <fpage>1367</fpage>–<lpage>1372</lpage>.</citation>
</ref>
<ref id="bibr13-1094342012436618">
<citation citation-type="other">
<person-group person-group-type="author">
<name>
<surname>Foggia</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2001</year>) <comment>The VFLib graph matching library, version 2.0</comment>.</citation>
</ref>
<ref id="bibr14-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Freeman</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>1977</year>) <article-title>A set of measures of centrality based on betweenness</article-title>. <source>Sociometry</source> <volume>40</volume>(<issue>1</issue>): <fpage>35</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr15-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fürlinger</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Wright</surname>
<given-names>NJ</given-names>
</name>
<name>
<surname>Skinner</surname>
<given-names>D</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Effective performance measurement at petascale using IPM</article-title>. In: <source>16th IEEE International Conference on parallel and distributed systems, Shanghai, China</source>, <comment>8–10 December 2010</comment>, pp. <fpage>373</fpage>–<lpage>380</lpage>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr16-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gibbons</surname>
<given-names>JD</given-names>
</name>
<name>
<surname>Chakraborti</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2010</year>) <source>Nonparametric statistical inference</source>. <publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>CRC Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Hagberg</surname>
<given-names>AA</given-names>
</name>
<name>
<surname>Schult</surname>
<given-names>DA</given-names>
</name>
<name>
<surname>Swart</surname>
<given-names>PJ</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Exploring network structure, dynamics, and function using NetworkX</article-title>. In: <source>7th python in science conference</source> (eds G Varoquaux, J Millman and T Vaught), <comment>Pasadena, USA, 19–24 August 2008</comment>, pp. <fpage>11</fpage>–<lpage>16</lpage>. </citation>
</ref>
<ref id="bibr18-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kuramochi</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Karypis</surname>
<given-names>G</given-names>
</name>
</person-group> (<year>2001</year>) <article-title>Frequent subgraph discovery</article-title>. In: <source>2001 IEEE International Conference on data mining, San Jose, USA</source>, <comment>29 November–2 December 2001</comment>, pp. <fpage>313</fpage>–<lpage>320</lpage>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr19-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ma</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Teo</surname>
<given-names>YM</given-names>
</name>
<name>
<surname>March</surname>
<given-names>V</given-names>
</name>
<etal/>
</person-group> (<year>2009</year>). <article-title>An approach for matching communication patterns in parallel applications</article-title>. In: <source>2009 IEEE international symposium on parallel and distributed processing</source>, <comment>25–29 May 2009</comment>, pp. <fpage>1</fpage>–<lpage>12</lpage>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr20-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Massey</surname>
<given-names>FJ</given-names>
</name>
</person-group> (<year>1951</year>) <article-title>The Kolmogorov–Smirnov test for goodness of fit</article-title>. <source>Journal of the American Statistical Association</source> <volume>46</volume>(<issue>253</issue>): <fpage>68</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr21-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Metzger</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Wen</surname>
<given-names>Z</given-names>
</name>
</person-group> (<year>2000</year>) <source>Automatic algorithm recognition and replacement: a new approach to program optimization</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr22-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Milo</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Shen-Orr</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Itzkovitz</surname>
<given-names>S</given-names>
</name>
<etal/>
</person-group> (<year>2002</year>) <article-title>Network motifs: simple building blocks of complex networks</article-title>. <source>Science</source> <volume>298</volume>(<issue>5594</issue>): <fpage>824</fpage>–<lpage>827</lpage>.</citation>
</ref>
<ref id="bibr23-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Myers</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Wison</surname>
<given-names>RC</given-names>
</name>
<name>
<surname>Hancock</surname>
<given-names>ER</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>Bayesian graph edit distance</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>22</volume>(<issue>6</issue>): <fpage>628</fpage>–<lpage>635</lpage>.</citation>
</ref>
<ref id="bibr24-1094342012436618">
<citation citation-type="other">
<person-group person-group-type="author">
<name>
<surname>Peisert</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Fingerprinting communication and computation on HPC machines</article-title>. <comment>Report no. LBNL-3483E, Lawrence Berkeley National Laboratory, Berkeley, USA</comment>.</citation>
</ref>
<ref id="bibr25-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Preissl</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Schulz</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Kranzlmüller</surname>
<given-names>D</given-names>
</name>
<etal/>
</person-group> (<year>2010</year>) <article-title>Transforming MPI source code based on communication patterns</article-title>. <source>Future Generation Computer Systems</source> <volume>26</volume>(<issue>1</issue>): <fpage>147</fpage>–<lpage>154</lpage>.</citation>
</ref>
<ref id="bibr26-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Qian</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Hintze</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Adami</surname>
<given-names>C</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Colored motifs reveal computational building blocks in the C. elegans brain</article-title>. <source>PLoS ONE</source> <volume>6</volume>(<issue>3</issue>): <fpage>e17013</fpage>.</citation>
</ref>
<ref id="bibr27-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Reichardt</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Alamino</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Saad</surname>
<given-names>D</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>The interplay of microscopic and mesoscopic structure in complex networks</article-title>. <source>PLoS ONE</source>:<comment>1012</comment>.<fpage>4524</fpage>. </citation>
</ref>
<ref id="bibr28-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Riesen</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Communication patterns</article-title>. In: <source>20th International Conference on parallel and distributed processing, Rhodes Island, Greece</source>, <comment>25–29 April 2006</comment>, pp. <fpage>275</fpage>–<lpage>282</lpage>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr29-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Robles-Kelly</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Hancock</surname>
<given-names>ER</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>Graph edit distance from spectral seriation</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>27</volume>(<issue>3</issue>): <fpage>365</fpage>–<lpage>378</lpage>.</citation>
</ref>
<ref id="bibr30-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sanfeliu</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Fu</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>1983</year>) <article-title>A distance measure between attributed relational graphs for pattern recognition</article-title>. <source>IEEE Transactions on Systems, Man and Cybernetics</source> <volume>13</volume>(<issue>3</issue>): <fpage>353</fpage>–<lpage>362</lpage>.</citation>
</ref>
<ref id="bibr31-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Shalf</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Kamil</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Oliker</surname>
<given-names>L</given-names>
</name>
<etal/>
</person-group> (<year>2005</year>) <article-title>Analyzing ultra-scale application communication requirements for a reconfigurable hybrid interconnect</article-title>. In: <source>2005 ACM/IEEE conference on supercomputing</source>, <comment>Seattle, USA, 12–18 November 2005</comment>, pp. <fpage>17</fpage>–<lpage>18</lpage>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr32-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shervashidze</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Schweitzer</surname>
<given-names>P</given-names>
</name>
<name>
<surname>van Leeuwen</surname>
<given-names>EJ</given-names>
</name>
<etal/>
</person-group> (<year>2011</year>) <article-title>Weisfeiler–Lehman graph kernels</article-title>. <source>Journal of Machine Learning Research</source> <volume>12</volume>: <fpage>2539</fpage>–<lpage>2561</lpage>.</citation>
</ref>
<ref id="bibr33-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ullmann</surname>
<given-names>JR</given-names>
</name>
</person-group> (<year>1976</year>) <article-title>An algorithm for subgraph isomorphism</article-title>. <source>Journal of the ACM</source> <volume>23</volume>(<issue>1</issue>): <fpage>31</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr34-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wernicke</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Efficient detection of network motifs</article-title>. <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source> <volume>3</volume>(<issue>4</issue>): <fpage>347</fpage>–<lpage>359</lpage>.</citation>
</ref>
<ref id="bibr35-1094342012436618">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wernicke</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Rasche</surname>
<given-names>F</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>FANMOD: A tool for fast network motif detection</article-title>. <source>Bioinformatics</source> <volume>22</volume>(<issue>9</issue>): <fpage>1152</fpage>–<lpage>1153</lpage>.</citation>
</ref>
<ref id="bibr36-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>West</surname>
<given-names>DB</given-names>
</name>
</person-group> (<year>2001</year>) <source>Introduction to graph theory</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr37-1094342012436618">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Whalen</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Security applications of the e-machine</article-title>. <comment>PhD thesis</comment>, <publisher-name>University of California</publisher-name>, <publisher-loc>Davis, USA</publisher-loc>.</citation>
</ref>
</ref-list>
</back>
</article>