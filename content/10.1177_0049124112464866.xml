<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><front><journal-meta><journal-id journal-id-type="publisher-id">SMR</journal-id><journal-id journal-id-type="hwp">spsmr</journal-id><journal-title>Sociological Methods &amp; Research</journal-title><issn pub-type="ppub">0049-1241</issn><issn pub-type="epub">1552-8294</issn><publisher><publisher-name>SAGE Publications</publisher-name><publisher-loc>Sage CA: Los Angeles, CA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1177/0049124112464866</article-id><article-id pub-id-type="publisher-id">10.1177_0049124112464866</article-id><article-categories><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group></article-categories><title-group><article-title>Should a Normal Imputation Model be Modified to Impute Skewed Variables?</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>von Hippel</surname><given-names>Paul T.</given-names></name><xref ref-type="aff" rid="aff1-0049124112464866">1</xref><xref ref-type="corresp" rid="corresp1-0049124112464866"/></contrib><bio><title>Author Biography</title><p><bold>Paul T. von Hippel</bold> is an assistant professor in the LBJ School of Public Affairs, University of Texas, Austin.</p></bio></contrib-group><aff id="aff1-0049124112464866"><label>1</label>LBJ School of Public Affairs, University of Texas, Austin, TX, USA</aff><author-notes><corresp id="corresp1-0049124112464866">Paul T. von Hippel, LBJ School of Public Affairs, University of Texas, 2315 Red River, Box Y, Austin, TX 78712, USA. Email: <email>paulvonhippel.utaustin@gmail.com</email></corresp></author-notes><pub-date pub-type="epub-ppub"><month>2</month><year>2013</year></pub-date><volume>42</volume><issue>1</issue><fpage>105</fpage><lpage>138</lpage><permissions><copyright-statement>© The Author(s) 2012</copyright-statement><copyright-year>2012</copyright-year><copyright-holder content-type="sage">SAGE Publications</copyright-holder></permissions><abstract><p>Researchers often impute continuous variables under an assumption of normality–yet many incomplete variables are skewed. We find that imputing skewed continuous variables under a normal model can lead to bias. The bias is usually mild for popular estimands such as means, standard deviations, and linear regression coefficients, but the bias can be severe for more shape-dependent estimands such as percentiles or the coefficient of skewness. We test several methods for adapting a normal imputation model to accommodate skewness, including methods that transform, truncate, or censor (round) normally imputed values as well as methods that impute values from a quadratic or truncated regression. None of these modifications reliably reduces the biases of the normal model, and some modifications can make the biases much worse. We conclude that, if one has to impute a skewed variable under a normal model, it is usually safest to do so without modifications–unless you are more interested in estimating percentiles and shape than in estimating means, variances, and regressions. In the conclusion, we briefly discuss promising developments in the area of continuous imputation models that do not assume normality.</p></abstract><kwd-group><kwd>missing data</kwd><kwd>missing values</kwd><kwd>incomplete data</kwd><kwd>regression</kwd><kwd>transformation</kwd><kwd>normalization</kwd><kwd>multiple imputation</kwd><kwd>imputation</kwd></kwd-group></article-meta></front><body><sec id="section1-0049124112464866"><title>Introduction</title><p>Imputation is an increasingly popular method for handling data with missing values. When using imputation, analysts fill in missing values with random draws from an imputation model and then fit the imputed data to an analysis model. In multiple imputation (MI), the process of imputation and analysis is repeated several times and the results of the several analyses are combined (<xref ref-type="bibr" rid="bibr35-0049124112464866">Rubin 1987</xref>; <xref ref-type="bibr" rid="bibr2-0049124112464866">Allison 2002</xref>; <xref ref-type="bibr" rid="bibr25-0049124112464866">Kenward and Carpenter 2007</xref>).</p><p>In an ideal world, the imputation model would perfectly represent the distribution of the data. But perfect fidelity can be very difficult to achieve and in practice it is often unnecessary. All that is necessary is that the imputation model preserves those aspects of the distribution <italic>that are relevant to the analysis model.</italic> For example, if the analyst plans only to estimates means, standard deviations, and the parameters of a linear regression model, then the imputation model needs only to preserve the means, variances, and covariances among the variables that will be analyzed.</p><p>When the goals of analysis are limited in this way, as they often are, a crude imputation model can yield usable results. For example, in some settings, it can be acceptable to impute dummy variables, squared terms, and interactions as though they were normal when conditioned on other variables (<xref ref-type="bibr" rid="bibr22-0049124112464866">Horton, Lipsitz, and Parzen 2003</xref>; <xref ref-type="bibr" rid="bibr3-0049124112464866">Allison 2005</xref>; <xref ref-type="bibr" rid="bibr6-0049124112464866">Bernaards, Belin, and Schafer 2007</xref>; <xref ref-type="bibr" rid="bibr42-0049124112464866">von Hippel 2009</xref>). The resulting imputed values will look implausible if inspected closely but that often has little effect on the analytic results. In fact, attempts to edit the imputed values to improve their plausibility can introduce bias by changing the variables’ means, variance, and covariances (<xref ref-type="bibr" rid="bibr22-0049124112464866">Horton et al. 2003</xref>; <xref ref-type="bibr" rid="bibr3-0049124112464866">Allison 2005</xref>; <xref ref-type="bibr" rid="bibr6-0049124112464866">Bernaards et al. 2007</xref>; <xref ref-type="bibr" rid="bibr42-0049124112464866">von Hippel 2009</xref>).</p><p>The point of imputation is not that the imputed values should <italic>look</italic> like observed values. The point is that the imputed variable should <italic>act</italic> like the observed variable when used in analysis.</p><p>This article considers methods for imputing a different class of nonnormal variables—namely, variables with skew. Like other nonnormal variables, skewed variables are often imputed as though they were conditionally normal. The word <italic>conditionally</italic> is important here; later, we will encounter situations where a variable is skewed and yet the residuals are approximately normal when the skewed variable is conditioned on other variables. In that situation, a conditionally normal imputation model may be perfectly specified even though the imputed variable is skewed. In many other situations, though, imputing a skewed variable from a normal model entails some degree of misspecification.</p><p>In calculations and simulations, we find that conditionally normal imputation of a skewed variable can often produce acceptable estimates if the quantities being estimated are means, variances, and regressions. The estimates do have biases under some circumstances but the biases are typically small. However, the biases grow much larger if we estimate quantities that depend more strongly on distributional shape—quantities such as percentiles or the coefficient of skewness.</p><p>To increase the skew of a normally imputed variable, popular references recommend modifications to the normal imputation model. Modifications include rounding (censoring) the imputed values, truncating the imputed values, imputing from a truncated regression model, or transforming the incomplete variable to better approximate normality. We evaluate all these methods as well as a new method that adds quadratic terms to the imputation model.</p><p>In univariate data, we find that such modifications can work well if very carefully applied. But in bivariate and trivariate data, we find that even careful modification of the normal imputation model is problematic. At best, modifications reduce the biases only a little; at worst, modifications make the biases much worse. We conclude that, if you have to impute a skewed variable as though it were normal, it is safest to do so without modification.</p><p>In the conclusion, we discuss nonnormal imputation models that are currently in development. Early evaluations of these methods look promising and we hope that they will soon become more widely available.</p><p>Our presentation proceeds in order of complexity, progressing from univariate to bivariate to multivariate data.</p></sec><sec id="section2-0049124112464866"><title>Univariate Data</title><p>Imputation is rarely useful in univariate data since, if univariate values are missing at random (MAR), the observed values are a random sample from the population and can be analyzed just as though the data were complete. Yet the simplicity of the univariate setting can help clarify the advantages and disadvantages of different imputation methods.</p><p>Suppose we have a simple random sample of <italic>n</italic> values from a nonnormal variable <italic>X</italic> with finite mean μ and finite variance σ<sup><xref ref-type="fn" rid="fn2-0049124112464866">2</xref></sup>. A randomly selected <italic>n</italic><sub>mis</sub> of the <italic>X</italic> values are missing, so that <italic>n</italic><sub>obs</sub> = <italic>n</italic>–<italic>n</italic><sub>mis</sub> values are observed. We assume that values are MAR (<xref ref-type="bibr" rid="bibr34-0049124112464866">Rubin 1976</xref>; <xref ref-type="bibr" rid="bibr20-0049124112464866">Heitjan and Basu 1996</xref>), which in the univariate setting means that the <italic>n</italic><sub>obs</sub> observed <italic>X</italic> values are a random sample from the <italic>n</italic> sampled cases and therefore a random sample from the population.</p><p>How well can normal imputation, with or without modification, impute this nonnormal variable?</p><sec id="section3-0049124112464866"><title>Fully Normal (FN) Imputation</title><p>The first and simplest technique is fully normal (FN) imputation (<xref ref-type="bibr" rid="bibr36-0049124112464866">Rubin and Schenker 1986</xref>), under which a nonnormal variable <italic>X</italic> is imputed as though it were normal.</p><p>Since the observed values are a random sample from the population, we can obtain consistent estimates <inline-formula id="inline-formula1-0049124112464866"><mml:math id="mml-inline1-0049124112464866"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> from the observed values alone, then use those estimates to impute the missing values with random draws from a normal distribution <inline-formula id="inline-formula2-0049124112464866"><mml:math id="mml-inline2-0049124112464866"><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. We then analyze the mix of imputed and observed values as though it were complete.</p><p>The consistency of the observed-value estimators <inline-formula id="inline-formula3-0049124112464866"><mml:math id="mml-inline3-0049124112464866"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> does not depend on <italic>X</italic> being normal. The most obvious observed-value estimator are the mean <inline-formula id="inline-formula4-0049124112464866"><mml:math id="mml-inline4-0049124112464866"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo accent="true" stretchy="false">ˉ</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and variance <inline-formula id="inline-formula5-0049124112464866"><mml:math id="mml-inline5-0049124112464866"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> of the observed values, which are minimum variance unbiased (MVU) estimators whether <italic>X</italic> is normal or not (<xref ref-type="bibr" rid="bibr15-0049124112464866">Halmos 1946</xref>). And the estimators that are commonly used in MI are obtained by supplementing <inline-formula id="inline-formula6-0049124112464866"><mml:math id="mml-inline6-0049124112464866"><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo accent="true" stretchy="false">ˉ</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="inline-formula7-0049124112464866"><mml:math id="mml-inline7-0049124112464866"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> with random variation to obtain Bayesian posterior draw (PD) estimators <inline-formula id="inline-formula8-0049124112464866"><mml:math id="mml-inline8-0049124112464866"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula id="inline-formula9-0049124112464866"><mml:math id="mml-inline9-0049124112464866"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>—<disp-formula id="disp-formula1-0049124112464866"><label>1</label><mml:math id="mml-disp1-0049124112464866"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>U</mml:mi></mml:mfrac></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:mi>U</mml:mi><mml:mo stretchy="false">~</mml:mo><mml:msubsup><mml:mi mathvariant="italic">χ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mo stretchy="false">+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi mathvariant="italic">ν</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math><graphic alternate-form-of="disp-formula1-0049124112464866" xlink:href="10.1177_0049124112464866-eq1.tif"/></disp-formula><disp-formula id="disp-formula2-0049124112464866"><mml:math id="mml-disp2-0049124112464866"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow/></mml:msubsup><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo accent="true" stretchy="false">ˉ</mml:mo></mml:mover><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mfenced></mml:math><graphic alternate-form-of="disp-formula2-0049124112464866" xlink:href="10.1177_0049124112464866-eq2.tif"/></disp-formula>—which simulate random draws from the posterior density of <inline-formula id="inline-formula10-0049124112464866"><mml:math id="mml-inline10-0049124112464866"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:msup><mml:mi mathvariant="italic">σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (<xref ref-type="bibr" rid="bibr36-0049124112464866">Rubin and Schenker 1986</xref>). Note that <inline-formula id="inline-formula11-0049124112464866"><mml:math id="mml-inline11-0049124112464866"><mml:msub><mml:mi mathvariant="italic">ν</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="disp-formula1-0049124112464866">equation (1)</xref> is the degrees of freedom in the Bayesian prior (<xref ref-type="bibr" rid="bibr44-0049124112464866">von Hippel 2012</xref>).</p><p>The PD estimators, like the MVU estimators, are consistent.<sup><xref ref-type="fn" rid="fn1-0049124112464866">1</xref></sup> However, the PD estimators are not particularly efficient, and <inline-formula id="inline-formula12-0049124112464866"><mml:math id="mml-inline12-0049124112464866"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> can be biased in small samples (<xref ref-type="bibr" rid="bibr44-0049124112464866">von Hippel 2012</xref>).</p><p>Small-sample properties of the MVU and PD estimators are discussed elsewhere (<xref ref-type="bibr" rid="bibr44-0049124112464866">von Hippel 2012</xref>). We can avoid some detail by assuming that we have a very large sample where <inline-formula id="inline-formula13-0049124112464866"><mml:math id="mml-inline13-0049124112464866"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> are practically indistinguishable from <inline-formula id="inline-formula14-0049124112464866"><mml:math id="mml-inline14-0049124112464866"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:msup><mml:mi mathvariant="italic">σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. Under that assumption, we are effectively imputing missing values from <inline-formula id="inline-formula15-0049124112464866"><mml:math id="mml-inline15-0049124112464866"><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:msup><mml:mi mathvariant="italic">σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, and any biases we encounter are asymptotic—that is, any biases we encounter cannot be eliminated by increasing the sample size.</p><p><xref ref-type="fig" rid="fig1-0049124112464866">Figure 1</xref> illustrates the large-sample properties of FN imputation in a setting where the observed data are skewed. Here the observed values come from a standard exponential density <inline-formula id="inline-formula16-0049124112464866"><mml:math id="mml-inline16-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, with mean <inline-formula id="inline-formula17-0049124112464866"><mml:math id="mml-inline17-0049124112464866"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and variance <inline-formula id="inline-formula18-0049124112464866"><mml:math id="mml-inline18-0049124112464866"><mml:msup><mml:mi mathvariant="italic">μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, while the imputed values <inline-formula id="inline-formula19-0049124112464866"><mml:math id="mml-inline19-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:msup><mml:mi mathvariant="italic">μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> come from a normal density with the same mean and variance as the observed exponential variable.</p><fig id="fig1-0049124112464866" position="float"><label>Figure 1.</label><caption><p>Distribution of a standard exponential observed variable and a normal imputed variable that has the same mean and variance. </p></caption><graphic xlink:href="10.1177_0049124112464866-fig1.tif"/></fig><p>Although the observed and imputed variables have the same mean and variance, they do not have the same distributional shape, and that implies that, in a mix of observed and imputed values, some quantities will be estimated with bias. For example, estimates of skewness will be biased toward zero since the imputed values have no skew. And the first sixteen percentiles, at least, will be negatively biased since 16 percent of the imputed distribution is negative while the observed distribution is strictly nonnegative. The lower panel of <xref ref-type="fig" rid="fig1-0049124112464866">Figure 1</xref> examines all the percentiles by comparing the cumulative distribution function (CDF) of the observed and imputed variables. Percentiles 24–89.5 are positively biased since the imputed CDF is right of the observed CDF, but all the other percentiles are negatively biased since the imputed CDF is left of the observed CDF. In some distributions, FN imputation of nonnormal variables causes more bias in the extreme percentiles than in percentiles near the median (<xref ref-type="bibr" rid="bibr11-0049124112464866">Demirtas, Freels, and Yucel 2008</xref>) but in this skewed example the 50th percentile has substantial bias while the 90th percentile has almost none.</p><p>In sum, when nonnormal data are imputed under a normal model, estimates of the mean and variance will be consistent, but there can be considerable bias in estimating the shape and percentiles. Simulations confirm that FN imputation yields consistent estimates for <inline-formula id="inline-formula20-0049124112464866"><mml:math id="mml-inline20-0049124112464866"><mml:mi mathvariant="italic">μ</mml:mi></mml:math></inline-formula> and <inline-formula id="inline-formula21-0049124112464866"><mml:math id="mml-inline21-0049124112464866"><mml:msup><mml:mi mathvariant="italic">σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> under a variety of nonnormal distributions (<xref ref-type="bibr" rid="bibr36-0049124112464866">Rubin and Schenker 1986</xref>; <xref ref-type="bibr" rid="bibr17-0049124112464866">He and Raghunathan 2006</xref>; <xref ref-type="bibr" rid="bibr12-0049124112464866">Demirtas and Hedeker 2008</xref>) but FN imputation yields biased estimates of many percentiles (<xref ref-type="bibr" rid="bibr17-0049124112464866">He and Raghunathan 2006</xref>; <xref ref-type="bibr" rid="bibr11-0049124112464866">Demirtas et al. 2008</xref>; <xref ref-type="bibr" rid="bibr12-0049124112464866">Demirtas and Hedeker 2008</xref>).</p></sec><sec id="section4-0049124112464866"><title>Imputing Within Bounds: Censoring and Truncation</title><p>What can make the observed and imputed distributions more similar? A popular approach is to bound normally imputed values within a plausible range. For example, in imputing a positively skewed variable like body weight, we might require the imputed values to be larger than the smallest observed body weight, or not so small as to be “biologically implausible” (<xref ref-type="bibr" rid="bibr9-0049124112464866">Centers for Disease Control and Prevention 2011</xref>). Options for bounding imputed variables are available in most popular imputation software, including IVEware, the MI procedure in SAS 9.2, the <italic>mi impute</italic> command in Stata 12, and the Missing Values Analysis package in SPSS 16.0.</p><p>There are two general approaches to bounding: censoring and truncation. For example, in <xref ref-type="fig" rid="fig1-0049124112464866">Figure 1</xref>, where we imputed a standard exponential variable as though it were normal, we could have used censoring or truncation to ensure that the normally imputed values were nonnegative. To do this, we would first generate normal imputations <inline-formula id="inline-formula22-0049124112464866"><mml:math id="mml-inline22-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> where <inline-formula id="inline-formula23-0049124112464866"><mml:math id="mml-inline23-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> are the mean and variance of the imputed variable before bounds have been imposed. We would then round negative imputed values up to zero (censoring), or reimpute negative values repeatedly until a positive value occurs (truncation). Truncation results in a distribution of imputed values that is strictly positive, while censoring results in a distribution that is strictly positive except for a point mass at <inline-formula id="inline-formula24-0049124112464866"><mml:math id="mml-inline24-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p><p>Bounding imputed values changes the mean and variance of the imputed variable. After bounding, the mean of the imputed variable is higher than <inline-formula id="inline-formula25-0049124112464866"><mml:math id="mml-inline25-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> because negative values have been shifted upward, and the variance of the imputed variable is smaller than <inline-formula id="inline-formula26-0049124112464866"><mml:math id="mml-inline26-0049124112464866"><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mrow></mml:msubsup></mml:math></inline-formula> because out-of-bounds values have been shifted inward. More specifically, if an imputed normal variable <inline-formula id="inline-formula27-0049124112464866"><mml:math id="mml-inline27-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is <italic>truncated</italic> to the left of <italic>c</italic>, the truncated variable has mean and variance<disp-formula id="disp-formula3-0049124112464866"><label>2</label><mml:math id="mml-disp3-0049124112464866"><mml:mtable columnalign="right left" columnspacing="thickmathspace" displaystyle="true" rowspacing=".5em"><mml:mtr><mml:mtd/><mml:mtd><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">λ</mml:mi></mml:mrow></mml:mrow><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi mathvariant="italic">δ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic alternate-form-of="disp-formula3-0049124112464866" xlink:href="10.1177_0049124112464866-eq3.tif"/></disp-formula>And if the same imputed normal variable is <italic>censored</italic> to the left of <italic>c</italic>, the mean and variance are<disp-formula id="disp-formula4-0049124112464866"><mml:math id="mml-disp4-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>c</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic alternate-form-of="disp-formula4-0049124112464866" xlink:href="10.1177_0049124112464866-eq4.tif"/></disp-formula><disp-formula id="disp-formula5-0049124112464866"><label>3</label><mml:math id="mml-disp5-0049124112464866"><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mfenced close=")" open="("><mml:mrow><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">+</mml:mo><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi mathvariant="italic">λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic alternate-form-of="disp-formula5-0049124112464866" xlink:href="10.1177_0049124112464866-eq5.tif"/></disp-formula>Here <inline-formula id="inline-formula28-0049124112464866"><mml:math id="mml-inline28-0049124112464866"><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the standardized boundary <italic>c</italic>;<inline-formula id="inline-formula29-0049124112464866"><mml:math id="mml-inline29-0049124112464866"><mml:mtext> </mml:mtext><mml:mi mathvariant="italic">ϕ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> and <inline-formula id="inline-formula30-0049124112464866"><mml:math id="mml-inline30-0049124112464866"><mml:msub><mml:mi mathvariant="italic">π</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> are the standard normal PDF and CDF evaluated at <inline-formula id="inline-formula31-0049124112464866"><mml:math id="mml-inline31-0049124112464866"><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>; <inline-formula id="inline-formula32-0049124112464866"><mml:math id="mml-inline32-0049124112464866"><mml:msub><mml:mi mathvariant="italic">λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mi mathvariant="italic">ϕ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow></mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the Mills ratio; and <inline-formula id="inline-formula33-0049124112464866"><mml:math id="mml-inline33-0049124112464866"><mml:msub><mml:mi mathvariant="italic">δ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi mathvariant="italic">λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> (e.g., <xref ref-type="bibr" rid="bibr23-0049124112464866">Johnson, Kotz, and Balakrishnan 1994</xref>, section 10.3; <xref ref-type="bibr" rid="bibr14-0049124112464866">Greene 1999</xref>:907; <xref ref-type="bibr" rid="bibr24-0049124112464866">Jöreskog 2002</xref>).</p><p>Bias can occur if we fail to anticipate the effect of bounding on the mean and variance of the imputed variable. To return to our running example, if the observed variable is standard exponential <inline-formula id="inline-formula34-0049124112464866"><mml:math id="mml-inline34-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, with mean <inline-formula id="inline-formula35-0049124112464866"><mml:math id="mml-inline35-0049124112464866"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and variance <inline-formula id="inline-formula36-0049124112464866"><mml:math id="mml-inline36-0049124112464866"><mml:msup><mml:mi mathvariant="italic">μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, then we might naively impute a normal variable with the same mean and variance <inline-formula id="inline-formula37-0049124112464866"><mml:math id="mml-inline37-0049124112464866"><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. But if we censor this normal variable to the left of <inline-formula id="inline-formula38-0049124112464866"><mml:math id="mml-inline38-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, the mean of the censored variable will be <inline-formula id="inline-formula39-0049124112464866"><mml:math id="mml-inline39-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">≈</mml:mo><mml:mn>1.08</mml:mn></mml:math></inline-formula> and the variance will be <inline-formula id="inline-formula40-0049124112464866"><mml:math id="mml-inline40-0049124112464866"><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">≈</mml:mo><mml:mn>0.75</mml:mn></mml:math></inline-formula>—that is, the mean will have a bias of 8 percent and the variance will have a bias of −25 percent. The biases will be even larger if we <italic>truncate</italic> the normal imputed variable to the left of <inline-formula id="inline-formula41-0049124112464866"><mml:math id="mml-inline41-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>; then the mean of the truncated variable will be <inline-formula id="inline-formula42-0049124112464866"><mml:math id="mml-inline42-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">≈</mml:mo><mml:mn>1.29</mml:mn></mml:math></inline-formula>, which is biased by 29 percent, and the variance will be <inline-formula id="inline-formula43-0049124112464866"><mml:math id="mml-inline43-0049124112464866"><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">≈</mml:mo><mml:mn>0.63</mml:mn></mml:math></inline-formula>, which is biased by –37 percent.</p><p>These biases can be avoided, or at least reduced, if we anticipate the effect of censoring on the mean and variance of the imputed variable. By inverting <xref ref-type="disp-formula" rid="disp-formula3-0049124112464866">equations (2)</xref> and (<xref ref-type="disp-formula" rid="disp-formula5-0049124112464866">3</xref>), we can choose <inline-formula id="inline-formula44-0049124112464866"><mml:math id="mml-inline44-0049124112464866"><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> so that, <italic>after</italic> bounds are imposed, the mean and variance of the bound imputations will be the same as those of <inline-formula id="inline-formula45-0049124112464866"><mml:math id="mml-inline45-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math>,</inline-formula> or at least close. Let us return to our example where the observed variable <inline-formula id="inline-formula46-0049124112464866"><mml:math id="mml-inline46-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow></mml:mfenced></mml:math></inline-formula> is standard exponential with mean <inline-formula id="inline-formula47-0049124112464866"><mml:math id="mml-inline47-0049124112464866"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and variance <inline-formula id="inline-formula48-0049124112464866"><mml:math id="mml-inline48-0049124112464866"><mml:msup><mml:mi mathvariant="italic">μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. If we want to get the same mean and variance from a normal variable that is censored on the left at <italic>c </italic>= 0, we can do so by choosing <inline-formula id="inline-formula49-0049124112464866"><mml:math id="mml-inline49-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">≈</mml:mo><mml:mn>.785</mml:mn></mml:math></inline-formula> and <inline-formula id="inline-formula50-0049124112464866"><mml:math id="mml-inline50-0049124112464866"><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow/></mml:msubsup><mml:mo stretchy="false">≈</mml:mo><mml:mn>1.29</mml:mn></mml:math></inline-formula>; then, after censoring, we will have a censored variable with a mean and variance of <inline-formula id="inline-formula51-0049124112464866"><mml:math id="mml-inline51-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.</p><p>Similarly, if we want to get a similar mean and variance from a normal variable that is truncated on the left at <italic>c </italic>= 0, we can get close by choosing <inline-formula id="inline-formula52-0049124112464866"><mml:math id="mml-inline52-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">−</mml:mo><mml:mn>6.28</mml:mn></mml:math></inline-formula> and <inline-formula id="inline-formula53-0049124112464866"><mml:math id="mml-inline53-0049124112464866"><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow/></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>3.02</mml:mn></mml:math></inline-formula>. Then after truncation we will have a truncated variable with a mean of <inline-formula id="inline-formula54-0049124112464866"><mml:math id="mml-inline54-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mn>1.1</mml:mn></mml:math></inline-formula> and a variance of <inline-formula id="inline-formula55-0049124112464866"><mml:math id="mml-inline55-0049124112464866"><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.</p><p>Notice that, although the censored normal imputations have the same mean as the observed variable, the truncated normal imputations do not. In fact, with <italic>c </italic>= 0, it is impossible to get the mean and variance of a censored normal variable to equal <inline-formula id="inline-formula56-0049124112464866"><mml:math id="mml-inline56-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> exactly. In fact, as we approach <inline-formula id="inline-formula57-0049124112464866"><mml:math id="mml-inline57-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, <inline-formula id="inline-formula58-0049124112464866"><mml:math id="mml-inline58-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow/></mml:msubsup></mml:math></inline-formula> approaches –∞. In the area near <inline-formula id="inline-formula59-0049124112464866"><mml:math id="mml-inline59-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mtext mathvariant="normal"> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> it is very hard to estimate the parameters <inline-formula id="inline-formula60-0049124112464866"><mml:math id="mml-inline60-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mrow/></mml:msubsup></mml:math></inline-formula> with any precision.</p><p>We have just seen that, even in a simple univariate setting, a truncated normal model can have trouble matching the moments of a variable that is not in fact truncated normal. When the truncated normal model is used for this purpose, the parameter estimates can be sensitive or even infinite. Later, we will encounter similar problems when we use the truncated model in a multivariate setting.</p><p><xref ref-type="fig" rid="fig2-0049124112464866">Figure 2</xref> illustrates our attempts to match the distribution of a standard exponential variable with a truncated or censored normal variable. The figure illustrates a naïve approach, where the mean and variance of the imputed variable are unbiased before censoring or truncation, but biased after. The figure also illustrates less-biased approaches, where the effect of truncation or censoring is anticipated before the imputations are drawn. Notice that, if the effects of truncation are anticipated, the truncated normal variable achieves a close match to the exponential distribution (lower left)—notwithstanding some bias in the mean.</p><fig id="fig2-0049124112464866" position="float"><label>Figure 2.</label><caption><p>Biased and less-biased ways of imputing a censored or truncated normal variable to match the distribution of an observed exponential variable.</p></caption><graphic xlink:href="10.1177_0049124112464866-fig2.tif"/></fig></sec><sec id="section5-0049124112464866"><title>Transformation</title><p>Instead of bounding imputed<italic> X</italic> values, some experts recommend <italic>transforming</italic> an incomplete skewed <italic>X</italic> variable to better approximate normality (<xref ref-type="bibr" rid="bibr39-0049124112464866">Schafer and Olsen 1998</xref>:550; <xref ref-type="bibr" rid="bibr38-0049124112464866">Schafer and Graham 2002</xref>:167; <xref ref-type="bibr" rid="bibr2-0049124112464866">Allison 2002</xref>:39; <xref ref-type="bibr" rid="bibr31-0049124112464866">Raghunathan et al. 2001</xref>:82-83). Following this advice, we would subject the observed values <inline-formula id="inline-formula61-0049124112464866"><mml:math id="mml-inline61-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> to a skew-reducing transformation such as a log or root, which yields a transformed variable <inline-formula id="inline-formula62-0049124112464866"><mml:math id="mml-inline62-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> with a mean and variance of <inline-formula id="inline-formula63-0049124112464866"><mml:math id="mml-inline63-0049124112464866"><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. We then apply FN imputation to the transformed variable to obtain normal imputed values <inline-formula id="inline-formula64-0049124112464866"><mml:math id="mml-inline64-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. Finally, we invert the transformation to recover the original observed values <inline-formula id="inline-formula65-0049124112464866"><mml:math id="mml-inline65-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and put the imputed values <inline-formula id="inline-formula66-0049124112464866"><mml:math id="mml-inline66-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> on a comparable scale. SAS automates this process in the MI procedure, where the TRANSFORM command offers a choice among a variety of nonlinear transformations including the log, power, <xref ref-type="bibr" rid="bibr8-0049124112464866">Box-Cox (1964)</xref>, logit, and exponential transformations (<xref ref-type="bibr" rid="bibr37-0049124112464866">SAS Institute 2001</xref>).</p><p>In the univariate setting, transformation works well if the transformed variable is in fact normal or close to it. For example, if we apply a log transformation to a lognormal variable, the transformed variable is exactly normal.</p><p>But transformation can yield substantial bias if the transformed variable is not close to normal. To return to our earlier example, if we sample an exponential variable <inline-formula id="inline-formula67-0049124112464866"><mml:math id="mml-inline67-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and transform it with a log, the logged variable <inline-formula id="inline-formula68-0049124112464866"><mml:math id="mml-inline68-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is far from normal (<xref ref-type="bibr" rid="bibr16-0049124112464866">Hawkins and Wixley 1986</xref>). If we impute a normal variable <inline-formula id="inline-formula69-0049124112464866"><mml:math id="mml-inline69-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> on the log scale, inverting the transformation will yield an imputed <inline-formula id="inline-formula70-0049124112464866"><mml:math id="mml-inline70-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> whose mean and variance are far from the mean and variance of the original exponential variable (<xref ref-type="bibr" rid="bibr17-0049124112464866">He and Raghunathan 2006</xref>).</p><p>The challenge of transformation is that skew-reducing transformations are nonlinear, and nonlinear transformations do complicated things to the mean and variance. Although on the transformed scale the observed variable <inline-formula id="inline-formula71-0049124112464866"><mml:math id="mml-inline71-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the imputed variable <inline-formula id="inline-formula72-0049124112464866"><mml:math id="mml-inline72-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> have approximately the same mean and variance <inline-formula id="inline-formula73-0049124112464866"><mml:math id="mml-inline73-0049124112464866"><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, after the transformation is inverted there is no guarantee that <inline-formula id="inline-formula74-0049124112464866"><mml:math id="mml-inline74-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="inline-formula75-0049124112464866"><mml:math id="mml-inline75-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, unless they have the same distribution, will also have the same mean and variance <inline-formula id="inline-formula76-0049124112464866"><mml:math id="mml-inline76-0049124112464866"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:msup><mml:mi mathvariant="italic">σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>.</p><p>The transformation method can yield better results if the transformation is carefully chosen. In imputing an exponential variable <inline-formula id="inline-formula77-0049124112464866"><mml:math id="mml-inline77-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>E</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, for example, it is not hard to see why the logarithm is a poor choice. The log of an exponential variable is undefined at <inline-formula id="inline-formula78-0049124112464866"><mml:math id="mml-inline78-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>, and although a value of exactly zero will never occur in an exponential sample, values close to zero occur frequently, and the logs of these near-zero values will be large negative numbers. So the log of a standard exponential sample has a long left tail and is far from normal.<sup><xref ref-type="fn" rid="fn2-0049124112464866">2</xref></sup></p><p><xref ref-type="fig" rid="fig3-0049124112464866">Figure 3</xref> illustrates two transformations that do a better job of normalizing an exponential variable <inline-formula id="inline-formula79-0049124112464866"><mml:math id="mml-inline79-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>: the square root transformation <inline-formula id="inline-formula80-0049124112464866"><mml:math id="mml-inline80-0049124112464866"><mml:msqrt><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:msqrt></mml:math></inline-formula> and the fourth root transformation <inline-formula id="inline-formula81-0049124112464866"><mml:math id="mml-inline81-0049124112464866"><mml:mroot><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula>. Unlike the log, both the square root and the fourth root are defined at <inline-formula id="inline-formula82-0049124112464866"><mml:math id="mml-inline82-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>. The square root transformation is common in data analysis while the fourth root is less common but comes as close to normalizing the exponential distribution as nearly any transformation can (<xref ref-type="bibr" rid="bibr16-0049124112464866">Hawkins and Wixley 1986</xref>).</p><fig id="fig3-0049124112464866" position="float"><label>Figure 3.</label><caption><p>Normal imputation of an observed exponential variable under square-root and fourth-root transformation.</p></caption><graphic xlink:href="10.1177_0049124112464866-fig3.tif"/></fig><p>Both the square-root and fourth-root transformations yield an imputed variable whose mean is unbiased, or nearly unbiased, when compared to the mean of an exponential variable. The variance of the imputed variable is biased, though less biased for the fourth root than for the square root.</p><p>More specifically,<sup><xref ref-type="fn" rid="fn3-0049124112464866">3</xref></sup> if we subject an observed standard exponential variable <inline-formula id="inline-formula83-0049124112464866"><mml:math id="mml-inline83-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> to a square root transformation, the result is a transformed variable <inline-formula id="inline-formula84-0049124112464866"><mml:math id="mml-inline84-0049124112464866"><mml:msqrt><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:msqrt><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> that has a Rayleigh distribution with mean <inline-formula id="inline-formula85-0049124112464866"><mml:math id="mml-inline85-0049124112464866"><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msqrt><mml:mi mathvariant="italic">π</mml:mi></mml:msqrt><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:math></inline-formula> and variance <inline-formula id="inline-formula86-0049124112464866"><mml:math id="mml-inline86-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:mi mathvariant="italic">π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:math></inline-formula>. On the transformed scale, we can generate a normal imputed variable <inline-formula id="inline-formula87-0049124112464866"><mml:math id="mml-inline87-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with the same mean <inline-formula id="inline-formula88-0049124112464866"><mml:math id="mml-inline88-0049124112464866"><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> and variance <inline-formula id="inline-formula89-0049124112464866"><mml:math id="mml-inline89-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. Finally, we invert the transformation, squaring all the values to recover the original observed variable <inline-formula id="inline-formula90-0049124112464866"><mml:math id="mml-inline90-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> and produce an imputed variable <inline-formula id="inline-formula91-0049124112464866"><mml:math id="mml-inline91-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> that, as the square of a normal variable, has a scaled noncentral chi-square distribution, with a mean of <inline-formula id="inline-formula92-0049124112464866"><mml:math id="mml-inline92-0049124112464866"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and a variance of <inline-formula id="inline-formula93-0049124112464866"><mml:math id="mml-inline93-0049124112464866"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">π</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>8</mml:mn><mml:mo stretchy="false">≈</mml:mo><mml:mn>.77</mml:mn></mml:math></inline-formula>. The mean of <inline-formula id="inline-formula94-0049124112464866"><mml:math id="mml-inline94-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is unbiased, but the variance of <inline-formula id="inline-formula95-0049124112464866"><mml:math id="mml-inline95-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, when compared to the variance <inline-formula id="inline-formula96-0049124112464866"><mml:math id="mml-inline96-0049124112464866"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> of the observed variable, is negatively biased by 23 percent.</p><p>We get better results with a fourth-root transformation, although there is still a small bias. If the observed variable <inline-formula id="inline-formula97-0049124112464866"><mml:math id="mml-inline97-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> has a standard exponential distribution, then the fourth-root transformed variable <inline-formula id="inline-formula98-0049124112464866"><mml:math id="mml-inline98-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mroot><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> is very close to normal with a mean of <inline-formula id="inline-formula99-0049124112464866"><mml:math id="mml-inline99-0049124112464866"><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">≈</mml:mo><mml:mn>.91</mml:mn></mml:math></inline-formula> and a variance of <inline-formula id="inline-formula100-0049124112464866"><mml:math id="mml-inline100-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">≈</mml:mo><mml:mn>.065</mml:mn></mml:math></inline-formula>. If we impute a normal variable on the transformed scale <inline-formula id="inline-formula101-0049124112464866"><mml:math id="mml-inline101-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="italic">μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> with the same mean and variance as <inline-formula id="inline-formula102-0049124112464866"><mml:math id="mml-inline102-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, then when we invert the transformation the imputed variable <inline-formula id="inline-formula103-0049124112464866"><mml:math id="mml-inline103-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mtext> </mml:mtext><mml:mi>t</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup></mml:math></inline-formula> has a mean of 1.006 and a variance of 1.135. The mean of <inline-formula id="inline-formula104-0049124112464866"><mml:math id="mml-inline104-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> has less than 1 percent bias while the variance of <inline-formula id="inline-formula105-0049124112464866"><mml:math id="mml-inline105-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is positively biased by 13.5 percent. The shape of the imputed distribution closely matches the observed variable except in the range very close to <italic>X </italic>= 0 (<xref ref-type="fig" rid="fig3-0049124112464866">Figure 3</xref>).</p><p>In short, in univariate data, the use of normalizing transformation to impute skewed variables can sometimes yield good results if the transformation is carefully chosen. However, we still cannot recommend transformation since it has disadvantages in bivariate settings, which we will discuss next.</p></sec></sec><sec id="section6-0049124112464866"><title>Bivariate Data</title><p>Bivariate data present new challenges for the imputation of skewed variables. In bivariate data, the imputation model should preserve not just the marginal distribution of the skewed variable; the imputation must also preserve the relationship between the skewed variable and other variables. This can be difficult.</p><p>To set the stage, suppose that we have, again, a standard exponential variable <inline-formula id="inline-formula106-0049124112464866"><mml:math id="mml-inline106-0049124112464866"><mml:mi>X</mml:mi><mml:mo stretchy="false">~</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, but now suppose that we also have a second variable <italic>Y</italic> that fits a linear regression on <italic>X</italic> with normal residuals:<disp-formula id="disp-formula6-0049124112464866"><label>4</label><mml:math id="mml-disp6-0049124112464866"><mml:mi>Y</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">where</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math><graphic alternate-form-of="disp-formula6-0049124112464866" xlink:href="10.1177_0049124112464866-eq6.tif"/></disp-formula><xref ref-type="fig" rid="fig4-0049124112464866">Figure 4</xref> displays <italic>n </italic>= 500 simulated observations on (<italic>X</italic>, <italic>Y</italic>) with <inline-formula id="inline-formula107-0049124112464866"><mml:math id="mml-inline107-0049124112464866"><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and <inline-formula id="inline-formula108-0049124112464866"><mml:math id="mml-inline108-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>.7</mml:mn></mml:math></inline-formula>. The figure shows a scatterplot for <italic>Y</italic> regressed on <italic>X</italic> with an ordinary least squares (OLS) line (left) and a scatterplot for <italic>X</italic> regressed on <italic>Y</italic> (right) with an OLS line and an OLS quadratic.</p><fig id="fig4-0049124112464866" position="float"><label>Figure 4.</label><caption><p>X is standard exponential and <italic>Y</italic> is conditionally normal. The left panel shows a scatterplot for the regression of <italic>Y </italic>on <italic>X</italic>, with an OLS line. The right panel shows a scatterplot for the regression of <italic>X</italic> on <italic>Y</italic>, with an OLS line and an OLS quadratic. </p></caption><graphic xlink:href="10.1177_0049124112464866-fig4.tif"/></fig><p>In evaluating imputation methods, it will be important to realize that the regression of <italic>Y</italic> on <italic>X</italic> satisfies the OLS assumptions, but the regression of <italic>X</italic> on <italic>Y</italic> does not. In particular, the regression of <italic>X</italic> on <italic>Y</italic> violates the OLS assumption that the conditional expectation of the residuals <inline-formula id="inline-formula109-0049124112464866"><mml:math id="mml-inline109-0049124112464866"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is zero for all <italic>Y</italic>. To the contrary, <inline-formula id="inline-formula110-0049124112464866"><mml:math id="mml-inline110-0049124112464866"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">&gt;</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> when <italic>Y</italic> is small and when <italic>Y</italic> is large; in the scatterplot this is evident from the fact that all the <italic>X</italic> values are above the OLS line when <italic>Y</italic> is small or large. The residuals are nonnormal as well, but this is less important since residual normality is not required for OLS estimates to be consistent (<xref ref-type="bibr" rid="bibr45-0049124112464866">Wooldridge 2001</xref>).</p><p>In fact, the regression of <italic>X</italic> on <italic>Y</italic> is not even linear. When <italic>X</italic> is nonnormal, a linear regression of <italic>Y</italic> on <italic>X</italic> does not necessarily imply a linear regression of <italic>X</italic> on <italic>Y</italic>. As <xref ref-type="fig" rid="fig4-0049124112464866">Figure 4</xref> show, the fit of the regression of <italic>X</italic> on <italic>Y</italic> can be improved, though not perfected, by adding a quadratic term <inline-formula id="inline-formula111-0049124112464866"><mml:math id="mml-inline111-0049124112464866"><mml:msup><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> to the regression. With the quadratic term added, the regression comes closer to satisfying the OLS assumption that <inline-formula id="inline-formula112-0049124112464866"><mml:math id="mml-inline112-0049124112464866"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mo mathvariant="normal" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p><p>Although the OLS estimates are not perfect fits for the regression of <italic>X</italic> on <italic>Y</italic>, over most of the range of <italic>X</italic> the linear fit is a decent approximation, and the quadratic fit is very good. We will find that that OLS estimates, though approximate, can be often be serviceable when we impute <italic>X</italic> conditionally on <italic>Y</italic>.</p><p>Having looked at complete data, we now suppose that some values are MAR. In the bivariate setting, the MAR assumption means the probability that a value is missing depends only on values that are observed. That is, the probability that <italic>Y</italic> is missing cannot depend on <italic>Y</italic>, but can depend on <italic>X</italic> in cases where <italic>X</italic> is observed. Likewise, the probability that <italic>X</italic> is missing can depend on observed <italic>Y</italic> values in cases where <italic>Y</italic> is observed (<xref ref-type="bibr" rid="bibr34-0049124112464866">Rubin 1976</xref>; <xref ref-type="bibr" rid="bibr20-0049124112464866">Heitjan and Basu 1996</xref>).</p><p>We can impute missing values using several different methods.</p><sec id="section7-0049124112464866"><title>Linear Regression Imputation</title><p>The simplest parametric imputation model assumes that the missing values fit a linear regression with normal residuals. We call this <italic>linear regression imputation</italic>.</p><p>To understand the technique, it is helpful to start with a situation where the assumptions of the imputation model are met. Imagine that <italic>X</italic> is complete and <italic>Y</italic> is MAR. Then the missing values fit the linear regression in <xref ref-type="disp-formula" rid="disp-formula6-0049124112464866">equation (4)</xref>, and the MAR assumption means that the probability <italic>Y</italic> is missing is independent of <inline-formula id="inline-formula113-0049124112464866"><mml:math id="mml-inline113-0049124112464866"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. In other words, the distribution of <inline-formula id="inline-formula114-0049124112464866"><mml:math id="mml-inline114-0049124112464866"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the same in the cases with <italic>Y</italic> observed as in the cases with <italic>Y</italic> missing, and therefore we can obtain consistent estimates of the regression parameters <inline-formula id="inline-formula115-0049124112464866"><mml:math id="mml-inline115-0049124112464866"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow/></mml:mover></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mtext>obs</mml:mtext><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mover accent="true"><mml:mi>β</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow/></mml:mover></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mtext>obs</mml:mtext><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow/></mml:mover></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mtext>obs</mml:mtext><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mrow/></mml:msubsup></mml:mrow></mml:math></inline-formula> by simply regressing the <italic>Y</italic> on <italic>X</italic> in the cases with <italic>Y</italic> observed (<xref ref-type="bibr" rid="bibr4-0049124112464866">Anderson 1957</xref>; <xref ref-type="bibr" rid="bibr28-0049124112464866">Little 1992</xref>; <xref ref-type="bibr" rid="bibr41-0049124112464866">von Hippel 2007</xref>). Then to impute missing <italic>Y</italic> values, we can simulate draws from the regression model in <xref ref-type="disp-formula" rid="disp-formula6-0049124112464866">equation (4)</xref>, with the observed <italic>Y</italic> estimates substituted for the parameters:<disp-formula id="disp-formula7-0049124112464866"><label>5</label><mml:math id="mml-disp7-0049124112464866"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mo>α</mml:mo><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mo>β</mml:mo><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>Y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext>where </mml:mtext><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula7-0049124112464866" xlink:href="10.1177_0049124112464866-eq7.tif"/></disp-formula>An important detail is what observed <italic>Y</italic> estimates should be used to run the imputation model. The simplest choice is the OLS estimates: <inline-formula id="inline-formula116-0049124112464866"><mml:math id="mml-inline116-0049124112464866"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">α</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">β</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. And the estimates that are usually used in imputation models are obtained by adding random variation to the OLS estimates to obtain Bayesian posterior draw (PD) estimates:<disp-formula id="disp-formula8-0049124112464866"><mml:math id="mml-disp8-0049124112464866"><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mtext>obs</mml:mtext><mml:mo>|</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mtext>OLS</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mtext>obs</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>U</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext>where </mml:mtext><mml:mi>U</mml:mi><mml:mo>~</mml:mo><mml:msubsup><mml:mi>χ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mtext>obs</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mtext>prior</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math><graphic alternate-form-of="disp-formula8-0049124112464866" xlink:href="10.1177_0049124112464866-eq8.tif"/></disp-formula><disp-formula id="disp-formula9-0049124112464866"><label>6</label><mml:math id="mml-disp9-0049124112464866"><mml:mfenced close="]" open="["><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">α</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">β</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">α</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">β</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">α</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">β</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo stretchy="false">,</mml:mo></mml:math><graphic alternate-form-of="disp-formula9-0049124112464866" xlink:href="10.1177_0049124112464866-eq9.tif"/></disp-formula>where <inline-formula id="inline-formula117-0049124112464866"><mml:math id="mml-inline117-0049124112464866"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the number of cases with <italic>Y</italic> observed, <inline-formula id="inline-formula118-0049124112464866"><mml:math id="mml-inline118-0049124112464866"><mml:msub><mml:mi mathvariant="italic">ν</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is the prior degrees of freedom and <inline-formula id="inline-formula119-0049124112464866"><mml:math id="mml-inline119-0049124112464866"><mml:mrow><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">α</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">β</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> is the usual covariance matrix of the OLS estimates, with the estimate <inline-formula id="inline-formula120-0049124112464866"><mml:math id="mml-inline120-0049124112464866"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> substituted for the parameter <inline-formula id="inline-formula121-0049124112464866"><mml:math id="mml-inline121-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> (<xref ref-type="bibr" rid="bibr29-0049124112464866">Little and Rubin 1989</xref>; <xref ref-type="bibr" rid="bibr26-0049124112464866">Kim 2004</xref>).</p><p>Like the OLS estimates, the PD estimates are consistent,<sup><xref ref-type="fn" rid="fn4-0049124112464866">4</xref></sup> and if <italic>Y</italic> is regressed on <italic>X</italic> in the imputed data, the resulting regression estimates are consistent as well, although <inline-formula id="inline-formula122-0049124112464866"><mml:math id="mml-inline122-0049124112464866"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> can be biased in small samples (<xref ref-type="bibr" rid="bibr26-0049124112464866">Kim 2004</xref>).</p><p>In sum, if <italic>Y</italic> is the incomplete variable, linear regression imputation yields consistent estimates.</p><p>If <italic>X</italic> is the incomplete variable, however, linear regression imputation can be inconsistent. With <italic>X</italic> incomplete, linear regression imputation assumes that <italic>X</italic> fits the following equation:<disp-formula id="disp-formula10-0049124112464866"><label>7</label><mml:math id="mml-disp10-0049124112464866"><mml:mi>X</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mi>Y</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math><graphic alternate-form-of="disp-formula10-0049124112464866" xlink:href="10.1177_0049124112464866-eq10.tif"/></disp-formula>But in discussing <xref ref-type="fig" rid="fig4-0049124112464866">Figure 4</xref> we saw that this equation is incorrect; instead, <inline-formula id="inline-formula123-0049124112464866"><mml:math id="mml-inline123-0049124112464866"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is nonnormal, the conditional expectation of <inline-formula id="inline-formula124-0049124112464866"><mml:math id="mml-inline124-0049124112464866"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is positive for small and large values of <italic>Y</italic>, and the conditional expectation of <inline-formula id="inline-formula125-0049124112464866"><mml:math id="mml-inline125-0049124112464866"><mml:mi>X</mml:mi></mml:math></inline-formula> is not a linear function of <italic>Y</italic>. Therefore, the parameters <inline-formula id="inline-formula126-0049124112464866"><mml:math id="mml-inline126-0049124112464866"><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="inline-formula127-0049124112464866"><mml:math id="mml-inline127-0049124112464866"><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, to the degree that they are meaningful, cannot be estimated consistently using OLS. And PD estimates that are based on the OLS estimates will be inconsistent as well.</p><p>Despite its inconsistency for incomplete <italic>X</italic>, linear regression imputation is a convenient approximation, and we will find out later that its biases are fairly small.</p></sec><sec id="section8-0049124112464866"><title>Quadratic Regression Imputation</title><p><italic>Quadratic regression</italic> <italic>imputation</italic> is an attempt to improve on linear regression imputation by adding a squared term <inline-formula id="inline-formula128-0049124112464866"><mml:math id="mml-inline128-0049124112464866"><mml:msup><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> to the regression equation for <italic>X</italic>. As we saw in <xref ref-type="fig" rid="fig4-0049124112464866">Figure 4</xref>, a quadratic term can improve the fit of the regression. We should recognize, however, that the quadratic model is not a perfect specification and will not fit all data as well as it fits the data in <xref ref-type="fig" rid="fig4-0049124112464866">Figure 4</xref>. A misspecified quadratic model is vulnerable to extrapolation error. That is, the curve that best fits the observed <italic>X</italic> values may not be the best fit for the missing <italic>X</italic> values if the observed and missing <italic>X</italic> values are in different parts of the distribution.</p></sec><sec id="section9-0049124112464866"><title>Imputing Within Bounds: Censoring, Truncation, and Truncated Regression</title><p>As in the univariate case, in the bivariate case, we can increase the skew of a normally imputed <italic>X</italic> by keeping the imputed values within bounds.</p><p>The simplest way to bound imputed values is <italic>censoring</italic>. We impute <italic>X</italic> values under a linear regression imputation model, and then round (censor) out-of-bounds <italic>X</italic> values up to the boundary value <italic>c</italic>. In our running example, where we impute an exponential variable as though it were conditionally normal, we would round negative imputed <italic>X</italic> values up to zero, which is the lower bound of the exponential distribution.</p><p>An alternative approach is <italic>truncation</italic>. Under a simple truncation algorithm, we impute <italic>X</italic> values under a linear regression imputation model, then reject any out-of-bounds (e.g., negative) <italic>X</italic> values and reimpute them until an in-bounds (e.g., positive) <italic>X</italic> value randomly occurs. This is the approach used by the MI procedure in SAS 9.2, which will terminate if the wait for an in-bounds value is too long. A faster approach is to impute from a truncated distribution using sampling importance resampling (<xref ref-type="bibr" rid="bibr33-0049124112464866">Robert 1995</xref>; <xref ref-type="bibr" rid="bibr31-0049124112464866">Raghunathan et al. 2001</xref>).</p><p>As in the univariate setting, in bivariate data censoring and truncation can improve the shape of the imputed distribution, but censoring and truncation also bias the mean and variance of <italic>X</italic>. And in the bivariate setting, censoring and truncation do not solve the problem that <italic>X</italic> has been imputed from a misspecified regression model whose parameters are inconsistently estimated. So bounding linear regression imputations has several sources of bias: bias from bounding imputed values, bias from a misspecified regression model, and bias from inconsistent parameter estimates. If we are lucky, these biases will offset each other; if we are unlucky, they will reinforce one another.</p><p>A more sophisticated approach is to impute values with a <italic>truncated regression model</italic> (<xref ref-type="bibr" rid="bibr13-0049124112464866">Goldberger 1981</xref>), which is implemented by IVEware’s BOUNDS statement and by Stata’s <italic>mi impute truncreg</italic> command (Raghunathan, Solenberger, and Van Hoewyk 2002; <xref ref-type="bibr" rid="bibr40-0049124112464866">Stata Corp. 2011</xref>). Under truncated regression imputation, we do not just truncate the imputed <italic>X</italic> values. We also estimate the regression parameters under the assumption that the observed <italic>X</italic> values come from a conditionally normal distribution that has been truncated in the same way. This is akin to the univariate approach that we discussed earlier, where the effect of truncation was anticipated in estimating the model parameters.</p><p>Like the normal regression model, the truncated regression model makes an assumption about the conditional distribution of <italic>X</italic>, and the assumption is not correct for many skewed variables. In <xref ref-type="fig" rid="fig4-0049124112464866">Figure 4</xref>, for example, <italic>X</italic> follows an exponential distribution, not a truncated conditional normal distribution. The truncated regression model is only an approximation, and the approximation is not necessarily better than the approximation offered by a model without truncation. In fact, our discussion of univariate data showed that, when the assumptions of a truncated model are violated, the model parameters can be infinite and estimates can be very sensitive. Later, a simulation will show that similar problems can occur in bivariate data.</p></sec><sec id="section10-0049124112464866"><title>Transformation</title><p>As in the univariate setting, in the bivariate setting it is commonly recommended that skewed incomplete variables be transformed to approximate normality before imputation (<xref ref-type="bibr" rid="bibr39-0049124112464866">Schafer and Olsen 1998</xref>:550; <xref ref-type="bibr" rid="bibr31-0049124112464866">Raghunathan et al. 2001</xref>:82-83; <xref ref-type="bibr" rid="bibr2-0049124112464866">Allison 2002</xref>:39; <xref ref-type="bibr" rid="bibr38-0049124112464866">Schafer and Graham 2002</xref>:167). But transformation of incomplete variables is often unnecessary and can lead to bias.</p><p>Again, it is helpful to return to a simple example where <italic>X</italic> is standard exponential and <italic>Y</italic> is conditionally normal (see <xref ref-type="disp-formula" rid="disp-formula6-0049124112464866">equation (4)</xref>). What should we do if <italic>Y</italic> is MAR? <italic>Y</italic> has some skew which it inherits from <italic>X</italic>, so we might be tempted to transform <italic>Y</italic> to a better approximation of normality. Yet transformation could only hurt the imputations, since the optimal imputation model is a linear regression that imputes <italic>Y</italic> on its original scale. Linear regression imputation is appropriate for <italic>Y</italic> because, although the marginal distribution of <italic>Y</italic> is skewed, the <italic>X</italic>−<italic>Y</italic> relationship is linear and the conditional distribution of <italic>Y</italic> is normal—that is, the residual <inline-formula id="inline-formula129-0049124112464866"><mml:math id="mml-inline129-0049124112464866"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is normal in the regression of <italic>Y</italic> on <italic>X</italic> (see <xref ref-type="disp-formula" rid="disp-formula6-0049124112464866">equation (4)</xref>). Transformation can only hurt the imputations by introducing nonlinearity and residual nonnormality.</p><p>So transformation can only hurt the imputation of <italic>Y</italic>. Transformation can also hurt the imputation of <italic>X</italic>, because transformation introduces curvature into the relationship between <italic>X</italic> and <italic>Y</italic>. In <xref ref-type="fig" rid="fig4-0049124112464866">Figure 4</xref>, for example, we have an exponentially distributed <italic>X</italic>, which, as we saw earlier, can be transformed to a very good approximation of normality by a fourth root transformation <inline-formula id="inline-formula130-0049124112464866"><mml:math id="mml-inline130-0049124112464866"><mml:mi>t</mml:mi><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mroot><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> (<xref ref-type="bibr" rid="bibr16-0049124112464866">Hawkins and Wixley 1986</xref>). Yet, if we use this transformation in a bivariate setting, the imputation model will be misspecified and incompatible with the analysis model. The analysis model is a linear regression that correctly assumes a linear relationship between <italic>Y</italic> and <italic>X</italic>, yet under transformation the imputation model assumes a linear relationship between <italic>Y</italic> and <inline-formula id="inline-formula131-0049124112464866"><mml:math id="mml-inline131-0049124112464866"><mml:mroot><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula>.</p><p>As an alternative to transforming<italic> X</italic> alone, we can transform <italic>all</italic> the variables in the imputation model. For example, if <italic>X</italic> is exponential, we can transform <italic>X</italic> to <inline-formula id="inline-formula132-0049124112464866"><mml:math id="mml-inline132-0049124112464866"><mml:mroot><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> and <italic>Y</italic> to <inline-formula id="inline-formula133-0049124112464866"><mml:math id="mml-inline133-0049124112464866"><mml:mroot><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> where <inline-formula id="inline-formula134-0049124112464866"><mml:math id="mml-inline134-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the smallest observed <italic>Y</italic> value, subtracted to ensure that the value under the radical is zero or more. Here the purpose of transforming <italic>Y</italic> is not to normalize <italic>Y</italic>, but to put the complete variable <italic>Y</italic> on a scale that is more nearly linear with respect to the transformed incomplete variable <italic>X</italic>. If both <italic>X</italic> and <italic>Y</italic> were incomplete, the choice of transformation would be less straightforward since different transformations would likely be optimal for each variable. And if there were more than two incomplete variables, choosing a suitable transformation could become unwieldy.</p></sec></sec><sec id="section11-0049124112464866"><title>Simulation Experiment</title><p>In this section, we carry out a simulation experiment to test the bias and efficiency of different methods for imputing a skewed <italic>X</italic> variable in a bivariate (<italic>X</italic>, <italic>Y</italic>) data set. We then extend the simulation to accommodate a third variable <italic>Z</italic>.</p><sec id="section12-0049124112464866"><title>Bivariate Design</title><p>In each simulated bivariate data set, <italic>Y</italic> fits a normal linear regression on <italic>X</italic>:<disp-formula id="disp-formula11-0049124112464866"><label>8</label><mml:math id="mml-disp11-0049124112464866"><mml:mi>Y</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math><graphic alternate-form-of="disp-formula11-0049124112464866" xlink:href="10.1177_0049124112464866-eq11.tif"/></disp-formula>We hold the slope and intercept constant at <inline-formula id="inline-formula135-0049124112464866"><mml:math id="mml-inline135-0049124112464866"><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> since changing these values to any nonzero value would be equivalent to a trivial shifting or rescaling of the axes. We also hold the sample size constant at <italic>n </italic>= 100 observations, so that we can avoid being distracted by small sample biases<sup><xref ref-type="fn" rid="fn5-0049124112464866">5</xref></sup> and focus on the asymptotic properties of the imputation methods.</p><p>The experiment independently manipulates four factors:<list list-type="order"><list-item><p>The first factor is the <italic>distribution</italic> of<italic> X</italic>. We let <inline-formula id="inline-formula136-0049124112464866"><mml:math id="mml-inline136-0049124112464866"><mml:mi>X</mml:mi><mml:mo stretchy="false">~</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">χ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> be a chi-square variable where the degree of freedom <inline-formula id="inline-formula137-0049124112464866"><mml:math id="mml-inline137-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> determines the mean <inline-formula id="inline-formula138-0049124112464866"><mml:math id="mml-inline138-0049124112464866"><mml:mi>E</mml:mi><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, the variance <inline-formula id="inline-formula139-0049124112464866"><mml:math id="mml-inline139-0049124112464866"><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, and the coefficient of skewness <inline-formula id="inline-formula140-0049124112464866"><mml:math id="mml-inline140-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">w</mml:mi></mml:mrow></mml:mrow><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced><mml:mo stretchy="false">=</mml:mo><mml:msqrt><mml:mn>8</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:msqrt></mml:math></inline-formula>. We set the degrees of freedom to four different levels <inline-formula id="inline-formula141-0049124112464866"><mml:math id="mml-inline141-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:mn>8</mml:mn></mml:math></inline-formula> so that the skew ranges from mild (1) to severe (<inline-formula id="inline-formula142-0049124112464866"><mml:math id="mml-inline142-0049124112464866"><mml:msqrt><mml:mn>8</mml:mn></mml:msqrt><mml:mo stretchy="false">≈</mml:mo><mml:mn>2.8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. Note that with <inline-formula id="inline-formula143-0049124112464866"><mml:math id="mml-inline143-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> the chi-square distribution reduces to the standard exponential distribution that we have used in all our examples up to this point.</p></list-item><list-item><p>The second factor is the strength of the <italic>X</italic>–<italic>Y</italic> relationship as measured by the regression’s <italic>coefficient of determination</italic> <inline-formula id="inline-formula144-0049124112464866"><mml:math id="mml-inline144-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. We set the coefficient of determination to four different levels <inline-formula id="inline-formula145-0049124112464866"><mml:math id="mml-inline145-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>.1</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:mn>.3</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:mn>.5</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:mn>.7</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:mn>.9</mml:mn></mml:math></inline-formula> by first choosing the desired value of <inline-formula id="inline-formula146-0049124112464866"><mml:math id="mml-inline146-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and then setting <inline-formula id="inline-formula147-0049124112464866"><mml:math id="mml-inline147-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:msubsup><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="normal">V</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>.</p></list-item><list-item><p>The third factor is the <italic>location</italic> of the missing<italic> X</italic> values. From each complete data set, we delete half the <italic>X</italic> values in three MAR patterns:<list list-type="alpha-lower"><list-item><p>To let values be missing completely at random (MCAR), we delete <italic>X</italic> values with a constant probability of ½.</p></list-item><list-item><p>To favor deletion of large <italic>X</italic> values—that is, values in the <italic>tail</italic> of the <italic>X</italic> distribution—we delete<italic> X</italic> values with probability <inline-formula id="inline-formula148-0049124112464866"><mml:math id="mml-inline148-0049124112464866"><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> where <inline-formula id="inline-formula149-0049124112464866"><mml:math id="mml-inline149-0049124112464866"><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the empirical cumulative distribution of<italic> Y.</italic></p></list-item><list-item><p>To favor deletion of small <italic>X</italic> values—that is, values near the <italic>peak</italic> of the <italic>X</italic> distribution—we delete<italic> X</italic> values with probability <inline-formula id="inline-formula150-0049124112464866"><mml:math id="mml-inline150-0049124112464866"><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>.</p></list-item></list>Note that we do not vary the <italic>fraction</italic> of missing <italic>X</italic> values, since it is clear that changing the fraction of missing values would not change the relative performance of the imputation methods. Deleting a larger or smaller fraction of values would just make the differences between the imputation methods more or less consequential.</p></list-item><list-item><p>The fourth factor is the <italic>method of imputation</italic>. We use the seven different methods described earlier:<list list-type="alpha-lower"><list-item><p>Linear regression imputation where <italic>X</italic> is regressed on <italic>Y</italic>.</p></list-item><list-item><p>Linear regression imputation with imputed <italic>X</italic> values censored below <inline-formula id="inline-formula151-0049124112464866"><mml:math id="mml-inline151-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p></list-item><list-item><p>Linear regression imputation with imputed <italic>X</italic> values truncated below <inline-formula id="inline-formula152-0049124112464866"><mml:math id="mml-inline152-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p></list-item><list-item><p>Quadratic regression imputation, where <italic>X </italic>is regressed on <italic>Y </italic>and <inline-formula id="inline-formula153-0049124112464866"><mml:math id="mml-inline153-0049124112464866"><mml:msup><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>.</p></list-item><list-item><p>The transform X method: linear regression imputation where <inline-formula id="inline-formula154-0049124112464866"><mml:math id="mml-inline154-0049124112464866"><mml:mroot><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> is regressed on Y. For the chi-square variable <inline-formula id="inline-formula155-0049124112464866"><mml:math id="mml-inline155-0049124112464866"><mml:mi>X</mml:mi></mml:math></inline-formula>, <inline-formula id="inline-formula156-0049124112464866"><mml:math id="mml-inline156-0049124112464866"><mml:mroot><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> is very close to normal for any degrees of freedom <inline-formula id="inline-formula157-0049124112464866"><mml:math id="mml-inline157-0049124112464866"><mml:mi mathvariant="italic">ν</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bibr16-0049124112464866">Hawkins and Wixley 1986</xref>).</p></list-item><list-item><p>The transform all method: linear regression imputation where <inline-formula id="inline-formula158-0049124112464866"><mml:math id="mml-inline158-0049124112464866"><mml:mroot><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> is regressed on <inline-formula id="inline-formula159-0049124112464866"><mml:math id="mml-inline159-0049124112464866"><mml:mroot><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula>. Here <inline-formula id="inline-formula160-0049124112464866"><mml:math id="mml-inline160-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> is the smallest <italic>Y</italic> value in each simulated data set, subtracted to ensure that the value under the radical is nonnegative.</p></list-item><list-item><p>Truncated regression imputation, with the lower truncation point at <inline-formula id="inline-formula161-0049124112464866"><mml:math id="mml-inline161-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>.</p></list-item></list></p></list-item></list>For each imputation method, we imputed each incomplete data set five times. In each of the five imputed data sets, we calculated the mean, standard deviation, and skew of <italic>X</italic>, and we fit an OLS regression of <italic>Y</italic> on <italic>X</italic>. We then averaged estimates from the five imputed data sets to yield a single set of MI estimates for each incomplete data set and imputation method.</p><p>All the imputation methods used the MI procedure in SAS 9.2, except for the truncated regression method, which used the <italic>mi impute truncreg</italic> command in Stata 12. Both SAS and Stata displayed some technical limitations. These limitations are not central to the results, but are worth describing briefly, as follows:</p><list list-type="bullet"><list-item><p>In truncating normal imputations, SAS’s MI procedure uses a simple rejection algorithm,<sup><xref ref-type="fn" rid="fn6-0049124112464866">6</xref></sup> rejecting negative values and reimputing them iteratively until a positive value randomly occurs. The wait for a positive imputation can be long, and in about 8 percent of data sets SAS terminated the attempt after 100 iterations. We reimputed these data sets with a lower truncation point of <inline-formula id="inline-formula162-0049124112464866"><mml:math id="mml-inline162-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">−</mml:mo><mml:mn>6</mml:mn></mml:math></inline-formula>, which means that the truncation was not as severe as we wanted it to be. We could have maintained a truncation point of <inline-formula id="inline-formula163-0049124112464866"><mml:math id="mml-inline163-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> if SAS truncated imputed values by sampling importance resampling, which is used by IVEware (<xref ref-type="bibr" rid="bibr31-0049124112464866">Raghunathan et al. 2001</xref>, <xref ref-type="bibr" rid="bibr32-0049124112464866">2002</xref>). Sampling importance resampling is faster than simple rejection and ensures that imputed values are within bounds (<xref ref-type="bibr" rid="bibr33-0049124112464866">Robert 1995</xref>).</p></list-item><list-item><p>In transforming imputed variable, SAS’s MI procedure<sup><xref ref-type="fn" rid="fn7-0049124112464866">7</xref></sup> refuses to inverse-transform any values that could not have been obtained by transformation. In imputing <italic>X</italic>, for example, we transformed the observed <italic>X</italic> values to <inline-formula id="inline-formula164-0049124112464866"><mml:math id="mml-inline164-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mroot><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula> and then supplemented the observed values with normal imputations <inline-formula id="inline-formula165-0049124112464866"><mml:math id="mml-inline165-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> which included some negative values. When we tried to invert the transformation by calculating <inline-formula id="inline-formula166-0049124112464866"><mml:math id="mml-inline166-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mn>4</mml:mn></mml:msubsup></mml:math></inline-formula>, SAS terminated on encountering negative values of <inline-formula id="inline-formula167-0049124112464866"><mml:math id="mml-inline167-0049124112464866"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> because a fourth root cannot be negative. We got around this problem by transforming and inverse-transforming the variable outside of the MI procedure.</p></list-item><list-item><p>Stata’s implementation of truncated regression imputation is quite slow, taking about twenty-four hours to impute all the experimental data sets (SAS imputed them in minutes). It is not clear whether this is an inherent problem with truncated regression or a problem in Stata’s implementation of it. In addition to being slow, the truncated regression model failed to converge in about 1 percent of incomplete data sets—typically data sets with low <inline-formula id="inline-formula168-0049124112464866"><mml:math id="mml-inline168-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and low <inline-formula id="inline-formula169-0049124112464866"><mml:math id="mml-inline169-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. While again this could be an implementation issue, we suspect that failure to converge resulted from the truncated regression model’s inherently poor fit to a chi-square variable <italic>X</italic> with low degrees of freedom <inline-formula id="inline-formula170-0049124112464866"><mml:math id="mml-inline170-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Recall that, even in the univariate setting, we found that fitting a truncated normal model to a chi-square variable with <inline-formula id="inline-formula171-0049124112464866"><mml:math id="mml-inline171-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula> could lead to infinite parameters and unstable parameter estimates. After failures to converge, we refit the truncated regression successfully by moving the lower truncation point to <inline-formula id="inline-formula172-0049124112464866"><mml:math id="mml-inline172-0049124112464866"><mml:mi>c</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>.</p></list-item></list></sec><sec id="section13-0049124112464866"><title>Illustrative Results</title><p><xref ref-type="fig" rid="fig5-0049124112464866">Figure 5</xref> uses all the imputation methods to impute a simulated data set with <inline-formula id="inline-formula173-0049124112464866"><mml:math id="mml-inline173-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>, <inline-formula id="inline-formula174-0049124112464866"><mml:math id="mml-inline174-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>.3</mml:mn></mml:math></inline-formula>, and <italic>X</italic> values MCAR. The true regression line—that is, the line that fits the complete-data population—is shown for reference.</p><fig id="fig5-0049124112464866" position="float"><label>Figure 5.</label><caption><p>Different imputation methods when the incomplete variable <italic>X</italic> is standard exponential with values missing completely at random, and the complete variable<italic> Y</italic> is conditionally normal. </p></caption><graphic xlink:href="10.1177_0049124112464866-fig5.tif"/></fig><p>Clearly, some methods handle these data better than others. Under linear regression imputation, the imputed values, although more symmetrically distributed than the observed values, nevertheless fit well around the true regression line. Censoring or truncating the linear regression imputations does not change the fit very much; censoring and truncating simply move the leftmost values a little to the right, and this has little influence since the points with the most influence on the regression line are on the far right, in the tail of the <italic>X</italic> distribution. Quadratic regression imputation produces fairly similar results, although the imputed <italic>X</italic> values are a little more dispersed than they are under linear regression imputation.</p><p>The transform <italic>X</italic> method and the transform all method have serious trouble with these data, because transformation curves the relationship between <italic>X</italic> and <italic>Y</italic>. In the tail, which has the most influence on the regression, all the imputed points lie below the true regression line. These points will negatively bias the estimated slope.</p><p>The truncated regression has even worse problems, with nearly all the imputed points in influential positions and below the true regression line. Evidently, the truncated regression model can be a poor fit to observed data that are not actually truncated, and very unrealistic parameter estimates can result. Recall that the truncated model can produce infinite parameter values even in a simple univariate setting.</p><p><xref ref-type="fig" rid="fig6-0049124112464866">Figure 6</xref> gives even more serious examples of bad imputations obtained from the truncated regression model. In one example, the imputed <italic>X</italic> values have a negative relationship to <italic>Y</italic>, even though the relationship in the observed data is clearly positive. Some imputed <italic>X</italic> values have values of 200 or greater, even though all the observed <italic>X</italic> values are less than 11. In one example, nearly all the imputed values are negative, even though the purpose of truncated regression here is to produce positive imputations. The presence of negative imputations, though extremely rare, suggests implementation problems that go beyond the basic difficulty of fitting a truncated model to nontruncated data.</p><fig id="fig6-0049124112464866" position="float"><label>Figure 6.</label><caption><p>Anomalous imputed data sets generated by truncated regression imputation in Stata 12. <italic>Note</italic>: Main effect of imputation method, averaged across other factors.</p></caption><graphic xlink:href="10.1177_0049124112464866-fig6.tif"/></fig></sec><sec id="section14-0049124112464866"><title>Comprehensive Results</title><p>The full bivariate simulation experiment yielded 36,000 sets of MI estimates—100 estimates for each of 360 different experimental conditions (4 <inline-formula id="inline-formula175-0049124112464866"><mml:math id="mml-inline175-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> values × 5 <inline-formula id="inline-formula176-0049124112464866"><mml:math id="mml-inline176-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> values × 3 missing value patterns × 6 imputation methods). Within each experimental condition, we calculated the mean and standard deviation of the 100 MI estimates in order to estimate the expectation and standard error of each imputed-based estimator under each experimental condition. From the expectation and standard error, we calculated the bias and root mean square error (RMSE). Finally, we divided the bias and RMSE by the true value of the estimand in order to get the <italic>relative bias</italic> and <italic>relative RMSE</italic>—that is, the bias and RMSE expressed as a percentage of the true value of the estimand.</p><p><xref ref-type="table" rid="table1-0049124112464866">Table 1</xref> summarizes the relative bias and relative RMSE of each imputation method, averaged across all the experimental conditions. Results are shown for seven different estimands. The first three estimands describe the marginal distribution of <italic>X</italic> in terms of the mean, standard deviation, and coefficient of skewness. The remaining four estimands describe the regression of <italic>Y</italic> on <italic>X</italic> in terms of the intercept, slope, residual standard deviation, and coefficient of determination.</p><table-wrap id="table1-0049124112464866" position="float"><label>Table 1.</label><caption><p>Results of Two-Variable Simulation: Main effect of imputation method.</p></caption><graphic alternate-form-of="table1-0049124112464866" xlink:href="10.1177_0049124112464866-table1.tif"/><table><thead><tr><th> </th><th colspan="6">Marginal Distribution of Incomplete <italic>X</italic></th><th colspan="8">Regression of Complete <italic>Y</italic> on Incomplete <italic>X</italic></th></tr><tr><th> </th><th colspan="2">Mean</th><th colspan="2"><italic>SD</italic></th><th colspan="2">Skew</th><th colspan="2">Slope</th><th colspan="2">Intercept</th><th colspan="2">Residual <italic>SD</italic></th><th colspan="2"><italic>R</italic><sup><xref ref-type="fn" rid="fn2-0049124112464866">2</xref></sup><italic><sub>Y|X</sub></italic></th></tr><tr><th>Imputation method</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th></tr></thead><tbody><tr><td>Linear regression</td><td>−6</td><td>15</td><td>−3</td><td>22</td><td>−52</td><td>57</td><td>1</td><td>26</td><td>15</td><td>95</td><td>0</td><td>12</td><td>3</td><td>37</td></tr><tr><td>Linear regression, censored</td><td>−1</td><td>13</td><td>−8</td><td>20</td><td>−37</td><td>46</td><td>5</td><td>27</td><td>−1</td><td>93</td><td>1</td><td>12</td><td>0</td><td>35</td></tr><tr><td>Linear regression, truncated</td><td>2</td><td>16</td><td>−9</td><td>21</td><td>−38</td><td>46</td><td>4</td><td>28</td><td>−6</td><td>96</td><td>2</td><td>12</td><td>−5</td><td>35</td></tr><tr><td>Quadratic regression</td><td>0</td><td>16</td><td>6</td><td>25</td><td>−31</td><td>57</td><td>−11</td><td>31</td><td>37</td><td>113</td><td>1</td><td>13</td><td>3</td><td>41</td></tr><tr><td>Truncated regression</td><td><sup>404936</sup></td><td><sup>4064963</sup></td><td><sup>417393</sup></td><td><sup>4181515</sup></td><td>−29</td><td>43</td><td>−8</td><td>28</td><td>18</td><td>102</td><td>0</td><td>11</td><td>5</td><td>39</td></tr><tr><td>Transform X</td><td>5</td><td>21</td><td>27</td><td>62</td><td>21</td><td>48</td><td>−17</td><td>30</td><td>47</td><td>106</td><td>14</td><td>21</td><td>−11</td><td>37</td></tr><tr><td>Transform all</td><td>−5</td><td>16</td><td>−1</td><td>30</td><td>−5</td><td>29</td><td>−20</td><td>31</td><td>74</td><td>118</td><td>25</td><td>32</td><td>−31</td><td>50</td></tr></tbody></table><table-wrap-foot><fn id="table-fn1-0049124112464866"><p><italic>Note</italic>: RMSE= root mean square error.</p></fn></table-wrap-foot></table-wrap><p>For all but one of the estimands, linear regression imputation yields estimates whose average relative bias is fairly small, often close to zero. Censoring or truncating the linear regression imputations yields similar results, with a little more bias for some estimands and a little less for others.<sup><xref ref-type="fn" rid="fn8-0049124112464866">8</xref></sup> All the other methods have more serious biases. The transformation methods have the worst biases for the regression slope and intercept, and the truncated regression method has the worst biases for the mean and standard deviation of <italic>X</italic>. The biases of the truncated regression method can be enormous, exceeding 400,000 percent for some parameters, because the method occasionally imputes wild outliers like those in <xref ref-type="fig" rid="fig6-0049124112464866">Figure 6</xref>.</p><p>In short, for the estimands that are emphasized in most social research—means, standard deviations, and regression parameters—linear regression imputation, with or without censoring or truncation, can often produce reasonable results. The other imputation methods are no better for these estimands, and sometimes much worse.</p><p>This is not to say that linear regression imputation is good for every estimand—it is not. In fact, linear regression imputation does a very poor job of estimating parameters that reflect distributional shape. In estimating the coefficient of skewness, for example, linear regression imputation had an average relative bias of –52 percent—that is, the estimated skew was less than half the true skew, on average. With one exception, the other methods do not estimate skew very well either; they are less biased than linear regression imputation but still have positive or negative biases exceeding 20 percent. The one method that estimates skew with little bias is the transform all method, but we cannot recommend that method because it has serious biases in estimating the regression parameters.</p><p>The online Appendix Table A1 (which can be found at http://smr.sagepub.com/supplemental/) summarizes the simulation results in more detail, breaking them down across different levels of the manipulated factors. The most striking result is that the differences of the methods are largest when missing values are in the tail. This makes sense because the tail values have the most influence in estimating the regression line. Another striking result is that the biases of the transformation methods are worst when <italic>X</italic> is highly skewed—which is exactly when one would be most tempted to use transformation.</p><p>The biases of the different methods vary from one simulated condition to another, and there are circumstances where transformation or quadratic regression or truncated regression gives the best results. It is tempting to imagine that you could obtain good results by picking and choosing different methods to suit different circumstances, but that it is a dangerous game with only small benefits for guessing right, and large penalties for guessing wrong.</p><p>The safest approach is to use a method that tends to have small biases in a wide variety of settings, and by that criterion the best choice for bivariate data is linear regression imputation, with or without truncation or censoring.</p></sec><sec id="section15-0049124112464866"><title>Trivariate Extension</title><p>We now extend the simulation to a regression where there are three variables: a complete dependent variable <italic>Z</italic> and two independent variables, one complete (<italic>Y</italic>), and one incomplete (<italic>X</italic>). To do this, we simply keep the <italic>X</italic> and <italic>Y</italic> variables from the two-variable simulation, and add a <italic>Z</italic> variable that fits a linear regression on <italic>X</italic> and <italic>Y</italic>, with normal residuals:<disp-formula id="disp-formula12-0049124112464866"><label>9</label><mml:math id="mml-disp12-0049124112464866"><mml:mi>Z</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mi>Y</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">~</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math><graphic alternate-form-of="disp-formula12-0049124112464866" xlink:href="10.1177_0049124112464866-eq12.tif"/></disp-formula>The manipulated factors are the same as before, with only minor adjustments:</p><list list-type="order"><list-item><p>The first factor is the distribution of <italic>X</italic>. Again <inline-formula id="inline-formula177-0049124112464866"><mml:math id="mml-inline177-0049124112464866"><mml:mi>X</mml:mi><mml:mo stretchy="false">~</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">χ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ν</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is a chi-square variable with degrees of freedom <inline-formula id="inline-formula178-0049124112464866"><mml:math id="mml-inline178-0049124112464866"><mml:mi mathvariant="italic">ν</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>,2,4,8.</p></list-item><list-item><p>The second factor is the squared correlation <inline-formula id="inline-formula179-0049124112464866"><mml:math id="mml-inline179-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">ρ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> between <italic>X</italic> and <italic>Y</italic>. This is the factor that we manipulated in the bivariate simulation, except that in the bivariate simulation <inline-formula id="inline-formula180-0049124112464866"><mml:math id="mml-inline180-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">ρ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> was interpreted as the coefficient of determination <inline-formula id="inline-formula181-0049124112464866"><mml:math id="mml-inline181-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> for the regression of <italic>Y</italic> on <italic>X</italic>.</p></list-item><list-item><p>The third factor is the pattern of missing <italic>X</italic> values. These patterns are defined in exactly the same way as in the bivariate setting. In the MCAR scenario, <italic>X</italic> values are deleted with probability ½; in the MAR tail pattern, <italic>X</italic> values are deleted with probability <inline-formula id="inline-formula182-0049124112464866"><mml:math id="mml-inline182-0049124112464866"><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>; and in the MAR peak pattern, <italic>X</italic> values are deleted with probability <inline-formula id="inline-formula183-0049124112464866"><mml:math id="mml-inline183-0049124112464866"><mml:mn>1</mml:mn><mml:mo stretchy="false">−</mml:mo><mml:mover accent="true"><mml:mi>F</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. Note that <italic>Z</italic> has no effect on whether <italic>X</italic> is missing, and that both <italic>Y</italic> and <italic>Z</italic> are complete.</p></list-item><list-item><p>The fourth factor is the imputation method. The methods are the same as in the bivariate simulation, except that <italic>X</italic> is imputed conditionally on <italic>Z </italic>as well as <italic>Y</italic>. In the transform all method, <italic>X</italic> is transformed to <inline-formula id="inline-formula184-0049124112464866"><mml:math id="mml-inline184-0049124112464866"><mml:mroot><mml:mi>X</mml:mi><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula>, <italic>Y</italic> is transformed to <inline-formula id="inline-formula185-0049124112464866"><mml:math id="mml-inline185-0049124112464866"><mml:mroot><mml:mrow><mml:mi>Y</mml:mi><mml:mo stretchy="false">−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula>, and <italic>Z</italic> is transformed to <inline-formula id="inline-formula186-0049124112464866"><mml:math id="mml-inline186-0049124112464866"><mml:mroot><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">−</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mroot></mml:math></inline-formula>, where Min(<italic>Y</italic>) and Min(<italic>Z</italic>) are the minimum <italic>Y</italic> and <italic>Z</italic> values in the imputed data set, subtracted to ensure that the value under the radical is nonnegative.</p></list-item></list><p>To avoid further complicating the experiment, we hold constant certain parameters of the regression of<italic> Z </italic>on <italic>X</italic> and <italic>Y</italic>. The constant parameters are the slopes and intercept, which are held constant at <inline-formula id="inline-formula187-0049124112464866"><mml:math id="mml-inline187-0049124112464866"><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>, and the coefficient of determination, which is held constant at <inline-formula id="inline-formula188-0049124112464866"><mml:math id="mml-inline188-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>.5</mml:mn></mml:math></inline-formula> by setting <inline-formula id="inline-formula189-0049124112464866"><mml:math id="mml-inline189-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mi>V</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mi>X</mml:mi><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mi>Y</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>.</p><p><xref ref-type="table" rid="table2-0049124112464866">Table 2</xref> summarizes the relative bias and relative RMSE of each imputation method for estimands relating to the marginal distribution of <italic>X</italic> and the regression of <italic>Z</italic> on <italic>X</italic> and <italic>Y</italic>. <xref ref-type="table" rid="table2-0049124112464866">Table 2</xref> presents average results; in the online appendix (which can be found at http://smr.sagepub.com/supplemental/) Table A2 breaks the results down by each manipulated factor.</p><table-wrap id="table2-0049124112464866" position="float"><label>Table 2.</label><caption><p>Results of Three-Variable Simulation: Main effect of imputation method.</p></caption><graphic alternate-form-of="table2-0049124112464866" xlink:href="10.1177_0049124112464866-table2.tif"/><table><thead><tr><th> </th><th colspan="6">Marginal distribution of incomplete X</th><th colspan="10">Regression of complete Z on incomplete X and complete Y</th></tr><tr><th> </th><th colspan="2">Mean</th><th colspan="2">SD</th><th colspan="2">Skew</th><th colspan="2">Slope of X</th><th colspan="2">Slope of Y</th><th colspan="2">Intercept</th><th colspan="2">Residual SD</th><th colspan="2">R<sup><xref ref-type="fn" rid="fn2-0049124112464866">2</xref></sup><sub>Y|X</sub></th></tr><tr><th>Imputation method</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th><th>Rel. bias (%)</th><th>Rel. RMSE (%)</th></tr></thead><tbody><tr><td>Linear regression</td><td>−5</td><td>14</td><td>−2</td><td>22</td><td>−51</td><td>57</td><td>−2</td><td>65</td><td>4</td><td>42</td><td>−1</td><td>157</td><td>−1</td><td>8</td><td>2</td><td>17</td></tr><tr><td>Linear regression, censored</td><td>−1</td><td>13</td><td>−7</td><td>19</td><td>−37</td><td>45</td><td>2</td><td>66</td><td>4</td><td>40</td><td>−18</td><td>155</td><td>−1</td><td>8</td><td>2</td><td>17</td></tr><tr><td>Linear regression, truncated</td><td>2</td><td>15</td><td>−8</td><td>20</td><td>−37</td><td>46</td><td>−1</td><td>65</td><td>6</td><td>41</td><td>−25</td><td>161</td><td>0</td><td>8</td><td>1</td><td>17</td></tr><tr><td>Quadratic regression</td><td>0</td><td>15</td><td>7</td><td>25</td><td>−29</td><td>57</td><td>−13</td><td>64</td><td>7</td><td>41</td><td>9</td><td>155</td><td>−1</td><td>8</td><td>2</td><td>17</td></tr><tr><td>Truncated regression</td><td align="center"><sup>78933932</sup></td><td><sup>607613905</sup></td><td><sup>71427514</sup></td><td><sup>546826216</sup></td><td>−27</td><td>42</td><td>−6</td><td>64</td><td>2</td><td>40</td><td>−4</td><td>157</td><td>−1</td><td>8</td><td>2</td><td>17</td></tr><tr><td>Transform X</td><td>6</td><td>20</td><td>30</td><td>63</td><td>24</td><td>51</td><td>−27</td><td>57</td><td>18</td><td>37</td><td>3</td><td>142</td><td>0</td><td>7</td><td>0</td><td>17</td></tr><tr><td>Transform all</td><td>−4</td><td>16</td><td>0</td><td>32</td><td>−5</td><td>30</td><td>−31</td><td>60</td><td>22</td><td>41</td><td>11</td><td>146</td><td>1</td><td>8</td><td>−1</td><td>17</td></tr></tbody></table><table-wrap-foot><fn id="table-fn3-0049124112464866"><p><italic>Note</italic>: RMSE= root mean square error.</p></fn></table-wrap-foot></table-wrap><p>The basic results of the three-variable simulation are similar to those of the two-variable simulation.</p><p>In estimating the regression parameters, and in estimating the mean and standard deviation of <italic>X</italic>, linear regression imputation gives perhaps the best results overall, with relative biases of 5 percent or less. Quadratic regression imputation gives similar results, as does censoring or truncating the linear regression imputations. The transformation methods give more biased regression parameters, and the truncated regression method occasionally imputes very large outliers and so gives seriously biased estimates of the mean and standard deviation.</p><p>In estimating the skew, all the methods give highly biased results except for the transform all method, which cannot be recommended since it gives the most biased estimates for the regression parameters. Again, the differences among the imputation methods are much more consequential if values are imputed in the tail rather than the peak of the <italic>X</italic> distribution (see Table A 2b, which can be found in the online Appendix at http://smr.sagepub.com/supplemental/).</p><p>In the three-variable simulation, the slope of <italic>Z</italic> on <italic>Y</italic> has bias, which is opposite in direction to the bias of the slope of <italic>Z</italic> on <italic>X</italic>. Bias in the slope of <italic>Z</italic> on <italic>Y</italic> occurs despite the fact that neither <italic>Z</italic> nor <italic>Y</italic> has any imputed values. The reason for this bias is that <italic>Y</italic> is correlated with <italic>X</italic>, so any bias in the slope of <italic>X</italic> engenders a compensating bias in the slope of <italic>Y</italic>. Table A2b in the online appendix (which can be found at http://smr.sagepub.com/supplemental/) confirms that the bias in the slope of <italic>Y</italic> is greatest when the squared correlation <inline-formula id="inline-formula190-0049124112464866"><mml:math id="mml-inline190-0049124112464866"><mml:msubsup><mml:mi mathvariant="italic">ρ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> between <italic>X</italic> and <italic>Y</italic> is large. Conceptually, the bias is similar to the bias that would occur if <italic>X</italic> were omitted or measured poorly.</p><p>An initially surprising result of the three-variable simulation is that the regression slopes estimated by the transformation methods, despite having the most bias, sometimes have smaller RMSE than the regression slopes estimated by other methods. The presumed reason for this is that the transformation methods, because they use misspecified imputation models, reduce the correlation between <italic>Y</italic> and the imputed <italic>X</italic>, and this reduced correlation yields smaller standard errors when <italic>X</italic> and <italic>Y</italic> are used to predict <italic>Z</italic>. Under some circumstances, the reduction in standard error can more than make up for the increase in bias.</p><p>Is the potential for reduction in RMSE a reason to use the transformation methods? No. First, reduced RMSE in the regression slope is limited to the simulations where the correlation between <italic>X</italic> and <italic>Y</italic> is very, very high. Table A 2b in the online appendix (which can be found at http://smr.sagepub.com/supplemental/) shows reduced RMSE in the simulation where <inline-formula id="inline-formula191-0049124112464866"><mml:math id="mml-inline191-0049124112464866"><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ρ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>.9</mml:mn></mml:math></inline-formula> (i.e., <inline-formula id="inline-formula192-0049124112464866"><mml:math id="mml-inline192-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ρ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">≈</mml:mo><mml:mn>.95</mml:mn></mml:math></inline-formula>) but no reduction in RMSE if <inline-formula id="inline-formula193-0049124112464866"><mml:math id="mml-inline193-0049124112464866"><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ρ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>.7</mml:mn></mml:math></inline-formula> (i.e., <inline-formula id="inline-formula194-0049124112464866"><mml:math id="mml-inline194-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">ρ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">≈</mml:mo><mml:mn>.84</mml:mn></mml:math></inline-formula>). Second, because RMSE was reduced by accident, we have no theory for exactly when a similar reduction in RMSE might be expected outside the simulation. Analysts who use the transformation methods are risking severe bias, with only a speculative hope of reduced RMSE. Analysts who are willing to trade some bias for a reduction in RMSE would be better advised to use a more principled method with stronger theoretical foundations, such as ridge regression (<xref ref-type="bibr" rid="bibr21-0049124112464866">Hoerl and Kennard 1970</xref>; <xref ref-type="bibr" rid="bibr30-0049124112464866">Muniz and Kibria 2009</xref>).</p></sec></sec><sec id="section16-0049124112464866"><title>Applied Examples</title><p>Authors often use incomplete skewed variables in applied research. In this section, we discuss two examples and evaluate the decisions that were made by the authors.</p><sec id="section17-0049124112464866"><title>Analysis of Female Legislative Candidates</title><p>In a cross-national study, <xref ref-type="bibr" rid="bibr27-0049124112464866">Kunovich and Paxton (2005</xref>) pointed out a strong bivariate relationship between two percentages that vary across the world’s <italic>n </italic>= 171 countries: the percentage of parliamentary <italic>legislators</italic> who are female (<italic>Y</italic>) and the percentage of parliamentary <italic>candidates</italic> who were female (<italic>X</italic>). The regression of <italic>Y</italic> on <italic>X</italic> is important because a slope of less than one suggests that female candidates lose more often than they win. <italic>Y</italic> was complete and skewed to the right, with a coefficient of skewness of 1.4. The skew of <italic>X</italic> is similar but harder to estimate from the observed data, since <italic>X</italic> was missing for more than half (ninety-nine) of the countries.</p><p>Following my advice, the authors imputed missing <italic>X</italic> values using the transform <italic>X</italic> method. It now appears, in light of results in this article, that my advice was misguided and could have caused substantial bias if missing <italic>X</italic> values had been primarily in the tail. Fortunately, <italic>X</italic> was missing primarily in the peak, and in this situation—with a skew of 1.4, values missing from the peak, and <inline-formula id="inline-formula195-0049124112464866"><mml:math id="mml-inline195-0049124112464866"><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">=</mml:mo><mml:mn>.63</mml:mn></mml:math></inline-formula>—simulation suggests that the transformation method is nearly unbiased, while imputation without transformation has a bias of about –12 percent (see Online Appendix A found at http://smr.sagepub.com/supplemental/). Note that the authors used a milder transformation than the fourth-root transformation evaluated in our simulation. In particular, the authors used the arcsine transformation <inline-formula id="inline-formula196-0049124112464866"><mml:math id="mml-inline196-0049124112464866"><mml:mrow><mml:msup><mml:mo form="prefix" movablelimits="false">sin</mml:mo><mml:mrow><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msqrt><mml:mi>X</mml:mi></mml:msqrt></mml:math></inline-formula>, which is practically indistinguishable from <inline-formula id="inline-formula197-0049124112464866"><mml:math id="mml-inline197-0049124112464866"><mml:msqrt><mml:mi>X</mml:mi></mml:msqrt></mml:math></inline-formula> for the range of <italic>X</italic> values in their data.</p><p>Reanalyzing the authors’ data without control variables, I found that the estimated slope of <italic>Y</italic> on <italic>X</italic> was .60 with the authors’ transformation, and .55 without transformation. My simulations suggest that the authors’ estimate is closer to the truth, but the difference is not large either way.</p></sec><sec id="section18-0049124112464866"><title>Analysis of Body Mass Index (BMI)</title><p>In a longitudinal study of <italic>n </italic>= 358 children, von Hippel, Nahhas, and Czerwinski (2012) estimated percentiles for change in BMI from age 3½ to age 18 years. BMI was measured every six months, but some measurements were missing. Our longitudinal imputation model was more complicated than the models considered in this article, but the model still assumed that the incomplete variables were conditionally normal. Skew in BMI was an important challenge, especially since the skew of the BMI distribution increases as children grow older.</p><p>As one of the authors, I knew that linear regression imputation could estimate the conditional mean of a skewed variable with little bias, but this was not reassuring since in this study we sought to estimate the percentiles instead of the mean. With or without transformation, it proved difficult to find an imputation model that gave plausible estimates for extreme tail percentiles such as the 90th or 95th.</p><p>In the end, we sidestepped the issue by imputing BMI increments—that is, changes in BMI from one measurement occasion to the next. Although BMI is skewed, the distribution of BMI increments is approximately symmetric and has only slightly more kurtosis than a normal distribution.</p></sec></sec><sec id="section19-0049124112464866"><title>Conclusion</title><p>On the whole, the simulation results suggest that when an incomplete variable has skew, linear regression often gives reasonable, though not unbiased, estimates for the quantities most commonly estimated in social science—namely, means, standard deviations, and linear regressions (see also <xref ref-type="bibr" rid="bibr11-0049124112464866">Demirtas et al. 2008</xref>). Ad hoc modifications of linear regression—through censoring, truncation, or transformation—rarely do much to improve the estimates, and in fact can make the estimates much worse.</p><p>Although the normal regression method is fairly good for estimating means, variances, and regressions, it can do a poor job of estimating quantities that depend on distributional shape. Such quantities include the coefficient of skewness, the percentiles, and quantities based on percentiles such as the Gini coefficient.</p><p>Although our discussion has been confined to normal imputation, we note that similar biases would be expected if estimates were obtained, without imputation, by applying maximum likelihood to the incomplete data under an assumption of normality. Maximum likelihood can be asymptotically biased when its distributional assumptions are violated (<xref ref-type="bibr" rid="bibr45a-0049124112464866">Yuan, Wallentin, &amp; Bentler, in press</xref>). For example, in the  Bivariate Data section, we discussed a situation where OLS estimates for the regression of <italic>X</italic> on <italic>Y</italic>—which are maximum likelihood estimates if <italic>X</italic> is conditionally normal—are biased for a skewed <italic>X</italic>.</p><p>To improve estimation from incomplete skewed variables, it would be helpful to have imputation methods that do not assume normality. One option is the distribution-free approach of imputing missing values by resampling values from similar cases. Variants of this idea are called hot-deck imputation, the approximate Bayesian bootstrap, or predictive mean matching (for a review, see <xref ref-type="bibr" rid="bibr5-0049124112464866">Andridge and Little 2010</xref>). For example, in bivariate data (<italic>X</italic>, <italic>Y</italic>) with <italic>Y</italic> complete and <italic>X</italic> MAR, one would fill in missing <italic>X</italic> values with observed <italic>X</italic> values resampled from cases with similar values of <italic>Y</italic>.</p><p>While imputation by resampling is initially attractive, it can work poorly when observed values are sparse in one part of the distribution. For example, suppose that <italic>X</italic> is missing if and only if <italic>Y </italic>&lt; 0. If we wish to impute the missing <italic>X</italic> values in cases with <italic>Y </italic>&lt; 0, we have to resample <italic>X</italic> values from cases with <italic>Y </italic>≥ 0—and this inevitably leads to bias. (See <xref ref-type="bibr" rid="bibr1-0049124112464866">Allison 2000</xref> for a similar but slightly more complicated example.) To take a less-artificial example from applied research, in our study of BMI growth among children, the very heaviest children had a number of missing measurements. These measurements could not be imputed by resampling, unless we were willing to resample BMIs from much lighter children.</p><p>A perhaps more promising idea is to model and impute nonnormal variables using flexible nonnormal distributions that can take a variety of shapes—such as Tukey’s <italic>gh</italic> distribution (<xref ref-type="bibr" rid="bibr17-0049124112464866">He and Raghunathan 2006</xref>), the Weibull or beta density (<xref ref-type="bibr" rid="bibr12-0049124112464866">Demirtas and Hedeker 2008</xref>), the generalized lambda distribution, or Fleishman power polynomials (<xref ref-type="bibr" rid="bibr10-0049124112464866">Demirtas 2010</xref>). These approaches are currently in the development stage. Initial evaluations suggest that they can mimic very well the shape of many nonnormal distributions and that they preserve the relationships among variables at least as well as normal imputation methods (<xref ref-type="bibr" rid="bibr17-0049124112464866">He and Raghunathan 2006</xref>, <xref ref-type="bibr" rid="bibr18-0049124112464866">2009</xref>, <xref ref-type="bibr" rid="bibr19-0049124112464866">2012</xref>; <xref ref-type="bibr" rid="bibr7-0049124112464866">Bondarenko and Raghunathan 2007</xref>; <xref ref-type="bibr" rid="bibr10-0049124112464866">Demirtas 2010</xref>; <xref ref-type="bibr" rid="bibr12-0049124112464866">Demirtas and Hedeker 2008</xref>). We look forward to these flexible methods being available in software so that they can be used in applied research and evaluated further.</p></sec></body><back><ack><title>Acknowledgments</title><p>I thank Ray Koopman for help with the calculations in the subsection on Imputing Within Bounds: Censoring and Truncation. I also thank Pamela Paxton and Sheri Kunovich for sharing the data in the subsection on Analysis of Female Legislative Candidates.</p></ack><fn-group><fn fn-type="conflict" id="fn9-0049124112464866"><label>Declaration of Conflicting Interests</label><p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn><fn fn-type="financial-disclosure" id="fn10-0049124112464866"><label>Funding</label><p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p></fn></fn-group><notes><title>Notes</title><fn-group><fn fn-type="other" id="fn1-0049124112464866"><label>1.</label><p>To see this, note that as <inline-formula id="inline-formula198-0049124112464866"><mml:math id="mml-inline198-0049124112464866"><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:math></inline-formula>, <inline-formula id="inline-formula199-0049124112464866"><mml:math id="mml-inline199-0049124112464866"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow/></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mtext>obs</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula200-0049124112464866"><mml:math id="mml-inline200-0049124112464866"><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> converge to <inline-formula id="inline-formula201-0049124112464866"><mml:math id="mml-inline201-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula202-0049124112464866"><mml:math id="mml-inline202-0049124112464866"><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>, while <inline-formula id="inline-formula203-0049124112464866"><mml:math id="mml-inline203-0049124112464866"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>U</mml:mi></mml:math></inline-formula> and <inline-formula id="inline-formula204-0049124112464866"><mml:math id="mml-inline204-0049124112464866"><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> converge to 1 and 0. Therefore <inline-formula id="inline-formula205-0049124112464866"><mml:math id="mml-inline205-0049124112464866"><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">μ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="inline-formula206-0049124112464866"><mml:math id="mml-inline206-0049124112464866"><mml:msubsup><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo mathvariant="normal" stretchy="false">,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> converge to <inline-formula id="inline-formula207-0049124112464866"><mml:math id="mml-inline207-0049124112464866"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">μ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula208-0049124112464866"><mml:math id="mml-inline208-0049124112464866"><mml:msup><mml:mrow><mml:mrow><mml:mi mathvariant="italic">σ</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mn mathvariant="normal">2</mml:mn></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula>—that is, the PD estimators are consistent.</p></fn><fn fn-type="other" id="fn2-0049124112464866"><label>2.</label><p>In simulations, we found that the skew of a logged exponential sample was about –1.14 but the true skew is undefined since ln(0) is undefined.</p></fn><fn fn-type="other" id="fn3-0049124112464866"><label>3.</label><p>The following calculations were assisted by Mathematica 8.</p></fn><fn fn-type="other" id="fn4-0049124112464866"><label>4.</label><p>To verify the consistency of the PD estimates, note that, as <inline-formula id="inline-formula209-0049124112464866"><mml:math id="mml-inline209-0049124112464866"><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:math></inline-formula>, <inline-formula id="inline-formula210-0049124112464866"><mml:math id="mml-inline210-0049124112464866"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">α</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">β</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi mathvariant="italic">σ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> converge to <inline-formula id="inline-formula211-0049124112464866"><mml:math id="mml-inline211-0049124112464866"><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msubsup><mml:mi mathvariant="italic">σ</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>, while <inline-formula id="inline-formula212-0049124112464866"><mml:math id="mml-inline212-0049124112464866"><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>U</mml:mi></mml:math></inline-formula> and <inline-formula id="inline-formula213-0049124112464866"><mml:math id="mml-inline213-0049124112464866"><mml:mover accent="true"><mml:mi>V</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">α</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">β</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>Y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>X</mml:mi><mml:mo stretchy="false">,</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> converge to 1 and 0. </p></fn><fn fn-type="other" id="fn5-0049124112464866"><label>5.</label><p>Small sample biases can afflict multiple imputation estimates even when the data are normal and the imputation model is correctly specified (<xref ref-type="bibr" rid="bibr26-0049124112464866">Kim 2004</xref>; <xref ref-type="bibr" rid="bibr12-0049124112464866">Demirtas and Hedeker 2008</xref>; von Hippel 2012).</p></fn><fn fn-type="other" id="fn6-0049124112464866"><label>6.</label><p>The rejection algorithm is invoked by the MINIMUM and MAXIMUM options in the MI procedure. </p></fn><fn fn-type="other" id="fn7-0049124112464866"><label>7.</label><p>The MI procedure implements transformation and inverse transformation via the TRANSFORM option.</p></fn><fn fn-type="other" id="fn8-0049124112464866"><label>8.</label><p>As <xref ref-type="fig" rid="fig2-0049124112464866">Figure 2</xref> would lead us to expect, censoring and truncation increase the mean and reduce the standard deviation. The effect of truncation is less than it should be because the software would not allow us to truncate all the data sets at <italic>c </italic>= 0.</p></fn></fn-group></notes><ref-list><title>References</title><ref id="bibr1-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>Paul D.</given-names></name></person-group> <year>2000</year>. <article-title>“Multiple Imputation for Missing Data: A Cautionary Tale.”</article-title> <source>Sociological Methods &amp; Research</source> <volume>28</volume>:<fpage>301</fpage>–<lpage>09</lpage>.</citation></ref><ref id="bibr2-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>Paul D.</given-names></name></person-group> <year>2002</year>. <source>Missing Data</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation></ref><ref id="bibr3-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>Paul D.</given-names></name></person-group> <year>2005</year>. <article-title>“Imputation of Categorical Variables With PROC MI.”</article-title> <comment>Prsented at the SAS Users Group International, 30th Meeting (SUGI 30), Philadelphia , PA.</comment></citation></ref><ref id="bibr4-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>T. W.</given-names></name></person-group> <year>1957</year>. <article-title>“Maximum Likelihood Estimates for a Multivariate Normal Distribution When Some Observations are Missing.”</article-title> <source>Journal of the American Statistical Association</source> <volume>52</volume>:<fpage>200</fpage>–<lpage>03</lpage>. </citation></ref><ref id="bibr5-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Andridge</surname><given-names>Rebecca R.</given-names></name><name><surname>Little</surname><given-names>Roderick J. A.</given-names></name></person-group>. <year>2010</year>. <article-title>“A Review of Hot Deck Imputation for Survey Non-response.”</article-title> <source>International Statistical Review</source> <volume>78</volume>:<fpage>40</fpage>–<lpage>64</lpage>. </citation></ref><ref id="bibr6-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Bernaards</surname><given-names>Coen A.</given-names></name><name><surname>Belin</surname><given-names>Thomas R.</given-names></name><name><surname>Schafer</surname><given-names>Joseph L.</given-names></name></person-group>. <year>2007</year>. <article-title>“Robustness of a Multivariate Normal Approximation for Imputation of Incomplete Binary Data.”</article-title> <source>Statistics in Medicine</source> <volume>26</volume>:<fpage>1368</fpage>–<lpage>82</lpage>. </citation></ref><ref id="bibr7-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Bondarenko</surname><given-names>Irina</given-names></name><name><surname>Raghunathan</surname><given-names>Trivellore E.</given-names></name></person-group>. <year>2007</year>. <article-title>“Multiple Imputations Using Sequential Semi and Nonparametric Regressions.”</article-title> <source>Proceedings of the Survey Research Methods Section</source>. <publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>American Statistical Association</publisher-name>.</citation></ref><ref id="bibr8-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Box</surname><given-names>G. E. P.</given-names></name><name><surname>Cox</surname><given-names>D. R.</given-names></name></person-group>. <year>1964</year>. <article-title>“An Analysis of Transformations.”</article-title> <source>Journal of the Royal Statistical Society Series B</source> <volume>26</volume>:<fpage>211</fpage>–<lpage>52</lpage>.</citation></ref><ref id="bibr9-0049124112464866"><citation citation-type="web"><collab collab-type="author">Centers for Disease Control and Prevention</collab>. <year>2011</year>. <article-title>“A SAS Program for the CDC Growth Charts.”</article-title> <comment>Retrieved July 13, 2011</comment> (<ext-link ext-link-type="uri" xlink:href="http://www.cdc.gov/nccdphp/dnpao/growthcharts/resources/sas.htm">http://www.cdc.gov/nccdphp/dnpao/growthcharts/resources/sas.htm</ext-link>).</citation></ref><ref id="bibr10-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Demirtas</surname><given-names>Hakan</given-names></name></person-group>. <year>2010</year>. <article-title>“An Application of Multiple Imputation under the Two Generalized Parametric Families.”</article-title> <source>Journal of Data Science</source> <volume>8</volume>:<fpage>443</fpage>–<lpage>55</lpage>.</citation></ref><ref id="bibr11-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Demirtas</surname><given-names>Hakan</given-names></name><name><surname>Freels</surname><given-names>Sally A.</given-names></name><name><surname>Yucel</surname><given-names>Recai M.</given-names></name></person-group>. <year>2008</year>. <article-title>“Plausibility of Multivariate Normality Assumption When Multiply Imputing Non-Gaussian Continuous Outcomes: A simulation Assessment.”</article-title> <source>Journal of Statistical Computation &amp; Simulation</source> <volume>78</volume>:<fpage>69</fpage>–<lpage>84</lpage>.</citation></ref><ref id="bibr12-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Demirtas</surname><given-names>Hakan</given-names></name><name><surname>Hedeker</surname><given-names>Donald</given-names></name></person-group>. <year>2008</year>. <article-title>“Imputing Continuous Data Under Some Non-Gaussian Distributions.”</article-title> <source>Statistica Neerlandica</source> <volume>62</volume>:<fpage>193</fpage>–<lpage>205</lpage>. </citation></ref><ref id="bibr13-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Goldberger</surname><given-names>Arthur S.</given-names></name></person-group> <year>1981</year>. <article-title>“Linear Regression after Selection.”</article-title> <source>Journal of Econometrics</source> <volume>15</volume>:<fpage>357</fpage>–<lpage>66</lpage>. </citation></ref><ref id="bibr14-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Greene</surname><given-names>William H.</given-names></name></person-group> <year>1999</year>. <source>Econometric Analysis</source>. <edition>4th ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation></ref><ref id="bibr15-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Halmos</surname><given-names>Paul R.</given-names></name></person-group> <year>1946</year>. <article-title>“The Theory of Unbiased Estimation.”</article-title> <source>The Annals of Mathematical Statistics</source> <volume>17</volume>:<fpage>34</fpage>–<lpage>43</lpage>. </citation></ref><ref id="bibr16-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname><given-names>Douglas M.</given-names></name><name><surname>Wixley</surname><given-names>R. A. J.</given-names></name></person-group>. <year>1986</year>. <article-title>“A Note on the Transformation of Chi-Squared Variables to Normality.”</article-title> <source>American Statistician</source> <volume>40</volume>:<fpage>296</fpage>–<lpage>98</lpage>. </citation></ref><ref id="bibr17-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>Yulei</given-names></name><name><surname>Raghunathan</surname><given-names>Trivellore E</given-names></name></person-group>. <year>2006</year>. <article-title>“Tukey’s gh Distribution for Multiple Imputation.”</article-title> <source>The American Statistician</source> <volume>60</volume>:<fpage>251</fpage>–<lpage>56</lpage>. </citation></ref><ref id="bibr18-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>Yulei</given-names></name><name><surname>Raghunathan</surname><given-names>Trivellore E.</given-names></name></person-group>. <year>2009</year>. <article-title>“On the Performance of Sequential Regression Multiple Imputation Methods with Non Normal Error Distributions.”</article-title> <source>Communications in Statistics - Simulation and Computation</source> <volume>38</volume>:<fpage>856</fpage>.</citation></ref><ref id="bibr19-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>Yulei</given-names></name><name><surname>Raghunathan</surname><given-names>Trivellore E.</given-names></name></person-group>. <year>2012</year>. <article-title>“Multiple Imputation Using Multivariate Gh Transformations.”</article-title> <source>Journal of Applied Statistics</source> <volume>39</volume>:<fpage>2177</fpage>–<lpage>98</lpage>.</citation></ref><ref id="bibr20-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Heitjan</surname><given-names>Daniel F.</given-names></name><name><surname>Basu</surname><given-names>Srabashi</given-names></name></person-group>. <year>1996</year>. <article-title>“Distinguishing ‘Missing at Random’ and ‘Missing Completely at Random’.”</article-title> <source>The American Statistician</source> <volume>50</volume>:<fpage>207</fpage>–<lpage>13</lpage>. </citation></ref><ref id="bibr21-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hoerl</surname><given-names>Arthur E.</given-names></name><name><surname>Kennard</surname><given-names>Robert W.</given-names></name></person-group>. <year>1970</year>. <article-title>“Ridge Regression: Biased Estimation for Nonorthogonal Problems.”</article-title> <source>Technometrics</source> <volume>12</volume>:<fpage>55</fpage>–<lpage>67</lpage>. </citation></ref><ref id="bibr22-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Horton</surname><given-names>Nicholas J.</given-names></name><name><surname>Lipsitz</surname><given-names>Stuart R.</given-names></name><name><surname>Parzen</surname><given-names>Michael</given-names></name></person-group>. <year>2003</year>. <article-title>“A Potential for Bias When Rounding in Multiple Imputation.”</article-title> <source>The American Statistician</source> <volume>57</volume>:<fpage>229</fpage>–<lpage>32</lpage>. </citation></ref><ref id="bibr23-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>Norman L.</given-names></name><name><surname>Kotz</surname><given-names>Samuel</given-names></name><name><surname>Balakrishnan</surname><given-names>N.</given-names></name></person-group>. <year>1994</year>. <source>Continuous Univariate Distributions</source>, <volume>Vol. 1</volume>. <edition>2nd ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley-Interscience</publisher-name>.</citation></ref><ref id="bibr24-0049124112464866"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Jöreskog</surname><given-names>Karl G.</given-names></name></person-group>. <year>2002</year>. <article-title>“Censored Variables and Censored Regression.”</article-title> <comment>Retrieved November 10, 2011</comment> (<ext-link ext-link-type="uri" xlink:href="http://www.ssicentral.com/lisrel/techdocs/censor.pdf">http://www.ssicentral.com/lisrel/techdocs/censor.pdf</ext-link>).</citation></ref><ref id="bibr25-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kenward</surname><given-names>Michael G.</given-names></name><name><surname>Carpenter</surname><given-names>James</given-names></name></person-group>. <year>2007</year>. <article-title>“Multiple Imputation: Current Perspectives.”</article-title> <source>Statistical Methods in Medical Research</source> <volume>16</volume>:<fpage>199</fpage>–<lpage>218</lpage>. </citation></ref><ref id="bibr26-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Jae Kwang</given-names></name></person-group>. <year>2004</year>. <article-title>“Finite Sample Properties of Multiple Imputation Estimators.”</article-title> <source>The Annals of Statistics</source> <volume>32</volume>:<fpage>766</fpage>–<lpage>83</lpage>. </citation></ref><ref id="bibr27-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kunovich</surname><given-names>S.</given-names></name><name><surname>Paxton</surname><given-names>Pamela</given-names></name></person-group>. <year>2005</year>. <article-title>“Pathways to Power: The Role of Political Parties in Women’s National Political Representation.”</article-title> <source>American Journal of Sociology</source> <volume>111</volume>:<fpage>505</fpage>–<lpage>52</lpage>. </citation></ref><ref id="bibr28-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Little</surname><given-names>Roderick J. A.</given-names></name></person-group> <year>1992</year>. <article-title>“Regression With Missing X’s: A Review.”</article-title> <source>Journal of the American Statistical Association</source> <volume>87</volume>:<fpage>1227</fpage>–<lpage>37</lpage>. </citation></ref><ref id="bibr29-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Little</surname><given-names>Roderick J. A.</given-names></name><name><surname>Rubin</surname><given-names>Donald B.</given-names></name></person-group>. <year>1989</year>. <article-title>“The Analysis of Social Science Data with Missing Values.”</article-title> <source>Sociological Methods &amp; Research</source> <volume>18</volume>:<fpage>292</fpage>–<lpage>326</lpage>. </citation></ref><ref id="bibr30-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Muniz</surname><given-names>Gisela</given-names></name><name><surname>Golam Kibria</surname><given-names>B. M.</given-names></name></person-group>. <year>2009</year>. <article-title>“On Some Ridge Regression Estimators: An Empirical Comparisons.”</article-title> <source>Communications in Statistics - Simulation and Computation</source> <volume>38</volume>:<fpage>621</fpage>–<lpage>30</lpage>.</citation></ref><ref id="bibr31-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Raghunathan</surname><given-names>Trivellore E.</given-names></name><name><surname>Lepkowski</surname><given-names>James M.</given-names></name><name><surname>Hoewyk</surname><given-names>John Van</given-names></name><name><surname>Solenberger</surname><given-names>Peter W.</given-names></name></person-group>. <year>2001</year>. <article-title>“A Multivariate Technique for Multiply Imputing Missing Values Using a Sequence of Regression Models.”</article-title> <source>Survey Methodology</source> <volume>27</volume>:<fpage>85</fpage>–<lpage>95</lpage>.</citation></ref><ref id="bibr32-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Raghunathan</surname><given-names>Trivellore E.</given-names></name><name><surname>Solenberger</surname><given-names>Peter W.</given-names></name><name><surname>Hoewyk</surname><given-names>John Van</given-names></name></person-group>. <year>2002</year>. <source>IVEware: Imputation and Variance Estimation Software</source>. <publisher-name>Ann Arbor</publisher-name>, <publisher-loc>MI</publisher-loc>.</citation></ref><ref id="bibr33-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Robert</surname><given-names>Christian P.</given-names></name></person-group> <year>1995</year>. <article-title>“Simulation of Truncated Normal Variables.”</article-title> <source>Statistics and Computing</source> <volume>5</volume>:<fpage>121</fpage>–<lpage>25</lpage>. </citation></ref><ref id="bibr34-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>Donald B.</given-names></name></person-group> <year>1976</year>. <article-title>“Inference and Missing Data.”</article-title> <source>Biometrika</source> <volume>63</volume>:<fpage>581</fpage>–<lpage>92</lpage>. </citation></ref><ref id="bibr35-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>Donald B.</given-names></name></person-group> <year>1987</year>. <source>Multiple Imputation for Nonresponse in Surveys</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation></ref><ref id="bibr36-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>Donald B.</given-names></name><name><surname>Schenker</surname><given-names>Nathaniel</given-names></name></person-group>. <year>1986</year>. <article-title>“Multiple Imputation for Interval Estimation From Simple Random Samples With Ignorable Nonresponse.”</article-title> <source>Journal of the American Statistical Association</source> <volume>81</volume>:<fpage>366</fpage>–<lpage>74</lpage>. </citation></ref><ref id="bibr37-0049124112464866"><citation citation-type="web"><collab collab-type="author">SAS Institute</collab>. <year>2001</year>. <article-title>“The MI Procedure.”</article-title> <source>SAS/STAT Software: Changes and Enhancements, Release 8.2</source>. <publisher-loc>Cary, NC</publisher-loc>: <publisher-name>SAS Institute</publisher-name>. <comment>Retrieved May 23, 2011</comment> (<ext-link ext-link-type="uri" xlink:href="http://support.sas.com/rnd/app/da/new/802ce/stat/chap9/index.htm">http://support.sas.com/rnd/app/da/new/802ce/stat/chap9/index.htm</ext-link>).</citation></ref><ref id="bibr38-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schafer</surname><given-names>Joseph L.</given-names></name><name><surname>Graham</surname><given-names>John W.</given-names></name></person-group>. <year>2002</year>. <article-title>“Missing Data: Our View of the State of the Art.”</article-title> <source>Psychological Methods</source> <volume>7</volume>:<fpage>147</fpage>–<lpage>77</lpage>. </citation></ref><ref id="bibr39-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schafer</surname><given-names>Joseph L.</given-names></name><name><surname>Olsen</surname><given-names>Maren K.</given-names></name></person-group>. <year>1998</year>. <article-title>“Multiple Imputation for Multivariate Missing-Data Problems: A Data Analyst’s Perspective.”</article-title> <source>Multivariate Behavioral Research</source> <volume>33</volume>:<fpage>545</fpage>.</citation></ref><ref id="bibr40-0049124112464866"><citation citation-type="web"><collab collab-type="author">Stata Corp</collab>. <year>2011</year>. <article-title>“Multiple Imputation for Missing Data.”</article-title> <comment>Retrieved July 2, 2011</comment> (<ext-link ext-link-type="uri" xlink:href="http://www.stata.com/stata11/mi.html">http://www.stata.com/stata11/mi.html</ext-link>).</citation></ref><ref id="bibr41-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>von</surname><given-names>Hippel</given-names>  </name><name><surname>Paul</surname><given-names>T</given-names></name></person-group>. <year>2007</year>. <article-title>“Regression With Missing Ys: An Improved Strategy For Analyzing Multiply Imputed Data.”</article-title> <source>Sociological Methodology</source> <volume>37</volume>:<fpage>83</fpage>–<lpage>117</lpage>. </citation></ref><ref id="bibr42-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>von</surname><given-names>Hippel</given-names></name><name><surname>Paul</surname><given-names>T</given-names></name></person-group>. <year>2009</year>. <article-title>“How To Impute Interactions, Squares, and Other Transformed Variables.”</article-title> <source>Sociological Methodology</source> <volume>39</volume>:<fpage>265</fpage>–<lpage>91</lpage>.</citation></ref><ref id="bibr43-0049124112464866"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>von</surname><given-names>Hippel</given-names></name><name><surname>Paul</surname><given-names>T</given-names></name></person-group>. <year>in press</year>. <article-title>“The Bias and Efficiency of Incomplete-Data Estimators in Small Univariate Normal Samples.”</article-title> <source>Sociological Methods &amp; Research</source>.</citation></ref><ref id="bibr44-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>von</surname><given-names>Hippel</given-names></name><name><surname>Paul</surname><given-names>T</given-names></name><name><surname>Nahhas</surname><given-names>Ramzi W.</given-names></name><name><surname>Czerwinski</surname><given-names>Stefan A.</given-names></name></person-group>. <year>2012</year>. <article-title>“Percentiles for Change in Body Mass Index (BMI) from Age 3½ to 18 Years.”</article-title> <comment>Unpublished manuscript</comment>.</citation></ref><ref id="bibr45-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Wooldridge</surname><given-names>Jeffrey M.</given-names></name></person-group> <year>2001</year>. <source>Econometric Analysis of Cross Section and Panel Data</source>. <edition>1st ed</edition>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation></ref><ref id="bibr45a-0049124112464866"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>K.–H.</given-names></name><name><surname>Wallentin</surname><given-names>F.</given-names></name><name><surname>Bentler</surname><given-names>P. M.</given-names></name></person-group>  (<year>in press</year>). <article-title>ML versus MI for missing data with violation of distribution conditions</article-title>. <source>Sociological Methods &amp; Research</source>.</citation></ref></ref-list></back></article>