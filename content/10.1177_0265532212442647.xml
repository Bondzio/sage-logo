<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LTJ</journal-id>
<journal-id journal-id-type="hwp">spltj</journal-id>
<journal-title>Language Testing</journal-title>
<issn pub-type="ppub">0265-5322</issn>
<issn pub-type="epub">1477-0946</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0265532212442647</article-id>
<article-id pub-id-type="publisher-id">10.1177_0265532212442647</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Quantifying the quality difference between L1 and L2 essays: A rating procedure with bilingual raters and L1 and L2 benchmark essays</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Tillema</surname><given-names>Marion</given-names></name>
<aff id="aff1-0265532212442647">Utrecht University, the Netherlands</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>van den Bergh</surname><given-names>Huub</given-names></name>
<aff id="aff2-0265532212442647">Utrecht University/University of Amsterdam, the Netherlands</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Rijlaarsdam</surname><given-names>Gert</given-names></name>
<aff id="aff3-0265532212442647">University of Amsterdam, the Netherlands</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Sanders</surname><given-names>Ted</given-names></name>
<aff id="aff4-0265532212442647">Utrecht University, the Netherlands</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0265532212442647">Marion Tillema, Trans 10, 3512 JK Utrecht, the Netherlands. Email: <email>kortman-tillema@home.nl</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>1</issue>
<fpage>71</fpage>
<lpage>97</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>It is the consensus that, as a result of the extra constraints placed on working memory, texts written in a second language (L2) are usually of lower quality than texts written in the first language (L1) by the same writer. However, no method is currently available for quantifying the quality difference between L1 and L2 texts. In the present study, we tested a rating procedure for enabling quality judgments of L1 and L2 texts on a single scale. Two main features define this procedure: 1) raters are bilingual or near native users of both the L1 and L2; 2) ratings are performed with L1 and L2 benchmark texts. Direct comparisons of observed L1 and L2 scores are only warranted if the ratings with L1 and L2 benchmarks are parallel tests and if the ratings are reliable. Results showed that both conditions are met. Effect sizes (<italic>Cohen’s d</italic>) indicate that, while score variances are large, there is a relatively large added L2 effect: in the investigated population, L2 text scores were much lower than L1 text scores. The tested rating procedure is a promising method for cross-national comparisons of writing proficiency.</p>
</abstract>
<kwd-group>
<kwd>international assessment</kwd>
<kwd>first language</kwd>
<kwd>parallel tests</kwd>
<kwd>second language</kwd>
<kwd>text quality</kwd>
<kwd>writing proficiency</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Several researchers have undertaken empirical studies in which they compared writing processes in the first language (L1) and a second language (L2). The general conclusion is that L2 writing processes differ from L1 writing processes. <xref ref-type="bibr" rid="bibr26-0265532212442647">Van Weijen, Van den Bergh, Rijlaarsdam and Sanders (2008)</xref>, for example, found that differences between tasks, in terms of the organization of the writing process, were larger in the L1 than in the L2. In other words, writers’ writing processes varied more in L1 than in L2. Differences between L1 and L2 writing processes were also demonstrated by <xref ref-type="bibr" rid="bibr3-0265532212442647">Chenoweth and Hayes (2001)</xref>. They found that, during L2 writing, there were lower levels of fluency (i.e. number of words written per minute), decreased burst length (i.e. number of words produced every time that a new piece of text content is generated) and more instances of revision than during L1 writing processes.</p>
<p>While it may be expected that these differences between L1 and L2 writing processes cause differences in the quality of the output, this effect has (to our knowledge) not yet been confirmed. A number of researchers have indicated that writing in an L2 is more demanding than writing in an L1, on the basis of different kinds of data (<xref ref-type="bibr" rid="bibr16-0265532212442647">Roca de Larios, Manchón, Murphy &amp; Marín, 2008</xref>; <xref ref-type="bibr" rid="bibr18-0265532212442647">Schoonen et al., 2003</xref>; <xref ref-type="bibr" rid="bibr19-0265532212442647">Thorson, 2000</xref>; <xref ref-type="bibr" rid="bibr26-0265532212442647">Van Weijen et al., 2008</xref>). However, to investigate whether the more demanding task of writing in an L2 does indeed cause L2 texts to be of lower quality than L1 texts, it is necessary to quantify the quality difference between the resulting L1 and L2 texts. This would allow for relating (quantified) processing differences to (quantified) differences between L1 and L2 texts. For this to be possible, the quality of both L1 and L2 texts must be expressed on the same scale. However, a method for achieving a single scale and enabling direct comparisons of L1 and L2 texts is currently not available.</p>
<p>One of the first and most substantial studies into the comparability of expressions of the quality of writing in different languages was a cross-national study by the International Association for the Evaluation of Educational Achievement (IEA), reported in <xref ref-type="bibr" rid="bibr9-0265532212442647">Gorman, Purves and Degenhart (1988)</xref> and <xref ref-type="bibr" rid="bibr12-0265532212442647">Purves (1992)</xref>. The researchers attempted to use a single scale to express the quality of texts in different languages and from different regions. Texts from 14 countries, written in the local L1, were rated by means of a scoring scheme which included only rating criteria which are common to all languages involved. Criteria which were assumed to hold cross-linguistically and cross-nationally were content, organization, style and tone, and overall impression. These criteria were assumed to reflect the construct of writing proficiency throughout languages and cultures, thus allowing for comparison and quantification across languages. The ratings of the 14 essay sets (each from another country) were performed within the countries where they were written, by L1 users of the language in question. Ratings were carried out using three benchmark essays per language and per criterion. These three benchmark essays represent different scale points, namely the mean of the scale, and high quality and low quality, respectively. The other essays were rated relative to these benchmarks. As benchmark essays were selected per participating country, different benchmark essays were used in each of the 14 countries.</p>
<p>The IEA researchers concluded that a comparison of text quality between languages was not possible: the rating criteria and scales were used differently by the raters from different countries. For example, whereas the raters in some countries used the entire scale, from minimum to maximum, the raters in other countries only used the upper part, from mean to maximum. While it is, strictly speaking, possible to interpret this result as indicating that all students in the latter country performed (homogeneously) well, it seems more likely that scale shrinkage occurred: the raters in the latter country did not assign any low scores, possibly due to cultural conventions. This renders the scales incomparable across countries and languages.</p>
<p><xref ref-type="bibr" rid="bibr25-0265532212442647">Van Weijen (2009)</xref> also tested a procedure designed to directly compare L1 and L2 essays. To control for problems due to different (groups of) raters (as occurred in the IEA study, cf. <xref ref-type="bibr" rid="bibr9-0265532212442647">Gorman, Purves &amp; Degenhart, 1988</xref>; <xref ref-type="bibr" rid="bibr12-0265532212442647">Purves, 1992</xref>), <xref ref-type="bibr" rid="bibr25-0265532212442647">Van Weijen (2009)</xref> employed a rating procedure in which the raters were presented with an essay set consisting solely of essays in L2. Included in this essay set were eight L1 essays (of average quality, each on a different topic) which had previously been translated into L2. The raters were unaware of this, so that they would apply their (implicit) L2 standards to this translated L1 essay. The idea behind this procedure was that the scores assigned to the translated versions of the L1 in the L2 rating session would probably be higher than their original scores (as assigned during the L1 rating session). This (mean) score difference would be an indication of the difference between the L1 and the L2 rating scale, thus giving the researchers a number with which to transform the L2 scale onto the L1 scale, or vice versa. However, for two out of the eight topics, the score L1-L2 difference was much larger than for the other six tasks. This could be due to an interaction effect of language and task (i.e. the quality difference between and L1 and an L2 text is different for different tasks) or due to translation inconsistencies. <xref ref-type="bibr" rid="bibr25-0265532212442647">Van Weijen (2009)</xref> notes that the translation of essays is a difficult task, not least because (language) errors had to be translated too. She concludes that this procedure did not result in a scale on which to place both the L1 and L2 essays and compare them directly.</p>
<p>To neutralize all these problems, <xref ref-type="bibr" rid="bibr25-0265532212442647">Van Weijen (2009</xref>, p. 171) suggests ratings by raters who are highly proficient speakers of both languages, preferably bilinguals, a procedure which is also suggested in <xref ref-type="bibr" rid="bibr13-0265532212442647">Purves, Gorman and Takala (1988</xref>, p. 51). This might tackle two rater problems. First of all, it is expected that bilingual raters are equally strict towards both languages. Raters who, unlike bilinguals, are more proficient in one of the two languages than in the other might not be equally strict towards both languages; that is, they might consistently assign higher scores to one of the languages (cf. <xref ref-type="bibr" rid="bibr23-0265532212442647">Van den Bergh &amp; Klein Gunnewiek, 2009</xref>, who found that raters awarded higher scores to texts if they were L1 users of the language of the text than if they were L2 users of the language), even if true scores are equal across languages. This bias is expected to be less prominent, or absent, in bilingual raters. In addition, a rater who is equally proficient in both languages is expected to be more likely to apply rating standards equally across languages, that is, to rate L1 and L2 texts with equal reliability, making the ratings expressible on the same scale and therefore comparable.</p>
<p>In the present study, then, we tested whether ratings by bilinguals of L1 and L2 essays are indeed expressible on one scale by implementing a procedure in which ratings are carried out with both L1 and L2 benchmark essays. The main aim of the present study is to investigate if the allocation of scores to L1 and L2 essays is similar with L1 and L2 benchmarks. If so, this would mean that there is no evidence to assume that the raters rated differently for different languages. The aim of the present study would then be warranted: to quantify the quality differences between L1 and L2 writing.</p>
<sec id="section1-0265532212442647" sec-type="methods">
<title>Method</title>
<sec id="section2-0265532212442647">
<title>Participants and procedures</title>
<sec id="section3-0265532212442647">
<title>Obtaining essays</title>
<p>One hundred and sixty short essays (about 250 to 300 words) were rated and compared in the present study. In line with the IEA study of written composition (<xref ref-type="bibr" rid="bibr9-0265532212442647">Gorman, Purves &amp; Degenhart, 1988</xref>; <xref ref-type="bibr" rid="bibr12-0265532212442647">Purves, 1992</xref>), the essays were written by 14- and 15-year-old students (<italic>N</italic> = 20; 10 female and 10 male). In addition, 15-year-olds are the target population of the international PISA assessments. Should a PISA assessment of writing literacy be set up, then tools for cross-national comparisons of writing become relevant, too. The participants were from three different third-year forms at the same school for pre-university secondary education. They were recruited by means of a call for volunteers, which was distributed by their Dutch teacher. All participants were native speakers of Dutch. They received a small financial reward for their participation. Parental consent was obtained.</p>
<p>Each student wrote four short argumentative essays in Dutch (L1) and four essays in English (L2). Multiple tasks were used per language, in order to be able to disentangle task effects and language effects. After all, if only one task were used per language, it would be impossible to know if any differences which are found are due to task or due to language (<xref ref-type="bibr" rid="bibr24-0265532212442647">Van den Bergh et al., 2009</xref>; <xref ref-type="bibr" rid="bibr25-0265532212442647">Van Weijen, 2009</xref>). All eight writing assignments were similar in terms of audience (peers), medium (a school-related magazine for secondary school students) and purpose (to convince the readers of your point of view), and differed only in terms of topic. An example of an assignment can be found in <xref ref-type="app" rid="app1-0265532212442647">Appendix A</xref>.</p>
<p>The available time for each essay was approximately 30 minutes, although participants were allowed to go on longer if they felt that their essays were not finished yet. No participant used more than 40 minutes. The students completed the essays during two separate days. Between the four tasks completed per day, participants were given a short break of about 10 to 15 minutes.</p>
<p>To avoid sequence effects and to control for effects of topic, several measures were taken. It has been established that topic can greatly influence the quality of writing (<xref ref-type="bibr" rid="bibr8-0265532212442647">Godshalk, Swineford &amp; Coffman, 1966</xref>; <xref ref-type="bibr" rid="bibr17-0265532212442647">Schoonen, 2005</xref>; <xref ref-type="bibr" rid="bibr25-0265532212442647">Van Weijen, 2009</xref>). Therefore, to disentangle language effects from topic effects, the topics were systematically balanced across languages, so that each topic occurred in both L1 and L2. So, writer 1 wrote on topics 1, 2, 3 and 4 in L1 and on topics 5, 6, 7, and 8 in L2, whereas writer 2 wrote on topics 2, 3, 4 and 5 in L1 and on topics 1, 6, 7 and 8 in L2, and so forth. The order in which L1 and L2 essays were written was also balanced across participants: 10 students first completed four L1 essays, and then wrote four L2 essays; the other 10 students first completed four L2 essays, and then wrote four L1 essays.</p>
<p>The essay set was randomly divided into eight subsamples. Each subsample contained 20 essays. As the complete essay set contained L1 (Dutch) and L2 (English) essays, so could – and did – each subsample. In addition, each subsample contained essays on various topics. After all, eight topics were used in the present study.</p>
</sec>
<sec id="section4-0265532212442647">
<title>Benchmark essays and scoring guide</title>
<p>Six benchmark essays were selected, one for each criterion per language. The positive effect of using benchmark essays on scale reliability was advocated and demonstrated by <xref ref-type="bibr" rid="bibr2-0265532212442647">Blok (1985)</xref>, <xref ref-type="bibr" rid="bibr11-0265532212442647">Kuhlemeier and Van den Bergh (1988)</xref>, <xref ref-type="bibr" rid="bibr12-0265532212442647">Purves (1992)</xref> and <xref ref-type="bibr" rid="bibr17-0265532212442647">Schoonen (2005)</xref>. The benchmark essays were selected from an essay set from a previous study (data collected by Rijlaarsdam &amp; Van den Bergh, 1996, University of Amsterdam, cf. <xref ref-type="bibr" rid="bibr5-0265532212442647">Couzijn, Van den Bergh &amp; Rijlaarsdam, 2002</xref>). The essays in this 1996 set were written on the basis of the same assignments as used in the present study, and by students of the same age as the participants in the current study. The benchmark essays represented the (approximate) average essay quality for the rating criterion in question. They were selected by two experienced raters, after elaborate inspection of both the 1996 essay set and the current essay set. All benchmark essays were essays on which the two raters were in agreement that they were of average quality. That is, they had to represent average quality for the specific rating criterion. In addition, they should not represent any extremes for the other two criteria. For example, essays which were average in terms of structure, but very poor or very good in terms of language use, could not be used as benchmarks for the ‘structure’ criterion (and neither for the language criterion). If benchmarks are of approximate average quality, this enhances the reliability of ratings. After all, if a benchmark essay represents an extreme on the scale (e.g. high quality), it becomes harder for the raters to rate the essays at the other extreme (e.g. low quality) reliably. As such, the suitability of the selected benchmarks is checked after the ratings by inspecting the reliabilities of the ratings.</p>
</sec>
<sec id="section5-0265532212442647">
<title>Ratings</title>
<p>Eight raters, excluding the two raters who selected the benchmark essays, were involved in this study. They were (near) native speakers of Dutch and English. They all had the Dutch nationality, but were also highly proficient in and familiar with the English language and its conventions, through years of personal and/or professional experience. All raters used English as a main language of communication during their education and work and were familiar with text conventions in both languages. They worked as teachers (one rater worked as a teacher of English at a regular school for secondary education and did a university major in English and had also lived in the UK; two raters taught in bilingual education) or in academic settings where English is used as one of the two main languages of professional communication (next to Dutch), both productively and receptively, on a daily basis. Three of them had also spent significant parts of their lives in English speaking communities (UK and USA, in this case). The raters were financially compensated for their work.</p>
<p>The essays were first rated on global quality. Global quality ratings should reflect the quality of each essay as a whole. They are, in other words, holistic ratings. <xref ref-type="bibr" rid="bibr17-0265532212442647">Schoonen (2005)</xref> demonstrated that holistic ratings (‘collected with essay scales’) have higher generalizability than analytic scores (‘with scoring guides’), thereby reinforcing <xref ref-type="bibr" rid="bibr28-0265532212442647">White’s (1984</xref>; <xref ref-type="bibr" rid="bibr29-0265532212442647">1985</xref>, as cited by <xref ref-type="bibr" rid="bibr27-0265532212442647">Weigle, 2002</xref>) claim that holistic ratings are preferred because of their validity. Because it has been observed that second language writers often develop different aspects of writing – such as grammar, style and argumentation – asynchronously (<xref ref-type="bibr" rid="bibr27-0265532212442647">Weigle, 2002</xref>, p. 120), the essays were also rated on two selected criteria, namely structure and language (see <xref ref-type="app" rid="app2-0265532212442647">Appendix B</xref> for definitions of all three rating criteria used in the present study). These two criteria are often used in assessment studies and are typically aspects on which L2 writing quality develops differently (cf. <xref ref-type="bibr" rid="bibr27-0265532212442647">Weigle, 2002</xref>, p. 120) than L1 writing quality. It should be noted that global quality comprises, among other aspects, structure and language (cf. <xref ref-type="bibr" rid="bibr1-0265532212442647">Bae and Bachman, 2010</xref>, who argue that ‘content’ comprises ‘coherence and organization’, the latter being similar or equal to the structure criterion in the presents study). It is therefore to be expected that ratings of global quality overlap with ratings of structure and language. Structure and language, on the other hand, are assumed to be separate aspects of writing quality. To minimize the possibility that the ratings for one criterion affect the ratings on the other criteria, the ratings were carried out in three different rounds (on different days, one round per rating criterion). During each new round, the ratings for previous criteria were no longer available to the raters.</p>
<p>In addition to a definition of each criterion, the raters were provided with an explanation of what was ‘average’ about the benchmark essay for the specific criterion in question, including passages from the benchmark essay (see <xref ref-type="app" rid="app3-0265532212442647">Appendix C</xref>). This procedure served to maximize interrater reliability. The raters had to award a score to each essay which expressed how much better or worse it was than the benchmark essay (cf. <xref ref-type="bibr" rid="bibr2-0265532212442647">Blok, 1985</xref>), which was given the randomly set score of 100. If an essay was awarded a score of 200, for example, this meant that the rater thought it was twice as good as the benchmark essay. If an essay received a score of 50, it meant that the rater thought it was half as good as the benchmark essay.</p>
<p>For efficiency reasons, we implemented a ‘design of overlapping rater teams’ (<xref ref-type="bibr" rid="bibr22-0265532212442647">Van den Bergh &amp; Eiting, 1989</xref>, p. 1). In such a design, the raters do not rate all the texts in the data set. Instead, each rater rates a randomly selected sample of texts (e.g. half of all the available texts, or 80 out of 100 available texts). By creating overlap, it is possible to estimate rater reliabilities. In the present study, each rater rated six out of the eight available subsamples. All raters worked in two conditions: in one condition they rated three subsamples (= 60 essays, on various topics, some written in L1, some written in L2) relative to an L1 benchmark essay (see <xref ref-type="table" rid="table1-0265532212442647">Table 1</xref>), while in the other condition they rated three other subsamples relative to an L2 benchmark essay. Each rater rates each essay only once (per criterion). That is, he or she never rates the same essay relative to both an L1 and an L2 benchmark.</p>
<table-wrap id="table1-0265532212442647" position="float">
<label>Table 1.</label>
<caption>
<p>Distribution of subsamples across raters. Each subsample contained both L1 and L2 essays. x = ratings with an L1 benchmark; o= ratings with an L2 benchmark</p>
</caption>
<graphic alternate-form-of="table1-0265532212442647" xlink:href="10.1177_0265532212442647-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left" colspan="8">Subsample<hr/></th>
</tr>
<tr>
<th/>
<th align="left">1</th>
<th align="left">2</th>
<th align="left">3</th>
<th align="left">4</th>
<th align="left">5</th>
<th align="left">6</th>
<th align="left">7</th>
<th align="left">8</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rater 1</td>
<td>x</td>
<td/>
<td/>
<td>o</td>
<td>o</td>
<td>o</td>
<td>x</td>
<td>x</td>
</tr>
<tr>
<td>Rater 2</td>
<td>o</td>
<td>o</td>
<td/>
<td/>
<td>x</td>
<td>x</td>
<td>x</td>
<td>o</td>
</tr>
<tr>
<td>Rater 3</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td/>
<td/>
<td>o</td>
<td>o</td>
<td>o</td>
</tr>
<tr>
<td>Rater 4</td>
<td>o</td>
<td>o</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td/>
<td/>
<td>o</td>
</tr>
<tr>
<td>Rater 5</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td>o</td>
<td>o</td>
<td>o</td>
<td/>
<td/>
</tr>
<tr>
<td>Rater 6</td>
<td/>
<td>o</td>
<td>o</td>
<td>o</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td/>
</tr>
<tr>
<td>Rater 7</td>
<td/>
<td>x</td>
<td>o</td>
<td>x</td>
<td>o</td>
<td/>
<td>o</td>
<td>x</td>
</tr>
<tr>
<td>Rater 8</td>
<td>o</td>
<td/>
<td>o</td>
<td>x</td>
<td/>
<td>x</td>
<td>o</td>
<td>x</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Each essay was rated by three raters relative to an L1 benchmark (on each of the three rating criteria – where the ratings of different criteria took place on different days), and relative to an L2 benchmark by three other raters.</p>
<p>For example, participant 1’s essay on the topic ‘camera surveillance’ belonged to subsample 3. As can be inferred from <xref ref-type="table" rid="table1-0265532212442647">Table 1</xref>, this essay was rated relative to an L1 benchmark by raters 3, 4 and 5 (who form a jury), on all three rating criteria (global, structure, language). It was rated relative to an L2 benchmark by raters 6, 7 and 8 (who form a jury), again on all three rating criteria.</p>
<p>The order in which benchmarks were used was balanced across raters: half of the raters (raters 1, 3, 5 and 7) performed the ratings with the L1 benchmarks first, and the ratings with the L2 benchmarks second; the other half (raters 2, 4, 6 and 8) performed the ratings with the L2 benchmarks first and the ratings with the L1 benchmarks second.</p>
<p><xref ref-type="app" rid="app4-0265532212442647">Appendix D</xref> contains a procedural manual with a step-by-step description of the rating procedure described above.</p>
</sec>
</sec>
<sec id="section6-0265532212442647">
<title>Analyses</title>
<p>Confirmatory factor analysis was conducted using Lisrel 8.16 to examine the latent structure of the ratings in both conditions (L1 and L2 benchmarks) simultaneously. Both invariant as well as variant restrictions were placed on the parameters (see <xref ref-type="fig" rid="fig1-0265532212442647">Figure 1</xref>). The three main steps in the analysis are described below.</p>
<fig id="fig1-0265532212442647" position="float">
<label>Figure 1.</label>
<caption>
<p>Invariant and variant restrictions imposed within and across conditions, r = rater</p>
</caption>
<graphic xlink:href="10.1177_0265532212442647-fig1.tif"/>
</fig>
<sec id="section7-0265532212442647">
<title>Restrictions of invariance across subsamples, within benchmark languages</title>
<p>Restrictions of invariance are imposed on parameters within the two conditions, that is, the sets of essays rated with L1 benchmarks and the set of essays rated with L2 benchmarks. As each subsample in the essay set was randomly assembled of 20 (L1 and L2) essays and the raters did not know to which subsample an essay belonged (and were unaware that there were any subsamples at all), it follows that raters should rate all subsamples in the same manner (i.e. with equal reliability) for each of the three subsamples which he or she rates per benchmark language. Within classical test theory, differences between subsamples (and within raters) can be interpreted as random error (cf. <xref ref-type="bibr" rid="bibr22-0265532212442647">Van den Bergh &amp; Eiting, 1989</xref>).Therefore, invariant restrictions are imposed on ratings of the three subsamples per benchmark language performed by each individual rater. That is, the ratings of each of the three subsamples (or groups: <italic>g</italic>) per benchmark language by a rater (<italic>r</italic> = 1, 2, …, 8) have an equal regression on the true scores <inline-formula id="inline-formula1-0265532212442647">
<mml:math display="inline" id="math1-0265532212442647">
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mi>r</mml:mi>
<mml:msup>
<mml:mi>g</mml:mi>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mi>r</mml:mi>
<mml:msup>
<mml:mi>g</mml:mi>
<mml:mo>″</mml:mo>
</mml:msup>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> and an equal error variance <inline-formula id="inline-formula2-0265532212442647">
<mml:math display="inline" id="math2-0265532212442647">
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mrow>
<mml:mi>ε</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mrow>
<mml:mi>ε</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:msup>
<mml:mi>g</mml:mi>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mrow>
<mml:mi>ε</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:msup>
<mml:mi>g</mml:mi>
<mml:mo>″</mml:mo>
</mml:msup>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. These two parameters together indicate the reliability of each rater (<italic>ρ = λ</italic><sup><italic>2</italic></sup>/[<italic>λ</italic><sup><italic>2</italic></sup> <italic>+ θ</italic><sub><italic>ϵ</italic></sub>]). These invariant restrictions hold within rating criteria.</p>
</sec>
<sec id="section8-0265532212442647">
<title>Measurement (in)variance across benchmark languages</title>
<p>The next step is to evaluate if raters are able to carry out ratings similarly across benchmark languages. If it turns out that they are, then there is no indication that the raters rated differently for different languages. Such a result therefore allows for comparison of L1 and L2 essay scores, as obtained from the current rating procedure. To test whether ratings are indeed stable across benchmark languages, a test of measurement invariance (<xref ref-type="bibr" rid="bibr10-0265532212442647">Jöreskog, 1971</xref>) is conducted: do raters rate the same construct, in the same way (i.e. with equal reliability), no matter the language of the benchmark essay? A χ2 statistic is used to evaluate the absolute fit and the difference in fit of five nested models posing increasing numbers of restrictions across benchmarks.</p>
<p>The first model is the <italic>non-congeneric</italic> model. This model allows for measurement variance: the correlation between scores on global quality, structure and language is different if the benchmark essay has a different language. If this model fits the data, it means that using a benchmark essay in a different language affects rater’s conceptions of the construct to be assessed. The next four models are all models of measurement invariance.</p>
<p>In the <italic>congeneric</italic> model, using a benchmark essay in a different language does not influence rater’s conceptions of the construct to be assessed. Correlations between true scores on global quality, structure and language are invariant across benchmark languages, i.e. the correlations between ratings of global quality (<italic>gq</italic>), structure (<italic>s</italic>) and language (<italic>l</italic>) are equal in the L1 benchmark and L2 benchmark condition. Note that this implies restrictions on the correlations between different rating criteria across benchmark languages, for instance <inline-formula id="inline-formula3-0265532212442647">
<mml:math display="inline" id="math3-0265532212442647">
<mml:mrow>
<mml:msubsup>
<mml:mi>ψ</mml:mi>
<mml:mrow>
<mml:mtext>gq</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>L</mml:mtext>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mtext>s</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>L</mml:mtext>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mtext>g</mml:mtext>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>ψ</mml:mi>
<mml:mrow>
<mml:mtext>s</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>L</mml:mtext>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mtext>gq</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>L</mml:mtext>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mtext>g</mml:mtext>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>. The congeneric model does allow for differences in regressions on true scores (<italic>λ</italic>) and error score variances (<italic>θ</italic><sub><italic>ϵ</italic></sub>). Although the reliability of each rater is equal across the three subsamples per benchmark language, this model involves two reliabilities per rater: one for essays rated with L1 benchmarks, one for essays rated with L2 benchmarks.</p>
<p>The <italic>tau-equivalent</italic> model is more restrictive. In this model, both correlations and true score variance are equal across benchmark languages. Therefore, the regression of the observed scores on each of the three rating criteria (global quality (<italic>gq</italic>), structure (<italic>s</italic>) and language (<italic>l</italic>)) on the true scores is equal across benchmark languages, for each rater. In other words, the regression of the rating criterion ‘global quality’ on the true scores with L1 benchmarks is equal to the regression of the rating criterion ‘global quality’ on the true scores with L2 benchmarks (i.e. <inline-formula id="inline-formula4-0265532212442647">
<mml:math display="inline" id="math4-0265532212442647">
<mml:mrow>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>,</mml:mo>
<mml:mi>g</mml:mi>
<mml:mi>q</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>L</mml:mi>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>,</mml:mo>
<mml:mi>g</mml:mi>
<mml:mi>q</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>L</mml:mi>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>). In the tau-equivalent model, error variance is not identical across benchmark languages. This could be the case, for example, if raters find rating essays which are in a different language than the benchmark essays (i.e. L1 essays with an L2 benchmark or L2 essays with an L1 benchmark) more difficult than rating essays in the same language as the benchmark.</p>
<p>The <italic>parallel_bm</italic> model is even more restrictive. In addition to equality of correlations and true score variance, it assumes equal error score variance across benchmark languages (i.e. <inline-formula id="inline-formula5-0265532212442647">
<mml:math display="inline" id="math5-0265532212442647">
<mml:mrow>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>L</mml:mi>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>L</mml:mi>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>). Regression on the true score (<italic>λ</italic>) and error score (<italic>θ</italic>) together indicate the reliability of each rater (<italic>ρ = λ</italic><sup><italic>2</italic></sup><italic>/[λ</italic><sup><italic>2</italic></sup> <italic>+ θ</italic><sub><italic>ϵ</italic></sub> <italic>]</italic>). It follows, then, that in the parallel_bm model, individual rater reliabilities are invariant across benchmark languages (but may be variant across the rating criteria within benchmark languages, e.g. the reliabilities for ‘global quality’ with L1 benchmarks may be different from the reliabilities for the ‘structure’ criterion rated with L1 benchmarks). Note that the parallel_bm model, while disallowing varying reliabilities within raters, does allow for different reliabilities between raters.</p>
<p>The final <italic>parallel_rc</italic> model (‘rc’ for rating criteria) is an extended version of the parallel_bm model, which imposes yet another restriction. In this model, regressions on the true score and variance of error scores are not only invariant across benchmark languages, but also across rating criteria (i.e. <inline-formula id="inline-formula6-0265532212442647">
<mml:math display="inline" id="math6-0265532212442647">
<mml:mrow>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>g</mml:mi>
<mml:mi>q</mml:mi>
</mml:mrow>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>λ</mml:mi>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
<mml:mi>g</mml:mi>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula7-0265532212442647">
<mml:math display="inline" id="math7-0265532212442647">
<mml:mrow>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>g</mml:mi>
<mml:mi>q</mml:mi>
</mml:mrow>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mi>g</mml:mi>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mi>θ</mml:mi>
<mml:mrow>
<mml:mi>r</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
<mml:mi>g</mml:mi>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> where ‘g’ is <italic>global quality</italic>, ‘s’ is <italic>structure</italic>, ‘l’ is <italic>language</italic>). That is, for each rater, the reliability of the rating of global quality is equal to the reliabilities of the ratings of structure and language, in either language.</p>
<p>Comparison of observed L1 and L2 scores as collected with L1 and L2 benchmarks is warranted under two conditions. First, the ratings with L1 and L2 benchmarks should be parallel tests (i.e. the parallel_bm or parallel_rc model is accepted). After all, only then can we assume that the scores are ‘represented by numbers on the same scale’ (<xref ref-type="bibr" rid="bibr10-0265532212442647">Jöreskog, 1971</xref>, p. 109). Second, the ratings must have been carried out with sufficient reliability in order for comparisons to be meaningful. Rater and jury reliabilities, as estimated within the preferred model, must therefore be inspected. Whereas the degree to which the three rating criteria (global quality, structure and language) correlate is not directly relevant for answering the main question of the present study (i.e. the strength of these correlations is not relevant for deciding whether ratings with L1 and L2 benchmarks are parallel tests – the correlations only need to be similar, not necessarily high or low), it is nevertheless interesting to inspect these correlations. After all, the degree to which these three criteria correlate may (partly) reflect their validity. Although it is to be expected that overall quality is related to both structure and language to some degree, for it to be valid to distinguish between these three criteria, the correlations between the ratings of the three criteria should not equal 1. This applies particularly to the correlation between the ratings of structure and language.</p>
</sec>
<sec id="section9-0265532212442647">
<title>Comparison of L1 and L2 essay scores</title>
<p>If a parallel model is found to best fit the observed data, and if the ratings are found to have been carried out with sufficient reliability, then a comparison of L1 and L2 essay scores is warranted. As the main question of the present study is to test the usability of the applied rating procedure with L1 and L2 benchmarks and raters who are (near) native speakers of both the L1 and the L2 for quantifying the quality difference between L1 and L2 essays, it is also of interest to investigate if it discriminates between L1 and L2 essays.</p>
<p>The size and significance level of the difference between L1 and L2 scores was established by submitting them to multilevel regression analysis, in which the language of the essay is the predictor variable. This model provides a distinction between the variance due to participant (i.e. differences between the averages of each writer), the variance due to topic (i.e. differences between topics) and residual variance (i.e. random error). The L2 scores are treated as the intercept, relative to which deviations due to a different language (i.e. essays in the L1) are modeled.</p>
</sec>
</sec>
</sec>
<sec id="section10-0265532212442647" sec-type="results">
<title>Results</title>
<sec id="section11-0265532212442647">
<title>Measurement (in)variance across benchmark languages</title>
<p><xref ref-type="table" rid="table2-0265532212442647">Table 2</xref> features chi-square statistics for the absolute fit of the five tested models, as well as for the comparison of the fit of the five models. The absolute fit of four of the five models is satisfactory. The non-congeneric model, the congerenic model, the tau-equivalent model and the parallel_bm model are good fits to the data: none of the models significantly deviate from the observed data (<italic>p</italic> &gt; .05 for all models). Therefore, there is no reason to assume that the models do not fit the data. The parallel_rc model, however, does not fit the data (<italic>p</italic> &lt; .001). Comparison of the five models shows that the fit of the congeneric model (which imposes the restriction that correlations between rating criteria should be equal across benchmark languages) is not significantly (<italic>p</italic> = .897) worse than the fit of the non-congeneric model (which does not impose any restrictions of invariance on the correlations between rating criteria). Similarly, moving from the congeneric model to the more restrictive tau-equivalent model does not significantly decrease the fit (<italic>p</italic> = .997). The same applies for the parallel_bm model: though it is more restrictive than the tau-equivalent model, its fit is not significantly worse than the fit of the tau-equivalent model (<italic>p</italic> = .912). The parallel_rc model, however, fits the data less well than the parallel_bm model (<italic>p</italic> = .001). As the parallel_rc model fits the data poorly, and because there is a significant difference in fit when compared to the other models, this model cannot be accepted. In other words, the assumption must be rejected that true and error score variance is equal across rating criteria. Therefore, the parallel_bm model in this case provides the most parsimonious description of the data.</p>
<table-wrap id="table2-0265532212442647" position="float">
<label>Table 2.</label>
<caption>
<p>Absolute fit and comparison of nested models testing the degree of measurement invariance between L1 and L2 benchmark essays</p>
</caption>
<graphic alternate-form-of="table2-0265532212442647" xlink:href="10.1177_0265532212442647-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Absolute fit of the five models</th>
<th align="left"><italic>χ</italic><sup><italic>2</italic></sup></th>
<th align="left"><italic>df</italic></th>
<th align="left"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Non-congeneric</td>
<td>1231.02</td>
<td>1152</td>
<td>.052</td>
</tr>
<tr>
<td>Congeneric</td>
<td>1320.89</td>
<td>1260</td>
<td>.11</td>
</tr>
<tr>
<td>Tau-equivalent</td>
<td>1330.31</td>
<td>1284</td>
<td>.18</td>
</tr>
<tr>
<td>Parallel_bm</td>
<td>1345.60</td>
<td>1308</td>
<td>.089</td>
</tr>
<tr>
<td>Parallel_rc</td>
<td>1779.10</td>
<td>1354</td>
<td>.001</td>
</tr>
<tr>
<th align="left">Comparison of models</th>
<th align="left"><italic>χ</italic><sup><italic>2</italic></sup></th>
<th align="left"><italic>df</italic></th>
<th align="left"><italic>p</italic></th>
</tr>
<tr>
<td>Non-congeneric vs. congeneric</td>
<td>89.87</td>
<td>108</td>
<td>.897</td>
</tr>
<tr>
<td>Congeneric vs. tau-equivalent</td>
<td>9.42</td>
<td>24</td>
<td>.997</td>
</tr>
<tr>
<td>Tau-equivalent vs. parallel_bm</td>
<td>15.29</td>
<td>24</td>
<td>.912</td>
</tr>
<tr>
<td>Parallel_bm vs. parallel_rc</td>
<td>433.50</td>
<td>46</td>
<td>.001</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>As the parallel_bm model is accepted, it may be assumed that the distribution of scores across L1 and L2 essays, as allocated by the raters, is similar, no matter the language of the benchmark essay. For all eight raters, the regression on the true score and residual variance, and thus reliabilities, are equal across benchmark languages. There is, in other words, no evidence to assume that the raters were unable to apply rating standards equally across languages. This result therefore allows for comparison of the scores of L1 and L2 essays.</p>
<sec id="section12-0265532212442647">
<title>Parameters estimated within the parallel_bm model: rater reliabilities and correlations between criteria</title>
<p>For the comparisons to be meaningful, it is essential that the ratings are carried out with sufficient reliability. All eight subsamples were rated by three raters for each benchmark language, who, in effect, form a jury. The jury number equals the number of the subsample which was rated by that particular group of raters. Hence, jury 1 for L1 benchmarks consists of different raters than jury 1 for L2 benchmarks. <xref ref-type="table" rid="table3-0265532212442647">Table 3</xref> shows the jury reliabilities, as estimated within the preferred parallel_bm model, for each rating criterion. In the L1 benchmark condition, the jury who rated subsample 1, for example, performed the rating for global quality with a reliability of .70. In the L2 benchmark condition, the jury who rated subsample 1 (consisting of three different raters than in the L1 benchmark condition) performed the global quality rating with a reliability of .71. Overall, 32 out of the 48 calculated jury reliabilities exceed .70, and the average jury reliability is .72. Only four reliabilities are lower than .60. In short, the ratings were performed with adequate reliability.</p>
<table-wrap id="table3-0265532212442647" position="float">
<label>Table 3.</label>
<caption>
<p>Jury reliabilities for L1 and L2 benchmarks (b.m.), as estimated within the preferred parallel model, as well as standardized regressions on the true scores (λ; standard errors between brackets) and on individual rater reliabilities (<italic>ρ</italic>), presented per rating criterion</p>
</caption>
<graphic alternate-form-of="table3-0265532212442647" xlink:href="10.1177_0265532212442647-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="4">L1 b.m.<hr/></th>
<th align="left" colspan="4">L2 b.m.<hr/></th>
</tr>
<tr>
<th align="left">Jury</th>
<th align="left">Global</th>
<th align="left">Structure</th>
<th align="left">Language</th>
<th align="left">Jury</th>
<th align="left">Global</th>
<th align="left">Structure</th>
<th align="left">Language</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>.76</td>
<td>.76</td>
<td>.81</td>
<td>1</td>
<td>.68</td>
<td>.65</td>
<td>.74</td>
</tr>
<tr>
<td>2</td>
<td>.80</td>
<td>.71</td>
<td>.80</td>
<td>2</td>
<td>.73</td>
<td>.68</td>
<td>.79</td>
</tr>
<tr>
<td>3</td>
<td>.74</td>
<td>.71</td>
<td>.85</td>
<td>3</td>
<td>.75</td>
<td>.51</td>
<td>.70</td>
</tr>
<tr>
<td>4</td>
<td>.71</td>
<td>.60</td>
<td>.71</td>
<td>4</td>
<td>.75</td>
<td>.67</td>
<td>.78</td>
</tr>
<tr>
<td>5</td>
<td>.73</td>
<td>.68</td>
<td>.79</td>
<td>5</td>
<td>.78</td>
<td>.72</td>
<td>.73</td>
</tr>
<tr>
<td>6</td>
<td>.73</td>
<td>.60</td>
<td>.72</td>
<td>6</td>
<td>.76</td>
<td>.76</td>
<td>.81</td>
</tr>
<tr>
<td>7</td>
<td>.76</td>
<td>.73</td>
<td>.74</td>
<td>7</td>
<td>.76</td>
<td>.63</td>
<td>.74</td>
</tr>
<tr>
<td>8</td>
<td>.74</td>
<td>.64</td>
<td>.65</td>
<td>8</td>
<td>.74</td>
<td>.76</td>
<td>.83</td>
</tr>
<tr>
<th align="left" colspan="4">Global quality<hr/></th>
<th align="left" colspan="2">Structure<hr/></th>
<th align="left" colspan="2">Language<hr/></th>
</tr>
<tr>
<th align="left" colspan="2">Rater</th>
<th align="left">λ (se)</th>
<th align="left"><italic>ρ</italic></th>
<th align="left">λ (se)</th>
<th align="left"><italic>ρ</italic></th>
<th align="left">λ (se)</th>
<th align="left"><italic>ρ</italic></th>
</tr>
<tr>
<td colspan="2">1</td>
<td>.69 (.11)</td>
<td>.48</td>
<td>.76 (.11)</td>
<td>.58</td>
<td>.65 (.11)</td>
<td>.42</td>
</tr>
<tr>
<td colspan="2">2</td>
<td>.74 (.11)</td>
<td>.55</td>
<td>.78 (.11)</td>
<td>.61</td>
<td>.70 (.12)</td>
<td>.49</td>
</tr>
<tr>
<td colspan="2">3</td>
<td>.75 (.11)</td>
<td>.56</td>
<td>.74 (.11)</td>
<td>.55</td>
<td>.85 (.12)</td>
<td>.72</td>
</tr>
<tr>
<td colspan="2">4</td>
<td>.60 (.12)</td>
<td>.36</td>
<td>.63 (.12)</td>
<td>.40</td>
<td>.79 (.12)</td>
<td>.62</td>
</tr>
<tr>
<td colspan="2">5</td>
<td>.72 (.12)</td>
<td>.52</td>
<td>.63 (.11)</td>
<td>.40</td>
<td>.79 (.11)</td>
<td>.62</td>
</tr>
<tr>
<td colspan="2">6</td>
<td>.72 (.11)</td>
<td>.52</td>
<td>.48 (.13)</td>
<td>.23</td>
<td>.75 (.12)</td>
<td>.56</td>
</tr>
<tr>
<td colspan="2">7</td>
<td>.79 (.11)</td>
<td>.62</td>
<td>.63 (.12)</td>
<td>.40</td>
<td>.62 (.12)</td>
<td>.38</td>
</tr>
<tr>
<td colspan="2">8</td>
<td>.59 (.12)</td>
<td>.35</td>
<td>.39 (.13)</td>
<td>.15</td>
<td>.59 (.13)</td>
<td>.35</td>
</tr>
</tbody>
</table>
</table-wrap>
<p><xref ref-type="table" rid="table3-0265532212442647">Table 3</xref> also features individual rater reliabilities, although they are less relevant than the jury reliabilities (cf. <xref ref-type="bibr" rid="bibr7-0265532212442647">Gebril, 2009</xref>; <xref ref-type="bibr" rid="bibr14-0265532212442647">Raymond, 1982</xref>; <xref ref-type="bibr" rid="bibr17-0265532212442647">Schoonen, 2005</xref>; <xref ref-type="bibr" rid="bibr22-0265532212442647">Van den Bergh &amp; Eiting, 1989</xref>; <xref ref-type="bibr" rid="bibr27-0265532212442647">Weigle, 2002</xref>, who all indicated that assessments by single raters are insufficient). <italic>λ</italic> indicates raters’ standardized regressions (all of them significant: <italic>λ &gt; 2*se</italic>) on the true scores, that is, the degree to which the observed scores, as assigned by the raters, relate to true scores.<sup><xref ref-type="fn" rid="fn1-0265532212442647">1</xref></sup> <italic>ρ</italic> reflects individual rater reliabilities. We can infer, for example, that rater 1 performed the ratings for global quality with a reliability of .48, indicating that this rater agreed only moderately with other raters.</p>
<p>The disattenuated<sup><xref ref-type="fn" rid="fn2-0265532212442647">2</xref></sup> correlations between the three rating criteria (global quality, structure and language) are shown in <xref ref-type="table" rid="table4-0265532212442647">Table 4</xref>. All correlations are strong, suggesting that the rating criteria were not fully distinguishable. Since global quality comprises structure and language (plus some other criteria), reasonably strong correlations between the ratings of global quality one the one hand and structure and language on the other hand were expected. Nevertheless, the perfect correlation between global quality and structure is a striking finding, which will be revisited in the Discussion.</p>
<table-wrap id="table4-0265532212442647" position="float">
<label>Table 4.</label>
<caption>
<p>Disattenuated correlations between rating criteria (standard errors between brackets)</p>
</caption>
<graphic alternate-form-of="table4-0265532212442647" xlink:href="10.1177_0265532212442647-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Global quality</th>
<th align="left">Structure</th>
<th align="left">Language</th>
</tr>
</thead>
<tbody>
<tr>
<td>Global quality</td>
<td>1</td>
<td/>
<td/>
</tr>
<tr>
<td>Structure</td>
<td>1 (.05)</td>
<td>1</td>
<td/>
</tr>
<tr>
<td>Language</td>
<td>.89 (.06)</td>
<td>.75 (.08)</td>
<td>1</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
</sec>
<sec id="section13-0265532212442647">
<title>Comparison of L1 and L2 essay scores</title>
<p>Now that parallelism and sufficient reliability have been established, the observed L1 and L2 essay scores can be compared: the quality difference between Dutch and English essays can be quantified. The results of this analysis are presented in <xref ref-type="table" rid="table5-0265532212442647">Table 5</xref> for each of the three rating criteria.</p>
<table-wrap id="table5-0265532212442647" position="float">
<label>Table 5.</label>
<caption>
<p>Regression results for the difference between L1 (Dutch) and L2 (English) essay scores, per benchmark language: intercepts (L2 scores), gradients (L1 scores; <italic>b</italic> value), standard errors (se), and variances (<italic>s</italic><sup><italic>2</italic></sup>) due to participant and topic of the assignment as well as residual variance (<italic>p</italic> &lt; 0.05)</p>
</caption>
<graphic alternate-form-of="table5-0265532212442647" xlink:href="10.1177_0265532212442647-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="7">L1 benchmark<hr/></th>
</tr>
<tr>
<th/>
<th align="left" colspan="2">Global quality<hr/></th>
<th align="left" colspan="2">Structure<hr/></th>
<th align="left" colspan="2">Language<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Estimate</th>
<th align="left">(se)</th>
<th align="left">Estimate</th>
<th align="left">(se)</th>
<th align="left">Estimate</th>
<th align="left">(se)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intercept (L2 essays)</td>
<td>102.00</td>
<td>8.67</td>
<td>94.80</td>
<td>7.27</td>
<td>79.96</td>
<td>4.50</td>
</tr>
<tr>
<td>b (deviation due to L1 essays)</td>
<td>44.08</td>
<td>8.69</td>
<td>23.36</td>
<td>5.49</td>
<td>40.42</td>
<td>3.61</td>
</tr>
<tr>
<td><italic>s</italic><sup><italic>2</italic></sup> (participants)</td>
<td>898.57</td>
<td>355.88</td>
<td>428.04</td>
<td>184.13</td>
<td>274.34</td>
<td>107.66</td>
</tr>
<tr>
<td><italic>s</italic><sup><italic>2</italic></sup> (topics)</td>
<td>62.28</td>
<td>79.94</td>
<td>131.27</td>
<td>67.15</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><italic>s</italic><sup><italic>2</italic></sup> (residue)</td>
<td>1780.58</td>
<td>218.67</td>
<td>1192.76</td>
<td>146.46</td>
<td>521.72</td>
<td>62.36</td>
</tr>
<tr>
<th align="left" colspan="7">L2 benchmark<hr/></th>
</tr>
<tr>
<th/>
<th align="left" colspan="2">Global quality<hr/></th>
<th align="left" colspan="2">Structure<hr/></th>
<th align="left" colspan="2">Language<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Estimate</th>
<th align="left">(se)</th>
<th align="left">Estimate</th>
<th align="left">(se)</th>
<th align="left">Estimate</th>
<th align="left">(se)</th>
</tr>
<tr>
<td>Intercept (L2 essays)</td>
<td>105.49</td>
<td>8.97</td>
<td>93.30</td>
<td>5.79</td>
<td>93.86</td>
<td>6.94</td>
</tr>
<tr>
<td>b (deviation due to L1 essays)</td>
<td>40.59</td>
<td>6.24</td>
<td>18.79</td>
<td>4.54</td>
<td>55.01</td>
<td>6.21</td>
</tr>
<tr>
<td><italic>s</italic><sup><italic>2</italic></sup> (participants)</td>
<td>1019.59</td>
<td>384.20</td>
<td>294.74</td>
<td>126.46</td>
<td>558.30</td>
<td>238.80</td>
</tr>
<tr>
<td><italic>s</italic><sup><italic>2</italic></sup> (topics)</td>
<td>80.60</td>
<td>32.29</td>
<td>68.44</td>
<td>33.85</td>
<td>76.64</td>
<td>33.85</td>
</tr>
<tr>
<td><italic>s</italic><sup><italic>2</italic></sup> (residue)</td>
<td>1544.12</td>
<td>189.57</td>
<td>816.51</td>
<td>100.28</td>
<td>1540.54</td>
<td>189.32</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0265532212442647"><p><italic>Notes</italic>:</p></fn>
<fn id="table-fn2-0265532212442647"><p>(1) The different intercept values between criteria cannot be interpreted. The three criteria were rated on three different occasions and each criterion had its own benchmark essay. In addition, the fact that the parallel_rc model was a poor fit of the data, indicates that the ratings of the three different criteria are not parallel tests. Their scales are therefore not comparable.</p></fn>
<fn id="table-fn3-0265532212442647"><p>(2) As ratings with L1 and L2 benchmarks have been established to be parallel tests, the differences between values of intercepts, gradients and variances are (if compared per rating criterium) not significantly different.</p></fn>
</table-wrap-foot></table-wrap>
<p>The positive regression weights (<italic>b</italic>-values) overall indicate that L1 texts have, on average, significantly higher scores than L2 texts for each criterion. When rated with L1 benchmarks, for example, L1 essays are on average awarded scores 44.08 points higher on global quality than L2 essays. For all other criteria, too, the L1 scores are (on average) higher than the L2 scores. Effect sizes (<italic>Cohen’s d)</italic> are reported in <xref ref-type="table" rid="table6-0265532212442647">Table 6</xref>, indicating the size of the difference between L1 and L2 essay scores relative to the variance due to participant (<italic>ES</italic><sub><italic>participant</italic></sub>), variance due to topic (<italic>ES</italic><sub><italic>topic</italic></sub>) and total variance (<italic>ES</italic><sub><italic>total</italic></sub>). For example, ES<sub>total</sub> (global quality, L1 benchmark) equals 44.08/√(898.57+62.28+1780.58), equals 44.08/√2741.43 = 44.08/52.36, equals 0.84. The size of the L1–L2 score difference relative to the total variance ranges between .55 and 1.43. This indicates that, while score variances are large, there is a (relatively) large added L2 effect. It is also interesting to observe that the L1–L2 difference adds substantially to the essay score differences between participants (<italic>Cohen’s d</italic> &gt; 1 in all cases) and between topics (<italic>Cohen’s d</italic> = 5.59 (global quality, L1 benchmark), 0.73 (structure, L1 benchmark), 4.52 (global quality, L2 benchmark), 8.28 (structure, L2 benchmark), 6.28 (language, L2 benchmark)). From this, it can be inferred, for example, that the difference between L1 and L2 essay scores for global quality is the size of approximately five standard deviations of the score differences due to topic. So, although there are variations according to rating criterion and benchmark language, the added language effect on essay scores is very large overall. In short, the applied rating procedure – with L1 and L2 benchmarks and raters who are (near) native speakers of both the L1 and the L2 – discriminates between L1 and L2 essays and quantifies the quality difference between the two.</p>
<table-wrap id="table6-0265532212442647" position="float">
<label>Table 6.</label>
<caption>
<p>Effect sizes (<italic>Cohen’s d</italic>) indicating the substantiveness of the difference between L1 and L2 essay scores relative to the variance due to participant (ES<sub>participant</sub>), variance due to topic (ES<sub>topic</sub>) and total variance (ES<sub>total</sub>). Effects sizes are presented per rating criterion and benchmark language.</p>
</caption>
<graphic alternate-form-of="table6-0265532212442647" xlink:href="10.1177_0265532212442647-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left" colspan="3">L1 benchmark<hr/></th>
<th align="left" colspan="3">L2 benchmark<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Global quality</th>
<th align="left">Structure</th>
<th align="left">Language</th>
<th align="left">Global quality</th>
<th align="left">Structure</th>
<th align="left">Language</th>
</tr>
</thead>
<tbody>
<tr>
<td>ES<sub>participant</sub></td>
<td>1.47</td>
<td>1.13</td>
<td>2.44</td>
<td>1.27</td>
<td>1.09</td>
<td>2.33</td>
</tr>
<tr>
<td>ES<sub>topic</sub></td>
<td>5.59</td>
<td>0.73</td>
<td>–</td>
<td>4.52</td>
<td>8.28</td>
<td>6.28</td>
</tr>
<tr>
<td>ES<sub>total</sub></td>
<td>0.84</td>
<td>0.56</td>
<td>1.43</td>
<td>0.79</td>
<td>0.55</td>
<td>1.18</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0265532212442647"><p><italic>Note</italic>: As no variance was established between scores on the ‘language’ criterion with L1 benchmarks (see <xref ref-type="table" rid="table5-0265532212442647">Table 5</xref>) due to topic, no effect size is presented for this condition in <xref ref-type="table" rid="table6-0265532212442647">Table 6</xref>.</p></fn>
</table-wrap-foot></table-wrap>
</sec>
</sec>
<sec id="section14-0265532212442647" sec-type="discussion">
<title>Discussion</title>
<p>To be able to quantify quality differences between L1 and L2 texts, the quality of both L1 and L2 texts must be expressed on the same scale. A method to achieve such a single scale and enable direct comparisons of L1 and L2 texts was, however, not yet available. In the present study, it was investigated whether a rating method with L1 and L2 benchmark essays and raters who are (near) native speakers of both L1 and L2 is suitable for direct comparisons of the rated quality of L1 and L2 essays. This was done by comparing the fit of five pre-specified nested models which all test the degree of measurement invariance across the L1 and L2 benchmark essays, and where each higher-level model imposes more restrictions of invariance across benchmark languages. It was found that there was no evidence against accepting the highly restrictive parallel_bm model, which means that ratings with L1 and L2 benchmarks may be assumed to be parallel tests. This indicates that the distribution of scores across L1 and L2 essays, as allocated by the raters, is similar, regardless of the language of the benchmark essay. In other words, there is no evidence to assume that the raters were unable to apply rating standards equally across languages. In addition, the raters performed their ratings with adequate levels of reliability. Therefore, the ratings of L1 and L2 essays are directly comparable and the quality difference (in terms of the allocated ratings) can be expressed. It was found that L1 essays received, on average, significantly higher ratings than L2 essays, indicating that the L1 essays are, in this specific population at least, of higher quality than the L2 essays.</p>
<p>The selection of raters who are (near) native users of L1 and L2 is essential to the creation of parallel ratings. Although the raters’ Dutch (L1) and English (L2) language proficiency was not measured objectively in the present study and a minimum level of language proficiency can therefore not be specified, it seems possible to list some features which all raters held in common. In the present study, raters who were capable of performing parallel ratings with L1 and L2 benchmarks answered the following criteria: a) they used L1 and L2 on a daily basis (L2 mostly for professional communication); b) they had jobs which involved dealing with texts and were as such familiar with text conventions in both languages; c) they had had an academic (linguistic) education; d) they had at least 18 years of experience with both languages. Most of them were probably slightly more proficient in Dutch (the present study’s L1) than in English (the present study’s L2). Apparently, this minor imbalance was unproblematic in the creation of parallel rating scales.</p>
<p>While estimating the effect of language (i.e. L1 or L2) on the rated quality of essays, relatively large participant- and task-related score variances were found, in addition to large residual variance. Although the variance between different assignments (<italic>s</italic><sup><italic>2</italic></sup> <italic>(topic)</italic>) is smaller than participant (<italic>s</italic><sup><italic>2</italic></sup> <italic>(participant)</italic>) and error variances (<italic>s</italic><sup><italic>2</italic></sup> <italic>(residue)</italic>), it is still quite large. This large between-assignment variance, which is all the more striking as the tasks in the present study are highly similar, indicates that measurements with few assignments are unreliable representations of writing skill. Based on the estimates in <xref ref-type="table" rid="table5-0265532212442647">Table 5</xref>, the reliability of a measurement of, for example, global quality with only one assignment (topic) has a reliability of only .33 if an L1 benchmark is used and .39 if an L2 benchmark is used. If global quality is assessed with four assignments per writer, the reliabilities of the obtained scores are .66 (L1 benchmark) and .72 (L2 benchmark). If eight assignments are used per writer, the reliabilities with which differences between writers can be established are .80 (L1 benchmark) and .83 (L2 benchmark). In other words, measurements of writing skill with few tasks are likely to be non-representative. <xref ref-type="bibr" rid="bibr21-0265532212442647">Van den Bergh (1988b)</xref> argues that writing assessments on the basis of single tasks might basically be regarded as single-item tests, which do not allow for generalizations about an individual’s writing ability. While the use of as many participants as possible is common practice in writing research, and rightly so, measurements of writing skill are still quite often conducted using just one writing assignment per condition. Clearly, this is not advisable (cf. <xref ref-type="bibr" rid="bibr7-0265532212442647">Gebril, 2009</xref>; <xref ref-type="bibr" rid="bibr17-0265532212442647">Schoonen, 2005</xref>), as it probably supplies insufficient information concerning an individual’s writing ability, and because it does not allow for establishing whether any effects found are really effects of the specified condition, or just effects due to task (<xref ref-type="bibr" rid="bibr15-0265532212442647">Rijlaarsdam et al., 2011</xref>). For example, differences between L1 and L2 writing can never be established if only one writing task is used per language (cf. <xref ref-type="bibr" rid="bibr25-0265532212442647">Van Weijen, 2009</xref>). After all, any differences found between the two languages might actually be due to task. Hence, the large language effect found in the present study could only be uncovered because multiple tasks were used in both L1 and L2. The minimum number of tasks to be used for a reliable assessment depends, among other things, on the applied rating method, number of raters, test population, benchmark language and task type (cf. <xref ref-type="bibr" rid="bibr4-0265532212442647">Coffman, 1966</xref>; <xref ref-type="bibr" rid="bibr17-0265532212442647">Schoonen, 2005</xref>; <xref ref-type="bibr" rid="bibr21-0265532212442647">Van den Bergh, 1988b</xref>). Residual variance is, in all cases, larger than the assignment-related and participant-related variance, indicating that a large part of the score differences cannot be attributed to differences between assignments or participants. Residual variance includes the interaction between participant and assignment (e.g. some assignments are more difficult than others, but this added difficulty will be different for different participants).</p>
<p>The jury reliabilities were substantially higher than individual rater reliabilities. In other words, the reliability of the measurement increases quite drastically if the ratings are performed by multiple raters. This finding reinforces a point which has been made by many (<xref ref-type="bibr" rid="bibr7-0265532212442647">Gebril, 2009</xref>; <xref ref-type="bibr" rid="bibr14-0265532212442647">Raymond, 1982</xref>; <xref ref-type="bibr" rid="bibr17-0265532212442647">Schoonen, 2005</xref>; <xref ref-type="bibr" rid="bibr22-0265532212442647">Van den Bergh &amp; Eiting, 1989</xref>; <xref ref-type="bibr" rid="bibr27-0265532212442647">Weigle, 2002</xref>), namely that assessments by single raters are insufficient. Just as single-item tests cannot provide reliable measurements, so single-rater assessments cannot either.</p>
<p>The correlations between the three different rating criteria (global quality, structure and language) are strong. This is not uncommon in essay assessment studies (cf. <xref ref-type="bibr" rid="bibr6-0265532212442647">De Glopper, 1988</xref>; <xref ref-type="bibr" rid="bibr20-0265532212442647">Van den Bergh, 1988a</xref>). In addition, the reported correlations are disattenuated correlations. These are always stronger than correlations uncorrected for measurement error, which are usually reported. Nevertheless, the ratings of the three criteria do not seem to be fully distinguishable. There are probably two explanations for this. One is that, to some extent, criteria may truly coincide (cf. <xref ref-type="bibr" rid="bibr1-0265532212442647">Bae &amp; Bachman, 2010</xref>). As such, global quality includes the two other rating criteria, structure and language. The strong correlations between global quality and structure and global quality and language should therefore probably not be seen as an indication of major validity problems with the applied rating criteria. After all, if criteria (partly) coincide, they are expected to correlate. On the other hand, the perfect correlation between global quality and structure in the present study probably exceeds the expected correlation and indeed raises questions about the validity of these two rating criteria. Were the raters able to look beyond the structure of an essay (which of course impacts its content and persuasive power) while rating its global quality? The second explanation for the high correlations is therefore the possible occurrence of halo effects. Raters’ global impressions of an essay may have spilled over to the ratings of single aspects (structure and language), or vice versa, even though care was taken in the present study to minimize this possibility (e.g. ratings of different criteria were carried out separately). This may be the reason why structure and language – the two rating criteria which are most expected to be independent categories – correlate relatively substantially, although to a lesser degree than they correlate with global quality. In any case, the strength of the correlations is not directly relevant for the question whether ratings with L1 and L2 benchmarks are parallel tests. The correlations are only required to be similar across benchmark languages.</p>
<p>In the present study, the L2 (English) was a language from a culture which is relatively similar to the L1 (Dutch) culture, which means that the ideas about what constitutes a good piece of writing are unlikely to differ greatly. If the L1 and L2 cultures are less similar to each other, it might be harder for the raters to rate L1 essays relative to L2 benchmarks and vice versa, even if they are (near) native speakers of both languages. In those cases, the benchmark essay may be so different from the essay to be rated, due to cultural standards, that comparisons become extremely hard, which will have a negative impact on rater reliability. In addition, the allocation of scores across L1 and L2 essays is, in such cases, likely to be different for the two benchmark languages. For example, the L2 essay scores may have a far smaller range than the L1 essay scores, if an L1 benchmark is used, whereas the range of L1 essays is likely to be smaller than the range of L2 essays if an L2 benchmark is used. In other words, true and error score variances might not be invariant across benchmark languages if the L1 and L2 cultures are very dissimilar, so that ratings with L1 and L2 benchmarks might in such cases not be parallel tests. Whether parallelism can be achieved in situations with different combinations of first and second languages should be tested for every new occasion in which it is applied.</p>
<p>Thus far, it had not been possible to express the quality of L1 and L2 essays on a single scale. In the present study ratings of L1 and L2 essays conducted by raters who are (near) native or bilingual speakers of both L1 and L2 with benchmark essays in both L1 and L2 were found to be parallel tests. Therefore, comparison of L1 and L2 essay scores is allowed and quality differences between the two languages can be expressed. While a test of parallelism will need to be conducted in every new instance where this procedure is used, the rating procedure applied in the present study seems a promising method for further comparisons of L1 and L2 writing, for example in different populations. In the population investigated in the present study, L2 writing quality was homogeneously lower than L1 writing quality.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-0265532212442647">
<title>Appendix A: Example of an assignment</title>
<boxed-text id="boxed-text1-0265532212442647">
<sec id="section15-0265532212442647">
<title>Surveillance cameras in inner city areas</title>
<p>NACP, the National Action Committee for Pupils, is organising a national essay contest, especially for pupils of your age. You’re also taking part. You absolutely want to win. The winning essay will be printed in PAUZE, a monthly magazine that is read by pupils your age from all over the Netherlands.</p>
<p>The subject of the essay has already been decided and was described in PAUZE as follows:</p>
<p>Due to the increase in crimes and meaningless acts of violence, more and more cities are choosing to place surveillance cameras in inner city areas. Not everyone is pleased about this. Some feel safe knowing that someone is ‘watching over’ them, while others consider it an invasion of their privacy. NACP is going to pay attention to this subject in a special edition of PAUZE. We want to hear from pupils what they think. Decide what you think and send us your response!</p>
<p><italic>Assignment</italic>:</p>
<p>Write an essay in which you give your opinion on the question:</p>
<p>‘Do surveillance cameras in inner city areas increase public security?’</p>
<p>The essay has to meet the following requirements, set by the Jury:</p>
<list id="list1-0265532212442647" list-type="order">
<list-item><p>Your essay must be (about) half a page in length.</p></list-item>
<list-item><p>You must do your best to convince your readers, fellow pupils, of your opinion.</p></list-item>
<list-item><p>You must give arguments to support your opinion.</p></list-item>
<list-item><p>Your essay must be structured in a good and logical way.</p></list-item>
<list-item><p>Your essay must look well-cared-for (think of language use and spelling).</p></list-item>
<list-item><p>In your essay you must use at least two extracts from the ‘References’ (see next page). You must include these extracts in your essay in a meaningful way.</p></list-item></list>
<p>You have 30 minutes to complete this assignment.</p>
<p>Good luck!</p>
</sec>
<sec id="section16-0265532212442647">
<title>References</title>
<p>Surveillance cameras help prevent crime, but they also increase the chance of tracking down the culprits … The cameras are placed in such a way that perpetrators of crimes within the inner city area are almost always registered … Incidentally, despite the presence of surveillance cameras, the security and well-being of the general public remains everyone’s concern. That’s why we still say: ‘if you spot trouble, warn the police!’</p>
<p>(Source: Maastricht County Council, <ext-link ext-link-type="uri" xlink:href="http://www.maastricht.nl">www.maastricht.nl</ext-link>, 2004)</p>
<p>The evaluation report of the project in Ede already mentioned that surveillance cameras don’t just take away ‘feelings of unease and insecurity.’ On the contrary, before the cameras were installed, 65% of people visiting the Museumplein never felt unsafe, whereas five months after installation the percentage had dropped to 57%. Scottish research has also shown that after a short decrease the ‘feelings of insecurity’ rise again.</p>
<p>(Source: Erik Timmerman, Leeuwarder Courant, July 7 2000)</p>
<p>Great Britain has become THE surveillance capital of Europe, without anyone noticing, says Barry Hugill, spokesman for the English civil rights group Liberty. Remarkably, the call for privacy is gradually being overshadowed by experts’ warning against ‘a false sense of security.’ Ian Brown, researcher for Information Policy Research…: ‘It is an illusion to think that cameras will provide security.’… One study showed that in areas with intensive camera surveillance crime rates dropped by 3 or 4 percent, whereas better street lighting can help reduce the number of incidents by up to 20 percent. The general public is usually less vigilant, as the number of cameras in the area increases.</p>
<p>(Adapted from: Steven de Jong, <ext-link ext-link-type="uri" xlink:href="http://www.politiek-digitaal.nl">www.politiek-digitaal.nl</ext-link>, August 30 2004)</p>
<p>The crime rate on the Wallen and in the vicinity of the Nieuwendijk in the centre of Amsterdam has decreased since the implementation of camera surveillance in March … In the Nieuwendijkkwartier especially, satisfaction prevails all round … In the vicinity of the Wallen, people are generally positive, although attention is drawn to the unwanted relocation of problems to other areas and changes in the group of troublemaking drug dealers and junkies. To combat these effects, the council will install three extra cameras.</p>
<p>(Adapted from: Centrum voorCriminaliteitspreventie en Veiligheid, <ext-link ext-link-type="uri" xlink:href="http://www.ccv.nu">www.ccv.nu</ext-link>, November 30 2004)</p>
<p>Security cameras in and around the Noorderstation are functioning with difficulty. It is even questionable whether the cameras, installed last year, are actually working. […] Not all the cameras are permanently on-line […] police spokesman Ed Kraszewskireporterd last week. They are chiefly there as a preventative measure. He believes that the presence of cameras will deter thieves and violent criminals from committing criminal acts, even if the cameras are not on-line.</p>
<p>(Source: Cees Vellekoop, Dagblad van het Noorden, August 18 2003)</p>
<p>Permanent camera surveillance in Sneek’s inner city is pointless. […] The city council based her decision, amongst other things, on the experiences of other city councils with security cameras. These experiences taught them that not all cities have had positive results and that the costs, especially personnel costs, have been substantial.</p>
<p>(Adapted from: Friesch Dagblad, April 6 2004)</p>
</sec>
</boxed-text>
</app>
<app id="app2-0265532212442647">
<title>Appendix B: Definitions of rating criteria, as provided to the raters</title>
<sec id="section17-0265532212442647">
<title>Global quality</title>
<p>General impression of the quality of the essay’s content, persuasiveness, reasoning, argumentation, goal orientation, reader orientation, language use and appearance.</p>
</sec>
<sec id="section18-0265532212442647">
<title>Structure</title>
<list id="list2-0265532212442647" list-type="bullet">
<list-item><p>Quality of reasoning and argumentation</p></list-item>
<list-item><p>Quality of the manner in which the essay’s build-up and appearance supports reasoning and argumentation.</p></list-item>
</list>
</sec>
<sec id="section19-0265532212442647">
<title>Language</title>
<list id="list3-0265532212442647" list-type="bullet">
<list-item><p>Spelling</p></list-item>
<list-item><p>Style, i.e.</p>
<p>accuracy of formulated language (‘to-the-point-ness’);</p>
<p>suitability of vocabulary and sentence complexity for intended readers and</p>
<p>medium;</p></list-item> <list-item><p>originality of language use</p></list-item>
<list-item><p>Tone (suitability for intended readers and medium).</p></list-item></list>
</sec>
</app>
<app id="app3-0265532212442647">
<title>Appendix C: Example of a benchmark essay</title>
<p>(rating criterion structure; L2 benchmark) and the accompanying description of what is ‘average’ about it</p>
<boxed-text id="boxed-text2-0265532212442647">
<sec id="section20-0265532212442647">
<title>Living alone yes or no?</title>
<p>The descision of living, together, get married or stay single should everyone descide for themselves. Because everyone has other needs. Some people like to live alone, and other people need someone around.</p>
<p>I think marridge isn’t a must, a person can live alone very well, without feeling miserable, but of coure not everyone.</p>
<p>Marridge should not be rushed, you should think it over very well, and wait for the perfect partner for <underline>you</underline>! And before you get married you should know eachother very well.</p>
<p>I want to get married in the future but first, the right man has to come along. I also think it’s better if you first live together and when it goes well, you can get married. So you will find out how living with each other in <underline>one</underline> house is.</p>
<p>I just think it would be very nice to live with the person you love, and see each other every day, especially waking up together each morning would be nice.</p>
<p>Quote: We expect single people to be somewhat lonely and unhappy, and we expect people who are married or who live together to be absolutely happy.</p>
<p>I think this is not true.</p>
<p>People who choose to live alone can be very happy, it depend on the person. And people who got married and have picked the right person can be happy too.</p>
<p>I think marridge is fine, but it should not be rushed, and if you live alone you can still get married as well.</p>
</sec>
</boxed-text>
<sec id="section21-0265532212442647">
<title>Explanation: What is average about this benchmark essay’s structure?</title>
<p>Its strong points in terms of structure:</p>
<list id="list4-0265532212442647" list-type="bullet">
<list-item><p>The main statement of this essay is clearly stated in the first paragraph.</p></list-item>
<list-item><p>Each paragraph pertains to one single idea/argument.</p></list-item>
<list-item><p>For every new or separate idea, there is a new paragraph.</p></list-item>
<list-item><p>Within paragraphs, there is a consistent line of reasoning.</p></list-item></list>
<p>Its weak points in terms of structure:</p>
<list id="list5-0265532212442647" list-type="bullet">
<list-item><p>Not every paragraph supports the main statement.</p></list-item>
<list-item><p>Some paragraphs interrupt the line of reasoning at text level.</p></list-item>
<list-item><p>Transitions are not clearly marked. The quote, for example, is not introduced. Because the quote is unrelated to the previous paragraph, the reader gets side-tracked.</p></list-item></list>
</sec>
</app>
<app id="app4-0265532212442647">
<title>Appendix D: Procedural manual</title>
<p>For this manual, we will work with the numbers of essays, participants and raters as used in the present study. The procedure can be adjusted to different amounts. To be able to make general claims about an individual’s writing proficiency in each of the languages under investigation, multiple tasks should be used per language.</p>
<sec id="section22-0265532212442647">
<title>A. Obtaining essays</title>
<list id="list6-0265532212442647" list-type="simple">
<list-item><p><bold>1.</bold> Create eight writing assignments, which differ as slightly as possible. For instance, the essays differ only in terms of topic: topics A, B, C, D, E, F, G, H.</p></list-item>
<list-item><p><bold>2.</bold> Each of the eight assignments should be available in both L1 and L2.</p></list-item>
<list-item><p><bold>3.</bold> Balance the topics across participants and experimental conditions (i.e. language in which the text is written) as follows:</p>
<p><table-wrap id="table7-0265532212442647" position="float">
<label>Table A1.</label>
<caption>
<p>Balancing topics (A, B, …, H) across participants and experimental conditions</p>
</caption>
<graphic alternate-form-of="table7-0265532212442647" xlink:href="10.1177_0265532212442647-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Participant</th>
<th align="left">Participant writes in L1</th>
<th align="left">Participant writes in L2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>A, B, C, D</td>
<td>E, F, G, H</td>
</tr>
<tr>
<td>2</td>
<td>B, C, D, E</td>
<td>F, G, H, A</td>
</tr>
<tr>
<td>3</td>
<td>C, D, E, F</td>
<td>G, H, A, B</td>
</tr>
<tr>
<td>4</td>
<td>D, E, F, G</td>
<td>H, A, B, C</td>
</tr>
<tr>
<td>5</td>
<td>E, F, G, H</td>
<td>A, B, C, D</td>
</tr>
<tr>
<td>6</td>
<td>F, G, H, A</td>
<td>B, C, D, E</td>
</tr>
<tr>
<td>etc.</td>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap></p></list-item>
<list-item><p><bold>4.</bold> The L1 and L2 writing sessions (= experimental conditions) are administered on different days. Balance the order in which experimental conditions are presented as follows:</p>
<p><table-wrap id="table8-0265532212442647" position="float">
<label>Table A2.</label>
<caption>
<p>The order of conditions per participant</p>
</caption>
<graphic alternate-form-of="table8-0265532212442647" xlink:href="10.1177_0265532212442647-table8.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Participant</th>
<th align="left">Day 1</th>
<th align="left">Day 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>L1 (topics A, B, C, D)</td>
<td>L2 (topics E, F, G, H)</td>
</tr>
<tr>
<td>2</td>
<td>L2 (topics F, G, H, A)</td>
<td>L1 (topics B, C, D, E)</td>
</tr>
<tr>
<td>3</td>
<td>L1 (topics C, D, E, F)</td>
<td>L2 (topics G, H, A, B)</td>
</tr>
<tr>
<td>4</td>
<td>L2 (topics H, A, B, C)</td>
<td>L1 (topics D, E, F, G)</td>
</tr>
<tr>
<td>5</td>
<td>L1 (topics E, F, G, H)</td>
<td>L2 (topics A, B, C, D)</td>
</tr>
<tr>
<td>6</td>
<td>L2 (topics B, C, D, E)</td>
<td>L1 (topics F, G, H, A)</td>
</tr>
<tr>
<td>etc.</td>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap></p></list-item></list>
</sec>
<sec id="section23-0265532212442647">
<title>B. Creating subsamples</title>
<list id="list7-0265532212442647" list-type="simple">
<list-item><p><bold>5.</bold> Mix the essays and create eight subsamples, which consist of 20 randomly selected essays each. Generally, each subsample will now contain both L1 and L2 essays on various topics, written by various participants.</p></list-item>
<list-item><p><bold>6.</bold> Number the essays in such a way that the raters cannot trace them back to specific authors. For example: subsample 1 contains essays 1 through 20, subsample 2 contains essays 21 through 40, and so forth. The researcher will keep a file where the new numbers are matched to the original participant and task numbers.</p></list-item></list>
</sec>
<sec id="section24-0265532212442647">
<title>C. Selecting benchmarks essays</title>
<list id="list8-0265532212442647" list-type="simple">
<list-item><p><bold>7.</bold> Select six benchmark essays:</p>
<list id="list9-0265532212442647" list-type="simple">
<list-item><p>* an L1 benchmark for global quality  * an L2 benchmark for global quality</p></list-item>
<list-item><p>* an L1 benchmark for structural quality  * an L2 benchmark for structural quality</p></list-item>
<list-item><p>* an L1 benchmark for language quality  * an L2 benchmark for language quality</p></list-item></list>
<p>(Other criteria of text quality could be used, but there must be multiple criteria.)</p></list-item>
<list-item><p><bold>8.</bold> The benchmark essays represent (approximate) average quality for each of the specific rating criteria. Which essays qualify as average, can be determined on the basis of a pre-rating session.</p></list-item></list>
</sec>
<sec id="section25-0265532212442647">
<title>D. Ratings</title>
<list id="list10-0265532212442647" list-type="simple">
<list-item><p><bold>9.</bold> Select raters who are highly and equally proficient in both L1 and L2, and aware of text conventions in each of the two languages. Ideally, they should be bilinguals.</p></list-item>
<list-item><p><bold>10.</bold> These raters cannot have been involved in the pre-rating session to select benchmark essays.</p></list-item>
<list-item><p><bold>11.</bold> The ratings for global quality are performed first. To minimize the possibility of occurrence of halo effects, at least a week should pass before the ‘structure’ ratings take place, and another week after that before the ‘language’ ratings are performed.</p></list-item>
<list-item><p><bold>12.</bold> Each rater rates three subsamples relative to the L1 benchmark for global quality, and three subsamples relative to the L2 benchmark for global quality:</p>
<p><table-wrap id="table9-0265532212442647" position="float">
<label>Table D1.</label>
<caption>
<p>Distribution of subsamples across raters<sup><xref ref-type="fn" rid="fn3-0265532212442647">3</xref></sup></p>
</caption>
<graphic alternate-form-of="table9-0265532212442647" xlink:href="10.1177_0265532212442647-table9.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left" colspan="8">Subsample<hr/></th>
</tr>
<tr>
<th/>
<th align="left">1</th>
<th align="left">2</th>
<th align="left">3</th>
<th align="left">4</th>
<th align="left">5</th>
<th align="left">6</th>
<th align="left">7</th>
<th align="left">8</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rater 1</td>
<td>x</td>
<td>x</td>
<td>x</td>
<td>o</td>
<td>o</td>
<td>o</td>
<td/>
<td/>
</tr>
<tr>
<td>Rater 2</td>
<td/>
<td>x</td>
<td>x</td>
<td>x</td>
<td>o</td>
<td>o</td>
<td>o</td>
<td/>
</tr>
<tr>
<td>Rater 3</td>
<td/>
<td/>
<td>x</td>
<td>x</td>
<td>x</td>
<td>o</td>
<td>o</td>
<td>o</td>
</tr>
<tr>
<td>Rater 4</td>
<td>o</td>
<td/>
<td/>
<td>x</td>
<td>x</td>
<td>x</td>
<td>o</td>
<td>o</td>
</tr>
<tr>
<td>Rater 5</td>
<td>o</td>
<td>o</td>
<td/>
<td/>
<td>x</td>
<td>x</td>
<td>x</td>
<td>o</td>
</tr>
<tr>
<td>Rater 6</td>
<td>o</td>
<td>o</td>
<td>o</td>
<td/>
<td/>
<td>x</td>
<td>x</td>
<td>x</td>
</tr>
<tr>
<td>Rater 7</td>
<td>x</td>
<td>o</td>
<td>o</td>
<td>o</td>
<td/>
<td/>
<td>x</td>
<td>x</td>
</tr>
<tr>
<td>Rater 8</td>
<td>x</td>
<td>x</td>
<td>o</td>
<td>o</td>
<td>o</td>
<td/>
<td/>
<td>x</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0265532212442647"><p><italic>Notes</italic>: x = rating with L1 benchmark; o = rating with L2 benchmark.</p></fn>
</table-wrap-foot>
</table-wrap></p></list-item>
<list-item><p><bold>13.</bold> Balance the order in which benchmark languages occur: 4 raters perform ratings with L2 benchmarks first, and ratings with L1 benchmarks second, and the 4 remaining raters perform ratings with L1 benchmarks first, and ratings with L2 benchmarks second.</p></list-item>
<list-item><p><bold>14.</bold> Provide the raters with a definition of the rating criterion, along with a description of what is average about the benchmark essay (see <xref ref-type="app" rid="app2-0265532212442647">Appendices B</xref> and <xref ref-type="app" rid="app3-0265532212442647">C</xref>).</p></list-item>
<list-item><p><bold>15.</bold> The benchmark essays are awarded the randomly set score of 100. Instruct the raters to award a score to each essay which expresses how much better or worse they think it is than the benchmark essay in terms of global quality. If an essay was awarded a score of 200, for example, this meant that the rater thought its global quality was twice as good as the benchmark essay. If an essay received a score of 50, it meant that the rater thought it was half as good as the benchmark essay.</p></list-item>
<list-item><p><bold>16.</bold> Steps 12, 13, 14 and 15 are repeated during the ‘structure’ and ‘language’ ratings. The only differences will be the rating criterion and (therefore) the benchmark essays.</p></list-item></list>
</sec>
</app>
</app-group>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This study was funded by the Utrecht Institute of Linguistics OTS, Utrecht University, Utrecht, the Netherlands.</p></fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0265532212442647">
<label>1.</label>
<p>Note that the true score can be viewed as the shared part of the ratings by all raters.</p></fn>
<fn fn-type="other" id="fn2-0265532212442647">
<label>2.</label>
<p>That is, correlations between true scores ridded with measurement unreliability.</p></fn>
<fn fn-type="other" id="fn3-0265532212442647">
<label>3.</label>
<p>As the reader can see in <xref ref-type="table" rid="table1-0265532212442647">Table 1</xref>, the advised allocation of subsamples to raters and benchmarks, as presented in <xref ref-type="table" rid="table9-0265532212442647">Table D1</xref>, was not completely followed in the reported study. This is due to a small logistic mishap, resulting in the need to shuffle the allocation of subsamples to raters (as compared to the advised allocation in <xref ref-type="table" rid="table9-0265532212442647">Table D1</xref>). Nevertheless, the set-up presented in <xref ref-type="table" rid="table1-0265532212442647">Table 1</xref> still suits the requirements of the study, namely that: (a) each subsample was rated three times relative to an L1 benchmark and three times relative to an L2 benchmark; (b) each rater rated three subsamples relative to an L1 benchmark and three subsamples relative to an L2 benchmark; and (c) each rater therefore rates each essay only once (for each criterion). That is, he or she never rates the same essay relative to both an L1 and an L2 benchmark.</p></fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bae</surname><given-names>J.</given-names></name>
<name><surname>Bachman</surname><given-names>L. F.</given-names></name>
</person-group> (<year>2010</year>). <article-title>An investigation of four writing traits and two tasks across two languages</article-title>. <source>Language Testing</source>, <volume>27</volume>(<issue>2</issue>), <fpage>213</fpage>–<lpage>234</lpage>.</citation>
</ref>
<ref id="bibr2-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Blok</surname><given-names>H.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Estimating the reliability, validity, and invalidity of essay ratings</article-title>. <source>Journal of Educational Measurement</source>, <volume>22</volume>, <fpage>41</fpage>–<lpage>52</lpage>.</citation>
</ref>
<ref id="bibr3-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chenoweth</surname><given-names>N. A.</given-names></name>
<name><surname>Hayes</surname><given-names>J. R.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Fluency in writing. Generating text in L1 and L2</article-title>. <source>Written Communication</source>, <volume>18</volume>(<issue>1</issue>), <fpage>80</fpage>–<lpage>98</lpage>.</citation>
</ref>
<ref id="bibr4-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Coffman</surname><given-names>W. E.</given-names></name>
</person-group> (<year>1966</year>). <article-title>On the validity of essay tests of achievement</article-title>. <source>Journal of Educational Measurement</source>, <volume>3</volume>, <fpage>151</fpage>–<lpage>156</lpage>.</citation>
</ref>
<ref id="bibr5-0265532212442647">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Couzijn</surname><given-names>M.</given-names></name>
<name><surname>van den Bergh</surname><given-names>H.</given-names></name>
<name><surname>Rijlaarsdam</surname><given-names>G.</given-names></name>
</person-group> <article-title>(2002)</article-title>. <conf-name>Writing in L1 and L2</conf-name>. <conf-loc>Does it make a difference? Staffordshire: Presentation at EARLI SIG Writing Conference</conf-loc>, <day>11–13</day> <month>July</month> <year>2002</year>.</citation>
</ref>
<ref id="bibr6-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>De Glopper</surname><given-names>K.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Schrijvenbeschreven</article-title>. [Writing described.] ‘s-Gravenhage, <publisher-loc>the Netherlands</publisher-loc>: <publisher-name>SVO</publisher-name>.</citation>
</ref>
<ref id="bibr7-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gebril</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Score generalizability of academic writing tasks: Does one method fit it all?</article-title> <source>Language Testing</source>, <volume>26</volume>, <fpage>507</fpage>–<lpage>531</lpage>.</citation>
</ref>
<ref id="bibr8-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Godshalk</surname><given-names>F. I.</given-names></name>
<name><surname>Swineford</surname><given-names>F.</given-names></name>
<name><surname>Coffman</surname><given-names>W. E.</given-names></name>
</person-group> (<year>1966</year>). <source>The measurement of writing ability</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>College Entrance Examination Board</publisher-name>.</citation>
</ref>
<ref id="bibr9-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gorman</surname><given-names>T. P.</given-names></name>
<name><surname>Purves</surname><given-names>A. C.</given-names></name>
<name><surname>Degenhart</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1988</year>). <source>The IEA study for written composition I: The international writing tasks and scoring scales</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Pergamon Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jöreskog</surname><given-names>K. G.</given-names></name>
</person-group> (<year>1971</year>). <article-title>Statistical analysis of sets of congeneric tests</article-title>. <source>Psychometrika</source>, <volume>36</volume>(<issue>2</issue>), <fpage>109</fpage>–<lpage>133</lpage>.</citation>
</ref>
<ref id="bibr11-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhlemeier</surname><given-names>H.</given-names></name>
<name><surname>Van den Bergh</surname><given-names>H.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Relationships between language skills and task effects</article-title>. <source>Perceptual and Motor Skills</source>, <volume>86</volume>, <fpage>443</fpage>–<lpage>463</lpage>.</citation>
</ref>
<ref id="bibr12-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Purves</surname><given-names>A. C.</given-names></name>
</person-group> (Ed.) (<year>1992</year>). <source>The IEA study of written composition II: Education and performance in fourteen countries</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Pergamon Press</publisher-name>.</citation>
</ref>
<ref id="bibr13-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Purves</surname><given-names>A. C.</given-names></name>
<name><surname>Gorman</surname><given-names>T. P.</given-names></name>
<name><surname>Takala</surname><given-names>S.</given-names></name>
</person-group> (<year>1988</year>). <article-title>The development of the scoring scheme and scales</article-title>. In <person-group person-group-type="editor">
<name><surname>Gorman</surname><given-names>T. P.</given-names></name>
<name><surname>Purves</surname><given-names>A. C.</given-names></name>
<name><surname>Degenhart</surname><given-names>R. E.</given-names></name>
</person-group> (Eds.), <source>The IEA study for written composition I: The international writing tasks and scoring scales</source> (pp. <fpage>41</fpage>–<lpage>58</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Pergamon Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Raymond</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1982</year>). <article-title>What we don’t know about the evaluation of writing</article-title>. <source>College Composition and Communication</source>, 33(4), <fpage>399</fpage>–<lpage>403</lpage>.</citation>
</ref>
<ref id="bibr15-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rijlaarsdam</surname><given-names>G.</given-names></name>
<name><surname>Van den Bergh</surname><given-names>H.</given-names></name>
<name><surname>Couzijn</surname><given-names>M.</given-names></name>
<name><surname>Janssen</surname><given-names>T.</given-names></name>
<name><surname>Braaksma</surname><given-names>M.</given-names></name>
<name><surname>Tillema</surname><given-names>M.</given-names></name>
<name><surname>Van Steendam</surname><given-names>E.</given-names></name>
<name><surname>Raedts</surname><given-names>M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Writing</article-title>. In <person-group person-group-type="editor">
<name><surname>Harris</surname><given-names>K.R.</given-names></name>
<name><surname>Graham</surname><given-names>S.</given-names></name>
<name><surname>Urdan</surname><given-names>T.</given-names></name>
</person-group> (Eds), <source>APA educational psychology handbook. – Vol. 3: Application to learning and teaching</source> (pp. <fpage>189</fpage>–<lpage>228</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr16-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Roca de Larios</surname><given-names>J.</given-names></name>
<name><surname>Manchón</surname><given-names>R.</given-names></name>
<name><surname>Murphy</surname><given-names>L.</given-names></name>
<name><surname>Marín</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The foreign language writer’s strategic behaviour in the allocation of time to writing processes</article-title>. <source>Journal of Second Language Writing</source>, <volume>17</volume>(<issue>1</issue>), <fpage>30</fpage>–<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr17-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schoonen</surname><given-names>R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Generalizability of writing scores: an application of structural equation modeling</article-title>. <source>Language Testing</source>, <volume>22</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr18-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schoonen</surname><given-names>R.</given-names></name>
<name><surname>Van Gelderen</surname><given-names>A.</given-names></name>
<name><surname>De Glopper</surname><given-names>K.</given-names></name>
<name><surname>Hulstijn</surname><given-names>J.</given-names></name>
<name><surname>Simis</surname><given-names>A.</given-names></name>
<name><surname>Snellings</surname><given-names>P.</given-names></name>
<name><surname>Stevenson</surname><given-names>M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>First language and second language writing: The role of linguistic knowledge, speed of processing and metacognitive knowledge</article-title>. <source>Language Learning</source>, <volume>53</volume>(<issue>1</issue>), <fpage>165</fpage>–<lpage>202</lpage>.</citation>
</ref>
<ref id="bibr19-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thorson</surname><given-names>H.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Using the computer to compare foreign and native language writing processes: Astatistical and case study approach</article-title>. <source>The Modern Language Journal</source>, <volume>84</volume>(<issue>2</issue>), <fpage>155</fpage>–<lpage>170</lpage>.</citation>
</ref>
<ref id="bibr20-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Van den Bergh</surname><given-names>H.</given-names></name>
</person-group> (<year>1988a</year>). <source>Examens geëxamineerd</source>. [<trans-source xml:lang="en">Exams examined</trans-source>.] <publisher-loc>’s-Gravenhage, the Netherlands</publisher-loc>: <publisher-name>SVO</publisher-name>.</citation>
</ref>
<ref id="bibr21-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van den Bergh</surname><given-names>H.</given-names></name>
</person-group> (<year>1988b</year>). <article-title>Schrijven en schrijven is twee: Een onderzoek naar de samenhang tussen prestaties op verschillende schrijftaken</article-title>. [<trans-title xml:lang="en">Writing and writing makes two: An investigation into the relation between performances on different writing tasks</trans-title>.] <source>Tijdschrift voor Onderwijsresearch</source>, <volume>13</volume>(<issue>6</issue>), <fpage>311</fpage>–<lpage>324</lpage>.</citation>
</ref>
<ref id="bibr22-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van den Bergh</surname><given-names>H.</given-names></name>
<name><surname>Eiting</surname><given-names>M.</given-names></name>
</person-group> (<year>1989</year>). <article-title>A method of estimating rater reliability</article-title>. <source>Journal of Educational Measurement</source>, <volume>26</volume>, <fpage>29</fpage>–<lpage>40</lpage>.</citation>
</ref>
<ref id="bibr23-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van den Bergh</surname><given-names>H.</given-names></name>
<name><surname>Klein Gunnewiek</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>CEF of cijfers bij de beoordeling van schrijfvaardigheid</article-title>. [<trans-title xml:lang="en">CEF or marks for judging writing ability</trans-title>.] <source>LevendeTalen Tijdschrift</source>, <volume>10</volume>, <fpage>3</fpage>–<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr24-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Van den Bergh</surname><given-names>H.</given-names></name>
<name><surname>Rijlaarsdam</surname><given-names>G.</given-names></name>
<name><surname>Janssen</surname><given-names>T.</given-names></name>
<name><surname>Braaksma</surname><given-names>M.</given-names></name>
<name><surname>Van Weijen</surname><given-names>D.</given-names></name>
<name><surname>Tillema</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Process execution of writing and reading.Considering textquality, learner and task characteristics</article-title>. In <person-group person-group-type="editor">
<name><surname>Shelley</surname><given-names>M. C.</given-names><suffix>II</suffix></name>
<name><surname>Yore</surname><given-names>L. D.</given-names></name>
<name><surname>Hand</surname><given-names>B.</given-names></name>
</person-group> (Eds.), <source>Quality research in literacy and science education</source> (pp. <fpage>399</fpage>–<lpage>425</lpage>). <publisher-loc>Dordrecht, the Netherlands</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr25-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Van Weijen</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <source>Writing processes, text quality, and task effects: Empirical studies in first and second language writing</source>. <publisher-loc>Utrecht</publisher-loc>: <publisher-name>LOT Dissertation Series</publisher-name>.</citation>
</ref>
<ref id="bibr26-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van Weijen</surname><given-names>D.</given-names></name>
<name><surname>Van den Bergh</surname><given-names>B.</given-names></name>
<name><surname>Rijlaarsdam</surname><given-names>G.</given-names></name>
<name><surname>Sanders</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Differences in process and process-product relations in L2 writing</article-title>. <source>ITL Applied Linguistics</source>, <volume>156</volume>, <fpage>203</fpage>–<lpage>226</lpage>.</citation>
</ref>
<ref id="bibr27-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Weigle</surname><given-names>S. C.</given-names></name>
</person-group> (<year>2002</year>). <source>Assessing writing</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr28-0265532212442647">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>White</surname><given-names>E. M.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Holisticism</article-title>. <source>College Composition and Communication</source>, <volume>35</volume>(<issue>4</issue>), <fpage>400</fpage>–<lpage>409</lpage>.</citation>
</ref>
<ref id="bibr29-0265532212442647">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>White</surname><given-names>E. M.</given-names></name>
</person-group> (<year>1985</year>). <source>Teaching and assessing writing</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>