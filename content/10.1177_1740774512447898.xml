<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">CTJ</journal-id>
<journal-id journal-id-type="hwp">spctj</journal-id>
<journal-id journal-id-type="nlm-ta">Clin Trials</journal-id>
<journal-title>Clinical Trials</journal-title>
<issn pub-type="ppub">1740-7745</issn>
<issn pub-type="epub">1740-7753</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1740774512447898</article-id>
<article-id pub-id-type="publisher-id">10.1177_1740774512447898</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A statistical approach to central monitoring of data quality in clinical trials</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Venet</surname><given-names>David</given-names></name>
<xref ref-type="aff" rid="aff1-1740774512447898">a</xref>
<xref ref-type="aff" rid="aff2-1740774512447898">b</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Doffagne</surname><given-names>Erik</given-names></name>
<xref ref-type="aff" rid="aff1-1740774512447898">a</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Burzykowski</surname><given-names>Tomasz</given-names></name>
<xref ref-type="aff" rid="aff1-1740774512447898">a</xref>
<xref ref-type="aff" rid="aff3-1740774512447898">c</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Beckers</surname><given-names>François</given-names></name>
<xref ref-type="aff" rid="aff4-1740774512447898">d</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Tellier</surname><given-names>Yves</given-names></name>
<xref ref-type="aff" rid="aff4-1740774512447898">d</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Genevois-Marlin</surname><given-names>Eric</given-names></name>
<xref ref-type="aff" rid="aff5-1740774512447898">e</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Becker</surname><given-names>Ursula</given-names></name>
<xref ref-type="aff" rid="aff6-1740774512447898">f</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bee</surname><given-names>Valerie</given-names></name>
<xref ref-type="aff" rid="aff7-1740774512447898">g</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Wilson</surname><given-names>Veronique</given-names></name>
<xref ref-type="aff" rid="aff7-1740774512447898">g</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Legrand</surname><given-names>Catherine</given-names></name>
<xref ref-type="aff" rid="aff8-1740774512447898">h</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Buyse</surname><given-names>Marc</given-names></name>
<xref ref-type="aff" rid="aff3-1740774512447898">c</xref>
<xref ref-type="aff" rid="aff9-1740774512447898">i</xref>
</contrib>
</contrib-group>
<aff id="aff1-1740774512447898"><label>a</label>International Drug Development Institute (IDDI), Louvain-la-Neuve, Belgium</aff>
<aff id="aff2-1740774512447898"><label>b</label>Institut de Recherches Interdisciplinaires et de Développements en Intelligence Artificielle (IRIDIA), Free University of Brussels, Brussels, Belgium</aff>
<aff id="aff3-1740774512447898"><label>c</label>Interuniversity Institute for Biostatistics and statistical Bioinformatics (I-BioStat), Hasselt University, Diepenbeek, Belgium</aff>
<aff id="aff4-1740774512447898"><label>d</label>GlaxoSmithKline Biologicals, Wavre, Belgium</aff>
<aff id="aff5-1740774512447898"><label>e</label>Sanofi-Aventis R&amp;D Biostatistics and Programming, Bridgewater, NJ, USA</aff>
<aff id="aff6-1740774512447898"><label>f</label>F. Hoffmann-LaRoche Ltd., Basel, Switzerland</aff>
<aff id="aff7-1740774512447898"><label>g</label>Translational Research in Oncology (TRIO), Paris, France</aff>
<aff id="aff8-1740774512447898"><label>h</label>Institute of Statistics, Biostatistics and Actuarial Sciences (ISBA), Université Catholique de Louvain, Louvain-la-Neuve, Belgium</aff>
<aff id="aff9-1740774512447898"><label>i</label>International Drug Development Institute (IDDI) Inc., Houston, TX, USA</aff>
<author-notes>
<corresp id="corresp1-1740774512447898">Marc Buyse, IDDI Inc., 363 N. Sam Houston Parkway East, Suite 1100, Houston, TX 77060, USA. Email: <email>marc.buyse@iddi.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>9</volume>
<issue>6</issue>
<issue-title>University of Pennsylvania Conference on Statistical Issues in Clinical Trials: Emerging statistical issues in the conduct and monitoring of clinical trials</issue-title>
<fpage>705</fpage>
<lpage>713</lpage>
<permissions>
<copyright-statement>© The Author(s), 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">The Society for Clinical Trials</copyright-holder>
</permissions>
<abstract>
<sec id="section1-1740774512447898">
<title>Background</title>
<p>Classical monitoring approaches rely on extensive on-site visits and source data verification. These activities are associated with high cost and a limited contribution to data quality. Central statistical monitoring is of particular interest to address these shortcomings.</p>
</sec>
<sec id="section2-1740774512447898">
<title>Purpose</title>
<p>This article outlines the principles of central statistical monitoring and the challenges of implementing it in actual trials.</p>
</sec>
<sec id="section3-1740774512447898">
<title>Methods</title>
<p>A statistical approach to central monitoring is based on a large number of statistical tests performed on all variables collected in the database, in order to identify centers that differ from the others. The tests generate a high-dimensional matrix of <italic>p</italic>-values, which can be analyzed by statistical methods and bioinformatic tools to identify extreme centers.</p>
</sec>
<sec id="section4-1740774512447898">
<title>Results</title>
<p>Results from actual trials are provided to illustrate typical findings that can be expected from a central statistical monitoring approach, which detects abnormal patterns that were not (or could not have been) detected by on-site monitoring.</p>
</sec>
<sec id="section5-1740774512447898">
<title>Limitations</title>
<p>Central statistical monitoring can only address problems present in the data. Important aspects of trial conduct such as a lack of informed consent documentation, for instance, require other approaches. The results provided here are empirical examples from a limited number of studies.</p>
</sec>
<sec id="section6-1740774512447898">
<title>Conclusion</title>
<p>Central statistical monitoring can both optimize on-site monitoring and improve data quality and as such provides a cost-effective way of meeting regulatory requirements for clinical trials.</p>
</sec>
</abstract>
</article-meta>
</front>
<body>
<sec id="section7-1740774512447898" sec-type="intro">
<title>Introduction</title>
<p>Over the last decade, clinical research costs have sky rocketed while new drug approvals have decreased by one-third [<xref ref-type="bibr" rid="bibr1-1740774512447898">1</xref>]. At the current pace of increase in costs, adequately sized clinical trials will either become infeasible, or because such costs will have to be reflected in the price of new drugs, they will eventually cause an intolerable burden on health care systems. Eisenstein et al. [<xref ref-type="bibr" rid="bibr1-1740774512447898">1</xref>] have studied typical trial costs and suggest different options to reduce some of these costs without compromising the scientific validity of the trials. The greatest potential savings lie in the labor-intensive activities such as on-site monitoring, which can represent as much as 30% of the total budget in large global clinical trials [<xref ref-type="bibr" rid="bibr2-1740774512447898">2</xref>]. It is therefore not surprising that the current practice of performing intensive on-site monitoring is coming into question. Recently, pragmatic, risk-based approaches have been proposed that improve the cost-effectiveness ratio without compromising the quality and integrity of clinical trials. A recent draft guidance of the U.S. Food and Drug Administration (FDA) [<xref ref-type="bibr" rid="bibr3-1740774512447898">3</xref>] reflects this trend and states unequivocally: “FDA encourages greater reliance on centralized monitoring practices than has been the case historically, with correspondingly less emphasis on on-site monitoring.” A recent reflection paper of the European Medicines Agency (EMA) [<xref ref-type="bibr" rid="bibr4-1740774512447898">4</xref>] expresses a similar view (footnotes added for clarity): “Adaptations to conventional GCP<sup><xref ref-type="fn" rid="fn1-1740774512447898">1</xref></sup> methods, for example, adaptation of on-site monitoring visits, sample/focused SDV<sup><xref ref-type="fn" rid="fn2-1740774512447898">2</xref></sup>, new central monitoring processes etc., subject to appropriate metrics being captured to determine when/if escalation in monitoring would be appropriate.” These texts establish the importance of central monitoring as a cost-effective way of ensuring data quality in clinical trials.</p>
<p>In this article, we first discuss monitoring practices and current experiments aimed at validating optimized monitoring strategies. We then describe the underlying principles of a statistical approach to central monitoring. We illustrate typical findings obtained with this approach in actual trials from different therapeutic areas. We conclude with a discussion of the potential role of central statistical monitoring and argue that it can both optimize on-site monitoring and improve the quality of clinical trial data.</p>
</sec>
<sec id="section8-1740774512447898">
<title>Background</title>
<sec id="section9-1740774512447898">
<title>Clinical trial monitoring</title>
<p>Clinical trial sponsors are required to set up appropriate measures to monitor the conduct of the trial. The aim of monitoring is to ensure the patients’ well-being, compliance with the approved protocol and regulatory requirements, and data accuracy and completeness [<xref ref-type="bibr" rid="bibr5-1740774512447898">5</xref>]. We shall focus our attention solely on the latter aspect of monitoring, leaving aside its other purposes that are arguably more important, such as checking the patient’s informed consent forms and training the local staff (usually at the beginning, but also during the study if and when deviations from the study protocol are observed).</p>
<p>Baigent et al. [<xref ref-type="bibr" rid="bibr6-1740774512447898">6</xref>] draw a useful distinction between three types of trial monitoring: oversight by trial committees, on-site monitoring, and central statistical monitoring. They argue that the three types of monitoring are useful in their own right to guarantee the quality of the trial data and the validity of the trial results. Oversight by trial committees is especially useful to prevent/detect errors in the trial design and interpretation of the results. On-site monitoring is especially useful to prevent/detect procedural errors in the trial conduct at participating centers (e.g., informed consent signed by the patients or legally acceptable representative). Statistical monitoring is especially useful to detect data errors, whether due to faulty equipments, negligence, or fraud.</p>
</sec>
<sec id="section10-1740774512447898">
<title>Current monitoring practices</title>
<p>The landscape of current monitoring practices has recently been studied in a survey [<xref ref-type="bibr" rid="bibr7-1740774512447898">7</xref>] targeting different types of organizations: academic groups, Contract Research Organizations (CROs), pharmaceutical companies, and device companies. More than 80% of trials regardless of type of organization included on-site visits with SDV, which consists of comparing information recorded in the Case Report Form (CRF) with the corresponding source documents. According to this survey, SDV tends to be done systematically on key variables (e.g., consent, eligibility, serious adverse events, and primary outcome), but less so on the secondary variables (e.g., secondary outcomes or nonserious adverse events). SDV detects discrepancies due to transcription errors from source documentation to CRF, but it may miss errors present in the source documents. It has therefore been argued that the contribution of SDV to data quality is minimal, with full (100%) verification of all source data being particularly cost-ineffective [<xref ref-type="bibr" rid="bibr8-1740774512447898">8</xref>]. The survey [<xref ref-type="bibr" rid="bibr7-1740774512447898">7</xref>] also revealed that few respondents use a centralized monitoring approach to guide on-site visits. In addition to high costs, the travel-related intensive on-site monitoring also has a nonnegligible ecological footprint. It has been estimated that travel by the trial team generates around 20% of the total carbon emission generated by a clinical trial [<xref ref-type="bibr" rid="bibr9-1740774512447898">9</xref>].</p>
<p>Extensive monitoring with 100% SDV is no longer favored, even in regulatory trials, and is being progressively replaced by “reduced monitoring,” which consists of controlling only a random sample of data. The random sampling can be performed at various levels: country, centers within countries, patients within centers, visits within patients, CRF pages within visits, and so on. Reduced monitoring is usually adapted to the risk associated with the experimental procedure. For instance, a trial involving innocuous procedures or well-known treatments could involve far less monitoring than a trial involving invasive procedures or experimental new drugs. Another option is “targeted” monitoring (also known as “adaptive” or “triggered” monitoring), where the intensity and frequency of on-site monitoring is triggered by key performance or risk indicators. These indicators typically focus on critical aspects of trial conduct: accrual performance (e.g., actual accrual rate compared with projected accrual rate and accrual patterns over time), protocol adherence (e.g., percentage of protocol deviations and percentage of dropouts), treatment compliance (e.g., percentage of dose reductions or delays), safety reporting (e.g., percentage of adverse events and serious adverse events reported), and data management (e.g., percentage of overdue forms, query rate, and query resolution time). Some authors estimate that targeted monitoring can lead to a reduction of the number of visits by up to 25% compared to traditional approaches [<xref ref-type="bibr" rid="bibr10-1740774512447898">10</xref>].</p>
<p>It seems somewhat paradoxical that statistical theory, which is so central to the design and analysis of clinical trials, has not been put to use to help optimize monitoring activities, even though the potential of statistics to uncover fraud in multicenter trials has received some attention [<xref ref-type="bibr" rid="bibr11-1740774512447898">11</xref>,<xref ref-type="bibr" rid="bibr12-1740774512447898">12</xref>]. In recent years, attention has shifted from the detection of fraud to central statistical monitoring as a tool to detect abnormal patterns in the data and, as such, to help focus monitoring activities on centers where they appear to be most needed [<xref ref-type="bibr" rid="bibr6-1740774512447898">6</xref>].</p>
</sec>
<sec id="section11-1740774512447898">
<title>Evaluation of monitoring strategies</title>
<p>In view of the rising costs of monitoring, several academic groups have proposed formal studies to evaluate the performance of different monitoring strategies. The impact of on-site monitoring on data quantity and quality was evaluated formally in a prospective study embedded in a randomized trial comparing two chemotherapy regimens for patients with breast cancer. In that study, participating centers were randomized to either undergo systematic on-site visits or no visits at all. Although the study was prematurely stopped and did not fully answer the question it had set out to address, it found no difference in data quality between centers that had undergone an initiation visit and those that had not [<xref ref-type="bibr" rid="bibr13-1740774512447898">13</xref>]. At least two other similar initiatives, driven by academic groups in France and Germany, are currently ongoing. The first one, OPTImisation of MONitoring (OPTIMON) [<xref ref-type="bibr" rid="bibr14-1740774512447898">14</xref>], is a prospective, randomized, noninferiority trial covering various treatments and indications. The participating centers are randomized to classical intensive monitoring (full SDV, frequent visits) or “light” monitoring, which is modulated according to the risk of the intervention to the patient. The primary outcome measures are the proportion of patients without informed consent problems, the number of serious adverse events reported, errors in the eligibility criteria, and the primary endpoint specific to each study (e.g., tumor shrinkage in oncology). Another initiative, ADAptiertes MONitoring (ADAMON) [<xref ref-type="bibr" rid="bibr15-1740774512447898">15</xref>], also investigates the performance of a risk-adapted monitoring strategy against classical, extensive monitoring. Here too, the participating centers are randomized to either monitoring strategy. At the end of the trial, on-site audits are performed to assess the outcome measure of interest, which is the occurrence of serious GCP violations. These initiatives are much needed to provide quantitative evidence of the impact on data quality that can be expected of different monitoring strategies.</p>
</sec></sec>
<sec id="section12-1740774512447898" sec-type="methods">
<title>Methods</title>
<sec id="section13-1740774512447898">
<title>Sources of data errors in clinical trials</title>
<p>It is useful, before discussing methods to discover errors in clinical trial data, to describe the various ways in which data errors can occur, for different statistical techniques may be required for different types of errors. Note that we focus here on data errors, at the exclusion of other important types of errors that have been reviewed elsewhere (such as design errors, procedural errors, and analytical errors) [<xref ref-type="bibr" rid="bibr6-1740774512447898">6</xref>]. It is convenient, for the sake of this limited discussion, to classify data errors into four main classes:</p>
<list id="list1-1740774512447898" list-type="order">
<list-item><p><bold>Completely unintentional data errors</bold>, such as data generated by a wrongly calibrated or imprecise instrument. These errors can typically be discovered through a shift in the distribution of values, or an abnormally large variability in this distribution.</p></list-item>
<list-item><p><bold>Data errors resulting from carelessness</bold>, such as data incorrectly copied from source documents to the CRF or from the CRF to a database. These errors may be hard to detect but are generally innocuous if their frequency is low. Missing data also fall in this category; these may be damaging to the trial results in so far as it is generally unreasonable to assume that such data are missing at random (let alone completely at random). Fortunately, missing data can be detected through frequency comparisons, with outlying centers (those with too much or too little missing data) being flagged.</p></list-item>
<list-item><p><bold>Fabricated data</bold>, such as missing or outlying values that are replaced by plausible values, for example, through interpolation from adjacent values. These errors are typically discovered through an abnormally small variability in the distribution of values, through multivariate tests, or through tests for the similarity of patterns in repeated measures. Extreme cases of data fabrication have been documented, in which the entire set of patients were either invented or extracted from a database of real patients not entered in the trial [<xref ref-type="bibr" rid="bibr16-1740774512447898">16</xref>].</p></list-item>
<list-item><p><bold>Data falsified to reach a desired objective</bold>, for example, to make a patient eligible or to show a treatment effect. These errors, when present, can have a devastating effect on the trial credibility, especially if they are aimed at magnifying treatment effects. They can be detected through comparisons of distributions or through center by treatment interactions.</p></list-item></list>
<p>The last two types of errors can be considered fraud and, as such, may need to receive more attention [<xref ref-type="bibr" rid="bibr11-1740774512447898">11</xref>]. Indeed, if the fraud is demonstrated, it may not be sufficient to correct or discard problematic data. The validity of the whole trial may come into question, and further actions will eventually be taken against the person responsible for the fraud. Other types of data errors may be important to detect as well, to the extent that they may impact the outcome of the trial. Errors that are unintentional or result from carelessness, that are not too common and are randomly distributed between the randomized groups, have little impact on the trial results and as such are less important to detect.</p>
</sec>
<sec id="section14-1740774512447898">
<title>Principles of statistical monitoring</title>
<p>The second European Stroke Prevention Study (ESPS2) [<xref ref-type="bibr" rid="bibr16-1740774512447898">16</xref>] provides a remarkable example of the effectiveness of statistical monitoring as compared with on-site visits for the detection of abnormal data patterns. ESPS2 was a randomized trial of aspirin and dipyridamole in patients with transient-ischemic attack or stroke. The study enrolled 6602 patients at 59 centers, plus 438 patients at a center that eventually had to be excluded from all analyses.</p>
<p><disp-quote>
<p>Fraud or misconduct at the center concerned was considered a possibility early in the recruitment. Despite intensive monitoring this could not be proven one way or the other and external audit was brought in. The audit also failed to establish guilt or innocence. [<xref ref-type="bibr" rid="bibr16-1740774512447898">16</xref>]</p>
</disp-quote></p>
<p>In the end, the center was excluded on the grounds that the distribution of dipyridamole and aspirin plasma concentrations differed significantly in the suspect center as compared to all other centers and was incompatible with the drug administration required by the study protocol. This case exemplifies a situation in which even the most careful on-site review cannot uncover unusual data patterns that are readily detected even by simple statistical methods used to compare distributions of continuous variables [<xref ref-type="bibr" rid="bibr11-1740774512447898">11</xref>,<xref ref-type="bibr" rid="bibr12-1740774512447898">12</xref>]. In less extreme cases, more sophisticated statistical methods will be required. We now turn to general principles underlying statistical monitoring of clinical trial data.</p>
<p>First, statistical monitoring relies on the highly structured nature of clinical data, since the same protocol is implemented identically in all participating centers, where data are collected using the same CRF [<xref ref-type="bibr" rid="bibr11-1740774512447898">11</xref>,<xref ref-type="bibr" rid="bibr12-1740774512447898">12</xref>]. Hence the same hierarchical data structure is used throughout the trial, with variables or items grouped by CRF page (or screen when electronic data capture is used), CRF pages or screens grouped by visit, visits grouped by patient, patients grouped by investigator, investigators grouped by center, centers grouped by country, and countries grouped by geographical region. When the trial is randomized, the group allocated by randomization provides another design feature that allows for specific statistical tests to be performed because baseline variables are not expected to differ between the randomized groups (but through the play of chance), while outcome variables are expected to differ about equally in all centers (but through the play of chance), if the treatments under investigation have a true effect. Abnormal trends and patterns in the data can be detected by comparing the distribution of all variables in each center against all other centers. Similar comparisons can also be made between other units of analysis, if the structure of the trial warrants it. Such comparisons can be performed either one variable at a time in a univariate fashion or with several variables, taking into account the multivariate structure of the data, or using longitudinal data when the variable is repeatedly measured over time.</p>
<p>Second, statistical checks are powerful tools because the multivariate structure and/or time dependence of variables are very sensitive to deviations (in the case of errors) and hard to mimic (in the case of fraud) [<xref ref-type="bibr" rid="bibr11-1740774512447898">11</xref>,<xref ref-type="bibr" rid="bibr12-1740774512447898">12</xref>]. Fabricated or falsified data, even if plausible univariately, are likely to exhibit abnormal multivariate patterns that are detectable statistically. In addition, humans are poor random number generators and are generally forgetful of natural constraints in the data. Tests on randomness can be used to detect invented data. Benford’s law on the distribution of the first digits, or tests for digit preference, can raise red flags [<xref ref-type="bibr" rid="bibr17-1740774512447898">17</xref>,<xref ref-type="bibr" rid="bibr18-1740774512447898">18</xref>]. Tests on dates can also be useful in detecting abnormalities in the distribution of days (e.g., a high proportion of visits during weekends may reveal data fabrication) [<xref ref-type="bibr" rid="bibr12-1740774512447898">12</xref>,<xref ref-type="bibr" rid="bibr18-1740774512447898">18</xref>].</p>
<p>Third, a key tenet of statistical monitoring is that every piece of information collected in the CRF during the conduct of the trial, and every variable coded in the clinical database is potentially indicative of data quality, not just those associated with a set of indicators predefined to reflect site performance in terms of data quality (“key risk indicators”). A statistical approach therefore requires a large number of statistical tests to be performed: tests on proportions of outliers, means, global variances, within-patient variances, event counts, distributions of categorical variables, proportion of week days, proportion of missing values, correlations between several variables, and so on. These tests generate a high-dimensional matrix of <italic>p</italic>-values, which can be analyzed by statistical methods and bioinformatic tools to identify outlying centers.</p>
</sec>
<sec id="section15-1740774512447898">
<title>Challenges in actual trials</title>
<p>Although the principles of statistical monitoring are simple enough, its application to ongoing trials turns out to be quite challenging for the following reasons:</p>
<list id="list2-1740774512447898" list-type="order">
<list-item><p><bold>Staggered availability of data</bold>: The staggered opening of centers and the staggered accrual of patients result in substantial differences, especially early on, between follow-up times and variables available for analysis at different centers. Yet central monitoring is potentially most valuable early in the trial, in order to remedy problems before it is too late to do so.</p></list-item>
<list-item><p><bold>Volume of data</bold>: Statistical analyses are only informative if the volume of data to be analyzed is reasonably large, which is not the case early after trial start. In some therapeutic areas and in rare indications, the number of patients per center can remain small even when the study is completed, which may limit the potential to identify problems in any individual center.</p></list-item>
<list-item><p><bold>Cleanliness of data</bold>: The data collected in ongoing trials are subject to typing mistakes, misspecified units, and so on, which result in aberrant values. Such outliers are typically detected and corrected by data management, but statistical monitoring cannot wait for full data cleaning. Although trivial errors may sometimes be informative of the performance of centers in terms of data quality, they can also dilute true signals by introducing random noise.</p></list-item>
<list-item><p><bold>Systematic differences between centers</bold>: Substantial variability is often observed between centers in multicentric trials, due to a host of factors such as socioeconomic differences between patient populations recruited, ethnic and cultural differences between countries, and so on. A typical example would be the expected differences in the distribution of the patient’s height and weight in a trial involving Asian, European, and North American countries. The central monitoring system must be able to adjust for such systematic differences.</p></list-item></list>
</sec></sec>
<sec id="section16-1740774512447898" sec-type="results">
<title>Results</title>
<p>The examples shown in this section come from analyses of actual trial datasets. These examples are illustrative only and do not aim at providing a comprehensive list of problems that can occur and be detected by either on-site or central statistical monitoring. Some details about the examples were hidden or modified to preserve source data anonymity.</p>
<sec id="section17-1740774512447898">
<title>Example 1 – data propagation</title>
<p>The first example comes from a multicenter randomized trial in ophthalmology that was used for marketing approval of a new drug. At each visit, the attending ophthalmologists were asked to measure the blood pressure and respiratory rate of the patients. Central statistical analysis revealed abnormally low within-patient variability for one of the participating centers. <xref ref-type="table" rid="table1-1740774512447898">Table 1</xref> shows the measurements for a particular patient from the outlying center.</p>
<table-wrap id="table1-1740774512447898" position="float">
<label>Table 1.</label>
<caption>
<p>Repeated measurements of vital signs for a particular patient in center X</p>
</caption>
<graphic alternate-form-of="table1-1740774512447898" xlink:href="10.1177_1740774512447898-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Visit</th>
<th align="left">Systolic blood pressure (mm Hg)</th>
<th align="left">Diastolic blood pressure (mm Hg)</th>
<th align="left">Respiratory rate (breath/min)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>130</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 1 – pretreatment</td>
<td>130</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 2 – pretreatment</td>
<td>130</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 3 – pretreatment</td>
<td>130</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 4 – pretreatment</td>
<td>130</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 5 – pretreatment</td>
<td>130</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 6 – pretreatment</td>
<td>120</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 7 – pretreatment</td>
<td>120</td>
<td>80</td>
<td>16</td>
</tr>
<tr>
<td>Visit 8 – pretreatment</td>
<td>120</td>
<td>80</td>
<td>16</td>
</tr>
</tbody>
</table></table-wrap>
<p>The blood pressure and respiratory rates were remarkably constant over the visits in center X, whereas some natural variation was expected and present in most other centers. Propagation of values between visits is a well-known phenomenon in clinical trials [<xref ref-type="bibr" rid="bibr11-1740774512447898">11</xref>,<xref ref-type="bibr" rid="bibr12-1740774512447898">12</xref>], especially for variables considered unimportant (such as, arguably, vital signs in ophthalmology). The center(s) in which data were simply propagated over visits might well have escaped the scrutiny of on-site monitors who check source data against CRF. All values, taken individually, were plausible but the unusual pattern appeared clearly when taking the longitudinal dimension of the data into account.</p>
</sec>
<sec id="section18-1740774512447898">
<title>Example 2 – biased scores</title>
<p>The second example comes from a multicenter randomized trial for a psychiatric disease. The trial had two stages: an open-label phase (eligibility screen), followed by a randomized double-blind phase (for eligible patients only). Eligibility criteria included a psychiatric score, which had to be less than 20 for a patient to be eligible. <xref ref-type="fig" rid="fig1-1740774512447898">Figure 1</xref> shows the distribution of scores at entry in the open-label phase (panel A) and prior to randomization (panel B).</p>
<fig id="fig1-1740774512447898" position="float">
<label>Figure 1.</label>
<caption>
<p>Distribution of psychiatric scores at entry in open-label phase (panel A) and prior to randomization (panel B). Patients with a psychiatric score below 20 were eligible.</p>
</caption>
<graphic xlink:href="10.1177_1740774512447898-fig1.tif"/></fig>
<p>Clearly, the shapes of these two distributions are different, with a tendency for investigators to report a lower score prior to randomization, so as to make the patient eligible for the trial (panel B). When the trial protocol was developed, it was assumed that about half the patients would satisfy the eligibility criteria; in reality, two-thirds of the patients were within the allowed criteria.</p>
</sec>
<sec id="section19-1740774512447898">
<title>Example 3 – known fraud</title>
<p>The third example is different in so far as it was known beforehand that one center was suspicious in this multicenter randomized trial. Indeed, center X was found to commit fraud after an audit revealed that the staff at this center completed information that was supposed to be completed by the patients. Interest therefore focused on whether central statistical monitoring could have detected this problem. The graph in panel A of <xref ref-type="fig" rid="fig2-1740774512447898">Figure 2</xref> shows the two first dimensions of a principal component analysis (PCA) performed on the matrix of log <italic>p</italic>-values of tests comparing the means of all variables. A PCA projects each multidimensional point (in this case center) into a smaller set of dimensions (two dimensions in this case), so that as much of the variability as possible is explained [<xref ref-type="bibr" rid="bibr19-1740774512447898">19</xref>].</p>
<fig id="fig2-1740774512447898" position="float">
<label>Figure 2.</label>
<caption>
<p>Principal component analysis of <italic>p</italic>-values. Centers are identified by a letter (denoting the country) and a number, except for center X where fraud had been identified. Centers far from the origin behave differently from the bulk of centers around the origin and are therefore more likely to be problematic. Panel A: analysis based on all data. Panel B: analysis based on patient-questionnaire data.</p>
<p>PCA: Principal component analysis.</p>
</caption>
<graphic xlink:href="10.1177_1740774512447898-fig2.tif"/></fig>
<p>The interpretation of this graph is quite intuitive, as many centers are clustered around the origin: these are probably nonproblematic centers. Centers far from the origin are possibly problematic. Center X, which falls furthest from the origin, appears as most likely to be problematic in this analysis.</p>
<p>A detailed review of the tests for which the <italic>p</italic>-values were highly significant in this center indicated both a shift in mean and in variance for the data coming from the patient questionnaire. The graph in panel B of <xref ref-type="fig" rid="fig2-1740774512447898">Figure 2</xref> shows the first two principal components based only on the analysis of the patient-questionnaire data. Center X appears as an outlier in this graph too.</p>
<p>Of note, other centers (D6 and F6 in the all-data analysis and D1 and E6 in the patient-questionnaire analysis) are almost as extreme as center X in the principal component analyses. Further central analyses would be useful to pinpoint the discrepancies detected in these centers. Confirmation of the potentially problematic nature of these centers could then be obtained by an on-site audit. In such a situation, the contribution of central statistical monitoring to classical monitoring lies in its ability to inform monitors of the nature and magnitude of possible problems, making monitoring visits more targeted and informed, and hopefully more efficient.</p>
</sec>
<sec id="section20-1740774512447898">
<title>Example 4 – miscalibrated equipment</title>
<p>The last example comes from an ongoing multicenter randomized trial in which the mean value of a variable in several centers in country X was shown to differ significantly from the mean value in all other centers. <xref ref-type="fig" rid="fig3-1740774512447898">Figure 3</xref> shows the distribution of the <italic>p</italic>-values obtained by comparing the mean of each center to the means of all other centers.</p>
<fig id="fig3-1740774512447898" position="float">
<label>Figure 3.</label>
<caption>
<p>Frequency distribution of <italic>p</italic>-values obtained by comparing a feature mean across centers. Centers are identified by a letter (denoting the country) and a number. Vertical lines indicate the <italic>p</italic>-values of specific centers (the height of these lines is unimportant). Country X clearly shows a large number of outlying centers.</p>
</caption>
<graphic xlink:href="10.1177_1740774512447898-fig3.tif"/></fig>
<p>The distribution clearly identifies a group of outlying centers belonging to country X. On-site inspections revealed that all these centers used equipment from the same lot, which was found to be miscalibrated. Note that individual <italic>p</italic>-values were not very significant because many centers were subject to the same problem.</p>
</sec></sec>
<sec id="section21-1740774512447898" sec-type="discussion">
<title>Discussion</title>
<p>In this article, we build on previous work that paved the way to central statistical monitoring [<xref ref-type="bibr" rid="bibr11-1740774512447898">11</xref>,<xref ref-type="bibr" rid="bibr12-1740774512447898">12</xref>,<xref ref-type="bibr" rid="bibr18-1740774512447898">18</xref>]. We focus on the detection of data errors and show, through actual examples, that central statistical monitoring can reveal data issues that had remained undiscovered after careful SDV and on-site checks. These data issues may in turn point to other problems, such as lack of resources or poor training at the centers concerned, which would call for corrective actions. Central statistical monitoring allowed highlighting problems, such as lack of variability in blood pressure measurements or implausible values in a questionnaire, which would not have been detected by key risk indicator methods. This is because the former approach compares centers on all possible features, while the latter approach focuses on specific, predefined features thought to be of particular relevance to data quality.</p>
<p>Central statistical monitoring has limitations of its own. At the beginning of a trial, the amount of information available to perform statistical tests may not be sufficient to detect abnormal trends or patterns in the data. In trials with many small centers, statistical monitoring may fail to detect centers with serious data problems, again due to the lack of sufficient information for statistical tests to raise red flags. Statistical monitoring relies on computerized data only and may therefore miss some types of fraud or serious errors that can be detected during site visits or audits (such as evidence provided in handwritten documents or in interviews with site personnel). Conversely, on-site visits may be made more efficient if monitors perform these visits with information about unexpected or strange data patterns identified by statistical monitoring in hand. Targeted monitoring differs from central statistical monitoring in that it relies on “key risk indicators,” the drawbacks of such an approach being the programming required for every new study, and the fact that not all data are exploited. In contrast, statistical monitoring takes advantage of all the data and requires no trial-specific programming, at the expense of being less specific.</p>
<p>There is currently little empirical evidence that reduced or targeted monitoring strategies may achieve the same level of data quality than extensive monitoring with full SDV – in fact, there has also been little theoretical research into quality standards for clinical trials [<xref ref-type="bibr" rid="bibr8-1740774512447898">8</xref>]. There is, however, some evidence that the findings made during on-site visits can be detected by central monitoring. Investigators at the UK Medical Research Council recently reviewed the findings made during monitoring visits in a large trial conducted in patients with human immunodeficiency virus (HIV) syndrome in Africa. Of 268 monitoring findings, 76 (28%) were also identified in the central database, 179 (67%) could have been identified through central checks, had these been in place, and only 13 (5%) would have required a site visit to be found [<xref ref-type="bibr" rid="bibr20-1740774512447898">20</xref>]. Clearly, extensive data checks during on-site monitoring visits are neither cost effective nor sustainable; in contrast, a statistical approach to quality assurance can yield large cost savings and yet increase the reliability of trial results.</p>
</sec>
<sec id="section22-1740774512447898" sec-type="conclusions">
<title>Conclusion</title>
<p>Central monitoring through advanced statistical and bioinformatic methods can detect abnormal patterns in the data. It can also help improve the effectiveness of on-site monitoring by prioritizing site visits and by guiding site visits with central statistical data checks.</p>
</sec>
</body>
<back>
<ack>
<p>The contents of this article were presented in part by the third author at the Conference on “Developing effective quality systems in clinical trials: an enlightened approach,” Washington, DC, October 13–14, 2010, and by the last author at the University of Pennsylvania “Annual conference on statistical issues in clinical trials,” Philadelphia, PA, April 13, 2011.</p>
</ack>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-1740774512447898">
<label>1.</label>
<p>Good Clinical Practice</p></fn>
<fn fn-type="other" id="fn2-1740774512447898">
<label>2.</label>
<p>Source Data Verification</p></fn>
</fn-group>
</notes>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The first author was supported by a FIRST post-doctoral grant of the Walloon Region, Belgium.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1740774512447898">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eisenstein</surname><given-names>E</given-names></name>
<name><surname>Collins</surname><given-names>R</given-names></name>
<name><surname>Cracknell</surname><given-names>BS</given-names></name>
<etal/>
</person-group>. <article-title>Sensible approaches for reducing clinical trial costs</article-title>. <source>Clin Trials</source> <year>2008</year>; <volume>5</volume>: <fpage>75</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr2-1740774512447898">
<label>2.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Califf</surname><given-names>RM</given-names></name>
</person-group>. <article-title>ACS and Acute heart failure models</article-title>. In <conf-name>Speaker presentation at the Institute of Medicine Workshop on Transforming Clinical Research in the United States</conf-name>, <publisher-loc>Washington, DC</publisher-loc>, <conf-date>7–8 October 2009</conf-date>. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.iom.edu/Reports/2010/Transforming-Clinical-Research-in-the-United-States.aspx">http://www.iom.edu/Reports/2010/Transforming-Clinical-Research-in-the-United-States.aspx</ext-link> (<access-date>accessed 13 November 2011</access-date>).</citation>
</ref>
<ref id="bibr3-1740774512447898">
<label>3.</label>
<citation citation-type="gov">
<collab>U.S. Department of Health and Human Services, Food and Drug Administration</collab>. <article-title>Guidance for industry oversight of clinical investigations – a risk-based approach to monitoring: Draft guidance</article-title>. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/UCM269919.pdf">http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/UCM269919.pdf</ext-link> (<access-date>accessed 13 November 2011</access-date>).</citation>
</ref>
<ref id="bibr4-1740774512447898">
<label>4.</label>
<citation citation-type="web">
<collab>European Medicines Agency</collab>. <article-title>Reflection paper on risk based quality management in clinical trials, EMA/INS/GCP/394194/2011</article-title>. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ema.europa.eu/docs/en_GB/document_library/Scientific_guideline/2011/08/WC500110059.pdf">http://www.ema.europa.eu/docs/en_GB/document_library/Scientific_guideline/2011/08/WC500110059.pdf</ext-link> (<access-date>accessed 13 November 2011</access-date>).</citation>
</ref>
<ref id="bibr5-1740774512447898">
<label>5.</label>
<citation citation-type="web">
<collab>International Conference on Harmonisation of Technical Requirements for Registration of Pharmaceuticals for Human Use</collab>. <article-title>ICH harmonised tripartite guideline: Guideline for good clinical practice E6(R1)</article-title>. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ich.org/fileadmin/Public_Web_Site/ICH_Products/Guidelines/Efficacy/E6_R1/Step4/E6_R1__Guideline.pdf">http://www.ich.org/fileadmin/Public_Web_Site/ICH_Products/Guidelines/Efficacy/E6_R1/Step4/E6_R1__Guideline.pdf</ext-link> (<access-date>accessed 13 November 2011</access-date>).</citation>
</ref>
<ref id="bibr6-1740774512447898">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Baigent</surname><given-names>C</given-names></name>
<name><surname>Harrel</surname><given-names>F</given-names></name>
<name><surname>Buyse</surname><given-names>M</given-names></name>
<name><surname>Emberson</surname><given-names>JR</given-names></name>
<name><surname>Altman</surname><given-names>DG</given-names></name>
</person-group>. <article-title>Ensuring trial validity by data quality assurance and diversification of monitoring methods</article-title>. <source>Clin Trials</source> <year>2008</year>; <volume>5</volume>: <fpage>49</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr7-1740774512447898">
<label>7.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Morrison</surname><given-names>B</given-names></name>
<name><surname>Cochran</surname><given-names>C</given-names></name>
<name><surname>White</surname><given-names>J</given-names></name>
<etal/>
</person-group>. <article-title>Monitoring the quality of conduct of clinical trials: A survey of current practices</article-title>. <source>Clin Trials</source> <year>2011</year>; <volume>8</volume>: <fpage>342</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr8-1740774512447898">
<label>8.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grimes</surname><given-names>D</given-names></name>
<name><surname>Hubacher</surname><given-names>D</given-names></name>
<name><surname>Nanda</surname><given-names>K</given-names></name>
<etal/>
</person-group>. <article-title>The good clinical practice guideline: A bronze standard for clinical research</article-title>. <source>Lancet</source> <year>2005</year>; <volume>366</volume>: <fpage>172</fpage>–<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr9-1740774512447898">
<label>9.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lyle</surname><given-names>K</given-names></name>
<name><surname>Dent</surname><given-names>L</given-names></name>
<name><surname>Bailly</surname><given-names>S</given-names></name>
<etal/>
</person-group>. <article-title>Carbon cost of pragmatic randomised controlled trials: Retrospective analysis of sample of trials</article-title>. <source>BMJ</source> <year>2009</year>; <volume>339</volume>: <fpage>b4187</fpage>.</citation>
</ref>
<ref id="bibr10-1740774512447898">
<label>10.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Cooley</surname><given-names>S</given-names></name>
<name><surname>Srinivasan</surname><given-names>B</given-names></name>
</person-group>. <article-title>Triggered monitoring</article-title>. <source>Applied Clinical Trials</source>, <month>August</month> <year>2010</year>, Available at: <ext-link ext-link-type="uri" xlink:href="http://www.appliedclinicaltrialsonline.com/appliedclinicaltrials/article/articleDetail.jsp?id=682214&amp;sk=8de7d898dc2eb0bdb7f98b5b7ab3c42f">http://www.appliedclinicaltrialsonline.com/appliedclinicaltrials/article/articleDetail.jsp?id=682214&amp;sk=8de7d898dc2eb0bdb7f98b5b7ab3c42f</ext-link>.</citation>
</ref>
<ref id="bibr11-1740774512447898">
<label>11.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Buyse</surname><given-names>M</given-names></name>
<name><surname>George</surname><given-names>S</given-names></name>
<name><surname>Evans</surname><given-names>S</given-names></name>
<etal/>
</person-group>. <article-title>The role of biostatistics in detection and treatment of fraud in clinical trials</article-title>. <source>Stat Med</source> <year>1999</year>; <volume>18</volume>: <fpage>3435</fpage>–<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr12-1740774512447898">
<label>12.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Evans</surname><given-names>S</given-names></name>
</person-group>. <article-title>Statistical aspects of the detection of fraud</article-title>. In <person-group person-group-type="editor">
<name><surname>Lock</surname><given-names>S</given-names></name>
<name><surname>Wells</surname><given-names>F</given-names></name>
<name><surname>Farthing</surname><given-names>M</given-names></name>
</person-group>. (eds). <source>Fraud and Misconduct in Biomedical Research</source> (<edition>3rd edn</edition>). <publisher-name>BMJ Publishing Group</publisher-name>, <publisher-loc>London</publisher-loc>, <year>2001</year>, pp. <fpage>186</fpage>–<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr13-1740774512447898">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lienard</surname><given-names>J-L</given-names></name>
<name><surname>Quinaux</surname><given-names>E</given-names></name>
<name><surname>Fabre-Guillevin</surname><given-names>E</given-names></name>
<etal/>
</person-group>. <article-title>Impact of on-site initiation visits on patient recruitment and data quality in a randomized trial of adjuvant chemotherapy for breast cancer</article-title>. <source>Clin Trials</source> <year>2006</year>; <volume>3</volume>: <fpage>486</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr14-1740774512447898">
<label>14.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Chêne</surname><given-names>G</given-names></name>
<name><surname>Jounot</surname><given-names>V</given-names></name>
</person-group>. <article-title>Optimon study: Optimisation of monitoring</article-title>. Available at: <ext-link ext-link-type="uri" xlink:href="https://ssl2.isped.u-bordeaux2.fr/OPTIMON/Documents.aspx">https://ssl2.isped.u-bordeaux2.fr/OPTIMON/Documents.aspx</ext-link> (<access-date>accessed 13 November 2011</access-date>).</citation>
</ref>
<ref id="bibr15-1740774512447898">
<label>15.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brosteanu</surname><given-names>O</given-names></name>
<name><surname>Houben</surname><given-names>P</given-names></name>
<name><surname>Ihrig</surname><given-names>K</given-names></name>
<etal/>
</person-group>. <article-title>Risk analysis and risk adapted on-site monitoring in noncommercial clinical trials</article-title>. <source>Clin Trials</source> <year>2009</year>; <volume>6</volume>: <fpage>585</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr16-1740774512447898">
<label>16.</label>
<citation citation-type="journal">
<collab>The ESPS2 Group</collab>. <article-title>European Stroke Prevention Study 2. Efficacy and safety data</article-title>. <source>J Neurol Sci</source> <year>1997</year>; <volume>151</volume>(<supplement>Suppl.</supplement>): <fpage>S1</fpage>–<lpage>S77</lpage>.</citation>
</ref>
<ref id="bibr17-1740774512447898">
<label>17.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hill</surname><given-names>TP</given-names></name>
</person-group>. <article-title>A statistical derivation of the significant-digit law</article-title>. <source>Stat Sci</source> <year>1996</year>; <volume>10</volume>: <fpage>354</fpage>–<lpage>63</lpage>.</citation>
</ref>
<ref id="bibr18-1740774512447898">
<label>18.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Buyse</surname><given-names>M</given-names></name>
<name><surname>Evans</surname><given-names>S</given-names></name>
</person-group>. <article-title>Fraud in clinical trials</article-title>. In <person-group person-group-type="editor">
<name><surname>Colton</surname><given-names>T</given-names></name>
<name><surname>Redmond</surname><given-names>C</given-names></name>
</person-group>. (eds). <source>Biostatistics in Clinical Trials</source>. <publisher-loc>Wiley, New York</publisher-loc>, <year>2001</year>, pp. <fpage>432</fpage>–<lpage>69</lpage>.</citation>
</ref>
<ref id="bibr19-1740774512447898">
<label>19.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Härdle</surname><given-names>W</given-names></name>
<name><surname>Simar</surname><given-names>L</given-names></name>
</person-group>. <source>Applied Multivariate Statistical Analysis</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc>, <year>2003</year>.</citation>
</ref>
<ref id="bibr20-1740774512447898">
<label>20.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bakobaki</surname><given-names>JM</given-names></name>
<name><surname>Rauchenberger</surname><given-names>M</given-names></name>
<name><surname>Joffe</surname><given-names>N</given-names></name>
<etal/>
</person-group>. <article-title>The potential for central monitoring techniques to replace on-site monitoring: Findings from an international multi-centre clinical trial</article-title>. <source>Clin Trials</source> <year>2011</year>; <volume>9</volume>: <fpage>257</fpage>–<lpage>64</lpage>.</citation>
</ref></ref-list>
</back>
</article>