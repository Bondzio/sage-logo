<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPA</journal-id>
<journal-id journal-id-type="hwp">spepa</journal-id>
<journal-title>Educational Evaluation and Policy Analysis</journal-title>
<issn pub-type="ppub">0162-3737</issn>
<issn pub-type="epub">1935-1062</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/0162373712462453</article-id>
<article-id pub-id-type="publisher-id">10.3102_0162373712462453</article-id>
<title-group>
<article-title>Can Research Design Explain Variation in Head Start Research Results? A Meta-Analysis of Cognitive and Achievement Outcomes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Shager</surname><given-names>Hilary M.</given-names></name>
<aff id="aff1-0162373712462453">University of Wisconsin-Madison</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Schindler</surname><given-names>Holly S.</given-names></name>
<aff id="aff2-0162373712462453">University of Washington, Seattle</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Magnuson</surname><given-names>Katherine A.</given-names></name>
<aff id="aff3-0162373712462453">University of Wisconsin-Madison</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Duncan</surname><given-names>Greg J.</given-names></name>
<aff id="aff4-0162373712462453">University of California, Irvine</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Yoshikawa</surname><given-names>Hirokazu</given-names></name>
<aff id="aff5-0162373712462453">Harvard University</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Hart</surname><given-names>Cassandra M. D.</given-names></name>
<aff id="aff6-0162373712462453">University of California, Davis</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>35</volume>
<issue>1</issue>
<fpage>76</fpage>
<lpage>95</lpage>
<history>
<date date-type="received">
<day>17</day>
<month>2</month>
<year>2012</year>
</date>
<date date-type="rev-recd">
<day>30</day>
<month>6</month>
<year>2012</year>
</date>
<date date-type="rev-recd">
<day>24</day>
<month>8</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>8</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012 AERA</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>This study explores the extent to which differences in research design explain variation in Head Start program impacts. We employ meta-analytic techniques to predict effect sizes for cognitive and achievement outcomes as a function of the type and rigor of research design, quality and type of outcome measure, activity level of control group, and attrition. Across program evaluations, the average program-level effect size was 0.27 standard deviations. About 41% of the variation in estimates across evaluations can be explained by research design features, including the extent to which the control group experienced other forms of early care or education, and 11% of the variation within programs can be explained by the quality and type of the outcome measures.</p>
</abstract>
<kwd-group>
<kwd>meta-analysis</kwd>
<kwd>Head Start</kwd>
<kwd>program evaluation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p><sc>The recognition</sc> that school-entry academic skills of poor children lag well behind those of their more advantaged peers has focused attention on early childhood education (ECE) as a potential vehicle for remediating early achievement gaps. Sharp increases in public spending on a variety of ECE programs over the past 20 years reflect the success of educators and advocates in arguing for the value of early education programs for disadvantaged children. This increased attention and funding has also produced a proliferation of ECE program evaluations. These studies can yield important information about differences in the effectiveness of particular program models, but only if we understand the context of ECE research and are confident that divergent findings reflect meaningful differences in program effectiveness rather than methodological differences in study design.</p>
<p>Researchers and policymakers face a daunting task in making sense of findings across studies. Evaluations vary greatly in methods, quality, and results, which leads to an “apples versus oranges” comparison problem. Previous reviews of ECE programs have described differences in research design as a part of their subjective, narrative analyses and have suggested that such features might be important (<xref ref-type="bibr" rid="bibr20-0162373712462453">Magnuson &amp; Shager, 2010</xref>; <xref ref-type="bibr" rid="bibr19-0162373712462453">Ludwig &amp; Phillips, 2007</xref>; <xref ref-type="bibr" rid="bibr21-0162373712462453">McKey et al., 1985</xref>). However, there has been little systematic empirical investigation of the role of study design features in explaining differing results. This meta-analysis first estimates the short-term cognitive and achievement impacts of Head Start, a federally funded early education program for low-income children, using the accumulated evidence of program evaluations conducted between 1965 and 2007. Second, this study investigates the role of research design in predicting program impacts to better understand the program’s effects. Taken together, these analyses will inform readers about the broad set of findings from rigorous Head Start evaluations and will provide important context for interpreting the findings of Head Start and possibly other ECE program evaluations by identifying research design features that are likely to predict larger (or smaller) program effects.</p>
<sec id="section1-0162373712462453">
<title>Background</title>
<p>Within the field of ECE, Head Start is the largest publicly funded early education program, serving more than 900,000 children with annual federal funding of more than $7.2 million (<xref ref-type="bibr" rid="bibr35-0162373712462453">U.S. Department of Health and Human Services [DHHS], 2010b</xref>). A centerpiece of President Lyndon B. Johnson’s “War on Poverty,” Head Start was designed as a holistic intervention to improve economically disadvantaged, preschool-aged children’s cognitive and social development by providing a comprehensive set of educational, health, nutritional, and social services, as well as opportunities for parent involvement (<xref ref-type="bibr" rid="bibr39-0162373712462453">Zigler &amp; Valentine, 1979</xref>). Despite the program’s holistic approach, most Head Start evaluations have focused primarily on academic and cognitive outcomes, with few studies including good measures of social, emotional, or health outcomes.</p>
<p>The extent to which Head Start children benefit from participating in the program remains a topic of debate, which began with the first national Head Start study (<xref ref-type="bibr" rid="bibr36-0162373712462453">Westinghouse Learning Corporation, 1969</xref>) and continued following the recent releases of findings from the program’s national random-assignment evaluation (<xref ref-type="bibr" rid="bibr33-0162373712462453">U.S. DHHS, 2010</xref>, January). The recent findings have been characterized by some critics as showing that Head Start has small and transitory effects on cognitive and achievement outcomes, which they have contrasted with the larger effects of other early education programs. <xref ref-type="bibr" rid="bibr1-0162373712462453">Besharov and Higney (2007)</xref> wrote,
<disp-quote>
<p>It seems reasonable to compare Head Start’s impact to that of both pre-K programs and center-based child care. According to recent studies, Head Start’s immediate or short-term impacts on a host of developmental indicators seem not nearly as large as those of either pre-K or center-based child care. (pp. 686–687)</p>
</disp-quote></p>
<p>Yet, other scholars have argued that methodological differences may at least in part explain the differences in program impacts across such ECE studies, suggesting that differences in study design make the evaluation results incomparable (<xref ref-type="bibr" rid="bibr7-0162373712462453">Cook &amp; Wong, 2007</xref>). How large a role these design features might play, however, has been only a matter of conjecture.</p>
<p>That an evaluation’s research design affects study results is widely accepted, and, as a result, experimental methods have been anointed as the gold standard for program evaluation (<xref ref-type="bibr" rid="bibr6-0162373712462453">Cook, Shadish, &amp; Wong, 2008</xref>). However, the magnitude and direction of bias from the use of nonexperimental methods are often unknown and may be specific to the program and population of study. Several meta-analyses of medical and social service interventions have found that low-quality study designs yield larger effect sizes than do high-quality ones (<xref ref-type="bibr" rid="bibr22-0162373712462453">Moher et al., 1998</xref>; <xref ref-type="bibr" rid="bibr27-0162373712462453">Schulz, Chalmers, Hayes, &amp; Altman, 1995</xref>; <xref ref-type="bibr" rid="bibr31-0162373712462453">Sweet &amp; Appelbaum, 2004</xref>). Alternatively, within Head Start research, the primary concern has been that low-quality, nonexperimental studies will understate program effects because they are likely to result in more advantaged comparison groups (<xref ref-type="bibr" rid="bibr9-0162373712462453">Currie &amp; Thomas, 1995</xref>).</p>
<p>Within-study comparisons of research designs provide the best way to assess the importance of research design, but such studies are rare. <xref ref-type="bibr" rid="bibr6-0162373712462453">Cook and colleagues’ (2008)</xref> recent examination of a small number of within-study comparisons of research designs concluded that high-quality nonexperimental study designs (regression discontinuity, matched intact local control groups, and modeling selection processes) can closely approximate experimental studies (also see <xref ref-type="bibr" rid="bibr29-0162373712462453">Shadish, Clark, &amp; Steiner, 2008</xref>; <xref ref-type="bibr" rid="bibr30-0162373712462453">Shadish, Galindo, Wong, Steiner, &amp; Cook, 2011</xref>). Unfortunately, to date, within-study research design comparisons have not been conducted within the ECE field; thus, although studies from other fields provide important insight into the general role of research methods, it is unclear if these findings will likely apply to Head Start evaluations. Moreover, such studies have not examined other important facets of study design that are likely to affect ECE studies, such as the activity level of the control group and attrition.</p>
<p>Some prior meta-analytic studies of ECE programs have given at least cursory attention to the question of whether study design and related threats to internal validity predict the magnitude of program impacts. The only existing meta-analysis of Head Start research, conducted more than 25 years ago by <xref ref-type="bibr" rid="bibr21-0162373712462453">McKey and colleagues (1985)</xref>, found short-term positive program impacts on cognitive test scores (effect sizes = 0.31 to 0.59). Initial descriptive analyses of methodological factors related to issues of internal validity, such as quality of study design and attrition, revealed only slight influences on the magnitude and direction of effect sizes, which led the authors to exclude these measures from the main analyses.</p>
<p>Typically, other ECE meta-analytic studies have used composite measures of study design, combining aspects of internal validity with other study features, and it is not surprising that these idiosyncratic researcher-designed measures have yielded mixed findings. Null associations between effect sizes and study design quality composites, which included a variety of indicators related to internal validity, were found in <xref ref-type="bibr" rid="bibr12-0162373712462453">Gorey’s (2001)</xref> and <xref ref-type="bibr" rid="bibr24-0162373712462453">Nelson, Westhues, and MacLeod’s (2003)</xref> meta-analyses of ECE programs with long-term follow-up studies. In contrast, <xref ref-type="bibr" rid="bibr3-0162373712462453">Camilli, Vargas, Ryan, and Barnett’s (2010)</xref> ECE meta-analysis found that studies with a higher quality design yielded larger effect sizes for cognitive outcomes. The measure of design quality was a dichotomous indicator based on such factors as attrition, baseline equivalence of treatment and control groups, high implementation fidelity, and adequate information provided for coders.</p>
<p>In this study, we investigate the importance of several aspects of study design related to internal validity, but do so without aggregating features of study quality into single composites that may obscure the importance of any specific evaluation feature. Considering the role of research methods in evaluation report findings is possible because Head Start has provided a fairly standardized set of services to a relatively homogenous population of children over a long period of time, and numerous evaluations have been conducted. Given the importance of ensuring the baseline equivalence of the treatment and control groups (<xref ref-type="bibr" rid="bibr9-0162373712462453">Currie &amp; Thomas, 1995</xref>; <xref ref-type="bibr" rid="bibr11-0162373712462453">Gibbs, Ludwig, &amp; Miller, 2012</xref>), <italic>we hypothesize that studies that used more rigorous methods to ensure similarity between treatment and control groups prior to program participation, particularly random assignment, will produce larger effect sizes</italic>. Likewise, since disadvantaged students most likely to benefit from the program are also most likely to be lost to follow-up, <italic>we predict that studies with higher levels of overall attrition may yield smaller effect sizes</italic>.</p>
<p>Another feature of ECE evaluation design overlooked in prior studies is the activity level of the control group. In the case of ECE evaluations, this is defined as participation in center-based care or preschool among control group children. Comparison group children’s participation in other early education and care programs is important because low-income children gain important academic and cognitive benefits from attending these programs. For example, <xref ref-type="bibr" rid="bibr38-0162373712462453">Zhai, Brooks-Gunn, and Waldfogel (2010)</xref> found that when Head Start attendees were compared with children who received parental or other informal care, they had better academic outcomes; however, they did no better on these outcomes when compared with children who attended other center-based care programs.</p>
<p>Control group activity varies considerably across ECE studies and is particularly high in more recent evaluations, given the high rates of participation in early care and education programs among children of working parents and the expansion of ECE programs in the decades since Head Start began. The issue of the activity of the control group figured prominently in discussions about the recent national Head Start evaluation (<xref ref-type="bibr" rid="bibr23-0162373712462453">National Forum on Early Childhood Policy and Programs, 2010</xref>). <xref ref-type="bibr" rid="bibr5-0162373712462453">Cook (2006)</xref> found that rates of participation of the control group in ECE settings during the preschool “treatment” year were more than 50% in the Head Start Impact Study (<xref ref-type="bibr" rid="bibr32-0162373712462453">U.S. DHHS, Administration for Children and Families, 2005</xref>), considerably higher than the approximately 20% found in state prekindergarten studies (<xref ref-type="bibr" rid="bibr37-0162373712462453">Wong, Cook, Barnett, &amp; Jung, 2008</xref>). He argued that these differences might in part explain the larger effect sizes associated with recent prekindergarten evaluations compared with the findings reported in the Head Start Impact Study. Our study is the first to empirically investigate whether the activity level in the control group predicts the magnitude of program impact effect sizes across Head Start evaluations. <italic>We hypothesize a negative relationship between effect size and the activity level of the control group</italic> (i.e., a measure of whether control group members sought alternative ECE services on their own).</p>
<p>Other features of study design also may predict evaluations’ effect sizes—in particular, the characteristics of the measures selected. First, the alignment between the intervention content and the skills being assessed may matter because academic skills are more likely to be improved by ECE and other instructional settings than general cognitive skills (<xref ref-type="bibr" rid="bibr4-0162373712462453">Christian, Morrison, Frazier, &amp; Massetti, 2000</xref>). Suggestive evidence that measures that are more closely aligned with the practices of classroom instruction may be more sensitive to early education is found in <xref ref-type="bibr" rid="bibr37-0162373712462453">Wong and colleagues’ (2008)</xref> regression discontinuity study of five state prekindergarten programs. All of the programs measured both children’s receptive vocabulary and their level of print awareness. Across the five programs, effects on print awareness were several times greater than the effects on receptive vocabulary. <italic>We hypothesize that types of skills that are more closely aligned with early education instruction, such as prereading and premath academic skills, will yield larger effect sizes than measures of more abstract and global cognitive skills, such as vocabulary and IQ</italic>.</p>
<p>A second feature related to measurement, reliability, has also been overlooked in prior studies. In general, measurement error is likely to attenuate associations between variables; thus, we might expect larger estimates from more reliable measures. However, if low reliability is a proxy for evaluator-developed tests, which indicates the assessment is likely to be closely aligned with the skills taught in a particular Head Start program, then low reliability may predict larger program impact effect sizes compared with standardized tests (<xref ref-type="bibr" rid="bibr25-0162373712462453">Rosenshine, 2010</xref>). <italic>Given this uncertainty, we do not have a clear hypothesis about the direction and magnitude of the association between measure reliability and Head Start program impact effect sizes and consider our analysis exploratory.</italic></p>
<p>A third feature of measures is the method of assessment, specifically whether data are collected via a direct assessment on a particular test or task or as an observational rating. Standardized direct assessments, which are designed to have high levels of reliability, are likely to introduce less measurement error than either teacher or parent reports of children’s cognitive skills. <xref ref-type="bibr" rid="bibr15-0162373712462453">Hoyt and Kerns’s (1999)</xref> meta-analysis of psychological studies found that rating bias was likely to be very low in measures that included explicit attributes (counts of particular behaviors) but quite prevalent in measures that required raters to make inferences (global ratings of achievement or skills). If ratings of young children elicit global ratings, then such assessments might be more biased than direct assessments of children’s skills. However, such a prediction is complicated by the fact that observer reports may be more aligned with skills taught in particular programs, which would suggest that these assessments may predict larger effect sizes. Moreover, it is likely that in many studies, raters, especially teachers and parents, were not blind to the children’s treatment status, in which case it is possible that the resulting bias would favor Head Start attendees and predict larger effect sizes (<xref ref-type="bibr" rid="bibr15-0162373712462453">Hoyt &amp; Kerns, 1999</xref>). <italic>Given competing arguments, we consider our examination of the method of assessment (direct assessment versus observation) to be exploratory and do not have a clear hypothesis about the direction or magnitude of its association with effect sizes.</italic></p>
</sec>
<sec id="section2-0162373712462453" sec-type="methods">
<title>Research Method</title>
<sec id="section3-0162373712462453">
<title>Meta-Analysis</title>
<p>To summarize more than 30 years of Head Start research and understand how specific features of research design may account for the heterogeneity in estimated Head Start effects, we conducted a meta-analysis, a method of quantitative research synthesis that uses prior study results as the unit of observation (<xref ref-type="bibr" rid="bibr8-0162373712462453">Cooper &amp; Hedges, 2009</xref>). To combine findings across studies, estimates are transformed into a common metric, an effect size, which expresses treatment-control differences as a fraction of the standard deviation of the given outcome. Outcomes from individual studies can then be used to estimate the average effect size across studies. In addition, meta-analysis can be used to test whether the average effect size differs by, for example, type and quality of research design. After defining the problem of interest, meta-analysis proceeds in the following steps, described below: (a) literature search, (b) data evaluation, and (c) data analysis.</p>
</sec>
<sec id="section4-0162373712462453">
<title>Literature Search</title>
<p>The Head Start studies analyzed in this article compose a subset of studies from a large meta-analytic database being compiled by the National Forum on Early Childhood Policy and Programs. This database includes studies of child and family policies, interventions, and prevention programs provided to children from the prenatal period to age five, building on previous meta-analytic databases created by Abt Associates and the National Institute for Early Education Research (NIEER; <xref ref-type="bibr" rid="bibr3-0162373712462453">Camilli et al., 2010</xref>; <xref ref-type="bibr" rid="bibr16-0162373712462453">Jacob, Creps, &amp; Boulay, 2004</xref>; <xref ref-type="bibr" rid="bibr17-0162373712462453">Layzer, Goodson, Bernstein, &amp; Price, 2001</xref>).</p>
<p>An important first step in a meta-analysis is to identify all relevant evaluations that meet one’s programmatic and methodological criteria for inclusion; therefore, a number of search strategies were used to locate as many published and unpublished Head Start evaluations conducted between 1965 and 2007 as possible. First, we conducted keyword searches in the ERIC, PsycINFO, EconLit, and Dissertation Abstracts databases, resulting in 304 Head Start evaluations. Next, we manually searched the websites of several policy institutes (e.g., RAND, Mathematica, NIEER) and state and federal departments (e.g., U.S. DHHS) as well as references mentioned in collected studies and other reviews. This search resulted in another 134 possible reports for inclusion in the database. In sum, 438 Head Start evaluations were identified, in addition to the 126 previously coded by Abt and NIEER.</p>
</sec>
<sec id="section5-0162373712462453">
<title>Data Evaluation</title>
<p>The next step in the meta-analysis process was to determine whether identified studies met our established inclusion criteria. To be included in our database, studies must have had (a) a comparison group (either an observed control or alternative treatment group) and (b) at least 10 participants in each condition, with attrition of less than 50% in each condition. Evaluations could have been experimental or quasi-experimental, using one of the following methods: regression discontinuity, fixed effects (individual or family), residualized or other longitudinal change models, difference in difference, instrumental variables, propensity score matching, or interrupted time series. Quasi-experimental evaluations not using one of the former analytic strategies were also screened in if they included a comparison group <italic>plus</italic> pre- and posttest information on the outcome of interest or demonstrated adequate comparability of groups on baseline characteristics. These criteria are more rigorous than those applied by <xref ref-type="bibr" rid="bibr21-0162373712462453">McKey et al. (1985)</xref>, Abt, and NIEER; for example, they eliminated all pre/post-only (no comparison group) studies as well as regression-based studies with baseline nonequivalent treatment and control groups.</p>
<p>For this particular study, we imposed some additional inclusion criteria. We included only studies that measure differences between Head Start participants and control groups that were assigned to receive no other services. For example, studies that compared Head Start attendees with children who attended another type of early education program or Head Start add-on program were excluded. However, studies were not excluded if families assigned to a no-treatment control group sought services of their own volition. In addition, we included only Head Start studies that had at least one measure of children’s cognitive or achievement outcomes. Outcome measures from other domains, such as socioemotional development and health, were too rare to be analyzed separately. Furthermore, to improve comparability across findings, we imposed limitations regarding the timing of study outcome measures. We limited our analysis to studies in which children received at least 75% of the intended Head Start treatment and for which outcomes were measured 12 or fewer months posttreatment.</p>
<p>The screening process, based on the above criteria, resulted in the inclusion of 57 Head Start publications or reports (see the <xref ref-type="app" rid="app1-0162373712462453">appendix</xref>). Of the 126 Head Start reports originally included in the Abt/NIEER database, 29 were eliminated from the database because they did not meet our research design criteria. The majority of the 438 additional reports identified by the research team’s search were excluded after reading the abstract (<italic>n</italic> = 243), indicating that they did not meet our inclusion criteria for obvious reasons (e.g., they were not quantitative evaluations of Head Start or did not have a comparison group). Of the 98 Head Start evaluation reports that were excluded after full-text screening, 90 were excluded because they did not meet our research design criteria; 53 of these were excluded specifically because they did not include a comparison group. Eight other reports were excluded based on other eligibility criteria (e.g., they reported results only for students with disabilities or did not include relevant outcomes). Our additional inclusion criteria for this article (e.g., short-term cognitive or achievement outcomes, no alternative treatment or curricular add-on studies) excluded 120 reports that remain coded in the database but are not included in this analysis.</p>
</sec>
<sec id="section6-0162373712462453">
<title>Coding Studies</title>
<p>The research team developed a protocol to codify information about study design, program and sample characteristics, and statistical information needed to compute effect sizes. This protocol serves as the template for the database and delineates all the information about an evaluation that we want to describe and analyze. A team of 10 graduate research assistants were trained as coders during a 3- to 6-month process that included instruction in evaluation methods, using the coding protocol, and computing effect sizes. Trainees were paired with experienced coders in multiple rounds of practice coding. Before coding independently, research assistants passed a reliability test comprising randomly selected codes from a randomly selected study. To pass the reliability test, researchers had to calculate 100% of the effect sizes correctly and achieve 80% reliability with a master coder for the remaining codes. In instances when research assistants were just under the threshold for effect sizes but were reliable on the remaining codes, they underwent additional effect size training before coding independently and were subject to periodic checks during their transition. Questions about coding were resolved in weekly research team conference calls.</p>
</sec>
<sec id="section7-0162373712462453">
<title>Database</title>
<p>The resulting database is organized in a four-level hierarchy (from highest to lowest): the study, the program, the contrast, and the effect size (see <xref ref-type="table" rid="table1-0162373712462453">Table 1</xref>). A “study” is defined as a collection of comparisons in which the treatment groups are drawn from the same pool of participants. A study may include evaluations of multiple “programs;” i.e., a particular type of Head Start or Head Start in a particular location. Each study also produces a number of “contrasts,” defined as a comparison between one group of children who received Head Start and another group of children who received no other services as a result of the study. Evaluations of programs within studies may include multiple contrasts; for example, results may be presented using more than one analytic method (e.g., ordinary least squares and fixed effects) or separate groups of children (e.g., 3- and 4-year-olds), and these are coded as different contrasts nested within one program, within one study. We include 33 Head Start programs in our analysis, but 5 of these programs provided only “missing” effect sizes (insufficient detail was provided in the report to calculate an effect size), and, as a result, our primary analyses consist of data from 28 programs. The 33 Head Start programs included in our meta-analysis include 40 separate contrasts of program main effects (excluding subgroup analyses, e.g., by gender or race). In turn, each contrast consists of a number of individual “effect sizes” (estimated standard deviation unit difference in an outcome between the children who experienced Head Start and those who did not). The 40 contrasts in this meta-analytic database provide a total of 313 effect sizes (72 of these effect sizes were coded as “missing”). These effect sizes combine information from a total of more than 160,000 observations. See the <xref ref-type="app" rid="app1-0162373712462453">appendix</xref> for a description of the Head Start studies, programs, and contrasts included in our analyses.</p>
<table-wrap id="table1-0162373712462453" position="float">
<label>Table 1</label>
<caption><p>Key Meta-Analysis Terms and <italic>n</italic> Values</p></caption>
<graphic alternate-form-of="table1-0162373712462453" xlink:href="10.3102_0162373712462453-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="center"/>
</colgroup>
<thead>
<tr>
<th align="left">Term</th>
<th align="center">Description</th>
<th align="center"><italic>n</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Report</td>
<td>Written evaluation of Head Start (e.g., a journal article, government report, book chapter)</td>
<td>57</td>
</tr>
<tr>
<td>Study</td>
<td>Collection of comparisons in which the treatment groups are drawn from the same pool of participants</td>
<td>27</td>
</tr>
<tr>
<td>Program</td>
<td>Particular type of Head Start or Head Start within a particular location</td>
<td>33</td>
</tr>
<tr>
<td>Contrast</td>
<td>Comparison between one group of children who received Head Start and another group of children who received no other services as a result of the study (although they may have sought services themselves)</td>
<td>40</td>
</tr>
<tr>
<td>Effect size</td>
<td>Measure of the difference in cognitive outcomes between the children who experienced Head Start and those who did not, expressed in standard deviation units (Hedges’s g)</td>
<td>313</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section8-0162373712462453">
<title>Effect Size Computation</title>
<p>Outcome information was reported in evaluations using a number of different statistics, which were converted to effect sizes (Hedges’s <italic>g</italic>) with the commercially available software package Comprehensive Meta-Analysis (<xref ref-type="bibr" rid="bibr2-0162373712462453">Borenstein, Hedges, Higgins, &amp; Rothstein, 2005</xref>). Hedges’s <italic>g</italic> is an effect size statistic that adjusts the standardized mean difference (Cohen’s <italic>d</italic>) to account for bias in the <italic>d</italic> estimator from small sample sizes.</p>
</sec>
<sec id="section9-0162373712462453">
<title>Dependent Variable</title>
<p>Descriptive information for the dependent measure (effect size) and key independent variables is provided in <xref ref-type="table" rid="table2-0162373712462453">Table 2</xref>. To account for the varying precision among effect size estimates, as well as the number of effect sizes within each program, these descriptive statistics and all subsequent analyses are weighted by the inverse of the variance of each effect size multiplied by the inverse of the number of effect sizes per program (<xref ref-type="bibr" rid="bibr8-0162373712462453">Cooper &amp; Hedges, 2009</xref>; <xref ref-type="bibr" rid="bibr18-0162373712462453">Lipsey &amp; Wilson, 2001</xref>). The dependent variables in these analyses are the effect sizes measuring the standardized difference in assessment of children’s cognitive skills and achievement between children who attended Head Start and the comparison group. Effect sizes ranged from −0.49 to 1.05, with an estimated weighted mean of 0.18 at the effect size level and an estimated weighted mean of 0.29 at the program level.</p>
<table-wrap id="table2-0162373712462453" position="float">
<label>Table 2</label>
<caption><p>Descriptive Information for Nonmissing Effect Sizes and Independent Variables (<italic>N</italic> = 241)</p></caption>
<graphic alternate-form-of="table2-0162373712462453" xlink:href="10.3102_0162373712462453-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Min</th>
<th align="center">Max</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">Study and program characteristics</td>
</tr>
<tr>
<td> Modern Head Start program (post-1974), effect size level</td>
<td>0</td>
<td>1</td>
<td>0.44</td>
<td>0.50</td>
</tr>
<tr>
<td> Modern Head Start program (post-1974), program level</td>
<td>0</td>
<td>1</td>
<td>0.25</td>
<td>0.44</td>
</tr>
<tr>
<td> Length of program (months, centered at 2), effect size level</td>
<td>0</td>
<td>8</td>
<td>6.20</td>
<td>3.12</td>
</tr>
<tr>
<td> Length of program (months, centered at 2), program level</td>
<td>0</td>
<td>8</td>
<td>5.18</td>
<td>3.70</td>
</tr>
<tr>
<td> Peer-refereed journal, effect size level</td>
<td>0</td>
<td>1</td>
<td>0.17</td>
<td>0.37</td>
</tr>
<tr>
<td> Peer-refereed journal, program level</td>
<td>0</td>
<td>1</td>
<td>0.21</td>
<td>0.42</td>
</tr>
<tr>
<td colspan="5">Design characteristics</td>
</tr>
<tr>
<td> Active control group, effect size level</td>
<td>0</td>
<td>1</td>
<td>0.35</td>
<td>0.48</td>
</tr>
<tr>
<td> Active control group, program level</td>
<td>0</td>
<td>1</td>
<td>0.14</td>
<td>0.36</td>
</tr>
<tr>
<td> Passive control group, effect size level</td>
<td>0</td>
<td>1</td>
<td>0.53</td>
<td>0.50</td>
</tr>
<tr>
<td> Passive control group, program level</td>
<td>0</td>
<td>1</td>
<td>0.68</td>
<td>0.48</td>
</tr>
<tr>
<td> Missing control group activity, effect size level</td>
<td>0</td>
<td>1</td>
<td>0.12</td>
<td>0.32</td>
</tr>
<tr>
<td> Missing control group activity, program level</td>
<td>0</td>
<td>1</td>
<td>0.18</td>
<td>0.39</td>
</tr>
<tr>
<td> Randomized controlled trial, effect size level</td>
<td>0</td>
<td>1</td>
<td>0.26</td>
<td>0.44</td>
</tr>
<tr>
<td> Randomized controlled trial, program level</td>
<td>0</td>
<td>1</td>
<td>0.18</td>
<td>0.39</td>
</tr>
<tr>
<td> Quasi-experimental study, effect size level</td>
<td>0</td>
<td>1</td>
<td>0.74</td>
<td>0.44</td>
</tr>
<tr>
<td> Quasi-experimental study, program level</td>
<td>0</td>
<td>1</td>
<td>0.82</td>
<td>0.39</td>
</tr>
<tr>
<td> Baseline covariates included, effect size level</td>
<td>0</td>
<td>1</td>
<td>0.49</td>
<td>0.50</td>
</tr>
<tr>
<td> Baseline covariates included, program level</td>
<td>0</td>
<td>1</td>
<td>0.14</td>
<td>0.36</td>
</tr>
<tr>
<td colspan="5">Dependent measure characteristics</td>
</tr>
<tr>
<td> Rating (by someone who knows child)</td>
<td>0</td>
<td>1</td>
<td>0.06</td>
<td>0.24</td>
</tr>
<tr>
<td> Observation (by researcher)</td>
<td>0</td>
<td>1</td>
<td>0.01</td>
<td>0.09</td>
</tr>
<tr>
<td> Performance measure</td>
<td>0</td>
<td>1</td>
<td>0.93</td>
<td>0.25</td>
</tr>
<tr>
<td> Skills not sensitive to instruction</td>
<td>0</td>
<td>1</td>
<td>0.67</td>
<td>0.47</td>
</tr>
<tr>
<td> Skills sensitive to instruction</td>
<td>0</td>
<td>1</td>
<td>0.33</td>
<td>0.47</td>
</tr>
<tr>
<td> Months posttreatment</td>
<td>−2.47</td>
<td>12</td>
<td>1.89</td>
<td>4.12</td>
</tr>
<tr>
<td colspan="5">Attrition (always  50%)</td>
</tr>
<tr>
<td> High attrition ( 10%)</td>
<td>0</td>
<td>1</td>
<td>0.36</td>
<td>0.48</td>
</tr>
<tr>
<td> Low attrition ( 10%)</td>
<td>0</td>
<td>1</td>
<td>0.42</td>
<td>0.50</td>
</tr>
<tr>
<td> Missing attrition information</td>
<td>0</td>
<td>1</td>
<td>0.22</td>
<td>0.41</td>
</tr>
<tr>
<td colspan="5">Reliability</td>
</tr>
<tr>
<td> High reliability (coefficient  .92)</td>
<td>0</td>
<td>1</td>
<td>0.21</td>
<td>0.41</td>
</tr>
<tr>
<td> Medium reliability (coefficient = .75–.91)</td>
<td>0</td>
<td>1</td>
<td>0.35</td>
<td>0.48</td>
</tr>
<tr>
<td> Low reliability (coefficient  .75)</td>
<td>0</td>
<td>1</td>
<td>0.18</td>
<td>0.39</td>
</tr>
<tr>
<td> Missing reliability coefficient</td>
<td>0</td>
<td>1</td>
<td>0.26</td>
<td>0.44</td>
</tr>
<tr>
<td>Effect size, effect size level</td>
<td>−0.49</td>
<td>1.05</td>
<td>0.18</td>
<td>0.22</td>
</tr>
<tr>
<td>Effect size, program level</td>
<td>−0.23</td>
<td>0.78</td>
<td>0.29</td>
<td>0.23</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0162373712462453">
<p><italic>Note</italic>. Weighted by inverse variance of effect size multiplied by inverse of number of effect sizes per program.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section10-0162373712462453">
<title>Independent Variables</title>
<p>Several key independent variables represent facets of each program’s research design. These study and Head Start characteristics do not vary within a program, so we present both program-level and effect-size-level descriptive information for these measures in <xref ref-type="table" rid="table2-0162373712462453">Table 2</xref>. We created a dummy variable indicating whether a study was randomized (reference category) or quasi-experimental. Two programs had designs that changed post hoc (the study was originally randomized but, for various reasons, became quasi-experimental in nature). In our primary specifications, these programs were coded as quasi-experimental. The majority of effect sizes (74%) come from quasi-experimental studies, although differences between program level and effect size level means suggest that randomized trials tended to have more outcome measures per study than those with quasi-experimental designs. In addition, we created a dummy variable to indicate whether baseline covariates were included in the analysis. Although the majority of programs (86%) did not include baseline covariates in their analyses, those that did had a large number of outcome measures.</p>
<p>We created a series of dummy variables indicating levels of overall attrition. Keeping in mind that attrition was truncated at 50% based on our screening criteria, attrition levels were constructed using quartile scores and defined as follows: low attrition (reference category), less than or equal to 10% (representing Quartiles 1, 2, and 3); high attrition, greater than 10% (representing Quartile 4); and missing overall attrition information. The plurality of effect sizes came from studies with 10% attrition or less.</p>
<p>We also coded the activity level of the control group using the following categories: passive (reference category), meaning that control group children received no alternative services; active, meaning some of the control group members sought services of their own volition; and a dummy variable indicating whether information regarding control group activity was missing from the report. Reports in which there was no mention of control group activity were coded as having missing information on this variable. Although the majority of effect sizes (53%) came from studies with passive control groups, studies in which the control group actively sought alternative services, specifically attendance at other center-based child care facilities or early education programs, tended to have more effect sizes per study. Studies that reported active control groups indicated that between 18% and 48% of the control group attended these types of programs.</p>
<p>Several of the key independent variables describe features of the outcome measures. We distinguished between effect sizes measuring achievement outcomes, such as reading, math, letter recognition, and numeracy skills, which may be more sensitive to typical classroom instruction, and those measuring cognitive outcomes less sensitive to instruction, including IQ, vocabulary, theory of mind, attention, task persistence, and syllabic segmentation, such as rhyming (see <xref ref-type="bibr" rid="bibr4-0162373712462453">Christian et al., 2000</xref>, for a discussion of this distinction). The majority of effect sizes (67%) were from the cognitive domain. Using a series of dummy variables, we also categorized effect sizes according to the type of measure employed by the researcher, indicating whether it was a performance test (reference category), a rating by someone the child knows (e.g., a teacher or parent), or an observational rating by a researcher. The majority of outcome measures were performance tests (93%).</p>
</sec>
<sec id="section11-0162373712462453">
<title>Control Variables</title>
<p>By limiting this study to Head Start evaluations, our analyses hold constant program features such as funding stream, program structure and requirements, and family socioeconomic background of children served. We do include other measured features of the evaluation studies in our analyses as controls because they may be confounded with research study design. Although Head Start is guided by a set of federal performance standards and other regulations, these have changed over time and may not reflect the experience of participants in all studies. A dummy variable was coded to indicate whether the program was a “modern” Head Start program, defined as post-1974, when the first set of Head Start quality guidelines were implemented. Although the majority of programs (75%) were older, 44% of effect sizes came from studies of modern Head Start programs.</p>
<p>Recognizing that the first iteration of Head Start was a shortened 6- to 8-week summer program, we also created a continuous variable indicating length of treatment measured in months and recentered at 2 months, so that the resulting coefficient indicates the effect of receiving a full academic year of Head Start versus a summer program.</p>
<p>Evaluations differ in the timing of the outcome assessments, and given that Head Start program impacts are found to decline over time, we included a continuous measure of the timing of the outcome, measured in months posttreatment. Given our screening criteria, this variable ranges from −2.47 to 12. Finally, we created a dummy variable indicating whether the evaluation was an article published in a peer-refereed journal. The reference category is an unpublished report, dissertation, or book chapter.</p>
</sec>
<sec id="section12-0162373712462453">
<title>Statistical Analysis</title>
<p>Our key research questions are the following: (a) What is the average program impact of Head Start on children’s cognitive and achievement outcomes? and (b) Is heterogeneity in effect sizes predicted by methodological aspects of the study design and attributes of the outcome measures? The nested structure of the data (effect sizes nested within programs) requires a multivariate, multilevel approach to modeling these associations (<xref ref-type="bibr" rid="bibr10-0162373712462453">de la Torre, Camilli, Vargas, &amp; Vernon, 2007</xref>). The Level 1 model (effect size level) is:</p>
<p>
<disp-formula id="disp-formula1-0162373712462453">
<mml:math display="block" id="math1-0162373712462453">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ES</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>ij</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mtext>i</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mtext>1i</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mtext>x</mml:mtext>
<mml:mrow>
<mml:mtext>1ij</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mo>.</mml:mo>
<mml:mspace width="0.5em"/>
<mml:mo>.</mml:mo>
<mml:mspace width="0.5em"/>
<mml:mo>.</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mtext>ki</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mtext>x</mml:mtext>
<mml:mrow>
<mml:mtext>kij</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mtext>e</mml:mtext>
<mml:mrow>
<mml:mtext>ij</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0162373712462453" xlink:href="10.3102_0162373712462453-eq1.tif"/>
</disp-formula>
</p>
<p>In this equation, the effect size <italic>j</italic> in program <italic>i</italic> is modeled as a function of the intercept (β<sub>0i</sub>), which represents the average (covariate adjusted) effect size for all programs; a series of key independent variables and related coefficients of interest (β<sub>1i</sub>x<sub>1ij</sub> + … + β<sub>ki</sub>x<sub>kij</sub>), which estimate the association between the effect size and aspects of the study design that vary at the effect size level; and a within-program error term (e<sub>ij</sub>). Study design covariates at this level include timing of outcome, type of outcome (rating or observation), whether or not baseline covariates are included, and domain of outcome (skills more or less sensitive to instruction).</p>
<p>The Level 2 equation (program level) models the intercept as a function of the grand mean effect size (β<sub>00</sub>), a series of covariates that represent aspects of study design and Head Start features that vary only at the program level (β<sub>01i</sub>x<sub>1i</sub> + … + β<sub>0ki</sub>x<sub>ki</sub>), and a between-program random error term (u<sub>i</sub>):</p>
<p>
<disp-formula id="disp-formula2-0162373712462453">
<mml:math display="block" id="math2-0162373712462453">
<mml:mrow>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mtext>i</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mn>00</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mtext>1i</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mtext>x</mml:mtext>
<mml:mrow>
<mml:mtext>1i</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mo>.</mml:mo>
<mml:mspace width="0.5em"/>
<mml:mo>.</mml:mo>
<mml:mspace width="0.5em"/>
<mml:mo>.</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mtext>ki</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mtext>x</mml:mtext>
<mml:mrow>
<mml:mtext>ki</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mtext>u</mml:mtext>
<mml:mtext>i</mml:mtext>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0162373712462453" xlink:href="10.3102_0162373712462453-eq2.tif"/>
</disp-formula>
</p>
<p>Variables at this level include type of research design, activity level of control group, and attrition. Covariates at this level include whether the effect size came from a peer-refereed journal article, the length of program, and whether the program was implemented post-1974.</p>
<p>This “mixed effects” model assumes that there are two sources of variation in the effect size distribution, beyond subject-level sampling error: (a) the “fixed” effects of variables that measure key features of the methods and other covariates and (b) remaining “random” unmeasured sources of variation between and within programs. Because of the small number of studies with multiple contrasts (<italic>n</italic> = 8) and multiple programs (<italic>n</italic> = 4), we do not model the third or fourth levels of nesting in our data (contrasts within programs and programs within studies, respectively).</p>
<p>To account for differences in effect size estimate precision, as well as the number of effect sizes within a particular program, all regressions were weighted by the inverse variance of each effect size multiplied by the inverse of the number of effect sizes per program (<xref ref-type="bibr" rid="bibr8-0162373712462453">Cooper &amp; Hedges, 2009</xref>; <xref ref-type="bibr" rid="bibr18-0162373712462453">Lipsey &amp; Wilson, 2001</xref>). Analyses were conducted in SAS, using the PROC MIXED procedure.</p>
<p>We began by entering each design factor independently and then included all relevant design covariates at the same time in our primary specification. We also tested several variations of the primary model specification; for example, we tested alternative specifications using a series of dummy variables indicating outcome measure reliability levels and a more nuanced set of research design variables. We conducted analyses including imputed missing effect sizes, without weights, and excluding the National Head Start Impact Study, the largest study in our sample.</p>
</sec>
</sec>
<sec id="section13-0162373712462453" sec-type="results">
<title>Results</title>
<sec id="section14-0162373712462453">
<title>Bivariate Results</title>
<p>The weighted result from an “empty model,” with no predictor variables, yields an intercept (average program-level effect size) of 0.27, which is significantly different from 0. As would be expected, this is very similar to the weighted mean estimated at the program level (0.29) but larger than the weighted mean at the effect size level (0.18). This illustrates the fact that there is more variation within evaluation programs than between programs (in both the weighted and unweighted data). In the weighted data, 80% of the variation in effect sizes is found within programs (and only 20% between programs). Perhaps somewhat surprisingly, programs that had negative or small effect sizes also tended to have large positive effect sizes.</p>
<p>Next, we turned to estimating the associations between single design factors and average effect size using a series of multilevel regressions (<xref ref-type="table" rid="table3-0162373712462453">Table 3</xref>). Regressions including categorical variables (<xref ref-type="table" rid="table3-0162373712462453">Table 3</xref>, columns 1–6 and 8–9) were run without intercepts; thus, the resulting coefficients indicate the average weighted effect size for programs in each category. Multilevel regressions including continuous measures of research design were run with an intercept; therefore, we include this estimate in columns 10 and 11 of <xref ref-type="table" rid="table3-0162373712462453">Table 3</xref> to show the relationship between an incremental increase in each continuous design variable and average effect size.</p>
<table-wrap id="table3-0162373712462453" position="float">
<label>Table 3</label>
<caption><p>Summary of Results From Regressions of Head Start Evaluation Effect Sizes on Single Research Design Factors</p></caption>
<graphic alternate-form-of="table3-0162373712462453" xlink:href="10.3102_0162373712462453-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7—Significant differences</th>
</tr>
</thead>
<tbody>
<tr>
<td>Modern HS (post-1974)</td>
<td>.23<xref ref-type="table-fn" rid="table-fn3-0162373712462453">*</xref></td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td>(.08)</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Not modern HS (pre-1975)</td>
<td>.29<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td>(.05)</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Active control group</td>
<td/>
<td>.08</td>
<td/>
<td/>
<td/>
<td/>
<td>Passive<sup><xref ref-type="table-fn" rid="table-fn3-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td/>
<td/>
<td>(.10)</td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Passive control group</td>
<td/>
<td>.31<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td/>
<td/>
<td>Active<sup><xref ref-type="table-fn" rid="table-fn3-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td/>
<td/>
<td>(.05)</td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Missing control group activity</td>
<td/>
<td>.29<xref ref-type="table-fn" rid="table-fn3-0162373712462453">*</xref></td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>(.10)</td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Randomized controlled trial</td>
<td/>
<td/>
<td>.33<xref ref-type="table-fn" rid="table-fn3-0162373712462453">*</xref></td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(.10)</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Quasi-experimental study</td>
<td/>
<td/>
<td>.26<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(.05)</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Rating (by adult who knows child)</td>
<td/>
<td/>
<td/>
<td>.45<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td>Performance<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>(.07)</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Observation (by researcher)</td>
<td/>
<td/>
<td/>
<td>.55<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td>Performance<sup><xref ref-type="table-fn" rid="table-fn3-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>(.16)</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Performance test</td>
<td/>
<td/>
<td/>
<td>.24<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td>Rating<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref>, observation<sup><xref ref-type="table-fn" rid="table-fn3-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>(.04)</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Skills sensitive to instruction</td>
<td/>
<td/>
<td/>
<td/>
<td>.40<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td>Skills not sensitive<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.05)</td>
<td/>
<td/>
</tr>
<tr>
<td>Skills not as sensitive to instruction</td>
<td/>
<td/>
<td/>
<td/>
<td>.25<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td>Skills sensitive<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.05)</td>
<td/>
<td/>
</tr>
<tr>
<td>High attrition (&gt; 10%)</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>.28<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.05)</td>
<td/>
</tr>
<tr>
<td>Low attrition (≤ 10%)</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>.33<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.06)</td>
<td/>
</tr>
<tr>
<td>Missing attrition information</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>.14</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.11)</td>
<td/>
</tr>
<tr>
<th/>
<th/>
<th/>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
<th align="center">11</th>
<th align="center">12—Significant differences</th>
</tr>
<tr>
<td colspan="3">Baseline covariates included</td>
<td>.20<xref ref-type="table-fn" rid="table-fn3-0162373712462453">*</xref></td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(.03)</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td colspan="3">Baseline covariates not included</td>
<td>.29<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>(.04)</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td colspan="3">Peer-refereed journal</td>
<td/>
<td>.43<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>(.09)</td>
<td/>
<td/>
<td>Not peer-refereed<sup><xref ref-type="table-fn" rid="table-fn3-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td colspan="3">Not peer-refereed journal</td>
<td/>
<td>.23<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>(.04)</td>
<td/>
<td/>
<td>Peer-refereed<sup><xref ref-type="table-fn" rid="table-fn3-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td colspan="3">Length of program (months, centered at 2)</td>
<td/>
<td/>
<td>.01</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.01)</td>
<td/>
<td/>
</tr>
<tr>
<td colspan="3">Months posttreatment</td>
<td/>
<td/>
<td>−.00</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.01)</td>
<td/>
<td/>
</tr>
<tr>
<td colspan="3">Intercept</td>
<td/>
<td/>
<td>.21<xref ref-type="table-fn" rid="table-fn3-0162373712462453">*</xref></td>
<td>.28<xref ref-type="table-fn" rid="table-fn3-0162373712462453">**</xref></td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>(.07)</td>
<td>(.04)</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0162373712462453">
<p><italic>Note</italic>. Multilevel models were estimated and regression coefficient estimates are reported with standard errors in parentheses provided below the estimates; <italic>N</italic> = 241 effect sizes nested in 28 programs. For columns 1–6 and 8–9, no intercept was estimated; therefore, the resulting coefficients represent the average effect size for programs in each category. Columns 7 and 12 list within-factor categorical means that are statistically significant compared with the indicated category. Multilevel regressions with continuous measures of research design were run with an intercept; therefore, estimates in columns 10 and 11 show the relationship between an incremental increase in each continuous design variable and average effect size.</p>
</fn>
<fn id="table-fn3-0162373712462453">
<label>†</label>
<p><italic>p</italic> &lt; .10. *<italic>p</italic> &lt; .05. **<italic>p</italic> &lt; .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Two features of the evaluation studies that we thought might predict the magnitude of effect sizes did not—analytic design (experimental vs. quasi-experimental and the inclusion of baseline covariates) and level of program or study attrition. One hypothesis was confirmed. We found that evaluation studies in which the control group actively sought alternative ECE services produced a smaller average effect size (0.08) than studies with passive control groups (0.31).</p>
<p>We found features of the outcomes themselves were quite strong predictors of program impact effect sizes. Consistent with our hypothesis, measures of skills more sensitive to instruction yielded a significantly larger average effect size (0.40) than measures of broader cognitive skills (0.25). Finally, we found that ratings (0.45) and observations (0.55) yielded significantly larger effect sizes than performance tests (0.24).</p>
<p>Only one of the covariates predicted effect size magnitudes. The average effect size from a study published in a peer-refereed journal (0.43) was larger than one produced by an unpublished study or book chapter (0.23), but this difference was only marginally significant. Whether the program was “modern,” the number of months since program completion, and the length of program were not associated with the size of effects.</p>
</sec>
<sec id="section15-0162373712462453">
<title>Multivariate Results</title>
<p>Our bivariate approach to modeling the predictive power of various study design features ignores the potential important confounds of other design variables; thus, it might yield biased results. Therefore, in our preferred primary specification, we included all design variables at once to investigate the independent and comparative role of each in affecting average effect size (column 1 of <xref ref-type="table" rid="table4-0162373712462453">Table 4</xref>). Coefficients from multivariate models indicate the strength of associations between our independent variables (measuring facets of research design) and effect sizes (differences between treatment and control groups expressed as a fraction of a standard deviation).</p>
<table-wrap id="table4-0162373712462453" position="float">
<label>Table 4</label>
<caption><p>Summary of Results From Multivariate Regressions of Head Start Evaluation Effect Sizes on Multiple Research Design Factors</p></caption>
<graphic alternate-form-of="table4-0162373712462453" xlink:href="10.3102_0162373712462453-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">1</th>
<th align="center">2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intercept</td>
<td>.30<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
<td>.33<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
</tr>
<tr>
<td/>
<td>(.14)</td>
<td>(.15)</td>
</tr>
<tr>
<td>Modern Head Start program (post-1974)</td>
<td>−.04</td>
<td>−.16</td>
</tr>
<tr>
<td/>
<td>(.12)</td>
<td>(.14)</td>
</tr>
<tr>
<td>Length of program (months, centered at 2)</td>
<td>.02<sup><xref ref-type="table-fn" rid="table-fn5-0162373712462453">†</xref></sup></td>
<td>.02<sup><xref ref-type="table-fn" rid="table-fn5-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td/>
<td>(.01)</td>
<td>(.01)</td>
</tr>
<tr>
<td>Peer-refereed journal</td>
<td>.28<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
<td>.31<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
</tr>
<tr>
<td/>
<td>(.09)</td>
<td>(.11)</td>
</tr>
<tr>
<td>Active control group</td>
<td>−.33<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
<td>−.35<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
</tr>
<tr>
<td/>
<td>(.12)</td>
<td>(.14)</td>
</tr>
<tr>
<td>Missing control group activity</td>
<td>−.01</td>
<td>−.07</td>
</tr>
<tr>
<td/>
<td>(.10)</td>
<td>(.12)</td>
</tr>
<tr>
<td>Quasi-experimental study</td>
<td>−.14</td>
<td>−.26<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
</tr>
<tr>
<td/>
<td>(.11)</td>
<td>(.12)</td>
</tr>
<tr>
<td>Baseline covariates included</td>
<td>−.11</td>
<td>−.06</td>
</tr>
<tr>
<td/>
<td>(.09)</td>
<td>(.10)</td>
</tr>
<tr>
<td>High attrition (&gt; 10%)</td>
<td>−.09</td>
<td>−.09</td>
</tr>
<tr>
<td/>
<td>(.07)</td>
<td>(.07)</td>
</tr>
<tr>
<td>Missing attrition information</td>
<td>−.04</td>
<td>.01</td>
</tr>
<tr>
<td/>
<td>(.13)</td>
<td>(.15)</td>
</tr>
<tr>
<td>Rating (by someone who knows child)</td>
<td>.16<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
<td/>
</tr>
<tr>
<td/>
<td>(.06)</td>
<td/>
</tr>
<tr>
<td>Observation (by researcher)</td>
<td>.32<xref ref-type="table-fn" rid="table-fn5-0162373712462453">*</xref></td>
<td/>
</tr>
<tr>
<td/>
<td>(.15)</td>
<td/>
</tr>
<tr>
<td>Skills sensitive to instruction</td>
<td>.13<xref ref-type="table-fn" rid="table-fn5-0162373712462453">**</xref></td>
<td>.17<xref ref-type="table-fn" rid="table-fn5-0162373712462453">**</xref></td>
</tr>
<tr>
<td/>
<td>(.03)</td>
<td>(.03)</td>
</tr>
<tr>
<td>Months posttreatment</td>
<td>.00</td>
<td>.00</td>
</tr>
<tr>
<td/>
<td>(.01)</td>
<td>(.01)</td>
</tr>
<tr>
<td>Medium reliability (coefficient = .75–.91)</td>
<td/>
<td>.09<sup><xref ref-type="table-fn" rid="table-fn5-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td/>
<td/>
<td>(.05)</td>
</tr>
<tr>
<td>Low reliability (coefficient &lt; .75)</td>
<td/>
<td>.20<xref ref-type="table-fn" rid="table-fn5-0162373712462453">**</xref></td>
</tr>
<tr>
<td/>
<td/>
<td>(.06)</td>
</tr>
<tr>
<td>Missing reliability coefficient</td>
<td/>
<td>.07<sup><xref ref-type="table-fn" rid="table-fn5-0162373712462453">†</xref></sup></td>
</tr>
<tr>
<td/>
<td/>
<td>(.04)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0162373712462453">
<p><italic>Note</italic>. Multilevel models were estimated with <italic>N</italic> = 241 effect sizes nested in 28 programs. Regression coefficient estimates are reported with standard errors provided in parentheses below the coefficients.</p>
</fn>
<fn id="table-fn5-0162373712462453">
<label>†</label>
<p><italic>p</italic> &lt; .10. *<italic>p</italic> &lt; .05. **<italic>p</italic> &lt; .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>In terms of program and study characteristics, we found that most study design features that were statistically significant in our bivariate analyses remained significant in our multivariate analyses. Results indicated a large negative association between effect size and having an active control group in which families independently sought alternative services (−0.33). Likewise, characteristics of the measures again predicted the magnitude of effect sizes. Compared with performance tests, both ratings by teachers and parents, as well as observational ratings by researchers, yielded larger effect sizes (0.16 and 0.32, respectively). As expected, measures of skills more sensitive to instruction produced larger effect sizes than those for less teachable cognitive skills (0.13).</p>
<p>If one considers a performance test a more reliable measure of children’s skills, compared to ratings by others, the fact that performance tests predicted smaller effect sizes is somewhat surprising. To test the role of measure reliability more directly, in an alternative specification we removed the variables indicating type of dependent measure (i.e., rating, observation) and instead included a series of dummy variables indicating the level of reliability of the outcome measure, based on coded reliability coefficients.<sup><xref ref-type="fn" rid="fn1-0162373712462453">1</xref></sup> Consistent with our primary specification findings, we found that <italic>less</italic> reliable measures yielded larger effect sizes (see <xref ref-type="table" rid="table4-0162373712462453">Table 4</xref>, column 2).</p>
<p>In terms of the control variables, again, we found evidence of possible publication bias, with studies published in peer-refereed journals also tending to yield effect sizes 0.28 standard deviations larger than those found in unpublished reports, dissertations, or book chapters. In addition, attending a full academic year of Head Start (10 months) emerged as marginally associated with a 0.16 standard deviation unit larger effect than attending a summer Head Start program (2 months).</p>
</sec>
<sec id="section16-0162373712462453">
<title>Robustness Checks</title>
<p>We undertook several additional analyses to determine the sensitivity of our findings to alternative model specifications. Most important, in some cases, authors reported that groups were compared on particular tests but did not report the results of these tests, or did not provide enough numerical information to compute an effect size (<italic>n</italic> = 72). We checked the robustness of results to four different assumptions about the magnitude of the missing effects and found that including the imputed effect sizes did not yield substantive changes in our findings (results available from the authors upon request).</p>
<p>We also tested a more nuanced set of research design indicators, distinguishing among types of nonexperimental studies (matching methods, baseline comparable treatment and control groups without matching, use of pretests), and none of these design indicator variables was statistically significant (compared with the reference category, randomization). We also estimated a model with a continuous measure of the year in which the program was studied, and results were similar. Finally, a possible remaining source of heterogeneity in our data is the demographic makeup of the study samples. Unfortunately, only about half of the studies reported demographic characteristics of the study samples in a way that we could quantify. Nevertheless, with the information available, we explored whether effect sizes might be predicted by the gender and by the racial composition of the sample (percentage boys versus girls; percentage Black, Hispanic, White). Findings indicated that effect sizes were not significantly predicted by these characteristics, in bivariate or multivariate models. Taken together, these findings suggest that our results are robust to a variety of alternative specifications and are not sensitive to the particular models we estimated.</p>
<p>Another concern is that results were being driven primarily by the National Head Start Impact Study, which includes 40 effect sizes and is heavily weighted because of its large sample size. When we excluded this study from our analysis, however, we obtained largely similar results. The magnitude of most coefficients stayed the same, although, because of a loss of statistical power, some of them became statistically insignificant (results available from the authors upon request). These findings suggest that the relationships between effect sizes and research design factors are not unique to the National Head Start Impact Study. Finally, we also found similar results for unweighted analyses, suggesting that studies with larger samples are not driving our findings.</p>
</sec>
</sec>
<sec id="section17-0162373712462453" sec-type="discussion">
<title>Discussion</title>
<p>This study provides an important contribution to the field of ECE research by using a new meta-analytic database to estimate the overall average effect of Head Start on children’s cognitive skills and achievement and exploring the role of methodological factors in explaining effect size variation. Understanding the role of research study design and methods is important so that scholars and policymakers better understand the empirical evidence about Head Start’s effectiveness. Overall, we found a statistically significant average effect size of 0.27, suggesting that the accumulated evaluation studies find that Head Start is effective in improving children’s short-term (less than 1 year posttreatment) cognitive and achievement outcomes. Several research design factors significantly predicted the heterogeneity of effect sizes. These factors accounted for approximately 41% of the variation between evaluation findings and 11% of the variation within evaluations, suggesting that evaluation research design can be quite consequential.</p>
<p>The resulting average 0.27 effect size suggests that Head Start program effects on children’s cognitive and achievement outcomes are on par with the effects of other large-scale ECE programs. This is a somewhat smaller effect on achievement and cognitive outcomes than found in the previous meta-analysis of Head Start conducted by <xref ref-type="bibr" rid="bibr21-0162373712462453">McKey et al. (1985)</xref> but larger than those reported in the first-year findings from the recent National Head Start Impact Study (<xref ref-type="bibr" rid="bibr32-0162373712462453">U.S. DHHS, Administration for Children and Families, 2005</xref>). The 0.27 estimate is also within the range of the overall average effect sizes on cognitive outcomes found in <xref ref-type="bibr" rid="bibr3-0162373712462453">Camilli et al. (2010)</xref>, measured across a wider set of ECE programs, and in <xref ref-type="bibr" rid="bibr37-0162373712462453">Wong et al. (2008)</xref>, measured in state prekindergarten programs, but smaller than the short-term cognitive effect sizes found in meta-analyses of more intensive programs with longitudinal follow-ups conducted (<xref ref-type="bibr" rid="bibr12-0162373712462453">Gorey, 2001</xref>; <xref ref-type="bibr" rid="bibr24-0162373712462453">Nelson et al., 2003</xref>).</p>
<p>Our substantively largest finding is that having an active control group—one in which children experienced other forms of center-based education and care—is associated with much smaller effect sizes than those produced by studies in which the control group is “passive” (i.e., receives no alternative ECE). Given that other types of ECE and center-based child care programs also increase children’s cognitive skills and achievement, this is to be expected (<xref ref-type="bibr" rid="bibr13-0162373712462453">Gormley, Phillips, &amp; Gayer, 2008</xref>; <xref ref-type="bibr" rid="bibr14-0162373712462453">Henry, Gordon, &amp; Rickman, 2006</xref>). Today, almost 70% of 4-year-olds and 40% of 3-year-olds attend some form of ECE (<xref ref-type="bibr" rid="bibr5-0162373712462453">Cook, 2006</xref>); thus, an active control group is likely to be the norm. As a result, Head Start evaluations in communities where many of the control group children have access to other ECE programs are likely to produce substantially smaller effect sizes than those in communities where there are few other ECE programs available. Such a pattern of small or even null effect sizes does not indicate that Head Start is ineffective at improving low-income children’s achievement and cognitive outcomes but follows from the fact that an array of other public and private ECE programs are both accessible and effective in improving low-income children’s cognitive and achievement outcomes (<xref ref-type="bibr" rid="bibr38-0162373712462453">Zhai, Waldfogel, &amp; Brooks-Gunn, 2010</xref>).</p>
<p>Our analysis is limited to Head Start evaluations, and it is unclear if our findings about the importance of active control groups will generalize to the evaluation of other ECE programs. If these findings were replicated with other types of programs, this result would also suggest that findings from the National Head Start Impact Study, which had relatively high rates of center-care attendance in the control group, and recent regression discontinuity design evaluations of state prekindergarten programs that have lower levels of such attendance in the control group are not directly comparable (<xref ref-type="bibr" rid="bibr5-0162373712462453">Cook, 2006</xref>). Thus, claims by some that Head Start is less effective than state prekindergarten programs seem premature, as the higher rate of center-based care and other ECE program attendance among the control group would predict smaller effect sizes for the Head Start study (also see <xref ref-type="bibr" rid="bibr11-0162373712462453">Gibbs, Ludwig, &amp; Miller, 2012</xref>).</p>
<p>A somewhat surprising finding from the current study is that the type of overall design (e.g., randomized vs. quasi-experimental) did not predict effect size. Based on prior work, we expected that more rigorous designs would yield larger effect sizes; however, our inclusion criteria regarding study design were typically more rigorous than those of previous meta-analyses of ECE programs. By limiting our study sample in this way, we give up some of the variation in design that might otherwise have predicted effect sizes. Nevertheless, these findings are in alignment with those of recent within-study research suggesting that in certain circumstances rigorous quasi-experimental methods can produce causal estimates similar to those produced by randomized controlled trials (<xref ref-type="bibr" rid="bibr6-0162373712462453">Cook et al., 2008</xref>) and further support the use of such methods to evaluate programs when randomized controlled trials are not feasible, as is often the case in education research (<xref ref-type="bibr" rid="bibr28-0162373712462453">Schneider et al., 2007</xref>).</p>
<p>We also predicted that attrition would be negatively associated with effect size; however, attrition was not a significant factor. The fact that the range of each measure was truncated in this study (attrition to less than 50% and posttreatment outcome measure timing to 12 or fewer months) may explain this lack of findings. In addition, we were able to investigate only overall study attrition, and it may be that what is most important in predicting program impacts is differential attrition across the control and treatment groups.</p>
<p>For these achievement and cognitive outcomes, more variability was found within program evaluations than across them, suggesting that features of the measures themselves are likely to be an important source of heterogeneity in program evaluation results. We found that the type of dependent measure is systematically related to effect size. Consistent with previous research, we found that achievement-based skills such as early reading, early math, and letter recognition skills appear to be more sensitive to Head Start attendance than cognitive skills such as IQ, vocabulary, and attention, which are less sensitive to classroom instruction (<xref ref-type="bibr" rid="bibr4-0162373712462453">Christian et al., 2000</xref>; <xref ref-type="bibr" rid="bibr37-0162373712462453">Wong et al., 2008</xref>). This finding has important implications for designers and evaluators of early intervention programs, namely, that expectations for effects on omnibus measures such as vocabulary or IQ should be lowered. At minimum, these sets of skills should be tested and considered separately.</p>
<p>Our finding that less reliable dependent measures yield larger effect sizes also argues for considering the quality of the measures when interpreting program evaluations results. Nonstandardized measures developed by researchers may tap into behaviors that are among those most directly targeted by the intervention services; therefore, it is not surprising that such measures tend to yield larger effect sizes. Ratings by parents, teachers, and researchers may also be subject to bias, however, because these individuals are likely to be aware of children’s participation in Head Start as well as the study purpose. In assessing program effectiveness, it is important to compare measures that are similar not only in content but also in method of assessment.</p>
<p>Although they were not the focus of our analyses, our control measures yielded some interesting results. We found that effect sizes from studies published in peer-refereed journals are larger than those found in unpublished reports and book chapters. Although research published in peer-refereed journals may be more rigorous than that found in unpublished sources, this result may also be a sign of the “file drawer” problem (i.e., that negative or null findings are less likely to be published) long lamented by meta-analysts (<xref ref-type="bibr" rid="bibr18-0162373712462453">Lipsey &amp; Wilson, 2001</xref>). This finding suggests that meta-analysts must be exhaustive in their searches for both published and unpublished studies and should code information regarding study quality (<xref ref-type="bibr" rid="bibr26-0162373712462453">Rothstein &amp; Hopewell, 2009</xref>).</p>
<p>In addition to the limitations already noted, we offer a few other caveats about this study. First, the nested nature of our data also posed analytic challenges that we could not overcome. Although our multilevel models account for the nesting of effect sizes within programs, there were additional sources of nonindependence in the data set that we were unable to model.</p>
<p>Second, the variation in methods and research design we exploited is naturally occurring; thus, our results are descriptive rather than causal. We modeled characteristics of the assessment measures at the effect size level, so these estimates are identified by variation in measures within evaluations. But our estimates of the predictive power of research design are identified by variation across evaluations and may be biased by unobserved correlates such as the quality of the program or the characteristics of local communities. Studies in other fields have minimized such bias by capitalizing on within-study variation to understand the role of research design, but, to date, such efforts have focused only on understanding under what conditions quasi-experimental designs replicate experimental designs (<xref ref-type="bibr" rid="bibr6-0162373712462453">Cook et al., 2008</xref>; <xref ref-type="bibr" rid="bibr29-0162373712462453">Shadish et al., 2008</xref>; <xref ref-type="bibr" rid="bibr30-0162373712462453">Shadish et al., 2011</xref>). Our meta-analysis is a useful and robust method to summarize studies from Head Start and to improve our understanding of how research design predicts effect sizes. Nevertheless, this study should serve only as an important first step in this line of inquiry. Future studies should undertake within-study design comparisons such as those reviewed in <xref ref-type="bibr" rid="bibr6-0162373712462453">Cook and colleagues (2008)</xref>, as these have the advantage of controlling for unobserved differences across evaluations.</p>
<p>By analyzing the findings of 28 Head Start evaluations, this study makes an important contribution to the field of ECE research. Although a substantial body of literature indicates that Head Start has meaningful effects on children’s cognitive and achievement outcomes, there are a variety of factors that might explain the magnitude of Head Start effects that are about the evaluation research designs, not just the program itself. Thus, comparing results from evaluation studies requires attention to both how the studies are conducted and which instruments are used to assess outcomes. By becoming more critical consumers and designers of such research, we can better understand how well Head Start programs are serving children and families. Important research design features that should be considered include whether the comparison group attended other ECE programs and characteristics of the outcome assessment; specifically, the content, method of assessment, and reliability of the instruments. Facing scarce resources and difficult funding decisions, policymakers would benefit greatly from the ability to compare the costs of different programs relative to their expected benefits across a broad set of school readiness indicators; thus, future work should consider whether and how estimates from diverse evaluations with differing methods and measures can be most effectively compared.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-0162373712462453">
<label>Appendix</label>
<sec id="section18-0162373712462453">
<title>Head Start Studies, Programs, and Contrasts Included in the Analysis</title>
<table-wrap id="table5-0162373712462453" position="float">
<graphic alternate-form-of="table5-0162373712462453" xlink:href="10.3102_0162373712462453-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Start date</th>
<th align="center">Study description</th>
<th align="center">Programs and contrasts included</th>
</tr>
</thead>
<tbody>
<tr>
<td valign="top">1965</td>
<td valign="top">Lincoln, Nebraska Summer Head Start</td>
<td>
<list id="list1-0162373712462453" list-type="order">
<list-item><p>Matched pairs: Head Start vs. No Head Start, no preschool (stay at home)</p></list-item>
<list-item><p>Unmatched pairs: Head Start vs. no Head Start, no preschool (stay at home)</p></list-item>
</list>
</td>
</tr>
<tr>
<td>1965</td>
<td>Duluth Summer Head Start</td>
<td>1. Head Start vs. no Head Start, no preschool (stay at home)</td>
</tr>
<tr>
<td>1965</td>
<td>Westinghouse-Ohio National Head Start Evaluation, 1965–1968</td>
<td>1. Head Start vs. no Head Start, no preschool</td>
</tr>
<tr>
<td>1965</td>
<td>Camden, New Jersey Summer Head Start</td>
<td>1. Summer Head Start vs. no Summer Head Start</td>
</tr>
<tr>
<td>1965</td>
<td>New Jersey Summer Head Start</td>
<td>1. One or two years of Summer Head Start vs. no Summer Head Start</td>
</tr>
<tr>
<td>1965</td>
<td>Cambridge, Massachusetts Summer Head Start</td>
<td>1. Summer Head Start vs. no Summer Head Start and Operation Checkup (medical exam)</td>
</tr>
<tr>
<td>1965</td>
<td>Cleveland Summer Head Start</td>
<td>1. Summer Head Start vs. no Head Start</td>
</tr>
<tr>
<td>1965</td>
<td>Kearney, Nebraska Summer Head Start</td>
<td>1. Summer Head Start vs. no Summer Head Start</td>
</tr>
<tr>
<td>1965</td>
<td>San Jose, California Summer Head Start</td>
<td>1. Summer Head Start vs. no Summer Head Start</td>
</tr>
<tr>
<td>1966<sup><xref ref-type="table-fn" rid="table-fn7-0162373712462453">a</xref></sup></td>
<td>New Haven Head Start Evaluation, Smaller Follow-up</td>
<td>1. Head Start vs. no Head Start</td>
</tr>
<tr>
<td>1966<sup><xref ref-type="table-fn" rid="table-fn7-0162373712462453">a</xref></sup></td>
<td>New Haven Head Start Evaluation, Larger Follow-up</td>
<td>1. Head Start vs. no Head Start</td>
</tr>
<tr>
<td>1966</td>
<td>Washington, DC Summer Head Start</td>
<td>1. Summer Head Start vs. no Head Start</td>
</tr>
<tr>
<td>1966<sup><xref ref-type="table-fn" rid="table-fn7-0162373712462453">a</xref></sup></td>
<td>Dade County, Florida Head Start Program Study (effects on self-concept, social skills, and language skills)</td>
<td>1. Head Start vs. no Head Start, no preschool</td>
</tr>
<tr>
<td>1966</td>
<td>Bicultural Preschool Program (Mexican American children)</td>
<td>1. Head Start vs. no Head Start, no preschool</td>
</tr>
<tr>
<td>1967<sup><xref ref-type="table-fn" rid="table-fn7-0162373712462453">a</xref></sup></td>
<td>New York City Head Start</td>
<td>1. Head Start vs. children about to enter Head Start</td>
</tr>
<tr>
<td>1967</td>
<td>Head Start, UCLA Evaluation</td>
<td>1. Head Start vs. no Head Start</td>
</tr>
<tr>
<td valign="top">1968</td>
<td valign="top">Rural Minnesota Head Start</td>
<td>
<list id="list2-0162373712462453" list-type="order">
<list-item><p>Head Start vs. no Head Start, but eligible for Head Start</p></list-item>
<list-item><p>Head Start vs. no Head Start, no preschool, and not eligible for Head Start</p></list-item>
</list>
</td>
</tr>
<tr>
<td valign="top">1968</td>
<td valign="top">Louisville Head Start Curriculum Comparison</td>
<td>
<list id="list3-0162373712462453" list-type="order">
<list-item><p>Bereiter–Engelmann Head Start vs. no Head Start, no preschool</p></list-item>
<list-item><p>DARCEE Head Start vs. no Head Start, no preschool</p></list-item>
<list-item><p>Montessori Head Start vs. no Head Start, no preschool</p></list-item>
<list-item><p>Traditional Head Start vs. no Head Start, no preschool</p></list-item>
</list>
</td>
</tr>
<tr>
<td valign="top">1968</td>
<td valign="top">Evaluation of Standard Head Start and Direct Instruction Head Start</td>
<td>
<list id="list4-0162373712462453" list-type="order">
<list-item><p>Head Start with Bereiter–Engelmann curriculum (direct instruction) vs. no Head Start, no preschool</p></list-item>
<list-item><p>Standard Head Start vs. no Head Start, no preschool</p></list-item>
</list>
</td>
</tr>
<tr>
<td>1969</td>
<td>Educational Testing Services Longitudinal Head Start Evaluation (Portland, Oregon and Trenton, New Jersey)</td>
<td>1. Head Start vs. no Head Start, no preschool</td>
</tr>
<tr>
<td valign="top">1971</td>
<td valign="top">Planned Variation Head Start Study</td>
<td>
<list id="list5-0162373712462453" list-type="order">
<list-item><p>Head Start with an added formal approach or curriculum vs. no Head Start, no preschool</p></list-item>
<list-item><p>Standard Head Start vs. no Head Start, no preschool</p></list-item>
</list></td>
</tr>
<tr>
<td>Start date</td>
<td>Study description</td>
<td>Programs and contrasts included</td>
</tr>
<tr>
<td valign="top">1979</td>
<td valign="top">Head Start Bilingual Bicultural Curriculum Models Project</td>
<td>
<list id="list6-0162373712462453" list-type="order">
<list-item><p>Bilingual Bicultural Head Start vs. no Head Start, no preschool (stay at home)</p></list-item>
<list-item><p>Standard Head Start vs. no Head Start, no preschool (stay at home)</p></list-item>
</list></td>
</tr>
<tr>
<td>1980<sup><xref ref-type="table-fn" rid="table-fn7-0162373712462453">a</xref></sup></td>
<td>New Haven Public-School-Sponsored Head Start</td>
<td>1. Head Start vs. no Head Start, no preschool</td>
</tr>
<tr>
<td>1985</td>
<td>Guam Head Start Evaluation</td>
<td>1. Head Start vs. no Head Start</td>
</tr>
<tr>
<td valign="top">1997</td>
<td valign="top">Early Childhood Longitudinal Study–Kindergarten Cohort Head Start Study</td>
<td>
<list id="list7-0162373712462453" list-type="order">
<list-item><p>White children: Head Start vs. no Head Start (mix of stay at home, other preschool, and child care)</p></list-item>
<list-item><p>Black children: Head Start vs. no Head Start (mix of stay at home, other preschool, and child care)</p></list-item>
<list-item><p>Hispanic children: Head Start vs. no Head Start (mix of stay at home, other preschool, and child care)</p></list-item>
</list>
</td>
</tr>
<tr>
<td>1998</td>
<td>Southeastern Head Start program of high quality</td>
<td>1. Head Start vs. Head Start wait list</td>
</tr>
<tr>
<td valign="top">2002</td>
<td valign="top">National Head Start Impact Study, First Year</td>
<td>
<list id="list8-0162373712462453" list-type="order">
<list-item><p>Three-year-olds, intent to treat ordinary least squares (weighted, controlling for demographics and pretest scores): Head Start vs. no Head Start (includes crossovers)</p></list-item>
<list-item><p>Four-year-olds, intent to treat ordinary least squares (weighted, controlling for demographics and pretest scores): Head Start vs. no Head Start</p></list-item>
<list-item><p>Three-year-olds, treatment on treated (Ludwig &amp; Phillips analysis): Head Start vs. no Head Start</p></list-item>
<list-item><p>Four-year-olds, treatment on treated (Ludwig &amp; Phillips analysis): Head Start vs. no Head Start</p></list-item>
</list></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0162373712462453">
<p><italic>Note.</italic> Unless otherwise specified, Head Start refers to a full-year academic program. Contrasts within a single program are numbered. Please see the following website for a full list of references for these studies: <ext-link ext-link-type="uri" xlink:href="http://developingchild.harvard.edu/download_file/-/view_inline/1203/">http://developingchild.harvard.edu/download_file/-/view_inline/1203/</ext-link>.</p>
</fn>
<fn id="table-fn7-0162373712462453">
<label>a.</label>
<p>If an actual start <italic>date</italic> for the program was not provided, we estimated the start date to be 2 years prior to report publication.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</app>
</app-group>
<fn-group>
<fn fn-type="financial-disclosure">
<p>The authors disclosed receipt of the following financial support for the research and/or authorship of this article: We are grateful to the following funders of the National Forum on Early Childhood Policy and Programs: the Birth to Five Policy Alliance, the Buffett Early Childhood Fund, Casey Family Programs, the McCormick Tribune Foundation, the Norlien Foundation, Harvard University, and an Anonymous Donor. We are also grateful to the Institute of Education Sciences, US Department of Education for supporting this research (#R305A110035), to Abt Associates, Inc. and the National Institute for Early Education Research for making their data available to us. Shager's work on this project was supported by the Institute of Education Sciences, U.S. Department of Education grant to the University of Wisconsin-Madison (#R305C050055).</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0162373712462453">
<label>1.</label>
<p>Categories were constructed based on quartile scores and defined as follows: high reliability (reference category), greater than or equal to .92 (representing Quartile 4); medium reliability, .91 to .75 (representing Quartiles 2 and 3); low reliability, less than .75, (representing Quartile 1). Our preference was to code any reliability coefficient provided for the specific study population; however, this information was rarely reported. If no coefficient was provided in the report, we attempted to find a reliability estimate from test manuals or another study. Any available type of reliability coefficient was recorded, although most were measures of internal consistency (Cronbach’s alpha). Because of this variability in source information and coefficient type, and the fact that we were still left with missing reliability coefficients for 38% of our effect sizes, we offer these results with caution.</p>
</fn>
</fn-group>
</notes>
<bio>
<title>Authors</title>
<p>HILARY M. SHAGER is a research analyst at the Wisconsin Department of Children and Families, 201 East Washington Avenue, Second Floor, P.O. Box 8916, Madison, WI 53708-8916, USA; <email>hilary.shager@wisconsin.gov</email>. Her primary research interests include early childhood education and social welfare policy.</p>
<p>HOLLY S. SCHINDLER is an assistant professor in the College of Education at the University of Washington, Miller Hall, Box 353600, Seattle, WA 98195-3600, USA, <email>hschindl@uw.edu</email>. Her research focuses on early childhood and family studies.</p>
<p>KATHERINE A. MAGNUSON is an associate professor of social work at the University of Wisconsin-Madison, and the Associate Director of the Institute for Poverty Research; 1350 University Ave., Madison, WI 53796; <email>kmagnuson@wisc.edu</email>. Her research focuses on early childhood and social welfare policy.</p>
<p>GREG J. DUNCAN is a Distinguished Professor in the School of Education at University of California, Irvine, 2056 Education, Mail code: 5500 Irvine CA 92697; <email>gduncan@uci.edu</email>. He has published extensively on child poverty and the importance of early academic skills, cognitive and emotional self-regulation as well as health in promoting children’s eventual success in school and the labor market.</p>
<p>HIROKAZU YOSHIKAWA is the Walter H. Gale Professor of Education at the Harvard Graduate School of Education; <email>yoshikhi@gse.harvard.edu</email>.</p>
<p>CASSANDRA M. D. HART is an assistant professor of education at the University of California, Davis; One Shields Avenue, Davis, CA 95616; <email>cmdhart@ucdavis.edu</email>. Her primary focus is on quantitative evaluations of state and national education policies.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Besharov</surname><given-names>D. J.</given-names></name>
<name><surname>Higney</surname><given-names>C. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Response to Barnett and Currie</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>26</volume>(<issue>3</issue>), <fpage>686</fpage>–<lpage>688</lpage>.</citation>
</ref>
<ref id="bibr2-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Borenstein</surname><given-names>M.</given-names></name>
<name><surname>Hedges</surname><given-names>L.</given-names></name>
<name><surname>Higgins</surname><given-names>J.</given-names></name>
<name><surname>Rothstein</surname><given-names>H.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Comprehensive Meta-Analysis (Version 2) [Computer software]</article-title>. <publisher-loc>Englewood, NJ</publisher-loc>: <publisher-name>Biostat</publisher-name>.</citation>
</ref>
<ref id="bibr3-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Camilli</surname><given-names>G.</given-names></name>
<name><surname>Vargas</surname><given-names>S.</given-names></name>
<name><surname>Ryan</surname><given-names>S.</given-names></name>
<name><surname>Barnett</surname><given-names>W. S.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Meta-analysis of the effects of early education interventions on cognitive and social development</article-title>. <source>Teachers College Record</source>, <volume>112</volume>(<issue>3</issue>), <fpage>579</fpage>–<lpage>620</lpage>.</citation>
</ref>
<ref id="bibr4-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christian</surname><given-names>K.</given-names></name>
<name><surname>Morrison</surname><given-names>F. J.</given-names></name>
<name><surname>Frazier</surname><given-names>J. A.</given-names></name>
<name><surname>Massetti</surname><given-names>G.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Specificity in the nature and timing of cognitive growth in kindergarten and first grade</article-title>. <source>Journal of Cognition and Development</source>, <volume>1</volume>(<issue>4</issue>), <fpage>429</fpage>–<lpage>448</lpage>.</citation>
</ref>
<ref id="bibr5-0162373712462453">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Cook</surname><given-names>T.</given-names></name>
</person-group> (<year>2006</year>). <source>What works in publicly funded pre-kindergarten education?</source> <conf-name>Presented at the Children's Achievement: What the Evidence Says about Teachers, Pre-K Programs and Economic Policies Policy Briefing</conf-name>, <conf-loc>Washington, D.C</conf-loc>.</citation>
</ref>
<ref id="bibr6-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cook</surname><given-names>T. D.</given-names></name>
<name><surname>Shadish</surname><given-names>W. R.</given-names></name>
<name><surname>Wong</surname><given-names>V. C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Three conditions under which experiments and observational studies produce comparable causal estimates: New findings from within-study comparisons</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>27</volume>(<issue>4</issue>), <fpage>724</fpage>–<lpage>750</lpage>.</citation>
</ref>
<ref id="bibr7-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cook</surname><given-names>T. D.</given-names></name>
<name><surname>Wong</surname><given-names>V. C.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The warrant for universal pre-K: Can several thin reeds make a strong policy boat?</article-title> <source>Social Policy Report</source>, <volume>21</volume>(<issue>3</issue>), <fpage>14</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr8-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cooper</surname><given-names>H.</given-names></name>
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Research synthesis as a scientific process</article-title>. In <person-group person-group-type="editor">
<name><surname>Cooper</surname><given-names>H.</given-names></name>
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
<name><surname>Valentine</surname><given-names>J. C.</given-names></name>
</person-group> (Eds.), <source>The handbook of research synthesis and meta-analysis</source> (<edition>2nd ed.</edition>, pp. <fpage>3</fpage>–<lpage>17</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Russell Sage</publisher-name>.</citation>
</ref>
<ref id="bibr9-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Currie</surname><given-names>J.</given-names></name>
<name><surname>Thomas</surname><given-names>D.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Does Head Start make a difference?</article-title> <source>American Economic Review</source>, <volume>85</volume>, <fpage>341</fpage>–<lpage>364</lpage>.</citation>
</ref>
<ref id="bibr10-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>de la Torre</surname><given-names>J.</given-names></name>
<name><surname>Camilli</surname><given-names>G.</given-names></name>
<name><surname>Vargas</surname><given-names>S.</given-names></name>
<name><surname>Vernon</surname><given-names>R. F.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Illustration of a multilevel model for meta-analysis</article-title>. <source>Measurement and Evaluation in Counseling and Development</source>, <volume>40</volume>, <fpage>169</fpage>–<lpage>180</lpage>.</citation>
</ref>
<ref id="bibr11-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gibbs</surname><given-names>C.</given-names></name>
<name><surname>Ludwig</surname><given-names>J.</given-names></name>
<name><surname>Miller</surname><given-names>D.</given-names></name>
</person-group> (<year>2012</year>). <source>Does Head Start do any lasting good?</source> (Working Paper 17452). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research</publisher-name>.</citation>
</ref>
<ref id="bibr12-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gorey</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Early childhood education: A meta-analytic affirmation of the short- and long-term benefits of educational opportunity</article-title>. <source>School Psychology Quarterly</source>, <volume>16</volume>(<issue>1</issue>), <fpage>9</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr13-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gormley</surname><given-names>W. T.</given-names><suffix>Jr.</suffix></name>
<name><surname>Phillips</surname><given-names>D.</given-names></name>
<name><surname>Gayer</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Preschool programs can boost school readiness</article-title>. <source>Science</source>, <volume>320</volume>, <fpage>1723</fpage>–<lpage>1724</lpage>.</citation>
</ref>
<ref id="bibr14-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Henry</surname><given-names>G. T.</given-names></name>
<name><surname>Gordon</surname><given-names>C. S.</given-names></name>
<name><surname>Rickman</surname><given-names>D. K.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Early education policy alternatives: Comparing quality and outcomes of Head Start and state prekindergarten</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>28</volume>, <fpage>77</fpage>–<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr15-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hoyt</surname><given-names>W. T.</given-names></name>
<name><surname>Kerns</surname><given-names>M.-D.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Magnitude and moderators of bias in observer ratings: A meta-analysis</article-title>. <source>Psychological Methods</source>, <volume>4</volume>(<issue>4</issue>), <fpage>403</fpage>–<lpage>424</lpage>.</citation>
</ref>
<ref id="bibr16-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>R. T.</given-names></name>
<name><surname>Creps</surname><given-names>C. L.</given-names></name>
<name><surname>Boulay</surname><given-names>B.</given-names></name>
</person-group> (<year>2004</year>). <source>Meta-analysis of research and evaluation studies in early childhood education</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Abt Associates</publisher-name>.</citation>
</ref>
<ref id="bibr17-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Layzer</surname><given-names>J. I.</given-names></name>
<name><surname>Goodson</surname><given-names>B. D.</given-names></name>
<name><surname>Bernstein</surname><given-names>L.</given-names></name>
<name><surname>Price</surname><given-names>C.</given-names></name>
</person-group> (<year>2001</year>). <source>National evaluation of family support programs, volume A: The meta-analysis, final report</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Abt Associates</publisher-name>.</citation>
</ref>
<ref id="bibr18-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lipsey</surname><given-names>M. W.</given-names></name>
<name><surname>Wilson</surname><given-names>D. B.</given-names></name>
</person-group> (<year>2001</year>). <source>Practical meta-analysis</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr19-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ludwig</surname><given-names>J.</given-names></name>
<name><surname>Phillips</surname><given-names>D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The benefits and costs of Head Start</article-title>. <source>Social Policy Report</source>, <volume>21</volume>(<issue>3</issue>), <fpage>3</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr20-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Magnuson</surname><given-names>K.</given-names></name>
<name><surname>Shager</surname><given-names>H.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Early education: Progress and promise for children from low-income families</article-title>. <source>Children and Youth Services Review</source>, <volume>32</volume>(<issue>9</issue>), <fpage>1186</fpage>-<lpage>1198</lpage>.</citation>
</ref>
<ref id="bibr21-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McKey</surname><given-names>R. H.</given-names></name>
<name><surname>Condelli</surname><given-names>L.</given-names></name>
<name><surname>Ganson</surname><given-names>H.</given-names></name>
<name><surname>Barrett</surname><given-names>B. J.</given-names></name>
<name><surname>McConkey</surname><given-names>C.</given-names></name>
<name><surname>Plantz</surname><given-names>M. C.</given-names></name>
</person-group> (<year>1985</year>). <source>The impact of Head Start on children, families and communities: Final report of the Head Start Evaluation, Synthesis and Utilization Project</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>CSR</publisher-name>.</citation>
</ref>
<ref id="bibr22-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moher</surname><given-names>D.</given-names></name>
<name><surname>Pham</surname><given-names>B.</given-names></name>
<name><surname>Jones</surname><given-names>A.</given-names></name>
<name><surname>Cook</surname><given-names>D. J.</given-names></name>
<name><surname>Jadad</surname><given-names>A. R.</given-names></name>
<name><surname>Moher</surname><given-names>M.</given-names></name>
<name><surname> . . . Klassen</surname><given-names>T. P.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Does quality of reports of randomized trials affect estimates of intervention efficacy reported in meta-analyses?</article-title> <source>Lancet</source>, <volume>352</volume>, <fpage>609</fpage>–<lpage>613</lpage>.</citation>
</ref>
<ref id="bibr23-0162373712462453">
<citation citation-type="web">
<collab>National Forum on Early Childhood Policy and Programs</collab>. (<year>2010</year>). <source>Understanding the Head Start Impact Study</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.developingchild.harvard.edu/">http://www.developingchild.harvard.edu/</ext-link></citation>
</ref>
<ref id="bibr24-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nelson</surname><given-names>G.</given-names></name>
<name><surname>Westhues</surname><given-names>A.</given-names></name>
<name><surname>MacLeod</surname><given-names>J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>A meta-analysis of longitudinal research on preschool prevention programs for children</article-title>. <source>Prevention and Treatment</source>, <volume>6</volume>, <fpage>1</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr25-0162373712462453">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Rosenshine</surname><given-names>B.</given-names></name>
</person-group> (<year>2010</year>, <month>March</month>). <source>Researcher-developed tests and standardized tests: A review of ten meta-analyses</source>. <conf-name>Paper presented at the Society for Research on Educational Effectiveness conference</conf-name>, <conf-loc>Washington, DC</conf-loc>.</citation>
</ref>
<ref id="bibr26-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rothstein</surname><given-names>H. R.</given-names></name>
<name><surname>Hopewell</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Grey literature</article-title>. In <person-group person-group-type="editor">
<name><surname>Cooper</surname><given-names>H.</given-names></name>
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
<name><surname>Valentine</surname><given-names>J. C.</given-names></name>
</person-group> (Eds.), <source>The handbook of research synthesis and meta-analysis</source> (<edition>2nd ed.</edition>, pp. <fpage>103</fpage>–<lpage>125</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Russell Sage</publisher-name>.</citation>
</ref>
<ref id="bibr27-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schulz</surname><given-names>K. F.</given-names></name>
<name><surname>Chalmers</surname><given-names>I.</given-names></name>
<name><surname>Hayes</surname><given-names>R. J.</given-names></name>
<name><surname>Altman</surname><given-names>D. G.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Empirical evidence of bias: Dimensions of methodological quality associated with estimates of treatment effects in controlled trials</article-title>. <source>Journal of the American Medical Association</source>, <volume>273</volume>(<issue>5</issue>), <fpage>408</fpage>–<lpage>412</lpage>.</citation>
</ref>
<ref id="bibr28-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schneider</surname><given-names>B.</given-names></name>
<name><surname>Carnoy</surname><given-names>M.</given-names></name>
<name><surname>Kilpatrick</surname><given-names>J.</given-names></name>
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>Shavelson</surname><given-names>R. J.</given-names></name>
</person-group> (<year>2007</year>). <source>Estimating causal effects using experimental and observational designs</source> (report from the Governing Board of the American Educational Research Association Grants Program). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr29-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shadish</surname><given-names>W. R.</given-names></name>
<name><surname>Clark</surname><given-names>M. H.</given-names></name>
<name><surname>Steiner</surname><given-names>P. M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Can nonrandomized experiments yield accurate answers? A randomized experiment comparing random and nonrandom assignments</article-title>. <source>Journal of the American Statistical Association</source>, <volume>103</volume>, <fpage>1334</fpage>–<lpage>1344</lpage>.</citation>
</ref>
<ref id="bibr30-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shadish</surname><given-names>W. R.</given-names></name>
<name><surname>Galindo</surname><given-names>R.</given-names></name>
<name><surname>Wong</surname><given-names>V. C.</given-names></name>
<name><surname>Steiner</surname><given-names>P. M.</given-names></name>
<name><surname>Cook</surname><given-names>T. D.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A randomized experiment comparing random and cutoff-based assignment</article-title>. <source>Psychological Methods</source>, <volume>16</volume>(<issue>2</issue>), <fpage>179</fpage>–<lpage>191</lpage>.</citation>
</ref>
<ref id="bibr31-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sweet</surname><given-names>M. A.</given-names></name>
<name><surname>Appelbaum</surname><given-names>M. I.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Is home visiting an effective strategy? A meta-analytic review of home visiting programs for families with young children</article-title>. <source>Child Development</source>, <volume>75</volume>(<issue>5</issue>), <fpage>1435</fpage>–<lpage>1456</lpage>.</citation>
</ref>
<ref id="bibr32-0162373712462453">
<citation citation-type="book">
<collab>U.S. Department of Health and Human Services, Administration for Children and Families</collab>. (<year>2005</year>, <month>May</month>). <source>Head Start impact study: First year findings</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr33-0162373712462453">
<citation citation-type="book">
<collab>U.S. Department of Health and Human Services, Administration for Children and Families</collab>. (<year>2010</year>, <month>January</month>). <source>Head Start impact study: Final report</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr34-0162373712462453">
<citation citation-type="gov">
<collab>U.S. Department of Health and Human Services, Administration for Children and Families, Office of Head Start</collab>. (<year>2010a</year>). <source>Head Start performance standards &amp; other regulations</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.acf.hhs.gov/programs/ohs/legislation/index.html">http://www.acf.hhs.gov/programs/ohs/legislation/index.html</ext-link></citation>
</ref>
<ref id="bibr35-0162373712462453">
<citation citation-type="gov">
<collab>U.S. Department of Health and Human Services, Administration for Children and Families, Office of Head Start</collab>. (<year>2010b</year>). <source>Head Start program fact sheet</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.acf.hhs.gov/programs/ohs/about/fy2010.html">http://www.acf.hhs.gov/programs/ohs/about/fy2010.html</ext-link></citation>
</ref>
<ref id="bibr36-0162373712462453">
<citation citation-type="book">
<collab>Westinghouse Learning Corporation</collab>. (<year>1969</year>). <article-title>The impact of Head Start: An evaluation of the effects of Head Start on children’s cognitive and affective development</article-title>. In <source>A report presented to the Office of Economic Opportunity</source> (PB 184328). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Distributed by Clearinghouse for Federal Scientific and Technical Information, U.S. Department of Commerce, National Bureau of Standards, Institute for Applied Technology</publisher-name>.</citation>
</ref>
<ref id="bibr37-0162373712462453">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wong</surname><given-names>V. C.</given-names></name>
<name><surname>Cook</surname><given-names>T. D.</given-names></name>
<name><surname>Barnett</surname><given-names>W. S.</given-names></name>
<name><surname>Jung</surname><given-names>K.</given-names></name>
</person-group> (<year>2008</year>). <article-title>An effectiveness-based evaluation of five state pre-kindergarten programs</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>27</volume>, <fpage>122</fpage>–<lpage>154</lpage>.</citation>
</ref>
<ref id="bibr38-0162373712462453">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Zhai</surname><given-names>F.</given-names></name>
<name><surname>Brooks-Gunn</surname><given-names>J.</given-names></name>
<name><surname>Waldfogel</surname><given-names>J.</given-names></name>
</person-group> (<year>2010</year>, <month>March</month>). <source>Head Start and urban children’s school readiness: A birth cohort study in 18 cities</source>. <conf-name>Paper presented at the Society for Research on Educational Effectiveness conference</conf-name>, <conf-loc>Washington, DC</conf-loc>.</citation>
</ref>
<ref id="bibr39-0162373712462453">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Zigler</surname><given-names>E.</given-names></name>
<name><surname>Valentine</surname><given-names>J.</given-names></name>
</person-group> (Eds.). (<year>1979</year>). <source>Project Head Start: A legacy of the war on poverty</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Free Press</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>