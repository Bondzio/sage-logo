<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ERG</journal-id>
<journal-id journal-id-type="hwp">sperg</journal-id>
<journal-title>Ergonomics in Design: The Quarterly of Human Factors Applications</journal-title>
<issn pub-type="ppub">1064-8046</issn>
<issn pub-type="epub">XXXX-XXXX</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1064804611428928</article-id>
<article-id pub-id-type="publisher-id">10.1177_1064804611428928</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Features</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>How to Build a Low-Cost Eye-Tracking System</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Pavlas</surname><given-names>Davin</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Lum</surname><given-names>Heather</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Salas</surname><given-names>Eduardo</given-names></name>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="other" id="bio1-1064804611428928">
<p>Davin Pavlas is a graduate of the Applied Experimental and Human Factors Psychology Program at the University of Central Florida. He obtained his PhD in 2010 through a dissertation that established a model of flow and play in games for learning. As a researcher at the Institute for Simulation and Training, he worked on a variety of projects that examine teams using simulations. To date, his research has examined the relationship between aesthetics and usability in user interfaces, the nature of flow state in games, the attributes of serious games that contribute to learning, and the role of play experience in learning.</p>
</fn>
<fn fn-type="other" id="bio2-1064804611428928">
<p>Heather Lum is a research associate at Penn State Erie, the Behrend College. She earned her PhD from the University of Central Florida in applied experimental and human factors psychology. Her research foci include interactions with and perceptions of robots. This extends to how humans use technology and how such use influences how people perceive one another. She also focuses on human-animal interactions, including but not limited to anthropomorphism, positive and negative associations with animals, and the use of animals in the aid/assistance of the elderly, at-risk youth, and other populations.</p>
</fn>
<fn fn-type="other" id="bio3-1064804611428928">
<p>Eduardo Salas is trustee chair and a professor of psychology at the University of Central Florida (UCF), where he also holds an appointment as program director for the Human Systems Integration Research Department at the Institute for Simulation and Training (IST). Previously, he was director of UCF’s Applied Experimental and Human Factors PhD Program. Before joining IST, he was a senior research psychologist and head of the Training Technology Development Branch of the Naval Air Warfare Center Training Systems Division for 15 years.</p>
</fn>
<fn fn-type="other">
<p>The views expressed in this work are those of the authors and should not be construed as official or as reflecting the views of the University of Central Florida or the Department of Defense or as endorsements of particular hardware or software production companies. This work was supported by funding from a Multidisciplinary University Research Initiative Grant from the Office of Naval Research (Contract No. N000140610446). This work was also supported by the Learning Institute for Elders at the University of Central Florida.ature</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2012</year>
</pub-date>
<volume>20</volume>
<issue>1</issue>
<fpage>18</fpage>
<lpage>23</lpage>
<permissions>
<copyright-statement>© 2012 Human Factors and Ergonomics Society</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<p>Eye tracking, previously the purview of well-funded laboratories, is now available to any individual who wishes to study gaze patterns. Advances in eye-tracking technology have made it possible for those with meager budgets but an abundance of motivation to engage in studies that examine participants’ eye movements and fixations. This article presents a how-to guide for creating low-cost eye-tracking solutions and includes discussion of optical hardware, tracking software, and data analysis programs. The wider availability of eye-tracking technology ensures that the broader scientific community has access to techniques that can inform design and enhance research.</p>
</abstract>
<kwd-group>
<kwd>physiological measure</kwd>
<kwd>usability</kwd>
<kwd>low-level cognitive measure</kwd>
<kwd>technology</kwd>
<kwd>measurement</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Systems with relatively good functionality can be built for about one-hundredth the cost of some commercial systems.</p>
<p><bold>Psychological researchers often seek</bold> to understand what is happening inside the mind. Unfortunately, the mind is not a particularly willing participant in these efforts, and psychologists must rely on a number of techniques to plumb its depths. One such technique enables researchers to examine what the individual is focusing his or her attention on. By tracking eye movements and fixations, it is possible to use the eye as a window into the mind. An operational definition by <xref ref-type="bibr" rid="bibr10-1064804611428928">Poole and Ball (2006)</xref> states</p>
<p>
<disp-quote>
<p>eye tracking is a technique whereby an individual’s eye movements are measured so that the researcher knows both where a person is looking at any given time and the sequence in which their eyes are shifting from one location to another. (p. 211)</p>
</disp-quote>
</p>
<p>Within the past few decades, technological advances have made it possible for researchers conducting studies involving eye tracking to focus on more complex cognitive measures, such as workload, memory, and attention (<xref ref-type="bibr" rid="bibr7-1064804611428928">Kowler, 1990</xref>). The primary factor limiting the use of eye trackers in many research facilities has been the systems’ high cost: Prices can range from $10,000 to $30,000, with many systems falling on the higher end of that spectrum.</p>
<p>The availability of low-cost eye-tracking systems will benefit researchers and human factors/ergonomics practitioners across a broad range of domains. In this article we offer a brief guide on how to create a low-cost eye tracker and describe how this tracker functions in an experiment. Although some of the options discussed in this guide involve modifying software or hardware, it is possible to create an eye-tracking system without programming skills, the ability to solder or otherwise modify electronics, or other special technical knowledge or skill.</p>
<sec id="section1-1064804611428928">
<title>Eye Tracking</title>
<p>Typically, an eye-tracking device is equipped with a camera that captures the movement of one or both eyes. Most types of eye trackers measure eye movement metrics, including fixations, saccades, scanpaths, and areas of interest. A <italic>saccade</italic> is a rapid intermittent eye movement that occurs when the eyes fixate on one point after another in the visual field; in contrast, a <italic>fixation</italic> is a relatively stable eye-in-head position within some threshold of dispersion (typically ~2°) for some minimum duration (typically 100 to 200 ms) and with a velocity below some threshold (typically 15° to 100° per second; see <xref ref-type="bibr" rid="bibr6-1064804611428928">Jacob &amp; Karn, 2003</xref>). <italic>Fixation-derived</italic> metrics include overall number of fixations, fixations per area of interest, fixation duration, and time to first fixation on target.</p>
<p>A <italic>scanpath</italic> is a complete saccade-fixation-saccade sequence. Typical scanpath-derived metrics include duration, length, density, regularity, saccade-fixation ratio, and direction. An optimal scanpath is a straight line to a desired target, with relatively short fixation duration at the target (<xref ref-type="bibr" rid="bibr3-1064804611428928">Goldberg &amp; Kotval, 1999</xref>). An <italic>area of interest</italic> is a portion of a display that is defined by the experimenter (<xref ref-type="bibr" rid="bibr6-1064804611428928">Jacob &amp; Karn, 2003</xref>). This area is also referred to as the <italic>look zone</italic> and is often defined before the experiment begins. For a review of eye-tracking techniques, see <xref ref-type="bibr" rid="bibr2-1064804611428928">Fisher, Monty, and Senders (1981)</xref>; <xref ref-type="bibr" rid="bibr4-1064804611428928">Hyona, Radach, and Deubel (2003)</xref>; <xref ref-type="bibr" rid="bibr5-1064804611428928">Jacob (1990)</xref>; and <xref ref-type="bibr" rid="bibr11-1064804611428928">Salvucci and Goldberg (2000)</xref>.</p>
</sec>
<sec id="section2-1064804611428928">
<title>Hardware for Low-Cost Eye-Tracking Systems</title>
<p>The cost of creating a custom eye-tracking system is remarkably low. The system generally consists of a camera, an infrared illumination source, and some sort of mount. Off-the-shelf components can fill all of these hardware requirements, allowing researchers to build effective eye-tracking systems for as little as $100 (not including the computer used to record data).</p>
<p>The most important element of any eye-tracking system is the camera. Consumer cameras have been increasing in quality to the point that off-the-shelf USB cameras can now serve as data input for eye-tracking systems. When choosing a camera for a custom eye tracker, it is important to note whether it is compatible with the software being used (e.g., ITU GazeTracker, EyeWriter). Determining compatibility is unfortunately often a matter of trial and error; however, others who use software can be an excellent resource for prior knowledge. For example, the ITU GazeTracker site includes a forum in which users can list the various hardware that they have discovered work well with the software.</p>
<p>The chosen camera should be light enough to mount on steadying hardware (e.g., glasses, headrest; see below) or allow for enough zooming to serve as a remote camera (i.e., positioned away from the participant rather than worn).</p>
<p>When choosing a camera, consider whether it has an infrared filter. A popular eye-tracking technique is to illuminate the eye with infrared light or to create glints on the eye with an infrared source. If the camera has an infrared filter, these efforts will be in vain – and given that infrared light is invisible to the human eye, this can be frustrating to diagnose if the problem is not known beforehand.</p>
<p>Illumination via infrared is possible with any number of commercially available “night vision” lights. For example, Sony produces a series of battery-powered infrared lights intended for mounting on camcorders. Alternatively, simple infrared LEDs can be used for a more portable solution (e.g., mounted to wearable glasses), although incorporating LEDs requires some electronics skill.</p>
<p>The final element of an eye-tracking system is the mount. Often, eye-tracking systems are mounted so as to be wearable by the participant and can be accomplished with strong copper wire or aircraft cable to mount the camera and illumination source to a pair of eyeglasses (with lenses removed). Thick-framed sunglasses are especially useful for such modification, as they are relatively sturdy and can support the camera and infrared light. Alternatively, the system can be mounted to an optometrist’s chinrest, which has the added advantage of keeping the participant’s head relatively stable. Because chinrests and headrests are generally produced by medical manufacturers, using them can drive up the price of the eye-tracking system by as much as $300. Although this solution is less portable than mounting the system to a pair of glasses, it results in a more sturdy and precise tracking device by limiting participants’ motion.</p>
</sec>
<sec id="section3-1064804611428928">
<title>Software for Low-Cost Systems</title>
<p>Eye tracking is impossible without software that is sophisticated enough to handle the complex task of analyzing live video of the human eye and determining what that eye is focused on. In the past, commercial offerings with substantial licensing fees have provided this functionality. Today, two open-source projects provide effective tools that can be used to rapidly develop eye-tracking systems. Both the EyeWriter (<xref ref-type="bibr" rid="bibr8-1064804611428928">Lieberman et al., 2010</xref>) and ITU GazeTracker (<xref ref-type="bibr" rid="bibr12-1064804611428928">San Agustin et al., 2010</xref>) systems are freely available systems that allow researchers to perform gaze-tracking (i.e., eye tracking of the user’s gaze on a computer screen) research. Although we aim to make this topic as accessible as possible, brief reference to specific portions of source code in our later discussion of the EyeWriter. For researchers who do not have access to programmers, the ITU GazeTracker should be considered, and we will address that system first.</p>
<p>The ITU GazeTracker is a user-friendly option for eye tracking because no modification to the source code is necessary to create an analyzable output file. The ITU GazeTracker can be set to “stream” data to another source via UDP (user datagram protocol). Using a freely available “sniffer” program (i.e., a program that analyzes data packets sent across the network), it is possible to record this data stream for later analysis. For the more technically inclined researcher, it is also feasible to create a program that records and processes this UDP stream.</p>
<p>The GazeTracker also includes the capability to create log files, although this feature is still relatively new and being tested by the developers. The final (and most user-friendly) option is to use the OGAMA (OpenGazeAndMouseAnalyzer) software, described later in this article, to record and analyze the data provided by the ITU GazeTracker.</p>
<p>The GazeTracker allows for tracking both pupil and glint. Glint tracking is the process of tracking “glints” of light on the eye’s cornea, which is useful in remote eye tracking (i.e., eye tracking via a camera that is not worn by the participant). The process of calibrating the ITU GazeTracker is similar to calibrating other eye-tracking software: The researcher must adjust the level of pupil detection and glint detection using a series of sliders.</p>
<p>When optimally configured, the video presented by the program should show the participant’s pupil filled in with a green color, with the rest of the eye shown normally. The slider also allows the researcher to set a minimum and maximum pupil size, which is necessary to account for eye movement and dilation.</p>
<boxed-text id="boxed-text1-1064804611428928">
<title>Where to Obtain Eye-Tracking Software</title>
<p>ASTEF: <ext-link ext-link-type="uri" xlink:href="http://w3.uniroma1.it/dinocera/astef/">http://w3.uniroma1.it/dinocera/astef/</ext-link></p>
<p>EyeWriter: <ext-link ext-link-type="uri" xlink:href="http://www.eyewriter.org/">http://www.eyewriter.org/</ext-link></p>
<p>ITU GazeTracker: <ext-link ext-link-type="uri" xlink:href="http://www.gazegroup.org/">http://www.gazegroup.org/</ext-link></p>
<p>OGAMA: <ext-link ext-link-type="uri" xlink:href="http://www.ogama.net/">http://www.ogama.net/</ext-link></p>
</boxed-text>
<p>One reason the research community might be interested in using the ITU GazeTracker is because of the availability of another software package, OGAMA. OGAMA is a free open-source tool that allows for the recording and analysis of eye and mouse movements (<xref ref-type="bibr" rid="bibr13-1064804611428928">Voßkühler, 2010</xref>). It supports commercial eye-tracking software as well as the ITU GazeTracker, making it an attractive option for researchers with limited budgets.</p>
<p>Using OGAMA, researchers can create a series of interactive or static stimuli similar to a PowerPoint presentation. The embedded ITU GazeTracker program is then used to track the participant’s eye movements as he or she is shown (or engages) the presentation. If the researcher so wishes, the participant’s mouse movements can also be captured. After data have been recorded, OGAMA’s suite of analysis tools presents a variety of analyses, including scanpaths (i.e., the path the participant’s eye traces on the screen), fixation on defined areas of interest, attention maps (i.e., “heat maps” of areas that drew attention), and more.</p>
<p><disp-quote>
<p>It is possible to create an eye-tracking system without programming skills or other special technical knowledge or skill.</p>
</disp-quote></p>
<p>For researchers interested in a simpler analysis tool, the ASTEF (A Simple Tool for Examining Fixations) program is a free but closed-source alternative. ASTEF is not as fully featured as OGAMA but allows for area-of-interest and scanpath analysis (<xref ref-type="bibr" rid="bibr1-1064804611428928">Camilli, Nacchia, Terenzi, &amp; Di Nocera, 2008</xref>).</p>
<p>Another open-source eye-tracking application, EyeWriter, was originally developed to allow graffiti artists with disabilities to project images that were drawn on the basis of eye movements (<xref ref-type="bibr" rid="bibr8-1064804611428928">Lieberman et al., 2010</xref>). Because the EyeWriter project is open-source and inherently requires coordinate-based gaze tracking, it is possible to modify the EyeWriter software to output a log of user gaze patterns. To do so, the researcher must modify the Java source code, which is freely available for download. Researchers who wish to pursue this option should either modify, or direct an experienced programmer to modify, the program’s main “draw” function.</p>
<p>Specifically, the “if(CM.bBeenFit)” section of the source code controls what happens when the software recognizes the user’s pupil and has enough information to determine where he or she is looking. Adding a few lines of code to create an output of the eye’s projected <italic>x/y</italic> coordinates on the screen along with the current time (in milliseconds) allows for later analysis of these data. Further data analysis allows for the calculation of a variety of typical eye-tracking metrics, including number and duration of fixations and areas of interest. After this modification is made, EyeWriter is relatively simple to use. The program is stand-alone, and calibration is a matter of adjusting the image from the camera with a point-and-click interface, then starting the automatic calibration. During this calibration, the user fixates on set points on the screen.</p>
<p>Although EyeWriter needs some modification to its source code to provide researcher-friendly output, it was developed with the use of user-friendly OpenFrameworks architecture and can be installed on both Windows and Macintosh platforms.</p>
</sec>
<sec id="section4-1064804611428928">
<title>Example of a Low-Cost Eye-Tracking System</title>
<p>The eye-tracking system described in this section was created and tested in five simple steps: camera identification and modification, illumination identification and modification, mount creation, software modification and installation, and calibration. For this example, we developed a chinrest-mounted system for use with a modified version of EyeWriter.</p>
<sec id="section5-1064804611428928">
<title>Step 1: Camera</title>
<p>The first step in creating the eye tracker was to select the camera. On the basis of the experiences of others with eye-tracking projects, we chose the Microsoft HD LifeCam. This camera is fairly lightweight and features an infrared filter. We removed that filter by opening the camera, carefully removing the lens, and not so carefully breaking the infrared filter into pieces. After the filter was removed, we reassembled the camera and tested it with the standard version of EyeWriter. This testing is simple to do informally: Simply hold the camera up to your eye, ensure the picture is focused, and start calibration. Calibration need not be perfect after this test, as it is performed only to ensure the camera is in working order. Although removing the filter is relatively easy for individuals who have no prior experience with electronics, one can also purchase infrared-capable cameras, such as the Elyssa DigiVue Nightvision, and avoid modifications entirely.</p>
</sec>
<sec id="section6-1064804611428928">
<title>Step 2: Illumination</title>
<p>We chose the Sony HVL-IRM battery-powered infrared light for our infrared source. No modification was required to use this device, as it was not mounted on the camera. We used the mounting bracket included with the device to prepare the system for mounting on the chinrest. We tested the system using the camera to ensure that the infrared LEDs were emitting the “invisible” infrared light. If the infrared illumination is working, the scene being viewed with the camera will appear illuminated when the infrared LEDs are activated.</p>
</sec>
<sec id="section7-1064804611428928">
<title>Step 3: Mount</title>
<p>With the camera and infrared illuminator prepared, we mounted the two systems onto an adjustable chinrest manufactured by Richmond Products. The camera was mounted on a flexible arm made of lamp cable pillaged from an unfortunate lamp. This equipment enabled us to adjust the camera’s position as needed. A similar mounting approach was used for the infrared light, which was positioned below the chinrest portion of the mount with the use of the manufacturer-provided camera rail.</p>
</sec>
<sec id="section8-1064804611428928">
<title>Step 4: Software</title>
<p>With the physical system in place, we installed the EyeWriter software on a desktop computer that would serve both as the means of recording and to present the stimulus. The source code for EyeWriter was compiled after the addition of a logging function, which was designed to create a simple text file listing <italic>x</italic>/<italic>y</italic> gaze coordinates and time. To aid later analysis, this file was formatted to be readable with the use of ASTEF. For researchers who do not wish to engage in any software modification, the system described thus far is completely compatible with ITU GazeTracker and OGAMA.</p>
</sec>
<sec id="section9-1064804611428928">
<title>Step 5: Testing</title>
<p>The final step of preparing the eye-tracking system for experimentation was to once again test the system. During this step, it was important to make sure the infrared illumination can be positioned as needed, that the camera is easy to maneuver, and that the data are being saved properly. This testing was conducted in the actual setting in which the system was to be used to ensure that background illumination was appropriate. Depending on the software used for tracking, researchers may wish to save eye tracker settings (i.e., contrast, brightness, pupil detection settings) for later use, as doing so will speed calibration during experimentation. As this example system was created using EyeWriter, the brightness, contrast, and pupil size settings were saved to a local file for reuse.</p>
<p>The total cost of this system (see <xref ref-type="fig" rid="fig1-1064804611428928">Figures 1</xref> and <xref ref-type="fig" rid="fig2-1064804611428928">2</xref>) was $275: $160 for the chinrest, $40 for the camera, $15 for the flexible lamp arm, and $60 for the infrared illumination. The eye tracker took about 10 hours to build, although the initial research that went into determining which components and software were available took far more time.</p>
<fig id="fig1-1064804611428928" position="float">
<label>Figure 1.</label>
<caption>
<p>Eye tracker made from chinrest, lamp arm, camera, and infrared light.</p>
</caption>
<graphic xlink:href="10.1177_1064804611428928-fig1.tif"/>
</fig>
<fig id="fig2-1064804611428928" position="float">
<label>Figure 2.</label>
<caption>
<p>Eye tracker, front view.</p>
</caption>
<graphic xlink:href="10.1177_1064804611428928-fig2.tif"/>
</fig>
</sec>
</sec>
<sec id="section10-1064804611428928">
<title>Example Use and Data Visualization</title>
<p>We created the system just described for use with EyeWriter and ASTEF as a proof of concept. However, because the system is effectively a piece of hardware that is used by software, it was also suitable for use with OGAMA software. One line of research we have been investigating deals with the aesthetic and usability design elements of Web sites. The use of eye tracking enabled us to examine whether participant ratings of these sites were contingent on fixations on specific design elements. Because OGAMA allows for relatively simple generation of areas of interest, scanpaths, and heat maps, it was well suited to this context. As with the EyeWriter software, pupil detection settings were created to account for lighting conditions. A heat map represents how much attention was given to particular regions of the stimulus, with darker (“hotter”) regions of the graphic indicating more fixations.</p>
<p>The data stored in OGAMA can be used for aggregated analysis (e.g., examining heat maps across all participants, viewing transitions between areas of interest) or for individual-level analysis (e.g., viewing a video playback of a particular participant’s scanpath). <xref ref-type="fig" rid="fig3-1064804611428928">Figure 3</xref> presents a sample area of interest and transition map calculated across 5 participants. The arrows represent transitions between areas, and the circles represent the average fixation duration for the region.</p>
<fig id="fig3-1064804611428928" position="float">
<label>Figure 3.</label>
<caption>
<p>Sample area of interest map generated via OGAMA (OpenGazeAndMouseAnalyzer) for 5 participants.</p>
</caption>
<graphic xlink:href="10.1177_1064804611428928-fig3.tif"/>
</fig>
</sec>
<sec id="section11-1064804611428928">
<title>Conclusion</title>
<p>In five relatively simple steps, it is possible to create a working eye-tracking system for use in gaze research, which has profound implications for the human factors/ergonomics community. Whereas eye tracking was once reserved for those with significant funding, rapid usability studies can now be conducted by any researcher with a modest budget. However, the issue of low-cost eye tracking is hardly resolved. The necessity of this article is evidence that low-cost eye tracking is not yet as straightforward as it should be. Future developments in low-cost eye tracking should focus on ensuring that the resulting technology is readily adoptable by researchers and practitioners without the need for custom programming.</p>
<p>Additionally, the quality of low-cost eye-tracking systems should continue to be improved. After using eye tracking in their laboratory, <xref ref-type="bibr" rid="bibr9-1064804611428928">Morimoto and Mimica (2005)</xref> devised a set of guidelines that should be met for eye tracking to be useful in experiments. These guidelines include being accurate, precise, and reliable while also working under a variety of environmental and participant conditions. In addition, the hardware should be unobtrusive, allow for freedom of movement (at minimum, head movement), and have real-time response. Although the example low-cost eye tracker meets some of these requirements, there is still room for improvement.</p>
<p>The future of eye-tracking research revolves around availability. As the technology underlying the creation of low-cost eye-tracking systems continues to improve, more and more researchers will be able to employ eye tracking in their studies. Although eye tracking is not the only method for attentional measurement, it is a useful tool for a variety of contexts. Researchers must choose the best tool available for a given job, but until recently, eye tracking has been unattainable for many. With the advent of low-cost eye tracking, it is now possible for these researchers to access this useful technology when science demands it. We hope that scientists who were previously unable to perform eye-tracking research can make use of the information in this article to advance the science using their own eye-tracking systems.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="bibr1-1064804611428928">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Camilli</surname><given-names>M.</given-names></name>
<name><surname>Nacchia</surname><given-names>R.</given-names></name>
<name><surname>Terenzi</surname><given-names>M.</given-names></name>
<name><surname>Di Nocera</surname><given-names>F.</given-names></name>
</person-group> (<year>2008</year>). <article-title>ASTEF: A simple tool for examining fixations</article-title>. <source>Behavior Research Methods</source>, <volume>40</volume>, <fpage>373</fpage>–<lpage>382</lpage>.</citation>
</ref>
<ref id="bibr2-1064804611428928">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Fisher</surname><given-names>D. F.</given-names></name>
<name><surname>Monty</surname><given-names>R. A.</given-names></name>
<name><surname>Senders</surname><given-names>J. W.</given-names></name>
</person-group> (Eds.). (<year>1981</year>). <source>Eye movements: Cognition and visual perception</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr3-1064804611428928">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldberg</surname><given-names>J. H.</given-names></name>
<name><surname>Kotval</surname><given-names>X. P.</given-names></name>
</person-group> (<year>1999</year>) <article-title>Computer interface evaluation using eye movements: Methods and constructs</article-title>. <source>International Journal of Industrial Ergonomics</source>, <volume>24</volume>, <fpage>631</fpage>–<lpage>645</lpage>.</citation>
</ref>
<ref id="bibr4-1064804611428928">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Hyona</surname><given-names>J.</given-names></name>
<name><surname>Radach</surname><given-names>R.</given-names></name>
<name><surname>Deubel</surname><given-names>H.</given-names></name>
</person-group> (Eds.). (<year>2003</year>). <source>The mind’s eyes: Cognitive and applied aspects of eye movements</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Elsevier Science</publisher-name>.</citation>
</ref>
<ref id="bibr5-1064804611428928">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>R. J. K.</given-names></name>
</person-group> (<year>1990</year>). <article-title>What you look at is what you get: Eye movement-based interaction techniques</article-title>. In <conf-name>Proceedings of the ACM CHI’90 Human Factors in Computing Systems Conference</conf-name> (pp. <fpage>11</fpage>–<lpage>18</lpage>). <conf-loc>New York: Addison Wesley/ACM Press</conf-loc>.</citation>
</ref>
<ref id="bibr6-1064804611428928">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>R. J. K.</given-names></name>
<name><surname>Karn</surname><given-names>K. S.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Eye tracking in human-computer interaction and usability research: Ready to deliver the promises [Section commentary]</article-title>. In <person-group person-group-type="editor">
<name><surname>Hyona</surname><given-names>J.</given-names></name>
<name><surname>Radach</surname><given-names>R.</given-names></name>
<name><surname>Deubel</surname><given-names>H.</given-names></name>
</person-group> (Eds.), <source>The mind’s eyes: Cognitive and applied aspects of eye movements</source> (pp. <fpage>573</fpage>–<lpage>603</lpage>). <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Elsevier Science</publisher-name>.</citation>
</ref>
<ref id="bibr7-1064804611428928">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kowler</surname><given-names>E.</given-names></name>
</person-group> (<year>1990</year>). <article-title>The role of visual and cognitive processes in the control of eye movements</article-title>. In <person-group person-group-type="editor">
<name><surname>Kowler</surname><given-names>E.</given-names></name>
</person-group> (Ed.), <source>Eye movements and their role in visual and cognitive processes</source> (pp. <fpage>1</fpage>–<lpage>70</lpage>). <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr8-1064804611428928">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lieberman</surname><given-names>Z.</given-names></name>
<name><surname>Powderly</surname><given-names>J.</given-names></name>
<name><surname>Roth</surname><given-names>E.</given-names></name>
<name><surname>Sugrue</surname><given-names>C.</given-names></name>
<name><surname>Quan</surname><given-names>T.</given-names></name>
<name><surname>Watson</surname><given-names>T.</given-names></name>
</person-group> (<year>2010</year>). <source>EyeWriter initiative</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.eyewriter.org/">http://www.eyewriter.org/</ext-link></comment></citation>
</ref>
<ref id="bibr9-1064804611428928">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Morimoto</surname><given-names>C. H.</given-names></name>
<name><surname>Mimica</surname><given-names>M. R. M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Eye gaze tracking techniques for interactive applications</article-title>. <source>Computer Vision and Image Understanding</source>, <volume>98</volume>, <fpage>4</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr10-1064804611428928">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Poole</surname><given-names>A.</given-names></name>
<name><surname>Ball</surname><given-names>L. J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Eye tracking in HCI and usability research: Current status and future prospects</article-title>. In <person-group person-group-type="editor">
<name><surname>Ghaoui</surname><given-names>C.</given-names></name>
</person-group> (Ed.), <source>Encyclopedia of human-computer interaction</source> (pp. <fpage>211</fpage>–<lpage>219</lpage>). <comment>Available from <ext-link ext-link-type="uri" xlink:href="http://www.interaction-design.org/encyclopedia/">http://www.interaction-design.org/encyclopedia/</ext-link></comment></citation>
</ref>
<ref id="bibr11-1064804611428928">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Salvucci</surname><given-names>D. D.</given-names></name>
<name><surname>Goldberg</surname><given-names>J. H.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Identifying fixations and saccades in eye-tracking protocols</article-title>. In <conf-name>Proceedings of the 2000 Symposium on Eye-Tracking Research &amp; Applications</conf-name> (pp. <fpage>71</fpage>–<lpage>78</lpage>). <conf-loc>New York: ACM Press</conf-loc>.</citation>
</ref>
<ref id="bibr12-1064804611428928">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>San Agustin</surname><given-names>J.</given-names></name>
<name><surname>Skovsgaard</surname><given-names>H.</given-names></name>
<name><surname>Mollenbach</surname><given-names>E.</given-names></name>
<name><surname>Barret</surname><given-names>M.</given-names></name>
<name><surname>Tall</surname><given-names>M.</given-names></name>
<name><surname>Hansen</surname><given-names>D. W.</given-names></name>
<name><surname>Hansen</surname><given-names>J. P.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Evaluation of a low-cost open-source gaze tracker</article-title>. In <conf-name>Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications</conf-name> (pp. <fpage>77</fpage>–<lpage>80</lpage>). <conf-loc>New York: ACM Press</conf-loc>.</citation>
</ref>
<ref id="bibr13-1064804611428928">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Voßkühler</surname><given-names>A.</given-names></name>
</person-group> (<year>2010</year>). <source>OGAMA (OpenGazeAndMouseAnalyzer): An open source software designed to analyze eye and mouse movements in a slideshow study</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.ogama.org/">http://www.ogama.org/</ext-link></comment></citation>
</ref>
</ref-list>
</back>
</article>