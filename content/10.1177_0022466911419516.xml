<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SED</journal-id>
<journal-id journal-id-type="hwp">spsed</journal-id>
<journal-id journal-id-type="nlm-ta">J Spec Educ</journal-id>
<journal-title>The Journal of Special Education</journal-title>
<issn pub-type="ppub">0022-4669</issn>
<issn pub-type="epub">1538-4764</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022466911419516</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022466911419516</article-id>
<title-group>
<article-title>Intervention Fidelity in Special and General Education Research Journals</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Swanson</surname><given-names>Elizabeth</given-names></name>
<xref ref-type="aff" rid="aff1-0022466911419516">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Wanzek</surname><given-names>Jeanne</given-names></name>
<xref ref-type="aff" rid="aff2-0022466911419516">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Haring</surname><given-names>Christa</given-names></name>
<xref ref-type="aff" rid="aff1-0022466911419516">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Ciullo</surname><given-names>Stephen</given-names></name>
<xref ref-type="aff" rid="aff1-0022466911419516">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>McCulley</surname><given-names>Lisa</given-names></name>
<xref ref-type="aff" rid="aff1-0022466911419516">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022466911419516"><label>1</label>The University of Texas at Austin, Austin, TX, USA</aff>
<aff id="aff2-0022466911419516"><label>2</label>Florida State University, Tallahassee, FL, USA</aff>
<author-notes>
<corresp id="corresp1-0022466911419516">Elizabeth Swanson, The Meadows Center for Preventing Educational Risk, The University of Texas at Austin, 1 University Station D4900, Austin, TX 78712, USA. E-mail: <email>easwanson@mail.utexas.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>47</volume>
<issue>1</issue>
<fpage>3</fpage>
<lpage>13</lpage>
<permissions>
<copyright-statement>© Hammill Institute on Disabilities 2011</copyright-statement>
<copyright-year>2011</copyright-year>
<copyright-holder content-type="society">Hammill Institute on Disabilities</copyright-holder>
</permissions>
<abstract>
<p>Treatment fidelity reporting practices are described for journals that published general and special education intervention research with high impact factors from 2005 through 2009. The authors reviewed research articles, reported the proportion of intervention studies that described fidelity measurement, detailed the components of fidelity measurement reported, and determined whether the components of fidelity reported differed based on the research design, the type of intervention, or the number of intervention sessions. Results indicate that even intervention research articles in high-quality general and special education journals inconsistently report fidelity (less than 70% of the articles). Authors of single-case studies most frequently reported the collection of intervention fidelity data (81.3% of articles, compared with 67.4% of treatment-comparison study articles). Of the 67% of articles that provided information about intervention fidelity procedure, only 9.8% provided data about the quality of the treatment intervention.</p>
</abstract>
<kwd-group>
<kwd>treatment fidelity</kwd>
<kwd>intervention fidelity</kwd>
<kwd>intervention research</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Broadly defined, intervention fidelity refers to the delivery of an intervention or program as designed (<xref ref-type="bibr" rid="bibr8-0022466911419516">Gresham, MacMillan, Beebe-Frankenberger, &amp; Bocian, 2000</xref>; <xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell, 2008</xref>). Collection and reporting of fidelity data in research reports are critical for determining why interventions succeed or fail (<xref ref-type="bibr" rid="bibr5-0022466911419516">Dusenbury, Brannigan, Falco, &amp; Hansen, 2003</xref>) and for determining whether an intervention can be scaled up to larger settings. Fidelity data are especially important when trying to account for negative or ambiguous findings and allow researchers to determine whether unsuccessful outcomes are due to ineffective interventions or due to a failure to implement the intervention as intended. This information not only helps researchers explain findings but can also serve to highlight areas of further study. For example, if one can attribute unsuccessful outcomes to poor implementation, it may signify the need for strengthening training materials or increasing classroom supports prior to generalizing with a wide variety of schools or teachers. However, unsuccessful outcomes may be attributed to poorly designed intervention, in which case intervention redesign is warranted.</p>
<p>Fidelity results also have implications for scaling up interventions in school settings. When an intervention produces positive outcomes and fidelity is high, the intervention can be adequately defined for scaling up in a broader variety of settings. Indeed, practitioners are becoming increasingly aware of the importance of choosing evidence-based interventions (see the No Child Left Behind Act of 2001 and the Individuals With Disabilities Education Act of 2004) that are feasible and effective and can be implemented with fidelity. Therefore, reports of research should describe the utility of the intervention, that is, the degree to which it is feasible and practical for implementation in schools (<xref ref-type="bibr" rid="bibr22-0022466911419516">U.S. Department of Education, 2011</xref>).</p>
<p>Because fidelity data has the potential to inform researchers’ work and practitioners’ intervention choices, it is receiving increased attention from funders and evaluators of research. For example, the National Institute of Child Health and Human Development expects researchers to attend to issues of fidelity measurement stating, “broader examination and measurement of the instructional context is strongly encouraged to document and inform our understanding of fidelity of implementation” (<xref ref-type="bibr" rid="bibr16-0022466911419516">National Institutes of Health, 2011</xref>). The Institute of Education Sciences encourages applicants to not only examine the efficacy of interventions but also to gather data to help explain the level of fidelity of implementation that is attained to help researchers identify the “conditions, tools and procedures” that are needed to support implementation of the intervention (<xref ref-type="bibr" rid="bibr22-0022466911419516">U.S. Department of Education, 2011</xref>). Likewise, organizations that evaluate the quality of intervention research, such as the What Works Clearinghouse and the National Center on Response to Intervention’s Technical Review Committee list several features of research design that improve confidence in findings. These include (a) the use of random assignment, (b) fidelity of implementation that is conducted with adequacy and provides evidence that the intervention was implemented as intended, and (c) the use of measures that are psychometrically reliable (<xref ref-type="bibr" rid="bibr15-0022466911419516">National Center on Response to Intervention, 2011</xref>; <xref ref-type="bibr" rid="bibr21-0022466911419516">U.S. Department of Education, 2008</xref>).</p>
<p>Although intervention fidelity data are critical to interpreting the outcomes of intervention research, practitioners’ intervention delivery, and successful funding for efficacy research, <xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell (2008)</xref> reported that fidelity of implementation is rarely reported in large-scale education studies or used for interpreting results. However, the issue of intervention fidelity is thoroughly addressed in the health literature (<xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell, 2008</xref>) and can serve as a resource to inform the field of education as we work to make forward progress in effective intervention fidelity data collection and reporting. In fact, among the top 20 most frequently cited articles with the term <italic>fidelity</italic> in the title (i.e., articles that most likely address issues directly related to fidelity), 14 are published in health-related, peer-reviewed journals. Only 2 are published in special education journals and 1 in general education journals. Therefore, it seems that the health field may contribute to our understanding of not only how to collect and report fidelity data but also how to use fidelity data to explain outcomes.</p>
<sec id="section1-0022466911419516">
<title>Fidelity Reporting in Health Research</title>
<p>Researchers conducting studies in the health sciences, where precise intervention delivery and accurate description of treatment components are critical to replication efforts, have influenced much of the understanding about intervention fidelity. Education research parallels health research, in that replication to broader populations is expected. In the past few years, authors in the health research community have written extensively about improving intervention fidelity reporting (e.g., <xref ref-type="bibr" rid="bibr5-0022466911419516">Dusenbury et al., 2003</xref>).</p>
<p>In particular, the Treatment Fidelity Workgroup of the National Institutes of Health Behavior Change Consortium identified fidelity measurement strategies that may be applied to health studies (<xref ref-type="bibr" rid="bibr1-0022466911419516">Bellg et al., 2004</xref>). First, the Fidelity Workgroup developed guidelines for monitoring the delivery of intervention. These guidelines encourage researchers to control for provider differences through close monitoring of participants’ perceptions of the provider (e.g., asking participants to provide feedback on provider effectiveness, monitoring participant complaints, and having providers work with multiple-treatment groups). Second, the Fidelity Workgroup suggested strategies to reduce differences within treatment to ensure that providers in the same condition deliver the same intervention. These strategies include using scripted intervention protocols, providing a treatment manual, and reviewing audio- and videotaped sessions. Third, the Fidelity Workgroup stressed adherence to treatment protocol, including content and dosage. The work group encouraged researchers to review the audio- or videotapes for protocol adherence and to ask providers to complete self-report checklists, indicating components delivered during the treatment. Finally, the Fidelity Workgroup emphasized minimizing contamination between treatment and control conditions, especially when implemented by the same provider. Here, the work group encouraged researchers to provide explicit training to providers regarding the rationale for keeping the conditions separate, frequent provider supervision, and audiotaped or in-person observations with review and feedback. The Fidelity Workgroup also suggested that participant exit interviews measure the extent to which participants in the control condition received treatment components. Within health research, authors not only suggest procedures for collecting fidelity data during intervention delivery but also provide guidelines for standardizing treatments to ensure that all participants receive the same treatment and systematically improving protocol adherence. Perhaps the idea of actively effecting protocol adherence is one way health research can inform general and special education intervention researchers’ work.</p>
</sec>
<sec id="section2-0022466911419516">
<title>Fidelity Reporting in Education Intervention Research</title>
<sec id="section3-0022466911419516">
<title>General Education Reviews of Fidelity Reporting</title>
<p>In 2004, the National Research Council (NRC) reviewed the quality of K-12 mathematics instructional evaluations funded by the National Science Foundation and reported that 44% of the 63 minimally adequate studies measured fidelity. Only one of these adjusted outcomes and conclusions was based on the collected fidelity data (<xref ref-type="bibr" rid="bibr17-0022466911419516">NRC, 2004</xref>). <xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell (2008)</xref> recently examined a corpus of 23 intervention studies that reported fidelity of implementation. Among these, 10 studies provided evidence of fidelity data collection of K-12 intervention for a core subject (e.g., science, social studies, math, reading) that can be implemented by a single teacher. Of these, 9 provided a quantitative measure of fidelity, whereas only 5 investigated how fidelity relates to outcomes.</p>
</sec>
<sec id="section4-0022466911419516">
<title>General Education Suggestions for Fidelity</title>
<p><xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell (2008)</xref> made several recommendations to researchers based on his review of fidelity reporting in the literature: (a) establish a theory that drives the intervention and determine what it means to implement with fidelity, (b) clearly define critical components of the intervention, (c) collect fidelity data on a randomly selected subset of intervention deliverers or on the entire sample, (d) measure fidelity to critical components and processes present in the intervention, and (e) test for and report the reliability and validity of the fidelity data collected.</p>
</sec>
<sec id="section5-0022466911419516">
<title>Special Education Reviews of Fidelity Reporting</title>
<p>Similar to general education, previous studies examining the extent to which fidelity has been reported in special education research have not yielded promising findings. In a review of all school-based experimental studies published in the <italic>Journal of Applied Behavior Analysis</italic> between 1991 and 2005, authors reported that only 30% of studies provided treatment fidelity data (<xref ref-type="bibr" rid="bibr14-0022466911419516">McIntyre, Gresham, DiGennaro, &amp; Reed, 2007</xref>). In a similar review, <xref ref-type="bibr" rid="bibr4-0022466911419516">Dane and Schneider (1998)</xref> evaluated intervention fidelity reports among primary and early secondary prevention studies of behavioral, social, or academic “maladjustment” from 1980 to 1994. Of the 162 studies reviewed, only 39 specified procedures for reporting treatment fidelity. The authors concluded that not conducting treatment fidelity checks threatens the internal validity of published studies.</p>
<p>In <xref ref-type="bibr" rid="bibr8-0022466911419516">2000</xref>, Gresham et al. examined three special education journals (<italic>Journal of Learning Disabilities, Learning Disability Quarterly</italic>, and <italic>Learning Disabilities Research and Practice</italic>) spanning a 5-year period to determine the extent to which fidelity measures were reported for intervention studies. Although 49% of the studies described treatment fidelity to some extent, 32.3% did not mention treatment fidelity at all. The authors reported that only 18.5% of the articles included evidence that treatment fidelity was measured at a rigorous level. The authors concluded that an absence or lack of consistency of fidelity reporting makes it difficult to determine whether intervention studies are effective.</p>
</sec>
<sec id="section6-0022466911419516">
<title>Special Education Suggestions for Fidelity</title>
<p>In 2005, a panel of researchers (Gersten, Fuchs, et al.) identified and described “quality indicators” for randomized control trials and quasiexperimental studies in special education. The resulting guidelines encouraged careful research design and transparent reporting of findings. Important recommendations included providing detailed participant descriptions, using multiple measures to capture an intervention’s effect, providing effect size calculations, and matching data analysis techniques to key research questions. Fidelity reporting was listed as essential for identifying interventions that are “evidence based,” suggesting that treatment and control conditions in experimental research should undergo complete and thorough investigations to establish intervention fidelity. Intervention fidelity reports, the panel suggested, should include data regarding the “surface features of fidelity implementation” through (a) reporting fidelity data collection and resulting scores, (b) conducting regular observations of the intervention, (c) using a checklist of treatment components to record whether the most critical aspects of the intervention occurred, (d) providing a record of the number of days or sessions the intervention was conducted, and (e) reporting interrater reliability among observers. In addition, the panel suggested that quality of implementation should be investigated and reported to determine whether an intervention was implemented with varying degrees of integrity (<xref ref-type="bibr" rid="bibr7-0022466911419516">Gersten, Fuchs, et al., 2005</xref>).</p>
<p>Quality indicators for single-case research have been developed in recent years as well (<xref ref-type="bibr" rid="bibr9-0022466911419516">Horner, Carr, Halle, McGee, Odom, et al., 2005</xref>; <xref ref-type="bibr" rid="bibr12-0022466911419516">Kratochwill, Hitchcock, Horner, Levin, Odom, Rindskopf, &amp; Shadish, 2010</xref>). Fidelity measurement was listed among others, as an essential component of high-quality single-case studies. “Fidelity of implementation is a significant concern within single-subject research because the independent variable is applied over time” (<xref ref-type="bibr" rid="bibr9-0022466911419516">Horner et al., 2005</xref>, p. 168). Researchers are encouraged to document and report fidelity of implementation via ongoing direct measurement during each phase of the study (<xref ref-type="bibr" rid="bibr9-0022466911419516">Horner et al., 2005</xref>).</p>
</sec>
</sec>
<sec id="section7-0022466911419516">
<title>Purpose</title>
<p>Work conducted in the health, general education, and special education fields provides fidelity data collection guidelines for intervention researchers. Given these guidelines, we were interested in the extent to which fidelity data have been collected and reported over the past few years. Therefore, the purpose of this article is twofold. First, to highlight the issue of fidelity reporting, it is important to examine the status of intervention fidelity reporting in the most widely cited journals in the fields of special education and general education. Second, it is possible that reports of fidelity procedures and data are lacking in identifiable ways. Identifying weaknesses in reporting provides authors a focus point for improving the collection and reporting of fidelity data.</p>
<p>We expand on previous examinations of treatment fidelity reporting in intervention research in several ways. First, a broader and larger group of journals in both general and special education were purposely selected based on two criteria: the journal published primarily intervention research and demonstrated high impact scores during a 5-year period. As previously mentioned, impact scores are indicative of how often articles are read and cited. Review of the most frequently cited articles is a good way to gauge the status of a field of study. Second, several components of fidelity (as suggested by <xref ref-type="bibr" rid="bibr7-0022466911419516">Gersten, Fuchs, et al., 2005</xref>; <xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell, 2008</xref>; and What Works Clearinghouse) were investigated, including the type of fidelity measure, the number of fidelity data points collected, and the extent to which fidelity was reported in different research designs (e.g., experimental study, single-case study).</p>
<p>The following research questions were addressed separately for general and special education journals:</p>
<list id="list1-0022466911419516" list-type="simple"><list-item><p><italic>Research Question 1</italic>: What proportion of intervention studies published in peer-reviewed journals reported fidelity?</p></list-item>
<list-item><p><italic>Research Question 2</italic>: What components of fidelity measurement were reported in published intervention studies?</p></list-item>
<list-item><p><italic>Research Question 3</italic>: Did reported components differ according to research design, intervention type, or number of intervention sessions?</p></list-item></list>
</sec>
<sec id="section8-0022466911419516" sec-type="methods">
<title>Method</title>
<sec id="section9-0022466911419516">
<title>Journal-Selection Criteria</title>
<p>For inclusion in this study, journals had to meet three criteria. First, journals had to be identified as either “general education” or “special education” by the Institute for Scientific Information (<xref ref-type="bibr" rid="bibr11-0022466911419516">Institute for Scientific Information, 2010</xref>). General education and special education journals were confirmed by publishers’ descriptions. Second, the journal must have published intervention research in the fields of reading, writing, and/or mathematics. Because the purpose of this article was to examine use of fidelity in academic intervention research, we excluded several general education and special education journals. For example, some journals focused primarily on reviews of research (e.g., <italic>Journal of Research in Special Education Needs</italic>) or other subject areas such as language or behavioral interventions (e.g., <italic>Journal of Speech, Language, and Hearing Research</italic>). Journals that disseminated primarily theory-driven information (e.g., <italic>Educational Philosophy and Theory</italic>) were also excluded.</p>
<p>Third, we selected the five journals with the highest impact factors in the areas of general and special education. The ISI defines an impact factor as “a measure of the frequency with which the average article in a journal has been cited in a given period of time” (<ext-link ext-link-type="uri" xlink:href="http://thomsonreuters.com/products_services/science/free/essays/impact_factor/">http://thomsonreuters.com/products_services/science/free/essays/impact_factor/</ext-link>). A journal’s 5-year impact factor is determined by counting the number of times articles from a 5-year time period were cited over the course of the following year and dividing that number by the total number of articles published over the same 5-year period. The resulting impact scores of educational journals most often fall within a range of 0.05 to 5.0 and allow researchers to compare the relative impact of publications of varying sizes, topics, or readerships without bias. The five general education journals that met the criteria for inclusion and their associated impact factors are as follows: <italic>American Education Research Journal</italic> (2.874), <italic>Scientific Studies of Reading</italic> (2.834), <italic>Early Childhood Research Quarterly</italic> (1.940), <italic>Elementary School Journal</italic> (1.876), and <italic>Reading Research Quarterly</italic> (1.717). The five special education journals with the highest impact factors were also included in the study: <italic>Annals of Dyslexia</italic> (3.259), <italic>Exceptional Children</italic> (2.894), <italic>Journal of Learning Disabilities</italic> (2.157), <italic>Dyslexia</italic> (1.938), and <italic>Journal of Special Education</italic> (1.938).</p>
</sec>
<sec id="section10-0022466911419516">
<title>Article-Selection Criteria</title>
<p>Authors hand-searched all issues of the identified peer-reviewed journals published between 2005 and 2009 (matching the years used to calculate the 5-year impact factor) and chose articles that met the following criteria:</p>
<list id="list2-0022466911419516" list-type="order"><list-item><p>Participants were in a school setting (prekindergarten through 12th grade).</p></list-item>
<list-item><p>The independent variable was described as a student-level reading, mathematics, or writing intervention. Observation studies of classroom practices with no manipulated independent variable (e.g., <xref ref-type="bibr" rid="bibr20-0022466911419516">Swanson &amp; Vaughn, 2010</xref>) were excluded.</p></list-item>
<list-item><p>A teacher, paraprofessional, or researcher delivered the intervention.</p></list-item>
<list-item><p>Student-level data were collected and reported.</p></list-item>
<list-item><p>Authors reported experimental, quasiexperimental, or single-subject designs.</p></list-item></list>
<p>Article-selection reliability checks were conducted, whereby a second author hand-searched 10% of the issues in each journal. Reliability for article selection was 97% across journals. The search yielded 76 intervention articles that met selection criteria.</p>
</sec>
<sec id="section11-0022466911419516">
<title>Code Sheet</title>
<p>Using a code sheet designed to guide systematic examination of reported fidelity procedures, each article was coded for variables, including journal title, type of intervention (i.e., math, reading, or writing), research design, number of intervention sessions, length of sessions, duration of intervention in weeks, frequency of intervention, fidelity data collection procedure, fidelity measures (e.g., self-administered checklist, observation, audio recording, video recording), and use of fidelity data in analysis and reported results. For each fidelity measure listed in an article, an additional code sheet was completed. For example, if observations were conducted, the coder completed an additional set of items that included number of observations, frequency of observations, and focus of observation (e.g., teacher or student).</p>
</sec>
<sec id="section12-0022466911419516">
<title>Procedure</title>
<sec id="section13-0022466911419516">
<title>Training and interrater reliability</title>
<p>The coding processes suggested by <xref ref-type="bibr" rid="bibr3-0022466911419516">Cooper, Hedges, and Valentine (2009)</xref> were adapted for the needs of our study. The first author trained two senior doctoral students on the use of the coding instrument during a series of meetings. All coders have experience conducting federally funded randomized control trials and participated in fidelity data collection during the intervention studies. First, an overview of the synthesis was provided. Each item on the code sheet was read and discussed. Then, for practice in using the code sheet, the first author guided several coding sessions where four separate articles were coded. After training, reliability of coding was established. The coding team (made up of the first author and two senior doctoral students) independently coded an article, then met to discuss each item. Percentage of agreement was established (agreements divided by agreements plus disagreements). Coders were asked to justify why codes were assigned, which provided an additional training opportunity. This process was conducted twice to meet the 90% interrater reliability requirement. Reliability after the first round of coding ranged from 85% to 97%. A second round of coding resulted in 90% to 95% agreement across items.</p>
</sec>
<sec id="section14-0022466911419516">
<title>Coding</title>
<p>The set of 76 articles were equally divided between the two senior doctoral students. Coding was completed through a four-step procedure that relied on independent coding and repeated meetings to discuss coding difficulties and ambiguities (<xref ref-type="bibr" rid="bibr3-0022466911419516">Cooper et al., 2009</xref>). These procedures not only increase the accuracy of codes but also serve to guard against coding bias. First, each doctoral student independently coded his or her assigned articles. Second, the coders traded articles. This time, the second coder checked all codes. Any discrepancies were noted. Third, the coders met to discuss discrepancies and came to consensus on the correct codes. Finally, the first author reviewed every code sheet for accuracy. Any questionable codes were discussed with the team to reach consensus on correct codes.</p>
</sec>
</sec>
</sec>
<sec id="section15-0022466911419516" sec-type="results">
<title>Results</title>
<p>Of the 76 intervention research articles that met selection criteria, 50 included a report of intervention fidelity. <xref ref-type="table" rid="table1-0022466911419516">Table 1</xref> summarizes treatment fidelity by research design, sample size, intervention type (mathematics, reading, writing, or a combination), duration of intervention (number of sessions and length of sessions), and implementer. A more in-depth examination of the characteristics of intervention fidelity data collection and reporting follows.</p>
<table-wrap id="table1-0022466911419516" position="float">
<label>Table 1.</label>
<caption>
<p>Number of Articles by Category That Report Fidelity Data Collection.</p></caption>
<graphic alternate-form-of="table1-0022466911419516" xlink:href="10.1177_0022466911419516-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="4">General education<hr/></th>
<th align="center" colspan="4">Special education<hr/></th>
</tr>
<tr>
<th align="left">Category</th>
<th align="center">Total articles</th>
<th align="center">Articles reporting fidelity procedures</th>
<th align="center">Articles reporting fidelity scores</th>
<th align="center">Articles reporting quality of implementation</th>
<th align="center">Total articles</th>
<th align="center">Articles reporting fidelity procedures</th>
<th align="center">Articles reporting fidelity scores</th>
<th align="center">Articles reporting quality of implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="9">Research design</td>
</tr>
<tr>
<td> Treatment comparison</td>
<td>19</td>
<td>11</td>
<td>9</td>
<td>3</td>
<td>24</td>
<td>18</td>
<td>14</td>
<td>0</td>
</tr>
<tr>
<td> Single case</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>12</td>
<td>8</td>
<td>8</td>
<td>0</td>
</tr>
<tr>
<td> Multiple treatment</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>13</td>
<td>7</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td> Single treatment</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td colspan="9">Sample size</td>
</tr>
<tr>
<td> 1–10</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>12</td>
<td>9</td>
<td>8</td>
<td>0</td>
</tr>
<tr>
<td> 11–20</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td> 21–50</td>
<td>4</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>15</td>
<td>8</td>
<td>7</td>
<td>0</td>
</tr>
<tr>
<td> 51–100</td>
<td>7</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>11</td>
<td>8</td>
<td>5</td>
<td>0</td>
</tr>
<tr>
<td> 101–200</td>
<td>7</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>8</td>
<td>5</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td> &gt;200</td>
<td>8</td>
<td>8</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>4</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td colspan="9">Intervention type</td>
</tr>
<tr>
<td> Reading only</td>
<td>20</td>
<td>12</td>
<td>8</td>
<td>1</td>
<td>35</td>
<td>23</td>
<td>15</td>
<td>1</td>
</tr>
<tr>
<td> Mathematics only</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>1</td>
<td>7</td>
<td>5</td>
<td>5</td>
<td>0</td>
</tr>
<tr>
<td> Writing only</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td> Mathematics and reading</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td> Reading and writing</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td colspan="9">Number of sessions</td>
</tr>
<tr>
<td> &lt;10</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>7</td>
<td>2</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td> 10–20</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>7</td>
<td>6</td>
<td>4</td>
<td>0</td>
</tr>
<tr>
<td> 21–40</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td> 41–60</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td> &gt;60</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td> NR</td>
<td>12</td>
<td>12</td>
<td>8</td>
<td>2</td>
<td>31</td>
<td>21</td>
<td>14</td>
<td>0</td>
</tr>
<tr>
<td colspan="9">Length of sessions</td>
</tr>
<tr>
<td> &lt;30 min</td>
<td>9</td>
<td>6</td>
<td>3</td>
<td>2</td>
<td>13</td>
<td>7</td>
<td>5</td>
<td>0</td>
</tr>
<tr>
<td> 31–45 min</td>
<td>8</td>
<td>6</td>
<td>5</td>
<td>0</td>
<td>21</td>
<td>16</td>
<td>11</td>
<td>0</td>
</tr>
<tr>
<td> 46–60 min</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>7</td>
<td>5</td>
<td>3</td>
<td>0</td>
</tr>
<tr>
<td> &gt;60 min</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td> NR</td>
<td>4</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>8</td>
<td>4</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td colspan="9">Implementer</td>
</tr>
<tr>
<td> Teacher</td>
<td>17</td>
<td>14</td>
<td>10</td>
<td>2</td>
<td>20</td>
<td>16</td>
<td>11</td>
<td>0</td>
</tr>
<tr>
<td> Researcher</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>11</td>
<td>7</td>
<td>6</td>
<td>0</td>
</tr>
<tr>
<td> Research assistant</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>4</td>
<td>3</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td> Computer</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td> Paraprofessional</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td> Other</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>5</td>
<td>5</td>
<td>3</td>
<td>0</td>
</tr>
<tr>
<td> NR</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0022466911419516">
<p>Abbreviation: NR, not reported.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<sec id="section16-0022466911419516">
<title>Report of Fidelity Data Collection and Resulting Scores</title>
<sec id="section17-0022466911419516">
<title>By research design</title>
<p>Of the 76 identified intervention studies, most used treatment-comparison designs (<italic>n</italic> = 43); the others reported multiple-treatment (with no “business-as-usual” comparison group; <italic>n</italic> = 17), single-group (<italic>n</italic> = 3), or single-case designs (<italic>n</italic> = 13). Authors of the reviewed single-case studies most frequently reported the collection of intervention fidelity data, 81.3% of the articles, compared with 67.4% of treatment-comparison study articles. Authors of multiple-treatment studies least frequently reported the collection of intervention fidelity data (56.3% of the articles). Of the 43 treatment-comparison articles, 29 reported fidelity procedures alone (67.4%) and 23 reported fidelity procedures and scores (53.4%). Within the set of treatment-comparison articles, we were also interested in identifying any fidelity reporting differences based on experimental (<italic>n</italic> = 27) and quasiexperimental (<italic>n</italic> = 16) study design. Among experimental studies, 19 of them (70%) reported fidelity procedures and 15 reported fidelity procedures and scores (56%). This trend was similar among quasiexperimental designed studies, with 10 studies reporting fidelity procedures (63%) and 8 studies reporting procedures and scores (50%).</p>
</sec>
<sec id="section18-0022466911419516">
<title>By intervention type</title>
<p>The greatest number of studies investigated the effectiveness of reading interventions (<italic>n</italic> = 55), followed by mathematics (<italic>n</italic> = 10), writing (<italic>n</italic> = 7), mathematics plus reading (<italic>n</italic> = 2), and reading plus writing (<italic>n</italic> = 2). Neither of the mathematics-plus-reading studies reported fidelity data collection procedures or scores. Authors of reading-plus-writing studies most frequently reported fidelity procedures (100% of the articles), followed by authors of writing studies (85.7% of the articles), mathematics-only studies (80% of the articles), and reading-only studies (63.6% of the articles). It is worth noting that the type of intervention with the smallest number of articles, reading plus writing, most frequently reported fidelity procedures, and the intervention type with the most articles, reading, had the lowest percentage of fidelity reporting. Authors of mathematics-only studies most frequently reported fidelity scores (80% of the articles), followed by authors of writing studies (71.4% of the articles), reading-only studies (41.8% of the articles), and the combination studies (no articles reported fidelity scores). Five articles reported quality of implementation: two reading-only studies and one each for mathematics-only, writing-only, and reading-plus-writing studies.</p>
</sec>
</sec>
<sec id="section19-0022466911419516">
<title>Fidelity Data Collection Procedures</title>
<sec id="section20-0022466911419516">
<title>By research design</title>
<p>Most authors of treatment-comparison studies who reported fidelity data collection relied on observations (<italic>n</italic> = 24). The number of observations ranged from 1 to 176. None of the articles included a copy of the observation form. However, descriptions of the observation form were provided in 12 articles. Although only 1 article explicitly reported random selection of sessions for observation and 2 articles explicitly reported nonrandom selection of sessions for observation, the other 22 articles did not report whether sessions were randomly chosen. In all, 12 articles reported an interrater reliability score among observers for fidelity. See <xref ref-type="table" rid="table2-0022466911419516">Table 2</xref> for other types of fidelity data collection that researchers used.</p>
<table-wrap id="table2-0022466911419516" position="float">
<label>Table 2.</label>
<caption>
<p>Types of Fidelity Data Collection by Research Design.</p></caption>
<graphic alternate-form-of="table2-0022466911419516" xlink:href="10.1177_0022466911419516-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Self-administered checklist (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Observation (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Audio recording (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Intervention dosage (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Video recordings (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Type not reported (<italic>n</italic>)<hr/></th>
</tr>
<tr>
<th align="left">Research design</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Treatment comparison</td>
<td>2</td>
<td>1</td>
<td>8</td>
<td>16</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Single case</td>
<td>0</td>
<td>7</td>
<td>1</td>
<td>5</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Multiple treatment</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>5</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Single treatment</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0022466911419516">
<p>Abbreviations: GE, = general education; SE, special education.</p>
</fn>
<fn id="table-fn3-0022466911419516">
<p>Some articles collected several types of fidelity data.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Seven authors of single-case studies who reported fidelity collection relied on self-report checklists completed by the teacher or researcher who implemented the intervention for the purpose of “protocol integrity” and to “ensure that instruction was delivered as planned.” All but two articles indicated that these checklists were completed for every lesson. One article reported completing the checklist once per week, and the final article did not report frequency of administration. One article provided a copy of the self-administered checklist, and one article described the checklist. In six of the studies, authors also conducted observations of instruction in addition to self-reports. The number of observations ranged from one lesson to 40% of all lessons delivered. No observation protocols were included or described in any of the articles. One article reported interrater reliability scores.</p>
</sec>
<sec id="section21-0022466911419516">
<title>By intervention type</title>
<p>Most authors of reading-only interventions relied on observations to collect intervention fidelity data (<italic>n</italic> = 31). The number of observations ranged from 1 to 176. No articles included a copy of the observation form; 14 articles contained a description of the form. One of the 31 articles reported random selection of intervention periods observed. A total of 19 articles reported interrater reliability scores. Of the 3 articles that reported use of a self-administered checklist, none reported the number completed. A copy of the checklist was provided in 1 article; a description was provided in the other 2. In all, 4 reading-only intervention articles reported using intervention dosage as a measure of intervention fidelity. They defined dosage as either number of pages read plus time spent reading or the number of lessons completed. In all cases, dosage was reported daily. Finally, video recordings were used in 3 articles. Only 1 article reported the number of videos collected (two per teacher). No article included a copy of the code sheet used to interpret the contents of the video, but 1 article described it. In one case, authors reported randomly selecting sessions to be video recorded.</p>
<p>Most mathematics-only intervention studies relied on observations to collect fidelity data. No article included a copy of the observation form, and only one included a description of the form. No authors reported whether observed intervention sessions were randomly selected. Two articles included reports of interrater reliability procedure and the resulting scores. One additional article provided an interrater reliability score but no procedure for establishing it. See <xref ref-type="table" rid="table3-0022466911419516">Table 3</xref> for other types of fidelity data collection that researchers used.</p>
<table-wrap id="table3-0022466911419516" position="float">
<label>Table 3.</label>
<caption>
<p>Types of Fidelity Data Collection by Intervention Type.</p></caption>
<graphic alternate-form-of="table3-0022466911419516" xlink:href="10.1177_0022466911419516-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Self-administered checklist (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Observation (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Audio recording (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Intervention dosage (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Video recordings (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Type not reported (<italic>n</italic>)<hr/></th>
</tr>
<tr>
<th align="left">Intervention type</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reading only</td>
<td>0</td>
<td>3</td>
<td>11</td>
<td>20</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Math only</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>4</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Writing only</td>
<td>1</td>
<td>5</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Reading plus writing</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0022466911419516">
<p>Abbreviations: GE, general education; SE, special education.</p>
</fn>
<fn id="table-fn5-0022466911419516">
<p>Some articles collected several types of fidelity data.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section22-0022466911419516">
<title>By number of intervention sessions</title>
<p><xref ref-type="bibr" rid="bibr7-0022466911419516">Gersten, Fuchs, et al. (2005)</xref> indicated that reporting the number of days and/or sessions during which intervention was conducted allowed reviewers to determine the intensity of the intervention and provide guidelines for replication. Although the authors did not offer specific guidelines for the number and frequency of data collection, they suggested that researchers collect fidelity data over the course of the study and on a regular basis. To determine the extent to which fidelity data were collected on a comprehensive and regular basis in the studies reviewed for this article, an evaluation of fidelity reporting practices according to the number of sessions follows.</p>
<p>A total of 32 of the 76 articles reported the number of sessions provided to students during the intervention period. Refer to <xref ref-type="table" rid="table4-0022466911419516">Table 4</xref> for information about the type of fidelity measures used. Of these 32 articles, 12 conducted fidelity observations. Authors of 6 articles provided enough information to calculate the proportion of sessions observed for fidelity purposes. Of these, 2 articles reported conducting observations for between 2% and 3% of intervention sessions (range of intervention sessions for these two articles was 41–155). In the article with 115 sessions, authors conducted fidelity observations at the beginning, middle, and end of the year, indicating that fidelity data were collected over the course of the study but perhaps not on a regular basis. One article reported that 15% of the sessions were observed but provided no information about the frequency of collection during the 53rd to 126th intervention sessions. Three articles reported observing between 30% and 40% of sessions. These 3 small-scale, multiple-treatment, or single-case studies reported fidelity observations over the course of the study and on a regular basis.</p>
<table-wrap id="table4-0022466911419516" position="float">
<label>Table 4.</label>
<caption>
<p>Types of Fidelity Data Collection by Number of Sessions.</p></caption>
<graphic alternate-form-of="table4-0022466911419516" xlink:href="10.1177_0022466911419516-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Self-administered checklist (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Observation (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Audio recording (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Intervention dosage (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Video recordings (<italic>n</italic>)<hr/></th>
<th align="center" colspan="2">Type not reported (<italic>n</italic>)<hr/></th>
</tr>
<tr>
<th align="left">Number of sessions</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
<th align="center">GE</th>
<th align="center">SE</th>
</tr>
</thead>
<tbody>
<tr>
<td>&lt; 10</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>10–20</td>
<td>0</td>
<td>4</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>21–40</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>41–60</td>
<td>9</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>&gt; 60</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>NR</td>
<td>0</td>
<td>2</td>
<td>10</td>
<td>17</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0022466911419516">
<p>Abbreviations: GE, general education; SE, special education.</p>
</fn>
<fn id="table-fn7-0022466911419516">
<p>Some articles collected several types of fidelity data.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section23-0022466911419516">
<title>Report of Intervention Quality</title>
<p>In the articles that reported fidelity data collection, authors most often cited reasons such as “to ensure treatment is delivered as intended” for data collection. No author explicitly stated that fidelity data were collected to also ascertain the degree to which interventions were delivered with quality. However, authors of five studies did report quality-related fidelity data.</p>
</sec>
<sec id="section24-0022466911419516">
<title>Fidelity Data Used to Explain Conclusions</title>
<p>One of the primary reasons to collect fidelity data during a study is to provide evidence of internal validity, ensuring that the intervention is related to the effect, and external validity, the degree to which study results may generalize to other settings. Of the 50 studies that reported fidelity data collection, only 2 provided a discussion of issues related to internal validity. One study (<xref ref-type="bibr" rid="bibr2-0022466911419516">Clements &amp; Sarama, 2008</xref>) provided a discussion of the way in which fidelity was related to achievement gains in the study sample. Although authors reported that fidelity scores were not related to gains at a statistically significant level, they were positively correlated with students’ gains in achievement. In another study (<xref ref-type="bibr" rid="bibr18-0022466911419516">O’Connor, White, &amp; Swanson, 2007</xref>), authors suspected that a lack of differences between groups in vocabulary outcomes was due to differential exposure to text. Through the use of fidelity data, they established that the amount of text read was different between groups; however, authors could not rule that the type of text read by the poor readers in the study contained too few unknown vocabulary words to provide the advantage of practicing new words in the context of connected text. Similarly, 4 studies included discussion of external validity, claiming that high fidelity ratings provided evidence that paraprofessionals and/or teachers were able to deliver interventions as designed. Fidelity scores were not included as moderators of student outcomes in any study.</p>
</sec>
<sec id="section25-0022466911419516">
<title>Fidelity Data Reporting in General and Special Education Journals</title>
<p>The last indicator we examined was the extent to which fidelity data were reported in general and special education journals (as defined by ISI Web of Knowledge Journal Citation Reports described in the “Method” section). Considerably more intervention studies were reported in special education journals (<italic>n</italic> = 50) than general education journals (<italic>n</italic> = 26), with a higher percentage of treatment fidelity reported in special education journals (68%) than general education journals (65%). The types of fidelity data collected were similarly reported in both general and special education journals.</p>
</sec></sec>
<sec id="section26-0022466911419516" sec-type="discussion">
<title>Discussion</title>
<p>The purpose of this study was to examine the reporting of intervention fidelity in articles published in high-impact general and special education journals. We used high-impact journals as a “best evidence” case of how intervention fidelity is being reported in research studies. As has been noted in previous research (e.g., <xref ref-type="bibr" rid="bibr13-0022466911419516">Mastropieri et al., 2009</xref>), only a small portion of the published articles in these journals reported intervention research. A total of 76 articles were reviewed, and 67% of those articles provided fidelity procedures for the interventions implemented. In all, 47% provided quantitative data (or scores) on fidelity implementation. These percentages are substantially higher than have been reported in previous research (e.g., <xref ref-type="bibr" rid="bibr8-0022466911419516">Gresham et al., 2000</xref>; <xref ref-type="bibr" rid="bibr17-0022466911419516">NRC, 2004</xref>; <xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell, 2008</xref>), suggesting that there may be an increase in the collection and reporting of fidelity data in recent years. Although fidelity of implementation procedures were provided in 67% of the intervention articles, only 6.6% of the articles provided information on the quality of implementation as <xref ref-type="bibr" rid="bibr7-0022466911419516">Gersten, Fuchs, et al. (2005)</xref> have recommended. There are several reasons why the quality of intervention implementation may not be a focus of data collection within current academic intervention research. First, it is possible that researchers consider quality but embed it in their fidelity procedures, rather than code it as a separate component. For example, on a checklist of intervention components, an interventionist may receive credit for implementing a comprehension strategy component only if the strategy is modeled with clear language. If this level of detail were not provided in the corresponding research article, the quality indicator would be missing. Second, quality of implementation can be difficult to measure because it requires some subjectivity in scoring. However, there is sufficient previous research to suggest that interrater reliability in scoring quality of instruction is possible and that quality of instruction and student outcomes are related (<xref ref-type="bibr" rid="bibr6-0022466911419516">Gersten, Baker, Haager, &amp; Graves, 2005</xref>). As Gersten, Fuchs, et al. suggested, collecting data on the quality of implementation may provide information that data collection only on the components of the intervention does not sufficiently capture. Thus, data on the quality of implementation may be particularly helpful in efficacy studies that intend to provide causal links between intervention and student outcomes.</p>
<p>Observation (live, audio, or video) was the most common form of data collection in the studies reviewed. Among the 50 studies that reported fidelity procedures, 88% (<italic>n</italic> = 44) reported using some type of classroom observation. Observations can provide precise estimates of intervention implementation and may be more reliable data than self-reports or questionnaires that depend on the implementer to objectively remember and report about the implementation. Observations can also be one of the most expensive forms of fidelity data collection, often requiring additional personnel to attend intervention sessions or to engage in time-consuming coding of intervention sessions. Our findings suggest that when researchers are reporting fidelity, they most often choose observation data. The time and cost of observation data may have been prohibitive for some of the studies that did not report fidelity data.</p>
<p>Both <xref ref-type="bibr" rid="bibr7-0022466911419516">Gersten, Fuchs, et al. (2005)</xref> and <xref ref-type="bibr" rid="bibr19-0022466911419516">O’Donnell (2008)</xref> highlighted the need for reporting reliability in fidelity data. However, only 45 of the articles collecting fidelity information with observation techniques provided information on the reliability of the raters collecting the observation data. Interrater reliability was generally high (more than 90%) when it was reported in the articles. When fidelity data are reliable, important information regarding the internal and external validity of the study can be ascertained; however, without evidence of reliability of the data, it is impossible to establish the extent to which the intervention was implemented as intended.</p>
<sec id="section27-0022466911419516">
<title>Limitations</title>
<p>The data presented in this study represent articles in high-impact general and special education journals publishing intervention research from 2005 to 2009. Of course, many journals were not included in this sample, making the sample not representative of all peer-reviewed articles. For example, only studies that manipulated academic interventions were included in this study. It is possible that fidelity reporting trends among studies that manipulate behavior-based interventions are quite different. While this synthesis cannot provide information related to fidelity data collection among behavior-based interventions, it is viable population of studies to investigate.</p>
<p>Our findings of fidelity data collection and reporting do not necessarily reflect the total quality of intervention studies being published in high-impact journals. There are many aspects related to the quality of intervention studies that we do not address here, with fidelity representing only one key area.</p>
</sec>
<sec id="section28-0022466911419516">
<title>Implications</title>
<p>Although we found that two thirds of the intervention research articles published in these high-impact journals provided information on fidelity of implementation, the fact that not all of the articles incorporated even basic fidelity information suggests that there is not yet a widespread standard for reporting fidelity data in intervention research. Admittedly, page limitations may have constrained the authors of the articles in terms of the amount of fidelity information that could be provided. For example, it is not surprising, considering page limitations, that none of the articles included the checklist used for fidelity data collection. Nevertheless, our findings suggest that reporting of fidelity data is becoming more common in published articles in both general and special education, at least in high-impact journals. In order for educators and other researchers to adequately interpret the results of intervention research, precise collection and reporting of fidelity data are required. In addition, the precision of fidelity information can be increased with reporting of fidelity scores and the examination of quality of instruction in addition to the occurrence of intervention steps or components (<xref ref-type="bibr" rid="bibr7-0022466911419516">Gersten, Fuchs, et al., 2005</xref>). Currently, only about half of the studies include fidelity scores, and very few studies have included quality of implementation as a component of fidelity data collection and reporting. Thus, there continues to be room for improvement for intervention researchers to collect and report more precise fidelity information in the form of quantitative scores and quality of intervention implementation. In addition, fidelity data are difficult to interpret when the reliability of the data are unknown. Less than half of the studies reporting on fidelity provided reliability information. To improve the quality of fidelity reporting in published articles, it is important for researchers to plan for fidelity data collection prior to study implementation so that critical components of the intervention can be identified, and research personnel can be trained adequately for collecting data allowing for adequate reporting of the reliability of the data collected. Continued increases in the reporting of fidelity data in published articles of intervention research along with increases in the quality of reporting will allow readers to better interpret article findings. Finally, journal editors serve as a “gate-keeper” in determining the quality level of research that enters into publication. If researchers submit manuscripts with key components of fidelity data collection and reporting missing, it is the journal editor’s charge to encourage authors to reveal more information. Although there is still much to learn about ways in which fidelity data can be used to explain results, it is clear that fidelity data used at a basic level (i.e., descriptively to explain whether the intervention was implemented as intended) are essential to high-quality research.</p>
</sec>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="other">
<label>Authors’ Note</label>
<p>The opinions expressed are those of the authors and do not represent views of the Institute of Education Sciences or the U.S. Department of Education.</p>
</fn>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by an award granted to The Meadows Center for Preventing Educational Risk at The University of Texas at Austin by the Institute of Education Sciences, U.S. Department of Education, the Reading for Understanding Research Initiative, R305F100013.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bellg</surname><given-names>A. J.</given-names></name>
<name><surname>Resnick</surname><given-names>B.</given-names></name>
<name><surname>Minicucci</surname><given-names>D. S.</given-names></name>
<name><surname>Ogedegbe</surname><given-names>G.</given-names></name>
<name><surname>Ernst</surname><given-names>D.</given-names></name>
<name><surname>Borrelli</surname><given-names>B.</given-names></name>
<name><surname>. . . Czajkowski</surname><given-names>S.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Enhancing treatment fidelity in health behavior change studies: Best practices and recommendations from the NIH behavior change consortium</article-title>. <source>Health Psychology</source>, <volume>23</volume>, <fpage>443</fpage>–<lpage>451</lpage>.</citation>
</ref>
<ref id="bibr2-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clements</surname><given-names>D. H.</given-names></name>
<name><surname>Sarama</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Experimental evaluation of the effects of a research-based preschool mathematics curriculum</article-title>. <source>American Educational Research Journal</source>, <volume>45</volume>, <fpage>443</fpage>–<lpage>494</lpage>.</citation>
</ref>
<ref id="bibr3-0022466911419516">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cooper</surname><given-names>H.</given-names></name>
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
<name><surname>Valentine</surname><given-names>J. C.</given-names></name>
</person-group> (<year>2009</year>). <source>The handbook of research synthesis and meta-analysis</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Russell Sage Foundation</publisher-name>.</citation>
</ref>
<ref id="bibr4-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dane</surname><given-names>A. V.</given-names></name>
<name><surname>Schneider</surname><given-names>B. H.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Program integrity in primary and early secondary prevention: Are implementation effects out of control?</article-title> <source>Clinical Psychology Review</source>, <volume>18</volume>, <fpage>23</fpage>–<lpage>35</lpage>.</citation>
</ref>
<ref id="bibr5-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dusenbury</surname><given-names>L.</given-names></name>
<name><surname>Brannigan</surname><given-names>R.</given-names></name>
<name><surname>Falco</surname><given-names>M.</given-names></name>
<name><surname>Hansen</surname><given-names>W. B.</given-names></name>
</person-group> (<year>2003</year>). <article-title>A review of research on fidelity of implementation: Implications for drug abuse prevention in school settings</article-title>. <source>Health Education Research</source>, <volume>18</volume>, <fpage>237</fpage>–<lpage>256</lpage>.</citation>
</ref>
<ref id="bibr6-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gersten</surname><given-names>R.</given-names></name>
<name><surname>Baker</surname><given-names>S. K.</given-names></name>
<name><surname>Haager</surname><given-names>D.</given-names></name>
<name><surname>Graves</surname><given-names>A. W.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Exploring the role of teacher quality in predicting reading outcomes for first-grade English learners</article-title>. <source>Remedial and Special Education</source>, <volume>26</volume>, <fpage>197</fpage>–<lpage>206</lpage>. doi:<pub-id pub-id-type="doi">10.1177/07419325050260040201</pub-id></citation>
</ref>
<ref id="bibr7-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gersten</surname><given-names>R.</given-names></name>
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Compton</surname><given-names>D.</given-names></name>
<name><surname>Coyne</surname><given-names>M.</given-names></name>
<name><surname>Greenwood</surname><given-names>C.</given-names></name>
<name><surname>Innocenti</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Quality indicators for group experimental and quasi-experimental research in special education</article-title>. <source>Exceptional Children</source>, <volume>71</volume>, <fpage>149</fpage>–<lpage>164</lpage>.</citation>
</ref>
<ref id="bibr8-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gresham</surname><given-names>F. M.</given-names></name>
<name><surname>MacMillan</surname><given-names>D. L.</given-names></name>
<name><surname>Beebe-Frankenberger</surname><given-names>M. E.</given-names></name>
<name><surname>Bocian</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Treatment integrity in learning disabilities intervention research: Do we really know how treatments are implemented?</article-title> <source>Learning Disabilities Research &amp; Practice</source>, <volume>15</volume>, <fpage>198</fpage>–<lpage>205</lpage>.</citation>
</ref>
<ref id="bibr9-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Horner</surname><given-names>R. H.</given-names></name>
<name><surname>Carr</surname><given-names>E. G.</given-names></name>
<name><surname>Halle</surname><given-names>J.</given-names></name>
<name><surname>McGee</surname><given-names>G.</given-names></name>
<name><surname>Odom</surname><given-names>S.</given-names></name>
<name><surname>Wolery</surname><given-names>M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The use of single-subject research to identify evidence-based practice in special education</article-title>. <source>Exceptional Children</source>, <volume>71</volume>, <fpage>165</fpage>-<lpage>179</lpage>.</citation>
</ref>
<ref id="bibr10-0022466911419516">
<citation citation-type="web">
<collab>Institute for Scientific Information</collab>. (<year>n.d.</year>). <source>Web of Knowledge Journal Citation Reports</source>. Available from <ext-link ext-link-type="uri" xlink:href="http://apps.isiknowledge.com">http://apps.isiknowledge.com</ext-link></citation>
</ref>
<ref id="bibr11-0022466911419516">
<citation citation-type="book">
<collab>Institute for Scientific Information</collab>. (<year>2010</year>). J<source>ournal citation reports</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>Institute for Scientific Information</publisher-name>.</citation>
</ref>
<ref id="bibr12-0022466911419516">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Kratochwill</surname><given-names>T. R.</given-names></name>
<name><surname>Hitchcock</surname><given-names>J.</given-names></name>
<name><surname>Horner</surname><given-names>R. H.</given-names></name>
<name><surname>Levin</surname><given-names>J. R.</given-names></name>
<name><surname>Odom</surname><given-names>S. L.</given-names></name>
<name><surname>Rindskopf</surname><given-names>D. M.</given-names></name>
<name><surname>Shadish</surname><given-names>W. R.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Single-case designs technical documentation</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/wwc/pdf/wwc_scd.pdf">http://ies.ed.gov/ncee/wwc/pdf/wwc_scd.pdf</ext-link></citation>
</ref>
<ref id="bibr13-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mastropieri</surname><given-names>M. A.</given-names></name>
<name><surname>Berkeley</surname><given-names>S.</given-names></name>
<name><surname>McDuffie</surname><given-names>K. A.</given-names></name>
<name><surname>Graff</surname><given-names>H.</given-names></name>
<name><surname>Marshak</surname><given-names>L.</given-names></name>
<name><surname>Conners</surname><given-names>N. A.</given-names></name>
<name><surname>. . . Cuenca-Sanchez</surname><given-names>Y.</given-names></name>
</person-group> (<year>2009</year>). <article-title>What is published in the field of special education? An analysis of 11 prominent journals</article-title>. <source>Exceptional Children</source>, <volume>76</volume>, <fpage>95</fpage>–<lpage>109</lpage>.</citation>
</ref>
<ref id="bibr14-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McIntyre</surname><given-names>L. L.</given-names></name>
<name><surname>Gresham</surname><given-names>F. M.</given-names></name>
<name><surname>DiGennaro</surname><given-names>F. D.</given-names></name>
<name><surname>Reed</surname><given-names>D. D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Treatment integrity of school-based interventions with children in the Journal of Applied Behavior Analysis 1991–2005</article-title>. <source>Journal of Applied Behavior Analysis</source>, <volume>40</volume>, <fpage>659</fpage>–<lpage>672</lpage>.</citation>
</ref>
<ref id="bibr15-0022466911419516">
<citation citation-type="web">
<collab>National Center on Response to Intervention</collab>. (<year>2011</year>). <source>Instruction tools chart</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.rti4success.org/tools_charts/instruction.php">http://www.rti4success.org/tools_charts/instruction.php</ext-link></citation>
</ref>
<ref id="bibr16-0022466911419516">
<citation citation-type="gov">
<collab>National Institutes of Health</collab>. (<year>2011</year>). <source>Learning disabilities research center request for application</source> (FOA Number RFA-HD-12-202). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://grants.nih.gov/grants/guide/rfa-files/RFA-HD-12-202.html">http://grants.nih.gov/grants/guide/rfa-files/RFA-HD-12-202.html</ext-link></citation>
</ref>
<ref id="bibr17-0022466911419516">
<citation citation-type="book">
<collab>National Research Council</collab>. (<year>2004</year>). <source>On evaluating curricular effectiveness: Judging the quality of K -12 mathematics evaluations</source>. <publisher-loc>Washington, DC:</publisher-loc> <publisher-name>The National Academies Press</publisher-name>.</citation>
</ref>
<ref id="bibr18-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>O’Connor</surname><given-names>R. E.</given-names></name>
<name><surname>White</surname><given-names>A.</given-names></name>
<name><surname>Swanson</surname><given-names>H. L.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Repeated reading versus continuous reading: Influences on reading fluency and comprehension</article-title>. <source>Exceptional Children</source>, 7<source>4</source>, <fpage>31</fpage>–<lpage>46</lpage>.</citation>
</ref>
<ref id="bibr19-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>O’Donnell</surname><given-names>C. L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Defining, conceptualizing, and measuring fidelity of implementation and its relationship to outcomes in K-12 curriculum intervention research</article-title>. <source>Review of Educational Research</source>, <volume>78</volume>, <fpage>33</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr20-0022466911419516">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>E. A.</given-names></name>
<name><surname>Vaughn</surname><given-names>S.</given-names></name>
</person-group> (<year>2010</year>). <article-title>An observation study of reading instruction provided to elementary students with learning disabilities in the resource room</article-title>. <source>Psychology in the Schools</source>, <volume>47</volume>, <fpage>481</fpage>–<lpage>492</lpage>.</citation>
</ref>
<ref id="bibr21-0022466911419516">
<citation citation-type="gov">
<collab>U.S. Department of Education</collab>. (<year>2008</year>). <source>WWC Procedures and Standards Handbook: Version 2.0</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Institute of Education Sciences</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/wwc/references/idocviewer/doc.aspx?docid=19&amp;tocid=1">http://ies.ed.gov/ncee/wwc/references/idocviewer/doc.aspx?docid=19&amp;tocid=1</ext-link></citation>
</ref>
<ref id="bibr22-0022466911419516">
<citation citation-type="book">
<collab>U.S. Department of Education</collab>. (<year>2011</year>). <source>Special education research grant request for application</source> (CFDA Number 84.324A). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Institute for Education Sciences</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>