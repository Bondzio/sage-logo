<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">APM</journal-id>
<journal-id journal-id-type="hwp">spapm</journal-id>
<journal-title>Applied Psychological Measurement</journal-title>
<issn pub-type="ppub">0146-6216</issn>
<issn pub-type="epub">1552-3497</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0146621612471888</article-id>
<article-id pub-id-type="publisher-id">10.1177_0146621612471888</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Two Approaches to Estimation of Classification Accuracy Rate Under Item Response Theory</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Lathrop</surname><given-names>Quinn N.</given-names></name>
<xref ref-type="aff" rid="aff1-0146621612471888">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Cheng</surname><given-names>Ying</given-names></name>
<xref ref-type="aff" rid="aff1-0146621612471888">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0146621612471888"><label>1</label>University of Notre Dame, IN, USA</aff>
<author-notes>
<corresp id="corresp1-0146621612471888">Ying Cheng, Department of Psychology, University of Notre Dame, 118 Haggar Hall, Notre Dame, IN 46556, USA. Email: <email>ycheng4@nd.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>37</volume>
<issue>3</issue>
<fpage>226</fpage>
<lpage>241</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Within the framework of item response theory (IRT), there are two recent lines of work on the estimation of classification accuracy (CA) rate. One approach estimates CA when decisions are made based on total sum scores, the other based on latent trait estimates. The former is referred to as the Lee approach, and the latter, the Rudner approach, each after its representative contributor. In this article, the two approaches are delineated in the same framework to highlight their similarities and differences. In addition, a simulation study manipulating IRT model, sample size, test length, and cut score location was conducted. The study investigated the empirical CA that can be achieved using either the total scores or the latent trait estimates. It also evaluated the performances of the two approaches in estimating their respective empirical CAs. Results on the empirical CA suggest that when the model fits, classifications made with the latent trait estimate shall be equally or more accurate than classifications made with total score. The magnitude of difference was governed by divergence from the one-parameter logistic (1PL) model. Both Lee and Rudner approaches provided good estimates of their respective empirical CAs for every condition that was simulated. Practical implications of the simulation results are discussed.</p>
</abstract>
<kwd-group>
<kwd>classification</kwd>
<kwd>classification accuracy</kwd>
<kwd>item response theory</kwd>
<kwd>cut scores</kwd>
<kwd>empirical accuracy</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Classification, as discussed here, is the act of categorizing examinees by comparing their scores on a test with given cut scores. “Classification accuracy” (CA) refers to the rate at which examinees are classified into the category they truly belong in based on their true ability level (<xref ref-type="bibr" rid="bibr10-0146621612471888">Lee, 2010</xref>). Previous simulation studies have shown that true CA increases with test length, and decreases when the cut score is located near the mean of the examinee distribution (<xref ref-type="bibr" rid="bibr21-0146621612471888">Wan, Brennan, &amp; Lee, 2007</xref>; <xref ref-type="bibr" rid="bibr23-0146621612471888">Wyse, 2011</xref>). But in reality, the true ability of an examinee is never known, so CA can only be estimated.</p>
<p>Recently, approaches to estimate CA under item response theory (IRT) have been developed along two research lines, one represented by <xref ref-type="bibr" rid="bibr11-0146621612471888">Lee, Hanson, and Brennan (2002)</xref> and <xref ref-type="bibr" rid="bibr10-0146621612471888">Lee (2010)</xref> and the other pioneered by <xref ref-type="bibr" rid="bibr16-0146621612471888">Rudner (2001</xref>, <xref ref-type="bibr" rid="bibr17-0146621612471888">2005</xref>). The two approaches have also been applied in real-data analyses. Estimating the accuracy of language proficiency classifications, <xref ref-type="bibr" rid="bibr25-0146621612471888">Zhang (2010)</xref> used the Rudner approach to estimate the CA of a language proficiency test under various IRT models. The Lee approach was used to estimate the CA of statewide language proficiency tests in K–12 schools as part of an annual technical report on the assessment (<xref ref-type="bibr" rid="bibr1-0146621612471888">Bergeson, 2007</xref>). They found that while CA was at acceptable levels at all grades, CA increased with grade so that younger students were estimated to be misclassified more often. Similarly, the Rudner approach has been applied to statewide math tests for adult education, showing that the tests had high levels of CA, which lent support to the use of these test scores in decision making (<xref ref-type="bibr" rid="bibr19-0146621612471888">Sireci et al., 2008</xref>).</p>
<p>While their use in real-data analysis is growing, theoretical examination of the performances of CA estimates from the Rudner and Lee approaches has been lacking. Important questions remain to be answered. For example, are the CA estimates accurate and precise? Which factors influence the performance of the CA estimates? Few studies have tapped on these issues, except <xref ref-type="bibr" rid="bibr13-0146621612471888">Martineau (2007)</xref> and <xref ref-type="bibr" rid="bibr24-0146621612471888">Wyse and Hao (2012)</xref>. <xref ref-type="bibr" rid="bibr13-0146621612471888">Martineau (2007)</xref> investigated the Rudner approach in simulations and found that the approach can produce a good estimate of true CA if the number of examinees is greater than 20, but this study did not look into the Lee approach. <xref ref-type="bibr" rid="bibr24-0146621612471888">Wyse and Hao (2012)</xref> used simulations to compare the person-based Rudner approach and the Lee approach, but they did not establish a true accuracy to which the estimates from either the Rudner or the Lee approach can be compared against. The true or empirical accuracy can be obtained by running multiple replications of the same simulated condition. Without the replications, no empirical accuracy can be established, consequently no information on bias or standard error (<italic>SE</italic>) of the estimates can be obtained. Moreover, the Rudner and the Lee approaches are trying to estimate different “true” accuracies, one based on decisions made with latent trait estimates and the other made with sum scores. The target true accuracies are different for either approach. Performance of a CA estimate can only be evaluated against its respective true CA. Comparing the estimates from Rudner and Lee approaches against each other does not tell us the accuracy or precision of these estimates. Furthermore, studies on the performance of the two approaches in response to different cut score locations and different IRT models are still lacking.</p>
<p>The purpose of this study is to fill in these gaps by first comparing and contrasting the two approaches in a unified framework, and then evaluating CA estimates from these two approaches in a simulation study. The study involves various factors, including IRT model, test length, and cut score locations, and investigates their effects on CA estimates. The authors first review the Lee and Rudner approaches in comparable terms below.</p>
<sec id="section1-0146621612471888">
<title>The Lee Approach</title>
<p>Denote the response pattern of a random examinee as <inline-formula id="inline-formula1-0146621612471888">
<mml:math display="inline" id="math1-0146621612471888">
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
<mml:mo>′</mml:mo>
<mml:mo>=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>3</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>J</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>, where <italic>V</italic><sub><italic>j</italic></sub> is the response to the item <italic>j</italic>; <inline-formula id="inline-formula2-0146621612471888">
<mml:math display="inline" id="math2-0146621612471888">
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>J</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula>; and <italic>J</italic> is the test length. <italic>V</italic><sub><italic>j</italic></sub> can take values <inline-formula id="inline-formula3-0146621612471888">
<mml:math display="inline" id="math3-0146621612471888">
<mml:mrow>
<mml:mi>m</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> when there are in total <inline-formula id="inline-formula4-0146621612471888">
<mml:math display="inline" id="math4-0146621612471888">
<mml:mrow>
<mml:mi>M</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> response categories. The realization of <inline-formula id="inline-formula5-0146621612471888">
<mml:math display="inline" id="math5-0146621612471888">
<mml:mrow>
<mml:mi mathvariant="bold">V</mml:mi>
<mml:mo>′</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> is <inline-formula id="inline-formula6-0146621612471888">
<mml:math display="inline" id="math6-0146621612471888">
<mml:mrow>
<mml:mi mathvariant="bold">v</mml:mi>
<mml:mo>′</mml:mo>
<mml:mo>=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>3</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>J</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. The total score as a random variable is denoted as <italic>X</italic> and its realization <italic>x</italic>, where <inline-formula id="inline-formula7-0146621612471888">
<mml:math display="inline" id="math7-0146621612471888">
<mml:mrow>
<mml:mi>X</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>′</mml:mo>
<mml:mi mathvariant="bold">V</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula8-0146621612471888">
<mml:math display="inline" id="math8-0146621612471888">
<mml:mrow>
<mml:mi>x</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>′</mml:mo>
<mml:mi mathvariant="bold">v</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula>. The Lee approach assumes that classifications are made on the basis of the total score <italic>x</italic>. Its goal is to find the probability of each possible total score given θ, and then aggregates the probabilities according to the cut scores. The conditional probability of an examinee at θ to reach a total score <inline-formula id="inline-formula9-0146621612471888">
<mml:math display="inline" id="math9-0146621612471888">
<mml:mrow>
<mml:mi>X</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> is calculated by summing the probabilities of all possible response patterns that would lead to that total score.</p>
<p>Calculation of the probability of each response pattern involves taking the product of the probabilities of the response to each item within that response pattern:</p>
<p><disp-formula id="disp-formula1-0146621612471888">
<mml:math display="block" id="math10-0146621612471888">
<mml:mrow>
<mml:mtext>Pr</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mo>∀</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi mathvariant="bold">v</mml:mi>
<mml:mspace width="0.25em"/>
<mml:mtext>where</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mn>1</mml:mn>
<mml:mo>′</mml:mo>
<mml:mi mathvariant="bold">v</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:munder>
<mml:mtext>Pr</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi mathvariant="bold">v</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0146621612471888" xlink:href="10.1177_0146621612471888-eq1.tif"/>
</disp-formula></p>
<p>where <inline-formula id="inline-formula10-0146621612471888">
<mml:math display="inline" id="math11-0146621612471888">
<mml:mrow>
<mml:mtext>Pr</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi mathvariant="bold">v</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mtext>Prob</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi mathvariant="bold">V</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="bold">v</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi>θ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>Π</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>J</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mtext>Prob</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. <inline-formula id="inline-formula11-0146621612471888">
<mml:math display="inline" id="math12-0146621612471888">
<mml:mrow>
<mml:mtext>Prob</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> is determined by the item response model and changes with the model’s parameters and structure.</p>
<p>For a dichotomous item, the possible responses would be 0 and 1, for an incorrect and correct response, respectively. In the case of the three-parameter logistic (3PL) model, <inline-formula id="inline-formula12-0146621612471888">
<mml:math display="inline" id="math13-0146621612471888">
<mml:mrow>
<mml:mtext>Prob</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">/</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:mtext>exp</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>7</mml:mn>
<mml:msub>
<mml:mrow>
<mml:mtext>α</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>β</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">]</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula13-0146621612471888">
<mml:math display="inline" id="math14-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>α</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula14-0146621612471888">
<mml:math display="inline" id="math15-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>β</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, and <italic>c</italic><sub><italic>j</italic></sub> are the discrimination, difficulty parameter, and pseudo-guessing parameter of the <italic>j</italic>th item, respectively, <inline-formula id="inline-formula15-0146621612471888">
<mml:math display="inline" id="math16-0146621612471888">
<mml:mrow>
<mml:mtext>Prob</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>−</mml:mo>
<mml:mtext>Prob</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. The two-parameter logistic (2PL) model is a special case of the 3PL model where all <inline-formula id="inline-formula16-0146621612471888">
<mml:math display="inline" id="math17-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula>. The one-parameter logistic (1PL) model further constrains all <inline-formula id="inline-formula17-0146621612471888">
<mml:math display="inline" id="math18-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>α</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> to be equal.</p>
<p>For graded response model (GRM) items, the possible responses would be <inline-formula id="inline-formula18-0146621612471888">
<mml:math display="inline" id="math19-0146621612471888">
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>M</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> for an item with <inline-formula id="inline-formula19-0146621612471888">
<mml:math display="inline" id="math20-0146621612471888">
<mml:mrow>
<mml:mi>M</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> ordered response categories. First, the probability to score above a given category is <inline-formula id="inline-formula20-0146621612471888">
<mml:math display="inline" id="math21-0146621612471888">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jm</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mtext>Prob</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">/</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:mtext>exp</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>7</mml:mn>
<mml:msub>
<mml:mrow>
<mml:mtext>α</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>β</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>jm</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula21-0146621612471888">
<mml:math display="inline" id="math22-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>α</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is the item discrimination parameter and <inline-formula id="inline-formula22-0146621612471888">
<mml:math display="inline" id="math23-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>β</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>jm</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is threshold parameter of the <italic>m</italic>th response category. Using the restrictions that <inline-formula id="inline-formula23-0146621612471888">
<mml:math display="inline" id="math24-0146621612471888">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula24-0146621612471888">
<mml:math display="inline" id="math25-0146621612471888">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula>, the probability of scoring within a given category is then <inline-formula id="inline-formula25-0146621612471888">
<mml:math display="inline" id="math26-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jm</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jm</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>m</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> (for more details, see <xref ref-type="bibr" rid="bibr18-0146621612471888">Samejima, 1969</xref>).</p>
<p>The calculation of <xref ref-type="disp-formula" rid="disp-formula1-0146621612471888">Equation 1</xref> becomes unmanageably large, as the number of items increase, because many possible response patterns can lead to the same total score. Nonetheless, recursive algorithms can be used to calculate <xref ref-type="disp-formula" rid="disp-formula1-0146621612471888">Equation 1</xref>. For algorithms for dichotomous and polytomous IRT models, see <xref ref-type="bibr" rid="bibr12-0146621612471888">Lord and Wingersky (1984)</xref> and <xref ref-type="bibr" rid="bibr7-0146621612471888">Hanson (1994)</xref>, respectively.</p>
<p>The cut scores for Lee’s approach are on the sum score scale and are organized in a vector such that <inline-formula id="inline-formula26-0146621612471888">
<mml:math display="inline" id="math27-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mo>&lt;</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mo>&lt;</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mo>&lt;</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>K</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, which classifies examinees into <italic>K</italic> groups. <inline-formula id="inline-formula27-0146621612471888">
<mml:math display="inline" id="math28-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula28-0146621612471888">
<mml:math display="inline" id="math29-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>K</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mtext>sup</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> are added, where <inline-formula id="inline-formula29-0146621612471888">
<mml:math display="inline" id="math30-0146621612471888">
<mml:mrow>
<mml:mtext>sup</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> equals the maximum possible total score (which equals <italic>J</italic> for dichotomous tests), to keep notation more consistent with the Rudner approach that will be described next. An examinee with total score <inline-formula id="inline-formula30-0146621612471888">
<mml:math display="inline" id="math31-0146621612471888">
<mml:mrow>
<mml:mi>X</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> is classified into category <italic>k</italic> if <inline-formula id="inline-formula31-0146621612471888">
<mml:math display="inline" id="math32-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>≤</mml:mo>
<mml:mi>x</mml:mi>
<mml:mspace width="0.25em"/>
<mml:mo>&lt;</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>. An examinee with <inline-formula id="inline-formula32-0146621612471888">
<mml:math display="inline" id="math33-0146621612471888">
<mml:mrow>
<mml:mi>X</mml:mi>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>K</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> will automatically be placed in the highest category <italic>K</italic>. In the simplest scenario where there is only one given cut score, <inline-formula id="inline-formula33-0146621612471888">
<mml:math display="inline" id="math34-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, a binary decision (e.g., pass/fail) is made. For all scores above or at <inline-formula id="inline-formula34-0146621612471888">
<mml:math display="inline" id="math35-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, examinees will be classified into Category 2 (i.e., pass), and for scores below <inline-formula id="inline-formula35-0146621612471888">
<mml:math display="inline" id="math36-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, examinees will be placed into Category 1 (i.e., fail). When there are multiple cut scores, the classification process can be viewed as applying multiple binary cuts successively. Usually by adding more cuts, the chance of misclassification increases. However, the methodology to estimate CA is fundamentally the same.</p>
<p>Note that if the cut scores are given on the θ scale, then they will be mapped to the expected score scale through test characteristic curve (TCC) and resulting numbers will be treated as if they are on the observed sum score scale (<xref ref-type="bibr" rid="bibr10-0146621612471888">Lee, 2010</xref>). A cut score of <inline-formula id="inline-formula36-0146621612471888">
<mml:math display="inline" id="math37-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> on the θ scale can be transformed through TCC to a cut score of <inline-formula id="inline-formula37-0146621612471888">
<mml:math display="inline" id="math38-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> by</p>
<p><disp-formula id="disp-formula2-0146621612471888">
<mml:math display="block" id="math39-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>J</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>m</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>M</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mi>m</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>P</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>jm</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0146621612471888" xlink:href="10.1177_0146621612471888-eq2.tif"/>
</disp-formula></p>
<p>Using the cut score vector and the results from the recursive algorithm, the probability of scoring in category <italic>k</italic> is</p>
<p><disp-formula id="disp-formula3-0146621612471888">
<mml:math display="block" id="math40-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>Lee</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>k</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>x</mml:mi>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:munderover>
<mml:mtext>Pr</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-0146621612471888" xlink:href="10.1177_0146621612471888-eq3.tif"/>
</disp-formula></p>
<p>which is the sum of the probabilities of scoring a total score that falls within category <italic>k</italic>. Suppose a four dichotomous item test with a single pass/fail cut score at 3. The failing category would include total scores of 0, 1, and 2, and the passing category would include total scores of 3 and 4. The probability of failing the test is equal to the sum of <inline-formula id="inline-formula38-0146621612471888">
<mml:math display="inline" id="math41-0146621612471888">
<mml:mrow>
<mml:mtext>Pr</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">|</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> when <italic>x</italic> equals 0, 1, and 2, and the probability of passing the test when <italic>x</italic> equals 3 and 4.</p>
<p>Conditional CA, as described in <xref ref-type="bibr" rid="bibr10-0146621612471888">Lee (2010)</xref>, requires knowledge of the true category the examinees belong in, which is unrealistic in practice. Operationally, the conditional CA can be estimated by using <inline-formula id="inline-formula39-0146621612471888">
<mml:math display="inline" id="math42-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> as a proxy of the true θ. The <inline-formula id="inline-formula40-0146621612471888">
<mml:math display="inline" id="math43-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> is first transformed by <xref ref-type="disp-formula" rid="disp-formula2-0146621612471888">Equation 2</xref> to the expected true score, <inline-formula id="inline-formula41-0146621612471888">
<mml:math display="inline" id="math44-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>τ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>. This <inline-formula id="inline-formula42-0146621612471888">
<mml:math display="inline" id="math45-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>τ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is then treated as if it is on the observed sum score scale, and is compared against the cut scores to yield the “true” category the examinee belongs in. However, there is no exact transformation to move between the expected true score and observed score scales (see <xref ref-type="bibr" rid="bibr8-0146621612471888">Kolen &amp; Brennan, 2004</xref>; <xref ref-type="bibr" rid="bibr12-0146621612471888">Lord &amp; Wingersky, 1984</xref>). Determining an examinee’s true category by comparing his or her <inline-formula id="inline-formula43-0146621612471888">
<mml:math display="inline" id="math46-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>τ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> to the given observed scale cut scores may introduce some unknown error due to the imperfect mapping. Using sample estimated <inline-formula id="inline-formula44-0146621612471888">
<mml:math display="inline" id="math47-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>, the conditional CA estimate under the Lee approach can be given as</p>
<p><disp-formula id="disp-formula4-0146621612471888">
<mml:math display="block" id="math48-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>γ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>Lee</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>,</mml:mo>
<mml:mtext>Lee</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>,</mml:mo>
<mml:mtext>Lee</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>k</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>for</mml:mi>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mtext>τ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:msub>
<mml:mo>∈</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>ε</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-0146621612471888" xlink:href="10.1177_0146621612471888-eq4.tif"/>
</disp-formula></p>
<p>which can be understood as the probability of an examinee’s total score and his or her estimated expected true score based on <inline-formula id="inline-formula45-0146621612471888">
<mml:math display="inline" id="math49-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> falling into the same category.</p>
<p>Note that <inline-formula id="inline-formula46-0146621612471888">
<mml:math display="inline" id="math50-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>γ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>Lee</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is conditional on θ. To estimate the CA in the population, or to obtain the marginal CA, for each examinee in the sample <inline-formula id="inline-formula47-0146621612471888">
<mml:math display="inline" id="math51-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>γ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>Lee</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> can be computed and then averaged across the entire sample. Or, instead of using <inline-formula id="inline-formula48-0146621612471888">
<mml:math display="inline" id="math52-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> from a sample, it is possible to estimate CA from a distributional perspective. In this case, no <inline-formula id="inline-formula49-0146621612471888">
<mml:math display="inline" id="math53-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> is needed and therefore there is no need to use <inline-formula id="inline-formula50-0146621612471888">
<mml:math display="inline" id="math54-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> to represent the examinee’s true status. This is discussed further in the ‘Marginal Indices’ section.</p>
</sec>
<sec id="section2-0146621612471888">
<title>The Rudner Approach</title>
<p><xref ref-type="bibr" rid="bibr16-0146621612471888">Rudner’s (2001</xref>, <xref ref-type="bibr" rid="bibr17-0146621612471888">2005</xref>) expected CA quantifies misclassification based on normally distributed measurement error. It has been shown through simulation studies to be a good point estimate of CA, under the assumption of normally distributed measurement error (<xref ref-type="bibr" rid="bibr13-0146621612471888">Martineau, 2007</xref>). In contrast to the Lee approach, Rudner’s approach assumes that <inline-formula id="inline-formula51-0146621612471888">
<mml:math display="inline" id="math55-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> is used directly to make classification decisions.</p>
<p>Assuming that <inline-formula id="inline-formula52-0146621612471888">
<mml:math display="inline" id="math56-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> is obtained through maximum likelihood estimation, <inline-formula id="inline-formula53-0146621612471888">
<mml:math display="inline" id="math57-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> is known to be asymptotically normally distributed with mean θ and standard deviation <inline-formula id="inline-formula54-0146621612471888">
<mml:math display="inline" id="math58-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, that is, <inline-formula id="inline-formula55-0146621612471888">
<mml:math display="inline" id="math59-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula56-0146621612471888">
<mml:math display="inline" id="math60-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is usually given by the inverse of the square root of the test information at θ. The test information function changes for different IRT models, but if maximum likelihood scoring is used, then the assumption of normally distributed measurement error holds asymptotically.</p>
<p>The cut scores are aligned on the latent trait scale as <inline-formula id="inline-formula57-0146621612471888">
<mml:math display="inline" id="math61-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mo>&lt;</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mo>&lt;</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mo>…</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mo>&lt;</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>K</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> and they classify examinees into <italic>K</italic> groups. Add <inline-formula id="inline-formula58-0146621612471888">
<mml:math display="inline" id="math62-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>∞</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula59-0146621612471888">
<mml:math display="inline" id="math63-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>K</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>∞</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula>, the theoretical min and max of possible cut scores. An examinee is classified into the <italic>k</italic>th category when his or her <inline-formula id="inline-formula60-0146621612471888">
<mml:math display="inline" id="math64-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> is greater than or equal to <inline-formula id="inline-formula61-0146621612471888">
<mml:math display="inline" id="math65-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> and less than <inline-formula id="inline-formula62-0146621612471888">
<mml:math display="inline" id="math66-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>. In the two-category classification situation, an examinee with <inline-formula id="inline-formula63-0146621612471888">
<mml:math display="inline" id="math67-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> below <inline-formula id="inline-formula64-0146621612471888">
<mml:math display="inline" id="math68-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is placed in Category 1 (i.e., fail), otherwise Category 2 (i.e., pass). If the cut scores are given as total scores, then they must be mapped to the θ scale, again through the TCC.</p>
<p>Then, the probability of scoring in category <italic>k</italic> with the Rudner approach is calculated as</p>
<p><disp-formula id="disp-formula5-0146621612471888">
<mml:math display="block" id="math69-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>Rud</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>k</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mi>Φ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>Φ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-0146621612471888" xlink:href="10.1177_0146621612471888-eq5.tif"/>
</disp-formula></p>
<p>where <inline-formula id="inline-formula65-0146621612471888">
<mml:math display="inline" id="math70-0146621612471888">
<mml:mrow>
<mml:mi>Φ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>q</mml:mi>
<mml:mo>,</mml:mo>
<mml:mtext>μ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>σ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> is the cumulative distribution function (CDF) of normal density at <italic>q</italic> with mean μ and standard deviation σ. <xref ref-type="disp-formula" rid="disp-formula5-0146621612471888">Equation 5</xref> represents the area under the <inline-formula id="inline-formula66-0146621612471888">
<mml:math display="inline" id="math71-0146621612471888">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>σ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> density between <inline-formula id="inline-formula67-0146621612471888">
<mml:math display="inline" id="math72-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula68-0146621612471888">
<mml:math display="inline" id="math73-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>.</p>
<p>Conditional CA is the probability of being placed in the category that the examinee truly belongs to given θ:</p>
<p><disp-formula id="disp-formula6-0146621612471888">
<mml:math display="block" id="math74-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>,</mml:mo>
<mml:mi>Rud</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>,</mml:mo>
<mml:mi>Rud</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>k</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>for</mml:mi>
<mml:mspace width="0.25em"/>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo>∈</mml:mo>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>λ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula6-0146621612471888" xlink:href="10.1177_0146621612471888-eq6.tif"/>
</disp-formula></p>
<p>Note again that the Rudner approach depends on the normality assumption of <inline-formula id="inline-formula69-0146621612471888">
<mml:math display="inline" id="math75-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>. <xref ref-type="bibr" rid="bibr6-0146621612471888">Guo (2006)</xref> discussed a modification of Rudner’s approach that evaluates the area under the posterior of θ directly. <xref ref-type="bibr" rid="bibr2-0146621612471888">Chang and Stout (1993)</xref> showed that the posterior of θ is asymptotically normal too, so the same procedure can still apply with an appropriately adjusted mean and standard deviation of the normal distribution.</p>
</sec>
<sec id="section3-0146621612471888">
<title>Marginal Indices</title>
<p>So far the authors have presented only conditional CA indices, in other words the CA for a given examinee or a given θ level. The marginal index, which gives the CA for a given population or an observed sample of examinees, is often of more interest. There are two ways to marginalize the conditional indices that <xref ref-type="bibr" rid="bibr10-0146621612471888">Lee (2010)</xref> made explicit, as briefly mentioned at the end of the section on ‘the Lee approach.’ They are called the D- and P-methods, respectively. The two methods are equally applicable to both Lee and Rudner approaches.</p>
<p>With the D-method, or the distribution-based method, the marginal CA is found by integrating the conditional CA over the θ domain:</p>
<p><disp-formula id="disp-formula7-0146621612471888">
<mml:math display="block" id="math76-0146621612471888">
<mml:mrow>
<mml:mtext>γ</mml:mtext>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∫</mml:mo>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>∞</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>∞</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mtext>γ</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>θ</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mi>g</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mtext>dθ</mml:mtext>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula7-0146621612471888" xlink:href="10.1177_0146621612471888-eq7.tif"/>
</disp-formula></p>
<p>where <inline-formula id="inline-formula70-0146621612471888">
<mml:math display="inline" id="math77-0146621612471888">
<mml:mrow>
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>θ</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> is the density of θ. The integral in <xref ref-type="disp-formula" rid="disp-formula7-0146621612471888">Equation 7</xref> can be approximated by using quadrature points and weights of the ability distribution. The quadrature points and weights could be estimated from the sample (as in <xref ref-type="bibr" rid="bibr10-0146621612471888">Lee, 2010</xref>) or could come from a theoretical distribution (e.g., <inline-formula id="inline-formula71-0146621612471888">
<mml:math display="inline" id="math78-0146621612471888">
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>). The conditional indices are computed with the quadrature points in lieu of <inline-formula id="inline-formula72-0146621612471888">
<mml:math display="inline" id="math79-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> and then are weighted accordingly. Assuming a θ distribution, with the D-method there is no need to get <inline-formula id="inline-formula73-0146621612471888">
<mml:math display="inline" id="math80-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>s, as CA can be calculated with only the item parameters and the cut score vector. However, the results would be expected to hold only as far as the assumed distribution holds.</p>
<p>The P-method is person based, and simply averages the conditional indices computed for each examinee in the sample. <xref ref-type="bibr" rid="bibr10-0146621612471888">Lee (2010)</xref> found that the D- and P-methods yield very similar results in a real-data example when coupled with the Lee approach but they have not been investigated when coupled with the Rudner approach. Furthermore, neither coupling has been examined in simulation studies. This study therefore investigates not only the performance of the Lee and Rudner approaches but also their performance when different marginalization method is used along with them.</p>
</sec>
<sec id="section4-0146621612471888">
<title>True Accuracy</title>
<p>Suppose an infinite number of test takers whose latent trait is θ take the test. Their response patterns may well differ and so would their latent trait estimates and total scores. The proportion of these test takers being correctly classified is the true conditional CA at θ. Integrating the true conditional CA over the true underlying θ distribution will lead to the true marginal CA rate, as in <xref ref-type="disp-formula" rid="disp-formula7-0146621612471888">Equation 7</xref>.</p>
<p>In reality, however, the true CA is never known because first, the true latent trait and its distribution are unknown and, second, the assumption of an infinite number of test takers (or equivalently, the same test taker taking the test an infinite number of times) can never be realized. But in simulation studies, the true θ can be controlled. A large, representative sample can be used to address these concerns. The resulting CA is called “empirical CA.” For the rest of the article, the empirical CA is used as a proxy of the true CA and we will not distinguish between them. The empirical CA serves as the yardstick against which the CA estimates from both the Rudner and Lee approaches are compared.</p>
<p>The two approaches, Lee versus Rudner, imply the use of different scores for classification, either the total score or the model-based ability estimate. Determining which leads to higher true CA, using total score or the latent trait estimate, is the first goal of the simulation study. Second, the estimates are compared from Lee and Rudner approaches to their respective true CAs in various conditions, such as different test lengths, sample sizes, cut score locations, and IRT models.</p>
</sec>
<sec id="section5-0146621612471888" sec-type="methods">
<title>Method</title>
<p>Tests of 10 to 80 items, in increments of 10, were constructed following either the 1PL, 2PL, or 3PL model with simulated item parameters. For all dichotomous models, the difficulty parameter was drawn from <inline-formula id="inline-formula74-0146621612471888">
<mml:math display="inline" id="math81-0146621612471888">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. To see the effect of the distribution of discrimination parameters on the outcomes with the 2PL and 3PL data, discrimination parameters were drawn from two different distributions. In the first condition, called the “narrow” condition, discrimination parameters were drawn from in <inline-formula id="inline-formula75-0146621612471888">
<mml:math display="inline" id="math82-0146621612471888">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>3</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. In the second condition, called the “wide” condition, discrimination parameters were drawn from ln <inline-formula id="inline-formula76-0146621612471888">
<mml:math display="inline" id="math83-0146621612471888">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>5</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. The “wide” condition represents further departure than the “narrow” condition from the 1PL. In both cases, discrimination parameters were truncated at 4. For the 3PL data, the guessing parameter was drawn from <inline-formula id="inline-formula77-0146621612471888">
<mml:math display="inline" id="math84-0146621612471888">
<mml:mrow>
<mml:mi>U</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>25</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>.</p>
<p>Test of 10, 20, 40, and 80 items were also constructed following a GRM with five ordered response categories (<xref ref-type="bibr" rid="bibr18-0146621612471888">Samejima, 1969</xref>). The discrimination parameter was drawn from ln <inline-formula id="inline-formula78-0146621612471888">
<mml:math display="inline" id="math85-0146621612471888">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>3</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. The first threshold was drawn from <inline-formula id="inline-formula79-0146621612471888">
<mml:math display="inline" id="math86-0146621612471888">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>5</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>, and threshold parameters 2 through 4 added a random variate from <inline-formula id="inline-formula80-0146621612471888">
<mml:math display="inline" id="math87-0146621612471888">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> to the previous threshold. Note that the item parameter generation schemes were motivated by <xref ref-type="bibr" rid="bibr14-0146621612471888">Orlando and Thissen (2000)</xref> for dichotomous models and <xref ref-type="bibr" rid="bibr22-0146621612471888">Woods (2007)</xref> for the GRM.</p>
<p>Examinees had abilities drawn from a standard normal distribution. Sample size was manipulated at 250, 500, and 1,000 when the indices were calculated with the P-method. For the D-method, 40 quadrature points and weights from <inline-formula id="inline-formula81-0146621612471888">
<mml:math display="inline" id="math88-0146621612471888">
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>~</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> were used. Note that this is the true ability distribution, and the latent trait distribution was not estimated from data.</p>
<p>The ability estimate <inline-formula id="inline-formula82-0146621612471888">
<mml:math display="inline" id="math89-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> was estimated with maximum likelihood and recorded. Any perfect scores, that is, incorrect or correct responses on all items, were given an arbitrarily large <inline-formula id="inline-formula83-0146621612471888">
<mml:math display="inline" id="math90-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> of −4 or 4, respectively.<sup><xref ref-type="fn" rid="fn1-0146621612471888">1</xref></sup> The <italic>SE</italic> of <inline-formula id="inline-formula84-0146621612471888">
<mml:math display="inline" id="math91-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> was the inverse of the square root of the test information at <inline-formula id="inline-formula85-0146621612471888">
<mml:math display="inline" id="math92-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>. The unweighted total score <italic>x</italic> was also recorded.</p>
<p>Then, CA estimates based on the Lee and Rudner approaches were computed with both the D- and P-methods with the true item parameters and the correct IRT model. A single cut score ranging from −2.0 to 2.0 on the θ scale, with an increment of .25, was given to divide the sample into two categories representing, for example, classifying examinees as pass/fail. By applying binary classifications in the simulation study, the authors are able to see the effect of the cut score’s location on the estimation of CA. Applying multiple cut scores simultaneously would confound this effect with the cut scores’ relative locations to each other. Because of this, only binary classifications were examined in this study. The appropriate cut score vectors were created for both approaches. The cut scores for the Lee approach were transformed from the θ scale by <xref ref-type="disp-formula" rid="disp-formula2-0146621612471888">Equation 2</xref> using the true item parameters.</p>
<p>Two empirical accuracies were calculated for each condition (i.e., test length × IRT model × cut score location), one representing making a classification based on <inline-formula id="inline-formula86-0146621612471888">
<mml:math display="inline" id="math93-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>, and the other based on the observed total score <italic>X</italic>. They serve as baselines for evaluating estimates of CA based on the Rudner approach and the Lee approach, respectively. Empirical accuracy was calculated as the proportion of examinees whose <inline-formula id="inline-formula87-0146621612471888">
<mml:math display="inline" id="math94-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> or total score <italic>x</italic> fell in the same category as their θ. With the D-method, the estimated indices were compared with empirical accuracies calculated from the 1,000 subject sample size.</p>
<p>Note that the two baselines may differ. Therefore, estimates from the Rudner approach and the Lee approach need to be evaluated against their respective baselines: The Rudner estimates should be compared against the <inline-formula id="inline-formula88-0146621612471888">
<mml:math display="inline" id="math95-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>-based empirical accuracy, whereas the Lee estimates should be compared against the <italic>X</italic>-based empirical accuracy.</p>
<p>CA estimates from the Lee and Rudner approaches were computed with the R (<xref ref-type="bibr" rid="bibr15-0146621612471888">R Development Core Team, 2011</xref>) package cacIRT (<xref ref-type="bibr" rid="bibr9-0146621612471888">Lathrop, 2011</xref>), freely available on CRAN. cacIRT can provide estimates using both approaches and also gives access to extendable code for researchers.</p>
<p>To summarize, factors manipulated in this study include IRT models and parameters (1PL, 2PL-narrow, 2PL-wide, 3PL-narrow, 3PL-wide, and GRM), test lengths (10-80 items by 10 for dichotomous models; 10, 20, 40, and 80 items for GRM), sample size (250, 500, 1,000, and the distribution-based D-method), and cut score locations (from −2 to 2 in increments of .25). Each condition was replicated 500 times. The empirical CA (γ) reported was averaged across the 500 replications of each condition. Mean bias of the CA estimates from Lee and Rudner approaches was calculated over the 500 replications. The <italic>SE</italic> of the indices was calculated as the standard deviation over the 500 replications.</p>
</sec>
<sec id="section6-0146621612471888" sec-type="results">
<title>Results</title>
<p>Within a given model condition and sample size, the empirical results can be visualized as a hypersurface in three-dimensional (3D) space. <xref ref-type="fig" rid="fig1-0146621612471888">Figure 1</xref> shows how empirical accuracy (computed from samples of 500 subjects) changes with test length and cut score location. Because interpreting 3D graphs on paper is difficult, the following results are presented by taking 2D slices of this 3D surface. <xref ref-type="fig" rid="fig2-0146621612471888">Figure 2</xref> shows typical slices that will be discussed later. The left panel of <xref ref-type="fig" rid="fig2-0146621612471888">Figure 2</xref> shows a slice of test length = 20. This slice illustrates the effect of cut score location on true CA. The right panel of <xref ref-type="fig" rid="fig2-0146621612471888">Figure 2</xref> is a slice at cut score of 0 (on the θ scale), which shows the effect of test length on true CA.</p>
<fig id="fig1-0146621612471888" position="float">
<label>Figure 1.</label>
<caption>
<p>Empirical classification accuracy surface by cut score location and test length.</p>
<p>Note: 1PL = one-parameter logistic.</p>
</caption>
<graphic xlink:href="10.1177_0146621612471888-fig1.tif"/>
</fig>
<fig id="fig2-0146621612471888" position="float">
<label>Figure 2.</label>
<caption>
<p>Empirical accuracy: 1PL.</p>
<p>Note: 1PL = one-parameter logistic.</p>
</caption>
<graphic xlink:href="10.1177_0146621612471888-fig2.tif"/>
</fig>
<p>In <xref ref-type="fig" rid="fig2-0146621612471888">Figure 2</xref>, data are generated from 1PL so the total score is a sufficient statistic for θ. Consequently, the true CA of using <inline-formula id="inline-formula89-0146621612471888">
<mml:math display="inline" id="math96-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> or <italic>x</italic> was identical. The CA was the lowest when the cut score occurs at the densest location in the ability distribution, when <inline-formula id="inline-formula90-0146621612471888">
<mml:math display="inline" id="math97-0146621612471888">
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula>. As the cut score became more extreme, CA increased. This confirms that CA should only be discussed conditioning on cut score locations (see <xref ref-type="bibr" rid="bibr11-0146621612471888">Lee et al., 2002</xref>; <xref ref-type="bibr" rid="bibr23-0146621612471888">Wyse, 2011</xref>). Moreover, as expected, when the test length increased, information about the examinees also increased and classifications became more accurate. While the true CA was monotone increasing in test length, the rate of increase slowed down as tests got longer and longer and as CA approached its upper bound of 1.</p>
<p>The cut score location affects marginal CA through two factors. First, when cut scores are located where the conditional standard error of measurement is large (i.e., where the test information is low), the conditional CA would be low. However, given a uniform measurement error, a larger ability density at the cut score location leads to lower marginal CA. Most tests, as the ones constructed in this simulation, have higher information where the ability distribution is the densest. For cut scores at the extremes of the ability distribution, conditional CA will be relatively lower due to higher conditional <italic>SE</italic>, but marginal CA will still be high due to low density at those locations. Similarly, marginal CA from a cut score at the center of the ability distribution will benefit from lower conditional <italic>SE</italic> but will still suffer from the high density near the cut score. The effect of cut score location on marginal CA is an overlay of the effects of both factors.</p>
<p><xref ref-type="table" rid="table1-0146621612471888">Table 1</xref> shows the performance of the CA estimates from the Lee and Rudner approaches for the 1PL data at three cut scores, θ = −1.5, 0, 1.5 and is organized by test length and sample size. In general, the biases and <italic>SE</italic>s were small. They were the largest when the test was short and the cut score was located at <inline-formula id="inline-formula91-0146621612471888">
<mml:math display="inline" id="math98-0146621612471888">
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> and both decreased as the test became longer or the cut score became more extreme. Mean bias was not influenced by sample size. The <italic>SE</italic>, however, did decrease with increased sample size for all test lengths. This means the variability of the CA estimate from sample to sample decreased for larger samples. The D-method performed very well, especially when coupled with the Lee approach where it outperformed the P-method in shorter tests.</p>
<table-wrap id="table1-0146621612471888" position="float">
<label>Table 1.</label>
<caption>
<p>Bias (and <italic>SE</italic>) of Accuracy Indices by Cut Score Location: 1PL.</p>
</caption>
<graphic alternate-form-of="table1-0146621612471888" xlink:href="10.1177_0146621612471888-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" rowspan="2">Sample size</th>
<th align="center" colspan="3">Rudner<hr/></th>
<th align="center" colspan="3">Lee<hr/></th>
</tr>
<tr>
<th align="left">No. of items</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>250</td>
<td>−.005 (.022)</td>
<td>−.008 (.024)</td>
<td>−.002 (.022)</td>
<td>.010 (.019)</td>
<td>.020 (.021)</td>
<td>.012 (.019)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>−.006 (.019)</td>
<td>−.008 (.018)</td>
<td>−.003 (.019)</td>
<td>.009 (.014)</td>
<td>.020 (.014)</td>
<td>.010 (.015)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>−.005 (.018)</td>
<td>−.007 (.016)</td>
<td>−.004 (.017)</td>
<td>.009 (.012)</td>
<td>.021 (.011)</td>
<td>.010 (.012)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.006 (.016)</td>
<td>−.008 (.011)</td>
<td>.007 (.016)</td>
<td>.000 (.008)</td>
<td>.001 (.010)</td>
<td>.001 (.008)</td>
</tr>
<tr>
<td>20</td>
<td>250</td>
<td>.000 (.013)</td>
<td>.001 (.018)</td>
<td>.001 (.013)</td>
<td>.002 (.013)</td>
<td>.008 (.018)</td>
<td>.003 (.012)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.000 (.010)</td>
<td>.002 (.012)</td>
<td>.000 (.010)</td>
<td>.002 (.009)</td>
<td>.009 (.012)</td>
<td>.002 (.009)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.008)</td>
<td>.001 (.009)</td>
<td>.000 (.008)</td>
<td>.002 (.007)</td>
<td>.008 (.008)</td>
<td>.003 (.006)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.003 (.009)</td>
<td>−.002 (.010)</td>
<td>.003 (.008)</td>
<td>.000 (.007)</td>
<td>.001 (.009)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td>40</td>
<td>250</td>
<td>.000 (.010)</td>
<td>.001 (.015)</td>
<td>.000 (.010)</td>
<td>.000 (.010)</td>
<td>.003 (.015)</td>
<td>.000 (.010)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.000 (.007)</td>
<td>.001 (.010)</td>
<td>.001 (.008)</td>
<td>.000 (.007)</td>
<td>.002 (.010)</td>
<td>.001 (.008)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.005)</td>
<td>.001 (.007)</td>
<td>.000 (.005)</td>
<td>.001 (.005)</td>
<td>.003 (.007)</td>
<td>.001 (.005)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.002 (.006)</td>
<td>.001 (.008)</td>
<td>.001 (.006)</td>
<td>.001 (.006)</td>
<td>.002 (.008)</td>
<td>.001 (.006)</td>
</tr>
<tr>
<td>80</td>
<td>250</td>
<td>.001 (.008)</td>
<td>.001 (.012)</td>
<td>.000 (.008)</td>
<td>.001 (.008)</td>
<td>.002 (.012)</td>
<td>.000 (.008)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.000 (.006)</td>
<td>.000 (.009)</td>
<td>.000 (.006)</td>
<td>.000 (.006)</td>
<td>.001 (.008)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.004)</td>
<td>.000 (.006)</td>
<td>.000 (.004)</td>
<td>.000 (.004)</td>
<td>.001 (.006)</td>
<td>.000 (.004)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.000 (.005)</td>
<td>.004 (.007)</td>
<td>.000 (.005)</td>
<td>.000 (.005)</td>
<td>.004 (.007)</td>
<td>.000 (.005)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0146621612471888">
<p>Note: <italic>SE</italic> = standard error; 1PL = one-parameter logistic.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Unlike the 1PL data, with 2PL data there will be a difference in true CA between using <italic>x</italic> or <inline-formula id="inline-formula92-0146621612471888">
<mml:math display="inline" id="math99-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> to make classifications. Further departure of the underlying model from the 1PL model will lead to bigger discrepancy between the two true CAs. Under 2PL, the discrepancy is governed by the variability in the discrimination parameters. As the discriminations vary more across items, bigger difference is expected between two true CAs. The 2PL results are presented in <xref ref-type="fig" rid="fig3-0146621612471888">Figure 3</xref>, which shows the true CA of using <italic>x</italic> or <inline-formula id="inline-formula93-0146621612471888">
<mml:math display="inline" id="math100-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> in the 2PL-narrow condition (the thinner curves) and 2PL-wide condition (the thicker curves). Overall, classifications made on the basis of <inline-formula id="inline-formula94-0146621612471888">
<mml:math display="inline" id="math101-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> were more accurate than classifications made on the basis of <italic>x</italic>. This difference diminished as the cut score became more extreme but remained fairly constant as the test length increased. The thicker curves enclosed the two thinner curves, showing that more widely varying discrimination parameters lead to larger difference between the CAs based on <italic>x</italic> and <inline-formula id="inline-formula95-0146621612471888">
<mml:math display="inline" id="math102-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>. As the discrimination parameters vary more between items, the superiority of using <inline-formula id="inline-formula96-0146621612471888">
<mml:math display="inline" id="math103-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> over <italic>x</italic> is more pronounced.</p>
<fig id="fig3-0146621612471888" position="float">
<label>Figure 3.</label>
<caption>
<p>Empirical accuracy: 2PL-wide versus 2PL-narrow.</p>
<p>Note: 2PL = two-parameter logistic.</p>
</caption>
<graphic xlink:href="10.1177_0146621612471888-fig3.tif"/>
</fig>
<p>The bias and <italic>SE</italic> of CA estimates from the Lee and Rudner approaches in the 2PL-narrow and 2PL-wide data were nearly identical and do not warrant presenting both results. So here only the bias and <italic>SE</italic>s of the indices under the 2PL-wide condition are presented (see <xref ref-type="table" rid="table2-0146621612471888">Table 2</xref>). It is important to note that with 2PL data the Lee and Rudner approaches are not estimating the same baseline, so comparisons across approaches within a condition are not of direct interest. That said, the performance of each index was good, with generally small bias and <italic>SE</italic> that further diminished with increasing test length and more extreme cut score locations. <italic>SE</italic> again decreased with sample size. The D-method again performed very well as well, especially when coupled with the Lee approach.</p>
<table-wrap id="table2-0146621612471888" position="float">
<label>Table 2.</label>
<caption>
<p>Bias (and <italic>SE</italic>) of Accuracy Indices by Cut Score Location: 2PL.</p>
</caption>
<graphic alternate-form-of="table2-0146621612471888" xlink:href="10.1177_0146621612471888-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" rowspan="2">Sample size</th>
<th align="center" colspan="3">Rudner<hr/></th>
<th align="center" colspan="3">Lee<hr/></th>
</tr>
<tr>
<th align="left">No. of items</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>250</td>
<td>.001 (.020)</td>
<td>−.007 (.025)</td>
<td>.003 (.021)</td>
<td>.005 (.017)</td>
<td>.013 (.020)</td>
<td>.006 (.019)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.002 (.018)</td>
<td>−.008 (.020)</td>
<td>.002 (.019)</td>
<td>.004 (.014)</td>
<td>.012 (.014)</td>
<td>.005 (.015)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.002 (.016)</td>
<td>−.008 (.018)</td>
<td>.003 (.017)</td>
<td>.005 (.012)</td>
<td>.013 (.011)</td>
<td>.006 (.013)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.009 (.015)</td>
<td>−.010 (.015)</td>
<td>.010 (.016)</td>
<td>.001 (.008)</td>
<td>.000 (.011)</td>
<td>.001 (.009)</td>
</tr>
<tr>
<td>20</td>
<td>250</td>
<td>.002 (.012)</td>
<td>.000 (.017)</td>
<td>.002 (.012)</td>
<td>.001 (.012)</td>
<td>.004 (.017)</td>
<td>.001 (.013)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.002 (.009)</td>
<td>.001 (.012)</td>
<td>.002 (.009)</td>
<td>.001 (.009)</td>
<td>.004 (.011)</td>
<td>.001 (.009)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.002 (.007)</td>
<td>.001 (.010)</td>
<td>.002 (.007)</td>
<td>.001 (.006)</td>
<td>.005 (.009)</td>
<td>.001 (.006)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.003 (.008)</td>
<td>−.001 (.010)</td>
<td>.004 (.008)</td>
<td>.000 (.007)</td>
<td>.001 (.010)</td>
<td>.001 (.007)</td>
</tr>
<tr>
<td>40</td>
<td>250</td>
<td>.001 (.009)</td>
<td>.000 (.013)</td>
<td>.000 (.010)</td>
<td>.000 (.010)</td>
<td>.002 (.014)</td>
<td>.000 (.010)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.001 (.007)</td>
<td>.000 (.010)</td>
<td>.000 (.007)</td>
<td>.000 (.007)</td>
<td>.002 (.010)</td>
<td>.000 (.007)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.001 (.005)</td>
<td>.001 (.007)</td>
<td>.001 (.005)</td>
<td>.000 (.005)</td>
<td>.002 (.007)</td>
<td>.000 (.005)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.001 (.006)</td>
<td>.002 (.008)</td>
<td>.001 (.006)</td>
<td>.000 (.006)</td>
<td>.002 (.008)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td>80</td>
<td>250</td>
<td>.001 (.008)</td>
<td>.000 (.011)</td>
<td>.001 (.008)</td>
<td>.000 (.008)</td>
<td>.001 (.012)</td>
<td>.000 (.008)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.000 (.005)</td>
<td>.000 (.008)</td>
<td>.000 (.006)</td>
<td>.000 (.005)</td>
<td>.001 (.008)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.004)</td>
<td>.000 (.006)</td>
<td>.000 (.004)</td>
<td>.000 (.004)</td>
<td>.001 (.006)</td>
<td>.000 (.004)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.000 (.004)</td>
<td>.004 (.007)</td>
<td>.000 (.005)</td>
<td>.000 (.005)</td>
<td>.004 (.007)</td>
<td>.000 (.005)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0146621612471888">
<p>Note: <italic>SE</italic> = standard error; 2PL = two-parameter logistic.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>For the 3PL data, <xref ref-type="fig" rid="fig4-0146621612471888">Figure 4</xref> shows how empirical CA changed with cut score location and test length. As with the other models, the 3PL data had the lowest CA when the cut score was made at <inline-formula id="inline-formula97-0146621612471888">
<mml:math display="inline" id="math104-0146621612471888">
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> and test length was short. Notably for the 3PL test, when the cut score on the θ scale was larger than 1, the CA of <italic>x</italic> and <inline-formula id="inline-formula98-0146621612471888">
<mml:math display="inline" id="math105-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> was very comparable. But at other cut scores, there was a clear superiority of using <inline-formula id="inline-formula99-0146621612471888">
<mml:math display="inline" id="math106-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> to make classification. This nonsymmetry was likely due to the presence of the <italic>c</italic> parameter. When the cut score was at the lower end and classifications made on the basis of <italic>x</italic>, an examinee could be classified into the higher category solely due to making correct guesses on items with larger <italic>c</italic> parameters. But the <inline-formula id="inline-formula100-0146621612471888">
<mml:math display="inline" id="math107-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> estimated from the 3PL has taken into account the <italic>c</italic> parameter automatically. Also notice that the CA from the 3PL data was lower than that of the 2PL data. This was expected as the data generated from the 3PL had lower test information due to the addition of the <italic>c</italic> parameter.</p>
<fig id="fig4-0146621612471888" position="float">
<label>Figure 4.</label>
<caption>
<p>Empirical accuracy: 3PL-wide versus 3PL-narrow.</p>
<p>Note: 3PL = three-parameter logistic.</p>
</caption>
<graphic xlink:href="10.1177_0146621612471888-fig4.tif"/>
</fig>
<p>Comparing the narrow and wide discrimination conditions, the effect of the variability of the discrimination parameters on CA estimates for the 3PL data had similar pattern as it did for the 2PL data but the effect size was smaller. The 3PL-wide showed a larger difference between <inline-formula id="inline-formula101-0146621612471888">
<mml:math display="inline" id="math108-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> or <italic>x</italic> for classifications than the 3PL-narrow. It is notable that the 3PL-narrow data showed a larger difference between using <inline-formula id="inline-formula102-0146621612471888">
<mml:math display="inline" id="math109-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> or <italic>x</italic> for classification than the 2PL-narrow data, and this is because 3PL diverges further from 1PL more than 2PL does. So when the probability of correctly answering items on a test has a nonzero lower asymptote, as often would be the case in multiple-choice tests, it is beneficial to use <inline-formula id="inline-formula103-0146621612471888">
<mml:math display="inline" id="math110-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> for classifications, regardless of the spread of discrimination parameters. This benefit would diminish when the cut score is above 1 (on the θ scale) as discussed.</p>
<p>As with the 2PL data, the performance of the indices in the 3PL-narrow and 3PL-wide conditions was so numerically similar that only the results for the 3PL-wide are displayed in <xref ref-type="table" rid="table3-0146621612471888">Table 3</xref>. For the Lee approach, the patterns and magnitudes of bias and <italic>SE</italic> are very similar to those with the 1PL and 2PL data. For the Rudner approach, the bias and <italic>SE</italic> under the 3PL are noticeably larger. The biases, while decreasing with test length, remained relatively large for all sample sizes with both the P-method and the D-method, particularly for cut scores at <inline-formula id="inline-formula104-0146621612471888">
<mml:math display="inline" id="math111-0146621612471888">
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula>. Although larger, the absolute magnitude of bias with the Rudner approach with 3PL data could still probably be safely ignored for many applied situations.</p>
<table-wrap id="table3-0146621612471888" position="float">
<label>Table 3.</label>
<caption>
<p>Bias (and <italic>SE</italic>) of Accuracy Indices by Cut Score Location: 3PL.</p>
</caption>
<graphic alternate-form-of="table3-0146621612471888" xlink:href="10.1177_0146621612471888-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" rowspan="2">Sample size</th>
<th align="center" colspan="3">Rudner<hr/></th>
<th align="center" colspan="3">Lee<hr/></th>
</tr>
<tr>
<th align="left">No. of items</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>250</td>
<td>−.010 (.023)</td>
<td>−.033 (.033)</td>
<td>−.011 (.027)</td>
<td>.005 (.018)</td>
<td>.018 (.022)</td>
<td>.006 (.021)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>−.009 (.022)</td>
<td>−.034 (.027)</td>
<td>−.014 (.025)</td>
<td>.005 (.015)</td>
<td>.018 (.017)</td>
<td>.004 (.017)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>−.009 (.019)</td>
<td>−.033 (.027)</td>
<td>−.012 (.024)</td>
<td>.004 (.011)</td>
<td>.018 (.013)</td>
<td>.005 (.016)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.002 (.018)</td>
<td>−.033 (.022)</td>
<td>.005 (.019)</td>
<td>.001 (.009)</td>
<td>.000 (.012)</td>
<td>.001 (.010)</td>
</tr>
<tr>
<td>20</td>
<td>250</td>
<td>−.003 (.015)</td>
<td>−.016 (.020)</td>
<td>−.006 (.013)</td>
<td>.002 (.013)</td>
<td>.007 (.019)</td>
<td>.000 (.014)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>−.004 (.011)</td>
<td>−.017 (.016)</td>
<td>−.006 (.011)</td>
<td>.003 (.010)</td>
<td>.007 (.013)</td>
<td>.000 (.010)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>−.004 (.009)</td>
<td>−.017 (.013)</td>
<td>−.006 (.009)</td>
<td>.002 (.008)</td>
<td>.006 (.009)</td>
<td>.000 (.007)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>−.002 (.010)</td>
<td>−.017 (.013)</td>
<td>.000 (.009)</td>
<td>.000 (.008)</td>
<td>.001 (.010)</td>
<td>.001 (.008)</td>
</tr>
<tr>
<td>40</td>
<td>250</td>
<td>−.004 (.011)</td>
<td>−.009 (.014)</td>
<td>−.004 (.011)</td>
<td>.000 (.011)</td>
<td>.003 (.015)</td>
<td>.000 (.010)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>−.003 (.008)</td>
<td>−.011 (.011)</td>
<td>−.004 (.007)</td>
<td>.001 (.008)</td>
<td>.002 (.012)</td>
<td>−.001 (.007)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>−.004 (.006)</td>
<td>−.009 (.009)</td>
<td>−.003 (.005)</td>
<td>.001 (.006)</td>
<td>.002 (.008)</td>
<td>.000 (.005)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>−.003 (.006)</td>
<td>−.009 (.009)</td>
<td>−.003 (.006)</td>
<td>.000 (.007)</td>
<td>.002 (.009)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td>80</td>
<td>250</td>
<td>−.003 (.009)</td>
<td>−.006 (.012)</td>
<td>−.002 (.008)</td>
<td>.000 (.009)</td>
<td>.001 (.013)</td>
<td>.001 (.009)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>−.003 (.007)</td>
<td>−.006 (.009)</td>
<td>−.003 (.006)</td>
<td>.000 (.006)</td>
<td>.001 (.009)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>−.003 (.005)</td>
<td>−.006 (.006)</td>
<td>−.003 (.004)</td>
<td>.001 (.005)</td>
<td>.001 (.006)</td>
<td>.000 (.004)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>−.003 (.005)</td>
<td>−.004 (.007)</td>
<td>−.002 (.005)</td>
<td>.000 (.005)</td>
<td>.003 (.008)</td>
<td>.000 (.005)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0146621612471888">
<p>Note: <italic>SE</italic> = standard error; 3PL = three-parameter logistic.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The results for the GRM data appear in <xref ref-type="fig" rid="fig5-0146621612471888">Figure 5</xref> and <xref ref-type="table" rid="table4-0146621612471888">Table 4</xref>. <xref ref-type="fig" rid="fig5-0146621612471888">Figure 5</xref> shows the true accuracies of using <inline-formula id="inline-formula105-0146621612471888">
<mml:math display="inline" id="math112-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> or <italic>x</italic> for classification by test length and by cut score location. Notably, the true accuracies for a test that follows a five-category GRM are much higher than a dichotomously scored test of similar length. For example, a CA of .94 with a cut score of <inline-formula id="inline-formula106-0146621612471888">
<mml:math display="inline" id="math113-0146621612471888">
<mml:mrow>
<mml:mtext>θ</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> is achieved at about 60, 50, and 70 items for data following the 1PL, 2PL, and 3PL models, respectively. The CA estimate from the GRM data well exceeds .94 by test length of 40. This reflects the higher information in polytomously scored tests. The performance of both the Rudner and Lee approaches, seen in <xref ref-type="table" rid="table4-0146621612471888">Table 4</xref>, is very good, showing extremely small biases across all sample sizes, test lengths, and cut score locations.</p>
<fig id="fig5-0146621612471888" position="float">
<label>Figure 5.</label>
<caption>
<p>Empirical accuracy: GRM.</p>
<p>Note: GRM = graded response model.</p>
</caption>
<graphic xlink:href="10.1177_0146621612471888-fig5.tif"/>
</fig>
<table-wrap id="table4-0146621612471888" position="float">
<label>Table 4.</label>
<caption>
<p>Bias (and <italic>SE</italic>) of Accuracy Indices by Cut Score Location: GRM.</p>
</caption>
<graphic alternate-form-of="table4-0146621612471888" xlink:href="10.1177_0146621612471888-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" rowspan="2">Sample Size</th>
<th align="center" colspan="3">Rudner<hr/></th>
<th align="center" colspan="3">Lee<hr/></th>
</tr>
<tr>
<th align="left">No. of items</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
<th align="center">−1.5</th>
<th align="center">0</th>
<th align="center">1.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>250</td>
<td>−.001 (.011)</td>
<td>.003 (.017)</td>
<td>−.001 (.010)</td>
<td>.000 (.012)</td>
<td>−.002 (.018)</td>
<td>.001 (.011)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.000 (.008)</td>
<td>.004 (.012)</td>
<td>−.001 (.007)</td>
<td>.001 (.008)</td>
<td>.000 (.012)</td>
<td>.000 (.008)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.006)</td>
<td>.004 (.009)</td>
<td>−.001 (.005)</td>
<td>.000 (.006)</td>
<td>.000 (.009)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.001 (.007)</td>
<td>.002 (.010)</td>
<td>.000 (.006)</td>
<td>.001 (.007)</td>
<td>.002 (.010)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td>20</td>
<td>250</td>
<td>.000 (.009)</td>
<td>.001 (.014)</td>
<td>.000 (.008)</td>
<td>.000 (.010)</td>
<td>−.001 (.015)</td>
<td>.000 (.009)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.000 (.007)</td>
<td>.002 (.010)</td>
<td>−.001 (.006)</td>
<td>.000 (.007)</td>
<td>.001 (.011)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.005)</td>
<td>.002 (.007)</td>
<td>.000 (.004)</td>
<td>.000 (.005)</td>
<td>.000 (.008)</td>
<td>.000 (.004)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.000 (.005)</td>
<td>.003 (.008)</td>
<td>.000 (.005)</td>
<td>.000 (.006)</td>
<td>.002 (.009)</td>
<td>.000 (.005)</td>
</tr>
<tr>
<td>40</td>
<td>250</td>
<td>.000 (.008)</td>
<td>.001 (.012)</td>
<td>−.001 (.007)</td>
<td>.000 (.008)</td>
<td>.001 (.012)</td>
<td>.000 (.007)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.001 (.006)</td>
<td>.001 (.009)</td>
<td>.000 (.005)</td>
<td>.000 (.006)</td>
<td>.000 (.009)</td>
<td>.000 (.005)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.004)</td>
<td>.001 (.006)</td>
<td>.000 (.003)</td>
<td>.000 (.004)</td>
<td>.000 (.006)</td>
<td>.000 (.004)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.001 (.005)</td>
<td>.003 (.007)</td>
<td>.000 (.004)</td>
<td>.001 (.005)</td>
<td>.003 (.007)</td>
<td>.000 (.005)</td>
</tr>
<tr>
<td>80</td>
<td>250</td>
<td>.000 (.006)</td>
<td>.001 (.010)</td>
<td>.000 (.006)</td>
<td>.000 (.007)</td>
<td>.000 (.011)</td>
<td>.000 (.006)</td>
</tr>
<tr>
<td/>
<td>500</td>
<td>.000 (.005)</td>
<td>.000 (.007)</td>
<td>.000 (.004)</td>
<td>.000 (.005)</td>
<td>.000 (.008)</td>
<td>.000 (.004)</td>
</tr>
<tr>
<td/>
<td>1,000</td>
<td>.000 (.003)</td>
<td>.000 (.005)</td>
<td>.000 (.003)</td>
<td>.000 (.003)</td>
<td>.000 (.006)</td>
<td>.000 (.003)</td>
</tr>
<tr>
<td/>
<td>D</td>
<td>.000 (.004)</td>
<td>.006 (.006)</td>
<td>.000 (.003)</td>
<td>.000 (.004)</td>
<td>.004 (.006)</td>
<td>.000 (.004)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0146621612471888">
<p>Note: <italic>SE</italic> = standard error; GRM = graded response model.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>In general, both the Lee and Rudner approaches provided good estimates of CA for every condition that was simulated. For most practical applications, such small bias would be negligible.</p>
</sec>
<sec id="section7-0146621612471888" sec-type="discussion">
<title>Discussion</title>
<p>This article investigated through simulation studies the difference in empirical accuracy between using latent trait estimate <inline-formula id="inline-formula107-0146621612471888">
<mml:math display="inline" id="math114-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> and using the total score <italic>x</italic> to make classification decisions. It is shown that when data follow the GRM, 2PL, or 3PL model, it is preferable to base one’s decision on <inline-formula id="inline-formula108-0146621612471888">
<mml:math display="inline" id="math115-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> instead of <italic>x</italic>. In many situations, however, the difference is very small, especially when the cut is far from the dense part of the latent trait distribution or when the discrimination parameters do not vary much across items. If the test is scaled using the 1PL model, <italic>x</italic> is a sufficient statistic of <inline-formula id="inline-formula109-0146621612471888">
<mml:math display="inline" id="math116-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> and using either makes no difference. So in many situations, choosing <inline-formula id="inline-formula110-0146621612471888">
<mml:math display="inline" id="math117-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> or <italic>x</italic> would result in small, if any, practical difference. Moreover, the score scale on the basis of which classification decisions are made is often predetermined, and is sometimes chosen for concerns that are not entirely psychometric. The location of the cut scores is also rarely flexible because the goal of the classification and the target population of examinees largely determine it.</p>
<p>Given that the cut scores and the choice of <inline-formula id="inline-formula111-0146621612471888">
<mml:math display="inline" id="math118-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> or <italic>x</italic> for classification are predetermined, the question of interest becomes whether the Rudner approach or the Lee approach can provide good estimates for their respective true accuracies. Results indicate that if the classification is made with <italic>x</italic>, Lee’s approach estimated the accuracy well. Lee’s approach, when coupled with the P-method, was slightly positively biased for short tests. While the D-method performed as well or better than the P-method, the D-method required an assumption of the distribution of the latent trait.</p>
<p>Rudner’s approach estimated the true accuracy of using <inline-formula id="inline-formula112-0146621612471888">
<mml:math display="inline" id="math119-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> well. But the pattern of bias changed with the IRT model. For 1PL, 2PL, and GRM, CA estimates from the Rudner approach had small bias. For 3PL, the Rudner approach consistently underestimated true accuracy by a relatively large but still practically small magnitude. Using the inverse of the square root of the test information as the <italic>SE</italic> of the <inline-formula id="inline-formula113-0146621612471888">
<mml:math display="inline" id="math120-0146621612471888">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mtext>ML</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is asymptotically correct, and as test length increases, it is expected, and seen in these results, that the bias decreases for all IRT models. But it is not surprising that the 3PL lags behind, especially for shorter tests, because its <inline-formula id="inline-formula114-0146621612471888">
<mml:math display="inline" id="math121-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> converges to the asymptotic distribution slower. The D- and P-methods when coupled with the Rudner approach tended to perform similarly, regardless of test length or sample size.</p>
<p>There are several limitations of the present study that the authors would like to address in future projects. First, model fit was assumed in this study. If the IRT model does not fit, then estimates of CA from both Rudner and Lee approaches will be affected. In reality, there is always some level of model misfit, but the tolerable degree of misfit in the context of CA estimation is currently unknown. Second, the true item parameters and ability distribution (for the D-method) were assumed to be known, which is rarely the truth in practice. The effect of item calibration error, and the effect of ignoring that error when estimating CA, may very well negatively affect the performance of the indices. Deviation from the assumed ability distribution will affect the marginal CA of the D-method as well. Third, to isolate the effect of cut score location, this study only looked at binary classifications, whereas investigating CA under multiple cut scores is also of practical importance. Future simulation studies could address the robustness of Lee’s and Rudner’s approaches to model misfit, item calibration error, and deviation from the assumed ability distribution, as well as the presence of multiple cut points. More broadly, the discussion of CA estimation has been extended to areas outside of the conventional IRT models. For example, CA estimation has been discussed under the cognitive diagnostic models (<xref ref-type="bibr" rid="bibr3-0146621612471888">Cui, Gierl, &amp; Chang, 2012</xref>; <xref ref-type="bibr" rid="bibr4-0146621612471888">de la Torre, Hong, &amp; Deng, 2010</xref>) and testlet models (<xref ref-type="bibr" rid="bibr25-0146621612471888">Zhang, 2010</xref>). It is also important to understand whether the CA estimation works as well under those models. Last but not the least, CA estimation when the decision making involves complex rules (<xref ref-type="bibr" rid="bibr5-0146621612471888">Douglas &amp; Mislevy, 2010</xref>) over multiple dimensions or multiple tests (<xref ref-type="bibr" rid="bibr20-0146621612471888">van Rijn, Béguin, &amp; Verstralen, 2012</xref>) is yet to be explored under IRT models.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0146621612471888">
<label>1.</label>
<p>The arbitrary values given to perfect scores will influence the estimates of Rudner’s classification accuracy (CA) because of the arbitrary values’ associated standard errors (<italic>SE</italic>s). The impact of this choice depends on how many examinees have perfect scores; shorter tests would be most sensitive to the arbitrarily assigned value of <inline-formula id="inline-formula115-0146621612471888">
<mml:math display="inline" id="math122-0146621612471888">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>θ</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula>.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0146621612471888">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bergeson</surname><given-names>T.</given-names></name>
</person-group> (<year>2007</year>, <month>August</month> <day>31</day>). <article-title>Washington language proficiency tests</article-title> (Tech. Rep.). <publisher-loc>Olympia, WA</publisher-loc>: <publisher-name>Harcourt Assessment</publisher-name>.</citation>
</ref>
<ref id="bibr2-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chang</surname><given-names>H.-H.</given-names></name>
<name><surname>Stout</surname><given-names>W.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The asymptotic posterior normality of the latent trait in an IRT models</article-title>. <source>Psychometrika</source>, <volume>58</volume>, <fpage>37</fpage>-<lpage>52</lpage>.</citation>
</ref>
<ref id="bibr3-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cui</surname><given-names>Y.</given-names></name>
<name><surname>Gierl</surname><given-names>M. J.</given-names></name>
<name><surname>Chang</surname><given-names>H.-H.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Estimating classification consistency and accuracy for cognitive diagnostic assessment</article-title>. <source>Journal of Educational Measurement</source>, <volume>49</volume>, <fpage>19</fpage>-<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr4-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>de la Torre</surname><given-names>J.</given-names></name>
<name><surname>Hong</surname><given-names>Y.</given-names></name>
<name><surname>Deng</surname><given-names>W.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Factors affecting the item parameter estimation and classification accuracy of the DINA models</article-title>. <source>Journal of Educational Measurement</source>, <volume>47</volume>, <fpage>227</fpage>-<lpage>249</lpage>.</citation>
</ref>
<ref id="bibr5-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Douglas</surname><given-names>K. M.</given-names></name>
<name><surname>Mislevy</surname><given-names>R. J.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Estimating classification accuracy for complex decision rules based on multiple scores</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>35</volume>, <fpage>280</fpage>-<lpage>306</lpage>.</citation>
</ref>
<ref id="bibr6-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guo</surname><given-names>F.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Expected classification accuracy using the latent distribution. Practical assessment</article-title>. <source>Research &amp; Evaluation</source>, <volume>11</volume>(<issue>6</issue>), <fpage>1</fpage>-<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr7-0146621612471888">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Hanson</surname><given-names>B. A.</given-names></name>
</person-group> (<year>1994</year>). <source>An extension of the Lord-Wingersky algorithm to polytomous items</source>. Unpublished research note.</citation>
</ref>
<ref id="bibr8-0146621612471888">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kolen</surname><given-names>M. J.</given-names></name>
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2004</year>). <source>Test equating, scaling, and linking: Methods and practices</source> (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr9-0146621612471888">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lathrop</surname><given-names>Q.</given-names></name>
</person-group> (<year>2011</year>). <source>cacIRT: Classification accuracy and consistency under item response theory</source> (R package version 1.0) [Computer software manual]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=cacIRT">http://CRAN.R-project.org/package=cacIRT</ext-link>
</citation>
</ref>
<ref id="bibr10-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>W.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Classification consistency and accuracy for complex assessments using item response theory</article-title>. <source>Journal of Educational Measurement</source>, <volume>47</volume>, <fpage>1</fpage>-<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr11-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>W.</given-names></name>
<name><surname>Hanson</surname><given-names>B. A.</given-names></name>
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Estimating consistency and accuracy indices for multiple classifications</article-title>. <source>Applied Psychological Measurement</source>, <volume>26</volume>, <fpage>412</fpage>-<lpage>432</lpage>.</citation>
</ref>
<ref id="bibr12-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lord</surname><given-names>F. M.</given-names></name>
<name><surname>Wingersky</surname><given-names>S, M.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Comparison of IRT true-score and equipercentile observed-score “equatings.”</article-title> <source>Applied Psychological Measurement</source>, <volume>8</volume>, <fpage>453</fpage>-<lpage>461</lpage>.</citation>
</ref>
<ref id="bibr13-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Martineau</surname><given-names>J. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>An expansion and practical evaluation of expected classification accuracy</article-title>. <source>Applied Psychological Measurement</source>, <volume>31</volume>, <fpage>181</fpage>-<lpage>194</lpage>.</citation>
</ref>
<ref id="bibr14-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Orlando</surname><given-names>M.</given-names></name>
<name><surname>Thissen</surname><given-names>D.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Likelihood-based item-fit indices for dichotomous item response theory models</article-title>. <source>Applied Psychological Measurement</source>, <volume>24</volume>, <fpage>50</fpage>-<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr15-0146621612471888">
<citation citation-type="web">
<collab>R Development Core Team</collab>. (<year>2011</year>). <source>R: A language and environment for statistical computing</source> [Computer software manual]. <publisher-loc>Vienna</publisher-loc>: <publisher-name>Austria</publisher-name>. Available from <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link>
</citation>
</ref>
<ref id="bibr16-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rudner</surname><given-names>L. M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Computing the expected proportions of misclassified examinees</article-title>. <source>Practical Assessment, Research &amp; Evaluation</source>, <volume>7</volume>(<issue>14</issue>), <fpage>1</fpage>-<lpage>5</lpage>.</citation>
</ref>
<ref id="bibr17-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rudner</surname><given-names>L. M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Expected classification accuracy</article-title>. <source>Practical Assessment, Research &amp; Evaluation</source>, <volume>10</volume>(<issue>13</issue>), <fpage>1</fpage>-<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr18-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Samejima</surname><given-names>F.</given-names></name>
</person-group> (<year>1969</year>). <article-title>Estimation of latent ability using a response pattern of graded scores [Monograph Supplement]</article-title>. <source>Psychometrika</source>, <volume>34</volume>, <fpage>100</fpage>-<lpage>114</lpage>.</citation>
</ref>
<ref id="bibr19-0146621612471888">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sireci</surname><given-names>S. G</given-names></name>
<name><surname>Baldwin</surname><given-names>P.</given-names></name>
<name><surname>Martone</surname><given-names>A.</given-names></name>
<name><surname>Zenisky</surname><given-names>A. L.</given-names></name>
<name><surname>Kaira</surname><given-names>L.</given-names></name>
<name><surname>Lam</surname><given-names>W.</given-names></name>
<name><surname>. . . Hambleton</surname><given-names>R. K.</given-names></name>
</person-group> (<year>2008</year>). <source>Massachusetts Adult Proficiency Tests technical manual: Version 2</source> (Research Report No. 677). <publisher-loc>Amherst</publisher-loc>: <publisher-name>Center for Educational Assessment, University of Massachusetts</publisher-name>.</citation>
</ref>
<ref id="bibr20-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>van Rijn</surname><given-names>P.</given-names></name>
<name><surname>Béguin</surname><given-names>A.</given-names></name>
<name><surname>Verstralen</surname><given-names>H.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Educational measurement issues and implications of high stakes decision making in final examinations in secondary education in the Netherlands</article-title>. <source>Assessment in Education: Principles, Policy &amp; Practice</source>, <volume>19</volume>, <fpage>117</fpage>-<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr21-0146621612471888">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wan</surname><given-names>L.</given-names></name>
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
<name><surname>Lee</surname><given-names>W.</given-names></name>
</person-group> (<year>2007</year>). <source>Estimating classification consistency for complex assessments</source>. <publisher-loc>Iowa City</publisher-loc>: <publisher-name>Center for Advanced Studies in Measurement and Assessment, University of Iowa</publisher-name>.</citation>
</ref>
<ref id="bibr22-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Woods</surname><given-names>C. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Ramsay curve IRT for Likert-type data</article-title>. <source>Applied Psychological Measurement</source>, <volume>31</volume>, <fpage>195</fpage>-<lpage>212</lpage>.</citation>
</ref>
<ref id="bibr23-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wyse</surname><given-names>A. E.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The potential impact of not being able to create parallel tests on expected classification accuracy</article-title>. <source>Applied Psychological Measurement</source>, <volume>35</volume>, <fpage>110</fpage>-<lpage>126</lpage>.</citation>
</ref>
<ref id="bibr24-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wyse</surname><given-names>A. E.</given-names></name>
<name><surname>Hao</surname><given-names>S.</given-names></name>
</person-group> (<year>2012</year>). <article-title>An evaluation of item response theory classification accuracy and consistency indices</article-title>. <source>Applied Psychological Measurement</source>, <volume>36</volume>, <fpage>602</fpage>-<lpage>624</lpage>.</citation>
</ref>
<ref id="bibr25-0146621612471888">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>B.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Assessing the accuracy and consistency of language proficiency classification under competing measurement models</article-title>. <source>Language Testing</source>, <volume>27</volume>, <fpage>119</fpage>-<lpage>140</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>