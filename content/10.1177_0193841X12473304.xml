<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><front><journal-meta><journal-id journal-id-type="publisher-id">ERX</journal-id><journal-id journal-id-type="hwp">sperx</journal-id><journal-id journal-id-type="nlm-ta">Eval Rev</journal-id><journal-title>Evaluation Review</journal-title><issn pub-type="ppub">0193-841X</issn><issn pub-type="epub">1552-3926</issn><publisher><publisher-name>SAGE Publications</publisher-name><publisher-loc>Sage CA: Los Angeles, CA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1177/0193841X12473304</article-id><article-id pub-id-type="publisher-id">10.1177_0193841X12473304</article-id><article-categories><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group></article-categories><title-group><article-title>Do Less Effective Teachers Choose Professional Development Does It Matter?</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Barrett</surname><given-names>Nathan</given-names></name><xref ref-type="aff" rid="aff1-0193841X12473304">1</xref><xref ref-type="corresp" rid="corresp1-0193841X12473304"/></contrib><contrib contrib-type="author"><name><surname>Butler</surname><given-names>J. S.</given-names></name><xref ref-type="aff" rid="aff2-0193841X12473304">2</xref></contrib><contrib contrib-type="author"><name><surname>Toma</surname><given-names>Eugenia F.</given-names></name><xref ref-type="aff" rid="aff2-0193841X12473304">2</xref></contrib><bio><title>Author Biographies</title><p><bold>Nathan Barrett</bold> is a Senior Research Associate at the Carolina Institute for Public Policy at the University of North Carolina at Chapel Hill. He specializes in policy evaluation, teacher effectiveness, and education finance. He currently leads several evaluation initiatives of Race to the Top in North Carolina.</p><p><bold>J. S. Butler</bold> is an econometrician working recently in Policy Analysis, Economics, Political Science, and Pharmacy, with interests in panel data, limited dependent variables, and hazard (event history) models. He works in education policy, an extension from previous research in labor economics and economic development.</p><p><bold>Eugenia F. Toma</bold> is the Wendell H. Ford Professor of Public Policy at the University of Kentucky's Martin School of Public Policy and Administration. She specializes in the economics of education. She currently serves as PI of a National Science Foundation award to evaluate the effectiveness of teacher professional development in Appalachia.</p></bio></contrib-group><aff id="aff1-0193841X12473304"><label>1</label>University of North Carolina at Chapel Hill, NC, USA</aff><aff id="aff2-0193841X12473304"><label>2</label>University of Kentucky, Lexington, KY, USA</aff><author-notes><corresp id="corresp1-0193841X12473304">Nathan Barrett, University of North Carolina at Chapel Hill, 218 Center for School Leadership Development, 140 Friday Center Drive, Chapel Hill, NC 27517, USA. Email: <email>nate.barrett@unc.edu</email></corresp></author-notes><pub-date pub-type="epub-ppub"><month>10</month><year>2012</year></pub-date><volume>36</volume><issue>5</issue><fpage>346</fpage><lpage>374</lpage><permissions><copyright-statement>© The Author(s) 2013</copyright-statement><copyright-year>2013</copyright-year><copyright-holder content-type="sage">SAGE Publications</copyright-holder></permissions><abstract><sec><title>Background:</title><p> In an ongoing effort to improve teacher quality, most states require continuing education or professional development for their in-service teachers. Studies evaluating the effectiveness of various professional development programs have assumed a normal distribution of quality of teachers participating in the programs. Because participation in many professional development programs is either targeted or voluntary, this article suggests past evaluations of the effectiveness of professional development may be subject to selection bias and policy recommendations may be premature.</p></sec> <sec><title>Research Design:</title><p> This article presents an empirical framework for evaluating professional development programs where treatment is potentially nonrandom, and explicitly accounts for the teacher's prior effectiveness in the classroom as a factor that may influence participation in professional development. This article controls for the influence of selection bias on professional development outcomes by generating a matched sample based on propensity scores and then estimating the program's effect. </p></sec> <sec><title>Results:</title><p> In applying this framework to the professional development program examined in this article, less effective teachers are found to be more likely to participate in the program, and correcting for this selection leads to different conclusions regarding the program's effectiveness than when ignoring teacher selection patterns.</p></sec></abstract><kwd-group><kwd>education</kwd><kwd>content area</kwd><kwd>economic evaluation</kwd><kwd>design and evaluation of programs and policies</kwd><kwd>outcome evaluation (other than economic evaluation)</kwd><kwd>design and evaluation of programs and policies</kwd><kwd>program implementation</kwd><kwd>economic evaluation</kwd><kwd>methodology</kwd></kwd-group></article-meta></front><body><sec id="section1-0193841X12473304"><title>Introduction</title><p>Academics and policy makers increasingly emphasize that one key to improving educational outcomes in the United States lies in enhancing the quality of teachers. After many years of research that failed to show significant and systematic effects of school-level inputs, including per pupil expenditures, teacher salaries, or pupil–teacher ratios on student outcomes, recent work illustrates that the quality of teachers is a significant factor in explaining student achievement (Boyd et al. 2005; Clotfelter, Ladd, and Vigdor 2007; Rivkin, Hanushek, and Kain 2005). For those already in the teaching profession, one of the ways to improve teacher quality is through professional development programs. While different methods of recruitment and training of teachers may affect the quality of future teachers, professional development programs focus on changing the quality of teachers currently in the classroom.<sup><xref ref-type="fn" rid="fn1-0193841X12473304">1</xref></sup></p><p>Recognizing the potential importance of professional development in improving teacher knowledge and student outcomes, the scholarly world has begun to analyze professional development programs in an effort to evaluate how effective this route may be. While studies that empirically examine professional development effects on student outcomes remain small in number, some scholars and policy makers are embracing professional development and going so far as to identify elements that are critical for effective professional development, particularly in the fields of math and science teaching (<xref ref-type="bibr" rid="bibr20-0193841X12473304">Loucks-Horsley et al. 2010</xref>; Desimone 2009; Wayne et al. 2008; Garet et al. 2001).</p><p>This article argues that a critical methodological question has been largely overlooked in previous studies. To date, most evaluations of the effectiveness of professional development programs in improving teacher knowledge or in enhancing student outcomes have implicitly assumed that a random distribution of teachers was assigned the professional development treatments. There is reason to suspect that this assumption may not be accurate; furthermore, it is not evident that random intervention is a preferred implementation strategy for professional development.<sup><xref ref-type="fn" rid="fn2-0193841X12473304">2</xref></sup> Accordingly, if participation is nonrandom, the effects of program participation may be subject to bias. This is particularly relevant if the decision to participate is based upon factors, such as own-teacher effectiveness, that influence the outcome variable being estimated.</p><p>This article provides a framework for evaluating professional development programs under nonexperimental conditions and applies this framework to a professional development program in which teachers voluntarily chose to participate. Using estimates of own-teacher effectiveness as a primary factor on which the decision to participate is based, this article explicitly accounts for the distribution of teachers who self-selected into the program and finds that, in the self-selection program considered here, less effective teachers were more likely to participate in professional development than the more effective teachers. Without controlling for this selection tendency, results suggest that the program was mostly unrelated to student outcomes and in one case had a statistically significant negative impact on student achievement. After controlling for the observed teacher selection bias, our results suggest that, on average, the program had a significant and positive effect on student outcomes. The article demonstrates important implications for the assessment of effective professional development and for the design of future policies regarding professional development.</p></sec><sec id="section2-0193841X12473304"><title>Professional Development Effectiveness Studies</title><p>Support for professional development is a common theme in education reform (<xref ref-type="bibr" rid="bibr14-0193841X12473304">Fullan and Hargreaves 1996</xref>). Many state teacher certification programs require ongoing professional development under the assumption that it contributes to teacher effectiveness and improves student outcomes. This assumption is not without some scientific support. Studies examining teachers’ perceptions of their own learning from professional development activities (Garet et al. 2001) have found that professional development increases teachers’ self-reported knowledge and skills. Ball and Cohen (1999), Hill and Ball (2004), Hill, Rowan, and Ball (2005), and Ball and Hill (2009) went beyond the self-reported perceptions of teachers to develop measures of teacher knowledge that were enhanced by professional development programs. Teachers of mathematics, in these studies, do show measured gains in mathematics knowledge with certain types of professional development, but not all. These studies found that focused programs and summer length institutes enhanced teachers’ knowledge of mathematics and teachers’ knowledge of mathematics is positively related to student gain scores. Desimone et al. (2002) and Desimone (2009) linked the type of professional development to observed changes in practice of teachers. They found that especially focused instructional practice can result in more use of the practices in the classroom.</p><p>While the evaluation research and the practice of professional development have evolved past the notion that any professional development is better than none, there are still unresolved issues, such as establishing a causal link between programs and student outcomes. Despite a literature that views professional development as a positive strategy for improving teacher quality, Wayne et al. (2008) argues that we know little about whether professional development generally delivers positive effects on student achievement. As a result, they call for more methodological diversity in trying to evaluate the effectiveness of professional development programs and, in particular, they call for experimental and quasi-experimental study designs.<sup><xref ref-type="fn" rid="fn3-0193841X12473304">3</xref></sup></p><p>This article will not purport to address all the issues in evaluating the effectiveness of professional development programs. Rather, we address a specific evaluation issue in this field that has been largely ignored. In particular, most previous studies have examined programs by implicitly assuming the teachers who participate in the professional development “treatment” are representative of all teachers. In reality, teachers are often chosen for participation through some specific selection criteria, or, as in the case examined in this article, the teachers volunteered for the treatment. Either a targeted or a voluntary participation process leads to the possibility of selection bias that may confound the estimated effects of the professional development. To correct for these possible biases, it is necessary to control for the teacher’s prior effectiveness, her content knowledge, her experience level, her motivation, and any other characteristics, including those associated with the school, that may have affected her choice to participate in the program. Failure to control for these factors implies that subsequent findings of program effectiveness are confounded because results may be reflecting these observed or unobserved underlying teacher participation characteristics rather than true program effects.</p><p>This article, like most in the professional development literature, provides an evaluation of a particular professional development program. But the major point of this evaluation will be to assess who selects into the professional development and to determine the influence of the selection on the estimated effects of professional teacher interventions. In other words, are there nonrandom characteristics of teachers that influence who participates in a particular type of professional development? If so, are the effects of the professional development treatments that are generated by evaluations biased by the nonrandom distribution of teachers who may select into the programs? Through the evaluation framework provided, we find that, indeed, correcting for this single potential source of bias both statistically and substantively changes, the estimated effects of the professional development program that we examine. Before describing our method for examining the effects of this selection bias, the next briefly describes the professional development treatment that is examined in this article.</p></sec><sec id="section3-0193841X12473304"><title>The Math and Science Partnerships: A Brief Description</title><p>As part of the vision of the <italic>No Child Left Behind Act,</italic> the National Science Foundation launched a new initiative in 2002 to improve the quality of teaching in the science, technology, engineering, and math (STEM) areas. The initiative, as envisioned at the National Science Foundation, was focused on the creation of partnerships between institutions of higher education and K–12 schools to increase the quality of K–12 STEM teachers. Previous studies have examined whether the partnership model (between an institution or institutions of higher education and a particular K-12 school) for professional development has been effective.<sup><xref ref-type="fn" rid="fn4-0193841X12473304">4</xref></sup> Some of these have been qualitative in nature and asked whether interventions have changed teacher or student attitudes or the culture of the school. Alternatively, instruments have been developed to assess whether teachers’ knowledge of the content of their STEM area increased after participation in a training program. Wong et al. (2009), Dimitrov (2009), and Foster, Toma, and Troske (2013) have looked at school-level data to measure effectiveness of the math and science partnerships in terms of student outcomes. Because these latter studies used school-level data, there has been no attention given to the possible individual-level biases in teacher participation propensities. This article will use student-level data that match teachers to students to examine the effectiveness of the intervention. By examining the program using micro data, we can explicitly address biases exhibited by teachers in choosing to participate in the professional development.</p><p>This article will examine the Appalachian Math and Science Partnership (AMSP), which is one of the largest initial partnership programs. The Appalachian region is especially interesting because of its poor, rural population, and long-standing achievement gap between rural and urban schools in the same states. Professional development in this rural setting is potentially of great importance in improving teacher quality than in the urban areas. Cowen et al. (2012) found even lower teacher mobility in rural Appalachian schools than the already low mobility found in more urban settings. Teachers who are hired in Appalachian schools typically have attended college in the region (Fowles et al. 2012) and choose to stay in place rather than to move out of the region. Consequently, one of the few remaining routes for teacher quality improvement is professional development.</p><p>In the AMSP, higher education faculty designed and delivered training programs for K–12 teachers of math and science, but this article will focus only on math training.<sup><xref ref-type="fn" rid="fn5-0193841X12473304">5</xref></sup> In all MSPs, it is assumed that the content expertise held by higher education faculty will translate into higher quality teaching at the K–12 level. This particular program generally exhibited characteristics such as intensive content training, which the literature is beginning to identify as essential to effective professional development (Garet et al. 2001; Wayne et. al. 2008; <xref ref-type="bibr" rid="bibr16-0193841X12473304">Garet et al. 2008</xref>; Desimone 2009; <xref ref-type="bibr" rid="bibr20-0193841X12473304">Loucks-Horsley et al. 2010</xref>).<sup><xref ref-type="fn" rid="fn6-0193841X12473304">6</xref></sup></p><p>At the inception of the AMSP program, superintendents of Appalachian districts were invited to participate in AMSP. Superintendents endorsed or did not endorse the training offered by AMSP. Beyond the initial agreement at the district level, there was no systematic means of selecting which teachers participated in the program. Professional development activities were announced and teachers either voluntarily chose to participate or not. Some teachers who were not teaching in an AMSP district also participated.<sup><xref ref-type="fn" rid="fn7-0193841X12473304">7</xref></sup> Note that in Kentucky, all K–12 teachers are required to participate in at least 4 days of professional development activities annually, and the AMSP professional development activities satisfied annual teacher professional development training as required by the Kentucky Department of Education.<sup><xref ref-type="fn" rid="fn8-0193841X12473304">8</xref></sup> Throughout this analysis, we implicitly are evaluating the AMSP effectiveness relative to the collection of all other types of in-service training.</p><p>Before developing a model to examine the effectiveness of professional development, it is useful to think conceptually about why we focus on the possibility of selection bias in the teacher participation decision. As stated above, Kentucky, like most states, requires teachers to participate in some sort of professional development each year as part of their continuing certification or licensure requirements. We are unaware of any state that requires programs to prove their effectiveness in order to be included in the list of acceptable professional development options. This raises an interesting dilemma for a teacher. Professional development programs likely vary along a number of relevant dimensions and no state has incorporated mechanisms that match teachers to professional development programs based on improving teacher effectiveness. The absence of these mechanisms means that the ultimate effectiveness of the professional development program depends on the motivation of the individual teachers in selecting into various programs.</p><p>Teachers who benefit most from professional development may or may not be the ones most likely to pursue effective training. To illustrate, one could argue that the teachers who are weakest in terms of their own value-added for the students are the ones expected to gain the most from training. Under increasing accountability of teachers, we may also expect them to be the most motivated to improve. But these teachers may also be generally less ambitious, as reflected by prior relative ineffectiveness in the classroom. Conversely, the teachers who are generally most motivated and those with the most effective teaching records may be more likely to enroll in rigorous professional development even though the value-added in knowledge and instructional effectiveness may be lowest for them. Finally, there may be no relationship between a teacher’s past effectiveness, other observable characteristics, or even unobservable characteristics that influence her decision to participate in professional development activities.</p><p>Regardless of the direction of the effect, if there are systematic factors (either observed or unobserved) that influence which teachers choose to enroll in professional development, and these effects are not controlled in an empirical model, the effectiveness of the intervention itself will not be properly estimated. The remainder of this article evaluates the choice to participate in AMSP and its implications for measuring the effectiveness of the program. The methods used in this article should be applicable to the broader set of MSPs, as well as to other professional development programs where participation is nonrandom.</p></sec><sec id="section4-0193841X12473304"><title>Data</title><p>The remainder of this article will provide an analysis of the AMSP using individual student–teacher observations from the school year 2000 to 2001 through 2007 to 2008.<sup><xref ref-type="fn" rid="fn9-0193841X12473304">9</xref></sup> This time period allows 2 years of observations prior to the AMSP intervention and covers its entire time of activity. Although the training program includes schools across four states, this article only includes students in Kentucky.<sup><xref ref-type="fn" rid="fn10-0193841X12473304">10</xref></sup> Like many states to date, Kentucky did not collect information that allows particular students to be matched to specific teachers over this time period. To successfully identify which students were in a particular teacher’s classroom, we solicited the cooperation of local school districts in the Appalachian portion of the state.<sup><xref ref-type="fn" rid="fn11-0193841X12473304">11</xref></sup> We invited all Appalachian districts to provide classroom roster data regardless of whether or not the school superintendents had officially agreed for their schools to participate in the partnership program. The roster data list the course, the teacher for the course, and all students who were enrolled in that course for each school and each school year. Our ability to match students to specific teachers over the past 8 years varies by district. Some districts retained all data from the past while others retained data for shorter periods of time.<sup><xref ref-type="fn" rid="fn12-0193841X12473304">12</xref></sup> As a result, we have a mixed panel of data. We assume that the missingness of roster data is random and unassociated with students outcomes. Other student characteristics available for the matching are time-invariant characteristics such as gender and race. Free and reduced price lunch data vary slightly by school year. The Kentucky Department of Education (KDE) provided the individual student demographic and test score data for this analysis.</p><p>Standardized math tests (known as Kentucky Core Content tests) are administered annually in all schools in Kentucky but not at all grade levels each year. From the 2000 to 2001 and from 2005 to 2006 school years, the state tested math in Grades 5, 8, and 11. Nationally standardized tests (Comprehensive Tests of Basic Skills [CTBS]) were also administered to students in Grades 3, 6, and 9 over the same years. Beginning in 2006–2007, state math testing was instituted annually in Grades 3–8 and retained for 11th grade. Because we use the change in math achievement as the dependent variable, this testing schema limits the number of years we can observe student achievement changes and subsequently the number of teachers for which we can estimate the decision to participate and the subsequent outcome of treatment.<sup><xref ref-type="fn" rid="fn13-0193841X12473304">13</xref></sup></p><p>As in many states, the scaling of the tests also changed over the examined period. The change in the state’s test in 2007 was such that the scale of scores in years prior cannot be reliably reconciled with those of 2007 and onward.<sup><xref ref-type="fn" rid="fn14-0193841X12473304">14</xref></sup> In addition, each grade-level test involves scores with different scales. A 500 on a fifth-grade math test was not designed to be equivalent to a 500 on an eighth-grade math test. Therefore, grade levels must be examined separately for evaluation purposes. Given the changing scale of the test score data over the time period we observe and the multiple exams (state and national),<sup><xref ref-type="fn" rid="fn15-0193841X12473304">15</xref></sup> we convert raw scores to <italic>z</italic> scores. <italic>Z</italic> scores are a nonlinear transformation and are frequently used for standardizing student test score data across multiple exams and scaling changes.</p><p>The professional development program was phased in with the peak level of teacher participation in 2005–2006, a slight decline in 2006–2007, and then a phase out. The Kentucky Education Professional Standards Board (EPSB) provided teacher-level data that begin in the 2000–2001 school year and continue until 2007–2008. The available teacher-level data are quite comprehensive, but we limit ourselves to teacher experience and highest degree achieved. All other characteristics of the teachers, such as Praxis scores, gender, and race, are time invariant and are captured in the teacher fixed effects in the model below. We also have school-level characteristics that are time varying.</p><p>Most of the districts sampled for the study are small, rural districts typically with a single high school but multiple elementary schools.<sup><xref ref-type="fn" rid="fn16-0193841X12473304">16</xref></sup> Of these 10 districts, 6 districts formally participated in the partnership professional development activities beginning in 2002–2003. Four districts did not officially participate with AMSP as a provider of professional development activities.<sup><xref ref-type="fn" rid="fn17-0193841X12473304">17</xref></sup> Regardless of official district-level participation, teachers in all districts were permitted to enroll in the training activities. In the nonparticipating districts, teachers crossed district lines for the training activities even though the superintendent had not formally joined AMSP. As illustrated in <xref ref-type="table" rid="table1-0193841X12473304">Table 1</xref>, participation rates tended to be higher in districts whose superintendents joined AMSP. In our non-AMSP districts, teachers who chose to participate in the professional development activities account for 10% of participation. The AMSP program provided the data on teacher participation.</p><table-wrap id="table1-0193841X12473304" position="float"><label>Table 1.</label><caption><p>Number of Participants in AMSP Professional Development in Math.<sup>a</sup></p></caption><graphic alternate-form-of="table1-0193841X12473304" xlink:href="10.1177_0193841X12473304-table1.tif"/><table><thead><tr><th>District</th><th>AMSP Participant</th><th>2002–2003</th><th>2003–2004</th><th>2004–2005</th><th>2005–2006</th><th>2006–2007</th><th>2007–2008</th></tr></thead><tbody><tr><td>1</td><td>Yes</td><td>0</td><td>5</td><td>3</td><td>9</td><td>10</td><td>1</td></tr><tr><td>2</td><td>Yes</td><td>0</td><td>0</td><td>14</td><td>34</td><td>20</td><td>21</td></tr><tr><td>3</td><td>Yes</td><td>0</td><td>2</td><td>0</td><td>5</td><td>2</td><td>15</td></tr><tr><td>4</td><td>No</td><td>1</td><td>1</td><td>9</td><td>5</td><td>0</td><td>2</td></tr><tr><td>5</td><td>Yes</td><td>0</td><td>4</td><td>2</td><td>4</td><td>0</td><td>0</td></tr><tr><td>6</td><td>No</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr><tr><td>7</td><td>Yes</td><td>0</td><td>2</td><td>1</td><td>5</td><td>1</td><td>0</td></tr><tr><td>8</td><td>No</td><td>0</td><td>0</td><td>3</td><td>2</td><td>1</td><td>0</td></tr><tr><td>9</td><td>Yes</td><td>0</td><td>2</td><td>32</td><td>16</td><td>10</td><td>13</td></tr><tr><td>10</td><td>No</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Total</td><td/><td>1</td><td>16</td><td>64</td><td>80</td><td>44</td><td>53</td></tr></tbody></table><table-wrap-foot><fn id="table-fn1-0193841X12473304"><p><italic>Note</italic>. <sup>a</sup>Teachers were able to participate in more than 1 year.</p></fn><fn id="table-fn2-0193841X12473304"><p>These figures represent unique teacher/year combinations and not unique teachers.</p></fn></table-wrap-foot></table-wrap><p>After receiving the data from multiple sources, we matched the data so that we observe each student who is tested in math in a given year and for which we can identify the teacher or teachers who taught that student in those content courses. <xref ref-type="table" rid="table2-0193841X12473304">Table 2</xref> provides descriptive statistics for the students, teachers, and schools for the 10 districts. The statistics vary somewhat by year, and we present school year 2005–2006 as our representative year because it is a year in which all districts provided roster data.</p><table-wrap id="table2-0193841X12473304" position="float"><label>Table 2.</label><caption><p>Descriptive Statistics of Sample in 2005.</p></caption><graphic alternate-form-of="table2-0193841X12473304" xlink:href="10.1177_0193841X12473304-table2.tif"/><table><thead><tr><th>Variable</th><th>Observations</th><th>Mean</th><th>Standard Deviation</th><th>Minimum</th><th>Maximum</th></tr></thead><tbody><tr><td>Free and Reduced Price Lunch</td><td>11,929</td><td>0.46</td><td>0.50</td><td>0</td><td>1</td></tr><tr><td>Female</td><td>11,695</td><td>0.48</td><td>0.50</td><td>0</td><td>1</td></tr><tr><td>Asian</td><td>11,715</td><td>0.02</td><td>0.12</td><td>0</td><td>1</td></tr><tr><td>Black</td><td>11,715</td><td>0.10</td><td>0.30</td><td>0</td><td>1</td></tr><tr><td>Hispanic</td><td>11,715</td><td>0.02</td><td>0.15</td><td>0</td><td>1</td></tr><tr><td>Native American</td><td>11,715</td><td>&lt;0.00</td><td>0.01</td><td>0</td><td>1</td></tr><tr><td>Other</td><td>11,715</td><td>0.01</td><td>0.09</td><td>0</td><td>1</td></tr><tr><td>Avg. Experience (School)</td><td>11,929</td><td>12.30</td><td>2.10</td><td>5.3</td><td>17.7</td></tr><tr><td>Enrollment</td><td>11,929</td><td>681.94</td><td>423.17</td><td>80</td><td>2,062</td></tr><tr><td>Percent Masters’ Degree</td><td>11,929</td><td>76.09</td><td>11.26</td><td>35</td><td>100</td></tr><tr><td>Percent Free and Reduced </td><td>11,929</td><td>0.51</td><td>0.212</td><td>0</td><td>.97</td></tr><tr><td>Expenditure per Student</td><td>11,929</td><td>5,708.21</td><td>1,208.24</td><td>3,908</td><td>12,837</td></tr><tr><td>Student Teacher Ratio</td><td>11,929</td><td>15.15</td><td>2.38</td><td>8</td><td>22</td></tr><tr><td>Student Computer Ratio</td><td>11,929</td><td>4.01</td><td>1.28</td><td>1.6</td><td>7.6</td></tr><tr><td>Math Index</td><td>11,929</td><td>71.52</td><td>12.35</td><td>41.97</td><td>112.69</td></tr><tr><td>Experience</td><td>11,929</td><td>12.085</td><td>8.973</td><td>0</td><td>34</td></tr><tr><td>Experience Squared</td><td>11,929</td><td>226.552</td><td>269.009</td><td>0</td><td>1,156</td></tr><tr><td>Highest Degree<sup>a</sup></td><td>11,929</td><td>3.083</td><td>1.301</td><td>1</td><td>7</td></tr></tbody></table><table-wrap-foot><fn id="table-fn1a-0193841X12473304"><p>a – Highest degree is coded as follows: 1=Bachelor’s, 2=5<sup>th</sup> year, 3=Planned 6<sup>th</sup> year, 4=Master’s, 5=Rank I, 6=Specialist, 7=Doctorate.</p></fn></table-wrap-foot></table-wrap></sec><sec id="section5-0193841X12473304"><title>Model</title><p>To illustrate the importance of addressing teacher selection into professional development treatments, we begin by presenting a typical model of estimating treatment effects in education policy. Normally, to estimate the effects of an intervention, postintervention outcome data are compared to preintervention, controlling for other factors that are expected to affect the outcome. The estimated coefficient on the intervention variable then illustrates the magnitude of the program effects. Building on a prototypical model in education policy, we estimate for each student–teacher year combination the student outcome with the following:<sup><xref ref-type="fn" rid="fn18-0193841X12473304">18</xref></sup><disp-formula id="disp-formula1-0193841X12473304"><label>1</label><mml:math id="mml-disp2-0193841X12473304"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mi>S</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo></mml:math><graphic alternate-form-of="disp-formula1-0193841X12473304" xlink:href="10.1177_0193841X12473304-eq1.tif"/></disp-formula></p><p>where <inline-formula id="inline-formula1-0193841X12473304"><mml:math id="mml-inline1-0193841X12473304"><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi mathvariant="normal">Z</mml:mi></mml:mrow></mml:math></inline-formula> is the relative<italic> z</italic> score change between years <italic>t </italic>− 1 and <italic>t</italic> for each student <italic>i</italic> in the sample on the standardized math exam in given year <italic>t</italic> and the <inline-formula id="inline-formula2-0193841X12473304"><mml:math id="mml-inline2-0193841X12473304"><mml:msubsup><mml:mi mathvariant="italic">β</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mi>s</mml:mi></mml:math></inline-formula> are estimated coefficients.<sup><xref ref-type="fn" rid="fn19-0193841X12473304">19</xref></sup> A vector of variables, <italic>D</italic>, describes the demographics of student <italic>i</italic> in year <italic>t</italic>; T is a vector of student <italic>i</italic>’s math teacher (<italic>j</italic>) experience and degree variables in year <italic>t</italic>; the vector, SC, describes characteristics of the school for which the student–teacher combination is observed in year <italic>t</italic>; and AMSP is a binary treatment variable with a value of 1 if student <italic>i</italic>’s teacher participated in AMSP and 0 if the teacher did not participate. Teacher <italic>j</italic>’s fixed effect is represented by <inline-formula id="inline-formula3-0193841X12473304"><mml:math id="mml-inline3-0193841X12473304"><mml:msub><mml:mi mathvariant="italic">θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula id="inline-formula4-0193841X12473304"><mml:math id="mml-inline4-0193841X12473304"><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is the teacher-clustered error term, and <inline-formula id="inline-formula5-0193841X12473304"><mml:math id="mml-inline5-0193841X12473304"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the idiosyncratic error. Estimation of <xref ref-type="disp-formula" rid="disp-formula1-0193841X12473304">Equation 1</xref> yields a predicted <italic>z</italic>-score change, <inline-formula id="inline-formula6-0193841X12473304"><mml:math id="mml-inline6-0193841X12473304"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mover accent="true"><mml:mi mathvariant="normal">Z</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each student, based on the student’s demographics, teacher, and school, as well as the effect of the teacher’s participation in AMSP.</p><p>If we focus specifically on the coefficient for the AMSP variable, we see that the results in <xref ref-type="table" rid="table3-0193841X12473304">Table 3</xref> indicate that the professional development treatment appears to have no statistical relationship or negatively affects student score changes in math, when we control for other variables that might affect outcomes.<sup><xref ref-type="fn" rid="fn20-0193841X12473304">20</xref></sup> Students of teachers who participated in AMSP had no response or lower test score changes than students whose teachers did not participate in the professional development across grade levels. The treatment appears to have had no impact or to have made teachers less effective when we control for other factors that are expected to influence score changes.</p><table-wrap id="table3-0193841X12473304" position="float"><label>Table 3.</label><caption><p>Estimated Coefficients of Participation and Correlations. </p></caption><graphic alternate-form-of="table3-0193841X12473304" xlink:href="10.1177_0193841X12473304-table3.tif"/><table><thead><tr><th/><th>Elementary</th><th>Middle</th><th>High</th></tr></thead><tbody><tr><td rowspan="2">AMSP Participation</td><td>0.020</td><td>0.017</td><td>-0.040**</td></tr><tr><td>(0.026)</td><td>(0.012)</td><td>(0.021)</td></tr><tr><td>Correlation of Participation to the error term of <xref ref-type="disp-formula" rid="disp-formula2-0193841X12473304">Equation 2</xref></td><td>-0.039***</td><td>-0.051***</td><td>-0.033***</td></tr></tbody></table><table-wrap-foot><fn id="table-fn2a-0193841X12473304"><p>Note: Coefficients with robust standard errors in parentheses are presented in the table.</p></fn><fn id="table-fn3-0193841X12473304"><p>** indicates 0.05 level of significance, *** indicates 0.01 level of significance.</p></fn></table-wrap-foot></table-wrap><p>We suspect the AMSP coefficient may be biased by the nonrandom participation of teachers, and thus we employ a standard test for endogeneity that is used when there is a lack of instruments or variables which influence teacher participation that do not also influence student outcomes.<sup><xref ref-type="fn" rid="fn21-0193841X12473304">21</xref></sup> This test evaluates the correlation between the error term of the model when estimated without the treatment effects (i.e., without AMSP in <xref ref-type="disp-formula" rid="disp-formula1-0193841X12473304">Equation 1</xref>) and the participation variable.<sup><xref ref-type="fn" rid="fn22-0193841X12473304">22</xref></sup> The significance of the correlations in <xref ref-type="table" rid="table3-0193841X12473304">Table 3</xref> suggests the presence of endogeneity.<sup><xref ref-type="fn" rid="fn23-0193841X12473304">23</xref></sup></p><p>In the absence of an instrumental variable, we consider propensity score matching (PSM) for selection bias correction. Ordinarily, the use of PSM would correct for this apparent selection bias by identifying observable factors that influence who selects into AMSP, controlling for those factors, and generating propensities of participation to create a matched sample of treated and nontreated teachers. Then program effectiveness can be examined while controlling for the selection bias.<sup><xref ref-type="fn" rid="fn24-0193841X12473304">24</xref></sup> However, in the program examined in this article, and hypothetically in many professional development instances, we suspect a motivating factor in participation is the unobserved effectiveness of the teacher. As discussed earlier, we do not know whether it is more effective or less effective teachers who are likely to enlist in the program but conceptually we expect that effectiveness matters. Thus, we cannot generate a propensity score for participation by simply estimating a model based on observables. Instead, we must first generate an estimate of teacher effectiveness that allows us to capture unobserved influences of the teacher on student outcomes. We do this by generating a teacher fixed effect from estimating for each student–teacher year combination the student outcome:<disp-formula id="disp-formula2-0193841X12473304"><label>2</label><mml:math id="mml-disp4-0193841X12473304"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mi>S</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo></mml:math><graphic alternate-form-of="disp-formula2-0193841X12473304" xlink:href="10.1177_0193841X12473304-eq2.tif"/></disp-formula></p><p>where the variables are as described in <xref ref-type="disp-formula" rid="disp-formula1-0193841X12473304">Equation 1</xref>. We estimate this equation for school years 2003 through 2008.<sup><xref ref-type="fn" rid="fn25-0193841X12473304">25</xref></sup> In the estimation, the current year and all preceding years are included to generate an estimate of the predicted student <italic>z</italic> score change and, for our purpose, an average teacher fixed effect for a given year <italic>t</italic>. For example, the estimated fixed effect for a given teacher <italic>j</italic> in any year <italic>t</italic> is the average effect of the unobservable characteristics of teacher <italic>j</italic> on her student’s outcomes from 2000 to 2001, or all prior years, through year <italic>t</italic>. This is done so that we can estimate a pretreatment fixed effect for each teacher.</p><p>This estimate of prior teacher effectiveness, based on unobservables, is substituted into the <xref ref-type="disp-formula" rid="disp-formula3-0193841X12473304">Equation 3</xref> below. Here, we estimate a probit equation to explain teacher participation in AMSP in year <italic>t</italic>. In particular, we estimate the probability of teacher j choosing to participate in the professional development activities in year <italic>t</italic> as: <disp-formula id="disp-formula3-0193841X12473304"><label>3</label><mml:math id="mml-disp6-0193841X12473304"><mml:mi>P</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>S</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mover accent="true"><mml:mi mathvariant="italic">θ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">ϵ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo></mml:math><graphic alternate-form-of="disp-formula3-0193841X12473304" xlink:href="10.1177_0193841X12473304-eq3.tif"/></disp-formula></p><p>where PD is a binary variable indicating whether a teacher chose to participate in the professional development activities of a given year or did not.<sup><xref ref-type="fn" rid="fn26-0193841X12473304">26</xref></sup> The independent variables include a vector of time-varying teacher characteristics <italic>T</italic>, for teacher <italic>j</italic> in year<italic> t</italic>; a vector of school-level characteristics SC, for teacher <italic>j</italic> in each year <italic>t</italic> including the lagged school’s performance level; and <inline-formula id="inline-formula7-0193841X12473304"><mml:math id="mml-inline7-0193841X12473304"><mml:mover accent="true"><mml:mi mathvariant="italic">θ</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover></mml:math></inline-formula> is the constructed measure of the teacher’s average past effectiveness in the classroom from <xref ref-type="disp-formula" rid="disp-formula2-0193841X12473304">Equation 2</xref>. We include this as a means of testing whether the teacher’s own effectiveness in the classroom motivates her to choose the professional development and, if so, whether the motivation is positively or negatively related to participation.</p><p>With these probit estimates, we then construct propensity scores for a given teacher’s propensity to enroll in professional development in a given year. We use the teacher propensity scores to create a reduced sample of teachers for which we can estimate the effectiveness of the professional development intervention. In effect, this reduced sample creates a treatment and a nontreatment counterfactual for participating teachers to control for the selection bias. We generate two matched samples based on matching each AMSP participant to a nonparticipant using nearest neighbor and caliper matching procedures.<sup><xref ref-type="fn" rid="fn27-0193841X12473304">27</xref></sup> This approach improves the sample by enhancing the ability of the sample to have the property of “overlap” (<xref ref-type="bibr" rid="bibr27-0193841X12473304">Wooldridge 2010</xref>, 910–911): “ … estimating the average treatment effect will require being able to observe both control and treated units for every outcome on [the data].”<sup><xref ref-type="fn" rid="fn28-0193841X12473304">28</xref></sup> The kernel densities of the propensity scores (see <xref ref-type="fig" rid="fig1-0193841X12473304">Figure 1</xref>) show that there is a considerable common support region for participants (treatment) and nonparticipants (control), and matching reduces the sample to equal groups of teachers (not equal groups of students) with extremely similar propensity scores. Examining the balance of the unmatched and matched samples in <xref ref-type="table" rid="table7-0193841X12473304">Table 7</xref> affirms that before matching there are large differences between the treatment and control groups in many of the covariates. When these differences are above what Rosenbaum and Rubin (1985) define as “large” they are considerably reduced in all covariates except for student–teacher ratio, which suggests that there is a difference in this covariate between the two groups. Therefore, the two smaller samples of teachers should be more reliable in estimating the effect of AMSP than the entire sample.</p><fig id="fig1-0193841X12473304" position="float"><label>Figure 1.</label><caption><p>Kernel densities of participation.</p></caption><graphic xlink:href="10.1177_0193841X12473304-fig1.tif"/></fig><p>Finally, with the nearest neighbor and caliper matched samples of teachers based on the estimates of a teacher’s propensity to participate, we are in a position to examine the question of whether the AMSP professional development intervention had an effect on the math teachers’ effectiveness as reflected in their students’ scores. To answer this question, we estimate the Model 1 above but with the two separate matched samples of teachers generated by the propensity scores. In particular, we estimate<disp-formula id="disp-formula4-0193841X12473304"><label>4</label><mml:math id="mml-disp8-0193841X12473304"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mi>S</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mi>A</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo></mml:math><graphic alternate-form-of="disp-formula4-0193841X12473304" xlink:href="10.1177_0193841X12473304-eq4.tif"/></disp-formula></p><p>where all variables are described as in <xref ref-type="disp-formula" rid="disp-formula1-0193841X12473304">Equation 1</xref> but the sample of teachers is now restricted to those generated by the matching procedures.</p></sec><sec id="section6-0193841X12473304"><title>Results</title><p>For our purpose, the primary intention for estimating <xref ref-type="disp-formula" rid="disp-formula2-0193841X12473304">Equation 2</xref> is to capture the teacher fixed effects. The full results of this estimation are summarized in <xref ref-type="table" rid="table4-0193841X12473304">Table 4</xref>. At each of the school levels, teacher fixed effects explain a significant portion of the variance in student performance. Teacher fixed effects explain 19% of the variance in elementary student score changes, 21% of the variance in middle school student score changes, and 17% of the variance in score changes of high school students. These results are consistent with the findings of Rivkin, Hanushek, and Kain (2005), and corroborated by others, that teachers matter in ways that are not easily observable. We are estimating fixed effects for each <italic>t</italic>−1 year of participation, so it is useful to examine the consistency of these estimates from year to year. <xref ref-type="table" rid="table5-0193841X12473304">Table 5</xref> presents the year-to-year comparisons of the teacher fixed effects. The correlation coefficients are all positive and statistically significant suggesting a strong relationship in the year-to-year estimates.<sup><xref ref-type="fn" rid="fn29-0193841X12473304">29</xref></sup></p><table-wrap id="table4-0193841X12473304" position="float"><label>Table 4.</label><caption><p>Predicted Student Achievement Change with Teacher Fixed Effects.</p></caption><graphic alternate-form-of="table4-0193841X12473304" xlink:href="10.1177_0193841X12473304-table4.tif"/><table><thead><tr><th>Independent Variables</th><th>Elementary </th><th>Middle</th><th>High</th></tr></thead><tbody><tr><td>Student Characteristics</td><td/><td/><td/></tr><tr><td> Free and Reduced Price (FRP) Lunch</td><td>−0.018*</td><td>0.009</td><td>−0.009</td></tr><tr><td/><td>(0.011)</td><td>(0.010)</td><td>(0.009)</td></tr><tr><td> Female</td><td>0.027***</td><td>0.023***</td><td>0.016*</td></tr><tr><td/><td>(0.010)</td><td>(0.008)</td><td>(0.010)</td></tr><tr><td> Asian</td><td>0.068*</td><td>0.079*</td><td>0.159***</td></tr><tr><td/><td>(0.036)</td><td>(0.043)</td><td>(0.038)</td></tr><tr><td> Black</td><td>−0.012</td><td>−0.001</td><td>−0.008</td></tr><tr><td/><td>(0.015)</td><td>(0.013)</td><td>(0.013)</td></tr><tr><td> Hispanic</td><td>0.051**</td><td>0.002</td><td>0.092***</td></tr><tr><td/><td>(0.025)</td><td>(0.030)</td><td>(0.038)</td></tr><tr><td> Native American</td><td>0.109</td><td>0.413*</td><td>−0.493***</td></tr><tr><td/><td>(0.167)</td><td>(0.211)</td><td>(0.150)</td></tr><tr><td> Other</td><td>0.056</td><td>0.081**</td><td>0.091**</td></tr><tr><td/><td>(0.036)</td><td>(0.037)</td><td>(0.042)</td></tr><tr><td>School Characteristics</td><td/><td/><td/></tr><tr><td> Class Size</td><td>−0.002**</td><td>0.0002</td><td>0.001</td></tr><tr><td/><td>(0.001)</td><td>(0.0009)</td><td>(0.001)</td></tr><tr><td> Average Experience</td><td>−0.014*</td><td>0.007</td><td>−0.009</td></tr><tr><td/><td>(0.009)</td><td>(0.008)</td><td>(0.007)</td></tr><tr><td> Enrollment (000’s)</td><td>−0.053</td><td>0.148**</td><td>−0.084**</td></tr><tr><td/><td>(0.152)</td><td>(0.071)</td><td>(0.038)</td></tr><tr><td> Percent Master's or Higher</td><td>−0.001</td><td>0.0009</td><td>0.0002</td></tr><tr><td/><td>(0.001)</td><td>(0.0007)</td><td>(0.0005)</td></tr><tr><td> Percent of FRP Students</td><td>−0.097**</td><td>0.060</td><td>−0.037</td></tr><tr><td/><td>(0.039)</td><td>(0.054)</td><td>(0.034)</td></tr><tr><td> Expenditure per Student (000’s)</td><td>0.017**</td><td>−0.0007</td><td>0.026*</td></tr><tr><td/><td>(0.008)</td><td>(0.009)</td><td>(0.015)</td></tr><tr><td> Student-Teacher Ratio</td><td>−0.006</td><td>−0.011**</td><td>0.020***</td></tr><tr><td/><td>(0.008)</td><td>(0.006)</td><td>(0.005)</td></tr><tr><td> Student-Computer Ratio</td><td>−0.003</td><td>−0.004</td><td>0.007</td></tr><tr><td/><td>(0.007)</td><td>(0.007)</td><td>(0.005)</td></tr><tr><td> Math Index</td><td>0.003***</td><td>0.002**</td><td>0.005***</td></tr><tr><td/><td>(0.001)</td><td>(0.0008)</td><td>(0.001)</td></tr><tr><td>Teacher Characteristics</td><td/><td/><td/></tr><tr><td> Highest Degree Obtained</td><td>0.012</td><td>−0.001</td><td>−0.006</td></tr><tr><td/><td>(0.014)</td><td>(0.020)</td><td>(0.012)</td></tr><tr><td> Experience</td><td>−0.009</td><td>−0.010</td><td>−0.003</td></tr><tr><td/><td>(0.014)</td><td>(0.016)</td><td>(0.009)</td></tr><tr><td> Experience Squared</td><td>−0.0001</td><td>−0.0001</td><td>−0.0001</td></tr><tr><td/><td>(0.0003)</td><td>(0.0004)</td><td>(0.0002)</td></tr><tr><td>N(Groups)</td><td>38796(1784)</td><td>45740(610)</td><td>32696(443)</td></tr><tr><td>F-Test </td><td>9.34</td><td>5.92</td><td>5.23</td></tr></tbody></table><table-wrap-foot><fn id="table-fn4-0193841X12473304"><p>Note: Coefficients with robust standard errors in parentheses are presented in the table.</p></fn></table-wrap-foot></table-wrap><table-wrap id="table5-0193841X12473304" position="float"><label>Table 5.</label><caption><p>Descriptive Statistics of Teacher Fixed Effects.</p></caption><graphic alternate-form-of="table5-0193841X12473304" xlink:href="10.1177_0193841X12473304-table5.tif"/><table><thead><tr><th/><th><bold>2003</bold></th><th><bold>2004</bold></th><th><bold>2005</bold></th><th><bold>2006</bold></th><th><bold>2007</bold></th><th><bold>2008</bold></th></tr></thead><tbody><tr><td>Correlation of Current Fixed Effect with Previous Year</td><td>0.635***</td><td>0.714***</td><td>0.812***</td><td>0.917***</td><td>0.794***</td><td>0.883***</td></tr><tr><td>Percentage of Teachers Who move more than Two Deciles in Either Direction</td><td>11.2%</td><td>14.1%</td><td>9.8%</td><td>10.4%</td><td>10.9%</td><td>7.3%</td></tr></tbody></table><table-wrap-foot><fn id="table-fn5-0193841X12473304"><p>Note: Coefficients with robust standard errors in parentheses are presented in the table.</p></fn><fn id="table-fn6-0193841X12473304"><p>*** indicates 0.01 level of significance.</p></fn></table-wrap-foot></table-wrap><p>Using the measure of past teacher effectiveness constructed from these fixed effects, in <xref ref-type="table" rid="table6-0193841X12473304">Table 6</xref> we present the results from estimating the probability that a teacher will choose to participate in AMSP professional development in any given year. The variable of greatest interest for our purposes is past teacher effectiveness, measured here by the lagged teacher fixed effect. The coefficient on the previous effectiveness variable is significant in explaining a teacher’s choice to participate in professional development. The negative sign implies that teachers with lower past effectiveness, as indicated by student scores, are more likely to choose to participate in AMSP professional development. From this perspective alone, it appears that selection of teachers into AMSP was not random. In particular, relatively less effective teachers more often chose to participate in AMSP professional development.</p><table-wrap id="table6-0193841X12473304" position="float"><label>Table 6.</label><caption><p>Predicted Teacher Participation.</p></caption><graphic alternate-form-of="table6-0193841X12473304" xlink:href="10.1177_0193841X12473304-table6.tif"/><table><thead><tr><th>Independent Variables</th><th>Coefficient</th><th><italic>SE</italic></th></tr></thead><tbody><tr><td colspan="3">Teacher characteristics</td></tr><tr><td> Previous effectiveness</td><td>−0.363***</td><td>.078</td></tr><tr><td> Max degree held</td><td>0.028</td><td>.029</td></tr><tr><td> Experience</td><td>0.011</td><td>.014</td></tr><tr><td> Experience squared</td><td>−0.001</td><td>.0004</td></tr><tr><td colspan="3">School characteristics</td></tr><tr><td> Average experience</td><td>0.008</td><td>.016</td></tr><tr><td> Enrollment (000’s)</td><td>−0.145***</td><td>.020</td></tr><tr><td> Percent master’s</td><td>−0.001</td><td>.002</td></tr><tr><td> Percent of FRP students</td><td>0.437***</td><td>.131</td></tr><tr><td> Expenditure per student (000’s)</td><td>−0.145***</td><td>.025</td></tr><tr><td> Student–Teacher Ratio</td><td>0.139***</td><td>.016</td></tr><tr><td> Student–Computer Ratio</td><td>−0.007</td><td>.023</td></tr><tr><td>Math Index</td><td>−0.014***</td><td>.002</td></tr><tr><td><italic>N</italic></td><td align="center">5,830</td><td/></tr><tr><td>Log likelihood</td><td>(−814.25)*** </td><td/></tr></tbody></table><table-wrap-foot><fn id="table-fn10a-0193841X12473304"><p>*Indicates .10 level of significance. **Indicates .05 level of significance. ***Indicates .01 level of significance.</p></fn></table-wrap-foot></table-wrap><table-wrap id="table7-0193841X12473304" position="float"><label>Table 7.</label><caption><p>Balance Statistics of Unmatched and Matched Samples.</p></caption><graphic alternate-form-of="table7-0193841X12473304" xlink:href="10.1177_0193841X12473304-table7.tif"/><table><thead><tr><th rowspan="2">Independent Variables</th><th colspan="3">Standardized Differences<sup>a</sup></th></tr><tr><th>Before Match</th><th>Nearest Neighbor Match</th><th>Caliper Match</th></tr></thead><tbody><tr><td colspan="4">Teacher characteristics</td></tr><tr><td> Previous effectiveness</td><td><bold>31.35</bold></td><td>3.14</td><td>6.33</td></tr><tr><td> Max degree held</td><td>11.43</td><td>6.53</td><td>8.81</td></tr><tr><td> Experience</td><td>1.88</td><td>3.21</td><td>1.88</td></tr><tr><td> Experience squared</td><td>6.39</td><td>1.80</td><td>7.54</td></tr><tr><td colspan="4">School characteristics</td></tr><tr><td> Average experience</td><td><bold>44.39</bold></td><td>13.41</td><td>2.58</td></tr><tr><td> Enrollment (000’s)</td><td>6.49</td><td>1.84</td><td>2.33</td></tr><tr><td> Percent master’s</td><td><bold>26.32</bold></td><td>8.34</td><td>10.97</td></tr><tr><td> Percent of FRP students</td><td><bold>24.02</bold></td><td>4.73</td><td>0.49</td></tr><tr><td> Expenditure per student (000’s)</td><td>9.16</td><td>2.32</td><td>3.30</td></tr><tr><td> Student–teacher ratio</td><td><bold>62.59</bold></td><td><bold>41.14</bold></td><td><bold>42.94</bold></td></tr><tr><td> Student–computer ratio</td><td><bold>23.86</bold></td><td>12.01</td><td>10.48</td></tr><tr><td> Math Index</td><td><bold>32.74</bold></td><td>12.15</td><td>3.89</td></tr><tr><td><italic>n</italic></td><td align="center">7,316</td><td align="center">460</td><td align="center">460</td></tr></tbody></table><table-wrap-foot><fn id="table-fn11-0193841X12473304"><p><italic>Note. </italic><sup>a</sup>Standardized differences are presented as absolute vales.</p></fn><fn id="table-fn12-0193841X12473304"><p>Values in boldface represent “large” standardized differences as suggested by Rosenbaum and Rubin (1985).</p></fn></table-wrap-foot></table-wrap><p>Note that school-level characteristics are also significant in explaining an individual teacher’s selection into the professional development. Teachers from larger schools were less likely to participate. As expected, because of the nature of this program, teachers from schools with a higher percentage of students with free and reduced price lunch were more likely to participate and those in schools with higher expenditures per pupil were less likely to participate. Teachers in schools with higher student teacher ratios were also more likely to participate. Finally, the quality of the school level output as reflected in the math index score of the school influenced participation probabilities. Teachers from better schools are less likely to participate. Taken together, the significant, negative teacher fixed effects and the school characteristics suggest that weaker teachers and those from lower quality schools are most likely to participate in AMSP.<sup><xref ref-type="fn" rid="fn30-0193841X12473304">30</xref></sup> The results provide evidence suggesting that teacher selection into the professional development activities of AMSP was not random. While we offered no prior expectation about the direction of bias influencing selection, it appears that relatively weaker teachers did select into the professional development.</p><p>The results from estimating the effects of AMSP on student score performance presented in <xref ref-type="table" rid="table8-0193841X12473304">Tables 8</xref> and <xref ref-type="table" rid="table9-0193841X12473304">9</xref> represent results from the nearest neighbor and caliper matching approaches, respectively. Teacher participation in the content-based AMSP professional development had a significant and positive effect on elementary and middle school student performance in both models. Using the propensity-derived samples of teachers and matching to their students over the entire time period observed, demonstrates that students of teachers who participated in the AMSP training experienced higher score growth than students whose teachers who did not participate at both the elementary and middle school levels. While payoffs do not appear for high school teachers, the results suggest that the selection bias correction is important when compared to the participation coefficient in the full sample shown in <xref ref-type="table" rid="table3-0193841X12473304">Table 3</xref>.<sup><xref ref-type="fn" rid="fn31-0193841X12473304">31</xref></sup></p><table-wrap id="table8-0193841X12473304" position="float"><label>Table 8.</label><caption><p>Predicted Student Achievement Change of Nearest Neighbor Matched Sample.</p></caption><graphic alternate-form-of="table8-0193841X12473304" xlink:href="10.1177_0193841X12473304-table8.tif"/><table><thead><tr><th>Independent Variables</th><th>Elementary </th><th>Middle</th><th>High</th></tr></thead><tbody><tr><td>Student Characteristics</td><td/><td/><td/></tr><tr><td>Free and Reduced Price (FRP) Lunch</td><td>−0.028*</td><td>0.013</td><td>−0.011</td></tr><tr><td/><td>(0.016)</td><td>(0.011)</td><td>(0.018)</td></tr><tr><td>Female</td><td>0.018</td><td>0.024**</td><td>0.010</td></tr><tr><td/><td>(0.018)</td><td>(0.010)</td><td>(0.016)</td></tr><tr><td>Asian</td><td>0.153</td><td>0.074</td><td>0.274***</td></tr><tr><td/><td>(0.116)</td><td>(0.070)</td><td>(0.083)</td></tr><tr><td>Black</td><td>−0.080*</td><td>−0.007</td><td>0.030</td></tr><tr><td/><td>(0.048)</td><td>(0.027)</td><td>(0.037)</td></tr><tr><td>Hispanic</td><td>−0.001</td><td>−0.012</td><td>0.167**</td></tr><tr><td/><td>(0.005)</td><td>(0.053)</td><td>(0.066)</td></tr><tr><td>Native American</td><td>0.356*</td><td>0.301</td><td>−0.552</td></tr><tr><td/><td>(0.185)</td><td>(0.416)</td><td>(0.477)</td></tr><tr><td>Other</td><td>0.117</td><td>0.129*</td><td>0.050</td></tr><tr><td/><td>(0.140)</td><td>(0.076)</td><td>(0.127)</td></tr><tr><td>School Characteristics</td><td/><td/><td/></tr><tr><td>Class Size</td><td>−0.004***</td><td>0.001</td><td>0.001</td></tr><tr><td/><td>(0.001)</td><td>(0.001)</td><td>(0.001)</td></tr><tr><td>Average Experience</td><td>−0.058***</td><td>0.014***</td><td>−0.001</td></tr><tr><td/><td>(0.010)</td><td>(0.005)</td><td>(0.012)</td></tr><tr><td>Enrollment (000’s)</td><td>−0.091***</td><td>0.298***</td><td>−0.137**</td></tr><tr><td/><td>(0.017)</td><td>(0.090)</td><td>(0.065)</td></tr><tr><td>Percent Master's or Higher</td><td>−0.001</td><td>0.001**</td><td>0.003**</td></tr><tr><td/><td>(0.001)</td><td>(0.0004)</td><td>(0.001)</td></tr><tr><td>Percent of FRP Students</td><td>−0.245***</td><td>0.009</td><td>−0.025</td></tr><tr><td/><td>(0.053)</td><td>(0.028)</td><td>(0.054)</td></tr><tr><td>Expenditure per Student (000’s)</td><td>0.070***</td><td>−0.008</td><td>0.020***</td></tr><tr><td/><td>(0.015)</td><td>(0.010)</td><td>(0.005)</td></tr><tr><td>Student-Teacher Ratio</td><td>−0.007</td><td>−0.026***</td><td>0.020**</td></tr><tr><td/><td>(0.009)</td><td>(0.005)</td><td>(0.010)</td></tr><tr><td>Student-Computer Ratio</td><td>−0.001</td><td>−0.004</td><td>0.026</td></tr><tr><td/><td>(0.017)</td><td>(0.007)</td><td>(0.021)</td></tr><tr><td>Math Index</td><td>0.008***</td><td>0.001*</td><td>0.008***</td></tr><tr><td/><td>(0.001)</td><td>(0.0005)</td><td>(0.002)</td></tr><tr><td>Teacher Characteristics</td><td/><td/><td/></tr><tr><td>Participation in AMSP</td><td>0.079**</td><td>0.070***</td><td>−0.009</td></tr><tr><td/><td>(0.032)</td><td>(0.015)</td><td>(0.028)</td></tr><tr><td>Highest Degree Obtained</td><td>0.015</td><td>−0.001</td><td>0.008</td></tr><tr><td/><td>(0.020)</td><td>(0.010)</td><td>(0.021)</td></tr><tr><td>Experience</td><td>−0.125***</td><td>0.020*</td><td>0.019</td></tr><tr><td/><td>(0.025)</td><td>(0.011)</td><td>(0.020)</td></tr><tr><td>Experience Squared</td><td>0.002**</td><td>−0.001***</td><td>0.0001</td></tr><tr><td/><td>(0.001)</td><td>(0.0003)</td><td>(0.0003)</td></tr><tr><td>N(Groups)</td><td>8275(184)</td><td>14576(132)</td><td>6991(75)</td></tr><tr><td>F-Test </td><td>7.35</td><td>5.45</td><td>5.84</td></tr></tbody></table><table-wrap-foot><fn id="table-fn7-0193841X12473304"><p>Note: Coefficients with robust standard errors in parentheses are presented in the table.</p></fn><fn id="table-fn8-0193841X12473304"><p>*indicates 0.10 level of significance ** indicates 0.05 level of significance, *** indicates 0.01 level of significance.</p></fn></table-wrap-foot></table-wrap><table-wrap id="table9-0193841X12473304" position="float"><label>Table 9.</label><caption><p>Predicted Student Achievement Change of Caliper Matched Sample.</p></caption><graphic alternate-form-of="table9-0193841X12473304" xlink:href="10.1177_0193841X12473304-table9.tif"/><table><thead><tr><th>Independent Variables</th><th>Elementary </th><th>Middle</th><th>High</th></tr></thead><tbody><tr><td>Student Characteristics</td><td/><td/><td/></tr><tr><td>Free and Reduced Price (FRP) Lunch</td><td>−0.022</td><td>0.017</td><td>−0.014</td></tr><tr><td/><td>(0.017)</td><td>(0.012)</td><td>(0.016)</td></tr><tr><td>Female</td><td>0.029*</td><td>0.028***</td><td>0.010</td></tr><tr><td/><td>(0.017)</td><td>(0.010)</td><td>(0.015)</td></tr><tr><td>Asian</td><td>0.023</td><td>0.013</td><td>0.243***</td></tr><tr><td/><td>(0.095)</td><td>(0.073)</td><td>(0.084)</td></tr><tr><td>Black</td><td>−0.013</td><td>−0.015</td><td>0.041</td></tr><tr><td/><td>(0.037)</td><td>(0.029)</td><td>(0.031)</td></tr><tr><td>Hispanic</td><td>−0.044</td><td>−0.067</td><td>0.075</td></tr><tr><td/><td>(0.066)</td><td>(0.056)</td><td>(0.066)</td></tr><tr><td>Native American</td><td>0.330*</td><td>0.596</td><td>−0.561</td></tr><tr><td/><td>(0.190)</td><td>(0.509)</td><td>(0.465)</td></tr><tr><td>Other</td><td>0.023</td><td>−0.033</td><td>−0.070</td></tr><tr><td/><td>(0.111)</td><td>(0.085)</td><td>(0.095)</td></tr><tr><td>School Characteristics</td><td/><td/><td/></tr><tr><td>Class Size</td><td>−0.004***</td><td>0.0003</td><td>−0.001</td></tr><tr><td/><td>(0.001)</td><td>(0.001)</td><td>(0.001)</td></tr><tr><td>Average Experience</td><td>−0.076***</td><td>0.024***</td><td>−0.001</td></tr><tr><td/><td>(0.010)</td><td>(0.006)</td><td>(0.012)</td></tr><tr><td>Enrollment (000’s)</td><td>−0.054**</td><td>0.448***</td><td>−0.032</td></tr><tr><td/><td>(0.022)</td><td>(0.110)</td><td>(0.053)</td></tr><tr><td>Percent Master's or Higher</td><td>0.001</td><td>0.001</td><td>−0.001</td></tr><tr><td/><td>(0.001)</td><td>(0.001)</td><td>(0.001)</td></tr><tr><td>Percent of FRP Students</td><td>−0.272***</td><td>−0.030</td><td>−0.153**</td></tr><tr><td/><td>(0.050)</td><td>(0.031)</td><td>(0.049)</td></tr><tr><td>Expenditure per Student (000’s)</td><td>0.069***</td><td>0.008</td><td>0.031</td></tr><tr><td/><td>(0.012)</td><td>(0.014)</td><td>(0.025)</td></tr><tr><td>Student-Teacher Ratio</td><td>−0.028***</td><td>−0.037***</td><td>0.020***</td></tr><tr><td/><td>(0.008)</td><td>(0.006)</td><td>(0.010)</td></tr><tr><td>Student-Computer Ratio</td><td>−0.001</td><td>−0.012</td><td>0.002</td></tr><tr><td/><td>(0.013)</td><td>(0.007)</td><td>(0.006)</td></tr><tr><td>Math Index</td><td>0.008***</td><td>0.002**</td><td>0.005***</td></tr><tr><td/><td>(0.001)</td><td>(0.0008)</td><td>(0.002)</td></tr><tr><td>Teacher Characteristics</td><td/><td/><td/></tr><tr><td>Participation in AMSP</td><td>0.100***</td><td>0.063***</td><td>−0.002*</td></tr><tr><td/><td>(0.032)</td><td>(0.015)</td><td>(0.001)</td></tr><tr><td>Highest Degree Obtained</td><td>−0.036*</td><td>0.001</td><td>0.024</td></tr><tr><td/><td>(0.021)</td><td>(0.011)</td><td>(0.015)</td></tr><tr><td>Experience</td><td>−0.098***</td><td>0.006</td><td>−0.030*</td></tr><tr><td/><td>(0.020)</td><td>(0.012)</td><td>(0.017)</td></tr><tr><td>Experience Squared</td><td>0.001*</td><td>−0.001***</td><td>0.0001</td></tr><tr><td/><td>(0.001)</td><td>(0.0003)</td><td>(0.0005)</td></tr><tr><td>N(Groups)</td><td>8524(184)</td><td>16743(132)</td><td>7438(76)</td></tr><tr><td>F-Test </td><td>8.42</td><td>6.77</td><td>6.21</td></tr></tbody></table><table-wrap-foot><fn id="table-fn9-0193841X12473304"><p>Note: Coefficients with robust standard errors in parentheses are presented in the table.</p></fn><fn id="table-fn10-0193841X12473304"><p>*indicates 0.10 level of significance ** indicates 0.05 level of significance, *** indicates 0.01 level of significance.</p></fn></table-wrap-foot></table-wrap><p>Other control variables generally behave as expected and are fairly consistent across the models based on the two matched samples. Measured student characteristics such as free and reduced lunch status, gender, and ethnicity are generally unrelated to changes in student outcomes. Female students in middle school, Asian students in high school, and Native American students in elementary school are all positively related to changes in achievement across both models. Many of the measured school characteristics are related to changes in student achievement across both models. However, there are several variables whose effects are counterintuitive. Average experience in elementary school has a negative effect on the change in student achievement while enrollment and student–teacher ratio have a positive effect in middle and high school, respectively. The measured teacher characteristics constant across models are both experience variables in elementary school and experience squared in middle school. The experience variables in the elementary school models suggest that experience is negatively related to student achievement growth but over time the negative effect diminishes. The negative effect of squared experience in middle school suggests diminishing returns to experience over time.</p><p>Taken together, the results from this analysis suggest that the AMSP program targeted weaker teachers because weaker teachers are affiliated with weaker schools. Looking at similar teachers and contrasting those who participated with those that did not, our results suggest the program, on average, did have positive effects for the students of elementary and middle school teachers but not for students of high school teachers. The reasons for the differences across grade levels are unknown although future qualitative work may be informative. But a major contribution of this article was to examine which teachers selected into the in-service professional development and to examine whether acknowledging the type of teachers in the program affects measured treatment effects. In other words, if selection into the program is based on factors that also influence the outcome on which the effectiveness of the program is based, then the results of the uncorrected model will be biased. The AMSP professional development was subscribed to by relatively less effective teachers and controlling for that selection provided an unbiased treatment effect at all school levels.</p></sec><sec id="section7-0193841X12473304"><title>Concluding Comments</title><p>A major current policy question is whether in-service training can improve teacher quality and student outcomes in K–12 schools. This article argues that the evidence on the effectiveness of professional development thus far is less definitive than it should be, in part, because past evaluations have failed to account for the nonrandom selection of teachers into the programs. If high-quality teachers, a priori, are provided with professional development, evidence may suggest an effective treatment even when the measured effect is actually reflecting the fact that the treatment is applied to already high-performing teachers. The reverse holds if less effective teachers are targeted for professional development.</p><p>This article recognizes this potential bias and provides a method for correcting the nonrandom selection into the program. The correction offered in this article is complicated by the fact that teaching effectiveness can be measured only partially with observable factors. By estimating the contribution of unobservable factors on a teacher’s own past effectiveness, we find that this particular program, through targeting poor performing districts, succeeded in attracting the weakest teachers into the professional development activities. Controlling for this, the professional development program shows positive effects. This article provides a road map for future evaluations not only of this program but other teacher professional development training programs in which selection of teachers is likely to be nonrandom and often based on unobservable teacher effectiveness. Only with evaluation replication for a variety of programs can policy advice about designing future professional development be offered.</p></sec></body><back><ack><title>Acknowledgments</title><p>We are grateful to participants at the 2011 meeting of the Association for Education Finance and Policy and to participants at the second IEB Economics of Education Conference, Barcelona, Spain. The authors are extremely grateful to assistance from Emily Bedwell, Terry Hibpshman, Tosha Kurzynske, and Su Troske for data collection, cleaning, and management. We are grateful to John Foster for valuable discussion and input and to Henry Braun for his consultation. We are also grateful to the Kentucky Education Professional Standards Board and local school districts in Kentucky for making the data available.</p></ack><fn-group><fn fn-type="conflict" id="fn32-0193841X12473304"><label>Declaration of Conflicting Interests</label><p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn><fn fn-type="financial-disclosure" id="fn33-0193841X12473304"><label>Funding</label><p>The authors disclosed receipt of the following financial support for the research, authorship and/or publication of this article: This project is supported by the National Science Foundation under grant number DUE-0830716.</p></fn></fn-group><notes><title>Notes</title><fn-group><fn fn-type="other" id="fn1-0193841X12473304"><label>1.</label><p>National Board Certification for in-service teachers is also an alternative route for enhancing teacher quality. See Goldhaber and Anthony (2007) for a description of the program and its effects on teacher quality.</p></fn><fn fn-type="other" id="fn2-0193841X12473304"><label>2.</label><p>For example, the implementation strategy for Race to the Top professional development calls for “data-informed professional development,” so that interventions are based upon teacher needs.</p></fn><fn fn-type="other" id="fn3-0193841X12473304"><label>3.</label><p>Angrist and Lavy (2001) examined a “natural” experiment that was created from a sudden infusion of money into professional development for teachers. They looked at nonreligious schools in Jerusalem and found positive effects for the teachers.</p></fn><fn fn-type="other" id="fn4-0193841X12473304"><label>4.</label><p>See Wong et al. (2008) for a special issue of the <italic>Peabody Journal of Education </italic>devoted to describing the design and implementation of these MSPs. In addition, a special issue of <italic>The Journal of Educational Research &amp; Policy Studies</italic> was devoted to the evaluation of these programs (Moyer-Packenham et al. 2009). See Wong et al. (2009), Dimitrov (2009), and Yin (2009) for analyses of the previous evaluations and some of the best efforts to date to evaluate these programs.</p></fn><fn fn-type="other" id="fn5-0193841X12473304"><label>5.</label><p>In a small number of cases, the sessions focused on content pedagogy.</p></fn><fn fn-type="other" id="fn6-0193841X12473304"><label>6.</label><p>An implication of this article is that we do not yet fully understand the elements necessary for effective professional development because of the methodological issues raised here.</p></fn><fn fn-type="other" id="fn7-0193841X12473304"><label>7.</label><p>In the sample used in this article, approximately 10% of the teachers who participated in professional development were not employed by AMSP districts. </p></fn><fn fn-type="other" id="fn8-0193841X12473304"><label>8.</label><p>The state does not systematically monitor the effectiveness of these alternative PD activities nor does it centrally collect information about the specific types of activities chosen by the teachers.</p></fn><fn fn-type="other" id="fn9-0193841X12473304"><label>9.</label><p>An earlier article looked at school-level data for all Kentucky public elementary and secondary schools from the school year 2001–2002 through 2005–2006 (Foster, Toma, and Troske 2013). That article addressed the issue of which particular school districts agreed to participate in AMSP and the subsequent effectiveness of the participation. School-level data are useful for identifying school-level effects from a professional development intervention, but they contain possible aggregation bias. A preferred measurement of the effectiveness of an individual teacher’s participation in any professional development activity involves matching students to teachers and comparing the effectiveness of those teachers who participated to that of those teachers who did not participate while controlling for other observable and unobservable factors that may influence student outcomes.</p></fn><fn fn-type="other" id="fn10-0193841X12473304"><label>10.</label><p>The different testing systems of the states and the different schedule of tests pose special and interesting econometric challenges for combining the data across the three states.</p></fn><fn fn-type="other" id="fn11-0193841X12473304"><label>11.</label><p>The Appalachian districts were chosen because they were the target of the AMSP PD activities. This evaluation did not accompany the AMSP but followed it—adding to the challenges of data collection.</p></fn><fn fn-type="other" id="fn12-0193841X12473304"><label>12.</label><p>The payoff to the districts for providing data is a promise that we will share our results about teacher effectiveness with them. The cost to participation for the district is staff time required to work with personnel from this project to extract the data. Ten school districts in eastern Kentucky provided useable data for this project. The study required matching data from several state-level administrative data bases as well as class roster data from individual school districts. The data were district-based and from a variety of state agencies and none were developed with the idea of being used for evaluation purposes. Much of the matching required name and birth date or other person-specific characteristics.</p></fn><fn fn-type="other" id="fn13-0193841X12473304"><label>13.</label><p>In particular, we look at math score changes between fifth and sixth grade and between eighth and ninth for years prior to 2006. Post-2006, we use all Grades 4 through 8. We calculate 11th-grade changes for these years, but the interval gap in testing makes these results less reliable.</p></fn><fn fn-type="other" id="fn14-0193841X12473304"><label>14.</label><p>This conclusion has been affirmed by officials of the KDE.</p></fn><fn fn-type="other" id="fn15-0193841X12473304"><label>15.</label><p>Ballou (2009) summarizes the many issues associated with scaling and the use of standardized test scores. An alternative to <italic>z</italic>-scores is to use ranks of students on exams. Friedman (1937) demonstrates that ranks are a linear transformation of the data and do not make distributional assumptions about the data. Furthermore, estimates using ranks converge to those using <italic>z</italic>-scores when the assumption of normality is valid and the number of ranks is sufficiently large. Ranks are well defined for any scale and comparable as long as the tests have validity for the same ability measure. We used ranks also to estimate the models presented in this article and the results are qualitatively the same as with <italic>z</italic>-scores. These results are available upon request from the authors.</p></fn><fn fn-type="other" id="fn16-0193841X12473304"><label>16.</label><p>Kentucky has traditionally had small, county-based school districts with additional districts for the county seats (typically the largest town) in many cases. Nine of the 10 districts in this sample are county-based. One of the advantages of this study is its focus on professional development in rural schools.</p></fn><fn fn-type="other" id="fn17-0193841X12473304"><label>17.</label><p>As noted earlier, all Kentucky teachers are required to participate in a minimum of 4 days of PD activities annually. The quality of the data on the alternative PD types varies considerably by district and there is no state collection of these data.</p></fn><fn fn-type="other" id="fn18-0193841X12473304"><label>18.</label><p>An alternative model specification is to use the level of score at the end of the period <italic>t</italic> as the dependent variable and the prior year (or beginning of period) score as an independent variable. Estimates from this specification are available upon request.</p></fn><fn fn-type="other" id="fn19-0193841X12473304"><label>19.</label><p>The grade level is not listed because math exams and year (<italic>t</italic>) are always associated with certain grade levels.</p></fn><fn fn-type="other" id="fn20-0193841X12473304"><label>20.</label><p>The full table of results is available upon request.</p></fn><fn fn-type="other" id="fn21-0193841X12473304"><label>21.</label><p>A standard approach is to employ an instrumental variable and test for endogeneity using the Wu-Hausman Test. However, we considered and tested several instruments but were unable to identify an instrument that was correlated with participation and not student outcomes. </p></fn><fn fn-type="other" id="fn22-0193841X12473304"><label>22.</label><p>Davidson and MacKinnon (1993, 187) point out that a test of endogeneity can be constructed using the correlation of possibly endogenous variables and residuals constructed without them (their formula 7.57 and subsequent discussion). In our case, there is one endogenous variable, so there is just one correlation. As they state, this is a simpler procedure than many available to test endogeneity.</p></fn><fn fn-type="other" id="fn23-0193841X12473304"><label>23.</label><p>The full results for the models estimated in <xref ref-type="table" rid="table3-0193841X12473304">Table 3</xref> are available upon request.</p></fn><fn fn-type="other" id="fn24-0193841X12473304"><label>24.</label><p>In an experimental setting, where treatment was randomized, the methods we use below would not be necessary.</p></fn><fn fn-type="other" id="fn25-0193841X12473304"><label>25.</label><p>The equations are estimated for these years only because these are when a teacher could decide to participate in the PD.</p></fn><fn fn-type="other" id="fn26-0193841X12473304"><label>26.</label><p>We model the participation decision as a yes or no. In reality, the teacher could choose among activities with varying hours. The propensity scores that we generate from this probit estimate are correlated at the .01 level of significance with the number of hours of participation. The hours of participation were clustered and not normally distributed.</p></fn><fn fn-type="other" id="fn27-0193841X12473304"><label>27.</label><p>The nearest neighbor matching involves sorting the data according to propensity scores and then matching a participant to the nearest nonparticipant with the most similar propensity score. The caliper matching method involves sorting the data according to propensity score and then choosing a nonparticipant randomly within at least .2 standard deviations of the propensity score from the participant.</p></fn><fn fn-type="other" id="fn28-0193841X12473304"><label>28.</label><p>This assumption is typically called the overlap assumption. Overlap means that, for any setting of the covariates in the assumed population, there is a chance of seeing units in both the control and the treatment groups. The use of propensity scores is described in detail by <xref ref-type="bibr" rid="bibr27-0193841X12473304">Wooldridge (2010</xref>, 909–915).</p></fn><fn fn-type="other" id="fn29-0193841X12473304"><label>29.</label><p>Another concern of estimating the teacher fixed effects is large year-to-year movement relative to the sample group of teachers. While it is not impossible for teachers to have significant gains or losses relative to the sample group from year to year, a large number of teachers doing so may cause concern about the legitimacy of the estimated effects. <xref ref-type="table" rid="table5-0193841X12473304">Table 5</xref> suggests that this movement classifies a high of 14.1% of all teachers in 2004 to a low of 7.3% in 2008.</p></fn><fn fn-type="other" id="fn30-0193841X12473304"><label>30.</label><p>We also ran the probit model using teacher effectiveness measured in deciles. The results support those reported here. Teachers in the lower deciles (2, 3, and 4) were more likely to participate while those in the 9th and 10th deciles (highest and best) were significantly less likely to participate in the professional development activities.</p></fn><fn fn-type="other" id="fn31-0193841X12473304"><label>31.</label><p>It is possible that the larger time gaps in high school testing explain the lack of significance in high school results.</p></fn></fn-group></notes><ref-list><title>References</title><ref id="bibr1-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Angrist</surname><given-names>J.</given-names></name><name><surname>Lavy</surname><given-names>V.</given-names></name></person-group>. <year>2001</year>. <article-title>“Does Teacher Training Affect Pupil Learning? Evidence from Matched Comparisons in Jerusalem Public Schools.”</article-title> <source>Journal of Labor Economics</source> <volume>19</volume>:<fpage>343</fpage>–<lpage>69</lpage>.</citation></ref><ref id="bibr2-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>D. L.</given-names></name><name><surname>Cohen</surname><given-names>D. K.</given-names></name></person-group>. <year>1999</year>. <article-title>“Developing Practice, Developing Practitioners: Toward a Practice-Based Theory of Professional Education</article-title>. In <source>Teaching as the Learning Profession</source>, edited by <person-group person-group-type="editor"><name><surname>Darling-Hammond</surname><given-names>L.</given-names></name><name><surname>Sykes</surname><given-names>G.</given-names></name></person-group>, <fpage>3</fpage>–<lpage>31</lpage>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation></ref><ref id="bibr3-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>D. L.</given-names></name><name><surname>Hill</surname><given-names>H. C.</given-names></name></person-group>. <year>2009</year>. <article-title>“The Curious-and Crucial-case of Mathematics Knowledge for Teaching.”</article-title> <source>Phi Delta Kappan</source> <volume>91</volume>:<fpage>68</fpage>–<lpage>71</lpage>.</citation></ref><ref id="bibr4-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ballou</surname><given-names>D.</given-names></name></person-group> <year>2009</year>. <article-title>“Test Scaling and Value-added Measurement.”</article-title> <source>Education Finance and Policy</source> <volume>4</volume>:<fpage>351</fpage>–<lpage>83</lpage>.</citation></ref><ref id="bibr5-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>D.</given-names></name><name><surname>Grossman</surname><given-names>P.</given-names></name><name><surname>Lankford</surname><given-names>H.</given-names></name><name><surname>Loeb</surname><given-names>S.</given-names></name><name><surname>Wyckoff</surname><given-names>J.</given-names></name></person-group>. <year>2005</year>. <article-title>“How Changes in Entry Requirements Alter the Teacher Workforce and Affect Student Achievement.”</article-title> <comment>NBER Working Paper Series. Working paper 11844, National Bureau of Economic Research, Cambridge, MA</comment>.</citation></ref><ref id="bibr6-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Clotfelter</surname><given-names>C.</given-names></name><name><surname>Ladd</surname><given-names>H.</given-names></name><name><surname>Vigdor</surname><given-names>J.</given-names></name></person-group>. <year>2007</year>. <article-title>“How and Why Do Teacher Credentials Matter for Student Achievement?”</article-title> <comment>Calder Working Paper #2</comment>.</citation></ref><ref id="bibr7-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cowen</surname><given-names>J.</given-names></name><name><surname>Butler</surname><given-names>J. S.</given-names></name><name><surname>Fowles</surname><given-names>J.</given-names></name><name><surname>Streams</surname><given-names>M.</given-names></name><name><surname>Toma</surname><given-names>E.</given-names></name></person-group>. <year>2012</year>. <article-title>“Teacher Retention in Appalachian Schools: Evidence From Kentucky.”</article-title> <source>Economics of Education Review</source> <volume>31</volume>:<fpage>431</fpage>–<lpage>41</lpage>.</citation></ref><ref id="bibr8a-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>R.</given-names></name><name><surname>MacKinnon</surname><given-names>J.</given-names></name></person-group>. <year>1993</year>. <source>Estimation and Inference in Econometrics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation></ref><ref id="bibr8-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>L. M.</given-names></name></person-group> <year>2009</year>. <article-title>“Improving Impact Studies of Teachers’ Professional Development: Toward Better Conceptualizations and Measures.”</article-title> <source>Educational Researcher</source> <volume>38</volume>:<fpage>181</fpage>–<lpage>99</lpage>.</citation></ref><ref id="bibr9-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>L. M.</given-names></name><name><surname>Porter</surname><given-names>A. C.</given-names></name><name><surname>Garet</surname></name></person-group>. <year>2002</year>. <article-title>“Effects of Professional Development on Teachers’ Instruction: Results from a Three-year Longitudinal Study.”</article-title> <source>Educational Evaluation and Policy Analysis</source> <volume>24</volume>:<fpage>81</fpage>–<lpage>112</lpage>.</citation></ref><ref id="bibr10-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dimitrov</surname><given-names>D.</given-names></name></person-group> <year>2009</year>. <article-title>“Intermediate Trends in Math and Science Partnership-Related Changes in Student Achievement with Management Information System Data.”</article-title> <source>The Journal of Education Research &amp; Policy Studies</source> <volume>9</volume>:<fpage>97</fpage>–<lpage>138</lpage>.</citation></ref><ref id="bibr11-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>J.</given-names></name><name><surname>Toma</surname><given-names>E.</given-names></name><name><surname>Troske</surname><given-names>S.</given-names></name></person-group>. <year>2013</year>. <article-title>“Does Teacher Professional Development Improve Math and Science Outcomes and Is It Cost Effective?”</article-title> <source>Journal of Education Finance</source> <comment>38:255–75</comment>.</citation></ref><ref id="bibr12-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Fowles</surname><given-names>J.</given-names></name><name><surname>Butler</surname><given-names>J. S.</given-names></name><name><surname>Cowen</surname><given-names>J.</given-names></name><name><surname>Streams</surname><given-names>M.</given-names></name><name><surname>Toma</surname><given-names>E.</given-names></name></person-group>. <year>2012</year>. <article-title>“Public Employee Quality in a Geographic Context: A Study of Rural Teachers.”</article-title> <comment>University of Kentucky Working Paper</comment>.</citation></ref><ref id="bibr13-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>M.</given-names></name></person-group> <year>1937</year>. <article-title>“The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance.”</article-title> <source>Journal of the American Statistical Association</source> <volume>32</volume>:<fpage>675</fpage>–<lpage>701</lpage>.</citation></ref><ref id="bibr14-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Fullan</surname><given-names>M.</given-names></name><name><surname>Hargreaves</surname><given-names>A.</given-names></name></person-group>. <year>1996</year>. <source>What’s Worth Fighting For in Your School?</source> <publisher-loc>New York</publisher-loc>: <publisher-name>Teachers College Press</publisher-name>.</citation></ref><ref id="bibr15-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Garet</surname><given-names>M. S.</given-names></name><name><surname>Porter</surname><given-names>A.</given-names></name><name><surname>Desimone</surname><given-names>L.</given-names></name><name><surname>Birman</surname><given-names>B.</given-names></name><name><surname>Yoon</surname><given-names>K.</given-names></name></person-group>. <year>2001</year>. <article-title>“What Makes Professional Development Effective? Results from a National Sample of Teachers.”</article-title> <source>American Educational Research Journal</source> <volume>38</volume>:<fpage>915</fpage>–<lpage>45</lpage>.</citation></ref><ref id="bibr16-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Garet</surname><given-names>M. S.</given-names></name><name><surname>Cronen</surname><given-names>S.</given-names></name><name><surname>Eaton</surname><given-names>M.</given-names></name><name><surname>Kurki</surname><given-names>A.</given-names></name><name><surname>Ludwig</surname><given-names>M.</given-names></name><name><surname>Jones</surname><given-names>W.</given-names></name><name><surname>Uekawa</surname><given-names>K.</given-names></name><name><surname>Falk</surname><given-names>A.</given-names></name><name><surname>Bloom</surname><given-names>H.</given-names></name><name><surname>Doolittle</surname><given-names>F.</given-names></name><name><surname>Zhu</surname><given-names>P.</given-names></name><name><surname>Sztejnber</surname><given-names>L.</given-names></name></person-group>. <year>2008</year>. <source>The Impact of Two Professional Development Interventions on Early Reading Instruction and Achievement</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education, NCEE</publisher-name> <fpage>2008</fpage>–<lpage>4031</lpage>.</citation></ref><ref id="bibr17-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Goldhaber</surname><given-names>D.</given-names></name><name><surname>Anthony</surname><given-names>E.</given-names></name></person-group>. <year>2007</year>. <article-title>“Can Teacher Quality be Effectively Assessed?”</article-title> <source>The Review of Economics and Statistics</source> <volume>89</volume>:<fpage>134</fpage>–<lpage>50</lpage>.</citation></ref><ref id="bibr19-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>H. C.</given-names></name><name><surname>Rowan</surname><given-names>B.</given-names></name><name><surname>Ball</surname><given-names>D. L.</given-names></name></person-group>. <year>2005</year>. <article-title>“Effects of Teachers’ Mathematical Knowledge for Teaching on Student Achievement.”</article-title> <source>American Education Research Journal</source> <volume>42</volume>:<fpage>371</fpage>–<lpage>406</lpage>.</citation></ref><ref id="bibr18-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>H. C.</given-names></name><name><surname>Ball</surname><given-names>D. L.</given-names></name></person-group>. <year>2004</year>. <article-title>“Learning Mathematics for Teaching: Results from California’s Mathematics Professional Development Institutes.”</article-title> <source>Journal for Research in Mathematics Education</source> <volume>35</volume>:<fpage>330</fpage>–<lpage>51</lpage>.</citation></ref><ref id="bibr20-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Loucks-Horsley</surname><given-names>S.</given-names></name><name><surname>Stiles</surname><given-names>K. E.</given-names></name><name><surname>Mundry</surname><given-names>S.</given-names></name><name><surname>Love</surname><given-names>N.</given-names></name><name><surname>Hewson</surname><given-names>P. W.</given-names></name></person-group>. <year>2010</year>. <source>Designing Professional Development for Teachers of Science and Mathematics</source>, <edition>3rd ed</edition>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin</publisher-name>.</citation></ref><ref id="bibr21-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Moyer-Packenham</surname><given-names>P.</given-names></name><name><surname>Yin</surname><given-names>R. K.</given-names></name><name><surname>Wong</surname><given-names>K.</given-names></name><name><surname>Davis</surname><given-names>D.</given-names></name></person-group>, eds. <year>2009</year>. <article-title>“Special Issue: Math and Science Partnership Program Evaluation.”</article-title> <source>The Journal of Educational Research &amp; Policy Studies</source> <volume>9:1–161</volume>.</citation></ref><ref id="bibr22-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rivkin</surname><given-names>S.</given-names></name><name><surname>Hanushek</surname><given-names>E.</given-names></name><name><surname>Kain</surname><given-names>J.</given-names></name></person-group>. <year>2005</year>. <article-title>“Teachers, Schools and Academic Achievement.”</article-title> <source>Econometrica</source> <volume>73</volume>:<fpage>417</fpage>–<lpage>58</lpage>.</citation></ref><ref id="bibr23-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rosenbaum</surname><given-names>P.</given-names></name><name><surname>Rubin</surname><given-names>D.</given-names></name></person-group>. <year>1985</year>. <article-title>“Constructing a Control Group Using Multivariate Matched Sampling Methods that Incorporate the Propensity Score.”</article-title> <source>American Statistician</source> <volume>3</volume>:<fpage>33</fpage>–<lpage>38</lpage>.</citation></ref><ref id="bibr24-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wayne</surname><given-names>A.</given-names></name><name><surname>Yoon</surname><given-names>K. S.</given-names></name><name><surname>Zhu</surname><given-names>P.</given-names></name><name><surname>Cronen</surname><given-names>S.</given-names></name><name><surname>Garet</surname><given-names>M.</given-names></name></person-group>. <year>2008</year>. <article-title>“Experimenting with Teacher Professional Development: Motive and Methods.”</article-title> <source>Educational Researcher</source> <volume>37</volume>:<fpage>469</fpage>–<lpage>79</lpage>.</citation></ref><ref id="bibr25-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>K.</given-names></name><name><surname>Boven</surname><given-names>M.</given-names></name><name><surname>Kim</surname><given-names>C.</given-names></name><name><surname>Socha</surname><given-names>T.</given-names></name></person-group>. <year>2009</year>. <article-title>“Comparison of MSP and Non-MSP Schools in Six States.”</article-title> <comment>The Journal of Educational Research &amp; Policy Studies</comment> <volume>9</volume>:<fpage>73</fpage>–<lpage>96</lpage>.</citation></ref><ref id="bibr26-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>K. K.</given-names></name><name><surname>Yin</surname><given-names>R. K.</given-names></name><name><surname>Moyer-Packenham</surname><given-names>P.</given-names></name><name><surname>Scherer</surname><given-names>J.</given-names></name></person-group>. <year>2008</year>. <article-title>“Introduction to the MSP-PE Special Issue on Math and Science Partnership Program: A First Comprehensive Evaluation.”</article-title> <comment>Peabody Journal of Education</comment> <volume>83</volume>:<fpage>479</fpage>–<lpage>85</lpage>.</citation></ref><ref id="bibr27-0193841X12473304"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Wooldridge</surname><given-names>Jeffrey N.</given-names></name></person-group> <year>2010</year>. <article-title>Econometric Analysis of Cross Section and Panel Data</article-title>, <edition>2nd ed</edition>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>The MIT Press</publisher-name>.</citation></ref><ref id="bibr28-0193841X12473304"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>R.</given-names></name></person-group> <year>2009</year>. <article-title>“Student Achievement Data and Findings, as Reported in MSPs’ Annual and Evaluation Reports.”</article-title> <comment>The Journal of Educational Research &amp; Policy Studies</comment> <volume>9</volume>:<fpage>139</fpage>–<lpage>61</lpage>.</citation></ref></ref-list></back></article>