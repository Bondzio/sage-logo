<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HFS</journal-id>
<journal-id journal-id-type="hwp">sphfs</journal-id>
<journal-title>Human Factors: The Journal of Human Factors and Ergonomics Society</journal-title>
<issn pub-type="ppub">0018-7208</issn>
<issn pub-type="epub">1547-8181</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0018720812440279</article-id>
<article-id pub-id-type="publisher-id">10.1177_0018720812440279</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Methods for the Analysis of Communication Special Section</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>(Non)cooperative Dialogues</article-title>
<subtitle>The Role of Emotions</subtitle>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Cooke</surname><given-names>Nancy J.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Duchon</surname><given-names>Andrew</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Gorman</surname><given-names>Jamie C.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Keyton</surname><given-names>Joann</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Miller</surname><given-names>Anne</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Cavicchio</surname><given-names>Federica</given-names></name>
<aff id="aff1-0018720812440279">Università di Trento, Rovereto, Italy</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Poesio</surname><given-names>Massimo</given-names></name>
<aff id="aff2-0018720812440279">University of Essex, Essex, United Kingdom, and Università di Trento, Rovereto, Italy</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0018720812440279">Federica Cavicchio, c/o CIMeC, Center for Mind/Brain Sciences, Università di Trento, Corso Bettini 31, 38068, Rovereto (Tn), Italy; <email>federica.cavicchio@unitn.it</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2012</year>
</pub-date>
<volume>54</volume>
<issue>4</issue>
<issue-title>Special Section: Methods for the Analysis of Communication</issue-title>
<fpage>546</fpage>
<lpage>559</lpage>
<history>
<date date-type="received">
<day>10</day>
<month>1</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>1</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>Copyright © 2012, Human Factors and Ergonomics Society</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<sec id="section1-0018720812440279">
<title>Objective:</title>
<p>The effect of emotion on (non)co-operation in unscripted, ecological communication is investigated.</p>
</sec>
<sec id="section2-0018720812440279">
<title>Background:</title>
<p>The participants in an interaction are generally cooperative in that, for instance, they tend to reduce the chance of misunderstandings in communication. However, it is also clear that cooperation is not complete. Positive and negative emotional states also appear to be connected to the participants’ commitment to cooperate or not, respectively. So far, however, it has proven remarkably difficult to test this because of the lack of entirely objective measurements of both cooperation levels and emotional responses.</p>
</sec>
<sec id="section3-0018720812440279">
<title>Method:</title>
<p>In this article, the authors present behavioral methods and coding schemes for analyzing cooperation and (surface) indicators of emotions in face-to-face interactions and show that they can be used to study the correlation between emotions and cooperation effectively.</p>
</sec>
<sec id="section4-0018720812440279">
<title>Results:</title>
<p>The authors observed large negative correlations between heart rate and cooperation, and a group of facial expressions was found to be predictive of the level of cooperation of the speakers.</p>
</sec>
<sec id="section5-0018720812440279">
<title>Conclusion:</title>
<p>It is possible to develop reliable methods to code for cooperation, and with such coding schemes it is possible to confirm the commonsense prediction that noncooperative behavior by a conversational participant affects the other participant in ways that can be measured quantitatively.</p>
</sec>
<sec id="section6-0018720812440279">
<title>Application:</title>
<p>These results shed light on an aspect of interaction that is crucial to building adaptive systems able to measure cooperation and to respond to the user’s affective states. The authors expect their methods to be applicable to building and testing such interaction systems.</p>
</sec>
</abstract>
<kwd-group>
<kwd>multimodal communication</kwd>
<kwd>face-to-face interaction</kwd>
<kwd>multimodal coding scheme</kwd>
<kwd>coding scheme validation</kwd>
<kwd>pragmatics</kwd>
<kwd>facial expressions of emotions</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section7-0018720812440279" sec-type="intro">
<title>Introduction</title>
<p>It is widely accepted that for the field of human–computer interaction (henceforth HCI) to develop anticipatory user interfaces that are human centered and based on naturally occurring multimodal human communication, it will be necessary to develop methods to understand and emulate behavioral cues such as affective and social signals (<xref ref-type="bibr" rid="bibr6-0018720812440279">Bianchi &amp; Lisetti, 2002</xref>). <xref ref-type="bibr" rid="bibr71-0018720812440279">Picard (1997)</xref> listed several applications where it is beneficial for computers to recognize human affective states: For example, recognizing the user’s emotions, the computer can become a more effective tutor and could learn the user’s preferences. Here, we are concerned with one specific effect of emotions—their impact on the level of cooperation of an agent—and with developing methods to analyze both cooperation and emotions that may be used in studying such effects.</p>
<p>The philosopher H. P. Grice was perhaps the first to point out the extent to which our ability to communicate effectively depends on speakers acting cooperatively (<xref ref-type="bibr" rid="bibr47-0018720812440279">Grice, 1975</xref>). This tendency to cooperation has been a key tenet of subsequent theorizing in pragmatics such as the work of Bratman and Tuomela in philosophy (<xref ref-type="bibr" rid="bibr10-0018720812440279">Bratman, 1987</xref>; <xref ref-type="bibr" rid="bibr87-0018720812440279">Tuomela, 2000</xref>), the highly influential work of Clark in psycholinguistics (e.g., <xref ref-type="bibr" rid="bibr11-0018720812440279">Brennan &amp; Clark, 1996</xref>; <xref ref-type="bibr" rid="bibr23-0018720812440279">Clark, 1996</xref>; <xref ref-type="bibr" rid="bibr24-0018720812440279">Clark &amp; Brennan, 1991</xref>; <xref ref-type="bibr" rid="bibr25-0018720812440279">Clark &amp; Krych, 2004</xref>; <xref ref-type="bibr" rid="bibr26-0018720812440279">Clark &amp; Schaefer, 1987</xref>; <xref ref-type="bibr" rid="bibr80-0018720812440279">Schober &amp; Clark, 1989</xref>), and work on intelligent agents in artificial intelligence (see, e.g., the articles in <xref ref-type="bibr" rid="bibr27-0018720812440279">Cohen, Morgan, &amp; Pollack, 1990</xref>). However, Grice’s early work already highlighted the difficulty of developing operational measures of cooperation, particularly in communication. And in recent years we have witnessed a reassessment of the extent to which people actually cooperate (<xref ref-type="bibr" rid="bibr48-0018720812440279">Grice, 1989</xref>).</p>
<p>Emotions are one of the factors shown to be an important predictor of (non)cooperation, for example, in studies based on the game theory. For instance, <xref ref-type="bibr" rid="bibr72-0018720812440279">Pillutla and Murnighan (1996)</xref> measured the feelings of respondents when confronted with unfair offers to predict their tendency to reject the offer and found that anger was positively correlated with the tendency to not cooperate. Other researchers showed that when respondents were treated unfairly, they felt not just anger but also sadness, irritation, and contempt (<xref ref-type="bibr" rid="bibr92-0018720812440279">Xiao &amp; Hauser, 2005</xref>). Many studies have claimed that cooperators can be identified by honest and nonfalsifiable emotive facial expressions such as Duchenne smiles, allowing for mutual selection among cooperators (<xref ref-type="bibr" rid="bibr8-0018720812440279">Boone &amp; Buck, 2003</xref>; <xref ref-type="bibr" rid="bibr13-0018720812440279">Brown, Palameta, &amp; Moore, 2003</xref>; <xref ref-type="bibr" rid="bibr57-0018720812440279">Krumhuber et al., 2007</xref>; <xref ref-type="bibr" rid="bibr61-0018720812440279">Matsumoto &amp; Willingham, 2006</xref>; <xref ref-type="bibr" rid="bibr62-0018720812440279">Mehu, Little, &amp; Dunbar, 2007</xref>; <xref ref-type="bibr" rid="bibr63-0018720812440279">Oda, Yamagata, Yabiku, &amp; Matsumoto-Oda, 2009</xref>). (Duchenne smiles involve the innervations of the orbicularis occuli, a facial muscle surrounding the eyes that is difficult to intentionally control, and have been empirically demonstrated to correlate with the experience of positive emotion [<xref ref-type="bibr" rid="bibr37-0018720812440279">Ekman &amp; Friesen, 1982</xref>; <xref ref-type="bibr" rid="bibr40-0018720812440279">Frank &amp; Ekman, 1993</xref>; <xref ref-type="bibr" rid="bibr41-0018720812440279">Frank et al., 1993]</xref>.) These findings suggested that cooperative individuals display higher levels of positive emotions than noncooperators, although the situation in which emotions are displayed may determine the degree to which they reflect cooperative disposition (<xref ref-type="bibr" rid="bibr61-0018720812440279">Matsumoto &amp; Willingham, 2006</xref>; <xref ref-type="bibr" rid="bibr62-0018720812440279">Mehu et al., 2007</xref>). However, these studies used ultimatum and/or trust games to trigger cooperation and noncooperation in participants.</p>
<p>Task-oriented dialogues are an effective methodology to investigate the relationship between cooperation and emotion in naturalistic dialogues, as they are likely to elicit unscripted dialogues without losing control of the context. In this work, we investigate the relationship between cooperation and emotion by collecting and analyzing task-oriented dialogues. A key challenge in doing this is to develop sound methodologies to analyze and encode those factors.</p>
<p>For the aspect of our work that is concerned with cooperation, we build on the work by <xref ref-type="bibr" rid="bibr28-0018720812440279">Davies (1998</xref>, <xref ref-type="bibr" rid="bibr29-0018720812440279">2006</xref>, <xref ref-type="bibr" rid="bibr30-0018720812440279">2007</xref>), who was the first to develop and test a coding scheme to measure the level of cooperation in a task dialogue. Other coding schemes, such as <xref ref-type="bibr" rid="bibr91-0018720812440279">Walton and Krabbe’s (1995)</xref> noncooperative features analysis and <xref ref-type="bibr" rid="bibr85-0018720812440279">Traum and Allen’s (1994)</xref> discourse obligations, did not determine interannotator agreement of their coding scheme. Davies’s coding scheme weights the level of effort that participants embark on with their utterances. Her system provided a positive and negative score for each dialogue move with respect to the effort involved. When an instance of a cooperative behavior was found, a positive coding was attributed with respect to the involved effort level (e.g., the repetition of an instruction had an attribute code of 3). Conversely, a negative coding was attributed when an instance where a particular behavior should have been used was not found (e.g., an <italic>inappropriate reply</italic>, the failure to introduce useful information when required, was coded as −3). Reliability tests for the coding scheme (<xref ref-type="bibr" rid="bibr28-0018720812440279">Davies, 1998</xref>) had kappa scores ranging from .69 to 1.0, but Davies remarked that agreement between coders was not significant for some of the features because they were seldom present in her corpus. One of our objectives in this work is to develop a highly reliable coding scheme for cooperation.</p>
<p>Emotion coding has proven equally challenging. There is a vast body of literature on affective computing and emotion coding (<xref ref-type="bibr" rid="bibr1-0018720812440279">Abrilian, Devillers, Buisine, &amp; Martin, 2005</xref>; <xref ref-type="bibr" rid="bibr16-0018720812440279">Callejas &amp; López-Cózar, 2008</xref>; <xref ref-type="bibr" rid="bibr31-0018720812440279">Devillers &amp; Martin, 2010</xref>; <xref ref-type="bibr" rid="bibr59-0018720812440279">Martin, Caridakis, Devillers, Karpouzis, &amp; Abrilian, 2006</xref>; <xref ref-type="bibr" rid="bibr89-0018720812440279">Vidrascu &amp; Devillers, 2007</xref>) and recognition (see, e.g., <xref ref-type="bibr" rid="bibr66-0018720812440279">Paiva, Prada, &amp; Picard, 2007</xref>; <xref ref-type="bibr" rid="bibr68-0018720812440279">Pantic &amp; Rothkrantz, 2000</xref>; <xref ref-type="bibr" rid="bibr70-0018720812440279">Pantic, Sebe, Cohn, &amp; Huang, 2005</xref>). In the past three decades, the theoretical analysis of emotions has shifted from discrete or basic emotion theories (<xref ref-type="bibr" rid="bibr33-0018720812440279">Ekman, 1984</xref>; <xref ref-type="bibr" rid="bibr51-0018720812440279">Izard, 1993</xref>; <xref ref-type="bibr" rid="bibr75-0018720812440279">Russell &amp; Barrett, 1999</xref>) to dynamic architectural frameworks (<xref ref-type="bibr" rid="bibr44-0018720812440279">Frijda, 1986</xref>, <xref ref-type="bibr" rid="bibr45-0018720812440279">2009</xref>; <xref ref-type="bibr" rid="bibr76-0018720812440279">Scherer, 1984</xref>, <xref ref-type="bibr" rid="bibr77-0018720812440279">2001</xref>, <xref ref-type="bibr" rid="bibr78-0018720812440279">2009</xref>; <xref ref-type="bibr" rid="bibr79-0018720812440279">Scherer &amp; Ceschi, 2000</xref>). Although the former were based on the study of a limited number of innate, hardwired affect programs for basic emotions, such as anger, fear, joy, sadness, and disgust, leading to prototypical response patterns, the latter are built on appraisal and motivational changes.</p>
<p>The appraisal mechanism requires interaction among many cognitive functions (i.e., to compare the features of stimulus events, to retrieve representation in memory, to respond at motivational urges) and their underlying neural circuits (<xref ref-type="bibr" rid="bibr78-0018720812440279">Scherer, 2009</xref>). In addition, the appraisal model of emotions relies heavily on implicit or explicit computation of probabilities of consequences, coping potential, and action alternatives. In appraisal theorists’ view, the emotion process is considered as a varying pattern of change in several subsystems of the organism that is integrated into coherent clusters (<xref ref-type="bibr" rid="bibr76-0018720812440279">Scherer, 1984</xref>, <xref ref-type="bibr" rid="bibr77-0018720812440279">2001</xref>). Results on emotion annotation provide support for the appraisal theory of emotion (<xref ref-type="bibr" rid="bibr86-0018720812440279">Truong, 2009</xref>).</p>
<p>But although appraisal models of emotion dominate in the scientific community, current corpus annotations for the study of verbal and nonverbal aspects of emotions tend to deal with a limited number of stereotypical emotions. The first example of a database focused on emotions is the collection of pictures by <xref ref-type="bibr" rid="bibr35-0018720812440279">Ekman and Friesen (1975)</xref>, which is based on an early version of basic emotion theory. Similarly, most facial expression recognition research (for comprehensive reviews, see <xref ref-type="bibr" rid="bibr39-0018720812440279">Fasel &amp; Luettin, 2003</xref>; <xref ref-type="bibr" rid="bibr69-0018720812440279">Pantic &amp; Rothkrantz, 2003</xref>) has been inspired by Ekman and Friesen’s work on action units (AUs), a set of visible muscle movements in the face proposed to code facial expressions, described in the Facial Action Coding System (<xref ref-type="bibr" rid="bibr36-0018720812440279">Ekman &amp; Friesen, 1978</xref>). Each AU has a muscular basis, and every facial expression can be described by a combination of AUs.</p>
<p>The seminal work by Ekman and Friesen inspired the collection of many emotive data sets, pictures, and audio and/or video. Most of the facial expression recognizers follow a feature-based approach detecting specific features such as the corners of the mouth or eyebrow shape (<xref ref-type="bibr" rid="bibr14-0018720812440279">Calder, Burton, Miller, Young, &amp; Akamatsu, 2001</xref>; <xref ref-type="bibr" rid="bibr15-0018720812440279">Calder &amp; Young, 2005</xref>; <xref ref-type="bibr" rid="bibr22-0018720812440279">Chen et al., 2005</xref>; <xref ref-type="bibr" rid="bibr64-0018720812440279">Oliver, Pentland, &amp; Berard, 2000</xref>) or a region-based approach in which facial motions are measured in certain regions on the face such as the eye or eyebrow and the mouth (<xref ref-type="bibr" rid="bibr7-0018720812440279">Black &amp; Yacoob, 1995</xref>; <xref ref-type="bibr" rid="bibr38-0018720812440279">Essa &amp; Pentland, 1997</xref>; <xref ref-type="bibr" rid="bibr60-0018720812440279">Mase, 1991</xref>; <xref ref-type="bibr" rid="bibr65-0018720812440279">Otsuka &amp; Ohya, 1997</xref>; <xref ref-type="bibr" rid="bibr74-0018720812440279">Rosenblum, Yacoob, &amp; Davis, 1996</xref>; <xref ref-type="bibr" rid="bibr93-0018720812440279">Yacoob &amp; Davis, 1996</xref>). In spite of the variety of approaches to facial expression analysis, the majority suffers from limitations. First, most of those methods handle only the six basic prototypical facial expressions of emotions. Second, they do not present a context-dependent interpretation of shown facial behavior; therefore, inferences about the expressed mood and attitude cannot be made by current facial “affect” analyzers.</p>
<p>Another problem with the current data sets is that in many multimodal (mainly audiovisual) corpora emotive facial expressions are produced by expert or semiexpert actors. It is often taken for granted that these expressions are the “gold standard” for studying facial display of emotions (<xref ref-type="bibr" rid="bibr34-0018720812440279">Ekman, 1992</xref>) without relying on any sort of annotation. But this is not completely true; as <xref ref-type="bibr" rid="bibr90-0018720812440279">Wagner (1993)</xref> pointed out, each actor’s production should be validated when assessing the real closeness to the “standard” emotion representation that a group of judges has in mind. More recently, emotion corpora have been collected using multiparty dialogues as in AMI (Augmented Multi-party Interaction) corpus (<xref ref-type="bibr" rid="bibr17-0018720812440279">Carletta, 2007</xref>).</p>
<p>Another popular (and inexpensive) way to collect emotion corpora is by recording them from TV shows, news programs, and interviews. This type of “ecological” corpora usually features a wide range of verbal and nonverbal behaviors (<xref ref-type="bibr" rid="bibr32-0018720812440279">Douglas-Cowie et al., 2005</xref>) that are often extremely difficult to classify and analyze in particular with categorical coding schemes of emotions (<xref ref-type="bibr" rid="bibr1-0018720812440279">Abrilian et al., 2005</xref>; <xref ref-type="bibr" rid="bibr16-0018720812440279">Callejas &amp; López-Cózar, 2008</xref>; <xref ref-type="bibr" rid="bibr59-0018720812440279">Martin et al., 2006</xref>). In general, previous attempts at coding emotions have produced results in poor agreement among annotators and consequently lack coding scheme reliability (for reviews, see <xref ref-type="bibr" rid="bibr4-0018720812440279">Artstein &amp; Poesio, 2008</xref>; <xref ref-type="bibr" rid="bibr20-0018720812440279">Cavicchio &amp; Poesio, 2008</xref>; <xref ref-type="bibr" rid="bibr53-0018720812440279">Jaimes &amp; Sebe, 2007</xref>).</p>
<p>The aim of the present research was to study the effect of emotion on (non)cooperation in unscripted, ecological communication. It is crucial, then, to examine the role of appraisal and how it affects the different response modalities such as facial expressions and physiological response. That was done by collecting a corpus of dialogues between speakers performing a direction-giving dialogue task, explicitly designed to elicit negative emotions; our hypothesis was that cooperation would be reduced in the speaker undergoing a negative emotion. This hypothesis was tested by coding the collected dialogues for cooperation and for facial correlates of emotion and by recording psycho-physiological measures, in particular heart rate (henceforth HR), which have been shown to correlate with negative emotions (<xref ref-type="bibr" rid="bibr46-0018720812440279">Gallo &amp; Matthews, 1999</xref>; <xref ref-type="bibr" rid="bibr55-0018720812440279">Kiecolt-Glaser, McGuire, Robles, &amp; Glaser, 2002</xref>; <xref ref-type="bibr" rid="bibr84-0018720812440279">Smyth et al., 1998</xref>; <xref ref-type="bibr" rid="bibr88-0018720812440279">van Eck, Berkhof, Nicolson, &amp; Sulon, 1996</xref>). But as <xref ref-type="bibr" rid="bibr52-0018720812440279">Jacob et al. (1999)</xref> showed, positive and negative emotions are associated with comparable levels of HR. Therefore, to disambiguate the emotive state it was essential to investigate the facial expressions that together with HR would be able to predict whether an agent engaged in an interaction is cooperating or not. The level of cooperation and the accompanying facial expression in the utterances following the noncooperative elicitation were measured using a novel coding scheme.</p>
<p>The structure of the article is as follows. In the second section we present the method adopted to collect and annotate the data. In the third section, we present the results. A discussion section follows.</p>
</sec>
<sec id="section8-0018720812440279" sec-type="methods">
<title>Method</title>
<sec id="section9-0018720812440279">
<title>The Task and the Participants</title>
<p>The task we used to elicit conversations is the Map Task (<xref ref-type="bibr" rid="bibr2-0018720812440279">Anderson et al., 1991</xref>), often used to study cooperative behavior (<xref ref-type="bibr" rid="bibr18-0018720812440279">Carletta et al., 1997</xref>; <xref ref-type="bibr" rid="bibr19-0018720812440279">Carletta &amp; Mellish, 1996</xref>; <xref ref-type="bibr" rid="bibr28-0018720812440279">Davies, 1998</xref>; <xref ref-type="bibr" rid="bibr50-0018720812440279">Isard &amp; Carletta, 1995</xref>). The Map Task involves two participants sitting opposite each other. Each of them has a map (see <xref ref-type="fig" rid="fig1-0018720812440279">Figure 1</xref>); one participant (the giver) has a route marked on her or his map, whereas the other one (the follower) does not. The participants are told that their goal is to reproduce the giver’s route on the follower’s map. They are told explicitly at the beginning of the task session that the maps are not identical.</p>
<fig id="fig1-0018720812440279" position="float">
<label>Figure 1.</label>
<caption>
<p>The Map Task: Maps for the instruction giver (lower part of the figure) and instruction follower (upper part of the map).</p>
</caption>
<graphic xlink:href="10.1177_0018720812440279-fig1.tif"/></fig>
<p>To elicit noncooperative behavior, during the interaction we delivered in carefully controlled circumstances a set of negative feedback on their task performance to one of the actors (for a review of the methodology, see <xref ref-type="bibr" rid="bibr3-0018720812440279">Anderson, Linden, &amp; Habra, 2005</xref>).</p>
<p>Participants were 14 Italian native speakers (7 men and 7 women; average age = 28.6, <italic>SD</italic> = 4.36) matched with a male confederate partner. The confederate was the same for all the interaction dialogues and was not related to the participants. A control group of 7 pairs of participants (2 female dyads, 2 male dyads, and 3 mixed-gender dyads; average age = 32.16, <italic>SD</italic> = 2.9) was also recorded while playing the Map Task with the same map but without negative feedback. The dyads were not related to each other. Game role (giver or follower) and gender were counterbalanced across the interactions.</p>
</sec>
<sec id="section10-0018720812440279">
<title>Apparatus, Procedure, and Materials</title>
<p>A BIOPAC MP150 system was used to record the electrocardiogram. Two Ag AgC1 surface electrodes were fixed on the participant’s wrists; a low-pass filter was settled at 100 Hz, and the sampling rate was 200 per second. HR was automatically calculated by the system as the number of heart beats per minute. Artifacts resulting from movements were automatically removed using BIOPAC’s Acknowledge software (Version 3.9.1). All sessions were video- and audiotaped with two VC-C50i Canon digital cameras and two free-field Sennheiser half-cardioid microphones with permanently polarized condenser, placed in front of each speaker.</p>
<p>In both the confederate and the control sessions, before starting the task, we recorded the participants’ HR for 10 min without challenging them. In the confederate session we recorded the HR during the first 3 min of interaction; we call this period the <italic>task condition</italic>. The confederate performed noncooperative utterances by acting the following <italic>negative emotion elicitation</italic> lines (adapted from <xref ref-type="bibr" rid="bibr3-0018720812440279">Anderson et al., 2005</xref>) at minutes 4, 9, and 14 of each interaction. Those time intervals were chosen to ensure HR recovery as cardiovascular activation after negative emotions lasts longer than after positive emotions (<xref ref-type="bibr" rid="bibr12-0018720812440279">Brosschot &amp; Thayer, 2003</xref>; <xref ref-type="bibr" rid="bibr67-0018720812440279">Palomba, Sarlo, Angrilli, Mini, &amp; Stegagno, 2000</xref>). In <xref ref-type="fig" rid="fig2-0018720812440279">Figure 2</xref>, a sketch of the experiment procedure is presented.</p>
<fig id="fig2-0018720812440279" position="float">
<label>Figure 2.</label>
<caption>
<p>Sketch of the experiment procedure in the confederate condition.</p>
</caption>
<graphic xlink:href="10.1177_0018720812440279-fig2.tif"/></fig>
<p>The following lines were delivered by the confederate to the followers:</p>
<list id="list1-0018720812440279" list-type="bullet">
<list-item><p>“You are sending me in the wrong direction, try to be more accurate!”</p></list-item>
<list-item><p>“It’s still wrong, you are not doing your best, try harder! Again, from where you stopped.”</p></list-item>
<list-item><p>“You’re obviously not good enough at giving instructions.”</p></list-item></list>
<p>When the confederate had the giver role, the script lines were the following:</p>
<list id="list2-0018720812440279" list-type="bullet">
<list-item><p>“You are going in the wrong direction, try to be more accurate!”</p></list-item>
<list-item><p>“It’s still wrong, you are not doing your best, try harder! Again, from where you stopped.”</p></list-item>
<list-item><p>“You’re obviously not good enough at following instructions.”</p></list-item></list>
<p>An HR reading was taken after each script sentence and lasted 2 minutes.</p>
<p>At the end of the interactions, the 5-point Self-Assessment Manikin Scale (adapted from <xref ref-type="bibr" rid="bibr9-0018720812440279">Bradley &amp; Lang, 1994</xref>) was completed by all the participants. The aim was to measure the valence (positive or negative) of the emotion felt by the participants during the interactions. Six of the participants rated the experience as quite negative, four rated the experience as almost negative, two participants rated it as negative, and two participants rated it as neutral.</p>
<p>Regarding the control group, we collected four HR readings at equal intervals during the task (at the beginning of the task and at minutes 2, 4, and 6 of the interaction).</p>
</sec>
<sec id="section11-0018720812440279">
<title>Coding of the Data</title>
<p>A total of 64 conversational turns from the confederate session and the control session were annotated. The annotated segments were taken at the same time points at which the HR was measured. For the confederate condition, the segments were taken at the beginning of the task and after each of the noncooperative utterances. For the control condition, they were taken at the four points at which the HR was measured (at the beginning of the task and after 2, 4, and 6 minutes of interaction). The 64 conversational turns were transcribed and aligned with the annotation of cooperation and facial expression (upper and lower face configuration). All the audiovisual excerpts were annotated using the annotation tool ANViL (ANnotation of Video and Language; <xref ref-type="bibr" rid="bibr56-0018720812440279">Kipp, 2001</xref>). Regarding facial expressions, any movements produced by the upper and lower part of the face were marked on different layers of the annotation tool and synchronized with the dialogue transcription. Six Italian native speakers independently coded a part of the collected corpus following the coding scheme guidelines (<xref ref-type="bibr" rid="bibr21-0018720812440279">Cavicchio &amp; Poesio, 2012</xref>).</p>
<sec id="section12-0018720812440279">
<title>Annotation of cooperation</title>
<p>The level of linguistic cooperation was annotated using a novel coding scheme that simplifies the coding scheme proposed by <xref ref-type="bibr" rid="bibr28-0018720812440279">Davies (1998</xref>, <xref ref-type="bibr" rid="bibr29-0018720812440279">2006</xref>). We reduced the number of annotation labels as Davies remarked that coder agreement was not significant for some of her scheme annotation features (<xref ref-type="bibr" rid="bibr28-0018720812440279">Davies, 1998</xref>). This means either that there was no agreement on the use of those features or that they are very rare.</p>
<p>Like Davies’s, our cooperation typology is based on the idea of having coders annotate utterances with moves in a slightly modified version of the HCRC dialogue moves coding scheme (<xref ref-type="bibr" rid="bibr18-0018720812440279">Carletta et al., 1997</xref>) and then assigning a numerical degree of cooperation to these moves: For example, spontaneously adding information is considered more cooperative than simply doing what asked to do. We used <italic>check, question answering</italic>, and <italic>giving instruction</italic> as measures of knowledge sharing (i.e., grounding) between the two speakers (see <xref ref-type="table" rid="table1-0018720812440279">Table 1</xref>). Our <italic>check</italic> code covers queries and clarifications or objections. In the Map Task dialogues, a question is a way to check the extent of knowledge shared by the two speakers. Another group of dialogue moves is related to question answering. The last group of dialogue moves concerns <italic>giving instruction.</italic> In the Map Task, instruction giving is the main task.</p>
<table-wrap id="table1-0018720812440279" position="float">
<label>Table 1:</label>
<caption>
<p>Coding Scheme for Cooperation Annotation</p>
</caption>
<graphic alternate-form-of="table1-0018720812440279" xlink:href="10.1177_0018720812440279-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Instructions (Cooperation Typology)</th>
<th align="center">Cooperation Level</th>
</tr>
</thead>
<tbody>
<tr>
<td>No answer to question: No answer given when required</td>
<td>0</td>
</tr>
<tr>
<td>Inappropriate reply: Failure to introduce useful information when required</td>
<td>0</td>
</tr>
<tr>
<td>No spontaneous add/Repetition of instruction: Information is not spontaneously added or repeated after a check</td>
<td>0</td>
</tr>
<tr>
<td>Giving instructions: Task baseline</td>
<td>1</td>
</tr>
<tr>
<td>Acknowledgment: A verbal response which minimally shows that the speaker has heard the move to which it responds</td>
<td>2</td>
</tr>
<tr>
<td>Question answering (Y/N): Yes–no reply to a check</td>
<td>2</td>
</tr>
<tr>
<td>Check: Questions (function or form) which solicit other understanding of information already offered</td>
<td>2</td>
</tr>
<tr>
<td>Repeating instructions: Repetition of an instruction following a check</td>
<td>2</td>
</tr>
<tr>
<td>Question answering + adding information: Yes–no reply + new information introduction</td>
<td>2</td>
</tr>
<tr>
<td>Spontaneous info/description adding: Introduces new information relevant to the task</td>
<td>2</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Regarding noncooperative dialogue moves, we coded when a speaker fails or refuses to answer a question, adds information, or does not repeat an instruction when required by the other speaker. To calculate the level of effort needed in each move, we attributed the lowest value (0) when a behavior requiring an effort (e.g., answer to a direct question) did not occur. A positive weighting value (2) was attributed when an effort move took place in the dialogue (e.g., adding information spontaneously). We also attributed a weight of 1 to the <italic>giving instruction</italic> move, which is the “minimum need” in the Map Task dialogue (<xref ref-type="bibr" rid="bibr28-0018720812440279">Davies, 1998</xref>). The <italic>effort</italic> was calculated on an individual basis and not as a joint activity because, as <xref ref-type="bibr" rid="bibr29-0018720812440279">Davies (2006)</xref> pointed out, in the Map Task the minimization of the effort is made on a speaker’s basis.</p>
<p>To check that annotators agreed on cooperation levels, a kappa statistic (<xref ref-type="bibr" rid="bibr83-0018720812440279">Siegel &amp; Castellan, 1988</xref>) was computed on the annotated data. A high level of agreement was found (κ = .83, <italic>p</italic> &lt; .01).</p>
</sec>
<sec id="section13-0018720812440279">
<title>Annotation of facial expressions</title>
<p>In our coding scheme facial expressions are <italic>deconstructed</italic> in eyebrows and mouth shapes (see <xref ref-type="table" rid="table2-0018720812440279">Table 2</xref>). For example, a smile was annotated as “)” and a large smile or a laugh was marked as “+).” The latter feature meant a bigger opening in the rim of the mouth or teeth showing through. Other annotated features are grimace “(,” asymmetric smiles (<italic>1cornerup</italic>), lips in normal position<italic>/</italic>closed mouth, lower lip biting, and open lips (<italic>O</italic>). Regarding eyebrows, annotators marked them in the normal position, frowning (two levels: <italic>frown</italic> and <italic>+frown</italic>, the latter meaning a deeper frown), and eyebrows up (<italic>up</italic> and <italic>+up</italic>).</p>
<table-wrap id="table2-0018720812440279" position="float">
<label>Table 2:</label>
<caption>
<p>Coding Scheme for Facial Expression Annotation</p>
</caption>
<graphic alternate-form-of="table2-0018720812440279" xlink:href="10.1177_0018720812440279-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Upper or Lower Face Configuration</th>
<th align="center">Annotation Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>Open mouth</td>
<td>O</td>
</tr>
<tr>
<td>Lips in relaxed position/closed mouth</td>
<td>closed</td>
</tr>
<tr>
<td>Lip corners up (e.g., smile)</td>
<td>)</td>
</tr>
<tr>
<td>Open smile or laugh</td>
<td>+)</td>
</tr>
<tr>
<td>Lip corners down (e.g., grimace)</td>
<td>(</td>
</tr>
<tr>
<td>Lower lip biting</td>
<td>Lbiting</td>
</tr>
<tr>
<td>1 mouth corner up (asymmetric smile)</td>
<td>1cornerUp</td>
</tr>
<tr>
<td>Eyebrows in normal position</td>
<td>- -</td>
</tr>
<tr>
<td>Eyebrows up</td>
<td>up</td>
</tr>
<tr>
<td>Eyebrows very up</td>
<td>+up</td>
</tr>
<tr>
<td>Frown</td>
<td>frown</td>
</tr>
<tr>
<td>Deep frown</td>
<td>+frown</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The six coders reached a high level of agreement in facial expression annotation (upper-face intercoder agreement: κ = .855, <italic>p</italic> &lt; .01; lower-face intercoder agreement: κ = .81, <italic>p</italic> &lt; .001).</p>
<p>An excerpt of a conversation along with the HR, cooperation level, and facial expressions is reported. In the following, the confederate had the role of the giver. After he delivered the second sentence of the script, the confederate giver asked a question to the follower, who did not answer.</p>
<p>&lt;<bold>Follower Heart Rate Mean</bold>&gt;</p>
<p>132 bpm</p>
<p>&lt;<bold>Giver confederate</bold> start = “0 sec” end = “8.92 sec”&gt;</p>
<p>It’s still wrong, you are not doing your best, try harder. Again, from where you stopped. Where do you stop?</p>
<p>&lt;<bold>Giver confederate</bold> start = “10.32 sec” end = “14.92 sec”&gt;</p>
<p>We’re at Mount Poldi turn right and do not dare to pass through any of the vineyards you’re at a church now</p>
<p>&lt;<bold>Follower</bold> start = “14.92 sec” end = “15.12 sec”&gt;</p>
<p>u-uhm</p>
<p>&lt;<bold>Giver confederate</bold> start = “15.12 sec” end = “23.52 sec”&gt;</p>
<p>Of course you’re at the church otherwise you’re going towards a lot of other churches I don’t know on your map but on mine there are an awful lot of other churches</p>
<p>&lt;<bold>Follower</bold> start = “23.56 sec” end = “24.04 sec”&gt;</p>
<p>Ok</p>
<p>&lt;<bold>Giver confederate</bold> start = “24 sec” end = “25.4 sec”&gt;</p>
<p>So you’re at this church</p>
<p>&lt;<bold>Giver confederate</bold> start = “25.8 sec” end = “26.4 sec”&gt;</p>
<p>infamous</p>
<p>&lt;<bold>Follower</bold> start = “26.6 sec” end = “28.04 sec”&gt;</p>
<p>Is it detached or</p>
<p>&lt;<bold>Giver confederate</bold> start = “29.08 sec” end = “32.76 sec”&gt;</p>
<p>Well some are detached as this one but in this case it is detached it is detached</p>
<p>&lt;<bold>Follower</bold> start = “30.76 sec” end = “31.56 sec”&gt;</p>
<p>no this one</p>
<p>&lt;<bold>Follower</bold> start = “32.56 sec” end = “34.2 sec”&gt;</p>
<p>ok it’s that one</p>
<p>&lt;<bold>Cooperation Type (Follower)</bold>&gt;</p>
<p>&lt;start = “8.92 sec” end = “10.32 sec”&gt;</p>
<p>Inappropriate reply (no giving info upon request)</p>
<p>&lt;start = “14.92 sec” end = “15.12 sec”&gt;</p>
<p>acknowledgement</p>
<p>&lt;start = “23.56 sec” end = “24.04 sec”&gt;</p>
<p>acknowledgement</p>
<p>&lt;start = “26.6 sec” end = “28.04 sec”&gt;</p>
<p>checking instructions</p>
<p>&lt;start = “30.76 sec” end = “31.56 sec”&gt;</p>
<p>checking instructions</p>
<p>&lt;start = “32.56 sec” end = “34.2 sec”&gt;</p>
<p>yes-answer</p>
<p>&lt;<bold>Mouth Movements Analysis (Follower)</bold>&gt;</p>
<p>&lt;start = “0.52 sec” end = “6.12 sec”&gt;</p>
<p>closed</p>
<p>&lt;start = “6.12 sec” end = “9.52 sec”&gt;</p>
<p>lip biting</p>
<p>&lt;start = “21.56 sec” end = “22.76 sec”&gt;</p>
<p>(</p>
<p>&lt;start = “22.76 sec” end = “22.96 sec”&gt;</p>
<p>closed</p>
<p>&lt;start = “28.04 sec” end = “29.08 sec”&gt;</p>
<p>(</p>
<p>&lt;start = “29.08 sec” end = “30.56 sec”&gt;</p>
<p>O</p>
<p>&lt;start = “31.56 sec” end = “32.36 sec”&gt;</p>
<p>closed</p>
<p>&lt;<bold>Eyebrows Movements Analysis (Follower)</bold>&gt;</p>
<p>&lt;start = “26.6 sec” end = “27.28 sec”&gt;</p>
<p>up</p>
</sec>
</sec>
</sec>
<sec id="section14-0018720812440279" sec-type="results">
<title>Results</title>
<sec id="section15-0018720812440279">
<title>Heart Rate and Cooperation</title>
<p>We tested whether the confederate’s uncooperative utterances would lead to a reduced level of cooperation in the participants. To test this, we first checked whether the eliciting protocol we adopted caused a change in the participants’ HR. A one-way within-subjects ANOVA was performed in the confederate and in the control condition. The HR was confronted over the five time points of interest (baseline, beginning of the interaction, after minute 4, after minute 9, after minute 14) for the confederate conditions.</p>
<p>Regarding the control group, in addition to the baseline value, HR was measured another four times taken at equal intervals dung the interactions (see the Apparatus, Procedure, and Materials section). As a result, we found a significant effect of time, <italic>F</italic>(1.5, 19.54) = 125, <italic>p</italic> &lt; .0001, in the confederate Map Task group but not in the plain Map Task, <italic>F</italic>(2.1, 27) = 1.9, <italic>p</italic> &lt; .16. Between-group post hoc tests run in the confederate condition showed that there was a significant effect of the three sentences with respect to the task condition.</p>
<p>After determining that our coding scheme for cooperation was reliable (see the Annotation of Cooperation section), we investigated the relationship between HR (following the anger or frustration provocation) and the level of cooperation. Our goal was to investigate whether HR could be one of the predictors of cooperative or noncooperative behavior in an interactive partner. Again, we took the four HR measures at the beginning of the task and after the confederate delivered the first, second, and third sentences. Then, we scored the cooperation level of 56 video excerpts of interactions with the confederate. The cooperation scores were normalized along each video segment performing the average cooperation level for that segment.</p>
<p>A linear regression with HR as the dependent variable and the normalized level of cooperation at the four time points (task, first, second, third sentences) as predictors was run. The statistical model was found to be significant (<italic>R</italic><sup>2</sup> = .85, <italic>SE</italic> = 6.3, <italic>p</italic> &lt; .001). HR was found to be negatively correlated with the normalized cooperation level (<italic>p</italic> &lt; .001) but positively correlated with sentence (<italic>p</italic> &lt; .001). That means that the HR linearly increase after each sentence and higher HR was found to be associated to lower level of cooperation. We investigated whether individual differences on HR baseline (i.e., the HR taken before the participants started the task) have any effect on cooperation, but we did not find any (<italic>p</italic> &lt; .4). A <italic>t</italic> test on residuals showed that the <italic>SE</italic> was not significantly different from 0 (<italic>p</italic> = 1) and the Shapiro–Wilk normality test had a <italic>p</italic> value of .31. An inspection of the quantile–quantile plot of residuals revealed an underlying normal distribution.</p>
</sec>
<sec id="section16-0018720812440279">
<title>Cooperation and Facial Expression</title>
<p>The hypothesis that nonverbal expressions of negative emotions such as specific facial configurations would also predict cooperative or uncooperative communication behavior was tested via a multinomial logistic regression. The dependent variable “cooperation” was a categorical variable with three levels: 0 (<italic>noncooperative</italic>), 1 (<italic>giving instructions</italic>), and 2 (<italic>cooperative</italic>). We set 1 as the reference level in the model.</p>
<p>The upper- and lower-face configurations annotated along with the three cooperation levels were included in the model. The model was significant (<italic>p</italic> &lt; .001), and mouth configuration was found to be more likely to predict cooperation than upper-face configuration. We found that with respect to giving instruction, cooperation is predicted by open smile (1.64, <italic>SE</italic> = 0.59, <italic>p</italic> &lt; .05), whereas noncooperation was negatively predicted by smile (–1.17, <italic>SE</italic> = 0.55, <italic>p</italic> &lt; .03; see <xref ref-type="fig" rid="fig3-0018720812440279">Figure 3</xref>).</p>
<fig id="fig3-0018720812440279" position="float">
<label>Figure 3.</label>
<caption>
<p>Predicted probabilities of noncooperation (black), giving instruction (gray), and cooperation (light gray) against face expressions of the mouth. The values reported on the table refer to the multinomial logit confronting the predicted probability of cooperation and noncooperation with the task baseline giving instruction.</p>
</caption>
<graphic xlink:href="10.1177_0018720812440279-fig3.tif"/></fig>
<p>To evaluate the model fit, the independence of irrelevant alternatives was tested via the Hausman–McFadden test. To implement this test, we ran an alternative multimodal logistic regression model with a subset comparing noncooperation with cooperation. As the sets of estimates were not statistically different from the ones of our model (<italic>p</italic> &lt; .98), the independence of irrelative alternatives held.</p>
</sec>
</sec>
<sec id="section17-0018720812440279" sec-type="discussion">
<title>Discussion</title>
<p>The results of the present study indicate that provocative sentences brought a significant change in the HR of the participants. That change did not occur in the HR of the control group playing the Map Task without the anger-provoking sentences. We showed that HR increased with new negative feedback. This finding fits with previous studies examining the role of hostile rumination in HR recovery (<xref ref-type="bibr" rid="bibr49-0018720812440279">Hogan &amp; Linden, 2002</xref>; <xref ref-type="bibr" rid="bibr82-0018720812440279">Schwartz et al., 2000</xref>). <xref ref-type="bibr" rid="bibr82-0018720812440279">Schwartz et al. (2000)</xref> found that after an anger provocation stimulus, participants who were given a chance to repetitively think about the anger-provoking event tended to have a slower HR recovery than those who are distracted. As reported by <xref ref-type="bibr" rid="bibr58-0018720812440279">Linden et al. (2003)</xref>, this effect might be linked to the continued focus on the preceding anger provocation stimulus.</p>
<p>In our data collection the slower recovery following the first anger-provoking stimulus could reflect higher HR values following the second and third stimuli. Moreover, we found that higher HR values are correlated with a lower level of cooperation. Regarding facial expression, we found that cooperation can be predicted on the basis of mouth configurations. This suggests that the mouth is the facial component to look at to reliably predict cooperation. Our model baseline, that is, giving instruction, was characterized mainly by smile that occurs in association with low HR.</p>
<p>A number of studies have pointed out that facial expressions, such as a smile, that appear to convey an emotion, often do not indicate the emotional state at all but have strictly communicative functions transcending the simple indications of one’s current feelings (<xref ref-type="bibr" rid="bibr5-0018720812440279">Bavelas, Black, Lemery, &amp; Mullett, 1987</xref>; <xref ref-type="bibr" rid="bibr42-0018720812440279">Fridlund, 1994</xref>, <xref ref-type="bibr" rid="bibr43-0018720812440279">2002</xref>; <xref ref-type="bibr" rid="bibr73-0018720812440279">Provine, 2001</xref>). Moreover, smiling individuals are rated as less socially dominant (<xref ref-type="bibr" rid="bibr54-0018720812440279">Keating &amp; Bai, 1986</xref>). In this view, a smile could effectively communicate empathy and willingness to cooperate. The facial expressions we found to be predictive of cooperation was an open smile, which, following previous studies (<xref ref-type="bibr" rid="bibr13-0018720812440279">Brown et al., 2003</xref>; <xref ref-type="bibr" rid="bibr62-0018720812440279">Mehu et al., 2007</xref>), may be linked to the tendency of cooperators to openly express their positive emotions. However, we found that open smiles were not correlated with higher levels of HR with respect to the task baseline, suggesting that they were not expressing a higher level of positive emotion activation.</p>
<p>Finally, that noncooperation was not predicted by any facial expression may seem surprising, but it is in fact consistent with the recent findings of <xref ref-type="bibr" rid="bibr81-0018720812440279">Schug, Matsumoto, Horita, Yamagishi, and Bonnet (2010)</xref>. Schug et al. examined the facial expressions of cooperators and noncooperators as they confronted unfair offers in the Ultimatum Game, finding that cooperators showed greater levels of Duchenne smiles compared to noncooperators, but both groups showed the same level of negative facial expressions when they were not cooperating. Schug et al. suggested that negative facial expression are “faked” or “masked” more often when interactors are not cooperating. Our results support the idea that when cooperating participants have the tendency to express more positive facial expression, whereas when they do not cooperate negative facial expressions as well as smiles are suppressed (and in our findings noncooperation was indeed negatively correlated with smile).</p>
<p>It is far more difficult masking or suppressing the psycho-physiological response than a facial expression though. Therefore, it is the combination of HR and facial expression that can reliably provide the modifications of the level of cooperation. As we have shown in our study, when investigating noncooperation HR is the principal index to take into account, whereas when investigating cooperation it is important to take into account both facial expression and HR. A low HR with an open smile expresses that the interactor is cooperating, whereas a high HR with a “fuzzier” facial expression expresses that she or he is not.</p>
<p>Last but not least, our coding scheme reliability was very high when compared to that achieved with other multimodal annotation schemes, in particular for emotion annotation. This is because we analyzed emotions with a coding scheme based on the decomposition of the several factors of an emotive event. In particular, we did not ask coders to label emotive terms directly. Given our results, it seems that the modeling of emotions is a multilevel process rather than a single-shot configuration prediction that can be labeled with an emotive word. Particularly, as we investigated a culturally homogenous group of participants, an interesting application of our coding scheme would be exploring how different emotive sets (positive or negative) modify cooperation in different cultural settings.</p>
<p>We believe that our coding scheme is an important step toward the creation of annotated multimodal resources for emotion recognition, which in turn are crucial for investigating multimodal communication and HCI.</p>
</sec>
<sec id="section18-0018720812440279">
<title>Key Points</title>
<list id="list3-0018720812440279" list-type="bullet">
<list-item><p>The article contributes an experimental design to compare cooperation levels with HR and facial expressions.</p></list-item>
<list-item><p>This article demonstrates the development of analysis techniques to establish that both HR and facial expressions predict (non)cooperation.</p></list-item>
</list>
</sec>
</body>
<back>
<ack>
<p>We wish to thank Marco Baroni for his comments on the statistical analysis and Francesco Vespignani for his help in setting up the psychophysiological recordings. All participants gave informed consent; the experimental protocol was approved by the Human Research Ethics Committee of Trento University. This research was supported by a PhD studentship from the Department of Information Science and Engineering, Università di Trento.</p>
</ack>
<bio>
<p>Federica Cavicchio is a Marie Curie postdoctoral fellow at the Center for Mind/Brain Sciences, Università di Trento, Rovereto, Italy. She obtained her PhD in cognitive and brain sciences at the Center for Mind/Brain, Università di Trento, in 2010. Her main fields of research are dialogue systems and psycholinguistics.</p>
<p>Massimo Poesio is full professor and director of the Cognition, Language, Interaction and Computation (CLIC) Lab at the Center for Mind/Brain Sciences, Università di Trento, Rovereto, Italy. His main field of research is natural language processing.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0018720812440279">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Abrilian</surname><given-names>S.</given-names></name>
<name><surname>Devillers</surname><given-names>L.</given-names></name>
<name><surname>Buisine</surname><given-names>S.</given-names></name>
<name><surname>Martin</surname><given-names>J.-C.</given-names></name>
</person-group> (<year>2005</year>, <month>July</month>). <source>EmoTV1: Annotation of real-life emotions for the specification of multimodal affective interfaces</source>. <conf-name>Paper presented at Human-Computer Interaction International</conf-name>, <conf-loc>Las Vegas, NV</conf-loc>.</citation>
</ref>
<ref id="bibr2-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>A.</given-names></name>
<name><surname>Bader</surname><given-names>M.</given-names></name>
<name><surname>Bard</surname><given-names>E.</given-names></name>
<name><surname>Boyle</surname><given-names>E.</given-names></name>
<name><surname>Doherty</surname><given-names>G. M.</given-names></name>
<name><surname>Garrod</surname><given-names>S.</given-names></name>
<name><surname>Isard</surname><given-names>S.</given-names></name>
<name><surname>Kowtko</surname><given-names>J.</given-names></name>
<name><surname>McAllister</surname><given-names>J.</given-names></name>
<name><surname>Miller</surname><given-names>J.</given-names></name>
<name><surname>Sotillo</surname><given-names>C.</given-names></name>
<name><surname>Thompson</surname><given-names>H. S.</given-names></name>
<name><surname>Weinert</surname><given-names>R.</given-names></name>
</person-group> (<year>1991</year>). <article-title>The HCRC Map Task Corpus</article-title>. <source>Language and Speech</source>, <volume>34</volume>, <fpage>351</fpage>–<lpage>366</lpage>.</citation>
</ref>
<ref id="bibr3-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>J. C.</given-names></name>
<name><surname>Linden</surname><given-names>W.</given-names></name>
<name><surname>Habra</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The importance of examining blood pressure reactivity and recovery in anger provocation research</article-title>. <source>International Journal of Psychophysiology</source>, <volume>57</volume>, <fpage>159</fpage>–<lpage>163</lpage>.</citation>
</ref>
<ref id="bibr4-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Artstein</surname><given-names>R.</given-names></name>
<name><surname>Poesio</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Inter-coder agreement for computational linguistics</article-title>. <source>Computational Linguistics</source>, <volume>34</volume>, <fpage>555</fpage>–<lpage>596</lpage>.</citation>
</ref>
<ref id="bibr5-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bavelas</surname><given-names>J. B.</given-names></name>
<name><surname>Black</surname><given-names>A.</given-names></name>
<name><surname>Lemery</surname><given-names>C. R.</given-names></name>
<name><surname>Mullett</surname><given-names>J.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Motor mimicry as primitive empathy</article-title>. In <person-group person-group-type="editor">
<name><surname>Eisenberg</surname><given-names>N.</given-names></name>
<name><surname>Strayer</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Empathy and its development</source> (pp. <fpage>317</fpage>–<lpage>338</lpage>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bianchi</surname><given-names>N.</given-names></name>
<name><surname>Lisetti</surname><given-names>C. L.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Modeling multimodal expression of user’s affective subjective experience</article-title>. <source>User Modeling and User-Adapted Interaction</source>, <volume>12</volume>, <fpage>49</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr7-0018720812440279">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Black</surname><given-names>M.</given-names></name>
<name><surname>Yacoob</surname><given-names>Y.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Tracking and recognizing rigid and non-rigid facial motions using local parametric models of image motion</article-title>. In <conf-name>International Conference on Computer Vision</conf-name> (pp. <fpage>374</fpage>–<lpage>381</lpage>). <conf-loc>Cambridge: MIT Press</conf-loc>.</citation>
</ref>
<ref id="bibr8-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boone</surname><given-names>R. T.</given-names></name>
<name><surname>Buck</surname><given-names>R.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Emotional expressivity and trustworthiness: The role of nonverbal behavior in the evolution of cooperation</article-title>. <source>Journal of Nonverbal Behavior</source>, <volume>27</volume>, <fpage>163</fpage>–<lpage>182</lpage>.</citation>
</ref>
<ref id="bibr9-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bradley</surname><given-names>M.</given-names></name>
<name><surname>Lang</surname><given-names>P.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Measuring emotion: The self-assessment manikin and the semantic differential</article-title>. <source>Journal of Behavioral Therapy and Experimental Psychiatry</source>, <volume>25</volume>, <fpage>49</fpage>–<lpage>59</lpage>.</citation>
</ref>
<ref id="bibr10-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bratman</surname><given-names>M.</given-names></name>
</person-group> (<year>1987</year>). <source>Intention, plans, and practical reason</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brennan</surname><given-names>S. E.</given-names></name>
<name><surname>Clark</surname><given-names>H. H.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Conceptual pacts and lexical choice in conversation</article-title>. <source>Journal of Experimental Psychology: Learning, Memory and Cognition</source>, <volume>22</volume>, <fpage>1482</fpage>–<lpage>1493</lpage>.</citation>
</ref>
<ref id="bibr12-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brosschot</surname><given-names>J. F.</given-names></name>
<name><surname>Thayer</surname><given-names>J. F.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Heart rate response is longer after negative emotions than after positive emotions</article-title>. <source>International Journal of Psychophysiology</source>, <volume>50</volume>, <fpage>181</fpage>–<lpage>187</lpage>.</citation>
</ref>
<ref id="bibr13-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>W. M.</given-names></name>
<name><surname>Palameta</surname><given-names>B.</given-names></name>
<name><surname>Moore</surname><given-names>C.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Are there non-verbal cues to commitment? An exploratory study using the zero-acquaintance video presentation paradigm</article-title>. <source>Evolutionary Psychology</source>, <volume>1</volume>, <fpage>42</fpage>–<lpage>69</lpage>.</citation>
</ref>
<ref id="bibr14-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Calder</surname><given-names>A. J.</given-names></name>
<name><surname>Burton</surname><given-names>M.</given-names></name>
<name><surname>Miller</surname><given-names>P.</given-names></name>
<name><surname>Young</surname><given-names>A. W.</given-names></name>
<name><surname>Akamatsu</surname><given-names>S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>A principal component analysis of facial expressions</article-title>. <source>Vision Research</source>, <volume>41</volume>, <fpage>1179</fpage>–<lpage>1208</lpage>.</citation>
</ref>
<ref id="bibr15-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Calder</surname><given-names>A. J.</given-names></name>
<name><surname>Young</surname><given-names>A. W.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Understanding facial identity and facial expression recognition</article-title>. <source>Nature Neuroscience Reviews</source>, <volume>6</volume>, <fpage>641</fpage>–<lpage>653</lpage>.</citation>
</ref>
<ref id="bibr16-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Callejas</surname><given-names>Z.</given-names></name>
<name><surname>López-Cózar</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Influence of contextual information in emotion annotation for spoken dialogue systems</article-title>. <source>Speech Communication</source>, <volume>50</volume>, <fpage>416</fpage>–<lpage>433</lpage>.</citation>
</ref>
<ref id="bibr17-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carletta</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Unleashing the killer corpus: Experiences in creating the multi-everything AMI Meeting Corpus</article-title>. <source>Language Resources and Evaluation</source>, <volume>41</volume>, <fpage>181</fpage>–<lpage>190</lpage>.</citation>
</ref>
<ref id="bibr18-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carletta</surname><given-names>J. C.</given-names></name>
<name><surname>Isard</surname><given-names>A.</given-names></name>
<name><surname>Isard</surname><given-names>S.</given-names></name>
<name><surname>Kowtko</surname><given-names>J.</given-names></name>
<name><surname>Doherty-Sneddon</surname><given-names>G.</given-names></name>
<name><surname>Anderson</surname><given-names>A.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The reliability of a dialogue structure coding scheme</article-title>. <source>Computational Linguistics</source>, <volume>23</volume>, <fpage>13</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr19-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carletta</surname><given-names>J.</given-names></name>
<name><surname>Mellish</surname><given-names>C.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Risk-taking and recovery in task-oriented dialogue</article-title>. <source>Journal of Pragmatics</source>, <volume>26</volume>, <fpage>71</fpage>–<lpage>107</lpage>.</citation>
</ref>
<ref id="bibr20-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cavicchio</surname><given-names>F.</given-names></name>
<name><surname>Poesio</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Annotation of emotion in dialogue: The Emotion in Cooperation Project</article-title>. In <person-group person-group-type="editor">
<name><surname>André</surname><given-names>E.</given-names></name>
<name><surname>Dybkjær</surname><given-names>L.</given-names></name>
<name><surname>Neumann</surname><given-names>H.</given-names></name>
<name><surname>Pieraccini</surname><given-names>R.</given-names></name>
<name><surname>Weber</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <source>Multimodal dialogue systems perception: Lecture notes in computer science</source> (pp. <fpage>233</fpage>–<lpage>239</lpage>). <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr21-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cavicchio</surname><given-names>F.</given-names></name>
<name><surname>Poesio</surname><given-names>M.</given-names></name>
</person-group> (<year>2012</year>). <article-title>The Rovereto Emotion and Cooperation Corpus: A new resource to investigate cooperation and emotions</article-title>. <source>Journal of Language and Resources Evaluation</source>. (<comment>Epub ahead of print</comment>.) doi:10.1007/s10579-011-9163-y<pub-id pub-id-type="doi">10.1007/s10579-011-9163-y</pub-id></citation>
</ref>
<ref id="bibr22-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>L. S.</given-names></name>
<name><surname>Travis Rose</surname><given-names>R.</given-names></name>
<name><surname>Parrill</surname><given-names>F.</given-names></name>
<name><surname>Han</surname><given-names>X.</given-names></name>
<name><surname>Tu</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>Z.</given-names></name>
<name><surname>Harper</surname><given-names>M.</given-names></name>
<name><surname>Quek</surname><given-names>F.</given-names></name>
<name><surname>McNeill</surname><given-names>D.</given-names></name>
<name><surname>Tuttle</surname><given-names>R.</given-names></name>
<name><surname>Huang</surname><given-names>T. S.</given-names></name>
</person-group> (<year>2005</year>). <source>VACE Multimodal Meeting Corpus. Lecture Notes in Computer Science</source>, <volume>3869</volume>, <fpage>40</fpage>–<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr23-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Clark</surname><given-names>H. H.</given-names></name>
</person-group> (<year>1996</year>). <source>Using language</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr24-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Clark</surname><given-names>H. H.</given-names></name>
<name><surname>Brennan</surname><given-names>S. E.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Grounding in communication</article-title>. In <person-group person-group-type="editor">
<name><surname>Resnick</surname><given-names>L.</given-names></name>
<name><surname>Levine</surname><given-names>J.</given-names></name>
<name><surname>Teasley</surname><given-names>S.</given-names></name>
</person-group> (Eds.), <source>Perspectives on socially shared cognition</source> (pp. <fpage>127</fpage>–<lpage>149</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr25-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clark</surname><given-names>H. H.</given-names></name>
<name><surname>Krych</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Speaking while monitoring addressees for understanding</article-title>. <source>Journal of Memory and Language</source>, <volume>50</volume>, <fpage>62</fpage>–<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr26-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clark</surname><given-names>H. H.</given-names></name>
<name><surname>Schaefer</surname><given-names>E. F.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Collaborating on contributions to conversations</article-title>. <source>Language and Cognitive Processes</source>, <volume>2</volume>, <fpage>19</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr27-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>P.</given-names></name>
<name><surname>Morgan</surname><given-names>J.</given-names></name>
<name><surname>Pollack</surname><given-names>M.</given-names></name>
</person-group> (<year>1990</year>). <source>Intentions in communication</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr28-0018720812440279">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Davies</surname><given-names>B. L.</given-names></name>
</person-group> (<year>1998</year>). <source>An empirical examination of cooperation, effort and risk in task-oriented dialogue</source> (<comment>Unpublished doctoral thesis</comment>). <publisher-name>University of Edinburgh</publisher-name>, <publisher-loc>Edinburgh, UK</publisher-loc>.</citation>
</ref>
<ref id="bibr29-0018720812440279">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Davies</surname><given-names>B. L.</given-names></name>
</person-group> (<year>2006</year>). <source>Testing dialogue principles in task-oriented dialogues: An exploration of cooperation, collaboration, effort and risk</source> (<comment>Leeds Working Papers in Linguistics and Phonetics 11</comment>). <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.leeds.ac.uk/linguistics/WPL/WP2006/2.pdf">http://www.leeds.ac.uk/linguistics/WPL/WP2006/2.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr30-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Davies</surname><given-names>B. L.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Grice’s cooperative principle: Meaning and rationality</article-title>. <source>Journal of Pragmatics</source>, <volume>39</volume>, <fpage>2308</fpage>–<lpage>2331</lpage>.</citation>
</ref>
<ref id="bibr31-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Devillers</surname><given-names>L.</given-names></name>
<name><surname>Martin</surname><given-names>J.-C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Emotional corpora</article-title>. In <person-group person-group-type="editor">
<name><surname>Pelachaud</surname><given-names>C.</given-names></name>
</person-group> (Ed.), <source>Emotions</source> (pp. <fpage>39</fpage>–<lpage>51</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr32-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Douglas-Cowie</surname><given-names>E.</given-names></name>
<name><surname>Devillers</surname><given-names>L.</given-names></name>
<name><surname>Martin</surname><given-names>J.-C.</given-names></name>
<name><surname>Cowie</surname><given-names>R.</given-names></name>
<name><surname>Savvidou</surname><given-names>S.</given-names></name>
<name><surname>Abrilian</surname><given-names>S.</given-names></name>
<name><surname>Cox</surname><given-names>C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Multimodal databases of everyday emotion: Facing up to complexity</article-title>. In <source>INTERSPEECH-2005</source> (pp. <fpage>813</fpage>–<lpage>816</lpage>).</citation>
</ref>
<ref id="bibr33-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Expression and the nature of emotion</article-title>. In <person-group person-group-type="editor">
<name><surname>Scherer</surname><given-names>K.</given-names></name>
<name><surname>Ekman</surname><given-names>P.</given-names></name>
</person-group> (Eds.), <source>Approaches to emotion</source> (pp. <fpage>319</fpage>–<lpage>343</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr34-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
</person-group> (<year>1992</year>). <article-title>An argument for basic emotions</article-title>. <source>Cognition and Emotion</source>, <volume>6</volume>, <fpage>169</fpage>–<lpage>200</lpage>.</citation>
</ref>
<ref id="bibr35-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W. V.</given-names></name>
</person-group> (<year>1975</year>). <source>Unmasking the face: A guide to recognizing emotions from facial clues</source>. <publisher-loc>Englewood Cliffs, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr36-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W. V.</given-names></name>
</person-group> (<year>1978</year>). <source>Facial action coding system: A technique for the measurement of facial movement</source>. <publisher-loc>Palo Alto, CA</publisher-loc>: <publisher-name>Consulting Psychologists Press</publisher-name>.</citation>
</ref>
<ref id="bibr37-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W. V.</given-names></name>
</person-group> (<year>1982</year>). <article-title>Felt, false, and miserable smiles</article-title>. <source>Journal of Nonverbal Behavior</source>, <volume>6</volume>, <fpage>238</fpage>-<lpage>252</lpage>.</citation>
</ref>
<ref id="bibr38-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Essa</surname><given-names>I.</given-names></name>
<name><surname>Pentland</surname><given-names>A.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Coding, analysis, interpretation, and recognition of facial expressions</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>19</volume>, <fpage>757</fpage>–<lpage>763</lpage>.</citation>
</ref>
<ref id="bibr39-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fasel</surname><given-names>B.</given-names></name>
<name><surname>Luettin</surname><given-names>J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Automatic facial expression analysis: A survey</article-title>. <source>Pattern Recognition</source>, <volume>36</volume>, <fpage>259</fpage>–<lpage>275</lpage>.</citation>
</ref>
<ref id="bibr40-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Frank</surname><given-names>M. G.</given-names></name>
<name><surname>Ekman</surname><given-names>P.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Not all smiles are created equal: The differences between enjoyment and nonenjoyment smiles</article-title>. <source>Humor</source>, <volume>6</volume>, <fpage>9</fpage>–<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr41-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Frank</surname><given-names>M. G.</given-names></name>
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W. V.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Behavioural markers and recognizability of the smile of enjoyment</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>64</volume>, <fpage>83</fpage>–<lpage>93</lpage>.</citation>
</ref>
<ref id="bibr42-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fridlund</surname><given-names>A.</given-names></name>
</person-group> (<year>1994</year>). <source>Human facial expression: An evolutionary view</source>. <publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr43-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fridlund</surname><given-names>A. J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>The behavioral ecology view of smiling and other facial expressions</article-title>. In <person-group person-group-type="editor">
<name><surname>Abel</surname><given-names>M. H.</given-names></name>
</person-group> (Ed.), <source>An empirical reflection on the smile</source> (pp. <fpage>45</fpage>–<lpage>82</lpage>). <publisher-loc>Lewiston, NY</publisher-loc>: <publisher-name>Edwin Mellen Press</publisher-name>.</citation>
</ref>
<ref id="bibr44-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Frijda</surname><given-names>N. H.</given-names></name>
</person-group> (<year>1986</year>). <source>The emotions</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr45-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Frijda</surname><given-names>N. H.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Emotions, individual differences and time course: Reflections</article-title>. <source>Cognition and Emotion</source>, <volume>23</volume>, <fpage>1444</fpage>–<lpage>1461</lpage>.</citation>
</ref>
<ref id="bibr46-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gallo</surname><given-names>L. C.</given-names></name>
<name><surname>Matthews</surname><given-names>K. A.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Do negative emotions mediate the association between socioeconomic status and health?</article-title> <source>Annals of the New York Academy of Science</source>, <volume>896</volume>, <fpage>226</fpage>–<lpage>245</lpage>.</citation>
</ref>
<ref id="bibr47-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Grice</surname><given-names>H. P.</given-names></name>
</person-group> (<year>1975</year>). <article-title>Logic and conversation</article-title>. In <person-group person-group-type="editor">
<name><surname>Cole</surname><given-names>P.</given-names></name>
<name><surname>Morgan</surname><given-names>J. L.</given-names></name>
</person-group> (Eds.), <source>Syntax and semantics, Vol. 3: Speech acts</source> (pp. <fpage>41</fpage>–<lpage>58</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr48-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Grice</surname><given-names>H. P.</given-names></name>
</person-group> (<year>1989</year>). <source>Studies in the way of words</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr49-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hogan</surname><given-names>B. E.</given-names></name>
<name><surname>Linden</surname><given-names>W.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Anger response styles affect reactivity and recovery to an anger challenge in cardiac patients</article-title>. <source>Annals of Behaviour of Medicine</source>, <volume>27</volume>, <fpage>38</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr50-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Isard</surname><given-names>A.</given-names></name>
<name><surname>Carletta</surname><given-names>J.</given-names></name>
</person-group> (<year>1995</year>). <source>Transaction and action coding in the Map Task Corpus</source> (<comment>Tech. Rep. HCRC/RP-65</comment>). <publisher-loc>Edinburgh, UK</publisher-loc>: <publisher-name>University of Edinburgh, Human Communication Research Centre</publisher-name>.</citation>
</ref>
<ref id="bibr51-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Izard</surname><given-names>C.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Four systems for emotion activation: Cognitive and non-cognitive processes</article-title>. <source>Psychological Review</source>, <volume>100</volume>, <fpage>60</fpage>–<lpage>69</lpage>.</citation>
</ref>
<ref id="bibr52-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>R. G.</given-names></name>
<name><surname>Thayer</surname><given-names>J. F.</given-names></name>
<name><surname>Manuck</surname><given-names>S. B.</given-names></name>
<name><surname>Muldoon</surname><given-names>M. F.</given-names></name>
<name><surname>Tamres</surname><given-names>L. K.</given-names></name>
<name><surname>Williams</surname><given-names>D. M.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Ambulatory blood pressure responses and the circumplex model of mood: A 4-day study</article-title>. <source>Psychosomatic Medicine</source>, <volume>61</volume>, <fpage>319</fpage>–<lpage>333</lpage>.</citation>
</ref>
<ref id="bibr53-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jaimes</surname><given-names>A.</given-names></name>
<name><surname>Sebe</surname><given-names>N.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Multimodal human-computer interaction: A survey</article-title>. <source>Computer Vision and Image Understanding</source>, <volume>108</volume>, <fpage>116</fpage>–<lpage>134</lpage>.</citation>
</ref>
<ref id="bibr54-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Keating</surname><given-names>C. F.</given-names></name>
<name><surname>Bai</surname><given-names>D. L.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Children’s attribution of social dominance from facial cues</article-title>. <source>Child Development</source>, <volume>57</volume>, <fpage>1269</fpage>–<lpage>1276</lpage>.</citation>
</ref>
<ref id="bibr55-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kiecolt-Glaser</surname><given-names>J. K.</given-names></name>
<name><surname>McGuire</surname><given-names>L.</given-names></name>
<name><surname>Robles</surname><given-names>T. F.</given-names></name>
<name><surname>Glaser</surname><given-names>R.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Emotions, morbidity, and mortality: New perspectives from psychoneuroimmunology</article-title>. <source>Annual Review of Psychology</source>, <volume>53</volume>, <fpage>83</fpage>–<lpage>107</lpage>.</citation>
</ref>
<ref id="bibr56-0018720812440279">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Kipp</surname><given-names>M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>ANVIL - A generic annotation tool for multimodal dialogue</article-title>. In <source>Proceedings of the 7th European Conference on Speech Communication and Technology</source> (Eurospeech) (pp. <fpage>1367</fpage>-<lpage>1370</lpage>). <publisher-loc>ISCA archive</publisher-loc>: <comment><ext-link ext-link-type="uri" xlink:href="http://www.isca-speech.org/archive/eurospeech_2001/e01_1367.html">http://www.isca-speech.org/archive/eurospeech_2001/e01_1367.html</ext-link></comment></citation>
</ref>
<ref id="bibr57-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Manstead</surname><given-names>A. S. R.</given-names></name>
<name><surname>Cosker</surname><given-names>D.</given-names></name>
<name><surname>Marshall</surname><given-names>D.</given-names></name>
<name><surname>Rosin</surname><given-names>P. L.</given-names></name>
<name><surname>Kappas</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Facial dynamics as indicators of trustworthiness and cooperative behavior</article-title>. <source>Emotion</source>, <volume>7</volume>, <fpage>730</fpage>–<lpage>735</lpage>.</citation>
</ref>
<ref id="bibr58-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Linden</surname><given-names>W.</given-names></name>
<name><surname>Hogan</surname><given-names>B.</given-names></name>
<name><surname>Rutledge</surname><given-names>T.</given-names></name>
<name><surname>Chawla</surname><given-names>A.</given-names></name>
<name><surname>Lenz</surname><given-names>J. W.</given-names></name>
<name><surname>Leung</surname><given-names>D.</given-names></name>
</person-group> (<year>2003</year>). <article-title>There is more to anger coping than “in” or “out.”</article-title> <source>Emotion</source>, <volume>3</volume>, <fpage>12</fpage>–<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr59-0018720812440279">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Martin</surname><given-names>J.-C.</given-names></name>
<name><surname>Caridakis</surname><given-names>G.</given-names></name>
<name><surname>Devillers</surname><given-names>L.</given-names></name>
<name><surname>Karpouzis</surname><given-names>K.</given-names></name>
<name><surname>Abrilian</surname><given-names>S.</given-names></name>
</person-group> (<year>2006</year>, <month>May</month>). <source>Manual annotation and automatic image processing of multimodal emotional behaviors: Validating the annotation of TV interviews</source>. <conf-name>Paper presented at the Fifth International Conference on Language Resources and Evaluation</conf-name>, <conf-loc>Genoa, Italy</conf-loc>.</citation>
</ref>
<ref id="bibr60-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mase</surname><given-names>K.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Recognition of facial expressions from optical flow</article-title>. <source>IEICE Transactions</source>, <volume>74</volume>, <fpage>3474</fpage>–<lpage>3483</lpage>.</citation>
</ref>
<ref id="bibr61-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Matsumoto</surname><given-names>D.</given-names></name>
<name><surname>Willingham</surname><given-names>B.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The thrill of victory and the agony of defeat: Spontaneous expressions of medal winners of the 2004 Athens Olympic Games</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>91</volume>, <fpage>568</fpage>–<lpage>581</lpage>.</citation>
</ref>
<ref id="bibr62-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mehu</surname><given-names>M.</given-names></name>
<name><surname>Little</surname><given-names>A. C.</given-names></name>
<name><surname>Dunbar</surname><given-names>R. I. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Duchenne smiles and the perception of generosity and sociability in faces</article-title>. <source>Journal of Evolutionary Psychology</source>, <volume>5</volume>, <fpage>133</fpage>–<lpage>146</lpage>.</citation>
</ref>
<ref id="bibr63-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Oda</surname><given-names>R.</given-names></name>
<name><surname>Yamagata</surname><given-names>N.</given-names></name>
<name><surname>Yabiku</surname><given-names>Y.</given-names></name>
<name><surname>Matsumoto-Oda</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Altruism can be assessed correctly based on impression</article-title>. <source>Human Nature</source>, <volume>20</volume>, <fpage>331</fpage>–<lpage>341</lpage>.</citation>
</ref>
<ref id="bibr64-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Oliver</surname><given-names>N.</given-names></name>
<name><surname>Pentland</surname><given-names>A.</given-names></name>
<name><surname>Berard</surname><given-names>F.</given-names></name>
</person-group> (<year>2000</year>). <article-title>LAFTER: A real-time face and lips tracker with facial expression recognition</article-title>. <source>Pattern Recognition</source>, <volume>33</volume>, <fpage>1369</fpage>–<lpage>1382</lpage>.</citation>
</ref>
<ref id="bibr65-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Otsuka</surname><given-names>T.</given-names></name>
<name><surname>Ohya</surname><given-names>J.</given-names></name>
</person-group> (<year>1997</year>). <source>A study of transformation of facial expressions based on expression recognition from temporal image sequences</source> (Tech. rep.). <publisher-loc>Tokyo, Japan</publisher-loc>: <publisher-name>Institute of Electronic information, and Communications Engineers</publisher-name>.</citation>
</ref>
<ref id="bibr66-0018720812440279">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Paiva</surname><given-names>A.</given-names></name>
<name><surname>Prada</surname><given-names>R.</given-names></name>
<name><surname>Picard</surname><given-names>R. W.</given-names></name>
</person-group> (<year>2007</year>, <month>September</month>). <source>Affective computing and intelligent interaction</source>. <conf-name>Paper presented at the Second International Conference on Affective Computing and Intelligent Interaction</conf-name>, <conf-loc>Lisbon, Portugal</conf-loc>.</citation>
</ref>
<ref id="bibr67-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Palomba</surname><given-names>D.</given-names></name>
<name><surname>Sarlo</surname><given-names>M.</given-names></name>
<name><surname>Angrilli</surname><given-names>A.</given-names></name>
<name><surname>Mini</surname><given-names>A.</given-names></name>
<name><surname>Stegagno</surname><given-names>L.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Cardiac responses associated with affective processing of unpleasant film stimuli</article-title>. <source>International Journal of Psychophysiology</source>, <volume>36</volume>, <fpage>45</fpage>–<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr68-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pantic</surname><given-names>M.</given-names></name>
<name><surname>Rothkrantz</surname><given-names>L. J. M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Automatic analysis of facial expressions: The state of the art</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>22</volume>, <fpage>1424</fpage>–<lpage>1445</lpage>.</citation>
</ref>
<ref id="bibr69-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pantic</surname><given-names>M.</given-names></name>
<name><surname>Rothkrantz</surname><given-names>L. J. M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Toward an affect- sensitive multimodal human-computer interaction</article-title>. <source>Proceedings of the IEEE</source>, <volume>91</volume>, <fpage>1370</fpage>–<lpage>1390</lpage>.</citation>
</ref>
<ref id="bibr70-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pantic</surname><given-names>M.</given-names></name>
<name><surname>Sebe</surname><given-names>N.</given-names></name>
<name><surname>Cohn</surname><given-names>J.</given-names></name>
<name><surname>Huang</surname><given-names>T. S.</given-names></name>
</person-group> (<year>2005</year>). <source>Affective multimodal human-computer interaction</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>.</citation>
</ref>
<ref id="bibr71-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Picard</surname><given-names>R. W.</given-names></name>
</person-group> (<year>1997</year>). <source>Affective computing</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr72-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pillutla</surname><given-names>M.</given-names></name>
<name><surname>Murnighan</surname><given-names>J.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Unfairness, anger, and spite: Emotional rejections of ultimatum offers. Organizational Behavior &amp; Human Decision Processes</article-title>, <volume>68</volume>, <fpage>208</fpage>–<lpage>224</lpage>.</citation>
</ref>
<ref id="bibr73-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Provine</surname><given-names>R. R.</given-names></name>
</person-group> (<year>2001</year>). <source>Laughter: A scientific investigation</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Penguin</publisher-name>.</citation>
</ref>
<ref id="bibr74-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rosenblum</surname><given-names>M.</given-names></name>
<name><surname>Yacoob</surname><given-names>Y.</given-names></name>
<name><surname>Davis</surname><given-names>L.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Human expression recognition from motion using radial basis function network architecture</article-title>. <source>IEEE Transactions on Neural Networks</source>, <volume>7</volume>, <fpage>1121</fpage>–<lpage>1138</lpage>.</citation>
</ref>
<ref id="bibr75-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>J. A.</given-names></name>
<name><surname>Barrett</surname><given-names>L. F.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Core affect, prototypical emotional episodes, and other things called emotion: Dissecting the elephant</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>76</volume>, <fpage>805</fpage>–<lpage>819</lpage>.</citation>
</ref>
<ref id="bibr76-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
</person-group> (<year>1984</year>). <source>Emotion as a multicomponent process: A model and some cross-cultural data</source>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr77-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
</person-group> (<year>2001</year>). <source>The nature and study of appraisal: A review of the issues</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr78-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The dynamic architecture of emotion: Evidence for the component process model</article-title>. <source>Cognition and Emotion</source>, <volume>23</volume>, <fpage>307</fpage>–<lpage>351</lpage>.</citation>
</ref>
<ref id="bibr79-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
<name><surname>Ceschi</surname><given-names>G.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Criteria for emotion recognition from verbal and nonverbal expression: Studying baggage loss in the airport</article-title>. <source>Personality and Social Psychology Bulletin</source>, <volume>26</volume>, <fpage>327</fpage>–<lpage>339</lpage>.</citation>
</ref>
<ref id="bibr80-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schober</surname><given-names>M. F.</given-names></name>
<name><surname>Clark</surname><given-names>H. H.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Understanding by addressees and over hearers</article-title>. <source>Cognitive Psychology</source>, <volume>21</volume>, <fpage>211</fpage>–<lpage>232</lpage>.</citation>
</ref>
<ref id="bibr81-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schug</surname><given-names>J.</given-names></name>
<name><surname>Matsumoto</surname><given-names>D.</given-names></name>
<name><surname>Horita</surname><given-names>Y.</given-names></name>
<name><surname>Yamagishi</surname><given-names>T.</given-names></name>
<name><surname>Bonnet</surname><given-names>K.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Emotional expressivity as a signal of cooperation</article-title>. <source>Evolution and Human Behavior</source>, <volume>31</volume>, <fpage>87</fpage>–<lpage>94</lpage>.</citation>
</ref>
<ref id="bibr82-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schwartz</surname><given-names>A.</given-names></name>
<name><surname>Gerin</surname><given-names>W.</given-names></name>
<name><surname>Christenfeld</surname><given-names>N.</given-names></name>
<name><surname>Glynn</surname><given-names>L.</given-names></name>
<name><surname>Davidson</surname><given-names>K.</given-names></name>
<name><surname>Pickering</surname><given-names>T.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Effects of an anger recall task on post-stress rumination and blood pressure recovery in men and women</article-title>. <source>Psychophysiology</source>, <volume>37</volume>, <fpage>12</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr83-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Siegel</surname><given-names>S.</given-names></name>
<name><surname>Castellan</surname><given-names>N. J.</given-names></name>
</person-group> (<year>1988</year>). <source>Nonparametric statistics for the behavioral sciences</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr84-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Smyth</surname><given-names>J.</given-names></name>
<name><surname>Ockenfels</surname><given-names>M. C.</given-names></name>
<name><surname>Porter</surname><given-names>L.</given-names></name>
<name><surname>Kirschbaum</surname><given-names>C.</given-names></name>
<name><surname>Hellhammer</surname><given-names>D. H.</given-names></name>
<name><surname>Stone</surname><given-names>A. A.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Stressors and mood measured on a momentary basis are associated with salivary cortisol secretion</article-title>. <source>Psychoneuroendocrinology</source>, <volume>23</volume>, <fpage>353</fpage>–<lpage>370</lpage>.</citation>
</ref>
<ref id="bibr85-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Traum</surname><given-names>D. R.</given-names></name>
<name><surname>Allen</surname><given-names>J. F.</given-names></name>
</person-group> (<year>1994</year>). <source>Discourse obligations in dialogue processing</source>. In <source>Proceedings of the 32nd annual meeting of ACL</source> (pp. <fpage>1</fpage>–<lpage>8</lpage>). <publisher-loc>Stroudsburg, PA</publisher-loc>: <publisher-name>ACL</publisher-name>. DOI 10.3115/981732.981733<pub-id pub-id-type="doi">10.3115/981732.981733</pub-id></citation>
</ref>
<ref id="bibr86-0018720812440279">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Truong</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>). <source>How does real affect affect affect recognition in speech?</source> (<comment>Unpublished doctoral dissertation</comment>). <publisher-name>University of Twente, Enschede</publisher-name>, <publisher-loc>Netherlands</publisher-loc>.</citation>
</ref>
<ref id="bibr87-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tuomela</surname><given-names>R.</given-names></name>
</person-group> (<year>2000</year>). <source>Cooperation: A philosophical study. Philosophical studies series 82</source>. <publisher-loc>Dordrecht, Netherlands</publisher-loc>: <publisher-name>Kluwer</publisher-name>.</citation>
</ref>
<ref id="bibr88-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>van Eck</surname><given-names>M.</given-names></name>
<name><surname>Berkhof</surname><given-names>H.</given-names></name>
<name><surname>Nicolson</surname><given-names>N.</given-names></name>
<name><surname>Sulon</surname><given-names>J.</given-names></name>
</person-group> (<year>1996</year>). <article-title>The effects of perceived stress, traits, mood states, and stressful daily events on salivary cortisol</article-title>. <source>Psychosomatic Medicine</source>, <volume>58</volume>, <fpage>447</fpage>–<lpage>458</lpage>.</citation>
</ref>
<ref id="bibr89-0018720812440279">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Vidrascu</surname><given-names>L.</given-names></name>
<name><surname>Devillers</surname><given-names>L.</given-names></name>
</person-group> (<year>2007</year>). <source>Five emotion classes detection in real-world call center data: The use of various types of paralinguistic features</source>. In <conf-name>International workshop on paralinguistic speech—Between models and data</conf-name>, <comment>Available at <ext-link ext-link-type="uri" xlink:href="http://www2.dfki.de/paraling07/papers/05.pdf">http://www2.dfki.de/paraling07/papers/05.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr90-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wagner</surname><given-names>H. L.</given-names></name>
</person-group> (<year>1993</year>). <article-title>On measuring performance in category judgment studies on nonverbal behavior</article-title>. <source>Journal of Non- verbal Behavior</source>, <volume>17</volume>, <fpage>3</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr91-0018720812440279">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Walton</surname><given-names>D.</given-names></name>
<name><surname>Krabbe</surname><given-names>E.</given-names></name>
</person-group> (<year>1995</year>). <source>Commitment in dialogue: Basic concepts of interpersonal reasoning</source>. <publisher-loc>Albany</publisher-loc>: <publisher-name>State University of New York Press</publisher-name>.</citation>
</ref>
<ref id="bibr92-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Xiao</surname><given-names>E.</given-names></name>
<name><surname>Houser</surname><given-names>D.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Emotion expression in human punishment behavior</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>102</volume>, <fpage>7398</fpage>–<lpage>7401</lpage>.</citation>
</ref>
<ref id="bibr93-0018720812440279">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yacoob</surname><given-names>Y.</given-names></name>
<name><surname>Davis</surname><given-names>L.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Recognizing human facial expressions from long image sequences using optical flow</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>18</volume>, <fpage>636</fpage>–<lpage>642</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>