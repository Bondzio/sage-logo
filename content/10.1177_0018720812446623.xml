<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HFS</journal-id>
<journal-id journal-id-type="hwp">sphfs</journal-id>
<journal-title>Human Factors: The Journal of Human Factors and Ergonomics Society</journal-title>
<issn pub-type="ppub">0018-7208</issn>
<issn pub-type="epub">1547-8181</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0018720812446623</article-id>
<article-id pub-id-type="publisher-id">10.1177_0018720812446623</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Psychomotor Processes</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Using Multisensory Cues to Facilitate Air Traffic Management</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Jacobs</surname><given-names>Karen</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Soares</surname><given-names>Marcelo</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Ngo</surname><given-names>Mary K.</given-names></name>
<aff id="aff1-0018720812446623">University of Oxford, Oxford, United Kingdom</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Pierce</surname><given-names>Russell S.</given-names></name>
<aff id="aff2-0018720812446623">University of California, Riverside</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Spence</surname><given-names>Charles</given-names></name>
<aff id="aff3-0018720812446623">University of Oxford, Oxford, United Kingdom</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0018720812446623">Mary Kim Ngo, Crossmodal Research Laboratory, Department of Experimental Psychology, University of Oxford, South Parks Rd., Oxford, OX1 3UD, United Kingdom; e-mail: <email>mngo1028@gmail.com</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>54</volume>
<issue>6</issue>
<issue-title>Special Section: Keynote Addresses From the 18th Triennial Congress of the International Ergonomics Association</issue-title>
<fpage>1093</fpage>
<lpage>1103</lpage>
<history>
<date date-type="received">
<day>10</day>
<month>10</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>3</day>
<month>4</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012, Human Factors and Ergonomics Society</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<sec id="section1-0018720812446623">
<title>Objective:</title>
<p>In the present study, we sought to investigate whether auditory and tactile cuing could be used to facilitate a complex, real-world air traffic management scenario.</p>
</sec>
<sec id="section2-0018720812446623">
<title>Background:</title>
<p>Auditory and tactile cuing provides an effective means of improving both the speed and accuracy of participants’ performance in a variety of laboratory-based visual target detection and identification tasks.</p>
</sec>
<sec id="section3-0018720812446623">
<title>Method:</title>
<p>A low-fidelity air traffic simulation task was used in which participants monitored and controlled aircraft. The participants had to ensure that the aircraft landed or exited at the correct altitude, speed, and direction and that they maintained a safe separation from all other aircraft and boundaries. The performance measures recorded included en route time, handoff delay, and conflict resolution delay (the performance measure of interest). In a baseline condition, the aircraft in conflict was highlighted in red (visual cue), and in the experimental conditions, this standard visual cue was accompanied by a simultaneously presented auditory, vibrotactile, or audiotactile cue.</p>
</sec>
<sec id="section4-0018720812446623">
<title>Results:</title>
<p>Participants responded significantly more rapidly, but no less accurately, to conflicts when presented with an additional auditory or audiotactile cue than with either a vibrotactile or visual cue alone.</p>
</sec>
<sec id="section5-0018720812446623">
<title>Conclusion:</title>
<p>Auditory and audiotactile cues have the potential for improving operator performance by reducing the time it takes to detect and respond to potential visual target events.</p>
</sec>
<sec id="section6-0018720812446623">
<title>Application:</title>
<p>These results have important implications for the design and use of multisensory cues in air traffic management.</p>
</sec>
</abstract>
<kwd-group>
<kwd>multisensory cuing</kwd>
<kwd>auditory cuing</kwd>
<kwd>vibrotactile cuing</kwd>
<kwd>temporal cuing</kwd>
<kwd>air traffic control</kwd>
<kwd>visual target detection</kwd>
<kwd>attention</kwd>
<kwd>multisensory facilitation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section7-0018720812446623" sec-type="intro">
<title>Introduction</title>
<p>It is well documented that auditory and tactile cues can help people to find and identify visual targets more efficiently than when no such cue is presented. The benefits of cross-modal cuing appear to be maximal when the cues are presented at the same time and from the same relative location (or direction) as the visual target (e.g., <xref ref-type="bibr" rid="bibr13-0018720812446623">Ho, Tan, &amp; Spence, 2006</xref>; <xref ref-type="bibr" rid="bibr23-0018720812446623">Santangelo, Ho, &amp; Spence, 2008</xref>; <xref ref-type="bibr" rid="bibr25-0018720812446623">Spence, 2012</xref>; <xref ref-type="bibr" rid="bibr33-0018720812446623">Van der Burg, Olivers, Bronkhorst, &amp; Theeuwes, 2008</xref>, <xref ref-type="bibr" rid="bibr34-0018720812446623">2009</xref>; <xref ref-type="bibr" rid="bibr36-0018720812446623">Vroomen &amp; de Gelder, 2000</xref>). These studies have demonstrated that not only can cross-modal cues enhance the perceived saliency of target visual events, but also such cues can guide a person’s attention toward the spatial location of the target visual event, even when the cues happen to provide no information with regard to the target’s exact location (e.g., see <xref ref-type="bibr" rid="bibr26-0018720812446623">Spence &amp; Driver, 2000</xref>).</p>
<p>The paradigms used to study the effectiveness of cross-modal auditory and tactile cuing have typically involved basic, low-level, visual perceptual tasks, such as visual search, simple detection, and visual target discrimination, in which the variables and stimuli can be systematically controlled. In visual search tasks, for example, researchers typically vary the set size (the number of stimuli presented on the screen at any one time), the presence of the target, and the similarity between the distractors and the target to examine how these factors might affect participants’ visual search performance. On a given trial, the target and distractors typically remain in static positions, and the participant simply has to search for, and identify, the target in the display.</p>
<p>The findings from such laboratory-based research paradigms have, however, been questioned because of their lack of ecological validity (e.g., <xref ref-type="bibr" rid="bibr38-0018720812446623">Wolfe, 1994</xref>; <xref ref-type="bibr" rid="bibr40-0018720812446623">Wolfe, Horowitz, &amp; Kenner, 2005</xref>; <xref ref-type="bibr" rid="bibr42-0018720812446623">Wolfe, Võ, Evans, &amp; Greene, 2011</xref>). First, visual search in the real world often involves complex, ecologically realistically targets and distractors (e.g., keys, faces, weapons; <xref ref-type="bibr" rid="bibr3-0018720812446623">Chen &amp; Zelinsky, 2006</xref>; <xref ref-type="bibr" rid="bibr16-0018720812446623">McCarley, Kramer, Wickens, Vidoni, &amp; Boot, 2004</xref>; <xref ref-type="bibr" rid="bibr35-0018720812446623">Van Wert, Horowitz, &amp; Wolfe, 2009</xref>; <xref ref-type="bibr" rid="bibr39-0018720812446623">Wolfe, 2001</xref>; <xref ref-type="bibr" rid="bibr40-0018720812446623">Wolfe et al., 2005</xref>, <xref ref-type="bibr" rid="bibr42-0018720812446623">2011</xref>; <xref ref-type="bibr" rid="bibr41-0018720812446623">Wolfe &amp; Van Wert, 2010</xref>). Second, observers are not always aware of the exact identity of the target. In addition, a clear view of the target may be obstructed or occluded by the presence of overlapping objects. Third, target events, such as bombs and air traffic conflicts—fortunately—occur very infrequently, unlike prevalence rates typically used in laboratory-based visual search experiments, which normally range from 50% target-present trials upward (cf. <xref ref-type="bibr" rid="bibr41-0018720812446623">Wolfe &amp; Van Wert, 2010</xref>).</p>
<p>Typical search times for simple, laboratory-based tasks average 400 to 500 ms for set sizes greater than 12, whereas complex search tasks often average upward of 1,000 ms (<xref ref-type="bibr" rid="bibr28-0018720812446623">Taylor &amp; Cutsuridis, 2011</xref>; <xref ref-type="bibr" rid="bibr30-0018720812446623">Treisman &amp; Gelade, 1980</xref>; <xref ref-type="bibr" rid="bibr31-0018720812446623">Treisman &amp; Sato, 1990</xref>; <xref ref-type="bibr" rid="bibr32-0018720812446623">Treisman &amp; Souther, 1985</xref>) and upward of 2,000 ms for searches for targets in naturalistic scenes (<xref ref-type="bibr" rid="bibr3-0018720812446623">Chen &amp; Zelinsky, 2006</xref>; <xref ref-type="bibr" rid="bibr28-0018720812446623">Taylor &amp; Cutsuridis, 2011</xref>; <xref ref-type="bibr" rid="bibr35-0018720812446623">Van Wert et al., 2009</xref>; <xref ref-type="bibr" rid="bibr38-0018720812446623">Wolfe, 1994</xref>; <xref ref-type="bibr" rid="bibr42-0018720812446623">Wolfe et al., 2011</xref>). Although 2,000 ms may be acceptable in laboratory settings, it might be considered too long (i.e., unacceptable) in real-world settings, where lives and safety often depend on an operator’s rapid response (e.g., to potential threats, road hazards, collisions; e.g., see <xref ref-type="bibr" rid="bibr12-0018720812446623">Ho &amp; Spence, 2005</xref>). Obermayer, Nugent, and Linville (2004) noted that change blindness is pervasive in applied settings, where “operators of such systems are often heavily loaded with concurrent visual search, situation assessment, voice communications, and control display manipulation tasks at large” (p. 205). Perhaps not surprisingly, this fact has spurred a lot of interest, for good reason, in finding ways to improve visual search and visual target detection and identification performance.</p>
<p>The potential benefits of cross-modal cuing observed in laboratory-based research have only recently been applied to settings such as air traffic control, military operations, and vehicular operation (e.g., <xref ref-type="bibr" rid="bibr2-0018720812446623">Brill et al., 2004</xref>; <xref ref-type="bibr" rid="bibr5-0018720812446623">Fitch, Keifer, Hankey, &amp; Kleiner, 2007</xref>; <xref ref-type="bibr" rid="bibr10-0018720812446623">Ho, Reed, &amp; Spence, 2006</xref>, <xref ref-type="bibr" rid="bibr11-0018720812446623">2007</xref>; <xref ref-type="bibr" rid="bibr12-0018720812446623">Ho &amp; Spence, 2005</xref>). However, the focus for much of this applied research seems to be on the use of multisensory warning signals for driving simulation tasks, whereas research on the potential benefits of implementing nonvisual warning signals in air traffic management environments seems to have fallen somewhat by the wayside. The fact that limited research has been conducted on warning signals in the air traffic management sector is not surprising, given the fact that for air traffic simulation tasks, the visual target is harder to define, control, and predict as compared with the situation in a driving simulator, where the visual target might be a clearly defined collision (front end or rear end) or specific visual target (e.g., a road sign or cyclist) in the simulated visual scene. Moreover, in a driving simulator, the operator is responsible only for the car he or she is driving, whereas in an air traffic management simulation, the operator is responsible for a number of aircraft and required to monitor them all simultaneously.</p>
<p>That said, there has been some intriguing cross-modal research in the area of air traffic control. For instance, <xref ref-type="bibr" rid="bibr9-0018720812446623">Hameed, Jayaraman, Ballard, and Sarter (2007)</xref> introduced tactile cues to an air traffic control simulation task and found that participants found it significantly easier to detect target visual events (e.g., requests for altitude change, potential conflict, or handoff, which occurs when an air traffic controller assumes responsibility for an aircraft that has been passed from a previous sector to his or her sector) and responded more rapidly to these events.</p>
<p>Although the results of Hameed et al.’s study are certainly promising, there are a few points worth noting about the conditions of the study. First, Hameed et al. only examined tactile cuing; they did not explore the potential benefits of either cross-modal auditory or audiotactile cuing. Second, the visual target events occurred quite frequently, once every 15 s, which is much more frequently than would be expected during realistic air traffic control situations (in which such events are exceedingly rare; visit <ext-link ext-link-type="uri" xlink:href="http://www.airsafe.com/ten_faq.htm">http://www.airsafe.com/ten_faq.htm</ext-link> for a recent review of incidents). In a real air-traffic control setting, cues occurring at this frequency would likely mean that a high percentage of the cues would be associated with events that do not require a response (e.g., false alarms or low-priority events). This frequency introduces the potential for the cry-wolf effect, whereby operators simply start to ignore the warning signals altogether if they occur too frequently (<xref ref-type="bibr" rid="bibr1-0018720812446623">Breznitz, 1983</xref>; however, see also <xref ref-type="bibr" rid="bibr37-0018720812446623">Wickens et al., 2009</xref>, for more recent counterevidence). In this case, the benefit of the warning signal, when actually needed, may no longer be apparent.</p>
<p>The goal of the present study was to examine whether the benefits of auditory (Experiment 1), tactile (Experiment 2), and audiotactile (Experiment 3) cuing, in addition to a visual cue, could be exploited in an air traffic control simulation in which participants were required to monitor and control a number of aircraft that were dynamically changing and being updated regularly. Importantly, the auditory and vibrotactile cues in the present study were presented only with a conflict situation (e.g., loss of separation with either a boundary or another aircraft), which comprised approximately 50% of all visual events, rather than with every visual event, given the fact that conflict situations are potentially more dangerous than aircraft coming into or exiting an operator’s airspace.</p>
</sec>
<sec id="section8-0018720812446623">
<title>Experiment 1</title>
<sec id="section9-0018720812446623">
<title>Method</title>
<sec id="section10-0018720812446623">
<title>Participants</title>
<p>For this experiment, 10 participants from the University of Oxford, ranging in age from 21 to 32 years of age (8 female; mean age of 27 years), took part. This study was performed in accordance with the ethical standards laid down in the 1964 Declaration of Helsinki, and all procedures were carried out with the adequate understanding and written consent of the participants. Formal approval to conduct the experiments described was obtained from the human participants review board of the Department of Experimental Psychology, Oxford. The experiment took approximately 1 hr and 15 min to complete. Participants received £10 in gift vouchers as compensation for taking part in the study.</p>
</sec>
<sec id="section11-0018720812446623">
<title>Air Traffic Scenarios Test (ATST)</title>
<p>The ATST is a low-fidelity, simplified air traffic control simulation developed by the Federal Aviation Administration (<ext-link ext-link-type="uri" xlink:href="http://www.faa.gov">www.faa.gov</ext-link>) in 1996. The participant’s task in the ATST is to guide icons representing aircraft to one of two airports or four sector gates (see <xref ref-type="fig" rid="fig1-0018720812446623">Figure 1</xref>). Sector gates represent points of exit from the sector that participants are controlling in the simulation. The participants were given instructions concerning how to achieve the goals of the simulation and how to obey various rules of flight by moving the aircraft through the airspace, maintaining safe separation, and ensuring that the aircraft landed or exited at the correct altitude, speed, and heading for its destination.</p>
<fig id="fig1-0018720812446623" position="float">
<label>Figure 1.</label>
<caption>
<p>A monochrome depiction of the Air Traffic Scenarios Test used in Experiments 1 through 3. Sector gates are marked with capital letters (A, B, C, or D), and airports are marked with lowercase letters (a or b). A data tag with the aircraft’s speed (F for fast, M for medium, or S for slow), altitude (3, 2, or 1), and destination (to a sector gate or airport) was provided next to the arrow representing each aircraft. Command buttons for altering the heading, speed, and altitude of the aircraft were placed on the top-right side of the screen. A 5-mile distance indicator and a time indicator were also displayed to the bottom left of the command buttons.</p>
</caption>
<graphic xlink:href="10.1177_0018720812446623-fig1.tif"/></fig>
<p>Participants chose from three altitudes (1 = low, 2 = medium, and 3 = high, which were separated by 1 nautical mile [nm]), three speeds (measured in nautical miles per 7-s radar sweep; F = 2.50 nm per sweep, M = 1.67 nm per sweep, and S = 0.83 nm per sweep), and eight headings (0 = north, 1 = northeast, 2 = east, 3 = southeast, 4 = south, 5 = southwest, 6 = west, and 7 = northwest). They were instructed to obey the following flight rules: A 5-nm lateral separation and 1-nm altitude-level vertical separation must be maintained between aircraft, aircraft must maintain a 5-nm lateral distance from all boundaries, aircraft must land at slow speed and low altitude, aircraft must land at airports from a heading of left to right, and aircraft must exit at fast speed and high altitude.</p>
<p>Participants controlled the aircraft by clicking on a target aircraft and then on the command buttons (for heading, speed, and altitude) on the right side of the display. The target aircraft turned yellow when it was clicked, indicating that a change was now ready to be made. The target aircraft remained yellow until the participant made a change (even if it was to maintain the same heading, speed, and altitude), at which point the aircraft returned to its original green color. The aircraft responded immediately to any change with 100% accuracy. Aircraft positions were updated every 7 s to simulate radar sweeps. Scenarios started with seven aircraft already in motion, and a new aircraft appeared in gray at the periphery of the airspace every 30 s. Aircraft in gray did not move and were considered to be awaiting handoff. When participants clicked on an aircraft’s icon to take the handoff, it turned green and began moving after the next “radar sweep.”</p>
<p>The ATST measures three aspects of performance (handoff delay, en route delay, and simulation errors). In our modified version of the ATST, conflict resolution delay was included as an additional performance measure. At the start of each radar sweep, each aircraft in gray awaiting its initial selection accrued 7 s of handoff delay. En route delay was calculated as the sum of the differences between the actual flight durations and optimal flight durations of each correctly handled aircraft. Optimal flight durations were determined by the simulator software as the time it would take for an aircraft to fly in a straight line from the point of origin to its destination at maximum speed. Handoff and en route delays were measured in cumulative seconds from the start of the aircraft entry into the simulation. Simulation errors were tallied whenever an aircraft lost separation, arrived at an incorrect destination, arrived improperly (i.e., at the wrong speed, altitude, or heading), or intersected a boundary, another aircraft, or an airport. Conflict resolution delay, or reaction time (RT), was measured (in milliseconds) from the onset of the loss of separation between an aircraft and either a boundary or another aircraft until the participant initiated a response (i.e., clicking on the aircraft in conflict in an attempt to resolve the conflict).</p>
</sec>
<sec id="section12-0018720812446623">
<title>Design</title>
<p>The experiment was conducted on a desktop computer with the use of Microsoft Visual Studio 2010. We created four air traffic scenarios (two practice and two test scenarios) that were designed to be equivalent in their level of difficulty. Each scenario started with seven aircraft already in motion, with a new aircraft appearing every 30 s. The participants controlled a minimum of seven aircraft at all times. Whenever the number of active aircraft in the scenario fell below seven (e.g., because an aircraft landed, exited, or crashed), an aircraft awaiting handoff automatically became active to fulfill the minimum seven aircraft load. Participants were required only to monitor and control the seven aircraft in motion, but they were also encouraged to try to take on more traffic if they felt that they could handle it.</p>
<p>Whenever an aircraft crashed into a boundary or another aircraft, a red cross appeared in place of the aircraft at the location of the crash. Whenever an aircraft violated separation requirements (with a boundary or with another aircraft), the aircraft was highlighted in red (i.e., an intramodal visual cue was presented) to indicate that a loss of separation had occurred. In the experimental condition of interest, a 500-Hz tone (44.1 kHz sampling rate, 16 bit) with a 60-ms duration (including a 5-ms fade-in and fade-out to avoid clicks) was played when the conflict occurred. Note that this cross-modal auditory cue was redundant in the sense that the red highlighting was also presented visually. The tone was presented from two Dell A215 PC loudspeaker cones (Dell Inc., UK), one placed 16° to either side of the center of the visual display. The presence or absence of the tone was varied on a block-by-block basis and counterbalanced across participants. The experiment consisted of two 10-min practice blocks and two 20-min test blocks.</p>
</sec>
<sec id="section13-0018720812446623">
<title>Procedure</title>
<p>Participants were first given instructions concerning how to achieve the goals of the simulation. The participants were also informed about the rules of flight that they were meant to follow. The participants were instructed to land and exit as many planes as possible but to ensure that the planes landed and exited safely, at the correct altitude and speed. Before engaging in practice, the experimenter provided participants with a brief demonstration and introduction to all the elements of the simulation. The participants were instructed on how to make changes to the aircraft, how to move the aircraft through the airspace, how to maintain safe separation, and how to land or exit the aircraft appropriately. The participants were also shown what a crash and loss-of-separation conflict would look (and sound) like. After participants confirmed that they understood the task, they began their first practice block, followed by the second practice and two test blocks; they were allowed to take breaks between blocks if they so desired.</p>
</sec>
</sec>
<sec id="section14-0018720812446623">
<title>Results and Discussion</title>
<p>We conducted separate paired-samples <italic>t</italic> tests to evaluate the differences between the cross-modal cue-present and cue-absent conditions for each measure of task performance (handoff delay, en route delay, simulation errors, and conflict resolution delay). The mean values for each measure of task performance are also reported for completeness in <xref ref-type="table" rid="table1-0018720812446623">Table 1</xref> for each condition and experiment. The analyses revealed that participants responded significantly more rapidly to a conflict when a cross-modal auditory cue was presented (in addition to the visual cue) at the time of the conflict (<italic>M</italic> = 2,111 ms, <italic>SE</italic> = 389 ms) as compared with when only the visual cue was presented (<italic>M</italic> = 3,151 ms, <italic>SE</italic> = 735 ms), <italic>t</italic>(9) = 2.55, <italic>p</italic> = .03. None of the other measures of task performance differed significantly between the cue-present and cue-absent conditions, <italic>t</italic> &lt; 1 for all comparisons.</p>
<table-wrap id="table1-0018720812446623" position="float">
<label>Table 1:</label>
<caption>
<p>Mean Conflict Resolution, Handoff, and En Route Delays and Simulation Errors for the Various Cue Conditions in Experiments 1 Through 3</p>
</caption>
<graphic alternate-form-of="table1-0018720812446623" xlink:href="10.1177_0018720812446623-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Cue Condition</th>
<th align="center">Conflict Resolution Delay (ms)</th>
<th align="center">Handoff Delay (s)</th>
<th align="center">En Route Delay (s)</th>
<th align="center">Simulation Errors</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">Experiment 1</td>
</tr>
<tr>
<td> Auditory absent</td>
<td>3,151<sup>a</sup></td>
<td>12,855</td>
<td>6,565</td>
<td>11</td>
</tr>
<tr>
<td> Auditory present</td>
<td>2,111<sup>a</sup></td>
<td>12,433</td>
<td>6,675</td>
<td>11</td>
</tr>
<tr>
<td colspan="5">Experiment 2</td>
</tr>
<tr>
<td> Tactile absent</td>
<td>2,898</td>
<td>9,445</td>
<td>6,513</td>
<td>16</td>
</tr>
<tr>
<td> Tactile present</td>
<td>2,606</td>
<td>9,082</td>
<td>6,342</td>
<td>13</td>
</tr>
<tr>
<td colspan="5">Experiment 3</td>
</tr>
<tr>
<td> No cue</td>
<td>4,104<sup>a, b</sup></td>
<td>5,767</td>
<td>4,822</td>
<td>17</td>
</tr>
<tr>
<td> Auditory</td>
<td>2,956<sup>a</sup></td>
<td>6,249</td>
<td>4,670</td>
<td>16</td>
</tr>
<tr>
<td> Vibrotactile</td>
<td>3,564<sup>c</sup></td>
<td>5,416</td>
<td>4,816</td>
<td>21</td>
</tr>
<tr>
<td> Audiotactile</td>
<td>2,282<sup>b, c</sup></td>
<td>3,664</td>
<td>4,986</td>
<td>16</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0018720812446623">
<p><italic>Note</italic>. Values with superscripts within the same column are significantly different (<italic>p</italic> &lt; .008) according to post hoc <italic>t</italic> tests, with Bonferroni correction.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The results of Experiment 1 demonstrate that the presentation of a cross-modal auditory cue, in addition to the existing intramodal visual cue used to signal conflict situations, gave rise to a significant reduction in the amount of time it took participants to respond to impending potential collisions. These results therefore support and extend the extant literature on visual search in the laboratory to a more applied setting and demonstrate the value of implementing multisensory, auditory warning signals in air traffic control environments not only during training but, more importantly, in real time.</p>
<p>Although the success of auditory cuing at facilitating performance has been demonstrated consistently in a variety of tasks involving speeded visual target detection and identification (e.g., <xref ref-type="bibr" rid="bibr8-0018720812446623">Haas, Pillalamarri, Stachowiak, &amp; Lattin, 2005</xref>; <xref ref-type="bibr" rid="bibr20-0018720812446623">Noesselt, Bergmann, Hake, Heinze, &amp; Fendrich, 2008</xref>; <xref ref-type="bibr" rid="bibr21-0018720812446623">Osborn, Sheldon, &amp; Baker, 1963</xref>; <xref ref-type="bibr" rid="bibr24-0018720812446623">Santangelo &amp; Spence, 2007</xref>; <xref ref-type="bibr" rid="bibr33-0018720812446623">Van der Burg et al., 2008</xref>, <xref ref-type="bibr" rid="bibr36-0018720812446623">Vroomen &amp; de Gelder, 2000</xref>), the effectiveness of tactile cuing has not always been shown (see <xref ref-type="bibr" rid="bibr13-0018720812446623">Ho, Tan, et al., 2006</xref>; <xref ref-type="bibr" rid="bibr23-0018720812446623">Santangelo et al., 2008</xref>; <xref ref-type="bibr" rid="bibr33-0018720812446623">Van der Burg et al., 2008</xref>; see also <xref ref-type="bibr" rid="bibr27-0018720812446623">Spence &amp; Santangelo, 2009</xref>, for a review). The goal of Experiment 2 was therefore to test whether the effectiveness of temporally synchronous tactile cuing could be demonstrated in the ATST.</p>
</sec>
</sec>
<sec id="section15-0018720812446623">
<title>Experiment 2</title>
<sec id="section16-0018720812446623">
<title>Method</title>
<p>For Experiment 2, 11 new participants from the University of Oxford (7 female; age range = 19 to 32 years; mean age = 26 years) took part. The experimental design and procedure were identical to those used in Experiment 1. Instead of an auditory cue, however, participants were now presented with a suprathreshold, 200-Hz vibrotactile cue at the moment when a conflict occurred. The vibration was presented for 50 ms and was presented through two VBW32 (Audiological Engineering Corporation, Somerville, MA) tactors fastened to a waist belt, with one tactor placed on either side of the participant’s waist. At the start of each experimental session, the experimenter ensured that participants could clearly perceive the vibration. We presented white noise through closed-cup headphones to mask any potential sounds coming from vibrotactors.</p>
</sec>
<sec id="section17-0018720812446623">
<title>Results and Discussion</title>
<p>Separate paired-samples <italic>t</italic> tests conducted on participants’ handoff delays, en route delays, simulation errors, and conflict resolution delays revealed no significant differences between the vibrotactile cue present and absent conditions, <italic>t</italic> &lt; 1 for all comparisons except for the comparison of simulation errors, <italic>t</italic>(10) = 1.66, <italic>p</italic> = .13. The results of Experiment 2 therefore demonstrate that temporally synchronous vibrotactile cues were ineffective at facilitating participants’ dynamic visual monitoring performance. Specifically, participants did not detect or respond to conflicts any more rapidly when the cross-modal vibrotactile cue was presented along with the visual cue as compared with when the visual cue was presented alone. This finding is consistent with previous research by <xref ref-type="bibr" rid="bibr13-0018720812446623">Ho, Tan, et al. (2006)</xref> demonstrating that vibrotactile cues did not give rise to any facilitation of visual target discrimination performance for their participants whereas auditory cues did. Although between-experiments comparisons of the four performance measures might have clarified the differential effects of the auditory cue in Experiment 1 and the vibrotactile cue in Experiment 2, it should be noted that the large individual differences in participants’ performance between the two experiments made such comparisons potentially problematic.</p>
<p>The fact that the auditory cues in Experiment 1 gave rise to a clear improvement in participants’ conflict resolution performance, whereas the vibrotactile cues in Experiment 2 did not, might lead one to conclude that tactile cuing is ineffective when compared with auditory cuing. However, before one can draw such a conclusion, it would be necessary to directly compare participants’ performance in the presence of either auditory or vibrotactile cues. The goal of Experiment 3 was therefore to directly compare participants’ performance on the ATST following either an auditory cue or a vibrotactile cue with a baseline condition in which no cross-modal cue was presented.</p>
<p>Previous research by <xref ref-type="bibr" rid="bibr11-0018720812446623">Ho et al. (2007)</xref> examined the use of auditory, vibrotactile, and audiotactile cues in an applied, real-world driving simulation task. Ho et al. observed that participants initiated braking responses significantly more rapidly when an audiotactile cue was presented simultaneously with the onset of an impending front-end collision than when either an auditory or vibrotactile cue was presented. As such, in Experiment 3, we additionally explored the use of audiotactile cuing to determine whether these multisensory cues might lead to a greater magnitude of facilitation than either auditory or vibrotactile cue alone.</p>
</sec>
</sec>
<sec id="section18-0018720812446623">
<title>Experiment 3</title>
<sec id="section19-0018720812446623">
<title>Method</title>
<p>For Experiment 3, 15 participants from the University of Oxford, ranging in age from 18 to 32 years of age (7 female; mean age = 24 years), took part. The experimental setup was identical to that used in Experiment 2. In addition to the vibrotactile cue present and absent conditions, however, this experiment also included additional auditory and audiotactile cuing conditions. The four cuing conditions (absent, auditory, vibrotactile, and audiotactile) were varied on a block-by-block basis and randomized across participants. As before, the absent condition refers to the absence of any cross-modal cue. Again, we presented white noise through closed-cup headphones to mask any potential sounds coming from vibrotactors. We ensured that the auditory cue could be clearly heard above the white noise in the audiotactile cuing condition. The experiment consisted of two 10-min practice blocks and four 20-min test blocks and took approximately 2 hr to complete.</p>
</sec>
<sec id="section20-0018720812446623">
<title>Results and Discussion</title>
<p>Separate repeated-measures ANOVAs were conducted for each measure of task performance (handoff delay, en route delay, simulation errors, and conflict resolution delay) with cuing condition (absent, auditory, vibrotactile, and audiotactile) as the within-participants factor. The ANOVA on the conflict resolution delay revealed a significant effect of cuing, <italic>F</italic>(3, 42) = 10.19, <italic>p</italic> &lt; .001. Post hoc <italic>t</italic> tests with Bonferroni’s correction for multiple comparisons highlighted that participants responded significantly more rapidly to a conflict following the presentation of the audiotactile cue (<italic>M</italic> = 2,282 ms) as compared with the vibrotactile cue (<italic>M</italic> = 3,485 ms; <italic>p</italic> = .001) or when the visual cue was presented in isolation (<italic>M</italic> = 4,104 ms; <italic>p</italic> &lt; .001) (see <xref ref-type="fig" rid="fig2-0018720812446623">Figure 2</xref>). The auditory cue (<italic>M</italic> = 2,956 ms) also gave rise to significantly faster responding than when the visual cue was presented alone, <italic>p</italic> = .005. There was no significant difference between participants’ conflict resolution delays in the auditory and tactile cuing conditions, <italic>p</italic> = .16, nor between the vibrotactile and cross-modal cue-absent conditions, <italic>p</italic> = .13.</p>
<fig id="fig2-0018720812446623" position="float">
<label>Figure 2.</label>
<caption>
<p>Mean conflict resolution delay (in milliseconds) for the four cuing conditions (absent, auditory, vibrotactile, and audiotactile) in Experiment 3. The error bars represent the standard errors of the means.</p>
</caption>
<graphic xlink:href="10.1177_0018720812446623-fig2.tif"/></fig>
<p>There were no significant differences among the cuing conditions in either the mean number of simulation errors, <italic>p</italic> = .35, or en route delays, <italic>F</italic> &lt; 1. There was, however, a significant effect of cuing condition on participants’ handoff delays, <italic>F</italic>(3, 42) = 3.74, <italic>p</italic> = .018. Post hoc <italic>t</italic> tests revealed that participants accepted handoffs significantly more rapidly following the presentation of the audiotactile cue (<italic>M</italic> = 3,664 ms) as compared with the auditory cue (<italic>M</italic> = 6,249 ms; <italic>p</italic> = .017) and the vibrotactile cue (<italic>M</italic> = 5,416 ms; <italic>p</italic> = .041) or when the visual cue was presented alone (<italic>M</italic> = 5,767 ms; <italic>p</italic> = .037). Note, however, that after applying a Bonferroni correction for multiple tests, none of these comparisons was significant at the <italic>p &lt;</italic> .008 level.</p>
<p>The results of Experiment 3 therefore suggest that only the auditory and audiotactile cues gave rise to a statistically significant and reliable improvement in the speed of participants’ responses to conflicts, whereas the vibrotactile cues did not give rise to any such performance improvements. Thus, partially consistent with the results of Experiment 2, the vibrotactile cue once again proved to be ineffective at facilitating participants’ performance in the present experiment. The results of Experiment 3 are also consistent with previous research by <xref ref-type="bibr" rid="bibr11-0018720812446623">Ho et al. (2007)</xref> and <xref ref-type="bibr" rid="bibr24-0018720812446623">Santangelo and Spence (2007)</xref> comparing the relative effectiveness of auditory and vibrotactile cues with multisensory audiotactile cues in visual tasks. In particular, multisensory audiotactile cues appear to be most consistently effective, especially when compared with the vibrotactile cues, at facilitating visual target detection and identification performance across experimental settings, including those carried out in laboratory-based and real-world paradigms.</p>
</sec>
</sec>
<sec id="section21-0018720812446623" sec-type="discussion">
<title>General Discussion</title>
<p>The results of the three experiments reported in the present study highlight a number of key points. First, the results of Experiments 1 and 3 demonstrated that an auditory cue that was presented at the same time as the onset of a conflict (in addition to the standard visual cue) resulted in faster responses to the conflicts than when only the visual cue was presented. Second, the results of Experiments 2 and 3 both suggest that vibrotactile cues are ineffective when it comes to signaling the occurrence of a conflict. Third, the results of Experiment 3 also suggest that multisensory audiotactile cues are most effective and give rise to the most consistent benefit in participants’ response times as compared with the vibrotactile cues. The audiotactile cues even showed some numerical advantages to the auditory cues. Taken together, then, the findings show that in a realistic simulation and complex task environment, temporally synchronous auditory and audiotactile warning signals effectively capture attention, whereas vibrotactile cues do not.</p>
<p>Previous laboratory-based studies examining temporally synchronous auditory and vibrotactile cuing in visual search have suggested that the binding of the simultaneously presented cross-modal cue and visual target ultimately gives rise to the enhanced saliency of the visual target stimulus, which in turn results in improved visual target identification performance (<xref ref-type="bibr" rid="bibr33-0018720812446623">Van der Burg et al., 2008</xref>, <xref ref-type="bibr" rid="bibr34-0018720812446623">2009</xref>). This idea is also in line with the redundant target effect (RTE), first characterized by <xref ref-type="bibr" rid="bibr17-0018720812446623">Molholm, Ritter, Javitt, and Foxe (2004)</xref>, whereby responses to the combined auditory and visual (bimodal) targets are much faster as compared with unimodal auditory and visual target stimuli. Molholm and her colleagues suggested that the RTE occurs early in perceptual processing, potentially modulating neural processes in what have traditionally been considered to be unisensory cortices (see <xref ref-type="bibr" rid="bibr7-0018720812446623">Ghazanfar &amp; Schroeder, 2006</xref>).</p>
<p>It is interesting to note that the vibrotactile cues were ineffective at facilitating performance in the more realistic ATST, whereas in the laboratory-based visual search and visual target discrimination tasks reported elsewhere (<xref ref-type="bibr" rid="bibr18-0018720812446623">Ngo &amp; Spence, 2010a</xref>, <xref ref-type="bibr" rid="bibr19-0018720812446623">2010b</xref>; <xref ref-type="bibr" rid="bibr34-0018720812446623">Van der Burg et al., 2009</xref>), the same vibrotactile cues were just as effective as the auditory cues. In fact, <xref ref-type="bibr" rid="bibr18-0018720812446623">Ngo and Spence (2010a</xref>, <xref ref-type="bibr" rid="bibr19-0018720812446623">2010b</xref>) demonstrated that both cross-modal auditory and vibrotactile cues were just as effective as the audiotactile cue at facilitating participants’ visual target identification performance. In contrast, the audiotactile cue gave rise to significantly better performance than the auditory or vibrotactile cues in the ATST used in the present study. Moreover, the performance benefit resulting from the presentation of the audiotactile cue was even greater than that observed following the presentation of the auditory cue.</p>
<p>Additionally, participants’ performance in terms of handoff delays also appeared to be facilitated by the presence of the audiotactile cues, despite the fact that the cues provided no information about the onset of any of the incoming aircraft. It is possible that the participants may have adopted an attentional set that was responsive to the most salient cue, which in this case happened to be the audiotactile cue (cf. <xref ref-type="bibr" rid="bibr6-0018720812446623">Folk, Remington, &amp; Johnston, 1992</xref>; <xref ref-type="bibr" rid="bibr14-0018720812446623">Leber, Kawahara, &amp; Gabari, 2009</xref>; <xref ref-type="bibr" rid="bibr43-0018720812446623">Yantis, 1993</xref>; see also <xref ref-type="bibr" rid="bibr29-0018720812446623">Theeuwes, 1994</xref>). In turn, this attentional set for the cue may have evoked an overall state of “attentional readiness” (<xref ref-type="bibr" rid="bibr43-0018720812446623">Yantis, 1993</xref>, p. 676), or elevated general arousal, in the audiotactile cuing block, which might help to explain why participants’ improved handoff delays were also observed in this condition.</p>
<p>Previous research by <xref ref-type="bibr" rid="bibr23-0018720812446623">Santangelo et al. (2008)</xref> and <xref ref-type="bibr" rid="bibr24-0018720812446623">Santangelo and Spence (2007)</xref> has demonstrated that in conditions of high perceptual load, only audiotactile cues effectively captured participants’ visuospatial attention, whereas auditory or vibrotactile cues failed to do so. The auditory and tactile cues appeared to be effective only in conditions of low perceptual load. The fact that the audiotactile cue gave rise to a more reliable improvement in the speed of participants’ responses to conflicts (and to handoffs) than did either of the other cues may similarly reflect high perceptual and cognitive load in the ATST (<xref ref-type="bibr" rid="bibr15-0018720812446623">Linnell &amp; Caparos, 2011</xref>; <xref ref-type="bibr" rid="bibr23-0018720812446623">Santangelo et al., 2008</xref>; <xref ref-type="bibr" rid="bibr24-0018720812446623">Santangelo &amp; Spence, 2007</xref>).</p>
<p>In our study, the participants were required to monitor a number of aircraft, all moving dynamically through a small and cluttered airspace, and to control the speed, altitude, and heading of the aircraft. The task was also time sensitive, as participants had to respond as rapidly as possible to conflicts to ensure that the aircraft did not crash and that they arrived or exited efficiently to and from airports and exit gates. Therefore, the ATST is considerably higher in both perceptual and cognitive load as compared with many laboratory-based visual search tasks used in previous research (e.g., <xref ref-type="bibr" rid="bibr18-0018720812446623">Ngo &amp; Spence, 2010a</xref>, <xref ref-type="bibr" rid="bibr19-0018720812446623">2010b</xref>; <xref ref-type="bibr" rid="bibr33-0018720812446623">Van der Burg et al., 2008</xref>, <xref ref-type="bibr" rid="bibr34-0018720812446623">2009</xref>). As such, and consistent with previous research, it is likely that the high perceptual and cognitive load of the ATST is what rendered the vibrotactile cue ineffective (and the unimodal auditory cue less effective) in comparison with the multisensory audiotactile cue, which was reliably effective at facilitating participants’ performance. It is likely that the combination of the auditory and tactile cue in the audiotactile cuing condition was more salient than either auditory or tactile cue alone, which in turn gave rise to more effective attentional capture and significantly more performance facilitation.</p>
<p>It should be noted that there were large variations in participants’ measures of task performance (e.g., handoff delays, en route delays, and simulation errors) between the three experiments reported in the present study. This problem may be associated with using realistic stimuli and setups, in which the environment and stimuli are typically more complex and less controlled or predictable and participants’ performance is more varied. In future research, it may be worthwhile to increase the amount of training and practice (rather than just two 10-min practice blocks as used in Experiments 1 through 3) for participants to reach a plateau in performance (<xref ref-type="bibr" rid="bibr22-0018720812446623">Pierce, 2012</xref>) and then to test the effects of nonvisual cuing thereafter. However, just what a “plateau in performance” (i.e., performance criterion) is exactly might be difficult to define in real-world tasks, such as air traffic management. Moreover, the plateau is likely to vary from person to person. Nevertheless, increasing training would not only potentially minimize the variance across experiments and participants, but it would also ensure that any changes in cuing effects are not simply attributable to time-on-task or practice effects.</p>
<p>Taken together, the results of the three experiments reported in the present study suggest that the best cues for improving the speed of participants’ responses to conflicts in an air traffic management environment appear to be multisensory audiotactile cues that are temporally synchronous with the onset of the conflict. These findings are consistent with those of <xref ref-type="bibr" rid="bibr11-0018720812446623">Ho et al.’s (2007)</xref> earlier study, in which it was demonstrated that when participants simulated a car-following scenario, they initiated their braking responses significantly more rapidly following the presentation of multisensory audiotactile warning signals than following the presentation of either unimodal auditory or vibrotactile warning signals. Thus, in real-world tasks that are cognitively demanding, temporally synchronous multisensory audiotactile cues appear to have the greatest potential for significantly improving operator performance and reducing the time it takes to detect and respond to potential visual target (e.g., threats, dangers, conflicts) events.</p>
</sec>
<sec id="section22-0018720812446623">
<title>Key Points</title>
<list id="list1-0018720812446623" list-type="bullet">
<list-item><p>In a realistic training simulation for potential air traffic controllers, the presentation of auditory, in addition to visual, cues gives rise to faster detection of target visual events (i.e., conflicts) as compared with when visual cues are presented alone.</p></list-item>
<list-item><p>Audiotactile cues give rise to even greater facilitation of participants’ conflict resolution performance than vibrotactile cues, supporting the results of other applied research (<xref ref-type="bibr" rid="bibr11-0018720812446623">Ho, Reed, &amp; Spence, 2007</xref>).</p></list-item>
<list-item><p>Temporally synchronous audiotactile cues can effectively capture attention in real-world tasks and demonstrate the potential for significantly improving operator performance and reducing the time it takes to detect and respond to potential visual target (e.g., threats, dangers, conflicts) events.</p></list-item>
</list>
</sec>
</body>
<back>
<ack>
<p>This research was supported in part by a Clarendon Fund Scholarship to the first author from Oxford University. We would like to thank Dana Broach and the Federal Aviation Administration (FAA) Civil Aerospace Medical Institute Training and Organizational Research Laboratory (AAM-520) for permission to use the Air Traffic Scenarios Test and for technical support. Any opinions or conclusions are those of the authors alone and may not reflect the official position or policy of the National Aeronautics and Space Administration (NASA), the FAA, or the U.S. Department of Transportation. Preparation of this manuscript was supported in part by the NASA Cooperative Agreement NNX09AU66A, Group 5 University Research Center: Center for Human Factors in Advanced Aeronautics Technologies (Brenda Collins, Technical Monitor).</p>
</ack>
<bio>
<p>Mary K. Ngo received her PhD in experimental psychology at the University of Oxford in 2012 and is a postdoctoral research associate in the Center for Human Factors in Advanced Aeronautics Technologies (CHAAT) at the California State University of Long Beach.</p>
<p>Russell S. Pierce received his MA in psychology at the University of California, Riverside, in 2009 and is currently a doctoral candidate in the Department of Psychology at the same institution.</p>
<p>Charles Spence is a professor of experimental psychology in the Department of Experimental Psychology at the University of Oxford. He received his PhD in experimental psychology at the University of Cambridge in 1995.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0018720812446623">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Breznitz</surname><given-names>S.</given-names></name>
</person-group> (<year>1983</year>). <source>Cry-wolf: The psychology of false alarms</source>. <publisher-loc>Hillsdale</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr2-0018720812446623">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brill</surname><given-names>J. C.</given-names></name>
<name><surname>Terrence</surname><given-names>P. I.</given-names></name>
<name><surname>Downs</surname><given-names>J. L.</given-names></name>
<name><surname>Gilson</surname><given-names>R. D.</given-names></name>
<name><surname>Hancock</surname><given-names>P. A.</given-names></name>
<name><surname>Mouloua</surname><given-names>M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Search space reduction via multi-sensory directional cueing</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 48th Annual Meeting</source> (pp. <fpage>2134</fpage>–<lpage>2136</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr3-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X.</given-names></name>
<name><surname>Zelinksy</surname><given-names>G.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Real-world visual search is dominated by top-down guidance</article-title>. <source>Vision Research</source>, <volume>46</volume>, <fpage>4118</fpage>–<lpage>4133</lpage>.</citation>
</ref>
<ref id="bibr4-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>DiVita</surname><given-names>J.</given-names></name>
<name><surname>Obermayer</surname><given-names>R.</given-names></name>
<name><surname>Nugent</surname><given-names>W.</given-names></name>
<name><surname>Linville</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Verification of the change blindness phenomenon while managing critical events on a combat information display</article-title>. <source>Human Factors</source>, <volume>46</volume>, <fpage>205</fpage>–<lpage>218</lpage>.</citation>
</ref>
<ref id="bibr5-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fitch</surname><given-names>G. M.</given-names></name>
<name><surname>Kiefer</surname><given-names>R. J.</given-names></name>
<name><surname>Hankey</surname><given-names>J. M.</given-names></name>
<name><surname>Kleiner</surname><given-names>B. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Toward developing an approach for alerting drivers to the direction of a crash threat</article-title>. <source>Human Factors</source>, <volume>49</volume>, <fpage>710</fpage>–<lpage>720</lpage>.</citation>
</ref>
<ref id="bibr6-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
<name><surname>Remington</surname><given-names>R. W.</given-names></name>
<name><surname>Johnston</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Involuntary covert orienting is contingent on attentional control settings</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>18</volume>, <fpage>1030</fpage>–<lpage>1044</lpage>.</citation>
</ref>
<ref id="bibr7-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ghazanfar</surname><given-names>A. A.</given-names></name>
<name><surname>Schroeder</surname><given-names>C. E.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Is the neocortex essentially multisensory?</article-title> <source>Trends in Cognitive Sciences</source>, <volume>10</volume>, <fpage>278</fpage>–<lpage>285</lpage>.</citation>
</ref>
<ref id="bibr8-0018720812446623">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Haas</surname><given-names>E. C.</given-names></name>
<name><surname>Pillalamarri</surname><given-names>R. S.</given-names></name>
<name><surname>Stachowiak</surname><given-names>C. C.</given-names></name>
<name><surname>Lattin</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2005</year>). <source>Audio cues to assist visual search in robotic system operator control unit displays</source>. <publisher-loc>Aberdeen Proving Ground, MD</publisher-loc>: <publisher-name>Army Research Laboratory</publisher-name>.</citation>
</ref>
<ref id="bibr9-0018720812446623">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hameed</surname><given-names>S.</given-names></name>
<name><surname>Jayaraman</surname><given-names>S.</given-names></name>
<name><surname>Ballard</surname><given-names>M.</given-names></name>
<name><surname>Sarter</surname><given-names>N.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Guiding visual attention by exploring crossmodal spatial links: An application in air traffic control</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 51st Annual Meeting</source> (pp. <fpage>220</fpage>–<lpage>224</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr10-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ho</surname><given-names>C.</given-names></name>
<name><surname>Reed</surname><given-names>N.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Assessing the effectiveness of “intuitive” vibrotactile warning signals in preventing front-to-rear-end collisions in a driving simulator</article-title>. <source>Accident Analysis &amp; Prevention</source>, <volume>38</volume>, <fpage>988</fpage>–<lpage>996</lpage>.</citation>
</ref>
<ref id="bibr11-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ho</surname><given-names>C.</given-names></name>
<name><surname>Reed</surname><given-names>N.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Multisensory in-car warning signals for collision avoidance</article-title>. <source>Human Factors</source>, <volume>49</volume>, <fpage>1107</fpage>–<lpage>1114</lpage>.</citation>
</ref>
<ref id="bibr12-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ho</surname><given-names>C.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Assessing the effectiveness of various auditory cues in capturing a driver’s visual attention</article-title>. <source>Journal of Experimental Psychology: Applied</source>, <volume>11</volume>, <fpage>157</fpage>–<lpage>174</lpage>.</citation>
</ref>
<ref id="bibr13-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ho</surname><given-names>C.</given-names></name>
<name><surname>Tan</surname><given-names>H. Z.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The differential effect of vibrotactile and auditory cues on visual spatial attention</article-title>. <source>Ergonomics</source>, <volume>49</volume>, <fpage>724</fpage>–<lpage>738</lpage>.</citation>
</ref>
<ref id="bibr14-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Leber</surname><given-names>A. B.</given-names></name>
<name><surname>Kawahara</surname><given-names>J.-I.</given-names></name>
<name><surname>Gabari</surname><given-names>Y.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Long-term abstract learning of attentional set</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>35</volume>, <fpage>1385</fpage>–<lpage>1397</lpage>.</citation>
</ref>
<ref id="bibr15-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Linnell</surname><given-names>K. J.</given-names></name>
<name><surname>Caparos</surname><given-names>S.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Perceptual and cognitive load interact to control the spatial focus of attention</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>37</volume>, <fpage>1643</fpage>–<lpage>1648</lpage>.</citation>
</ref>
<ref id="bibr16-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McCarley</surname><given-names>J. S.</given-names></name>
<name><surname>Kramer</surname><given-names>A. F.</given-names></name>
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
<name><surname>Vidoni</surname><given-names>E. D.</given-names></name>
<name><surname>Boot</surname><given-names>W. R.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Visual skills in airport-security screening</article-title>. <source>Psychological Science</source>, <volume>15</volume>, <fpage>302</fpage>–<lpage>306</lpage>.</citation>
</ref>
<ref id="bibr17-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Molholm</surname><given-names>S.</given-names></name>
<name><surname>Ritter</surname><given-names>W.</given-names></name>
<name><surname>Javitt</surname><given-names>D. C.</given-names></name>
<name><surname>Foxe</surname><given-names>J. J.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Visual-auditory multisensory object recognition in humans: A high-density electrophysiological study</article-title>. <source>Cerebral Cortex</source>, <volume>14</volume>, <fpage>452</fpage>–<lpage>465</lpage>.</citation>
</ref>
<ref id="bibr18-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ngo</surname><given-names>M. K.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2010a</year>). <article-title>Auditory, tactile, and multisensory cues facilitate search for dynamic visual stimuli</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>72</volume>, <fpage>1654</fpage>–<lpage>1665</lpage>.</citation>
</ref>
<ref id="bibr19-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ngo</surname><given-names>M. K.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2010b</year>). <article-title>Crossmodal facilitation of masked visual target identification</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>72</volume>, <fpage>1938</fpage>–<lpage>1947</lpage>.</citation>
</ref>
<ref id="bibr20-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Noesselt</surname><given-names>T.</given-names></name>
<name><surname>Bergmann</surname><given-names>D.</given-names></name>
<name><surname>Hake</surname><given-names>M.</given-names></name>
<name><surname>Heinze</surname><given-names>H.-J.</given-names></name>
<name><surname>Fendrich</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Sound increases the saliency of visual events</article-title>. <source>Brain Research</source>, <volume>1220</volume>, <fpage>157</fpage>–<lpage>163</lpage>.</citation>
</ref>
<ref id="bibr21-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Osborn</surname><given-names>W. C.</given-names></name>
<name><surname>Sheldon</surname><given-names>R. W.</given-names></name>
<name><surname>Baker</surname><given-names>R. A.</given-names></name>
</person-group> (<year>1963</year>). <article-title>Vigilance performance under conditions of redundant and nonredundant signal presentation</article-title>. <source>Journal of Applied Psychology</source>, <volume>47</volume>, <fpage>130</fpage>–<lpage>134</lpage>.</citation>
</ref>
<ref id="bibr22-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pierce</surname><given-names>R. S.</given-names></name>
</person-group> (<year>2012</year>). <article-title>The effect of SPAM administration during a dynamic simulation</article-title>. <source>Human Factors</source>, <volume>54</volume>, <fpage>838</fpage>–<lpage>848</lpage>.</citation>
</ref>
<ref id="bibr23-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Santangelo</surname><given-names>V.</given-names></name>
<name><surname>Ho</surname><given-names>C.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Capturing spatial attention with multisensory cues</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>15</volume>, <fpage>398</fpage>–<lpage>403</lpage>.</citation>
</ref>
<ref id="bibr24-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Santangelo</surname><given-names>V.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Multisensory cues capture spatial attention regardless of perceptual load</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>33</volume>, <fpage>1311</fpage>–<lpage>1321</lpage>.</citation>
</ref>
<ref id="bibr25-0018720812446623">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Multisensory perception, cognition, and behavior: Evaluating the factors modulating multisensory integration</article-title>. In <person-group person-group-type="editor">
<name><surname>Stein</surname><given-names>B. E.</given-names></name>
</person-group> (Ed.), <source>The new handbook of multisensory processing</source> (pp. <fpage>241</fpage>–<lpage>264</lpage>). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr26-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spence</surname><given-names>C.</given-names></name>
<name><surname>Driver</surname><given-names>J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Attracting attention to the illusory location of a sound: Reflexive crossmodal orienting and ventriloquism</article-title>. <source>NeuroReport</source>, <volume>11</volume>, <fpage>2057</fpage>–<lpage>2061</lpage>.</citation>
</ref>
<ref id="bibr27-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spence</surname><given-names>C.</given-names></name>
<name><surname>Santangelo</surname><given-names>V.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Capturing spatial attention with multisensory cues: A review</article-title>. <source>Hearing Research</source>, <volume>258</volume>, <fpage>134</fpage>–<lpage>142</lpage>.</citation>
</ref>
<ref id="bibr28-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>J. G.</given-names></name>
<name><surname>Cutsiridus</surname><given-names>V.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Saliency, attention, active visual search, and pictures scanning</article-title>. <source>Cognitive Computation</source>, <volume>3</volume>, <fpage>1</fpage>–<lpage>3</lpage>.</citation>
</ref>
<ref id="bibr29-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Stimulus-driven capture and attentional set: Selective search for color and visual abrupt onsets</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>20</volume>, <fpage>799</fpage>–<lpage>806</lpage>.</citation>
</ref>
<ref id="bibr30-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treisman</surname><given-names>A. M.</given-names></name>
<name><surname>Gelade</surname><given-names>G.</given-names></name>
</person-group> (<year>1980</year>). <article-title>A feature-integration theory of attention</article-title>. <source>Cognitive Psychology</source>, <volume>12</volume>, <fpage>97</fpage>–<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr31-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treisman</surname><given-names>A. M.</given-names></name>
<name><surname>Sato</surname><given-names>S.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Conjunction search revisited</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>16</volume>, <fpage>459</fpage>–<lpage>478</lpage>.</citation>
</ref>
<ref id="bibr32-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treisman</surname><given-names>A. M.</given-names></name>
<name><surname>Souther</surname><given-names>J.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Search asymmetry: A diagnostic for preattentive processing of separable features</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>114</volume>, <fpage>285</fpage>–<lpage>310</lpage>.</citation>
</ref>
<ref id="bibr33-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Bronkhorst</surname><given-names>A. W.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Pip and pop: Nonspatial auditory signals improve spatial visual search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>34</volume>, <fpage>1053</fpage>–<lpage>1065</lpage>.</citation>
</ref>
<ref id="bibr34-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Bronkhorst</surname><given-names>A. W.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Poke and pop: Tactile-visual synchrony increases visual saliency</article-title>. <source>Neuroscience Letters</source>, <volume>450</volume>, <fpage>60</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr35-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van Wert</surname><given-names>M. J.</given-names></name>
<name><surname>Horowitz</surname><given-names>T. S.</given-names></name>
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Even in correctable search, some types of rare targets are frequently missed</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>71</volume>, <fpage>541</fpage>–<lpage>553</lpage>.</citation>
</ref>
<ref id="bibr36-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vroomen</surname><given-names>J.</given-names></name>
<name><surname>de Gelder</surname><given-names>B.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Sound enhances visual perception: Cross-modal effects of auditory organization on vision</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>26</volume>, <fpage>1583</fpage>–<lpage>1590</lpage>.</citation>
</ref>
<ref id="bibr37-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
<name><surname>Rice</surname><given-names>S.</given-names></name>
<name><surname>Keller</surname><given-names>D.</given-names></name>
<name><surname>Hutchins</surname><given-names>S.</given-names></name>
<name><surname>Hughes</surname><given-names>J.</given-names></name>
<name><surname>Clayton</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>). <article-title>False alerts in air traffic control conflict alerting system: Is there a “cry-wolf” effect?</article-title> <source>Human Factors</source>, <volume>51</volume>, <fpage>446</fpage>–<lpage>462</lpage>.</citation>
</ref>
<ref id="bibr38-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Visual search in continuous, naturalistic stimuli</article-title>. <source>Vision Research</source>, <volume>34</volume>, <fpage>1187</fpage>–<lpage>1195</lpage>.</citation>
</ref>
<ref id="bibr39-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Asymmetries in visual search: An introduction</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>63</volume>, <fpage>381</fpage>–<lpage>389</lpage>.</citation>
</ref>
<ref id="bibr40-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
<name><surname>Horowitz</surname><given-names>T. S.</given-names></name>
<name><surname>Kenner</surname><given-names>N. M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Rare items often missed in visual searches</article-title>. <source>Nature</source>, <volume>435</volume>, <fpage>439</fpage>–<lpage>440</lpage>.</citation>
</ref>
<ref id="bibr41-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
<name><surname>Van Wert</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Varying target prevalence reveals two dissociable decision criteria in visual search</article-title>. <source>Current Biology</source>, <volume>20</volume>, <fpage>121</fpage>–<lpage>124</lpage>.</citation>
</ref>
<ref id="bibr42-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
<name><surname>Võ</surname><given-names>M. L.-H.</given-names></name>
<name><surname>Evans</surname><given-names>K. K.</given-names></name>
<name><surname>Greene</surname><given-names>M. R.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Visual search in scenes involves selective and nonselective pathways</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>15</volume>, <fpage>77</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr43-0018720812446623">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yantis</surname><given-names>S.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Stimulus-driven attentional capture and attentional control settings</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>19</volume>, <fpage>676</fpage>–<lpage>681</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>