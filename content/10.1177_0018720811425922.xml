<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HFS</journal-id>
<journal-id journal-id-type="hwp">sphfs</journal-id>
<journal-title>Human Factors: The Journal of Human Factors and Ergonomics Society</journal-title>
<issn pub-type="ppub">0018-7208</issn>
<issn pub-type="epub">1547-8181</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0018720811425922</article-id>
<article-id pub-id-type="publisher-id">10.1177_0018720811425922</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Methods for the Analysis of Communication Special Section</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Detection of Affective States From Text and Speech for Real-Time Human–Computer Interaction</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Cooke</surname><given-names>Nancy J.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Duchon</surname><given-names>Andrew</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Gorman</surname><given-names>Jamie C.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Keyton</surname><given-names>Joann</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Miller</surname><given-names>Anne</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Calix</surname><given-names>Ricardo A.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Javadpour</surname><given-names>Leili</given-names></name>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Knapp</surname><given-names>Gerald M.</given-names></name>
</contrib>
<aff id="aff1-0018720811425922">Louisiana State University, Baton Rouge, Louisiana</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0018720811425922">Gerald M. Knapp, Industrial Engineering, 3128 Patrick F. Taylor Hall, Louisiana State University, Baton Rouge, LA 70803; e-mail: <email>gknapp@lsu.edu</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2012</year>
</pub-date>
<volume>54</volume>
<issue>4</issue>
<issue-title>Special Section: Methods for the Analysis of Communication</issue-title>
<fpage>530</fpage>
<lpage>545</lpage>
<history>
<date date-type="received">
<day>13</day>
<month>1</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>9</month>
<year>2011</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012, Human Factors and Ergonomics Society</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<sec id="section1-0018720811425922">
<title>Objective:</title>
<p>The goal of this work is to develop and test an automated system methodology that can detect emotion from text and speech features.</p>
</sec>
<sec id="section2-0018720811425922">
<title>Background:</title>
<p>Affective human–computer interaction will be critical for the success of new systems that will be prevalent in the 21st century. Such systems will need to properly deduce human emotional state before they can determine how to best interact with people.</p>
</sec>
<sec id="section3-0018720811425922">
<title>Method:</title>
<p>Corpora and machine learning classification models are used to train and test a methodology for emotion detection. The methodology uses a stepwise approach to detect sentiment in sentences by first filtering out neutral sentences, then distinguishing among positive, negative, and five emotion classes.</p>
</sec>
<sec id="section4-0018720811425922">
<title>Results:</title>
<p>Results of the classification between emotion and neutral sentences achieved recall accuracies as high as 77% in the University of Illinois at Urbana-Champaign (UIUC) corpus and 61% in the Louisiana State University medical drama (LSU-MD) corpus for emotion samples. Once neutral sentences were filtered out, the methodology achieved accuracy scores for detecting negative sentences as high as 92.3%.</p>
</sec>
<sec id="section5-0018720811425922">
<title>Conclusion:</title>
<p>Results of the feature analysis indicate that speech spectral features are better than speech prosodic features for emotion detection. Accumulated sentiment composition text features appear to be very important as well. This work contributes to the study of human communication by providing a better understanding of how language factors help to best convey human emotion and how to best automate this process.</p>
</sec>
<sec id="section6-0018720811425922">
<title>Application:</title>
<p>Results of this study can be used to develop better automated assistive systems that interpret human language and respond to emotions through 3-D computer graphics.</p>
</sec>
</abstract>
<kwd-group>
<kwd>knowledge representation</kwd>
<kwd>cognitive processes</kwd>
<kwd>language</kwd>
<kwd>human–computer interaction</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section7-0018720811425922" sec-type="intro">
<title>Introduction</title>
<p>Affective human–computer interaction (HCI) will be critical for the success of new technologies that will be prevalent in the 21st century. There will be continued rapid development of automated assistive systems that help humans to live better, more productive lives. These will be active assistive systems such as robot aides used in hospitals, homes, office, and other environments. Such systems will need to be able to properly deduce human emotional state before they can determine how to best interact with people.</p>
<p>The goal of this work is to develop and test an automated system methodology that can detect emotion from text and speech features for use in affective HCI systems (text and speech refer to the semantic and acoustic components of speech). Emotion detection can be useful in many areas such as text-to-scene processing, cognitive ergonomics, game design, movie animation, and health care. The field of cognitive ergonomics could greatly benefit from the use of machine learning techniques to automate tasks and develop environments that can adapt to human behavior and emotions. Aircraft cockpits, hospital operating rooms, military simulators, and other work environments could automatically adapt based on a person’s mood. This can have great benefits in safety and increased productivity.</p>
<p>Specifically, in health care, there is currently a need to automate many tasks. High costs and the increase in the size of the elderly population have put a strain on the supply of care giving medical personnel. Many tasks that require medical support staff such as rehabilitation and elderly care are expensive and are not being performed. In addition, it has been suggested that caring for people needs to have a human touch (<xref ref-type="bibr" rid="bibr29-0018720811425922">Tapus, Mataric, &amp; Scassellati, 2007</xref>). This means that automated systems that can detect human behavior and emotion are needed to perform these tasks, especially when dealing with elderly patients and children.</p>
<p>A current research area is the use of socially enabled robots to treat patients with autism, rehabilitation needs, and other problems. <xref ref-type="bibr" rid="bibr30-0018720811425922">Tapus, Tapus, and Mataric (2008)</xref> have shown that patients have better performance in their rehabilitation tasks when assisted by automated systems that appropriately respond to their behavior. They also note that autistic children who interact with robots can improve behavior and cognitive processing. Unfortunately, at this point, smart automated aides are difficult to implement because they need to interpret human emotions from language inputs.</p>
<p>Emotion detection from language inputs is a difficult task because emotions can be very subjective and the set of features that captures emotion in language is not clearly defined. Vocabulary approaches alone are not enough because emotion words that are usually associated with specific emotion classes can be used for other emotions. An example of this is when people use sarcasm, where they say one thing but they mean another. Acoustic features are usually related to some emotions, but noise and other factors can affect the ability of a system to predict emotion. All these aspects contribute to make automatic emotion detection a challenging area of research.</p>
<p>The methodology proposed in this work uses a stepwise approach to detect emotion from language at the sentence level by first distinguishing between emotion and neutral classes and then among positive, negative, and five emotion classes. The methodology is trained and tested on two affect corpora: (a) the children’s stories corpus developed at the University of Illinois at Urbana-Champaign (UIUC) by <xref ref-type="bibr" rid="bibr2-0018720811425922">Alm (2008)</xref> and (b) the new medical TV drama corpus developed at Louisiana State University (LSU). The UIUC affect corpus is narrative based, with one speaker per story, low noise, and longer, more grammatical sentences. In contrast, the LSU medical drama (LSU-MD) corpus is dialogue based, with multiple speakers, high noise, and shorter utterances.</p>
<p>Results of the methodology using various supervised learning techniques, feature sets, and corpora are presented and discussed. Using two corpora to train and test the model helped to identify the differences and similarities in emotion detection between narrative- and dialogue-based communication. Finally, feature analysis and an application of the methodology in the HCI cycle are presented.</p>
</sec>
<sec id="section8-0018720811425922">
<title>Literature Review</title>
<p>Although there have been several attempts to develop affective HCI systems, additional work needs to be done in the area of affective natural language processing. In the following sections, studies that have addressed the issue of emotion detection in just one language medium (text or speech) or that integrate both mediums are discussed.</p>
<sec id="section9-0018720811425922">
<title>Speech Approaches</title>
<p>Two common issues for emotion detection from speech are (a) which features to use and (b) which segment of the signal should be used. On the first issue, <xref ref-type="bibr" rid="bibr12-0018720811425922">Chuang and Wu (2004)</xref> showed that emotions in speech are related to prosody features such as pitch and energy (formants). These authors described how pitch for happiness or anger is higher than for sadness. (Pitch is a perceptual property of a signal defined by <xref ref-type="bibr" rid="bibr17-0018720811425922">Jurafsky and Martin, 2008</xref>, as a “mental sensation of fundamental frequency” that can be detected by using the higher cepstral values of a signal.) <xref ref-type="bibr" rid="bibr12-0018720811425922">Chuang and Wu (2004)</xref> also indicated that the concentration of acoustic energy (formants) in speech associated with anger or surprise is greater than the energy associated with fear. Formants are the amplitude peaks in given frequency ranges that can correlate to emotions. They correspond to a resonance in the vocal tract. Higher amplitudes correlate to louder sounds. Praat gives the frequency value (in hertz) where the amplitude peak is for the given formant.</p>
<p>Other authors such as <xref ref-type="bibr" rid="bibr22-0018720811425922">Luengo, Navas, and Hernaez (2010)</xref> concluded that spectral features (Mel frequency cepstral coefficients; MFCCs) have a higher contribution to emotion detection. From perceptual experiments (<xref ref-type="bibr" rid="bibr17-0018720811425922">Jurafsky &amp; Martin, 2008</xref>), it has been concluded that humans focus on only certain frequency components of the signal. MFCCs are usually very useful in emotion detection because the frequency bands are mapped to the specific Mel scale of the human auditory system and are uniformly spaced. This frequency mapping to the Mel scale helps to better capture the speech signal differences that are relevant to speech.</p>
<p>With regard to the second issue of speech segment length, <xref ref-type="bibr" rid="bibr18-0018720811425922">Klabbers, Mishra, and van Santen (2007)</xref> conclude that the length of the speech segments is important for emotion detection. Different segment lengths have been used to extract information from the speech signal. In <xref ref-type="bibr" rid="bibr15-0018720811425922">Hansen and Bou-Ghazale (1997)</xref>, the authors used word-level features to detect stress in utterances using a vocabulary of 35 words. This work concluded that “when stress is present, recognition rates decrease significantly.” In contrast, <xref ref-type="bibr" rid="bibr5-0018720811425922">Busso, Lee, and Narayanan (2009)</xref> concluded that statistics such as the mean of the pitch contour at the sentence level are a better indicator of emotion than other shorter term statistics (e.g., at the phrase or word level).</p>
</sec>
<sec id="section10-0018720811425922">
<title>Text Approaches</title>
<p>Emotion detection studies in text can loosely be classified into lexical-based and high-semantic-based studies. In general, lexical-based studies for affect detection from text use a bag of words approach combined with other text features such as syntactic elements to perform classification. <xref ref-type="bibr" rid="bibr26-0018720811425922">Osherenko (2008)</xref>, for instance, used the presence or absence of negations and intensifiers as features to train and test an emotion detection model. Similarly, <xref ref-type="bibr" rid="bibr32-0018720811425922">Tokuhisa, Inui, and Matsumoto (2008)</xref> performed emotion detection in Japanese text using a stepwise classification approach and a sentiment polarity dictionary of words. In <xref ref-type="bibr" rid="bibr8-0018720811425922">Calix, Mallepudi, Chen, and Knapp (2010)</xref>, an automatic feature extraction approach was proposed that used annotated affect corpora to extract word features for use in emotion classification. This work also provides additional background on many other studies on emotion detection in text.</p>
<p>Studies that include higher semantic ideas include <xref ref-type="bibr" rid="bibr25-0018720811425922">Moilanen and Pulman (2007)</xref> and <xref ref-type="bibr" rid="bibr20-0018720811425922">Liu, Lieberman, and Selker (2003)</xref>. <xref ref-type="bibr" rid="bibr25-0018720811425922">Moilanen and Pulman (2007)</xref> proposed the use of sentiment composition to detect emotion in text. Sentiment composition says that the sentiment polarity of a sentence is the result of the sentiment polarity of its constituents. <xref ref-type="bibr" rid="bibr20-0018720811425922">Liu et al. (2003)</xref> and <xref ref-type="bibr" rid="bibr28-0018720811425922">Singh (2005)</xref>, on the other hand, propose semantic approaches that use knowledge bases to obtain additional information about the meaning of words and their relations.</p>
</sec>
<sec id="section11-0018720811425922">
<title>Multimodal Approaches</title>
<p><xref ref-type="bibr" rid="bibr12-0018720811425922">Chuang and Wu (2004)</xref> and <xref ref-type="bibr" rid="bibr9-0018720811425922">Castellano, Kessous, and Caridakis (2007)</xref> showed that multimodal (two or more mediums such as text, audio) emotion recognition performs better than unimodal (just text or audio) emotion recognition. <xref ref-type="bibr" rid="bibr27-0018720811425922">Sebe, Cohen, and Huang (2005)</xref> note that determining the optimal point at which different modes of information should be merged is crucial and still an open research problem. Studies such as <xref ref-type="bibr" rid="bibr31-0018720811425922">Tawari and Trivedi (2010)</xref> propose the use of context (e.g., gender) and other sources to analyze emotion.</p>
</sec>
</sec>
<sec id="section12-0018720811425922" sec-type="methods">
<title>Method</title>
<p>Based on all the previously described issues, the following methodology for emotion detection in text and speech is proposed. The methodology uses a stepwise approach to detect emotion in sentences by first filtering out neutral sentences, then distinguishing among positive, negative, and five emotion classes. Text and speech features are combined in feature vectors of 69 features each to train and test the emotion detection model.</p>
<p>The first step in the methodology compares neutral versus emotional sentences. The second step compares positive and negative sentences, and the third step uses five emotion classes, which are happy, sad, angry, surprised, and afraid, to perform classification. The emotion classification is performed at the sentence level (UIUC corpus) or utterance level (LSU-MD corpus) rather than at the phrase or word level to take advantage of the higher amount of information available in longer communication segments. For the LSU-MD corpus, the additional step of filtering out sentences with three words or fewer is also taken. This step is helpful since most really short utterances (common in dialogue-based corpora) do not contribute much to semantic content and can be better handled by heuristic rules or speech-only approaches. Context is used in the methodology by utilizing previous sentences’ emotional intensities as features (see the following text features section).</p>
<sec id="section13-0018720811425922">
<title>Multimodal Corpora</title>
<p>The UIUC children’s stories corpus (<xref ref-type="bibr" rid="bibr2-0018720811425922">Alm, 2008</xref>) and the LSU-MD corpus were used to train and test the emotion detection models. The UIUC corpus has been extended for this work by adding audio recordings for 89 stories from the corpus (<xref ref-type="bibr" rid="bibr7-0018720811425922">Calix &amp; Knapp, 2011</xref>). Sentence boundaries matching the sentences and assigned classes in the text corpus were annotated on the speech signals using Praat’s manual annotation tool. The files are available in MP3 format but were converted to WAV format to perform the feature extraction. Characteristics of the different speakers that recorded the stories in this corpus can be seen in <xref ref-type="table" rid="table1-0018720811425922">Table 1</xref>.</p>
<table-wrap id="table1-0018720811425922" position="float">
<label>Table 1:</label>
<caption>
<p>Children’s Stories Speech Data Set Characteristics</p>
</caption>
<graphic alternate-form-of="table1-0018720811425922" xlink:href="10.1177_0018720811425922-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Number of Audio Recordings</th>
<th align="center">Male</th>
<th align="center">Female</th>
<th align="center">Number of Speakers</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grimm</td>
<td>59</td>
<td>38</td>
<td>21</td>
<td>32</td>
</tr>
<tr>
<td>Potter</td>
<td>18</td>
<td>11</td>
<td>7</td>
<td>15</td>
</tr>
<tr>
<td>Andersen</td>
<td>12</td>
<td>5</td>
<td>7</td>
<td>9</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The emotion annotations in the UIUC corpus were performed by two human annotators. Analysis of the annotation scheme (<xref ref-type="bibr" rid="bibr2-0018720811425922">Alm, 2008</xref>) indicated that there is some variability in the annotation as emotion detection can be a subjective issue. Six emotion labels from the original eight annotated at UIUC were used for the analysis presented in this article. The six emotion labels are used for the sake of comparison as they are common in the literature and referred to as the “Big Six” (<xref ref-type="bibr" rid="bibr2-0018720811425922">Alm, 2008</xref>). Other, works such as <xref ref-type="bibr" rid="bibr23-0018720811425922">Matthews, Jones, and Chamberlain (1990)</xref> have explored the use of a set of nondiscrete emotion dimensions such as arousal to capture more than six emotions. However, this approach makes classification more difficult as the scales are continuous instead of discrete. Current research work indicates that activation, expectation, valence, and power account for most emotion classes (<xref ref-type="bibr" rid="bibr14-0018720811425922">Fontaine, Scherer, Roesch, &amp; Ellsworth, 2007</xref>).</p>
<p>The LSU-MD corpus consists of utterances collected from six episodes of the popular television medical drama <italic>Grey’s Anatomy</italic>. Using conversations from this medium helps to train and test the model on a noisy environment where the speakers are performing acted tasks, in this case practicing medicine. The conversations include medical dialogue and TV drama emotional content. This is especially useful for the task proposed in this article of emotion detection. Both corpora can be obtained from LSU Natural Language Processing (<xref ref-type="bibr" rid="bibr21-0018720811425922">LSU-NLP; 2011</xref>).</p>
</sec>
<sec id="section14-0018720811425922">
<title>Speech Features</title>
<p>In this work, speech features were extracted at the sentence or utterance level (<xref ref-type="bibr" rid="bibr5-0018720811425922">Busso et al., 2009</xref>) using Praat scripts (<xref ref-type="bibr" rid="bibr3-0018720811425922">Boersma &amp; Weenink, 2005</xref>). The features included pitch average and max, intensity average and max, formants (F1–F5), and mean and standard deviation for 12 MFCCs. After sampling the acoustic signal, the features were extracted as feature vectors containing 33 features per sentence or utterance.</p>
</sec>
<sec id="section15-0018720811425922">
<title>Text Features</title>
<p>A total of 36 text-based features were used (see <xref ref-type="table" rid="table2-0018720811425922">Table 2</xref>). These features included the counts of emotion words and phrases per class, counts of some intensifiers, syntactic features, sentiment composition features, and accumulated sentiment flow features. Five lists of emotion words were used to perform the counts per emotion class (happy, sad, angry, surprised, and afraid). These counts are used to create the emotion signals that quantify the intensity of each emotion per sentence throughout the story or conversation. These lists were collected manually and automatically and then expanded using WordNet and ConceptNet (<xref ref-type="bibr" rid="bibr16-0018720811425922">Havasi, Speer, &amp; Alonso, 2007</xref>). Parts of speech and syntactic annotations were used to extract additional syntactic information such as intensifiers, subjects and objects, and so on. These syntactic annotations were performed using the Stanford parser and BART toolkit (<xref ref-type="bibr" rid="bibr33-0018720811425922">Versley et al., 2008</xref>).</p>
<table-wrap id="table2-0018720811425922" position="float">
<label>Table 2:</label>
<caption>
<p>Text Feature Description</p>
</caption>
<graphic alternate-form-of="table2-0018720811425922" xlink:href="10.1177_0018720811425922-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Names</th>
<th align="center">Heuristic Rule for Calculation</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="5">Sentiment composition current sentence</td>
<td>If word from sentence in happy or surprised words list:</td>
</tr>
<tr>
<td>SentCompCurr += 1</td>
</tr>
<tr>
<td>Negative_list = [sad, angry, afraid]</td>
</tr>
<tr>
<td>If word from sentence in Negative_list:</td>
</tr>
<tr>
<td>SentCompCurr −= 1</td>
</tr>
<tr>
<td rowspan="2">Sentiment composition accumulated whole story for happy, sad, angry, surprised, and afraid</td>
<td>SentCompAccumHappy = SentCompAccumHappy + counthap</td>
</tr>
<tr>
<td>Note: same for other 4 emotion classes</td>
</tr>
<tr>
<td>Sentiment composition change previous current accumulated</td>
<td>SentCompChanPrevCurrAccum = SentCompAccumCombined—SentCompAccumPrevSent</td>
</tr>
<tr>
<td rowspan="3">Sentiment composition accumulated whole story for POS, NEG, POS1, NEG1</td>
<td>CompAccumPos = CompAccumPos + w * counthap</td>
</tr>
<tr>
<td>CompAccumNeg = CompAccumNeg + w * (countsad + countang + countafr)</td>
</tr>
<tr>
<td>Note: w parameter changed manually</td>
</tr>
<tr>
<td rowspan="2">Sentiment composition accumulated whole story for the previous sentence</td>
<td>SentCompAccumPrevSent (s)= SentCompAccumCombined (s-1)</td>
</tr>
<tr>
<td>Note: s stands for sentence</td>
</tr>
<tr>
<td>Sentiment composition accumulated whole story combined</td>
<td>SentCompAccumCombined = SentCompAccumCombined + counthap—countsad—countang—countafr + countsup</td>
</tr>
<tr>
<td rowspan="3">Changes in happy, sad, angry, surprised, and afraid</td>
<td>ChangeHap = counthap—PreviousHap</td>
</tr>
<tr>
<td>PreviousHap = counthap</td>
</tr>
<tr>
<td>Note: Same for 4 other classes</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Sentiment composition features in this work follow a simpler approach than the one proposed in <xref ref-type="bibr" rid="bibr25-0018720811425922">Moilanen and Pulman (2007)</xref>. This simplified approach is used to avoid the time-consuming task of hand coding many rules. Here, the five emotion lists and simple per-class rules are used as decision points to determine sentiment composition per class. If positive emotion words are found in the sentence, the sentiment composition feature is incremented by +1. If negative emotion words are found, the sentiment composition feature is decreased by −1.</p>
<p>To capture story context and previous information, the accumulated sentiment flow throughout a story is also recorded. Accumulated sentiment flow features take into consideration the emotion energy (previous counts) of previous sentences in the story. Additional sentiment delta features are used to calculate the sentiment change between the current sentence and previous sentences. A more detailed description of the text-based features can be obtained from <xref ref-type="bibr" rid="bibr6-0018720811425922">Calix (2011)</xref>.</p>
</sec>
<sec id="section16-0018720811425922">
<title>Classification Model</title>
<p>Classification methodologies are used to perform the analysis. Classifiers are mathematical techniques used to learn ways to separate data from two or more classes. In general, classifiers sample data called features from an input medium and associate those features with preassigned classes. In this case, features are extracted from sentences in text and speech form. Since each sentence has an annotated label or class, each set of features can be associated with its assigned class. Using an annotated corpus, multiple samples (sentences) can be represented in the form of a class and a vector of features (the vector space model). This information can later be divided into a training subset and a testing subset. The training subset can be used to train a model such as a linear equation or plane that separates the samples from two distinct classes. The test subset is used to test the models prediction accuracy.</p>
<p>Classification methodologies employed include support vector machines (<xref ref-type="bibr" rid="bibr4-0018720811425922">Burges, 1998</xref>) using LIBSVM (<xref ref-type="bibr" rid="bibr10-0018720811425922">Chang &amp; Lin, 2001</xref>) and Naïve Bayes, multilayer perceptron (MLP), decision trees, random forests, and the k-nearest neighbor classifier (<xref ref-type="bibr" rid="bibr34-0018720811425922">Witten &amp; Frank, 2005</xref>) using the Waikato Environment for Knowledge Analysis. Naïve Bayes is a probabilistic classifier that usually performs worst with emotional data. Therefore, it helps to set the baseline for the classification task. Random forest classifiers consist of several decision trees and usually produce good results with emotion data. Artificial neural networks (MLP) and support vector machines (SVMs) are classifiers that can handle nonlinearly separable data. In theory, this capability allows them to model data that may be more difficult to classify. SVM is a kernel-based machine learning technique that uses kernel functions such as the radial basis function (RBF) kernel to map data to higher dimensional spaces. An RBF kernel is a mathematical technique that allows for data transformation. This mathematical ability allows the classifiers to perform better than other non-kernel-based classifiers (<xref ref-type="bibr" rid="bibr34-0018720811425922">Witten &amp; Frank, 2005</xref>). The k-nearest neighbor classifier is also a good technique with the added advantage that it does not require parameter tuning.</p>
</sec>
</sec>
<sec id="section17-0018720811425922" sec-type="results">
<title>Analysis and Results</title>
<p>Results of the classification task at each step (<xref ref-type="table" rid="table3-0018720811425922">Table 3</xref>) of the methodology are presented in this section. The experiments were conducted on 89 stories from the UIUC corpus and on six episodes from the LSU-MD corpus. Each corpus is randomly divided into 80% of the samples for training and 20% for testing.</p>
<table-wrap id="table3-0018720811425922" position="float">
<label>Table 3:</label>
<caption>
<p>Emotion Analysis Phases</p>
</caption>
<graphic alternate-form-of="table3-0018720811425922" xlink:href="10.1177_0018720811425922-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Analysis Phase</th>
<th align="center">Reason for This Phase</th>
<th align="center">Phase Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Phase 1</td>
<td>To set baseline for analysis with just speech features</td>
<td>Speech affect detection</td>
</tr>
<tr>
<td>Phase 2</td>
<td>To avoid class imbalance by combining emotion samples</td>
<td>Multimodal affect detection for two classes (emotion vs. neutral)</td>
</tr>
<tr>
<td>Phase 3</td>
<td>To evaluate accuracy with class imbalance</td>
<td>Multimodal affect detection for three classes (positive, negative, neutral)</td>
</tr>
<tr>
<td>Phase 4</td>
<td>To compare to results from Phase 3</td>
<td>Multimodal affect detection for positive vs. negative</td>
</tr>
<tr>
<td>Phase 5</td>
<td>To measure fine-grained emotion detection</td>
<td>Multimodal affect detection for five emotion classes</td>
</tr>
<tr>
<td>Phase 6</td>
<td>To compare different communication mediums</td>
<td>Generalization of the methodology on a medical drama corpus</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>An analysis of the methodology using speech features only from the UIUC corpus was performed first to determine the contribution of speech features to the classification problem. This analysis was followed by an analysis of the stepwise methodology using multimodal features from the UIUC corpus. The final section shows the results of the generalization of the methodology on the LSU-MD corpus. Each section, with recall classification results, is followed by feature analysis using chi-square feature selection techniques (<xref ref-type="bibr" rid="bibr34-0018720811425922">Witten &amp; Frank, 2005</xref>). Chi-square feature ranking is a technique used to calculate the likelihood that a feature is correlated with a class (emotion). Based on the annotations in the corpus, this technique can estimate likelihoods per feature and rank the features that are most useful in the classification. This helps to identify which features are important for emotion detection. The sequence of steps in the analysis is summarized in <xref ref-type="table" rid="table3-0018720811425922">Table 3</xref>, and results of each phase in the analysis are described in the following sections.</p>
<sec id="section18-0018720811425922">
<title>Phase 1: Speech Affect Detection</title>
<p>In this section, an analysis (<xref ref-type="table" rid="table4-0018720811425922">Table 4</xref>) of the impact of 33 speech features alone is performed. Classification results indicate that positive samples are easier to detect than are negative samples. The speech analysis indicated that MFCCs have the highest contribution to emotion detection.</p>
<table-wrap id="table4-0018720811425922" position="float">
<label>Table 4:</label>
<caption>
<p>Speech Affect Detection Using Multiple Classifiers (UIUC corpus)</p>
</caption>
<graphic alternate-form-of="table4-0018720811425922" xlink:href="10.1177_0018720811425922-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="10">Multiple Classifier Comparison—Training Set (5,383) and Testing Set (1,346)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM-RBF (c: 2.5, g: 2.5)<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive (2,503)</td>
<td>47.7</td>
<td>52.3</td>
<td>37.9</td>
<td>62.1</td>
<td>54.4</td>
<td>45.6</td>
<td>55.7</td>
<td>44.3</td>
<td>49.8</td>
<td>50.2</td>
</tr>
<tr>
<td>Negative (1,177)</td>
<td>12.2</td>
<td>87.8</td>
<td>50.7</td>
<td>49.3</td>
<td>15.4</td>
<td>84.6</td>
<td>19.0</td>
<td>81.0</td>
<td>27.1</td>
<td>72.9</td>
</tr>
<tr>
<td>Neutral (3,049)</td>
<td>87.2</td>
<td>12.8</td>
<td>39.1</td>
<td>60.9</td>
<td>73.0</td>
<td>27.0</td>
<td>68.2</td>
<td>31.8</td>
<td>58.0</td>
<td>42.0</td>
</tr>
<tr>
<td>All</td>
<td>59.6</td>
<td>40.4</td>
<td>40.6</td>
<td>59.4</td>
<td>56.3</td>
<td>43.7</td>
<td>55.3</td>
<td>44.7</td>
<td>49.8</td>
<td>50.2</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0018720811425922">
<p><italic>Note</italic>. UIUC = University of Illinois at Urbana-Champaign; SVM = support vector machine; RBF = radial basis function; c = cost; g = gamma; Corr. = correct; Incorr. = incorrect.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section19-0018720811425922">
<title>Phase 2: Multimodal Affect Detection for Two Classes (Emotion Versus Neutral)</title>
<p>In this section, classification results on the two-class problem in the UIUC corpus are presented using both text and speech features. This is the first step of the methodology that helps to filter out neutral sentences. It is important to filter out neutral sentences because in general they account for half of all the samples in the corpus. The data set contains 3,680 emotion samples and 3,049 neutral samples. In machine learning, dealing with class imbalances in the data set is a very important step that if not addressed properly can negatively affect the learning process of the models. The best results of the classification methodology (<xref ref-type="table" rid="table5-0018720811425922">Table 5</xref>) were obtained using the random forest (68.6%) and SVM (71%) classifiers. The SVM classifier normalized the data and used an RBF kernel (cost: 4; gamma: 1). Feature analysis (<xref ref-type="table" rid="table6-0018720811425922">Table 6</xref>) using chi-square feature ranking indicates that sentiment composition text features have the highest contribution to classification accuracies but that speech features also play an important role.</p>
<table-wrap id="table5-0018720811425922" position="float">
<label>Table 5:</label>
<caption>
<p>Multimodal Emotion Classification Between Emotion and Neutral Classes</p>
</caption>
<graphic alternate-form-of="table5-0018720811425922" xlink:href="10.1177_0018720811425922-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Multiple Classifier Comparison—Training Set (5,383) and Testing Set (1,346)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Decision Trees<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Emotion (1)</td>
<td>71.2</td>
<td>28.8</td>
<td>48.5</td>
<td>51.5</td>
<td>61.5</td>
<td>38.5</td>
<td>77.7</td>
<td>22.3</td>
<td>63.1</td>
<td>36.9</td>
<td>70.5</td>
<td>29.5</td>
</tr>
<tr>
<td>Neutral (2)</td>
<td>71.6</td>
<td>28.4</td>
<td>71.8</td>
<td>28.2</td>
<td>70.3</td>
<td>29.7</td>
<td>57.4</td>
<td>42.6</td>
<td>60.4</td>
<td>39.6</td>
<td>65.7</td>
<td>34.3</td>
</tr>
<tr>
<td>All</td>
<td>71.4</td>
<td>28.6</td>
<td>58.9</td>
<td>41.1</td>
<td>65.5</td>
<td>34.5</td>
<td>68.6</td>
<td>31.4</td>
<td>61.9</td>
<td>38.1</td>
<td>68.4</td>
<td>31.6</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0018720811425922">
<p><italic>Note</italic>. SVM = support vector machine; Corr. = correct; Incorr. = incorrect.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table6-0018720811425922" position="float">
<label>Table 6:</label>
<caption>
<p>Feature Analysis for Emotion Versus Neutral Classes (Phase 2)</p>
</caption>
<graphic alternate-form-of="table6-0018720811425922" xlink:href="10.1177_0018720811425922-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Rank</th>
<th align="center">Chi</th>
<th align="center">Feature</th>
<th align="center">Rank</th>
<th align="center">Chi</th>
<th align="center">Feature</th>
<th align="center">Rank</th>
<th align="center">Chi</th>
<th align="center">Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>220</td>
<td>Sentiment composition accumulated whole story combined (Text)</td>
<td>17</td>
<td>95</td>
<td>Angry phrases count (Text)</td>
<td>33</td>
<td>53</td>
<td>Happy words list (Text)</td>
</tr>
<tr>
<td>2</td>
<td>180</td>
<td>Sentiment composition change previous current accumulated (Text)</td>
<td>18</td>
<td>93</td>
<td>Afraid words list (Text)</td>
<td>34</td>
<td>51</td>
<td>Intensity max (Speech)</td>
</tr>
<tr>
<td>3</td>
<td>167</td>
<td>Sentiment composition accumulated whole story negative 1 (Text)</td>
<td>19</td>
<td>91</td>
<td>MFCC 11 mean (Speech)</td>
<td>35</td>
<td>48</td>
<td>NNP (Text)</td>
</tr>
<tr>
<td>4</td>
<td>158</td>
<td>Sentiment composition accumulated whole story negative (Text)</td>
<td>20</td>
<td>87</td>
<td>MFCC 5 mean (Speech)</td>
<td>36</td>
<td>44</td>
<td>MFCC 3 mean (Speech)</td>
</tr>
<tr>
<td>5</td>
<td>153</td>
<td>MFCC 2 mean (Speech)</td>
<td>21</td>
<td>87</td>
<td>Surprised phrases count (Text)</td>
<td>37</td>
<td>38</td>
<td>Afraid change (Text)</td>
</tr>
<tr>
<td>6</td>
<td>150</td>
<td>MFCC 12 mean (Speech)</td>
<td>22</td>
<td>87</td>
<td>Sad phrases count (Text)</td>
<td>38</td>
<td>38</td>
<td>MFCC 10 mean (Speech)</td>
</tr>
<tr>
<td>7</td>
<td>145</td>
<td>Sentiment composition accumulated whole story afraid (Text)</td>
<td>23</td>
<td>87</td>
<td>Happy phrases count (Text)</td>
<td>39</td>
<td>33</td>
<td>MFCC 7 std. (Speech)</td>
</tr>
<tr>
<td>8</td>
<td>144</td>
<td>Sentiment composition accumulated whole story sad (Text)</td>
<td>24</td>
<td>87</td>
<td>Afraid phrases count (Text)</td>
<td>40</td>
<td>30</td>
<td>Intensity avg. (Speech)</td>
</tr>
<tr>
<td>9</td>
<td>142</td>
<td>Number of words in sentence (Text)</td>
<td>25</td>
<td>80</td>
<td>Sad change (Text)</td>
<td>41</td>
<td>29</td>
<td>MFCC 1 std. (Speech)</td>
</tr>
<tr>
<td>10</td>
<td>139</td>
<td>Sentiment composition accum. whole story previous sentence (Text)</td>
<td>26</td>
<td>73</td>
<td>Sentiment composition accumulated whole story surprised (Text)</td>
<td>42</td>
<td>24</td>
<td>F0 max (Speech)</td>
</tr>
<tr>
<td>11</td>
<td>138</td>
<td>Sentiment composition accumulated whole story angry (Text)</td>
<td>27</td>
<td>71</td>
<td>Angry words list (Text)</td>
<td>43</td>
<td>24</td>
<td>Object environments (Text)</td>
</tr>
<tr>
<td>12</td>
<td>138</td>
<td>MFCC 9 mean (Speech)</td>
<td>28</td>
<td>71</td>
<td>MFCC 4 mean (Speech)</td>
<td>44</td>
<td>18</td>
<td>Surprised words list (Text)</td>
</tr>
<tr>
<td>13</td>
<td>104</td>
<td>Sad words list</td>
<td>29</td>
<td>61</td>
<td>MFCC 10 standard deviation (Speech)</td>
<td>45</td>
<td>18</td>
<td>Happy change (Text)</td>
</tr>
<tr>
<td>14</td>
<td>103</td>
<td>Sentiment composition accumulated whole story positive 1 (Text)</td>
<td>30</td>
<td>61</td>
<td>Angry change (Text)</td>
<td>46</td>
<td>14</td>
<td>Surprised change (Text)</td>
</tr>
<tr>
<td>15</td>
<td>101</td>
<td>Sentiment composition accumulated whole story positive (Text)</td>
<td>31</td>
<td>60</td>
<td>MFCC 6 mean (Speech)</td>
<td>47</td>
<td>12</td>
<td>Subject actor (Text)</td>
</tr>
<tr>
<td>16</td>
<td>101</td>
<td>Sentiment composition accumulated whole story happy (Text)</td>
<td>32</td>
<td>53</td>
<td>MFCC 2 standard deviation (Speech)</td>
<td>48</td>
<td>10</td>
<td>Sentiment composition current sentence NPs (Text)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0018720811425922">
<p><italic>Note</italic>. <xref ref-type="table" rid="table2-0018720811425922">Table 2</xref> provides additional description of how the text features were calculated. MFCC = Mel frequency cepstral coefficients.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section20-0018720811425922">
<title>Phase 3: Multimodal Affect Detection for Three Classes (Positive, Negative, and Neutral)</title>
<p>In this section, classification results on the three-class problem in the UIUC corpus are presented. The data set includes 2,503 positive samples, 1,177 negative samples, and 3,049 neutral samples. The best results of the classification on the test set (<xref ref-type="table" rid="table7-0018720811425922">Table 7</xref>) were obtained using the random forest (59%) classifier and SVM (62%) classifier with RBF kernel (cost: 4; gamma: 1). This analysis helps to illustrate that neutral classes, if not filtered out, decrease the performance of the classifier. Analysis after filtering out neutral samples is presented in the next section.</p>
<table-wrap id="table7-0018720811425922" position="float">
<label>Table 7:</label>
<caption>
<p>Multimodal Emotion Classification Analysis Using Three Classes</p>
</caption>
<graphic alternate-form-of="table7-0018720811425922" xlink:href="10.1177_0018720811425922-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Multiple Classifier Comparison—Training Set (5,383) and Testing Set (1,346)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Decision Trees<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Negative (1)</td>
<td>56.9</td>
<td>43.1</td>
<td>33.3</td>
<td>66.7</td>
<td>61.5</td>
<td>38.5</td>
<td>55.7</td>
<td>44.3</td>
<td>49.0</td>
<td>51.0</td>
<td>55.7</td>
<td>44.3</td>
</tr>
<tr>
<td>Positive (2)</td>
<td>31.7</td>
<td>68.3</td>
<td>45.7</td>
<td>54.3</td>
<td>28.5</td>
<td>71.5</td>
<td>30.3</td>
<td>69.7</td>
<td>34.4</td>
<td>65.6</td>
<td>44.8</td>
<td>55.2</td>
</tr>
<tr>
<td>Neutral (3)</td>
<td>78.1</td>
<td>21.9</td>
<td>65.5</td>
<td>34.5</td>
<td>68.2</td>
<td>31.8</td>
<td>67.0</td>
<td>33.0</td>
<td>58.5</td>
<td>41.5</td>
<td>67.8</td>
<td>32.2</td>
</tr>
<tr>
<td>All</td>
<td>62.3</td>
<td>37.7</td>
<td>49.8</td>
<td>50.2</td>
<td>59.1</td>
<td>40.9</td>
<td>56.6</td>
<td>43.4</td>
<td>50.9</td>
<td>49.1</td>
<td>59.4</td>
<td>40.6</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0018720811425922">
<p><italic>Note</italic>. SVM = support vector machine; Corr. = correct; Incorr. = incorrect.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section21-0018720811425922">
<title>Phase 4: Multimodal Affect Detection for Positive Versus Negative</title>
<p>In this section, all neutral samples are filtered out and the classification model tries to separate between positive and negative classes. The UIUC subset includes 2,503 positive samples and 1,177 negative samples. All classification models performed better once neutral sentences were filtered out (<xref ref-type="table" rid="table8-0018720811425922">Table 8</xref>). SVM-RBF (cost: 4; gamma: 1) achieved the best results (76.9%).</p>
<table-wrap id="table8-0018720811425922" position="float">
<label>Table 8:</label>
<caption>
<p>Multimodal Emotion Classification After Filtering Out the Neutral Class</p>
</caption>
<graphic alternate-form-of="table8-0018720811425922" xlink:href="10.1177_0018720811425922-table8.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Positive vs. Negative: Multiple Classifier Comparison—Training Set (2,944) and Testing Set (736)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Decision Trees<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Negative (1)</td>
<td>89.7</td>
<td>10.3</td>
<td>71.7</td>
<td>28.3</td>
<td>92.3</td>
<td>7.7</td>
<td>76.1</td>
<td>23.9</td>
<td>78.1</td>
<td>21.9</td>
<td>79.4</td>
<td>20.6</td>
</tr>
<tr>
<td>Positive (2)</td>
<td>50.8</td>
<td>49.2</td>
<td>57.0</td>
<td>43.0</td>
<td>45.0</td>
<td>55.0</td>
<td>61.2</td>
<td>38.8</td>
<td>48.3</td>
<td>51.7</td>
<td>62.0</td>
<td>38.0</td>
</tr>
<tr>
<td>All</td>
<td>76.9</td>
<td>23.1</td>
<td>66.8</td>
<td>33.2</td>
<td>76.8</td>
<td>23.2</td>
<td>71.2</td>
<td>28.8</td>
<td>68.3</td>
<td>31.7</td>
<td>73.6</td>
<td>26.4</td>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<th/>
<th align="center" colspan="12">Five Emotion Classes: Multiple Classifier Comparison—Training Set (2,944) and Testing Set (736)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Decision Trees<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
<tr>
<td>Angry (1)</td>
<td>48.8</td>
<td>51.2</td>
<td>14.6</td>
<td>85.4</td>
<td>53.7</td>
<td>46.3</td>
<td>35.8</td>
<td>64.2</td>
<td>43.1</td>
<td>56.9</td>
<td>52.8</td>
<td>47.2</td>
</tr>
<tr>
<td>Fear (2)</td>
<td>66.5</td>
<td>33.5</td>
<td>25.8</td>
<td>74.2</td>
<td>57.7</td>
<td>42.3</td>
<td>55.5</td>
<td>44.5</td>
<td>40.7</td>
<td>59.3</td>
<td>50.5</td>
<td>49.5</td>
</tr>
<tr>
<td>Happy (3)</td>
<td>73.9</td>
<td>26.1</td>
<td>43.0</td>
<td>57.0</td>
<td>70.5</td>
<td>29.5</td>
<td>58.9</td>
<td>41.1</td>
<td>47.3</td>
<td>52.7</td>
<td>65.7</td>
<td>34.3</td>
</tr>
<tr>
<td>Sad (4)</td>
<td>33.1</td>
<td>66.9</td>
<td>61.4</td>
<td>38.6</td>
<td>39.4</td>
<td>60.6</td>
<td>24.4</td>
<td>75.6</td>
<td>33.9</td>
<td>66.1</td>
<td>45.7</td>
<td>54.3</td>
</tr>
<tr>
<td>Surprised (5)</td>
<td>37.1</td>
<td>62.9</td>
<td>10.3</td>
<td>89.7</td>
<td>32.0</td>
<td>68.0</td>
<td>33.0</td>
<td>67.0</td>
<td>33.0</td>
<td>67.0</td>
<td>38.1</td>
<td>61.9</td>
</tr>
<tr>
<td>All</td>
<td>56.0</td>
<td>44.0</td>
<td>32.9</td>
<td>67.1</td>
<td>54.1</td>
<td>45.9</td>
<td>44.8</td>
<td>55.2</td>
<td>40.8</td>
<td>59.2</td>
<td>52.7</td>
<td>47.3</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0018720811425922">
<p><italic>Note</italic>. SVM = support vector machine; Corr. = correct; Incorr. = incorrect.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section22-0018720811425922">
<title>Phase 5: Multimodal Affect Detection for Five Emotion Classes</title>
<p>In this section, all emotion sentences are analyzed using five emotion classes (happy, sad, angry, surprised, and afraid). The UIUC subset includes 643 samples for angry, 907 for afraid, 1,013 for happy, 617 for sad, and 500 for surprised. To visualize misclassification errors, the confusion matrix for the five classes using the SVM classifier can be seen in <xref ref-type="table" rid="table9-0018720811425922">Table 9</xref>.</p>
<table-wrap id="table9-0018720811425922" position="float">
<label>Table 9:</label>
<caption>
<p>Confusion Matrix for Five Classes</p>
</caption>
<graphic alternate-form-of="table9-0018720811425922" xlink:href="10.1177_0018720811425922-table9.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Angry</th>
<th align="center">Fear</th>
<th align="center">Happy</th>
<th align="center">Sad</th>
<th align="center">Surprised</th>
</tr>
</thead>
<tbody>
<tr>
<td>Angry</td>
<td>60</td>
<td>27</td>
<td>22</td>
<td>6</td>
<td>8</td>
</tr>
<tr>
<td>Fear</td>
<td>21</td>
<td>121</td>
<td>24</td>
<td>6</td>
<td>10</td>
</tr>
<tr>
<td>Happy</td>
<td>8</td>
<td>23</td>
<td>153</td>
<td>9</td>
<td>14</td>
</tr>
<tr>
<td>Sad</td>
<td>13</td>
<td>37</td>
<td>27</td>
<td>42</td>
<td>8</td>
</tr>
<tr>
<td>Surprised</td>
<td>16</td>
<td>16</td>
<td>22</td>
<td>7</td>
<td>36</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Results of the classification task using five emotion classes are presented in <xref ref-type="table" rid="table8-0018720811425922">Table 8</xref>. Overall, the random forest and SVM classifiers performed best. The SVM classifier normalized the data and used an RBF kernel (cost: 4; gamma: 1). Feature analysis using chi-square feature ranking is presented in <xref ref-type="table" rid="table10-0018720811425922">Table 10</xref>. The feature analysis shows that text-based accumulated sentiment composition features have the highest contribution to emotion detection for the five-class problem. Some examples of incorrectly classified samples from the UIUC corpus are provided in <xref ref-type="table" rid="table11-0018720811425922">Table 11</xref>.</p>
<table-wrap id="table10-0018720811425922" position="float">
<label>Table 10:</label>
<caption>
<p>Feature Analysis for Five-Class Problem</p>
</caption>
<graphic alternate-form-of="table10-0018720811425922" xlink:href="10.1177_0018720811425922-table10.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Rank</th>
<th align="center">Chi</th>
<th align="center">Feature</th>
<th align="center">Rank</th>
<th align="center">Chi</th>
<th align="center">Feature</th>
<th align="center">Rank</th>
<th align="center">Chi</th>
<th align="center">Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>352</td>
<td>Happy words list (Text)</td>
<td>17</td>
<td>165</td>
<td>Sentiment composition accumulated whole story sad (Text)</td>
<td>33</td>
<td>57</td>
<td>MFCC 8 std. (Speech)</td>
</tr>
<tr>
<td>2</td>
<td>308</td>
<td>Sentiment composition accumulated whole story surprise (Text)</td>
<td>18</td>
<td>164</td>
<td>MFCC 2 mean (Speech)</td>
<td>34</td>
<td>56</td>
<td>Sad phrases count (Text)</td>
</tr>
<tr>
<td>3</td>
<td>296</td>
<td>Sentiment composition accumulated whole story angry (Text)</td>
<td>19</td>
<td>144</td>
<td>MFCC 9 mean (Speech)</td>
<td>35</td>
<td>55</td>
<td>MFCC 9 std. (Speech)</td>
</tr>
<tr>
<td>4</td>
<td>289</td>
<td>Sentiment composition accumulated whole story positive 1 (Text)</td>
<td>20</td>
<td>135</td>
<td>MFCC 10 mean (Speech)</td>
<td>36</td>
<td>51</td>
<td>Intensity avg. (Speech)</td>
</tr>
<tr>
<td>5</td>
<td>289</td>
<td>Sentiment composition accumulated whole story positive (Text)</td>
<td>21</td>
<td>118</td>
<td>MFCC 12 standard deviation (Speech)</td>
<td>37</td>
<td>51</td>
<td>MFCC 1 mean (Speech)</td>
</tr>
<tr>
<td>6</td>
<td>289</td>
<td>Sentiment composition accumulated whole story happy (Text)</td>
<td>22</td>
<td>113</td>
<td>MFCC 4 standard deviation (Speech)</td>
<td>38</td>
<td>50</td>
<td>F0 avg. (Speech)</td>
</tr>
<tr>
<td>7</td>
<td>288</td>
<td>Sentiment composition accumulated whole story afraid (Text)</td>
<td>23</td>
<td>108</td>
<td>MFCC 7 mean (Speech)</td>
<td>39</td>
<td>50</td>
<td>MFCC 8 mean (Speech)</td>
</tr>
<tr>
<td>8</td>
<td>284</td>
<td>MFCC 11 mean (Speech)</td>
<td>24</td>
<td>98</td>
<td>Sad words list (Text)</td>
<td>40</td>
<td>49</td>
<td>Angry change (Text)</td>
</tr>
<tr>
<td>9</td>
<td>277</td>
<td>Sentiment composition accumulated whole story combined (Text)</td>
<td>25</td>
<td>95</td>
<td>Angry words list (Text)</td>
<td>41</td>
<td>48</td>
<td>Surprised words list (Text)</td>
</tr>
<tr>
<td>10</td>
<td>269</td>
<td>Sentiment composition accumulated whole story negative (Text)</td>
<td>26</td>
<td>88</td>
<td>Sad change (Text)</td>
<td>42</td>
<td>48</td>
<td>MFCC 6 mean (Speech)</td>
</tr>
<tr>
<td>11</td>
<td>264</td>
<td>Sentiment composition accum. whole story previous sentence (Text)</td>
<td>27</td>
<td>83</td>
<td>MFCC 3 standard deviation (Speech)</td>
<td>43</td>
<td>47</td>
<td>MFCC 11 std. (Speech)</td>
</tr>
<tr>
<td>12</td>
<td>263</td>
<td>Sentiment composition accumulated whole story negative 1 (Text)</td>
<td>28</td>
<td>79</td>
<td>Number of words in sentence (Text)</td>
<td>44</td>
<td>44</td>
<td>Afraid words list (Text)</td>
</tr>
<tr>
<td>13</td>
<td>231</td>
<td>MFCC 12 mean (Speech)</td>
<td>29</td>
<td>75</td>
<td>MFCC 7 standard deviation (Speech)</td>
<td>45</td>
<td>42</td>
<td>Surprise change (Text)</td>
</tr>
<tr>
<td>14</td>
<td>226</td>
<td>Happy change (Text)</td>
<td>30</td>
<td>75</td>
<td>Sentiment composition current sentence NPs (Text)</td>
<td>46</td>
<td>42</td>
<td>Sentiment comp. change previous current accum. (Text)</td>
</tr>
<tr>
<td>15</td>
<td>218</td>
<td>MFCC 3 mean (Speech)</td>
<td>31</td>
<td>66</td>
<td>Angry phrases count (Text)</td>
<td>47</td>
<td>40</td>
<td>Intensify emotion (Text)</td>
</tr>
<tr>
<td>16</td>
<td>167</td>
<td>MFCC 1 standard deviation (Speech)</td>
<td>32</td>
<td>64</td>
<td>MFCC 4 mean (Speech)</td>
<td>48</td>
<td>38</td>
<td>Surprised phrases count (Text)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0018720811425922">
<p><italic>Note</italic>. <xref ref-type="table" rid="table2-0018720811425922">Table 2</xref> provides additional description of how the text features were calculated. MFCC = Mel frequency cepstral coefficients.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table11-0018720811425922" position="float">
<label>Table 11:</label>
<caption>
<p>Incorrectly Classified Samples From the UIUC Corpus</p>
</caption>
<graphic alternate-form-of="table11-0018720811425922" xlink:href="10.1177_0018720811425922-table11.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Annotated Class</th>
<th align="center">Predicted Class</th>
<th align="center">Sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td>Surprised</td>
<td>Angry</td>
<td>Mr. Tod was mystified; he sat quite still, and listened attentively.</td>
</tr>
<tr>
<td>Happy</td>
<td>Angry</td>
<td>Yes—there was no doubt about it—it had turned out even better than he had planned; the pail had hit poor old Tommy Brock, and killed him dead!</td>
</tr>
<tr>
<td>Afraid</td>
<td>Happy</td>
<td>Oh; oh! they are coming back!</td>
</tr>
<tr>
<td>Afraid</td>
<td>Angry</td>
<td>What dreadful bad language!</td>
</tr>
<tr>
<td>Afraid</td>
<td>Sad</td>
<td>He lost one of his shoes among the cabbages, and the other shoe amongst the potatoes.</td>
</tr>
<tr>
<td>Surprised</td>
<td>Angry</td>
<td>Why on earth don’t you run away? exclaimed the horrified Pigling.</td>
</tr>
<tr>
<td>Afraid</td>
<td>Happy</td>
<td>She was excited and half-frightened.</td>
</tr>
<tr>
<td>Angry</td>
<td>Happy</td>
<td>And Mr. McGregor was very angry too.</td>
</tr>
<tr>
<td>Sad</td>
<td>Angry</td>
<td>Timmy coughed and groaned, because his ribs hurted him.</td>
</tr>
<tr>
<td>Surprised</td>
<td>Happy</td>
<td>Why, I see a table spread with all kinds of good things, and robbers sitting round it making merry.</td>
</tr>
<tr>
<td>Angry</td>
<td>Happy</td>
<td>But the cat, not understanding this joke, sprang at his face, and spat, and scratched at him.</td>
</tr>
<tr>
<td>Afraid</td>
<td>Happy</td>
<td>Thereupon she took her leave of her father, and rode away with them, and rode to the court of her former betrothed, whom she loved so dearly.</td>
</tr>
<tr>
<td>Happy</td>
<td>Afraid</td>
<td>The woman who had hoped to find a good sale, gave him what he desired, but went away quite angry and grumbling.</td>
</tr>
<tr>
<td>Afraid</td>
<td>Angry</td>
<td>If we quarrel with him, and he strikes about him, seven of us will fall at every blow; not one of us can stand against him.</td>
</tr>
<tr>
<td>Afraid</td>
<td>Angry</td>
<td>But he did not venture to give him his dismissal, for he dreaded lest he should strike him and all his people dead, and place himself on the royal throne.</td>
</tr>
<tr>
<td>Afraid</td>
<td>Sad</td>
<td>It is rather dark, said he; “they forgot to build windows in this room to let the sun in; a candle would be no bad thing.”</td>
</tr>
<tr>
<td>Sad</td>
<td>Angry</td>
<td>Then you may imagine how she wept over her poor children.</td>
</tr>
<tr>
<td>Sad</td>
<td>Afraid</td>
<td>She sought her children, but they were nowhere to be found.</td>
</tr>
<tr>
<td>Angry</td>
<td>Afraid</td>
<td>The miller thought to himself: “The wolf wants to deceive someone,” and refused; but the wolf said: “If you will not do it, I will devour you.”</td>
</tr>
<tr>
<td>Afraid</td>
<td>Sad</td>
<td>In his trouble and fear he went down into the courtyard and took thought how to help himself out of his trouble.</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section23-0018720811425922">
<title>Phase 6: Generalization of the Methodology on a Medical Drama Corpus</title>
<p>In this section, the analysis of the methodology using 2,657 annotated samples (utterances) from a medical drama corpus is presented. The 2,657 samples consisted of 1,246 emotion samples and 1,411 neutral samples. Sentences with three words or fewer were filtered out for this analysis. <xref ref-type="table" rid="table12-0018720811425922">Table 12</xref> presents the recall accuracy results of the classification task between emotional and neutral samples. The SVM model, which was trained using a polynomial kernel, performed best overall (recall: 61%).</p>
<table-wrap id="table12-0018720811425922" position="float">
<label>Table 12:</label>
<caption>
<p>Multimodal Emotion Classification Between Emotion and Neutral Classes for the Medical Corpus</p>
</caption>
<graphic alternate-form-of="table12-0018720811425922" xlink:href="10.1177_0018720811425922-table12.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Multiple Classifier Comparison—Training Set (2,126) and Testing Set (531)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Decision Trees<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Emotion (1)</td>
<td>61</td>
<td>39</td>
<td>53</td>
<td>47</td>
<td>58</td>
<td>42</td>
<td>61</td>
<td>39</td>
<td>49</td>
<td>51</td>
<td>57</td>
<td>43</td>
</tr>
<tr>
<td>Neutral (2)</td>
<td>55</td>
<td>45</td>
<td>66</td>
<td>34</td>
<td>56</td>
<td>44</td>
<td>49</td>
<td>51</td>
<td>58</td>
<td>42</td>
<td>59</td>
<td>41</td>
</tr>
<tr>
<td>All</td>
<td>58</td>
<td>42</td>
<td>60</td>
<td>40</td>
<td>57</td>
<td>43</td>
<td>54</td>
<td>46</td>
<td>54</td>
<td>46</td>
<td>58</td>
<td>42</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-0018720811425922">
<p><italic>Note</italic>. SVM = support vector machine; Corr. = correct; Incorr. = incorrect.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Feature analysis for the classification between emotion and neutral classes in the LSU-MD corpus indicated that the speech features were slightly more useful than were text features for emotion detection when compared to the UIUC corpus. This result is to be expected since each sentence or utterance in the medical drama corpus is shorter. As a result, there are fewer words for semantic analysis. The most useful text features were accumulated sentiment composition features for the emotion classes.</p>
<p><xref ref-type="table" rid="table13-0018720811425922">Table 13</xref> presents the recall accuracy results of the classification task for positive and negative classes and the five emotion classes. After filtering out neutral samples and utterances with three words or fewer, the data set consisted of 1,246 samples. The number of samples per emotion class was as follows: 969 negative, 277 positive, 277 happy, 106 sad, 429 angry, 200 surprised, and 234 afraid. As can be seen, the data are highly imbalanced, and there are fewer samples per class. To address this issue, the synthetic minority over-sampling technique (SMOTE) proposed by <xref ref-type="bibr" rid="bibr11-0018720811425922">Chawla, Bowyer, Hall, and Kegelmeyer (2002)</xref> was applied to the data. Class imbalances can cause classification algorithms to underperform. The SMOTE technique oversamples minority classes of a data set to even out the representation of the data. The number of samples per class after resampling was as follows: 969 negative, 554 positive, 426 happy, 424 sad, 429 angry, 420 surprised, and 444 afraid.</p>
<table-wrap id="table13-0018720811425922" position="float">
<label>Table 13:</label>
<caption>
<p>Multimodal Emotion Classification After Filtering Out the Neutral Class</p>
</caption>
<graphic alternate-form-of="table13-0018720811425922" xlink:href="10.1177_0018720811425922-table13.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Positive vs. Negative: Multiple Classifier Comparison—Training Set (1,218) and Testing Set (305)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Decision Trees<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Negative (1)</td>
<td>83</td>
<td>17</td>
<td>66</td>
<td>34</td>
<td>92</td>
<td>8</td>
<td>84</td>
<td>16</td>
<td>78</td>
<td>22</td>
<td>71</td>
<td>29</td>
</tr>
<tr>
<td>Positive (2)</td>
<td>69</td>
<td>31</td>
<td>65</td>
<td>35</td>
<td>60</td>
<td>40</td>
<td>63</td>
<td>37</td>
<td>59</td>
<td>41</td>
<td>81</td>
<td>19</td>
</tr>
<tr>
<td>All</td>
<td>78</td>
<td>22</td>
<td>65</td>
<td>35</td>
<td>80</td>
<td>20</td>
<td>76</td>
<td>24</td>
<td>71</td>
<td>29</td>
<td>75</td>
<td>25</td>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<th/>
<th align="center" colspan="12">Five Emotion Classes: Multiple Classifier Comparison—Training Set (1,714) and Testing Set (429)<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">SVM<hr/></th>
<th align="center" colspan="2">Naïve Bayes<hr/></th>
<th align="center" colspan="2">Random Forest<hr/></th>
<th align="center" colspan="2">Multilayer Perceptron<hr/></th>
<th align="center" colspan="2">Decision Trees<hr/></th>
<th align="center" colspan="2">Nearest Neighbor<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
<th align="center">Corr. (%)</th>
<th align="center">Incorr. (%)</th>
</tr>
<tr>
<td>Angry (1)</td>
<td>56</td>
<td>44</td>
<td>34</td>
<td>66</td>
<td>49</td>
<td>51</td>
<td>32</td>
<td>68</td>
<td>31</td>
<td>69</td>
<td>33</td>
<td>67</td>
</tr>
<tr>
<td>Fear (2)</td>
<td>73</td>
<td>27</td>
<td>41</td>
<td>59</td>
<td>55</td>
<td>45</td>
<td>53</td>
<td>47</td>
<td>45</td>
<td>55</td>
<td>75</td>
<td>25</td>
</tr>
<tr>
<td>Happy (3)</td>
<td>61</td>
<td>39</td>
<td>29</td>
<td>71</td>
<td>38</td>
<td>62</td>
<td>39</td>
<td>61</td>
<td>40</td>
<td>60</td>
<td>69</td>
<td>31</td>
</tr>
<tr>
<td>Sad (4)</td>
<td>84</td>
<td>16</td>
<td>60</td>
<td>40</td>
<td>64</td>
<td>36</td>
<td>51</td>
<td>49</td>
<td>48</td>
<td>52</td>
<td>91</td>
<td>9</td>
</tr>
<tr>
<td>Surprised (5)</td>
<td>71</td>
<td>29</td>
<td>40</td>
<td>60</td>
<td>43</td>
<td>57</td>
<td>50</td>
<td>50</td>
<td>36</td>
<td>64</td>
<td>80</td>
<td>20</td>
</tr>
<tr>
<td>All</td>
<td>69</td>
<td>31</td>
<td>41</td>
<td>59</td>
<td>50</td>
<td>50</td>
<td>45</td>
<td>55</td>
<td>40</td>
<td>60</td>
<td>70</td>
<td>30</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn8-0018720811425922">
<p><italic>Note</italic>. SVM = support vector machine; Corr. = correct; Incorr. = incorrect.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Classification results after class resampling are in the 56% to 84% range and are much better (<xref ref-type="table" rid="table13-0018720811425922">Table 13</xref>). Both SVM classification models for “positive versus negative” (cost: 32; gamma: 0.1) and “five emotions” (cost: 100; gamma: 0.9) used an RBF kernel.</p>
</sec>
</sec>
<sec id="section24-0018720811425922" sec-type="discussion">
<title>Discussion</title>
<p>The lower performance of the methodology in the “emotion versus neutral” problem using the LSU-MD corpus can be attributed to following factors: fragmented sentences, shorter sentences, environment noise, dialogue format of the text, sarcasm, and smaller sample size for training and testing. In addition, adults who read children’s stories may be more expressive.</p>
<p>The health care data used in this study are dialogue based. Therefore, the samples are less likely to be fully formed sentences like in the children’s stories corpus and have fewer words per sentence. For example, the samples in the medical drama corpus included many utterances that are not always grammatical. In fact, the presence of many short statements such as “yes,” “good,” and “OK” presented a challenge to the classification model, which expected longer sentences. This reason influenced the approach of filtering out samples with three words or fewer. Furthermore, the speech recordings included all kinds of background noise such as beepers, music, and other types of noise. This noise may have confused the detection system.</p>
<p>Another factor that may have influenced the performance is the flow of the conversations. Dialogue-based conversations have a different flow than narrative-based stories. In dialogue formats, the sentiment flow jumps from one emotion to another since utterances are coming from different speakers. Finally, in dialogue systems, speakers do not have to describe the environment as in children’s stories, and they make substantial use of humor and sarcasm.</p>
<p>Although the results were less accurate for the “emotion versus neutral” problem in the medical drama corpus, the model still performed moderately well. Both text and speech features contributed to the detection. The analysis suggests that text features can be more discriminative on communication data with richer language (e.g., children’s stories). On the other hand, for short utterances, acoustic features appear to be more discriminative.</p>
</sec>
<sec id="section25-0018720811425922" sec-type="conclusions">
<title>Conclusions and Future Work</title>
<p>Classification between emotion and neutral sentences achieved recall accuracies as high as 77% for emotion samples in a children’s stories corpus and 61% in a medical drama corpus. Once neutral sentences were filtered out, the system achieved recall accuracy scores for detecting negative sentences as high as 92.3%. The methodology presented in this work using semantically rich and imbalanced corpora produced results that are consistent with results presented in <xref ref-type="bibr" rid="bibr12-0018720811425922">Chuang and Wu (2004)</xref>, <xref ref-type="bibr" rid="bibr2-0018720811425922">Alm (2008)</xref>, and <xref ref-type="bibr" rid="bibr5-0018720811425922">Busso et al. (2009)</xref>. These results are important because they represent a baseline for emotion detection studies using imbalanced data.</p>
<p>The feature analysis helps to corroborate recent findings by <xref ref-type="bibr" rid="bibr22-0018720811425922">Luengo et al. (2010)</xref> that spectral features (MFCCs) are better than prosodic features (pitch) for emotion detection. The results also show promising results for text-based features that consider accumulated sentiment composition in a story or conversation. The results of this study suggest that gold-standard corpora can be used to develop and tune emotion recognition systems but that specific characteristics of the application domain must also be considered.</p>
<p>To visualize the system response to emotion detection, the approach uses the detected emotional states as context to adapt and adjust a 3-D graphics-based virtual environment and virtual agent. The environment in which a 3-D virtual agent interacts is modified in response to the emotion inputs. <xref ref-type="fig" rid="fig1-0018720811425922">Figure 1</xref> presents examples of the rendered virtual worlds using cube mapping and morphing (<xref ref-type="bibr" rid="bibr1-0018720811425922">Akenine-Moller, Haines, &amp; Hoffman, 2008</xref>) and environment maps (<xref ref-type="bibr" rid="bibr13-0018720811425922">Cube Maps, 2010</xref>). The emotion gradient (upper-right panel) represents that the color background can be gradually changed to reflect emotion. The other renderings represent system responses using background and facial expressions on an embodied conversation agent.</p>
<fig id="fig1-0018720811425922" position="float">
<label>Figure 1.</label>
<caption>
<p>Emotion-sensitive virtual world renderings.</p>
</caption>
<graphic xlink:href="10.1177_0018720811425922-fig1.tif"/>
</fig>
<p>Future work will focus on improving classification accuracy and on exploring emotion magnitude prediction from text and speech. Other communication areas that require higher semantic understanding such as humor and sarcasm are not addressed in this work. <xref ref-type="bibr" rid="bibr24-0018720811425922">McGraw and Warren (2010)</xref>, for example, argue that laughter results from violations to norms that are seen as benign. Future work will address these issues by exploring the use of semantic data structures, contradiction detection, and comedy or sarcasm knowledge bases. The difference between grammatical and nongrammatical communication will also be studied in more depth. Nongrammatical approaches will require the use of a new parser that can focus on this type of sentence structure.</p>
<p>As discussed in <xref ref-type="bibr" rid="bibr19-0018720811425922">Larsen and McGraw (2011)</xref>, people may hold multiple emotions simultaneously. The method used in this study currently predicts only one class per sentence. Future work will consider the ability of people to feel concurrent emotional states by developing separate classifiers per emotion class.</p>
</sec>
<sec id="section26-0018720811425922">
<title>Key Points</title>
<list id="list1-0018720811425922" list-type="bullet">
<list-item><p>Classification between emotion and neutral sentences achieved recall accuracies as high as 77% in the UIUC corpus and 61% in the LSU-MD corpus.</p></list-item>
<list-item><p>Once neutral sentences were filtered out, the system achieved accuracy scores for detecting negative sentences as high as 92.3%.</p></list-item>
<list-item><p>Feature analysis indicates that speech spectral features (MFCCs) are better than speech prosodic features (pitch) for emotion detection.</p></list-item>
<list-item><p>Accumulated sentiment composition text features appear to be very important for emotion detection.</p></list-item>
</list>
</sec>
</body>
<back>
<bio>
<p>Ricardo A. Calix received a BS in industrial and systems engineering from the Universidad Tecnologica Centroamericana, Tegucigalpa, Honduras, in 2001 and an MBA from Louisiana State University (LSU), Baton Rouge, in 2006. He received MS and PhD degrees in engineering science with concentrations in information technology and engineering from LSU, Baton Rouge, in 2010 and 2011, respectively. His research interests include human–computer interaction, semantic analysis, natural language processing, and machine learning.</p>
<p>Leili Javadpour received a BE in industrial engineering from Isfahan University of Technology in 2007 and an MS in product design and management from Liverpool University in 2009. She is currently pursuing a PhD in engineering science with concentrations in information technology and engineering at Louisiana State University. Her research interests include semantic analysis and machine learning.</p>
<p>Gerald M. Knapp received BS and MS degrees in industrial engineering from the State University of New York, Buffalo, in 1987 and 1989, respectively, and a PhD degree in industrial engineering from the University of Iowa, Iowa City, in 1992. He is Fred B. &amp; Ruth B. Zigler Associate Professor of Engineering at Louisiana State University, Baton Rouge. His research interests include semantic analysis and natural language processing, information systems, and human–computer interaction.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0018720811425922">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Akenine-Moller</surname><given-names>T.</given-names></name>
<name><surname>Haines</surname><given-names>E.</given-names></name>
<name><surname>Hoffman</surname><given-names>N.</given-names></name>
</person-group> (<year>2008</year>). <source>Real-time rendering</source>. <publisher-loc>Wellesley, MA</publisher-loc>: <publisher-name>A K Peters</publisher-name>.</citation>
</ref>
<ref id="bibr2-0018720811425922">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Alm</surname><given-names>C. O.</given-names></name>
</person-group> (<year>2008</year>). <source>Affect in text and speech</source> (<comment>Unpublished doctoral dissertation</comment>). <publisher-name>University of Illinois at Urbana-Champaign</publisher-name>.</citation>
</ref>
<ref id="bibr3-0018720811425922">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Boersma</surname><given-names>P.</given-names></name>
<name><surname>Weenink</surname><given-names>D.</given-names></name>
</person-group> (<year>2005</year>). <source>Praat: Doing phonetics by computer</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.fon.hum.uva.nl/praat/">http://www.fon.hum.uva.nl/praat/</ext-link></comment></citation>
</ref>
<ref id="bibr4-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Burges</surname><given-names>C. J.</given-names></name>
</person-group> (<year>1998</year>). <article-title>A tutorial on support vector machines for pattern recognition</article-title>. <source>Data Mining and Knowledge Discovery</source>, <volume>2</volume>, <fpage>121</fpage>–<lpage>167</lpage>.</citation>
</ref>
<ref id="bibr5-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Busso</surname><given-names>C.</given-names></name>
<name><surname>Lee</surname><given-names>S.</given-names></name>
<name><surname>Narayanan</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Analysis of emotionally salient aspects of fundamental frequency for emotion detection</article-title>. <source>IEEE Transactions on Audio, Speech, and Language Processing</source>, <volume>17</volume>(<issue>4</issue>), <fpage>582</fpage>–<lpage>596</lpage>.</citation>
</ref>
<ref id="bibr6-0018720811425922">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Calix</surname><given-names>R.</given-names></name>
</person-group> (<year>2011</year>). <source>Automated semantic understanding of human emotions in writing and speech</source> (<comment>Unpublished doctoral dissertation</comment>). <publisher-name>Louisiana State University</publisher-name>, <publisher-loc>Baton Rouge</publisher-loc>.</citation>
</ref>
<ref id="bibr7-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Calix</surname><given-names>R.</given-names></name>
<name><surname>Knapp</surname><given-names>G.</given-names></name>
</person-group> (<year>2011</year>, <month>February</month>). <source>Affect Corpus 2.0: An extension of a corpus for actor level emotion magnitude detection</source>. <conf-name>Paper presented at the 2nd ACM Multimedia Systems conference</conf-name>, <conf-loc>San Jose, CA</conf-loc>.</citation>
</ref>
<ref id="bibr8-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Calix</surname><given-names>R.</given-names></name>
<name><surname>Mallepudi</surname><given-names>S.</given-names></name>
<name><surname>Chen</surname><given-names>B.</given-names></name>
<name><surname>Knapp</surname><given-names>G.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Emotion recognition in text for 3D facial expression rendering</article-title>. <source>IEEE Transactions on Multimedia</source>, <volume>12</volume>, <fpage>544</fpage>–<lpage>551</lpage>.</citation>
</ref>
<ref id="bibr9-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Castellano</surname><given-names>G.</given-names></name>
<name><surname>Kessous</surname><given-names>L.</given-names></name>
<name><surname>Caridakis</surname><given-names>G.</given-names></name>
</person-group> (<year>2007</year>, <month>September</month>). <source>Multimodal emotion recognition from expressive faces, body gestures and speech</source>. <conf-name>Paper presented at the Doctoral Consortium of the 2nd International Conference on Affective and Intelligent Interaction</conf-name>, <conf-loc>Lisbon, Portugal</conf-loc>.</citation>
</ref>
<ref id="bibr10-0018720811425922">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Chang</surname><given-names>C.-C.</given-names></name>
<name><surname>Lin</surname><given-names>C.</given-names></name>
</person-group> (<year>2001</year>). <source>LIBSVM: A library for support vector machines</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">http://www.csie.ntu.edu.tw/~cjlin/libsvm</ext-link></comment></citation>
</ref>
<ref id="bibr11-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chawla</surname><given-names>N.</given-names></name>
<name><surname>Bowyer</surname><given-names>K.</given-names></name>
<name><surname>Hall</surname><given-names>L.</given-names></name>
<name><surname>Kegelmeyer</surname><given-names>W.</given-names></name>
</person-group> (<year>2002</year>). <article-title>SMOTE: Synthetic minority over-sampling technique</article-title>. <source>Journal of Artificial Intelligence Research</source>, <volume>16</volume>, <fpage>341</fpage>–<lpage>378</lpage>.</citation>
</ref>
<ref id="bibr12-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chuang</surname><given-names>Z.</given-names></name>
<name><surname>Wu</surname><given-names>CH</given-names></name>
</person-group>. (<year>2004</year>). <article-title>Multi-modal emotion recognition from speech and text</article-title>. <source>Computational Linguistics and Chinese Language Processing</source>, <volume>9</volume>(<issue>2</issue>), <fpage>45</fpage>–<lpage>62</lpage>.</citation>
</ref>
<ref id="bibr13-0018720811425922">
<citation citation-type="web">
<collab>Cube Maps</collab>. (<year>2010</year>). <article-title>Cubical environment maps</article-title>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.codemonsters.de/home/content.php?show=cubemaps">http://www.codemonsters.de/home/content.php?show=cubemaps</ext-link></comment></citation>
</ref>
<ref id="bibr14-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fontaine</surname><given-names>J.</given-names></name>
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
<name><surname>Roesch</surname><given-names>E.</given-names></name>
<name><surname>Ellsworth</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The world of emotions is not two-dimensional</article-title>. <source>Psychological Science</source>, <volume>18</volume>, <fpage>1050</fpage>–<lpage>1057</lpage>.</citation>
</ref>
<ref id="bibr15-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Hansen</surname><given-names>J. H. L.</given-names></name>
<name><surname>Bou-Ghazale</surname><given-names>S.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Getting started with SUSAS: A speech under simulated and actual stress database</article-title>. In <conf-name>EUROSPEECH-97: International Conference on Speech Communication and Technology</conf-name> (<volume>Vol. 4</volume>, pp. <fpage>1743</fpage>–<lpage>1746</lpage>). <conf-loc>Rhodes, Greece</conf-loc>.</citation>
</ref>
<ref id="bibr16-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Havasi</surname><given-names>C.</given-names></name>
<name><surname>Speer</surname><given-names>R.</given-names></name>
<name><surname>Alonso</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>, <month>July</month>). <source>ConceptNet 3: A flexible, multilingual semantic network for common sense knowledge</source>. <conf-name>Paper presented at the 22nd Conference on Artificial Intelligence</conf-name>, <conf-loc>Vancouver, Canada</conf-loc>.</citation>
</ref>
<ref id="bibr17-0018720811425922">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jurafsky</surname><given-names>D.</given-names></name>
<name><surname>Martin</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <source>Speech and language processing</source> (<edition>2nd ed.</edition>). <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr18-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Klabbers</surname><given-names>E.</given-names></name>
<name><surname>Mishra</surname><given-names>T.</given-names></name>
<name><surname>van Santen</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Analysis of affective speech recordings using the super-positional intonation model</article-title>. In <conf-name>Proceedings of the 6th ISCA Workshop on Speech Synthesis</conf-name> (pp. <fpage>339</fpage>–<lpage>344</lpage>). <publisher-loc>Bonn, Germany</publisher-loc>: <publisher-name>ISCA</publisher-name>.</citation>
</ref>
<ref id="bibr19-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Larsen</surname><given-names>J.</given-names></name>
<name><surname>McGraw</surname><given-names>A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Further evidence for mixed emotions</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>100</volume>, <fpage>1095</fpage>–<lpage>1110</lpage>.</citation>
</ref>
<ref id="bibr20-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>H.</given-names></name>
<name><surname>Lieberman</surname><given-names>H.</given-names></name>
<name><surname>Selker</surname><given-names>T.</given-names></name>
</person-group> (<year>2003</year>). <article-title>A model of textual affect sensing using real-world knowledge</article-title>. In <conf-name>Proceedings of the 2003 International Conference on Intelligent User Interfaces</conf-name> (pp. <fpage>125</fpage>–<lpage>132</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>.</citation>
</ref>
<ref id="bibr21-0018720811425922">
<citation citation-type="web">
<collab>LSU-NLP</collab>. (<year>2011</year>). <source>LSU Semantic Analysis Lab</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://nlp.lsu.edu/">http://nlp.lsu.edu/</ext-link></comment></citation>
</ref>
<ref id="bibr22-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Luengo</surname><given-names>I.</given-names></name>
<name><surname>Navas</surname><given-names>E.</given-names></name>
<name><surname>Hernaez</surname><given-names>I.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Feature analysis and evaluation for automatic emotion identification in speech</article-title>. <source>IEEE Transactions on Multimedia</source>, <volume>12</volume>, <fpage>490</fpage>–<lpage>501</lpage>.</citation>
</ref>
<ref id="bibr23-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Matthews</surname><given-names>G.</given-names></name>
<name><surname>Jones</surname><given-names>D.</given-names></name>
<name><surname>Chamberlain</surname><given-names>A.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Refining the measurement of mood: The UWIST Mood Adjective Checklist</article-title>. <source>British Journal of Psychology</source>, <volume>81</volume>, <fpage>17</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr24-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McGraw</surname><given-names>A.</given-names></name>
<name><surname>Warren</surname><given-names>C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Benign violations: Making immoral behavior funny</article-title>. <source>Psychological Science</source>, <volume>20</volume>, <fpage>1</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr25-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moilanen</surname><given-names>K.</given-names></name>
<name><surname>Pulman</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Sentiment composition</article-title>. In <source>Proceedings of Recent Advances in Natural Language Processing</source> (pp. <fpage>378</fpage>–<lpage>382</lpage>).</citation>
</ref>
<ref id="bibr26-0018720811425922">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Osherenko</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Towards semantic affect sensing in sentences</article-title>. In <source>Proceedings of Artificial Intelligence and Simulation of Behavior</source> (pp. <fpage>41</fpage>–<lpage>44</lpage>). <publisher-loc>London</publisher-loc>: <publisher-name>Society for the Study of Artificial Intelligence and Simulation of Behaviour</publisher-name>.</citation>
</ref>
<ref id="bibr27-0018720811425922">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sebe</surname><given-names>N.</given-names></name>
<name><surname>Cohen</surname><given-names>I.</given-names></name>
<name><surname>Huang</surname><given-names>T.</given-names></name>
</person-group> (<year>2005</year>). <source>Handbook of pattern recognition and computer vision</source>. <publisher-loc>Hackensack, NJ</publisher-loc>: <publisher-name>World Scientific</publisher-name>.</citation>
</ref>
<ref id="bibr28-0018720811425922">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Singh</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <source>EM-ONE: An architecture for reflective common sense thinking</source> (<comment>Unpublished PhD thesis</comment>). <publisher-name>Massachusetts Institute of Technology</publisher-name>, <publisher-loc>Cambridge</publisher-loc>.</citation>
</ref>
<ref id="bibr29-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tapus</surname><given-names>A.</given-names></name>
<name><surname>Mataric</surname><given-names>M.</given-names></name>
<name><surname>Scassellati</surname><given-names>B.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The grand challenges in socially assistive robotics</article-title>. <source>IEEE Robotics and Automation Magazine</source>, <volume>14</volume>, <fpage>35</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr30-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tapus</surname><given-names>A.</given-names></name>
<name><surname>Tapus</surname><given-names>C.</given-names></name>
<name><surname>Mataric</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>User-robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy</article-title>. <source>Intelligent Service Robotics</source>, <volume>1</volume>, <fpage>169</fpage>–<lpage>183</lpage>.</citation>
</ref>
<ref id="bibr31-0018720811425922">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tawari</surname><given-names>A.</given-names></name>
<name><surname>Trivedi</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Speech emotion analysis: exploring the role of context</article-title>. <source>IEEE Transactions on Multimedia</source>, <volume>12</volume>, <fpage>502</fpage>–<lpage>509</lpage>.</citation>
</ref>
<ref id="bibr32-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Tokuhisa</surname><given-names>R.</given-names></name>
<name><surname>Inui</surname><given-names>K.</given-names></name>
<name><surname>Matsumoto</surname><given-names>Y.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Emotion classification using massive examples extracted from the web</article-title>. In <conf-name>Proceedings of the 22nd International Conference on Computational Linguistics</conf-name> (pp. <fpage>881</fpage>–<lpage>888</lpage>). <publisher-loc>Stroudsburg, PA</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>.</citation>
</ref>
<ref id="bibr33-0018720811425922">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Versley</surname><given-names>Y.</given-names></name>
<name><surname>Ponzetto</surname><given-names>S. P.</given-names></name>
<name><surname>Poesio</surname><given-names>M.</given-names></name>
<name><surname>Eidelman</surname><given-names>V.</given-names></name>
<name><surname>Jern</surname><given-names>A.</given-names></name>
<name><surname>Smith</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>X.</given-names></name>
<name><surname>Moschitti</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>, <month>May</month>). <source>BART: A modular toolkit for co-reference resolution</source>. <conf-name>Paper presented at the Language Resources and Evaluation Conference</conf-name>, <conf-loc>Marrakech, Morocco</conf-loc>.</citation>
</ref>
<ref id="bibr34-0018720811425922">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Witten</surname><given-names>I.</given-names></name>
<name><surname>Frank</surname><given-names>E.</given-names></name>
</person-group> (<year>2005</year>). <source>Data mining: Practical machine learning tools and techniques</source> (<edition>2nd ed.</edition>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>