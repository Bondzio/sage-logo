<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LDQ</journal-id>
<journal-id journal-id-type="hwp">spldq</journal-id>
<journal-title>Learning Disability Quarterly</journal-title>
<issn pub-type="ppub">0731-9487</issn>
<issn pub-type="epub">XXXX-XXXX</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0731948711428773</article-id>
<article-id pub-id-type="publisher-id">10.1177_0731948711428773</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Recipient of CLD 2009 Outstanding Researcher Award</subject>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</subj-group>
</article-categories>
<title-group>
<article-title>High-Stakes Testing for Students With Mathematics Difficulty</article-title>
<subtitle>Response Format Effects in Mathematics Problem Solving</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Powell</surname><given-names>Sarah R.</given-names></name>
<xref ref-type="aff" rid="aff1-0731948711428773">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0731948711428773"><label>1</label>University of Virginia, Charlottesville, VA, USA</aff>
<author-notes>
<corresp id="corresp1-0731948711428773">Sarah R. Powell, 417 Emmet Street South, Charlottesville, VA 22904 Email: <email>srpowell@virginia.edu</email></corresp>
<fn fn-type="other" id="bio1-0731948711428773">
<p>Sarah R. Powell, PhD, is an assistant professor at the University of Virginia. Her research interests focus on effective instruction for students with mathematics difficulties.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>35</volume>
<issue>1</issue>
<fpage>3</fpage>
<lpage>9</lpage>
<permissions>
<copyright-statement>© Council for Learning Disabilities 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Council for Learning Disabilities</copyright-holder>
</permissions>
<abstract>
<p>Students with disabilities are frequently granted accommodations for high-stakes standardized tests to provide them an opportunity to demonstrate their academic knowledge without interference from their disability. One type of possible accommodation, test response format, concerns whether students respond in multiple-choice or constructed-response format. An experimental study was conducted to assess the performance differences of third-grade students with mathematics difficulty on a test of mathematics problem solving as a function of response format. Students responding in the multiple-choice format had a significant advantage over students answering in the constructed-response format.</p>
</abstract>
<kwd-group>
<kwd>assessment</kwd>
<kwd>accommodations</kwd>
<kwd>mathematics assessment</kwd>
<kwd>mathematics difficulty</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>To “level the playing field” for students with disabilities, accommodations on standardized tests are often used (<xref ref-type="bibr" rid="bibr9-0731948711428773">Fuchs, Fuchs, Eaton, Hamlett, &amp; Karns, 2000</xref>). An accommodation does not change the construct measured by the test (<xref ref-type="bibr" rid="bibr21-0731948711428773">Lazarus, Thurlow, Lail, &amp; Christensen, 2009</xref>), but it does allow students with disabilities to demonstrate their knowledge by removing barriers that contribute to variance due to the disability. Accommodations may include, but are not limited to, testing in small groups or individually, reading aloud test items, extending testing time, using scribes for writing, and permitting larger print on the test (<xref ref-type="bibr" rid="bibr2-0731948711428773">Bielinski, Ysseldyke, Bolt, Friedebach, &amp; Friedebach, 2001</xref>; <xref ref-type="bibr" rid="bibr3-0731948711428773">Bolt &amp; Thurlow, 2004</xref>; <xref ref-type="bibr" rid="bibr17-0731948711428773">Johnson, 2000</xref>).</p>
<p>Over the past two decades, all states have begun to allow accommodations for students with disabilities on high-stakes state assessments (<xref ref-type="bibr" rid="bibr21-0731948711428773">Lazarus et al., 2009</xref>). More recently, the federal government has allowed a maximum of 2% of a school’s population to take an alternate assessment with modified achievement standards for state assessments (<xref ref-type="bibr" rid="bibr10-0731948711428773">Fuchs, Seethaler, Fuchs, &amp; Hamlett, 2008</xref>). Although there are tremendous differences from state to state as to which accommodations are permissible, most states use the student’s Individualized Education Plan (IEP) as a guide for determining which accommodations are appropriate (<xref ref-type="bibr" rid="bibr21-0731948711428773">Lazarus et al., 2009</xref>) and allow students to receive more than one type of test accommodation (<xref ref-type="bibr" rid="bibr2-0731948711428773">Bielinski et al., 2001</xref>).</p>
<p>In this introduction, prior work on test accommodations in mathematics is discussed. Next, the test accommodation of response format, on which the present study is focused, is highlighted. Then, prior research on performance differences based on response format is presented. Finally, how the present study extends prior work is explained.</p>
<sec id="section1-0731948711428773">
<title>Mathematics Test Accommodations</title>
<p>Prior work has examined the effects that accommodations have on mathematics tests for students with learning disabilities. <xref ref-type="bibr" rid="bibr19-0731948711428773">Ketterlin-Geller, Yovanoff, and Tindal (2007)</xref> examined the accommodations of reading test items aloud and simplifying test item language for students in Grade 3. Students were randomly assigned to answer the first half of the test with or without accommodations; they then answered the second half of the test in the opposite manner. Before administration of the mathematics test, students were placed into groups reflecting their reading competence: lower performing reader (below the 25th percentile on a standardized reading test) and higher performing reader (above the 50th percentile). The accommodation of reading the text aloud was beneficial for lower performing readers on test items with high language complexity and mathematics difficulty. On the other hand, for problems with low language complexity and mathematics difficulty, the reading-aloud accommodation did not provide a differential boost for lower performing readers. With a <italic>differential boost</italic>, students with disabilities benefit significantly more from an accommodation compared to students without disabilities (<xref ref-type="bibr" rid="bibr8-0731948711428773">Fuchs &amp; Fuchs, 2001</xref>). There were no accommodation differences for the higher performing readers and no differences for any student with the simplified language accommodation.</p>
<p><xref ref-type="bibr" rid="bibr9-0731948711428773">Fuchs et al. (2000)</xref> administered mathematics tests of computation, applications, and problem solving to students with and without learning disabilities in Grades 4 and 5. All students were permitted accommodations (i.e., extended testing time, questions read aloud, use of calculator, and use of a scribe when requested) on equivalent forms of tests. On the computation and applications tests, students with disabilities did not benefit from the accommodations more than students without disabilities. On problem-solving tests, however, students with disabilities demonstrated a differential boost over students without disabilities with the accommodations of extended testing time, questions read aloud, and use of a scribe when requested. These findings favoring accommodations on problem-solving tests corroborate the work of <xref ref-type="bibr" rid="bibr19-0731948711428773">Ketterlin-Geller et al. (2007)</xref>, which indicates that some accommodations, including reading text aloud, are beneficial when tests are more complex and difficult.</p>
<p>Also working with students with and without disabilities, <xref ref-type="bibr" rid="bibr25-0731948711428773">Schulte, Elliott, and Kratochwill (2001)</xref> administered a standardized test of mathematics with IEP-designed accommodations to fourth graders with and without accommodations. The two most common accommodations were extended testing time and items read aloud. Other accommodations included use of touch math strips, taking the test in a separate setting from other students, and use of guide cards for reading. Although students with and without disabilities benefited from the accommodations, gains were larger for students with disabilities. On multiple-choice test items, students with disabilities receiving accommodations profited more than students without disabilities receiving accommodations. The same pattern was not true on constructed-response test items, where no accommodation benefits were noted.</p>
<p>Synthesizing across 28 accommodation studies, <xref ref-type="bibr" rid="bibr26-0731948711428773">Sireci, Scarpati, and Li (2005)</xref> generated two conclusions about accommodations. First, and in accord with <xref ref-type="bibr" rid="bibr9-0731948711428773">Fuchs et al. (2000)</xref> and <xref ref-type="bibr" rid="bibr25-0731948711428773">Schulte et al. (2001)</xref>, the accommodation of extended testing time was beneficial for students with disabilities. Although all students, not just those with disabilities, benefited from the extended testing time accommodation, students with disabilities benefited to a greater extent (i.e., experienced a differential boost). Second, and along the lines of <xref ref-type="bibr" rid="bibr9-0731948711428773">Fuchs et al. (2000)</xref>, <xref ref-type="bibr" rid="bibr19-0731948711428773">Ketterlin-Geller et al. (2007)</xref>, and <xref ref-type="bibr" rid="bibr25-0731948711428773">Schulte et al. (2001)</xref>, the accommodation of reading items aloud on mathematics tests was beneficial for students with disabilities the majority of the time.</p>
</sec>
<sec id="section2-0731948711428773">
<title>Multiple-Choice and Constructed-Response Formats</title>
<p>Because some states (e.g., Kentucky, Hawaii, Michigan, Missouri, Washington) and the National Assessment of Educational Progress (NAEP) require students to answer mathematics questions in a constructed-response format, one potentially important accommodation for students with disabilities is the response format. This is the focus of the present study. With a response format test accommodation, item stems (i.e., questions) are identical, but constructed-response test items are transformed to multiple-choice options. Only when the response format does not alter the construct of the test item (<xref ref-type="bibr" rid="bibr21-0731948711428773">Lazarus et al., 2009</xref>) can it be considered an accommodation. For example, if the question stem is <italic>Ray owns 5 cats and 2 dogs. How many animals does he own?</italic>, the constructed-response answer would be 7. Multiple-choice answers not altering the construct of this test item would be <italic>(a) 5, (b) 7, (c) 3</italic>. However, a multiple-choice answer that would change the construct would be <italic>Which is true? (a) Ray owns more cats than dogs, (b) Ray owns fewer cats than dogs, (c) Ray owns the same number of cats and dogs</italic>.</p>
<p>A few studies conducted at the high school and college levels have assessed performance differences as a function whether students respond in multiple-choice and constructed-response tests. <xref ref-type="bibr" rid="bibr11-0731948711428773">Garner and Engelhard (1999)</xref> assessed high school students on mathematics tests in multiple-choice and constructed-response formats with a focus on gender differences. Girls demonstrated an advantage over boys on algebra items in the multiple-choice format; however, for the remaining test items (e.g., computation, data analysis, geometry), boys had a slight advantage over girls in the multiple-choice format. On constructed-response items, when students were matched by ability level, boys performed similarly to girls, having only a slight advantage on geometry and measurement problem types.</p>
<p>Also at the high school level, <xref ref-type="bibr" rid="bibr18-0731948711428773">Katz, Bennett, and Berger (2000)</xref> looked at SAT Mathematics test items administered in both multiple-choice and constructed-response formats with a focus on the strategies that students use to solve items in each response format. Participants were tested individually and, during administration, they were encouraged to “talk aloud” whatever they would say to themselves when solving a problem. Results were mixed: On some test items, the constructed-response format was more difficult for students; on other test items, the multiple-choice format presented a greater challenge. Students used a variety of traditional and nontraditional strategies across response formats. <xref ref-type="bibr" rid="bibr18-0731948711428773">Katz et al. (2000)</xref> noted that educators tend to favor constructed-response formats because they believe this response format requires higher order thinking skills (<xref ref-type="bibr" rid="bibr20-0731948711428773">Ku, 2009</xref>). If, however, students use a variety of strategies regardless of response format, as suggested by the inconsistent pattern of findings in the Katz et al. study, multiple-choice items may assess just as well as constructed-response items.</p>
<p>Along the same lines but outside of mathematics, <xref ref-type="bibr" rid="bibr22-0731948711428773">Lukhele, Thissen, and Wainer (1994)</xref> analyzed performance differences on advanced placement chemistry and history tests for high school students. There were no discernible differences between performance for the multiple-choice and constructed-response items. <xref ref-type="bibr" rid="bibr22-0731948711428773">Lukhele et al. (1994)</xref> concluded that without a significant benefit of constructed-response items, multiple-choice items could be used without hesitation to save time and money. In addition, on an advanced placement test of computer science for high school students, <xref ref-type="bibr" rid="bibr1-0731948711428773">Bennett, Rock, and Wang (1991)</xref> found that multiple-choice and constructed-response items were often testing similar constructs even though students scored slightly higher on multiple-choice items. <xref ref-type="bibr" rid="bibr1-0731948711428773">Bennett et al. (1991)</xref>, however, suggested that test format should depend on the objectives and outcomes of the test, even if the format is less efficient.</p>
<p>The work of <xref ref-type="bibr" rid="bibr4-0731948711428773">Bridgeman (1992)</xref> at the college level corroborates this body of work at the high school level. On the quantitative section of the Graduate Record Exam, students took a multiple-choice version via computer and a constructed-response version. Some students found multiple-choice items to be easy, whereas the same item stem with a constructed-response format was more difficult. Scores between the multiple-choice and constructed-response tests were, however, comparable. Unlike <xref ref-type="bibr" rid="bibr11-0731948711428773">Garner and Engelhard (1999)</xref>, Bridgeman found no gender differences between response formats.</p>
</sec>
<sec id="section3-0731948711428773">
<title>Purpose of the Present Study</title>
<p>Prior work (e.g., <xref ref-type="bibr" rid="bibr9-0731948711428773">Fuchs et al., 2000</xref>; <xref ref-type="bibr" rid="bibr19-0731948711428773">Ketterlin-Geller et al., 2007</xref>) suggests that some test accommodations are beneficial for students with learning disabilities. Specifically, the accommodations of extended test time and reading of test items aloud provide students with learning disabilities a differential boost over students without disabilities, especially on mathematics items that are more complex or difficult. With all students required to take high-stakes standardized assessments, another accommodation that may prove helpful to students with learning disabilities is that of test response format. Some research conducted at the high school and college levels (e.g., <xref ref-type="bibr" rid="bibr1-0731948711428773">Bennett et al., 1991</xref>; <xref ref-type="bibr" rid="bibr11-0731948711428773">Garner &amp; Engelhard, 1999</xref>; <xref ref-type="bibr" rid="bibr18-0731948711428773">Katz et al., 2000</xref>) reveals performance differences between multiple-choice and constructed-response formats; other research does not (e.g., <xref ref-type="bibr" rid="bibr4-0731948711428773">Bridgeman, 1992</xref>; <xref ref-type="bibr" rid="bibr22-0731948711428773">Lukhele et al., 1994</xref>).</p>
<p>Whereas <xref ref-type="bibr" rid="bibr25-0731948711428773">Schulte et al. (2001)</xref> looked at the effect of accommodations on multiple-choice and constructed-response items, a thorough investigation of the literature revealed no studies comparing multiple-choice and constructed-response formats for the same item stems in the elementary grades or for students with mathematics difficulty (MD). Therefore, this study was conducted to explore test response format differences between third-grade students with MD. The interest was mathematics performance as a function of not only whether students experienced MD but also whether that MD occurred with or without concurrent difficulty in reading given that students with concurrent difficulty tend to experience greater deficits on tests of word-problem solving (e.g., <xref ref-type="bibr" rid="bibr14-0731948711428773">Hanich, Jordan, Kaplan, &amp; Dick, 2001</xref>; <xref ref-type="bibr" rid="bibr24-0731948711428773">Powell, Fuchs, Fuchs, Cirino, &amp; Fletcher, 2009</xref>). Students were randomly assigned to multiple-choice and constructed-response testing formats, and then a standardized mathematics problem-solving test was administered.</p>
</sec>
<sec id="section4-0731948711428773" sec-type="methods">
<title>Method</title>
<sec id="section5-0731948711428773">
<title>Participants</title>
<p>Participants (<italic>n</italic> = 149) were drawn from 79 third-grade classrooms in 31 schools in two southeastern cities. Students were screened as having MD as follows. The Arithmetic subtest of the <italic>Wide Range Achievement Test–Revised</italic> (WRAT; <xref ref-type="bibr" rid="bibr32-0731948711428773">Wilkinson, 1993</xref>) was administered in a whole-class format to all students in the 79 classrooms with parental consent. Students scoring below the 26th percentile on the WRAT-Arithmetic were assessed individually on WRAT-Reading and on the two-subtest <italic>Wechsler Abbreviated Scale of Intelligence</italic> (WASI; <xref ref-type="bibr" rid="bibr31-0731948711428773">The Psychological Corporation, 1999</xref>). All students scoring between the 26th and 39th percentiles on WRAT-Reading or earning a <italic>T</italic> score below 30 on both WASI subtests were excluded. MD subtype was categorized as MD-only (&lt; 26th percentile on WRAT-Arithmetic; &gt; 39th percentile on WRAT-Reading) or as MD concomitant with reading difficulties (MDRD; &lt; 26th percentile on WRAT-Arithmetic; &lt; 26th percentile on WRAT-Reading).</p>
<p>It is important to note that, in the literature, mathematics disability is operationalized as low mathematics performance and referred to as <italic>mathematics difficulty</italic>. In this study, some students with MD were identified by their school as having a learning disability in mathematics and, therefore, received special education services. Most of the students, however, struggled with low mathematics performance without an official diagnosis. The criteria used in this study for identifying students with MD are comparable, if not more stringent, than the MD cutoff points at the 25th, 31st, 35th, or 45th percentiles commonly used in other MD research (see <xref ref-type="bibr" rid="bibr23-0731948711428773">Mazzocco, 2005</xref>). Also, based on the percentage of students who met study inclusion criteria from students with parental consent (<italic>n</italic> = 1,682), MD participants represented the lowest 8.9% of screened students. This phenomenon, whereby a cut-point on a commercial, standardized test yields a substantially smaller percentage of screened students than expected given the norms of the test, has been documented frequently in literature on MD (e.g., <xref ref-type="bibr" rid="bibr7-0731948711428773">Fuchs et al., 2005</xref>). It indicates that the samples in these studies are similar to school-identified MD (and suggests an inadequate floor in the primary grades on many commercial, standardized achievement tests).</p>
<p>The final sample comprised 41 MD-only and 108 MDRD students. The 149 students were randomly assigned, blocking by site and MD subtype, to receive the multiple-choice (<italic>n</italic> = 72) or constructed-response (<italic>n</italic> = 77) testing formats of the <italic>Iowa Tests of Basic Skills</italic> (ITBS) Level 9. In preliminary analyses, there were no significant interactions involving site or MD subtype. Therefore, site or MD status is not discussed further in this article.</p>
<p>Participants did not differ on demographics (sex, race, subsidized lunch status, special education status, retained status, and English-language learner status) as a function of testing response format (see <xref ref-type="table" rid="table1-0731948711428773">Table 1</xref>). To describe the performance of these students, the WRAT-Arithmetic, WRAT-Reading, and WASI data were used along with teacher rankings of reading and mathematics performance and teacher ratings of inattention and hyperactivity/impulsivity from the <italic>SWAN Rating Scale</italic> (<xref ref-type="bibr" rid="bibr27-0731948711428773">Swanson, n.d.</xref>; <xref ref-type="bibr" rid="bibr28-0731948711428773">Swanson et al., n.d.</xref>). There were no significant differences between students assigned to the two testing response formats on WRAT-Arithmetic, WRAT-Reading, or WASI or on the SWAN or teacher ratings of reading and mathematics levels (see <xref ref-type="table" rid="table2-0731948711428773">Table 2</xref>).</p>
<table-wrap id="table1-0731948711428773" position="float">
<label>Table 1.</label>
<caption><p>Study Participant Demographics</p></caption>
<graphic alternate-form-of="table1-0731948711428773" xlink:href="10.1177_0731948711428773-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Multiple-choice format (<italic>n</italic> = 72)</th>
<th align="center" colspan="2">Constructed-response format (<italic>n</italic> = 77)</th>
</tr>
<tr>
<th align="left">Variable</th>
<th align="center"><italic>n</italic></th>
<th align="center">%</th>
<th align="center"><italic>n</italic></th>
<th align="center">%</th>
</tr>
</thead>
<tbody>
<tr>
<td>Male</td>
<td>41</td>
<td>56.9</td>
<td>44</td>
<td>57.1</td>
</tr>
<tr>
<td colspan="5">Ethnicity</td>
</tr>
<tr>
<td> African American</td>
<td>41</td>
<td>56.9</td>
<td>43</td>
<td>55.8</td>
</tr>
<tr>
<td> Caucasian</td>
<td>7</td>
<td>9.7</td>
<td>8</td>
<td>10.4</td>
</tr>
<tr>
<td> Hispanic</td>
<td>22</td>
<td>30.6</td>
<td>22</td>
<td>28.6</td>
</tr>
<tr>
<td> Other</td>
<td>2</td>
<td>2.8</td>
<td>4</td>
<td>5.2</td>
</tr>
<tr>
<td>Subsidized lunch</td>
<td>55</td>
<td>76.4</td>
<td>57</td>
<td>74.0</td>
</tr>
<tr>
<td>School-identified disability</td>
<td>21</td>
<td>29.2</td>
<td>22</td>
<td>28.6</td>
</tr>
<tr>
<td>English-language learners</td>
<td>13</td>
<td>18.1</td>
<td>16</td>
<td>20.8</td>
</tr>
<tr>
<td>Retained</td>
<td>26</td>
<td>36.1</td>
<td>23</td>
<td>29.9</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table2-0731948711428773" position="float">
<label>Table 2.</label>
<caption><p>Screening Data, Teacher Rating Scales, and ITBS Data</p></caption>
<graphic alternate-form-of="table2-0731948711428773" xlink:href="10.1177_0731948711428773-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Multiple-choice format (<italic>n</italic> = 72)</th>
<th align="center" colspan="2">Constructed-response format (<italic>n</italic> = 77)</th>
</tr>
<tr>
<th align="left">Variable</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">Screening measure</td>
</tr>
<tr>
<td> WRAT-Arithmetic</td>
<td>21.58</td>
<td>1.95</td>
<td>21.51</td>
<td>2.02</td>
</tr>
<tr>
<td> WRAT-Reading</td>
<td>26.29</td>
<td>4.82</td>
<td>25.95</td>
<td>4.54</td>
</tr>
<tr>
<td> WASI IQ</td>
<td>86.07</td>
<td>10.96</td>
<td>85.32</td>
<td>11.92</td>
</tr>
<tr>
<td colspan="5">Teacher rating scales</td>
</tr>
<tr>
<td> Reading level<sup><xref ref-type="table-fn" rid="table-fn2-0731948711428773">a</xref></sup></td>
<td>2.78</td>
<td>0.51</td>
<td>2.68</td>
<td>0.57</td>
</tr>
<tr>
<td> Math level<sup><xref ref-type="table-fn" rid="table-fn2-0731948711428773">a</xref></sup></td>
<td>2.76</td>
<td>0.43</td>
<td>2.70</td>
<td>0.52</td>
</tr>
<tr>
<td> SWAN<sup><xref ref-type="table-fn" rid="table-fn3-0731948711428773">b</xref></sup></td>
<td>3.63</td>
<td>0.98</td>
<td>3.43</td>
<td>1.04</td>
</tr>
<tr>
<td colspan="5">ITBS</td>
</tr>
<tr>
<td> Level 9<sup><xref ref-type="table-fn" rid="table-fn4-0731948711428773">c</xref></sup></td>
<td>4.48</td>
<td>2.63</td>
<td>3.44</td>
<td>2.03</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0731948711428773">
<p>Note: WRAT = <italic>Wide Range Achievement Test–Revised</italic> (<xref ref-type="bibr" rid="bibr32-0731948711428773">Wilkinson, 1993</xref>); WASI = <italic>Wechsler Abbreviated Scale of Intelligence</italic> (<xref ref-type="bibr" rid="bibr31-0731948711428773">The Psychological Corporation, 1999</xref>); SWAN = <italic>SWAN Rating Scale</italic> (<xref ref-type="bibr" rid="bibr27-0731948711428773">Swanson, n.d.</xref>); ITBS = <italic>Iowa Tests of Basic Skills</italic>.</p>
</fn>
<fn id="table-fn2-0731948711428773">
<label>a</label>
<p>1 = above grade level, 2 = at grade level, 3 = below grade level.</p>
</fn>
<fn id="table-fn3-0731948711428773">
<label>b</label>
<p>1 = far below, 2 = below, 3 = slightly below, 4 = average, 5 = slightly above, 6 = above, 7 = far above.</p>
</fn>
<fn id="table-fn4-0731948711428773">
<label>c</label>
<p>Score was corrected for multiple-choice format.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section6-0731948711428773">
<title>Measure</title>
<p>The Math Problem Solving and Data Interpretation subtest of the ITBS, Level 9, Form A (<xref ref-type="bibr" rid="bibr16-0731948711428773">Hoover, Dunbar, &amp; Frisbie, 2001</xref>), was administered in the spring. On the ITBS Level 9, students solve 22 word problems with multiple-choice answers. Each multiple-choice problem has a correct response and three distracters. Nine are straightforward word problems; 13 require students to interpret and use information from graphs or pictures to answer word problems. Typical administration of the ITBS Level 9 highlights three sample items and then allows 30 min for students to complete the test without the examiner reading the word problems aloud. Because many of the MD students struggled with reading difficulties, the ITBS was administered aloud to every student with a standard accommodation of reading problems to remove reading as a source of variance from the study findings. The examiner read each problem twice for all students, an accommodation described in the administration booklet of the ITBS Level 9 as appropriate. At Grades 1 through 5, Kuder-Richardson 20 is 0.83 to 0.87.</p>
<p>The ITBS was administered in two formats: multiple-choice and constructed-response. Across both formats, all test items were read aloud and reread. To create the constructed-response format, the multiple-choice responses for 10 items were deleted. It was not possible to change 12 of the 22 test items into constructed-response items because the multiple-choice options were not numerical answers (i.e., “multiply 48 by 12” or “How many stuffed animals Mike brought”). Of the 10 items, 5 were straightforward word problems and 5 had information embedded within pictures or graphs. Because creating multiple-choice items can be a lengthy and tricky process where a balanced answer choice key and proper distracters are necessary (<xref ref-type="bibr" rid="bibr13-0731948711428773">Haladyna, Downing, &amp; Rodriguez, 2002</xref>; <xref ref-type="bibr" rid="bibr30-0731948711428773">Taylor, 2005</xref>), a standardized test with multiple-choice items was used instead of a constructed-response test where it would have been necessary to create the multiple-choice items. On the ITBS, the multiple-choice format required students to select among four answer choices. Students circled a letter next to their selected answer. The mode of response for the constructed-response format was a written answer. Blank space was provided beneath each item for the student to work. For the study purpose, the score was based on the 10 items for which a constructed-response mode could be derived.</p>
</sec>
<sec id="section7-0731948711428773">
<title>Procedure</title>
<p>The ITBS was administered during the first 2 weeks of March at one study site and during the first 2 weeks of April at the other site. At each site and in each condition, testing occurred in one 30-min small-group testing session in a quiet area outside of the regular classroom. Two examiners conducted each session. One examiner read verbatim from the testing script while the other examiner walked around the room and ensured that students were working on the correct page and had sharpened pencils. Examiners were 28 research assistants working for two universities at the two sites. Some examiners were working on or had already attained master’s or doctoral degrees in education-related fields; others were former teachers. Across sites, examiners were trained to administer the test following the same testing procedures and read from the same testing script.</p>
</sec>
<sec id="section8-0731948711428773">
<title>Data Analysis</title>
<p>To correct for the possibility of guessing a correct answer from the multiple-choice answers, a correction procedure was employed (<xref ref-type="bibr" rid="bibr6-0731948711428773">Diamond &amp; Evans, 1973</xref>). For each student who was administered the multiple-choice format of the ITBS, a correction procedure of C – (I / 3) was calculated to adjust the raw score. In this formula, C represents the number of correct items, I represents the number of incorrect items, and 3 represents the number of multiple-choice responses per test item (4) minus 1.</p>
<p>After the scores of the multiple-choice students were corrected, a preliminary analysis was run using hierarchical linear modeling (HLM). Because students were nested within classrooms and schools, it was important to account for the nested structure of the data (e.g., students at Level 1, classrooms at Level 2, and schools at Level 3). Results from the HLM analysis showed that the classroom- and school-level effects were not significant (<italic>p</italic> = .44 and <italic>p</italic> &gt; .50, respectively). The variance accounted for by classroom-level variance was 4.59%. The school level accounted for 0.02% of the variance. It is not surprising that the classroom- and school-level variance was not significant, because the number of students with MD within each classroom (<italic>M</italic> = 1.89, median = 2) and classrooms within schools (<italic>M</italic> = 2.58, median = 2) was small. Because classroom- and school-level effects were not significant, the data for the HLM analysis are not presented in this article.</p>
<p>Instead, a one-way analysis of variance (ANOVA) was applied to the data with response format (multiple-choice vs. constructed-response) as the factor. To assess performance differences as a function of response format, ITBS raw scores for constructed-response students and corrected scores for multiple-choice students were the dependent variable. The one-way ANOVA was used to determine any significant differences between multiple-choice and constructed-response format student groups. The effect size (ES) was calculated by subtracting the means and dividing by the pooled standard deviation (<xref ref-type="bibr" rid="bibr15-0731948711428773">Hedges &amp; Olkin, 1985</xref>). A post hoc power analysis demonstrated sufficient power (i.e., 1.0) to detect the study’s calculated ES with a sample of 149 students.</p>
</sec>
</sec>
<sec id="section9-0731948711428773" sec-type="results">
<title>Results</title>
<p>See <xref ref-type="table" rid="table2-0731948711428773">Table 2</xref> for ITBS scores as a function of testing response condition. The main effect of response format was significant, <italic>F</italic>(1, 147) = 7.35, <italic>p</italic> = .008, with students responding in the multiple-choice format enjoying an advantage over students responding in the constructed-response format. The ES favoring multiple-choice over constructed-response format students was 0.44 standard deviations.</p>
</sec>
<sec id="section10-0731948711428773" sec-type="discussion">
<title>Discussion</title>
<p>The purpose of this study was to explore performance differences as a function of testing response format on a high-stakes mathematics test. Students responding in the multiple-choice format significantly outperformed students responding in the constructed-response format (ES = 0.44). This result, favoring the multiple-choice testing response format, has also been reported for secondary students (<xref ref-type="bibr" rid="bibr1-0731948711428773">Bennett et al., 1991</xref>; <xref ref-type="bibr" rid="bibr11-0731948711428773">Garner &amp; Engelhard, 1999</xref>).</p>
<p>As to why the multiple-choice format has a significant advantage over constructed-response format, even after guessing was accounted for as a potential factor by the correction formula, the advantage may occur because students are given the opportunity to discriminate between reasonable and unreasonable responses. With this discrimination, students show mathematical competence even if they are incapable of deriving an answer independently. In terms of discrimination of multiple-choice items on the ITBS, the items followed typical multiple-choice conventions. The correct answer was placed randomly among three distracters. <xref ref-type="bibr" rid="bibr5-0731948711428773">Bruno and Dirkzwager (1995)</xref> believe three distracters to be a sufficient number because, with fewer distracters, the probability of guessing the correct answer increases (<xref ref-type="bibr" rid="bibr12-0731948711428773">Haladyna, 1999</xref>). On the ITBS, all the distracters were plausible and many were common errors that students might make (<xref ref-type="bibr" rid="bibr13-0731948711428773">Haladyna et al., 2002</xref>).</p>
<p>In this study, as the multiple-choice items were altered into constructed-response items, the question stems remained the same. Identical question stems are crucial for comparing multiple-choice and constructed-response items (<xref ref-type="bibr" rid="bibr12-0731948711428773">Haladyna, 1999</xref>). Even with identical questions, students answering in the multiple-choice format enjoyed a 1-point advantage (after multiple-choice scores were corrected for guessing) over students answering in the constructed-response format. Whether the advantage of multiple-choice format occurs due to discrimination skills or for other reasons, the multiple-choice format may be an accommodation that could be used for students with disabilities as found in the present study or in <xref ref-type="bibr" rid="bibr25-0731948711428773">Schulte et al. (2001)</xref>.</p>
<sec id="section11-0731948711428773">
<title>Implications for Practice</title>
<p>As states are faced with the decisions about which students will take alternate assessments and which accommodations are proper to use for students with disabilities (<xref ref-type="bibr" rid="bibr10-0731948711428773">Fuchs et al., 2008</xref>), the present study indicates that using a multiple-choice testing response may represent a tenable accommodation. As suggested by <xref ref-type="bibr" rid="bibr8-0731948711428773">Fuchs and Fuchs (2001)</xref>, an accommodation, if appropriate, provides students with disabilities with a differential boost over students without disabilities. With a differential boost, students with disabilities benefit substantially more from the accommodation than students without disabilities. Determining whether this accommodation of multiple-choice format provides a differential boost for students with disabilities requires additional research, in which the performance of students with and without disabilities is directly compared on a constructed-response and multiple-choice version of the same mathematics test.</p>
<p>The accommodation of multiple-choice format is becoming more important as the number of states choosing to use constructed-response items for mathematics assessments is on the rise (<xref ref-type="bibr" rid="bibr29-0731948711428773">Tankersley, 2007</xref>). As states struggle to identify strategies for a modified assessment, in which the content of test items can be preserved even as the level of difficulty can be reduced, the option of a multiple-choice format may be enticing for school districts and prove advantageous for students with disabilities. In addition, practically, administration of a test using a multiple-choice format is much easier and cheaper to administer than a constructed-response test (<xref ref-type="bibr" rid="bibr22-0731948711428773">Lukhele et al., 1994</xref>). Multiple-choice tests also tend to have easier administration and more reliable scoring than constructed-response tests (<xref ref-type="bibr" rid="bibr12-0731948711428773">Haladyna, 1999</xref>).</p>
<p>Before closing, it is important to note two important weaknesses in the present study. First, the number of ITBS Level 9 items that could be converted to work in both testing response formats (multiple-choice and constructed-response) was only a subset of the entire pool of items. Twelve of the 22 items could not be converted to a constructed-response format without changing the construct (<xref ref-type="bibr" rid="bibr21-0731948711428773">Lazarus et al., 2009</xref>). This suggests the potential difficulty of adapting constructed-response items to multiple-choice formats, and future work should investigate this issue. Second, as already mentioned, non-MD students were not included for comparison purposes in the present study to assess whether the accommodation of testing response format provides a differential boost for students with disabilities. This issue also needs to be pursued.</p>
<p>Overall, the results from the present study indicate that the use of a multiple-choice format may promote stronger mathematics assessment performance among students with MD in the elementary grades. It is important to note that the present findings favoring multiple-choice formats differ from previous secondary and postsecondary research indicating that testing response formats do not influence student performance (<xref ref-type="bibr" rid="bibr4-0731948711428773">Bridgeman, 1992</xref>; <xref ref-type="bibr" rid="bibr22-0731948711428773">Lukhele et al., 1994</xref>). On the other hand, it does corroborate the high school research of <xref ref-type="bibr" rid="bibr1-0731948711428773">Bennett et al. (1991)</xref> and <xref ref-type="bibr" rid="bibr11-0731948711428773">Garner and Engelhard (1999)</xref>, where students solved multiple-choice items correctly more often than constructed-response items. Further research should replicate the present study with a larger number of MD and non-MD students to gain better understanding of the effects of testing response options on tests of mathematical problem solving.</p>
</sec>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research was supported in part by Grant #1 P01046261 from the National Institute of Child Health and Human Development (NICHD) to the University of Houston, with a subcontract to Vanderbilt University, and by Core Grant #HD15052 from NICHD to Vanderbilt University. Statements do not reflect the position or policy of these agencies, and no official endorsement by them should be inferred.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bennett</surname><given-names>R. E.</given-names></name>
<name><surname>Rock</surname><given-names>D. A.</given-names></name>
<name><surname>Wang</surname><given-names>M.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Equivalence of free-response and multiple-choice items</article-title>. <source>Journal of Educational Measurement</source>, <volume>28</volume>, <fpage>77</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr2-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bielinski</surname><given-names>J.</given-names></name>
<name><surname>Ysseldyke</surname><given-names>J. E.</given-names></name>
<name><surname>Bolt</surname><given-names>S.</given-names></name>
<name><surname>Friedebach</surname><given-names>M.</given-names></name>
<name><surname>Friedebach</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Prevalence of accommodations for students with disabilities participating in a statewide testing program</article-title>. <source>Assessment for Effective Intervention</source>, <volume>26</volume>, <fpage>21</fpage>–<lpage>28</lpage>. doi:10.1177/073724770102600205<pub-id pub-id-type="doi">10.1177/073724770102600205</pub-id></citation>
</ref>
<ref id="bibr3-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bolt</surname><given-names>S. E.</given-names></name>
<name><surname>Thurlow</surname><given-names>M. L.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Five of the most frequently allowed testing accommodations in state policy: Synthesis of research</article-title>. <source>Remedial and Special Education</source>, <volume>25</volume>, <fpage>141</fpage>–<lpage>152</lpage>.</citation>
</ref>
<ref id="bibr4-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bridgeman</surname><given-names>B.</given-names></name>
</person-group> (<year>1992</year>). <article-title>A comparison of quantitative questions in open-ended and multiple-choice formats</article-title>. <source>Journal of Educational Measurement</source>, <volume>29</volume>, <fpage>253</fpage>–<lpage>271</lpage>.</citation>
</ref>
<ref id="bibr5-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bruno</surname><given-names>J. E.</given-names></name>
<name><surname>Dirkzwager</surname><given-names>A.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Determining the optimal number of alternatives to a multiple-choice test item: An information rhetoric perspective</article-title>. <source>Educational and Psychological Measurement</source>, <volume>55</volume>, <fpage>959</fpage>–<lpage>966</lpage>.</citation>
</ref>
<ref id="bibr6-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Diamond</surname><given-names>J.</given-names></name>
<name><surname>Evans</surname><given-names>W.</given-names></name>
</person-group> (<year>1973</year>). <article-title>The correction for guessing</article-title>. <source>Review of Educational Research</source>, <volume>43</volume>, <fpage>181</fpage>–<lpage>191</lpage>.</citation>
</ref>
<ref id="bibr7-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Compton</surname><given-names>D. L.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
<name><surname>Paulsen</surname><given-names>K.</given-names></name>
<name><surname>Bryant</surname><given-names>J. D.</given-names></name>
<name><surname>Hamlett</surname><given-names>C. L.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The prevention, identification, and cognitive determinants of math difficulty</article-title>. <source>Journal of Educational Psychology</source>, <volume>97</volume>, <fpage>493</fpage>–<lpage>515</lpage>. doi:10.1037/0022-0663.97.3.493<pub-id pub-id-type="doi">10.1037/0022-0663.97.3.493</pub-id></citation>
</ref>
<ref id="bibr8-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Helping teachers formulate sound test accommodation decisions for students with learning disabilities</article-title>. <source>Learning Disabilities Research and Practice</source>, <volume>16</volume>, <fpage>174</fpage>–<lpage>181</lpage>.</citation>
</ref>
<ref id="bibr9-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
<name><surname>Eaton</surname><given-names>S. B.</given-names></name>
<name><surname>Hamlett</surname><given-names>C. L.</given-names></name>
<name><surname>Karns</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Supplementing teacher judgments of mathematics test accommodations with objective data sources</article-title>. <source>School Psychology Review</source>, <volume>29</volume>, <fpage>65</fpage>–<lpage>85</lpage>.</citation>
</ref>
<ref id="bibr10-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Seethaler</surname><given-names>P. M.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
<name><surname>Hamlett</surname><given-names>C. L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Using curriculum-based measurement to identify the 2% population</article-title>. <source>Journal of Disability Policy Studies</source>, <volume>19</volume>, <fpage>153</fpage>–<lpage>161</lpage>. doi:10.1177/1044207308327471<pub-id pub-id-type="doi">10.1177/1044207308327471</pub-id></citation>
</ref>
<ref id="bibr11-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Garner</surname><given-names>M.</given-names></name>
<name><surname>Engelhard</surname><given-names>G.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Gender differences in performance on multiple-choice and constructed response mathematics items</article-title>. <source>Applied Measurement in Education</source>, <volume>12</volume>, <fpage>29</fpage>–<lpage>51</lpage>. doi:10.1207/s15324818ame1201_3<pub-id pub-id-type="doi">10.1207/s15324818ame1201_3</pub-id></citation>
</ref>
<ref id="bibr12-0731948711428773">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T. M.</given-names></name>
</person-group> (<year>1999</year>). <source>Developing and validating multiple-choice test items</source> (<edition>2nd ed.</edition>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr13-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T. M.</given-names></name>
<name><surname>Downing</surname><given-names>S. M.</given-names></name>
<name><surname>Rodriguez</surname><given-names>M. C.</given-names></name>
</person-group> (<year>2002</year>). <article-title>A review of multiple-choice item-writing guidelines for classroom assessment</article-title>. <source>Applied Measurement in Education</source>, <volume>15</volume>, <fpage>309</fpage>–<lpage>334</lpage>. doi:10.1207/S15324818AME1503_5<pub-id pub-id-type="doi">10.1207/S15324818AME1503_5</pub-id></citation>
</ref>
<ref id="bibr14-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hanich</surname><given-names>L. B.</given-names></name>
<name><surname>Jordan</surname><given-names>N. C.</given-names></name>
<name><surname>Kaplan</surname><given-names>D.</given-names></name>
<name><surname>Dick</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Performance across different areas of mathematical cognition in children with learning difficulties</article-title>. <source>Journal of Educational Psychology</source>, <volume>93</volume>, <fpage>615</fpage>–<lpage>626</lpage>. doi:10.1037//0022-0663.93.3.615<pub-id pub-id-type="doi">10.1037//0022-0663.93.3.615</pub-id></citation>
</ref>
<ref id="bibr15-0731948711428773">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
<name><surname>Olkin</surname><given-names>I.</given-names></name>
</person-group> (<year>1985</year>). <source>Statistical methods for meta-analysis</source>. <publisher-loc>Orlando, FL</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr16-0731948711428773">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hoover</surname><given-names>H. D.</given-names></name>
<name><surname>Dunbar</surname><given-names>S. B.</given-names></name>
<name><surname>Frisbie</surname><given-names>D. A.</given-names></name>
</person-group> (<year>2001</year>). <source>Iowa Tests of Basic Skills–Form A</source>. <publisher-loc>Rolling Meadows, IL</publisher-loc>: <publisher-name>Riverside</publisher-name>.</citation>
</ref>
<ref id="bibr17-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>E. S.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The effects of accommodations on performance assessments</article-title>. <source>Remedial and Special Education</source>, <volume>21</volume>, <fpage>261</fpage>–<lpage>267</lpage>.</citation>
</ref>
<ref id="bibr18-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Katz</surname><given-names>I. R.</given-names></name>
<name><surname>Bennett</surname><given-names>R. E.</given-names></name>
<name><surname>Berger</surname><given-names>A. E.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Effects of response format on difficulty of SAT-Mathematics items: It’s not the strategy</article-title>. <source>Journal of Educational Measurement</source>, <volume>37</volume>, <fpage>39</fpage>–<lpage>57</lpage>.</citation>
</ref>
<ref id="bibr19-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ketterlin-Geller</surname><given-names>L. R.</given-names></name>
<name><surname>Yovanoff</surname><given-names>P.</given-names></name>
<name><surname>Tindal</surname><given-names>G.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Developing a new paradigm for conducting research on accommodations in mathematics testing</article-title>. <source>Exceptional Children</source>, <volume>73</volume>, <fpage>331</fpage>–<lpage>347</lpage>.</citation>
</ref>
<ref id="bibr20-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ku</surname><given-names>K.Y.L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Assessing students’ critical thinking performance: Urging for measurements using multi-response format</article-title>. <source>Thinking Skills and Creativity</source>, <volume>4</volume>, <fpage>70</fpage>–<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr21-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lazarus</surname><given-names>S. S.</given-names></name>
<name><surname>Thurlow</surname><given-names>M. L.</given-names></name>
<name><surname>Lail</surname><given-names>K. E.</given-names></name>
<name><surname>Christensen</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A longitudinal analysis of state accommodations policies: Twelve years of change, 1993–2005</article-title>. <source>The Journal of Special Education</source>, <volume>43</volume>, <fpage>67</fpage>–<lpage>80</lpage>.</citation>
</ref>
<ref id="bibr22-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lukhele</surname><given-names>R.</given-names></name>
<name><surname>Thissen</surname><given-names>D.</given-names></name>
<name><surname>Wainer</surname><given-names>H.</given-names></name>
</person-group> (<year>1994</year>). <article-title>On the relative value of multiple-choice, constructed response, and examinee-selected items on two achievement tests</article-title>. <source>Journal of Educational Measurement</source>, <volume>31</volume>, <fpage>234</fpage>–<lpage>250</lpage>.</citation>
</ref>
<ref id="bibr23-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mazzocco</surname><given-names>M.M.M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Challenges in identifying target skills for math disability screening and intervention</article-title>. <source>Journal of Learning Disabilities</source>, <volume>38</volume>, <fpage>318</fpage>–<lpage>232</lpage>.</citation>
</ref>
<ref id="bibr24-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Powell</surname><given-names>S. R.</given-names></name>
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
<name><surname>Cirino</surname><given-names>P. T.</given-names></name>
<name><surname>Fletcher</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Do word-problem features differentially affect problem difficulty as a function of students’ mathematics difficulty with and without reading difficulty?</article-title> <source>Journal of Learning Disabilities</source>, <volume>42</volume>, <fpage>99</fpage>–<lpage>110</lpage>. doi:10.1177/0022219408326211<pub-id pub-id-type="doi">10.1177/0022219408326211</pub-id></citation>
</ref>
<ref id="bibr25-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schulte</surname><given-names>A.A.G.</given-names></name>
<name><surname>Elliott</surname><given-names>S. N.</given-names></name>
<name><surname>Kratochwill</surname><given-names>T. R.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Effects of testing accommodations on standardized mathematics test scores: An experimental analysis of the performances of students with and without disabilities</article-title>. <source>School Psychology Review</source>, <volume>30</volume>, <fpage>527</fpage>–<lpage>547</lpage>.</citation>
</ref>
<ref id="bibr26-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sireci</surname><given-names>S. G.</given-names></name>
<name><surname>Scarpati</surname><given-names>S. E.</given-names></name>
<name><surname>Li</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Test accommodations for students with disabilities: An analysis of the interaction hypothesis</article-title>. <source>Review of Educational Research</source>, <volume>75</volume>, <fpage>457</fpage>–<lpage>490</lpage>.</citation>
</ref>
<ref id="bibr27-0731948711428773">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>J. M.</given-names></name>
</person-group> (<comment>n.d.</comment>). <source>The SWAN Rating Scale</source>. <comment>Available at <ext-link ext-link-type="uri" xlink:href="http://www.klis.com/chandler/office/SWAN_SCALE.pdf">www.klis.com/chandler/office/SWAN_SCALE.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr28-0731948711428773">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>J.</given-names></name>
<name><surname>Schuck</surname><given-names>S.</given-names></name>
<name><surname>Mann</surname><given-names>M.</given-names></name>
<name><surname>Carlson</surname><given-names>C.</given-names></name>
<name><surname>Hartman</surname><given-names>M.</given-names></name>
<name><surname>Sergeant</surname><given-names>J.</given-names></name>
<name><surname>. . . McCleary</surname><given-names>R.</given-names></name>
</person-group> (<comment>n.d.</comment>). <source>Categorical and dimensional definitions and evaluations of symptoms of ADHD: The SNAP and SWAN Ratings Scales</source>. <access-date>Retrieved August 19, 2007</access-date>, <comment>from <ext-link ext-link-type="uri" xlink:href="http://www.adhd.net">http://www.adhd.net</ext-link></comment></citation>
</ref>
<ref id="bibr29-0731948711428773">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tankersley</surname><given-names>K.</given-names></name>
</person-group> (<year>2007</year>). <source>Tests that teach: Using standardized tests to improve instruction</source>. <publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>Association for Supervision and Curriculum Development</publisher-name>.</citation>
</ref>
<ref id="bibr30-0731948711428773">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>A. K.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Violating conventional wisdom in multiple choice test construction</article-title>. <source>College Student Journal</source>, <volume>39</volume>, <fpage>141</fpage>–<lpage>148</lpage>.</citation>
</ref>
<ref id="bibr31-0731948711428773">
<citation citation-type="book">
<collab>The Psychological Corporation</collab>. (<year>1999</year>). <source>Wechsler Abbreviated Scale of Intelligence</source>. <publisher-loc>San Antonio, TX</publisher-loc>: <publisher-name>Harcourt Brace</publisher-name>.</citation>
</ref>
<ref id="bibr32-0731948711428773">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wilkinson</surname><given-names>G. S.</given-names></name>
</person-group> (<year>1993</year>). <source>Wide Range Achievement Test–Revision 3</source>. <publisher-loc>Wilmington, DE</publisher-loc>: <publisher-name>Wide Range</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>