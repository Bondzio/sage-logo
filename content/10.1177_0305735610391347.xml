<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">POM</journal-id>
<journal-id journal-id-type="hwp">sppom</journal-id>
<journal-title>Psychology of Music</journal-title>
<issn pub-type="ppub">0305-7356</issn>
<issn pub-type="epub">1741-3087</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0305735610391347</article-id>
<article-id pub-id-type="publisher-id">10.1177_0305735610391347</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Genre identification of very brief musical excerpts</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Mace</surname><given-names>Sandra T.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Wagoner</surname><given-names>Cynthia L.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Teachout</surname><given-names>David J.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Hodges</surname><given-names>Donald A.</given-names></name>
</contrib>
<aff id="aff1-0305735610391347">University of North Carolina at Greensboro, USA</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0305735610391347">Sandra T. Mace, Music Research Institute, School of Music, University of North Carolina at Greensboro, P.O. Box 26170, Greensboro, NC 27402-6170, USA. [email: <email>stmace@uncg.edu</email>]</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2012</year>
</pub-date>
<volume>40</volume>
<issue>1</issue>
<fpage>112</fpage>
<lpage>128</lpage>
<permissions>
<copyright-statement>© Society for Education, Music, and Psychology Research 2011</copyright-statement>
<copyright-year>2011</copyright-year>
<copyright-holder content-type="society">Society for Education, Music, and Psychology Research</copyright-holder>
</permissions>
<abstract>
<p>The purpose of this study was to examine how well individuals were able to identify different music genres from very brief excerpts and whether musical training, gender and preference played a role in genre identification. Listeners were asked to identify genre from classical, jazz, country, metal, and rap/hip hop excerpts that were 125, 250, 500, or 1000 ms in length. Participants (<italic>N</italic> = 347), students recruited from three college campuses in the southeast region of the USA, were found to be quite successful in identifying the genre of brief excerpts, even at 125 ms. Length of excerpt significantly affected participants’ ability to identify genre with longer time lengths leading to greater accuracy. Classical, metal, and rap/hip hop excerpts were correctly identified more often than were country or jazz excerpts. Further, there were many distinct interactions across lengths among genres. Musical training did not affect participants’ ability to identify excerpts overall or by length, but training was found to affect genre identification: those with training were better able to identify classical and jazz excerpts while those without training were better able to identify rap/hip hop excerpts. Gender did not affect participants’ ability to identify excerpts overall or by length, but gender was found to affect genre identification: males were better able to identify metal excerpts. Preference did affect participants’ ability to identify excerpts; most favorite genres were identified more accurately than all other genres and least favorite genres were identified less accurately than all other genres. In general, these findings support a primary conclusion that people are adept at identifying particular genres when presented with excerpts that are one second or less.</p>
</abstract>
<kwd-group>
<kwd>brief excerpt</kwd>
<kwd>excerpt length</kwd>
<kwd>gender</kwd>
<kwd>genre</kwd>
<kwd>musical training</kwd>
<kwd>preference</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Nearly everyone has had the experience of rapidly scanning through radio stations, hunting for a favorite kind of music (<xref ref-type="bibr" rid="bibr13-0305735610391347">Gjerdingen &amp; Perrott, 2008</xref>; <xref ref-type="bibr" rid="bibr35-0305735610391347">Perrott &amp; Gjerdingen, 1999</xref>). It often sounds something like this:</p>
<graphic position="anchor" xlink:href="10.1177_0305735610391347-img1.tif"/>
<p>Amazingly, most listeners can perform this search mission very deftly, needing only the merest snippet to determine, first, whether a station is playing music and, second, whether the genre being broadcast is one that is desirable at the moment. Most listeners are able to make a choice in a much shorter time period than that provided by the scan function of most radios. Previous researchers (<xref ref-type="bibr" rid="bibr3-0305735610391347">Ashley, 2008</xref>; <xref ref-type="bibr" rid="bibr6-0305735610391347">Bigand, Filipic, &amp; Lalitte, 2005</xref>; <xref ref-type="bibr" rid="bibr34-0305735610391347">Peretz, Gagnon, &amp; Bouchard, 1998</xref>) found listeners were able to determine the valence of musical excerpts in as little as one-eighth of a second (i.e., 125 ms), which makes sense from an evolutionary standpoint.</p>
<p>Our brains are wired with rapid response mechanisms so that we can react instantly to perceived danger. When people encounter a sudden, unexpected sound, there is an immediate reaction called variously in the literature <italic>acoustic startle reflex</italic> (<xref ref-type="bibr" rid="bibr49-0305735610391347">Yeomans &amp; Frankland, 1996</xref>), <italic>auditory startle response</italic> (<xref ref-type="bibr" rid="bibr30-0305735610391347">Müller et al., 2003</xref>), or <italic>auditory brainstem response</italic> (<xref ref-type="bibr" rid="bibr15-0305735610391347">Hall, 2007</xref>). This is a rapid brainstem reflex that includes such motor responses as an eye blink, a facial grimace, head turning, increased muscle tension in the arms, and so on. Initial reactions can be registered in as little as 5 to 6 ms (<xref ref-type="bibr" rid="bibr15-0305735610391347">Hall, 2007</xref>) and motor responses can occur from 14 to 22 ms after the presentation of an auditory stimulus (<xref ref-type="bibr" rid="bibr21-0305735610391347">Kofler et al., 2001</xref>; <xref ref-type="bibr" rid="bibr49-0305735610391347">Yeomans &amp; Frankland, 1996</xref>). Such a brainstem reflex is also a rudimentary system that provides only minimal information, enough for an immediate response to avoid danger (<xref ref-type="bibr" rid="bibr24-0305735610391347">LeDoux, 1998</xref>).</p>
<p>A somewhat slower pathway, running from the medial geniculate nucleus in the thalamus to the auditory cortex, provides more information (<xref ref-type="bibr" rid="bibr23-0305735610391347">LeDoux, 1995</xref>). This cortical pathway is necessary for interpreting additional information about a complex stimulus. There are two, at least partially distinct, pathways for the ‘what’ and ‘where’ aspects of a sound (<xref ref-type="bibr" rid="bibr8-0305735610391347">De Santis, Clarke, &amp; Murray, 2007</xref>). In two experiments, the mean peak latency of a stimulus as it reached the auditory cortex was 45 ms (<xref ref-type="bibr" rid="bibr19-0305735610391347">Howard et al., 2000</xref>) and neural signals indicating multisensory gestalts, combining auditory and somatosensory (skin touch) information, were obtained at 50 ms (<xref ref-type="bibr" rid="bibr31-0305735610391347">Murray et al., 2005</xref>). Thus, this ‘slower pathway’ is still extremely rapid.</p>
<p>A sudden sound activates both fast and slow pathways. Stimulus information is first sent from the inner ear via the auditory nerve through lower brainstem way stations to the auditory thalamus (<xref ref-type="bibr" rid="bibr26-0305735610391347">Li, Stutzmann, &amp; LeDoux, 1996</xref>). At the auditory thalamus, the pathways divide; the fast pathway sends information directly to the amygdala, which is concerned with emotional processing, especially of fear. The slower pathway runs from the auditory thalamus to the primary auditory cortex, from there to the auditory association cortex, and then back to the amygdala. The fast and slow pathways converge in individual cells in the lateral amygdala. Information received at the lateral amygdala via the auditory cortex arrives approximately 20 ms after signals from the fast pathway are registered.</p>
<p>Fast and slow pathways are active during music listening (<xref ref-type="bibr" rid="bibr20-0305735610391347">Huron, 2006</xref>; <xref ref-type="bibr" rid="bibr42-0305735610391347">Sloboda &amp; Juslin, 2001</xref>). Certain unexpected features in the music, such as a cymbal crash or sudden modulation, may cause a startle reaction via the fast pathway. More sophisticated recognition of musical elements, say conscious awareness of a shift from major to minor, comes via the slower, cortical pathway. Each layer, from brainstem to auditory thalamus to primary auditory cortex to auditory association cortex, adds more information to the mix. Even beyond these are additional areas, such as frontal lobe and limbic regions that are concerned with evaluation and appreciation of musical experiences. <xref ref-type="bibr" rid="bibr25-0305735610391347">Lee, Skoe, Kraus, &amp; Ashley (2008)</xref> demonstrated that musical training affected auditory brainstem responses, and <xref ref-type="bibr" rid="bibr9-0305735610391347">Diethe, Teodoru, Furl, and Shawe-Taylor (2009)</xref> used magnetoencephalography to demonstrate that while it is feasible to determine which genre a listener is attending to from brain activations, it is not a reliable method. However, very little research has examined musical judgments based on these response mechanisms.</p>
<p>In a set of experiments concerning music and emotion in a brain-damaged patient, <xref ref-type="bibr" rid="bibr34-0305735610391347">Peretz, Gagnon and Bouchard (1998)</xref> also included data from four normal controls (i.e., mean age: 41.2 years, minimal musical training). From prior research, half of 32 musical excerpts—orchestral, piano with orchestra, or piano solo—were shown to evoke a sense of happiness and half to evoke a sense of sadness. These excerpts were presented to the four control participants always from the beginning but lasting only 500 ms and increasing by a second each time to a length of 7.5 seconds. There were a total of 256 trials (i.e., 32 excerpts each presented at eight different lengths: 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5 and 7.5 seconds). Excerpts were presented in a random order and participants were asked to rate each one on a happy–sad continuum of 10 points.</p>
<p>Researchers found the participants were nearly as consistent at rating the 500 ms excerpts as they were at rating the longer excerpts. For the happy excerpts, participants gave the shortest excerpt a mean rating of slightly more than 7, while the 4.5 second excerpt had an average rating of slightly more than 8, with all other lengths rated between 7 and 8. For the sad excerpts, the 500 ms excerpt was rated just below 4, the 3.5 second excerpt at 3, with all other lengths in between. Because a previous experiment in this series had confirmed tempo and mode to be prime indicators of emotion, the investigators determined the 500 ms happy excerpts contained 4.4 events and the sad excerpts contained 1.7 events on average. Thus, it is possible listeners used event density as a prime clue for determining emotional valence. Fourteen of the 16 excerpts contained sufficient information to determine mode (i.e., a tonic triad or major third), while only five of the 16 sad excerpts did so. Thus, mode may have contributed useful information also, especially in the happy excerpts.</p>
<p>To confirm the results obtained with the 500 ms excerpts were not due to some artifact such as prior exposure, an extension of the study was administered with two control groups of 10 naive subjects each. The first group of naive subjects heard the excerpts presented in random order as before, only this time each excerpt was also presented in a 250 ms version. As before, participants judged emotional valence of the 250 ms sample nearly the same as all the other lengths. The second group of naive participants heard each of the 32 excerpts presented progressively, by increments of 250 ms, beginning with 250 ms and continuing systematically to 3 seconds. Although there were slightly larger variations in emotion ratings from the shortest to longest excerpts, all the results taken together indicated that listeners could make immediate judgments of emotional valence along a happy–sad dimension.</p>
<p><xref ref-type="bibr" rid="bibr3-0305735610391347">Ashley (2008)</xref> used a subset of the excerpts used by <xref ref-type="bibr" rid="bibr34-0305735610391347">Peretz et al. (1998)</xref> at even shorter time courses: 25, 50, 100, 250, 500 and 1000 ms. In this case, participants used 10-point scales to rate the excerpts along the following dimensions: happy–sad, slow–fast, dark–bright, bass–treble, heavy–light, thick–thin, and also as major–minor. Many judgments of valence and mode made at 50 ms corresponded with those of longer time courses and nearly all the categories reached stability by 100 ms.</p>
<p><xref ref-type="bibr" rid="bibr6-0305735610391347">Bigand et al. (2005)</xref> asked musically trained and untrained listeners to place 27 musical excerpts in geometric space, where the vertical axis indicated arousal level, the horizontal axis indicated emotional valence and a third axis separated pieces with broad melodic contours from those that proceeded harmonically or by broken arpeggios. Participants did this twice, once with the excerpts at approximately 25 seconds in length and again at 1 second lengths. There was no distinction in geometric solutions between musically trained and untrained listeners. Also, multidimensional scaling was only weakly affected by length of excerpt; emotional responses did not differ between 1-second and 25-second excerpts.</p>
<p>In a follow-up experiment, <xref ref-type="bibr" rid="bibr6-0305735610391347">Bigand et al. (2005)</xref> asked participants to listen to excerpts starting at 250 ms and increasing by 500 ms to 1 second, 2 seconds, 5 seconds and approximately 20 seconds. Participants were asked to indicate on a subjective scale how ‘moving’ the excerpts sounded. Excerpts were chosen so that half would be considered strongly moving and the other half less moving. One group of musically trained and untrained subjects listened to classical excerpts, while a second group did the same for pop/rock examples. They were told to:</p>
<p><disp-quote>
<p>imagine that they were looking for moving (emotional) music on a radio. Shifting from one radio station to another would be an ecological situation that was similar to the experiment. The critical question of the experiment was to specify how many milliseconds they needed to decide whether the excerpts they were listening to were highly moving. (<xref ref-type="bibr" rid="bibr6-0305735610391347">Bigand et al., 2005</xref>, p. 434)</p>
</disp-quote></p>
<p>Results were highly consistent for both musically trained and untrained listeners and for both classical and popular genres. In all cases, judgments made with the 250 ms excerpts were the same as judgments made for the longer excerpts.</p>
<p><xref ref-type="bibr" rid="bibr35-0305735610391347">Perrott and Gjerdingen (1999)</xref> presented a touchstone paper that likened the rapid identification of music genres to scanning a radio dial. As described in the published version (<xref ref-type="bibr" rid="bibr13-0305735610391347">Gjerdingen &amp; Perrott, 2008</xref>), they asked 52 university students to listen to 400 examples in five excerpt lengths (i.e., 250 ms, 325 ms, 400 ms, 475 ms, and 3 seconds), with four vocal and four instrumental excerpts of each of 10 genres (i.e., blues, classical, country, dance, jazz, Latin, pop, R&amp;B, rap, and rock). For 3-second excerpts, listeners’ choices agreed with categories assigned by music companies about 70% of the time. Because shorter excerpts were extracted from the 3-second excerpts, researchers could compare listeners’ choices for shorter excerpts with the categories they assigned to the longest samples. Although there was considerable variation among genres, recognition scores were above chance for all genres at all excerpt lengths. Researchers found little or no effects of training, but listeners were more accurate with instrumental than vocal excerpts.</p>
<p>From these few experiments it is apparent that listeners can make very rapid judgments about the music they hear. Whether these judgments are made subcortically (<xref ref-type="bibr" rid="bibr34-0305735610391347">Peretz et al., 1998</xref>) or require cognitive processing (<xref ref-type="bibr" rid="bibr6-0305735610391347">Bigand et al., 2005</xref>) remains in doubt. Also, the time course of the presumably more sophisticated judgment of genre identification has been investigated in only one study (<xref ref-type="bibr" rid="bibr13-0305735610391347">Gjerdingen &amp; Perrott, 2008</xref>). The purpose of the present study was to extend this study. None of Gjerdingen and Perrott’s participants were music majors. They did conduct a pilot study with a small group of music students (<italic>N</italic> unspecified) and found no significant difference in the results compared to non-music majors. They also did not find a gender effect, but as two-thirds of their listeners were female, they acknowledge that their data set was not ideal. They collected the original data for their experiment in 1999, and acknowledged that several of their genres are no longer viable labels. Finally, they did not investigate the issue of preference.</p>
<p>Therefore, the purpose of the present study was to examine how well individuals were able to identify different music genres from very brief excerpts. Musically trained and untrained listeners were asked to identify the genre from classical, jazz, country, metal and rap/hip hop excerpts that were 125, 250, 500, or 1000 ms in length. Specific research questions were:</p>
<list id="list1-0305735610391347" list-type="order">
<list-item>
<p>How well were participants able to identify the correct genre for each length of excerpt?</p>
</list-item>
<list-item>
<p>Were there significant differences in accuracy by excerpt length and by genre?</p>
</list-item>
<list-item>
<p>Were possible differences among excerpt length or genre affected by musical training?</p>
</list-item>
<list-item>
<p>Were possible differences among excerpt length or genre affected by gender?</p>
</list-item>
<list-item>
<p>Did preferences affect participants’ identification accuracy?</p>
</list-item>
</list>
<sec id="section1-0305735610391347" sec-type="methods">
<title>Methods</title>
<sec id="section2-0305735610391347">
<title>Participants</title>
<p>Participants for the present experiment were 347 students recruited from three college campuses in the southeast region of the USA. <xref ref-type="table" rid="table1-0305735610391347">Table 1</xref> provides additional demographic data. For this study, age was not a factor of interest. Data included in the present study were from participants 18 years or older. Participants were undergraduate students, thus, most of them were between 18 and 24 years of age. Music training was operationally defined as having 3 or more years of formal music instruction. Approximately 72% of the participants reported that they were trained musicians.</p>
<table-wrap id="table1-0305735610391347" position="float">
<label>Table 1.</label>
<caption>
<p>Demographic data on participants</p>
</caption>
<graphic alternate-form-of="table1-0305735610391347" xlink:href="10.1177_0305735610391347-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Males</th>
<th align="left">Females</th>
<th align="left">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gender</td>
<td>129</td>
<td>218</td>
<td>347</td>
</tr>
<tr>
<th/>
<th align="left">Trained<sup><xref ref-type="table-fn" rid="table-fn1-0305735610391347">a</xref></sup></th>
<th align="left">Untrained</th>
<th/>
</tr>
<tr>
<td>Training</td>
<td>251</td>
<td>91</td>
<td>342<sup><xref ref-type="table-fn" rid="table-fn1-0305735610391347">b</xref></sup></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0305735610391347">
<p><italic>Notes</italic>: <sup>a</sup> Participants classified as ‘trained’ included those having 3 or more years of formal music training. <sup>b</sup> Because some participants did not provide training information, this number does not match the total number of participants (<italic>N</italic> = 347).</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section3-0305735610391347">
<title>Procedures for creating the stimulus CD and test form</title>
<p>Five genres were selected for the study: classical, jazz, country, metal and rap/hip hop. When considering genres, opera and pop were originally included. However, during initial sampling, opera was easily confused with classical when using samples one second or shorter. Pop contained elements of jazz, rap/hip hop, country, or metal within the brief excerpts that were used. Thus, to eliminate confounding cross-genre variables, the decision was made to eliminate opera and pop from the final genre list.</p>
<p>An ecological approach was used in choosing the format of music files for the stimulus CD. In a survey conducted by <xref ref-type="bibr" rid="bibr2-0305735610391347">Alloy Media and Marketing (2009)</xref>, researchers found that 74% of college students report ownership of an MP3 player. Whether downloaded as MP3 files, or transferred from CD to computer to MP3 player, MP3 encoded music files are apparently the most prevalent form used by the population in this study. Therefore, downloading MP3 files and using no subsequent compression was the most ecologically valid approach to creating the stimulus CD.</p>
<p>Ten examples of each genre were selected in the following manner. After determining the five genres to be used, investigators selected recordings from lists maintained by iTunes, a software-based online digital media store operated by Apple, Inc. The iTunes store was chosen because of its place as the leader in music sales with the population tested. As of the first quarter of 2009, iTunes purchases accounted for 25% of sales in the music market. The iTunes store outsold all other individual outlets with the second-place outlet accounting for 14% of sales in the music market, which includes sales of MP3 and compact discs (<xref ref-type="bibr" rid="bibr10-0305735610391347">‘Digital music increases share of overall music sales volume in the US,’ n.d.</xref>). Classical recordings came from the iTunes Essentials play list ‘Classical: 20th Century’, skipping any vocal works. For purposes of this study, examples chosen for the classical genre represent western art music, rather than that of the Classical music period generally understood to be art music composed between 1750 and 1820. Examples from the 20th century iTunes list were found to represent multiple composers, styles and instrument combinations in the classical genre. Also, because music from the other genres came from the 20th century, choosing classical music from the 20th century avoided the problem of having listeners choose those items only because they were not modern. That is, a Baroque piece might be identified not so much because it represented classical music, but because it did not fit the sounds of the other four 20th-century styles. Jazz recordings were selected from the iTunes Essentials play list ‘Jazz 101’. Because vocal jazz music was determined to be confused with country, vocal jazz recordings were omitted from consideration, and one jazz/pop-crossover and one fusion recording were eliminated. For rap/hip hop and country, the ‘Top Songs’ iTunes lists of the week were used. Starting with number 1, recordings were chosen in the order listed. Artist repetition was avoided by skipping to the next recording on the list and no single artist was represented more than once within a genre. Metal recordings were selected from the ‘Heavy Metal’ featured albums list, under the genre heading of ‘Rock’. Albums were selected according to most recent release dates, and the first track on each album was selected. All recordings were purchased between 10 and 22 September 2008. Because these lists are based on the most popular purchases transacted on iTunes, the lists change frequently. For a complete list of recordings used in the present study, see <xref ref-type="table" rid="table2-0305735610391347">Table 2</xref>.</p>
<table-wrap id="table2-0305735610391347" position="float">
<label>Table 2.</label>
<caption>
<p>Music selections list</p>
</caption>
<graphic alternate-form-of="table2-0305735610391347" xlink:href="10.1177_0305735610391347-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Genre</th>
<th align="left">Song</th>
<th align="left">Artist</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classical</td>
<td>Fanfare for the Common Man</td>
<td>Copland</td>
</tr>
<tr>
<td/>
<td>Adagio for Strings from String Quartet, Op. 11</td>
<td>Barber</td>
</tr>
<tr>
<td/>
<td>Piano Concerto in F Major: III</td>
<td>Gershwin</td>
</tr>
<tr>
<td/>
<td>Concierto de Aranjuez: II</td>
<td>Rodrigo</td>
</tr>
<tr>
<td/>
<td>Trois Gymnopedies: I</td>
<td>Satie</td>
</tr>
<tr>
<td/>
<td>Short Ride in a Fast Machine</td>
<td>Adams</td>
</tr>
<tr>
<td/>
<td>The Planets, Op. 32: IV</td>
<td>Holst</td>
</tr>
<tr>
<td/>
<td>The Unanswered Question</td>
<td>Ives</td>
</tr>
<tr>
<td/>
<td>Festive Overture, Op. 96</td>
<td>Shostakovich</td>
</tr>
<tr>
<td/>
<td>Symphony No. 1 in D Major, Op. 25: I</td>
<td>Prokofiev</td>
</tr>
<tr>
<td>Jazz</td>
<td>Take Five</td>
<td>Dave Brubeck</td>
</tr>
<tr>
<td/>
<td>So What</td>
<td>Miles Davis</td>
</tr>
<tr>
<td/>
<td>In the Mood</td>
<td>Glenn Miller</td>
</tr>
<tr>
<td/>
<td>Bedtime</td>
<td>King Cole Trio</td>
</tr>
<tr>
<td/>
<td>Chill</td>
<td>Clear Voyage</td>
</tr>
<tr>
<td/>
<td>In A Sentimental Mood</td>
<td>John Coltrane and Duke Ellington</td>
</tr>
<tr>
<td/>
<td>Sing, Sing, Sing</td>
<td>Benny Goodman</td>
</tr>
<tr>
<td/>
<td>Goodbye Pork Pie Hat</td>
<td>Charles Mingus</td>
</tr>
<tr>
<td/>
<td>Straight No Chaser</td>
<td>Thelonius Monk</td>
</tr>
<tr>
<td/>
<td>Body &amp; Soul</td>
<td>Coleman Hawkins</td>
</tr>
<tr>
<td>Country</td>
<td>Johnny &amp; June</td>
<td>Heidi Newfield</td>
</tr>
<tr>
<td/>
<td>All I Want To Do</td>
<td>Sugarland</td>
</tr>
<tr>
<td/>
<td>Just a Dream</td>
<td>Carrie Underwood</td>
</tr>
<tr>
<td/>
<td>Don’t Think I Don’t Think About It</td>
<td>Darius Rucker</td>
</tr>
<tr>
<td/>
<td>Do You Believe Me Now</td>
<td>Jimmy Wayne</td>
</tr>
<tr>
<td/>
<td>Should’ve Said No</td>
<td>Taylor Swift</td>
</tr>
<tr>
<td/>
<td>She Never Cried in Front of Me</td>
<td>Toby Keith</td>
</tr>
<tr>
<td/>
<td>Everybody Wants to Go to Heaven</td>
<td>Kenny Chesney</td>
</tr>
<tr>
<td/>
<td>Troubadour</td>
<td>George Strait</td>
</tr>
<tr>
<td/>
<td>Gunpowder &amp; Lead</td>
<td>Miranda Lambert</td>
</tr>
<tr>
<td>Metal</td>
<td>Troops of Doom</td>
<td>Sepultura</td>
</tr>
<tr>
<td/>
<td>Walk With Me in Hell</td>
<td>Sacrament</td>
</tr>
<tr>
<td/>
<td>Demon’s Kiss</td>
<td>Blue Oyster Cult</td>
</tr>
<tr>
<td/>
<td>Holy Wars … the Punishment Due</td>
<td>Megadeth</td>
</tr>
<tr>
<td/>
<td>Night Songs</td>
<td>Cinderella</td>
</tr>
<tr>
<td/>
<td>Welcome to the Jungle</td>
<td>Guns N’ Roses</td>
</tr>
<tr>
<td/>
<td>Star Spangled Banner</td>
<td>Exodus</td>
</tr>
<tr>
<td/>
<td>Heavy Metal Thunder</td>
<td>Saxon</td>
</tr>
<tr>
<td/>
<td>Madhouse</td>
<td>Anthrax</td>
</tr>
<tr>
<td/>
<td>Rock You Like A Hurricane</td>
<td>Scorpions</td>
</tr>
<tr>
<td>Rap/hip hop</td>
<td>Whatever You Like</td>
<td>T.I.</td>
</tr>
<tr>
<td/>
<td>Swagga Like Us</td>
<td>Jaz-Z &amp; T.I.</td>
</tr>
<tr>
<td/>
<td>In the Ayer</td>
<td>Flo Rida</td>
</tr>
<tr>
<td/>
<td>Can’t Believe It</td>
<td>T-Pain</td>
</tr>
<tr>
<td/>
<td>Got Money</td>
<td>Lil Wayne &amp; T-Pain</td>
</tr>
<tr>
<td/>
<td>Dangerous</td>
<td>Kardinal Offishall</td>
</tr>
<tr>
<td/>
<td>Swing</td>
<td>Savage &amp; Solilja Boy</td>
</tr>
<tr>
<td/>
<td>My Life</td>
<td>The Game</td>
</tr>
<tr>
<td/>
<td>What Them Girls Like</td>
<td>Ludacris</td>
</tr>
<tr>
<td/>
<td>Lolli Lolli (Pop that Body)</td>
<td>Three 6 Mafia</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>To assure the selection of examples was representative of genre, each recording was previewed by two of the researchers and a volunteer, who were all professional musicians familiar with the genres included in the present study. To be included, recordings needed to meet the following criteria: (1) employ instruments commonly associated with the genre; (2) employ stylistic performance practices (e.g., articulation, note duration, and phrasing) associated with the genre; and (3) neither the music nor the artist would be considered crossover. A list of random numbers was used to determine the starting point within each recording. For many pieces chosen, the introductory material could have caused confusion as a result of crossover and minimal dynamic levels. Therefore, the range of random numbers was set at 30 to 240, to reflect item selections beginning no less than 30 seconds and no more than 4 minutes from the beginning of any track. If the starting time for the excerpt exceeded the length of a particular recording, that random number was skipped and the next random number was used. A spreadsheet was kept with the random numbers used for the starting point and the length of the excerpt.</p>
<p>Four excerpts, one at each of the lengths to be examined in the present study (i.e., 125, 250, 500, and 1000 ms) were prepared from each of the 50 recordings, using Sound Forge (<xref ref-type="bibr" rid="bibr44-0305735610391347">Sony Creative Software, 2008</xref>) on a Dell 690 computer. The resultant 200 excerpts included 10 excerpts for each of the four lengths, for each of the five genres. Each excerpt began at a different place in the recording according to the list of random numbers. That is, excerpts shorter than 1 second were not chosen from within the 1 second excerpt. Thus, no learning effect was likely to have taken place.</p>
<p>Once the 200 excerpts were created, another random number list was created to organize them into 10 groups of 20 excerpts each. This was done so listeners could complete groups of 20 followed by a break, rather than completing all 200 items at once. Participants were allowed 4 seconds to determine the genre and mark the answer sheet for each item. To allow participants time to focus on each new group, 4 additional seconds of silence were inserted between each group of 20 items.</p>
<p>Each of the 10 groups of 20 items was created to have an equal number of genres and an equal number of excerpt lengths. Once the excerpts were placed into the 10 groups, another randomization placed the 20 excerpts of each group and the 10 groups into their final order for the recording. In the recording, 8 seconds of silence were inserted to allow for turning the answer sheet to continue from group 4 to group 5. In addition to the 200 items, 3 second samples of each genre were used as practice examples. Recordings for practice examples were taken from tracks other than those used for the 200 items.</p>
<p>Instructions for taking the Genre Identification Test were written and then recorded. These instructions and the 200 excerpts were burned onto a CD. Each sound file was normalized such that once set, the intensity control of the stereo amplifier did not need to be adjusted. Two different pilot-test administrations resulted in minor revisions before creating the final CD. A bubble-sheet test form was developed and piloted in the same pilot administrations. Using Remark Classic OMR software (<xref ref-type="bibr" rid="bibr14-0305735610391347">Gravic, 2009</xref>), a final version was developed that could be machine scored.</p>
</sec>
<sec id="section4-0305735610391347">
<title>Test procedures</title>
<p>During test administration, all participants were provided pencils and test forms. At campuses A and B, participants enrolled in a music appreciation course completed the task during a regular class meeting. Participants at campus C completed the task during a recital-attendance class held in a performance hall or during a regular class meeting. Consent information sheets and a brief description of the project were provided prior to administration of the test. Participants completed a demographics section of the test form providing the following information: (a) gender; (b) whether they were 18 years of age or older; (c) consent to use data for research; (d) whether they had three years or more of formal music training (e.g., read music, took lessons on an instrument, etc.); (e) most preferred music genre (from the five genres included in this study); and (f) least preferred music genre. To satisfy Institutional Review Board guidelines, data from participants younger than 18 were not used. One 3-second example of each genre was played and the recorded instructions provided correct answers before proceeding with the test items.</p>
<p>The sound systems used at campuses A and B were permanently installed multi-component stereos and stereo speakers. The sound system at campus C included a Yamaha CDX-490 CD player, a NAD S300 Amplifier, and two JBL Professional monitors (i.e., speakers). Playback equipment used at all campuses provided clarity of sound quality and more than sufficient amplification.</p>
<p>Following data collection, test forms were scanned using a National Computer Systems (NCS) Pearson Opscan 6 (NCS is now owned by Scantron Corporation). Because the software has limited capacity for data analysis, it was necessary to create multiple answer keys to obtain data necessary for answering stated research questions. Cronbach’s Alpha coefficient used to determine that reliability for the Genre Identification Test was .90.</p>
</sec>
</sec>
<sec id="section5-0305735610391347" sec-type="results">
<title>Results</title>
<p>The purpose of the present study was to determine the extent to which listeners could identify the genre of very brief examples across five music genres and four excerpt lengths. The results for 347 participants who completed the Genre Identification Test are presented in <xref ref-type="table" rid="table3-0305735610391347">Table 3</xref>. Regarding research question 1, participants were able to identify the genre of brief excerpts very well. Even at 125 ms, participants performed considerably above chance level (i.e., 20%).</p>
<table-wrap id="table3-0305735610391347" position="float">
<label>Table 3.</label>
<caption>
<p>Accuracy of genre identification</p>
</caption>
<graphic alternate-form-of="table3-0305735610391347" xlink:href="10.1177_0305735610391347-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Category</th>
<th align="left"><italic>N</italic></th>
<th align="left"><italic>M</italic></th>
<th align="left">SD</th>
<th align="left">% correct</th>
</tr>
</thead>
<tbody>
<tr>
<td>All genres at 125 ms</td>
<td>347</td>
<td>27.06</td>
<td>5.66</td>
<td>54% of 50 items</td>
</tr>
<tr>
<td>All genres at 250 ms</td>
<td>347</td>
<td>37.95</td>
<td>4.31</td>
<td>77% of 50 items</td>
</tr>
<tr>
<td>All genres at 500 ms</td>
<td>347</td>
<td>41.39</td>
<td>3.83</td>
<td>83% of 50 items</td>
</tr>
<tr>
<td>All genres at 1000 ms</td>
<td>347</td>
<td>44.55</td>
<td>3.73</td>
<td>89% of 50 items</td>
</tr>
<tr>
<td>All genres at all lengths</td>
<td>347</td>
<td>151.43</td>
<td>14.95</td>
<td>76% of 200 items</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Additional details on test performance are presented in <xref ref-type="table" rid="table4-0305735610391347">Table 4</xref>. Participants were most accurate in rap/hip hop 1000 ms (94%), classical 1000 ms (93%), metal 1000 ms (92%), and rap/hip hop 500 ms (91%). Performance was between 80 – 90% accurate in seven categories (i.e., classical 500 ms, jazz 500 and 1000 ms, country 1000 ms, metal 250 and 500 ms, rap/hip hop 250 ms). Performance was below 50% accuracy in only one category (i.e., rap/hip hop 125 ms). Participants were able to make correct judgments at 125 ms at very impressive levels for classical – 70%, jazz – 53%, country – 56%, and metal – 51%. While they were less successful for rap/hip hop at 125 ms – 40%, such an accuracy level is still well above chance.</p>
<table-wrap id="table4-0305735610391347" position="float">
<label>Table 4.</label>
<caption>
<p>Percentage of correct Genre Identification Test items according to genre and excerpt length</p>
</caption>
<graphic alternate-form-of="table4-0305735610391347" xlink:href="10.1177_0305735610391347-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Category</th>
<th align="left"><italic>N</italic></th>
<th align="left"><italic>M</italic></th>
<th align="left">SD</th>
<th align="left">% correct of items</th>
</tr>
</thead>
<tbody>
<tr>
<td>Classical overall</td>
<td>347</td>
<td>31.58</td>
<td>3.98</td>
<td>79% of 40</td>
</tr>
<tr>
<td>Classical 125 ms</td>
<td>347</td>
<td>6.99</td>
<td>1.67</td>
<td>70% of 10</td>
</tr>
<tr>
<td>Classical 250 ms</td>
<td>347</td>
<td>6.79</td>
<td>1.44</td>
<td>68% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Classical 500 ms</td>
<td>347</td>
<td>8.53</td>
<td>1.20</td>
<td>85% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Classical 1000 ms</td>
<td>347</td>
<td>9.27</td>
<td>1.05</td>
<td>93% of 10</td>
</tr>
<tr>
<td>Jazz overall</td>
<td>347</td>
<td>28.72</td>
<td>4.73</td>
<td>71% of 40</td>
</tr>
<tr>
<td>Jazz 125 ms</td>
<td>347</td>
<td>5.31</td>
<td>1.97</td>
<td>53% of 10</td>
</tr>
<tr>
<td>Jazz 250 ms</td>
<td>347</td>
<td>7.20</td>
<td>1.60</td>
<td>72% of 10</td>
</tr>
<tr>
<td>Jazz 500 ms</td>
<td>347</td>
<td>8.12</td>
<td>1.20</td>
<td>81% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Jazz 1000 ms</td>
<td>347</td>
<td>8.09</td>
<td>1.32</td>
<td>81% of 10</td>
</tr>
<tr>
<td>Country overall</td>
<td>347</td>
<td>28.86</td>
<td>4.99</td>
<td>72% of 40</td>
</tr>
<tr>
<td>Country 125 ms</td>
<td>347</td>
<td>5.65</td>
<td>2.03</td>
<td>56% of 10</td>
</tr>
<tr>
<td>Country 250 ms</td>
<td>347</td>
<td>7.66</td>
<td>1.61</td>
<td>77% of 10</td>
</tr>
<tr>
<td>Country 500 ms</td>
<td>347</td>
<td>7.01</td>
<td>1.62</td>
<td>70% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Country 1000 ms</td>
<td>347</td>
<td>8.55</td>
<td>1.30</td>
<td>85% of 10</td>
</tr>
<tr>
<td>Metal overall</td>
<td>347</td>
<td>31.20</td>
<td>3.77</td>
<td>78% of 40</td>
</tr>
<tr>
<td>Metal 125 ms</td>
<td>347</td>
<td>5.08</td>
<td>1.60</td>
<td>51% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Metal 250 ms</td>
<td>347</td>
<td>8.27</td>
<td>1.16</td>
<td>83% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Metal 500 ms</td>
<td>347</td>
<td>8.64</td>
<td>1.27</td>
<td>86% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Metal 1000 ms</td>
<td>347</td>
<td>9.21</td>
<td>0.99</td>
<td>92% of 10</td>
</tr>
<tr>
<td>Rap/hip hop overall</td>
<td>347</td>
<td>31.06</td>
<td>3.92</td>
<td>78% of 40</td>
</tr>
<tr>
<td>Rap/hip hop 125 ms</td>
<td>347</td>
<td>4.03</td>
<td>1.83</td>
<td>40% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Rap/hip hop 250 ms</td>
<td>347</td>
<td>8.51</td>
<td>1.36</td>
<td>85% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Rap/hip hop 500 ms</td>
<td>347</td>
<td>9.09</td>
<td>0.96</td>
<td>91% of 10</td>
</tr>
<tr>
<td><xref ref-type="table-fn" rid="table-fn2-0305735610391347">*</xref>Rap/hip hop 1000 ms</td>
<td>347</td>
<td>9.43</td>
<td>0.95</td>
<td>94% of 10</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0305735610391347">
<p><italic>Note</italic>: Rows with * indicate accuracy between 80% and 94%.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>For research questions 2, 3, 4, and 5, a conservative alpha level of .01 was established to control for Type I error. For all data that violated the sphericity assumption, using Mauchley’s Test, the Greenhouse–Geisser correction was used, and degrees of freedom were expressed with non-integer values. The purpose of the second research question was to explore whether significant differences in accuracy identification existed due to excerpt length and genre. A 4 × 5 (Length × Genre) repeated-measures ANOVA was performed, with excerpt length and genre both serving as within-subjects variables. Significant main effects were found for excerpt length, <italic>F</italic> (2.22, 763.38) = 2514.81, <italic>p</italic> = .000, η<sup>2</sup> = .88 and for genre, <italic>F</italic> (3.71, 1282.39) = 54.98, <italic>p</italic> = .000, η<sup>2</sup> = .14. Significant main effect differences for excerpt length and genre were examined through post-hoc pairwise comparisons of the means using Bonferroni’s adjustment. Differences were found among all excerpt lengths with accuracy increasing significantly (<italic>p</italic> &lt; .001) for each successive excerpt length (i.e., 125 ms <italic>M</italic> = 5.41, SD = 1.13; 250 ms <italic>M</italic> = 7.59, SD = 0.86; 500 ms <italic>M</italic> = 8.28, SD = 0.77; 1000 ms <italic>M</italic> = 8.91, SD = 0.75). Regarding genre, classical (<italic>M</italic> = 7.90, SD = 0.99), metal (<italic>M</italic> = 7.80, SD = 0.94), and rap/hip hop (<italic>M</italic> = 7.77, SD = 0.98) excerpts were identified significantly more accurately (<italic>p</italic> = .000) than were country (<italic>M</italic> = 7.22, SD = 1.25) or jazz excerpts (<italic>M</italic> = 7.18, SD = 1.18). No significant differences in accuracy were found among classical, metal, and rap/hip hop excerpts or between country and jazz excerpts.</p>
<p>A statistically significant two-way interaction effect was found between excerpt length and genre, <italic>F</italic> (10.08, 3487.55) = 212.26, <italic>p</italic> = .000, η<sup>2</sup> = .38. As <xref ref-type="fig" rid="fig1-0305735610391347">Figure 1</xref> shows, accuracy of genre identification is distinct for each length being tested. Particularly noteworthy is that classical decreased from being the most accurately identified genre at 125 ms to being the least accurately identified genre at 250 ms. Simultaneously, rap/hip hop increased from being the least accurately identified genre at 125 ms to being most accurately identified genre at 250 ms. Further, rap/hip hop continued as the most accurately identified genre at 500 ms and 1000 ms.</p>
<fig id="fig1-0305735610391347" position="float">
<label>Figure 1.</label>
<caption>
<p>Accuracy of genre identification across excerpt lengths</p>
</caption>
<graphic xlink:href="10.1177_0305735610391347-fig1.tif"/>
</fig>
<p>The purpose of the third research question was to examine whether significant accuracy identification differences regarding excerpt length by genre were due to musical training. A three-way repeated-measures ANOVA was performed, with training serving as a between-subjects variable and excerpt length and genre both serving as within-subjects variables. The between-subjects main effect of training, examining the overall accuracy identification difference between those with training (<italic>M</italic> = 7.60, SD = 0.78) and those without training (<italic>M</italic> = 7.56, SD = 0.73), was found to be non-significant, <italic>F</italic> (1, 340) = .585, <italic>p</italic> = .445, η<sup>2</sup> = .00. The two-way interaction effect for excerpt length by training was also found to be non-significant, <italic>F</italic> (2.20, 748.97) = 4.261, <italic>p</italic> = .012, η<sup>2</sup> = .01.</p>
<p>The two-way interaction effect for genre by training was found to be significant, <italic>F</italic> (3.76, 1278.92) = 21.72, <italic>p</italic> = .000, η<sup>2</sup> = .06. Univariate analyses revealed those with training were significantly more accurate in identifying jazz excerpts, <italic>F</italic> (1, 340) = 28.29, <italic>p</italic> = .000, η<sup>2</sup> = .08, while those without training were significantly more accurate in identifying rap/hip hop excerpts, <italic>F</italic> (1, 340) = 21.05, <italic>p</italic> = .000, η<sup>2</sup> = .06. <xref ref-type="fig" rid="fig2-0305735610391347">Figure 2</xref> shows the two-way interaction for genre by training. Further, the three-way interaction effect for genre by length by training was found to be significant, <italic>F</italic> (10.13, 3445.41) = 3.30, <italic>p</italic> = .000, η<sup>2</sup> = .01. Univariate analyses revealed those with training were significantly more accurate in identifying classical 1000 ms excerpts, <italic>F</italic> (1, 340) = 8.81, <italic>p</italic> = .003, η<sup>2</sup> = .03, jazz 125 ms excerpts, <italic>F</italic> (1, 340) = 28.84, <italic>p</italic> = .000, η<sup>2</sup> = .08, jazz 250 ms excerpts, <italic>F</italic> (1, 340) = 15.24, <italic>p</italic> = .000, η<sup>2</sup> = .04, and jazz 500 ms excerpts, <italic>F</italic> (1, 340) = 15.06, <italic>p</italic> = .000, η<sup>2</sup> = .04. Those without training were significantly more accurate in identifying rap/hip hop 125 ms, <italic>F</italic> (1, 340) = 10.81, <italic>p</italic> = .001, η<sup>2</sup> = .03, rap/hip hop 250 ms, <italic>F</italic> (1, 340) = 15.63, <italic>p</italic> = .000, η<sup>2</sup> = .04, and rap/hip hop 500 ms, <italic>F</italic> (1, 340) = 20.06, <italic>p</italic> = .000, η<sup>2</sup> = .06.</p>
<fig id="fig2-0305735610391347" position="float">
<label>Figure 2.</label>
<caption>
<p>Two-way interaction effect of genre by training</p>
</caption>
<graphic xlink:href="10.1177_0305735610391347-fig2.tif"/>
</fig>
<p>The purpose of the fourth research question was to examine whether significant accuracy identification differences regarding excerpt length by genre were due to gender. A three-way repeated-measures ANOVA was performed, with gender serving as a between-subjects variable and excerpt length and genre both serving as within-subjects variables. The between subjects main effect of gender, examining the overall accuracy identification difference between males (<italic>M</italic> = 7.60, SD = 0.78) and females (<italic>M</italic> = 7.56, SD = 0.73), was found to be non-significant, <italic>F</italic> (1, 345) = .23, <italic>p</italic> = .630, η<sup>2</sup> = .00. The two-way interaction effect for excerpt length by gender was also found to be non-significant, <italic>F</italic> (2.22, 765.77) = 4.13, <italic>p</italic> = .013, η<sup>2</sup> = .01. However, the two-way interaction effect for genre by gender was found to be significant, <italic>F</italic> (3.74, 1288.67) = 7.44, <italic>p</italic> = .000, η<sup>2</sup> = .02. <xref ref-type="fig" rid="fig3-0305735610391347">Figure 3</xref> shows the interaction for genre by gender. Univariate analyses revealed males to be significantly more accurate in identifying metal excerpts, <italic>F</italic> (1, 345) = 7.49, <italic>p</italic> = .007, η<sup>2</sup> = .02. The three-way interaction effect for genre by length by gender was also found to be significant, <italic>F</italic> (10.05, 3468.66) = 2.35, <italic>p</italic> = .009, η<sup>2</sup> = .01. Univariate analyses revealed males to be significantly more accurate with identifying jazz 250 ms excerpts, <italic>F</italic> (1, 345) = 8.97, <italic>p</italic> = .003, η<sup>2</sup> = .03, and metal 125 ms excerpts <italic>F</italic> (1, 345) = 14.68, <italic>p</italic> = .000, η<sup>2</sup> = .04, while females were significantly more accurate with identifying rap/hip hop 250 ms, <italic>F</italic> (1, 345) = 10.82, <italic>p</italic> = .001, η<sup>2</sup> = .03.</p>
<fig id="fig3-0305735610391347" position="float">
<label>Figure 3.</label>
<caption>
<p>Two-way interaction effect of genre by gender</p>
</caption>
<graphic xlink:href="10.1177_0305735610391347-fig3.tif"/>
</fig>
<p>The purpose of the fifth research question was to determine whether or not preference played a role in participants’ ability to identify genre accurately. For each participant, a mean score was calculated from results of all excerpts not designated as being the most preferred genre and was labeled ‘other than most preferred.’ A paired-samples <italic>t</italic>-test was performed to determine whether a significant difference existed between participants’ most preferred genre scores and the mean scores of their other-than-most-preferred genres. From the results, participants’ identification of most preferred genre (<italic>M</italic> = 7.2, SD = 0.93) was significantly more accurate than identification of their other-than-most-preferred genres (<italic>M</italic> = 7.48, SD = 0.31), <italic>t</italic> (339) = 9.08 (two-tailed), <italic>p</italic> = .000, <italic>d</italic> = .49.</p>
<p>Similarly, a mean score was calculated from results of all excerpts not designated as being the least preferred genre and was labeled ‘other than least preferred.’ A paired-samples <italic>t</italic>-test was performed to determine whether a significant difference existed between participants’ least preferred genre scores and the mean scores of their other-than-least-preferred genres. From the results, participants’ identification of least preferred genre (<italic>M</italic> = 7.41, SD = 1.07) was found to be significantly less accurate than identification of their other-than-least-preferred genres (<italic>M</italic> = 7.61, SD = 0.79), <italic>t</italic> (341) = -3.72 (two-tailed), <italic>p</italic> = .000, <italic>d</italic> = .21.</p>
</sec>
<sec id="section6-0305735610391347" sec-type="discussion">
<title>Discussion</title>
<p>Anecdotally, listeners have an amazing ability to make very rapid judgments about the music they are hearing. For example, while scanning through radio stations, many listeners make choices upon hearing extremely brief snippets of music selections. Biologically, this ability can be accounted for as a function of the auditory brainstem response (<xref ref-type="bibr" rid="bibr15-0305735610391347">Hall, 2007</xref>). Previous researchers have demonstrated listeners can make judgments about the emotional quality of a musical excerpt that is as brief as 250 ms (<xref ref-type="bibr" rid="bibr6-0305735610391347">Bigand et al., 2005</xref>; <xref ref-type="bibr" rid="bibr34-0305735610391347">Peretz et al., 1998</xref>) or even 50 ms (<xref ref-type="bibr" rid="bibr3-0305735610391347">Ashley, 2008</xref>). Because only Gjerdingen and Perrot (2008) have investigated the rapid identification of genres from very brief excerpts, the purpose of the present study was to extend their study.</p>
<p>Participants in the present study were university students; both trained (<italic>n</italic> = 251) and untrained (<italic>n</italic> = 91). They listened to excerpts from classical (instrumental), jazz, country, metal and rap/hip hop genres. All five genres were represented by 10 excerpts each of 125, 250, 500, and 1000 ms presented in random and counterbalanced order for a total of 200 test items. Participants also indicated their most and least preferred genre in order to determine whether preference affected accuracy of judgment.</p>
<p>According to the results, participants were very successful at identifying genres of very brief excerpts. Overall, they were correct 76% of the time. At 1000 ms, accuracy reached 89% for all genres combined. Even at 125 ms, listeners performed well above chance: classical (70%), jazz (53%), country (56%), metal (51%), and rap/hip hop (40%). Clearly, these participants were able to make fairly accurate judgments on the basis of extremely limited information.</p>
<p>In general, the length of excerpt made a significant difference in performance, with longer time lengths expectedly leading to greater accuracy. Regarding genre differences, classical, metal and rap/hip hop were more easily identified than were country or jazz. Although it is difficult to provide concrete reasoning for the results, perhaps the amplified nature of metal and rap/hip hop produces uniquely associated timbres, while classical has a historically grounded and recognizable set of timbres. Jazz and country, on the other hand, include a substantially wider range of possible associated timbres. From the interaction effect of genre by length, it is evident that the accuracy of genre identification is distinct for each length being tested. Replication studies and/or data from additional participants are needed to determine whether the distinct results of the present study can be supported.</p>
<p>No overall significant difference was found between those with musical training and those without. Similarly, identification accuracy across excerpt length was not affected by training. However, identification accuracy of genre was found to be affected by musical training, with trained musicians significantly better able to identify classical and jazz excerpts of varying lengths and those untrained significantly better able to identify rap/hip hop excerpts of varying lengths. Perhaps familiarity might be at play such that trained musicians may be more likely to have encountered classical and jazz music while those untrained may be more likely to have encountered rap/hip hop music.</p>
<p>Many of the results for gender are similar to those regarding training. No overall significant difference was found between males and females. Similarly, identification accuracy across excerpt length was not affected by gender. However, identification accuracy of genre was found to be affected by gender, with males significantly better able to identify jazz 250 ms and metal 125 ms excerpts and females significantly better able to identify rap/hip hop 250 ms excerpts. In general, fewer significant differences occur as a result of gender than of training. Those few gender differences found in the present study were somewhat unexpected and will require additional research to substantiate or refute.</p>
<p>Preference was found to have played a role in participants’ ability to identify genre of very brief excerpts accurately. Participants’ most preferred genre was identified significantly more often than all other genres; and their least preferred genre was identified significantly less often than all other genres. Perhaps such a result was due to familiarity.</p>
<p>These findings support a primary conclusion that people are very adept at identifying particular music genres when presented with excerpts that are 1 second in length or less. The literature on auditory brainstem response provides a biological explanation for how this is possible. In previous studies, researchers found listeners can make emotional judgments very rapidly and the current results extend this ability to the identification of specific genres. Based on <xref ref-type="bibr" rid="bibr3-0305735610391347">Ashley’s (2008)</xref> findings that listeners can make judgments of valence and mode as rapidly as 100 ms, and in some cases in 50 ms, it is tempting to speculate that genre identification takes slightly longer. That is, perhaps one can first make judgments of emotional valence and then move on to more discrete categorizations. Perhaps, too, valence judgments are made via the fast path analysis and genre identification is handled via the slower pathway (<xref ref-type="bibr" rid="bibr23-0305735610391347">LeDoux, 1995</xref>, <xref ref-type="bibr" rid="bibr24-0305735610391347">1998</xref>). However, further studies are needed to test these hypotheses.</p>
<p>In general, the results of the present study support and extend the findings of <xref ref-type="bibr" rid="bibr13-0305735610391347">Gjerdingen and Perrott (2008)</xref>. In both studies, listeners were able to recognize the genre of very brief excerpts. In Gjerdingen and Perrott, the shortest excerpts were 250 ms, while in the current study they were 125 ms. Gjerdingen and Perrott found no differences due to gender or training, but did not examine either variable fully. In the current study minor differences were found due to gender and to training. Gjerdingen and Perrot did not investigate the role of preference; the current study found that preference did play a significant role.</p>
<p>One way of accounting for the rapid genre identification abilities of the participants in the present study, and for the roles of gender, training and preference, may be found in the Preference for Prototypes Theory (PPT). First proposed by <xref ref-type="bibr" rid="bibr48-0305735610391347">Whitfield and Slatter (1979)</xref> and further developed by <xref ref-type="bibr" rid="bibr28-0305735610391347">Martindale and Moore (1988)</xref>, PPT arises out of work in cognitive psychology (e.g., <xref ref-type="bibr" rid="bibr27-0305735610391347">Martindale, 1984</xref>; <xref ref-type="bibr" rid="bibr37-0305735610391347">Posner &amp; Keele, 1968</xref>). The basic idea is that people classify objects by matching incoming information with previously formed schema or prototypes. Preferred categories are represented in more well-defined, discrete prototypes than non-preferred categories. For example, over time, listeners place different features of the music they hear into categories that gradually form recognizable prototypes. For a person raised in a home where country music is frequently heard, musical features such as the twang of a steel guitar or particular vocal qualities become associated with the label <italic>country music</italic>.</p>
<p><xref ref-type="bibr" rid="bibr38-0305735610391347">Reber, Schwartz, and Winkielman (2004)</xref> proposed that the more fluently one can process aspects of a contemplated object (e.g., a painting or a musical experience), the more positive their aesthetic response. This underlying cause has been demonstrated with literature (<xref ref-type="bibr" rid="bibr36-0305735610391347">Piters &amp; Stokmans, 2000</xref>), paintings (<xref ref-type="bibr" rid="bibr12-0305735610391347">Farkas, 2002</xref>; <xref ref-type="bibr" rid="bibr18-0305735610391347">Hekkert &amp; Wieringen, 1990</xref>), and furniture (<xref ref-type="bibr" rid="bibr48-0305735610391347">Whitfield &amp; Slatter, 1979</xref>). <xref ref-type="bibr" rid="bibr43-0305735610391347">Smith and Melara (1990)</xref> and <xref ref-type="bibr" rid="bibr41-0305735610391347">Repp (1997)</xref> confirmed the effect of processing fluency on musical choices.</p>
<p>PPT is not universally supported (e.g., see <xref ref-type="bibr" rid="bibr7-0305735610391347">Boselie, 1991</xref>), however, others (<xref ref-type="bibr" rid="bibr17-0305735610391347">Hekkert &amp; Snelders, 1995</xref>; <xref ref-type="bibr" rid="bibr29-0305735610391347">Martindale, Moore, &amp; West, 1988</xref>; <xref ref-type="bibr" rid="bibr32-0305735610391347">North &amp; Hargreaves, 2000</xref>; <xref ref-type="bibr" rid="bibr45-0305735610391347">Whitfield, 1983</xref>, <xref ref-type="bibr" rid="bibr46-0305735610391347">2000</xref>, <xref ref-type="bibr" rid="bibr47-0305735610391347">2009</xref>) argue that it is a viable theory and is compatible with the dominant arousal-mediating theory of <xref ref-type="bibr" rid="bibr4-0305735610391347">Berlyne (1971</xref>, <xref ref-type="bibr" rid="bibr5-0305735610391347">1974</xref>). Moreover, PPT does fit data from the current study rather nicely. First, there is an evolutionary basis for PPT; it would have been greatly to our ancestors’ advantage to identify prey or predators rapidly. Early on, we placed the shape, color, sounds, and so on, of each animal encountered into various categories. Matching a novel stimulus such as a growl or a chirp to preformed categories allowed for faster identification.</p>
<p>Second, PPT can account for gender, training, and preference. That is, it is reasonable to hypothesize that males and females, trained and untrained listeners, and those who prefer a particular genre will hear certain sounds more frequently than others and will thus have developed more crystallized categories for certain sounds than for others. For example, a highly trained female violinist will have developed stronger schema for classical music than an untrained male who listens to a great deal of rap. If both are presented with a brief burst of orchestral music, he might rapidly decide ‘not rap’, without being able to place it into the classical music genre as quickly as she could, because it fits her preferred prototype. Of course, there is an extensive literature on music preferences that more fully explicates how we make musical choices (e.g., <xref ref-type="bibr" rid="bibr1-0305735610391347">Abeles &amp; Chung, 1996</xref>; <xref ref-type="bibr" rid="bibr16-0305735610391347">Hargreaves, North, &amp; Tarrant, 2006</xref>; <xref ref-type="bibr" rid="bibr22-0305735610391347">Lamont, 2009</xref>; <xref ref-type="bibr" rid="bibr33-0305735610391347">North &amp; Hargreaves, 2008</xref>; <xref ref-type="bibr" rid="bibr39-0305735610391347">Rentfrow &amp; Gosling, 2003</xref>, <xref ref-type="bibr" rid="bibr40-0305735610391347">2006</xref>).</p>
<p>While administering the Genre Identification Test, it was remarkable that many participants were observed whispering the name of the artist or even the titles of some of the examples. Both musically trained and untrained listeners identified specific titles and artists. For instance, one young man wrote on his paper the decade that a metal artist recorded a particular piece. Another looked up from his test form and named the composition, adding, ‘Nice choice!’ Several participants identified rap/hip hop artists, and after one classroom administration, participants asked whether specific artists and songs were included in the examples. Indeed, all artists and titles the participants questioned had been included. This shows an amazing capacity for individuals to identify not only music genre, but also artist and composition in a very short period of time.</p>
<p>Many participants were quite interested in knowing how well they did on the test, and were disappointed that they would not be able to discern their individual scores. Frequently, participants commented about the length of the test, with about half saying they tired toward the end and the other half reporting that they felt they had improved at identification as they progressed through the 200 items.</p>
<p>For the current study, vocal pieces were eliminated from the genres of classical and jazz. To determine the effect of vocals on genre recognition, a future study could be designed to determine accuracy in identification of vocal music, including genres such as opera, country, metal and musical theater (Broadway). Another focus for such studies could be that of specific performance areas to determine accuracy in identification of specific compositions with a high degree of familiarity. For example, one might choose standard piano compositions from which to create a similar stimulus recording for participants who are accomplished pianists. Participants could be asked to identify titles of specific pieces from brief music examples. Numerous additional studies could be designed to explore this fascinating and quite extraordinary ability to make rapid musical judgments based on extremely brief snippets of music. The results of such a line of research would have both practical applications, as in the burgeoning area of Music Information Retrieval (<xref ref-type="bibr" rid="bibr11-0305735610391347">Downie, 2008</xref>), and would also lead to a deeper understanding of human musical processing.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="bibr1-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Abeles</surname><given-names>H.</given-names></name>
<name><surname>Chung</surname><given-names>J.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Responses to music</article-title>. In <person-group person-group-type="editor">
<name><surname>Hodges</surname><given-names>D.</given-names></name>
</person-group> (Ed.), <source>Handbook of music psychology</source> (<edition>2nd ed.</edition>; pp. <fpage>285</fpage>–<lpage>342</lpage>). <publisher-loc>San Antonio, TX</publisher-loc>: <publisher-name>IMR Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0305735610391347">
<citation citation-type="web">
<collab>Alloy Media and Marketing</collab>. (<year>2009</year>). <article-title>Totally wired campus – the class of 2013 gets high ‘tech’ grades: Alloy media and marketing</article-title>. <access-date>Retrieved 25 April 2010</access-date>, <comment>from <ext-link ext-link-type="uri" xlink:href="http://74.125.45.132/search?q=cache:KLk-zD8g5HgJ:www.alloymarketing.com/investor_relations/news_releases/doc/Alloy-Harris-CE-TECH11-1209FINALPR.doc+The+2009+Alloy+College+Explorer+study&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-a">http://74.125.45.132/search?q=cache:KLk-zD8g5HgJ:www.alloymarketing.com/investor_relations/news_releases/doc/Alloy-Harris-CE-TECH11-1209FINALPR.doc+The+2009+Alloy+College+Explorer+study&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us&amp;client=firefox-a</ext-link></comment></citation>
</ref>
<ref id="bibr3-0305735610391347">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Ashley</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <source>Affective and perceptual responses to very brief musical stimuli</source>. <conf-name>Paper presented at the International Conference on Music Perception and Cognition</conf-name>, <conf-date>25–29 August</conf-date>, <conf-loc>Sapporo, Japan</conf-loc>.</citation>
</ref>
<ref id="bibr4-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Berlyne</surname><given-names>D.</given-names></name>
</person-group> (<year>1971</year>). <source>Aesthetics and psychobiology</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Appleton-Century-Crofts</publisher-name>.</citation>
</ref>
<ref id="bibr5-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Berlyne</surname><given-names>D.</given-names></name>
</person-group> (<year>1974</year>). <source>Studies in the new experimental aesthetics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr6-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bigand</surname><given-names>E.</given-names></name>
<name><surname>Filipic</surname><given-names>S.</given-names></name>
<name><surname>Lalitte</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The time course of emotional responses to music</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>1060</volume>, <fpage>429</fpage>–<lpage>437</lpage>.</citation>
</ref>
<ref id="bibr7-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boselie</surname><given-names>F.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Against prototypicality as a central concept in aesthetics</article-title>. <source>Empirical Studies of the Arts</source>, <volume>9</volume>(<issue>1</issue>), <fpage>65</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr8-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>De Santis</surname><given-names>L.</given-names></name>
<name><surname>Clarke</surname><given-names>S.</given-names></name>
<name><surname>Murray</surname><given-names>M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Automatic and intrinsic auditory ‘what’ and ‘where’ processing in humans revealed by electrical neuroimaging</article-title>. <source>Cerebral Cortex</source>, <volume>17</volume>, <fpage>9</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr9-0305735610391347">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Diethe</surname><given-names>T.</given-names></name>
<name><surname>Teodoru</surname><given-names>G.</given-names></name>
<name><surname>Furl</surname><given-names>N.</given-names></name>
<name><surname>Shawe-Taylor</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Sparse multiview methods for classification of musical genre from magnetoencephalography recordings</article-title>. In <person-group person-group-type="editor">
<name><surname>Louhivuori</surname><given-names>J.</given-names></name>
<name><surname>Eerola</surname><given-names>T.</given-names></name>
<name><surname>Saarikallio</surname><given-names>S.</given-names></name>
<name><surname>Himberg</surname><given-names>T.</given-names></name>
<name><surname>Eerola</surname><given-names>P.</given-names></name>
</person-group> (Eds.), <conf-name>Proceedings of the 7th Triennial Conference of the European Society for the Cognition of Music</conf-name>, <conf-loc>Jyväskylä, Finland</conf-loc>.</citation>
</ref>
<ref id="bibr10-0305735610391347">
<citation citation-type="web">
<collab>Digital music increases share of overall music sales volume in the US</collab>. (<comment>n.d.</comment>). <access-date>Retrieved 16 August 2010</access-date>, <comment>from <ext-link ext-link-type="uri" xlink:href="http://www.npd.com/press/releases/press_090818.html">http://www.npd.com/press/releases/press_090818.html</ext-link></comment></citation>
</ref>
<ref id="bibr11-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Downie</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The music information retrieval evaluation exchange (2005–2007): A window into music information retrieval research</article-title>. <source>Acoustical Science and Technology</source>, <volume>29</volume>(<issue>4</issue>), <fpage>247</fpage>–<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr12-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Farkas</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Prototypicality-effect in surrealist paintings</article-title>. <source>Empirical Studies of the Arts</source>, <volume>20</volume>, <fpage>127</fpage>–<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr13-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gjerdingen</surname><given-names>R.</given-names></name>
<name><surname>Perrott</surname><given-names>D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Scanning the dial: The rapid recognition of music genres</article-title>. <source>Journal of New Music Research</source>, <volume>37</volume>(<issue>2</issue>), <fpage>93</fpage>–<lpage>100</lpage>.</citation>
</ref>
<ref id="bibr14-0305735610391347">
<citation citation-type="book">
<collab>Gravic</collab>. (<year>2009</year>). <source>Remark Classic OMR: Version 3.0</source>. <publisher-loc>Malvern, PA</publisher-loc>. <publisher-name>Gravic Inc</publisher-name>.</citation>
</ref>
<ref id="bibr15-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hall</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <source>New handbook of auditory evoked responses</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Pearson Education</publisher-name>.</citation>
</ref>
<ref id="bibr16-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hargreaves</surname><given-names>D.</given-names></name>
<name><surname>North</surname><given-names>A.</given-names></name>
<name><surname>Tarrant</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Musical preference and taste in childhood and adolescence</article-title>. In <person-group person-group-type="editor">
<name><surname>McPherson</surname><given-names>G.</given-names></name>
</person-group> (Ed.), <source>The child as musician</source>, (pp. <fpage>135</fpage>–<lpage>154</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hekkert</surname><given-names>P.</given-names></name>
<name><surname>Snelders</surname><given-names>H.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Prototypicality as an explanatory concept in aesthetics: A reply to Boselie (1991)</article-title>. <source>Empirical Studies of the Arts</source>, <volume>13</volume>(<issue>2</issue>), <fpage>149</fpage>–<lpage>160</lpage>.</citation>
</ref>
<ref id="bibr18-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hekkert</surname><given-names>P.</given-names></name>
<name><surname>Wieringen</surname><given-names>P.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Complexity and prototypicality as determinants of appraisal of cubist paintings</article-title>. <source>British Journal of Psychology</source>, <volume>81</volume>, <fpage>483</fpage>–<lpage>495</lpage>.</citation>
</ref>
<ref id="bibr19-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Howard</surname><given-names>M.</given-names></name>
<name><surname>Volkov</surname><given-names>I.</given-names></name>
<name><surname>Mirsky</surname><given-names>R.</given-names></name>
<name><surname>Garell</surname><given-names>P.</given-names></name>
<name><surname>Noh</surname><given-names>M.</given-names></name>
<name><surname>Granner</surname><given-names>M.</given-names></name><etal/>
</person-group>. (<year>2000</year>). <article-title>Auditory cortex on the human posterior superior temporal gyrus</article-title>. <source>The Journal of Comparative Neurology</source>, <volume>416</volume>, <fpage>79</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr20-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Huron</surname><given-names>D.</given-names></name>
</person-group> (<year>2006</year>). <source>Sweet anticipation: Music and the psychology of expectation</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>The MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr21-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kofler</surname><given-names>M.</given-names></name>
<name><surname>Müller</surname><given-names>J.</given-names></name>
<name><surname>Wenning</surname><given-names>G.</given-names></name>
<name><surname>Reggiani</surname><given-names>L.</given-names></name>
<name><surname>Hollosi</surname><given-names>P.</given-names></name>
<name><surname>Bösch</surname><given-names>S.</given-names></name><etal/>
</person-group>. (<year>2001</year>). <article-title>The auditory startle reaction in Parkinsonian disorders</article-title>. <source>Movement Disorders</source>, <volume>16</volume>(<issue>1</issue>), <fpage>62</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr22-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lamont</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Musical preferences</article-title>. In <person-group person-group-type="editor">
<name><surname>Hallam</surname><given-names>S.</given-names></name>
<name><surname>Cross</surname><given-names>I.</given-names></name>
<name><surname>Thaut</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <source>The Oxford handbook of music psychology</source> (pp. <fpage>160</fpage>–<lpage>168</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr23-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>LeDoux</surname><given-names>J.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Emotion: Clues from the brain</article-title>. <source>Annual Review of Psychology</source>, <volume>46</volume>, <fpage>209</fpage>–<lpage>235</lpage>.</citation>
</ref>
<ref id="bibr24-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>LeDoux</surname><given-names>J.</given-names></name>
</person-group> (<year>1998</year>). <article-title>The emotional brain</article-title>. In <person-group person-group-type="editor">
<name><surname>Jenkins</surname><given-names>J.</given-names></name>
<name><surname>Oatley</surname><given-names>K.</given-names></name>
<name><surname>Stein</surname><given-names>N.</given-names></name>
</person-group> (Eds.), <source>Human emotions: A Reader</source> (pp. <fpage>98</fpage>–<lpage>111</lpage>. <publisher-loc>Malden, MA</publisher-loc>: <publisher-name>Blackwell</publisher-name>.</citation>
</ref>
<ref id="bibr25-0305735610391347">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>K.</given-names></name>
<name><surname>Skoe</surname><given-names>E.</given-names></name>
<name><surname>Kraus</surname><given-names>N.</given-names></name>
<name><surname>Ashley</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <source>The effect of musical training on the subcortical processing of musical intervals</source>. <conf-name>Paper presented at the International Conference on Music Perception and Cognition</conf-name>, <conf-date>25–29 August</conf-date>, <conf-loc>Sapporo, Japan</conf-loc>.</citation>
</ref>
<ref id="bibr26-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Stutzmann</surname><given-names>G.</given-names></name>
<name><surname>LeDoux</surname><given-names>J.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Convergent but temporally separated inputs to lateral amygdala neurons from the auditory thalamus and auditory cortex use different postsynaptic receptors: In vivo intracellular and extracellular recordings in fear conditioning pathways</article-title>. <source>Learning and Memory</source>, <volume>3</volume>: <fpage>229</fpage>–<lpage>242</lpage>.</citation>
</ref>
<ref id="bibr27-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Martindale</surname><given-names>C.</given-names></name>
</person-group> (<year>1984</year>). <article-title>The pleasures of thought: A theory of cognitive hedonics</article-title>. <source>Journal of Mind and Behavior</source>, <volume>5</volume>(<issue>1</issue>), <fpage>49</fpage>–<lpage>80</lpage>.</citation>
</ref>
<ref id="bibr28-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Martindale</surname><given-names>C.</given-names></name>
<name><surname>Moore</surname><given-names>K.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Priming, prototypicality, and preference</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>14</volume>, <fpage>661</fpage>–<lpage>670</lpage>.</citation>
</ref>
<ref id="bibr29-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Martindale</surname><given-names>C.</given-names></name>
<name><surname>Moore</surname><given-names>K.</given-names></name>
<name><surname>West</surname><given-names>A.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Relationship of preference judgments to typicality, novelty, and mere exposure</article-title>. <source>Empirical Studies of the Arts</source>, <volume>6</volume>(<issue>1</issue>), <fpage>79</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr30-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Müller</surname><given-names>J.</given-names></name>
<name><surname>Kofler</surname><given-names>M.</given-names></name>
<name><surname>Wenning</surname><given-names>G.</given-names></name>
<name><surname>Seppi</surname><given-names>K.</given-names></name>
<name><surname>Valls-Solé</surname><given-names>J.</given-names></name>
<name><surname>Poewe</surname><given-names>W.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Auditory startle response in cervical dystonia</article-title>. <source>Movement Disorders</source>, <volume>18</volume>(<issue>12</issue>), <fpage>1522</fpage>–<lpage>1526</lpage>.</citation>
</ref>
<ref id="bibr31-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Murray</surname><given-names>M.</given-names></name>
<name><surname>Molholm</surname><given-names>S.</given-names></name>
<name><surname>Michel</surname><given-names>C.</given-names></name>
<name><surname>Heslenfield</surname><given-names>D.</given-names></name>
<name><surname>Ritter</surname><given-names>W.</given-names></name>
<name><surname>Javitt</surname><given-names>D.</given-names></name><etal/>
</person-group>. (<year>2005</year>). <article-title>Grabbing your ear: Rapid auditory-somatosensory multisensory interactions in low-level sensory cortices are not constrained by stimulus alignment</article-title>. <source>Cerebral Cortex</source>, <volume>15</volume>, <fpage>963</fpage>–<lpage>974</lpage>.</citation>
</ref>
<ref id="bibr32-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>North</surname><given-names>A.</given-names></name>
<name><surname>Hargreaves</surname><given-names>D.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Collative variables versus prototypicality</article-title>. <source>Empirical Studies of the Arts</source>, <volume>8</volume>(<issue>1</issue>), <fpage>13</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr33-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>North</surname><given-names>A.</given-names></name>
<name><surname>Hargreaves</surname><given-names>D.</given-names></name>
</person-group> (<year>2008</year>). <source>The social and applied psychology of music</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr34-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Peretz</surname><given-names>I.</given-names></name>
<name><surname>Gagnon</surname><given-names>L.</given-names></name>
<name><surname>Bouchard</surname><given-names>B.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Music and emotion: Perceptual determinants, immediacy, and isolation after brain damage</article-title>. <source>Cognition</source>, <volume>68</volume>, <fpage>111</fpage>–<lpage>141</lpage>.</citation>
</ref>
<ref id="bibr35-0305735610391347">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Perrott</surname><given-names>D.</given-names></name>
<name><surname>Gjerdingen</surname><given-names>R.</given-names></name>
</person-group> (<year>1999</year>). <source>Scanning the dial: An exploration of factors in the identification of musical style</source>. <conf-name>Paper presented at the meeting of the Society for Music Perception and Cognition</conf-name>, <conf-date>15 August</conf-date>, <conf-loc>Evanston, IL</conf-loc>.</citation>
</ref>
<ref id="bibr36-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Piters</surname><given-names>R.</given-names></name>
<name><surname>Stokmans</surname><given-names>M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Genre categorization and its effect on preference for fiction books</article-title>. <source>Empirical Studies of the Arts</source>, <volume>18</volume>, <fpage>159</fpage>–<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr37-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Posner</surname><given-names>M.</given-names></name>
<name><surname>Keele</surname><given-names>S.</given-names></name>
</person-group> (<year>1968</year>). <article-title>On the genesis of abstract ideas</article-title>. <source>Journal of Experimental Psychology</source>, <volume>77</volume>, <fpage>353</fpage>–<lpage>363</lpage>.</citation>
</ref>
<ref id="bibr38-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reber</surname><given-names>R.</given-names></name>
<name><surname>Schwartz</surname><given-names>N.</given-names></name>
<name><surname>Winkielman</surname><given-names>P.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Processing fluency and aesthetic pleasure: Is beauty in the perceiver’s processing experience?</article-title> <source>Personality and Social Psychology Review</source>, <volume>8</volume>, <fpage>364</fpage>–<lpage>382</lpage>.</citation>
</ref>
<ref id="bibr39-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rentfrow</surname><given-names>P.</given-names></name>
<name><surname>Gosling</surname><given-names>S.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The do re mi’s of everyday life: The structure and personality correlates of music preferences</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>84</volume>(<issue>6</issue>), <fpage>11236</fpage>–<lpage>1256</lpage>.</citation>
</ref>
<ref id="bibr40-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rentfrow</surname><given-names>P.</given-names></name>
<name><surname>Gosling</surname><given-names>S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Message in a bottle: The role of music preferences in interpersonal perception</article-title>. <source>Psychological Science</source>, <volume>17</volume>(<issue>3</issue>), <fpage>236</fpage>–<lpage>242</lpage>.</citation>
</ref>
<ref id="bibr41-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Repp</surname><given-names>B.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The aesthetic quality of a quantitatively average music performance: Two preliminary experiments</article-title>. <source>Music Perception</source>, <volume>14</volume>, <fpage>419</fpage>–<lpage>444</lpage>.</citation>
</ref>
<ref id="bibr42-0305735610391347">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sloboda</surname><given-names>J.</given-names></name>
<name><surname>Juslin</surname><given-names>P.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Psychological perspectives on music and emotion</article-title>. In <person-group person-group-type="editor">
<name><surname>Juslin</surname><given-names>P.</given-names></name>
<name><surname>Sloboda</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Music and emotion: Theory and research</source>, (pp. <fpage>71</fpage>–<lpage>104</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr43-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Smith</surname><given-names>J.</given-names></name>
<name><surname>Melara</surname><given-names>R.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Aesthetic preference and syntactic prototypicality in music: ‘Tis the gift to be simple</article-title>. <source>Cognition</source>, <volume>34</volume>, <fpage>279</fpage>–<lpage>298</lpage>.</citation>
</ref>
<ref id="bibr44-0305735610391347">
<citation citation-type="book">
<collab>Sony Creative Software</collab>. (<year>2008</year>). <source>Sound Forge 8</source>. <publisher-loc>Madison, WI</publisher-loc>. <publisher-name>Sony Creative Software Inc</publisher-name>.</citation>
</ref>
<ref id="bibr45-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Whitfield</surname><given-names>T.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Predicting preference for familiar, everyday objects: An experimental confrontation between two theories of aesthetic behaviour</article-title>. <source>Journal of Experimental Psychology</source>, <volume>3</volume>, <fpage>221</fpage>–<lpage>237</lpage>.</citation>
</ref>
<ref id="bibr46-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Whitfield</surname><given-names>T.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Beyond prototypicality: Toward a categorical-motivation model of aesthetics</article-title>. <source>Empirical Studies of the Arts</source>, <volume>18</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr47-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Whitfield</surname><given-names>T.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Theory confrontation: Testing the categorical-motivation model</article-title>. <source>Empirical Studies of the Arts</source>, <volume>27</volume>(<issue>1</issue>), <fpage>43</fpage>–<lpage>59</lpage>.</citation>
</ref>
<ref id="bibr48-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Whitfield</surname><given-names>T.</given-names></name>
<name><surname>Slatter</surname><given-names>P.</given-names></name>
</person-group> (<year>1979</year>). <article-title>The effect of categorization and prototypicality on aesthetic choice in a furniture selection task</article-title>. <source>British Journal of Psychology</source>, <volume>70</volume>, <fpage>65</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr49-0305735610391347">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yeomans</surname><given-names>J.</given-names></name>
<name><surname>Frankland</surname><given-names>P.</given-names></name>
</person-group> (<year>1996</year>). <article-title>The acoustic startle reflex: Neurons and connections</article-title>. <source>Brain Research Reviews</source>, <volume>21</volume>(<issue>3</issue>), <fpage>301</fpage>–<lpage>314</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>