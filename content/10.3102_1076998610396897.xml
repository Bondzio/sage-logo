<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JEB</journal-id>
<journal-id journal-id-type="hwp">spjeb</journal-id>
<journal-title>Journal of Educational and Behavioral Statistics</journal-title>
<issn pub-type="ppub">1076-9986</issn>
<issn pub-type="epub">1935-1054</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/1076998610396897</article-id>
<article-id pub-id-type="publisher-id">10.3102_1076998610396897</article-id>
<title-group>
<article-title>Robust Means Modeling</article-title>
<subtitle>An Alternative for Hypothesis Testing of Independent Means Under Variance Heterogeneity and Nonnormality</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fan</surname>
<given-names>Weihua</given-names>
</name>
<aff id="aff1-1076998610396897">University of Houston</aff>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hancock</surname>
<given-names>Gregory R.</given-names>
</name>
<aff id="aff2-1076998610396897">University of Maryland, College Park</aff>
</contrib>
</contrib-group>
<author-notes>
<fn fn-type="other" id="fn1-1076998610396897">
<p>WEIHUA FAN is an assistant professor in the Department of Educational Psychology at University of Houston, 423 Farish Hall, Houston, TX 77204; e-mail: <email>wfan2@uh.edu</email>. Her research focuses on advanced quantitative research methods and their applications to educational and behavioral issues.</p>
</fn>
<fn fn-type="other" id="fn2-1076998610396897">
<p>GREGORY R. HANCOCK is Professor and Chair, Department of Measurement, Statistics and Evaluation, 1230 Benjamin Building, University of Maryland, College Park, MD 20742-1115; e-mail: <email>ghancock@umd.edu</email>. His research interests are latent variable models, including those modeling mean and growth structures.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>37</volume>
<issue>1</issue>
<fpage>137</fpage>
<lpage>156</lpage>
<history>
<date date-type="received">
<day>20</day>
<month>1</month>
<year>2010</year>
</date>
<date date-type="rev-recd">
<day>23</day>
<month>6</month>
<year>2010</year>
</date>
<date date-type="accepted">
<day>6</day>
<month>11</month>
<year>2010</year>
</date>
</history>
<permissions>
<copyright-statement>© American Educational Research Association 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>This study proposes robust means modeling (RMM) approaches for hypothesis testing of mean differences for between-subjects designs in order to control the biasing effects of nonnormality and variance inequality. Drawing from structural equation modeling (SEM), the RMM approaches make no assumption of variance homogeneity and employ robust estimation/rescaling strategies in order to alleviate reliance on normality. A Monte Carlo simulation is conducted to compare the Type I error rate and the power of the proposed six RMM test statistics to five analysis of variance (ANOVA)-based statistics, the latter of which have also employed trimmed means and Winsorized variances to enhance robustness. Various simulation factors manipulated include variance inequality, sample-size pairings with group variances, degree of nonnormality, alpha level for hypothesis tests, and effect size. Results show that the proposed RMM methods are indeed superior to the traditional ANOVA-based methods.</p>
</abstract>
<kwd-group>
<kwd>robust means modeling</kwd>
<kwd>hypothesis testing of mean equality</kwd>
<kwd>nonnormality</kwd>
<kwd>variance heterogeneity</kwd>
<kwd>Welch statistics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The traditional between-subjects analysis of variance (ANOVA) <italic>F</italic> test and Student’s <italic>t</italic> test rest on assumptions of population normality and homogeneity of variance, both routinely violated in applied research in the social and behavioral sciences. As indicated in a considerable volume of methodological literature, (a) when a larger variance is associated with a larger sample size, the actual Type I error declines below the nominal α level (known as a <italic>positive condition</italic>); (b) when a larger variance is associated with a smaller sample size, the actual Type I error increases above the nominal α level (known as a <italic>negative condition</italic>; e.g., <xref ref-type="bibr" rid="bibr15-1076998610396897">Glass, Peckham, &amp; Sanders, 1972</xref>; <xref ref-type="bibr" rid="bibr29-1076998610396897">Overall, Atlas, &amp; Gibson, 1995</xref>); and (c) when sample sizes are equal, the Type I error rate is still affected when variances are heterogeneous and data are nonnormal (e.g., <xref ref-type="bibr" rid="bibr8-1076998610396897">Brown &amp; Forsythe, 1974</xref>; <xref ref-type="bibr" rid="bibr18-1076998610396897">Hsiung &amp; Olejnik, 1996</xref>; <xref ref-type="bibr" rid="bibr36-1076998610396897">Tomarken &amp; Serlin, 1986</xref>).</p>
<p>In response to the challenges associated with the traditional <italic>F</italic> test, applied researchers may choose to use an alternative test procedure; four particular options appear to be the most prevalent in controlling Type I error as well as providing competitive power under varying variance heterogeneous conditions. First, the Welch test (<xref ref-type="bibr" rid="bibr37-1076998610396897">Welch, 1951</xref>), which keeps variances unpooled and adjusts the degrees of freedom (<italic>df</italic>), has been included in almost all studies investigating alternatives to the ANOVA <italic>F</italic> test (e.g., <xref ref-type="bibr" rid="bibr2-1076998610396897">Algina, Oshima, &amp; Lin, 1994</xref>; <xref ref-type="bibr" rid="bibr11-1076998610396897">Clinch &amp; Keselman, 1982</xref>; <xref ref-type="bibr" rid="bibr24-1076998610396897">Lix, Keselman, &amp; Keselman, 1996</xref>; <xref ref-type="bibr" rid="bibr28-1076998610396897">Oshima &amp; Algina, 1992</xref>; <xref ref-type="bibr" rid="bibr40-1076998610396897">Wilcox, Charlin, &amp; Thompson, 1986</xref>). The literature indicates that this test is relatively robust to departures from variance heterogeneity; if sample size is small, however, the ability of Welch’s test to control the Type I error rate to the nominal α level may become compromised as variance heterogeneity increases or as the number of groups increases (<xref ref-type="bibr" rid="bibr39-1076998610396897">Wilcox, 1988</xref>).</p>
<p>A second option is the Brown–Forsythe test (<xref ref-type="bibr" rid="bibr8-1076998610396897">Brown &amp; Forsythe, 1974</xref>), which transforms scores to their absolute deviations from the median; this method has also been shown to provide reasonably good control over the Type I error rate under many conditions. A number of studies, however, have shown that in some situations the procedure is not quite comparable to the Welch test (or to the James second-order test; see below), depending upon the pattern of the population variances (e.g., <xref ref-type="bibr" rid="bibr36-1076998610396897">Tomarken &amp; Serlin, 1986</xref>; <xref ref-type="bibr" rid="bibr40-1076998610396897">Wilcox et al., 1986</xref>).</p>
<p>Third, the James second-order test (<xref ref-type="bibr" rid="bibr20-1076998610396897">James, 1951</xref>), which weights sample means using inverse sample variances to determine a replacement for the grand mean, has been recommended by several studies (e.g., <xref ref-type="bibr" rid="bibr24-1076998610396897">Lix et al., 1996</xref>; <xref ref-type="bibr" rid="bibr28-1076998610396897">Oshima &amp; Algina, 1992</xref>; <xref ref-type="bibr" rid="bibr39-1076998610396897">Wilcox, 1988</xref>); however, these studies have also warned that this approach may not always control the Type I error rate when sample size is small and distributions are asymmetric nonnormal. Fourth and finally is the Alexander–Govern approximation (<xref ref-type="bibr" rid="bibr1-1076998610396897">Alexander &amp; Govern, 1994</xref>), which applies a normalizing transformation to the James procedure. This has been recommended by, for example, <xref ref-type="bibr" rid="bibr34-1076998610396897">Schneider and Penfield (1997)</xref>, as the best alternative to the ANOVA <italic>F</italic> test when variances are heterogeneous due to its simplicity of computation and its relative superiority when considering both Type I error and power under most experimental conditions. Given that this is a more recent approach, however, it has not been studied nearly as extensively as other more established methods.</p>
<p>Overall, the evidence regarding these alternative statistics suggests that they can generally control the rate of Type I error when population variances are heterogeneous, as long as there is sufficient sample size and normally distributed data. The literature also indicates, however, that these tests can become liberal when the data are both heterogeneous and nonnormal, particularly when the design is unbalanced. Studies of statistical robustness of these alternatives suggest that no one approach is best in all situations and the major limitation of these statistics is their sensitivity to the nature of the population distributions.</p>
<p>To further reduce the biasing effects of nonnormality, some researchers (see, e.g., <xref ref-type="bibr" rid="bibr23-1076998610396897">Lix &amp; Keselman, 1998</xref>) have proposed to use the trimmed means and Winsorized variances rather than the usual least squares statistics when applying alternative approaches, in particular Welch and Welch derivatives. Studies of such approaches in independent sample designs have shown that incorporating trimmed means and Winsorized variances helps the test statistics to exact better control of Type I error better under nonnormality and variance heterogeneity (see, e.g., <xref ref-type="bibr" rid="bibr21-1076998610396897">Keselman, Algina, Wilcox, &amp; Kowalchuk, 2000</xref>; <xref ref-type="bibr" rid="bibr22-1076998610396897">Keselman, Kowalchuk, &amp; Lix, 1998</xref>; <xref ref-type="bibr" rid="bibr41-1076998610396897">Wilcox, Keselman, Muska, &amp; Cribbie, 2000</xref>). That said, this method is usually criticized because (a) it alters the test hypothesis of equality of means to equality of trimmed means after removing the extreme observations, and (b) the Type I error rate and power rate are still not satisfactory with a moderate to large degree of nonnormality.</p>
<sec id="section2-1076998610396897">
<title>A Different Approach</title>
<p>Although considerable efforts have been made to address the violations of the assumptions of variance homogeneity and normality, as summarized briefly above, perhaps a more desirable approach would be one that makes neither assumption in the first place. Several properties of a modern modeling technique known as <italic>structured means modeling</italic> (SMM; <xref ref-type="bibr" rid="bibr35-1076998610396897">Sörbom, 1974</xref>), developed from structural equation modeling (SEM), make it possible. First, Sörbom’s general approach combines ANOVA’s goals with confirmatory factor analysis, leading to tests of mean equality at the latent variable level (see, e.g., <xref ref-type="bibr" rid="bibr16-1076998610396897">Hancock, 2003</xref>, <xref ref-type="bibr" rid="bibr17-1076998610396897">2004</xref>). A special case of this general model can be easily derived to test mean equality on an observed variable. Second, the model makes no assumptions about homogeneity of variance; that is, the SMM framework allows variances to be heterogeneous across populations by never constraining them in the first place. Third, many robust estimation approaches have been proposed in the SEM/SMM literature in order to accommodate nonnormal distributions. As such, the potential exists for a framework of mean equality testing that is free from the assumptions of variance homogeneity and normality.</p>
<p>The current investigation, which focuses on independent groups designs, will proceed in two parts. First, the methodology referred to herein as <italic>robust means modeling</italic> (RMM) is formalized, drawing from the principles of SEM and SMM and incorporating <xref ref-type="bibr" rid="bibr9-1076998610396897">Browne’s (1982</xref>, <xref ref-type="bibr" rid="bibr10-1076998610396897">1984</xref>) asymptotic distribution free (ADF) estimator and its alternatives in an attempt to achieve robustness to the biasing effects of nonnormality. Second, a simulation investigation will be conducted on the proposed RMM approaches to assess the Type I error and power rates under a wide variety of experimental conditions; ANOVA and its aforementioned alternatives will be examined simultaneously for comparison purposes.</p>
</sec>
<sec id="section3-1076998610396897">
<title>RMM</title>
<p>Growing out of SEM, SMM was developed to examine between- or within-population differences on an underlying latent construct. In order to estimate the differences between populations on latent variable means, it expands the factor model to incorporate a mean structure and intercepts. For a set of <italic>p</italic> observed <italic>x</italic> indicators of latent construct ξ, <bold>x</bold> values in the <italic>k</italic>th population may be expressed in a <italic>p</italic> × 1 vector as
<disp-formula id="disp-formula1-1076998610396897">
<label>1</label>
<mml:math id="mml-disp1-1076998610396897">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">x</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">ν</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">Λ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mi mathvariant="italic">ξ</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="italic">δ</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula1-1076998610396897" xlink:href="10.3102_1076998610396897-eq1.tif"/>
</disp-formula>where <bold>ν</bold>
<sub>
<italic>k</italic>
</sub> is a <italic>p</italic> × 1 vector of intercept values, <bold>Λ</bold>
<sub>
<italic>k</italic>
</sub> is a <italic>p</italic> × 1 vector of λ loadings, and <bold>δ</bold> is a <italic>p</italic> × 1 vector of normal errors. In order to apply the SMM model to the between-subject testing of measured variable mean equality, the model is simply adjusted so that there is no latent factor and only one observed variable is involved. The model in <xref ref-type="disp-formula" rid="disp-formula1-1076998610396897">Equation 1</xref> can be simplified to the measured variable mean structure model
<disp-formula id="disp-formula2-1076998610396897">
<label>2</label>
<mml:math id="mml-disp2-1076998610396897">
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">ν</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="italic">δ</mml:mi>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula2-1076998610396897" xlink:href="10.3102_1076998610396897-eq2.tif"/>
</disp-formula>The univariate null hypothesis of interest is thus H<sub>0</sub>: <italic>v</italic>
<sub>1</sub> = <italic>v</italic>
<sub>2</sub> = … = <italic>v<sub>K</sub>
</italic>, which is tested by constraining population means to be equivalent while still allowing for variances of δ to be heterogeneous (i.e., leaving them unconstrained).</p>
<p>Estimation within SMM is traditionally handled using maximum likelihood (ML), which is the default in all major SEM software packages. The multigroup ML fit function is a weighted combination of fit across the <italic>K</italic> samples:
<disp-formula id="disp-formula3-1076998610396897">
<label>3</label>
<mml:math id="mml-disp3-1076998610396897">
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">ML</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>K</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">S</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">m</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mrow>
<mml:mi mathvariant="bold">Σ</mml:mi>
</mml:mrow>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover accent="true">
<mml:mtext>μ</mml:mtext>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula3-1076998610396897" xlink:href="10.3102_1076998610396897-eq3.tif"/>
</disp-formula>where <italic>n<sub>k</sub>
</italic> is the <italic>k</italic>th group’s sample size, <italic>N</italic> = <italic>n</italic>
<sub>1</sub> + … + <italic>n<sub>K</sub>
</italic>, <bold>S</bold>
<sub>
<italic>k</italic>
</sub> is the <italic>k</italic>th group’s observed covariance matrix, <bold>m</bold>
<sub>
<italic>k</italic>
</sub> is the <italic>k</italic>th group’s observed mean vector, <inline-formula id="inline-formula1-1076998610396897">
<mml:math id="mml-inline1-1076998610396897">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="normal">Σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> is the <italic>k</italic>th group’s model-implied covariance matrix, <inline-formula id="inline-formula2-1076998610396897">
<mml:math id="mml-inline2-1076998610396897">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mtext>μ</mml:mtext>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> is the <italic>k</italic>th group’s model-implied mean vector, and <italic>F<sub>k</sub>
</italic> is the <italic>k</italic>th group’s ML fit function
<disp-formula id="disp-formula4-1076998610396897">
<label>4</label>
<mml:math id="mml-disp4-1076998610396897">
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo form="prefix" movablelimits="false">ln</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="normal">Σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi>t</mml:mi>
<mml:mi>r</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">S</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="normal">Σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mo form="prefix" movablelimits="false">ln</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">S</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo stretchy="false">+</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">m</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mtext>μ</mml:mtext>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mi mathvariant="normal">Σ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">m</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mtext>μ</mml:mtext>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula4-1076998610396897" xlink:href="10.3102_1076998610396897-eq4.tif"/>
</disp-formula>(see, e.g., <xref ref-type="bibr" rid="bibr5-1076998610396897">Bollen, 1989</xref>). Of course the above simplifies considerably in the univariate (<italic>p</italic> = 1) case.</p>
<p>The <italic>K</italic>-group fit function converts to its corresponding test statistic as <italic>T</italic>
<sub>ML</sub> = (<italic>N</italic>−1)<italic>F</italic>
<sub>ML</sub>, with <italic>df</italic> equal to <italic>Kp</italic>(<italic>p</italic> + 3)/2 − <italic>q</italic>, where <italic>p</italic> is the number of observed variables and <italic>q</italic> is the number of parameters estimated across all groups. In the RMM approach specifically, this statistic provides a direct test of the population mean constraint; under normality, the test statistic is approximately distributed as a χ<sup>2</sup> with <italic>df</italic> = <italic>K −</italic> 1. That is, a total of <italic>K</italic> + 1 parameters (<italic>K</italic> population variances and one common population mean) are estimated from the 2<italic>K</italic> means and variances in the data, thereby yielding a test with 2<italic>K −</italic> (<italic>K</italic> + 1) = <italic>K −</italic> 1 <italic>df</italic>. SEM software such as EQS, LISREL, and Mplus can easily conduct the test of the above equality constraint and provide the associated <italic>T</italic>
<sub>ML</sub> test statistic (<xref ref-type="app" rid="app1-1076998610396897">Appendix A</xref>). If <italic>T</italic>
<sub>ML</sub> exceeds the desired α-level critical value, then the null hypothesis of population mean equality is rejected, implying at least some population mean differences. If <italic>T</italic>
<sub>ML</sub> does not exceed the critical value, then population mean equality remains tenable.</p>
<p>Within SEM in general, the <italic>T</italic>
<sub>ML</sub> statistic has been shown to follow its reference χ<sup>2</sup> distribution under normality; however, it becomes increasingly biased with increasing nonnormality (<xref ref-type="bibr" rid="bibr12-1076998610396897">Curran, West, &amp; Finch, 1996</xref>; <xref ref-type="bibr" rid="bibr38-1076998610396897">West, Finch, &amp; Curran, 1995</xref>), which is further exacerbated at relatively sample sizes (<xref ref-type="bibr" rid="bibr27-1076998610396897">Nevitt &amp; Hancock, 2004</xref>). It is worth noting, however, that these findings are all drawn from the literature on latent factor models with multiple measured variables; how the <italic>T</italic>
<sub>ML</sub> performs in the univariate case has not been studied explicitly.</p>
<p>Alternatives have been proposed in the SEM literature to overcome ML’s dependence on normality, starting first with <xref ref-type="bibr" rid="bibr9-1076998610396897">Browne’s (1982</xref>, <xref ref-type="bibr" rid="bibr10-1076998610396897">1984</xref>) ADF estimation (see <xref ref-type="bibr" rid="bibr25-1076998610396897">Muthén, 1989</xref>, for a discussion of ADF in the context of SMM). In its general form, the <italic>K</italic>-group ADF fit function is of the form
<disp-formula id="disp-formula5-1076998610396897">
<label>5</label>
<mml:math id="mml-disp5-1076998610396897">
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">ADF</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>K</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">s</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mtext>σ</mml:mtext>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>′</mml:mo>
</mml:msup>
</mml:mrow>
<mml:msubsup>
<mml:mi mathvariant="normal">Γ</mml:mi>
<mml:mi>k</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">s</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mtext>σ</mml:mtext>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula5-1076998610396897" xlink:href="10.3102_1076998610396897-eq5.tif"/>
</disp-formula>where <bold>s</bold>
<sub>
<italic>k</italic>
</sub> is a <italic>p</italic>* × 1 vector of observed first- and second-order moments (where <italic>p</italic>* = <italic>p</italic>(<italic>p</italic> + 3)/2), <inline-formula id="inline-formula3-1076998610396897">
<mml:math id="mml-inline3-1076998610396897">
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mover accent="true">
<mml:mtext>σ</mml:mtext>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> is a <italic>p</italic>* × 1 vector of model-implied first- and second-order moments, and Γ<sub>
<italic>k</italic>
</sub> is a <italic>p</italic>* × <italic>p</italic>* fourth-order weight matrix, the considerable details of which are left to the interested reader (see, e.g., <xref ref-type="bibr" rid="bibr25-1076998610396897">Muthén, 1989</xref>). The resulting test statistic is <italic>T</italic>
<sub>ADF</sub> = (<italic>N</italic> − 1)<italic>F</italic>
<sub>ADF</sub>, which, for the RMM case, tests the cross-population mean equality constraint and is asymptotically distributed as χ<sup>2</sup> with <italic>K</italic> − 1 <italic>df</italic>. Simulation research has shown <italic>T</italic>
<sub>ADF</sub> to be relatively unaffected by distributional characteristics (see, e.g., <xref ref-type="bibr" rid="bibr38-1076998610396897">West et al., 1995</xref>). Although the ADF approach has been criticized of requiring large sample sizes for complex models (e.g., <xref ref-type="bibr" rid="bibr12-1076998610396897">Curran et al., 1996</xref>), within the proposed RMM framework this method may work reasonably well, given that there is only a single observed variable per population.</p>
<p>A second alternative for dealing with nonnormality is <xref ref-type="bibr" rid="bibr33-1076998610396897">Satorra and Bentler’s (1988)</xref> rescaled test statistic, extended to mean structures by <xref ref-type="bibr" rid="bibr32-1076998610396897">Satorra (1992)</xref>. It is not an estimation method per se, but rather adjusts the normal theory <italic>T</italic>
<sub>ML</sub> by a scalar value that is a fairly complex function of the fourth-order moments in the sample data (related to Γ above), of the model-implied first and second-order moments, and the derivatives of those model-implied moments with respect to all model parameters (see <xref ref-type="bibr" rid="bibr32-1076998610396897">Satorra, 1992</xref>, for details). The asymptotic distribution of <italic>T</italic>
<sub>SB</sub> is generally unknown; however, when H<sub>0</sub> is true, its first moment matches a central χ<sup>2</sup> distribution with, in the RMM case, <italic>K</italic> – 1 <italic>df</italic>. Relative to T<sub>ADF</sub>, <italic>T</italic>
<sub>SB</sub> has been shown to have a generally better asymptotic correspondence to its reference χ<sup>2</sup> distribution under nonnormality and function better at smaller sample sizes (e.g., <xref ref-type="bibr" rid="bibr12-1076998610396897">Curran et al., 1996</xref>). Its relative behavior in the univariate RMM case remains to be investigated.</p>
<p>The third and fourth alternatives are estimation methods by <xref ref-type="bibr" rid="bibr42-1076998610396897">Yuan and Bentler (1997</xref>, <xref ref-type="bibr" rid="bibr43-1076998610396897">1999</xref>), yielding test statistics <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub> that make corrections to <italic>T</italic>
<sub>ADF</sub> for small sample sizes. Specifically,
<disp-formula id="disp-formula6-1076998610396897">
<label>6</label>
<mml:math id="mml-disp6-1076998610396897">
<mml:msub>
<mml:mi>T</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">YB</mml:mi>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi>T</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">ADF</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:msub>
<mml:mi>T</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">ADF</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula6-1076998610396897" xlink:href="10.3102_1076998610396897-eq6.tif"/>
</disp-formula>which follows a central χ<sup>2</sup> distribution with the same model <italic>df</italic> as <italic>T</italic>
<sub>ADF</sub> (when H<sub>0</sub> is true). Their second modification to ADF appeals to the <italic>F</italic> distribution, transforming <italic>T</italic>
<sub>ADF</sub> based upon the logic of the transformation applied to Hotelling’s <italic>T</italic>
<sup>2</sup> statistic in multivariate analysis of variance (MANOVA). Observing that <italic>T</italic>
<sup>2</sup> is a quadratic form, similar in structure to the ADF fit function, they proposed to rescale <italic>T</italic>
<sub>ADF</sub> to an <italic>F</italic>-distributed statistic,
<disp-formula id="disp-formula7-1076998610396897">
<label>7</label>
<mml:math id="mml-disp7-1076998610396897">
<mml:msub>
<mml:mi>T</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">YB</mml:mi>
<mml:mn mathvariant="normal">2</mml:mn>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow/>
</mml:mrow>
<mml:mo stretchy="false">[</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>K</mml:mi>
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>q</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mfenced close="[" open="]">
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
</mml:mfenced>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>K</mml:mi>
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>q</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">]</mml:mo>
<mml:msub>
<mml:mi>T</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">ADF</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula7-1076998610396897" xlink:href="10.3102_1076998610396897-eq7.tif"/>
</disp-formula>with numerator and denominator <italic>df</italic> of <italic>Kp</italic>* – <italic>q</italic> and <italic>N</italic> – (<italic>Kp</italic>* – <italic>q</italic>), respectively. In the specific case of RMM, the numerator and denominator <italic>df</italic> for <italic>T</italic>
<sub>YB2</sub> reduce to <italic>K</italic> − 1 and <italic>N</italic> – <italic>K</italic> + 1, respectively. Within SEM in general, simulation research has found that <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub> maintain adequate control of Type I error rates as compared to <italic>T</italic>
<sub>ADF</sub>, and yield adequate power with both normal and nonnormal data when sample size is small (<xref ref-type="bibr" rid="bibr27-1076998610396897">Nevitt &amp; Hancock, 2004</xref>). One would expect reasonable performance for simplified models, such as RMM, as well.</p>
<p>Finally, the last alternative method is <xref ref-type="bibr" rid="bibr3-1076998610396897">Bartlett’s (1950)</xref> correction to the ML test statistic (see <xref ref-type="bibr" rid="bibr14-1076998610396897">Fouladi, 1999</xref>). Within the context of exploratory factor analysis with <italic>m</italic> latent constructs and small sample sizes, Bartlett suggested a correction to the ML test statistic, which translates to
<disp-formula id="disp-formula8-1076998610396897">
<label>8</label>
<mml:math id="mml-disp8-1076998610396897">
<mml:msub>
<mml:mi>T</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">BC</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow/>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>p</mml:mi>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">3</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">2</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mi>m</mml:mi>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">3</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">11</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">6</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">ML</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula8-1076998610396897" xlink:href="10.3102_1076998610396897-eq8.tif"/>
</disp-formula>should more closely follow a χ<sup>2</sup> distribution (with <italic>Kp</italic>* – <italic>q df</italic>) than the usual <italic>T</italic>
<sub>ML</sub> statistic. This adjusted statistic is equivalent to applying a multiplicative correction to <italic>T</italic>
<sub>ML</sub> (or to any test statistic) of the form
<disp-formula id="disp-formula9-1076998610396897">
<label>9</label>
<mml:math id="mml-disp9-1076998610396897">
<mml:mi>c</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">2</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mi>p</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">4</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">5</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">6</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula9-1076998610396897" xlink:href="10.3102_1076998610396897-eq9.tif"/>
</disp-formula>For the case of SEM, <xref ref-type="bibr" rid="bibr27-1076998610396897">Nevitt and Hancock (2004)</xref> also found this statistics to perform well in small samples; its performance for the univariate mean structure case remains to be seen.</p>
<p>In sum, if the above alternative estimation and adjustment strategies accommodate nonnormality satisfactorily under the univariate SMM framework, then RMM methods should constitute a satisfactory alternative for testing mean equality under variance heterogeneity and nonnormality. What makes the RMM approaches even more attractive is that the techniques are available within with popular SEM software. The current investigation will examine the viability of these RMM methods using Monte Carlo simulation, comparing their Type I error rates and power, and examining their performance in comparison with the aforementioned ANOVA-based options. The methods for conducting this investigation are detailed in the next section.</p>
</sec>
<sec id="section4-1076998610396897">
<title>Method</title>
<sec id="section5-1076998610396897">
<title>Test Statistics Examined</title>
<p>The investigation examined the performance of five ANOVA-based methods incorporating both trimmed means and Winsorized variances, which are the ANOVA <italic>F</italic> test (for reference), Welch’s test (W), the Brown–Forsythe test (BF), James' second-order test (J), and the Alexander–Govern test (AG). For the RMM methods, this study includes the applications of univariate SMM with <italic>T</italic>
<sub>ML</sub>, <italic>T</italic>
<sub>ADF</sub>, <italic>T</italic>
<sub>SB</sub>, <italic>T</italic>
<sub>YB1</sub>, <italic>T</italic>
<sub>YB2</sub>, and <italic>T</italic>
<sub>BC</sub>.</p>
</sec>
<sec id="section6-1076998610396897">
<title>Data Generation and Modeling</title>
<p>All simulated data were generated using <xref ref-type="bibr" rid="bibr31-1076998610396897">SAS (2008)</xref>. The test statistics for Welch, Brown–Forsythe, James, and Alexander–Govern incorporated trimmed means and Winsorized variances into the procedures and were all calculated and analyzed using SAS. The <italic>T</italic>
<sub>ML</sub>, <italic>T</italic>
<sub>ADF</sub>, and <italic>T</italic>
<sub>SB</sub> test statistics were obtained by conducting univariate SMM within EQS 6.1 (<xref ref-type="bibr" rid="bibr4-1076998610396897">Bentler, 2004</xref>); based on this output, the <italic>T</italic>
<sub>YB1</sub>
<italic>, T</italic>
<sub>YB2</sub>, and <italic>T</italic>
<sub>BC</sub> test statistics were then computed post hoc.</p>
<p>Each replication generated <italic>K-</italic>independent samples with sample sizes described below. For each cell of the design, 1,000 replications were generated, tallying the rate of false rejections (i.e., Type I errors). All population means were set equal to zero when Type I error probabilities were assessed. All tests were omnibus tests of the overall null hypothesis conducted at the α = .01, .05, and .10 levels; using Bradley’s liberal criterion (<xref ref-type="bibr" rid="bibr7-1076998610396897">Bradley, 1978</xref>), the corresponding robustness intervals are (.005, .015), (.025, .075), and (.05, .15), respectively. Due to the similar patterns of results observed across group conditions for <italic>K</italic> = 2, 3, and 4 and different α levels, to reduce the large volume of results generated from this investigation, only selected tables of results at the α = .05 level are presented. Full results are available from the authors.</p>
</sec>
<sec id="section7-1076998610396897">
<title>Conditions Manipulated</title>
<p>The simulation design systematically manipulated four conditions: number of groups (<italic>K</italic>), degree of nonnormality, degree of variance heterogeneity, and sample size. For all cells of this design, all of the aforementioned ANOVA-based test statistics (with trimmed means and Winsorized variances) and RMM test statistics were computed for each replication. The group conditions investigated were <italic>K</italic> = 2, 3, and 4 independent samples. The specific sample size and variance conditions are presented below for each of the <italic>K</italic> conditions.</p>
<sec id="section8-1076998610396897">
<title>K = 2</title>
<p>The scores in the first sample were multiplied by a constant so that the standard deviation ratio σ<sub>1</sub>:σ<sub>2</sub> was 1:1, 2.5:1, or 4:1. The total sample size <italic>N</italic> = <italic>n</italic>
<sub>1</sub> + <italic>n</italic>
<sub>2</sub> was fixed at <italic>N</italic> = 20, 100, and 500, respectively, and the ratio of sample sizes <italic>n</italic>
<sub>1</sub>:<italic>n</italic>
<sub>2</sub> was 4:1, 1:1, and 1:4, respectively. For example, when <italic>N</italic> = 100, the three pairs of sample sizes were 80:20, 50:50, and 20:80; each of these sample size conditions in turn was crossed with the standard deviation ratio conditions to create 24 nonredundant conditions. Note that when the standard deviation ratio was 1:1, the sample size ratios of 4:1 and 1:4 provided two redundant conditions; thus, one condition is removed from reporting.</p>
</sec>
<sec id="section10-1076998610396897">
<title>K = 3</title>
<p>The scores in the first and second samples were multiplied by constants so that the standard deviation ratio σ<sub>1</sub>:σ<sub>2</sub>:σ<sub>3</sub> was made to follow 1:1:1, 1:2.5:4, and 4:2.5:1. The total sample size <italic>N</italic> = <italic>n</italic>
<sub>1</sub> + <italic>n</italic>
<sub>2</sub> + <italic>n</italic>
<sub>3</sub> was fixed at 90, 180, and 900, respectively, and the ratio of sample sizes, <italic>n</italic>
<sub>1</sub>:<italic>n</italic>
<sub>2</sub>:<italic>n</italic>
<sub>3</sub>, was 4:2.5:1, 1:1:1, and 1:2.5:4, respectively. For example, when <italic>N</italic> = 90, the three pairs of sample sizes were 48:30:12, 30:30:30, and 12:30:48; each of these conditions in turn was crossed with the standard deviation ratio conditions to create 15 nonredundant conditions. Note that when the standard deviation ratio was 1:1:1, the sample size ratios of 4:2.5:1 and 1:2.5:4 provided two redundant conditions; thus, one condition is removed from reporting.</p>
</sec>
<sec id="section11-1076998610396897">
<title>K = 4</title>
<p>The scores in the first three samples were multiplied by constants so that the ratio σ<sub>1</sub>:σ<sub>2</sub>:σ<sub>3</sub>:σ<sub>4</sub> was made to follow 4:3:2:1, 1:1:1:1, and 1:2:3:4. The total sample size <italic>N</italic> = <italic>n</italic>
<sub>1</sub>+<italic>n</italic>
<sub>2</sub>+<italic>n</italic>
<sub>3</sub>+<italic>n</italic>
<sub>4</sub> was fixed at 120, 240, and 1,200, respectively, and the ratio of sample sizes <italic>n</italic>
<sub>1</sub>:<italic>n</italic>
<sub>2</sub>:<italic>n</italic>
<sub>3</sub>:<italic>n</italic>
<sub>4</sub> was 5:4:2:1, 1:1:1:1, and 1:2:4:5, respectively. For example, when <italic>N</italic> = 120, the three pairs of sample sizes were 50:40:20:10, 30:30:30:30, and 10:20:40:50; each of these conditions in turn was crossed with the standard deviation ratio conditions to create 15 nonredundant conditions. Note that when the standard deviation ratio was 1:1:1:1, the sample size ratios of 4:3:2:1 and 1:2:3:4 provided two redundant conditions; thus, one condition will be removed from reporting.</p>
<p>With regard to distributions, data for each sample came from populations with the same distributional shape; there were four such populations. Distribution 1 is multivariate normal with zero skew and kurtosis (0, 0). Distribution 2 is elliptical distribution with skew and kurtosis of (0, 3). Distribution 3 is nonnormal and asymmetric with skew and kurtosis of (0.75, 0). Distribution 4 is nonnormal and asymmetric with skew and kurtosis of (3, 21). Data were generated using <xref ref-type="bibr" rid="bibr31-1076998610396897">SAS (2008)</xref>; Distributions 2 and 3 used <xref ref-type="bibr" rid="bibr30-1076998610396897">Ramberg and Schmeiser’s (1974)</xref> power transformation of uniform variables to obtain a generalized lambda distribution, while Distribution 4 used the programming described by <xref ref-type="bibr" rid="bibr26-1076998610396897">Nevitt and Hancock (1999)</xref> that follows the <xref ref-type="bibr" rid="bibr13-1076998610396897">Fleishman’s (1978)</xref> polynomial transformation.</p>
</sec>
</sec>
<sec id="section12-1076998610396897">
<title>Design and Execution</title>
<p>The study examined the 11 test statistics under a total of 1,944 nonredundant conditions arising from the crossing of sample size, number of groups, variance ratios, distributional forms, and Type I error rates, as described above. For each cell, 1,000 independent data sets were generated and the same simulated data set was used to determine the 11 test statistics. These data were used to estimate Type I error rates associated with each test statistic under all cell conditions.</p>
</sec>
<sec id="section13-1076998610396897">
<title>Power Analysis</title>
<p>For test statistics that yielded an observed Type I error rate that was within or below the robustness interval, power was examined by simulating a new series of 1,000 replications using the same conditions described above, but with unequal population means. For each of the same conditions described previously, two false null conditions were created for each value of <italic>K</italic>. For <italic>K</italic> = 2, mean differences were induced that yielded population standardized effect sizes of <italic>d</italic> = .2 and <italic>d</italic> = .8 for both the homogeneous and heterogeneous variance cases. In these cases, the first population always had a positive mean and the second population had a mean of zero (e.g., µ<sub>1</sub> = 0.8, µ<sub>2</sub> = 0). For <italic>K</italic> = 3 and <italic>K</italic> = 4, mean differences were induced to yield population standardized effect sizes of <italic>f</italic> = .1 and <italic>f</italic> = .4, for both the homogeneous and the heterogeneous variance cases (where for the heterogeneous variance cases the denominator of the effect size was based on a sample size weighted variance, <inline-formula id="inline-formula4-1076998610396897">
<mml:math id="mml-inline4-1076998610396897">
<mml:msup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>K</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi>n</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:msubsup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mi>k</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
<mml:mi>N</mml:mi>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>). For <italic>K</italic> = 3, the population means were equally spaced around zero (e.g., µ<sub>1</sub> = 0.4, µ<sub>2</sub> = 0, µ<sub>3</sub> = −0.4); when <italic>K</italic> = 4, the second and third population means were zero while the first and fourth were equally positive and negative, respectively (e.g., µ<sub>1</sub> = 0.4, µ<sub>2</sub> = 0, µ<sub>3</sub> = 0, µ<sub>4</sub> = −0.4). For each eligible statistic within each condition, the number of rejections (out of 1,000 replications) was tallied in order to estimate relative power of the omnibus test of mean differences.</p>
</sec>
</sec>
<sec id="section14-1076998610396897">
<title>Results</title>
<sec id="section15-1076998610396897">
<title>Nonconvergence</title>
<p>Rates of nonconvergence for the <italic>T</italic>
<sub>ML</sub>, <italic>T</italic>
<sub>ADF</sub>, and <italic>T</italic>
<sub>SB</sub> test statistics were tracked for all cells in the Type I error portion of the study. Because the test statistics <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub> are derived from the <italic>T</italic>
<sub>ADF</sub>, and the test statistic <italic>T</italic>
<sub>BC</sub> is based on <italic>T</italic>
<sub>ML</sub>, these corresponding test statistics produce identical nonconvergence rates. Overall, practically speaking, nonconvergence was not a problem. When <italic>K</italic> &gt; 2, almost no nonconvergence occurred; similarly, when <italic>K</italic> = 2 with moderate and large sample sizes, no nonconvergence occurred. When <italic>K</italic> = 2 with small sample sizes, nonconvergence reached its highest rate, but still only reaching 1.7%; specifically, ML (and SB) yielded nonconvergence rates of 1.7%, 0.6%, 1.3% and 1.0% across distributions with skewness and kurtosis of (0, 0), (0, 3), (.75, 0), and (3, 21), respectively, with difficulties occurring most notably in negative conditions and conditions with increased variance heterogeneity. Even more impressive was the behavior of ADF, with no nonconvergent replications in any conditions.</p>
</sec>
<sec id="section16-1076998610396897">
<title>Type I Errors</title>
<p>Results of the Type I error investigation’s 1944 nonredundant conditions for each of the 11 test statistics are summarized below by distribution.</p>
<sec id="section17-1076998610396897">
<title>Normal distribution (0, 0)</title>
<p>When <italic>K</italic> = 2, all the Welch-type statistics and RMM methods were robust when sample sizes were moderate or large across different variance heterogeneity ratios; when sample sizes were small and unequal, the Welch-type statistics delivered Type I error rates that mostly fell below the lower boundary of the robustness range. Impressively, the <italic>T</italic>
<sub>ML</sub>, <italic>T</italic>
<sub>SB</sub>, and <italic>T</italic>
<sub>BC</sub> test statistics were robust across all <italic>K</italic> = 2 conditions. With <italic>K</italic> = 2 the <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub> test statistics also controlled the Type I error rate well, with only one or two inflated rejection rates when sample sizes were small under negative conditions. The <italic>T</italic>
<sub>ADF</sub> test statistic, however, provided inflated Type I error rates when <italic>K</italic> = 2, except under positive conditions. Finally, when <italic>K</italic> = 3 and <italic>K</italic> = 4, both the Welch-type statistics and the RMM approaches were robust across virtually all conditions of sample sizes and variance ratios. Select results are displayed in <xref ref-type="table" rid="table1-1076998610396897">Table 1</xref>
for the <italic>K</italic> = 2 case.</p>
<table-wrap id="table1-1076998610396897" position="float">
<label>Table 1.</label>
<caption>
<p>Type I Error Rates (%) for .05-Level Tests When K = 2, Under Select Conditions</p>
</caption>
<graphic alternate-form-of="table1-1076998610396897" xlink:href="10.3102_1076998610396897-table1.tif"/>
<table>
<thead>
<tr>
<th>Distribution</th>
<th>
<italic>n</italic>
<sub>1</sub>: <italic>n</italic>
<sub>2</sub>
</th>
<th>
<italic>σ</italic>
<sub>1</sub>/ <italic>σ</italic>
<sub>2</sub>
</th>
<th>F</th>
<th>W</th>
<th>BF</th>
<th>AG</th>
<th>J</th>
<th>
<italic>T</italic>
<sub>ML</sub>
</th>
<th>
<italic>T</italic>
<sub>ADF</sub>
</th>
<th>
<italic>T</italic>
<sub>SB</sub>
</th>
<th>
<italic>T</italic>
<sub>YB1</sub>
</th>
<th>
<italic>T</italic>
<sub>YB2</sub>
</th>
<th>
<italic>T</italic>
<sub>BC</sub>
</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="8">(0, 0)</td>
<td rowspan="3">4:16</td>
<td>1</td>
<td>4.1</td>
<td>
<bold>1.5</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>
<bold>2.0</bold>
</td>
<td>
<bold>1.7</bold>
</td>
<td>4.2</td>
<td>
<bold>8.4</bold>
</td>
<td>4.2</td>
<td>6.1</td>
<td>6.9</td>
<td>3.9</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>1.7</bold>
</td>
<td>
<bold>1.1</bold>
</td>
<td>
<bold>1.1</bold>
</td>
<td>
<bold>2.1</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>4.3</td>
<td>
<bold>8.8</bold>
</td>
<td>4.3</td>
<td>6.8</td>
<td>
<bold>7.6</bold>
</td>
<td>3.5</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>2.4</bold>
</td>
<td>
<bold>1.0</bold>
</td>
<td>
<bold>1.0</bold>
</td>
<td>
<bold>2.0</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>4.2</td>
<td>
<bold>9.6</bold>
</td>
<td>4.2</td>
<td>
<bold>7.9</bold>
</td>
<td>
<bold>8.3</bold>
</td>
<td>3.4</td>
</tr>
<tr>
<td rowspan="2">16:4</td>
<td>2.5</td>
<td>
<bold>0.1</bold>
</td>
<td>
<bold>1.4</bold>
</td>
<td>
<bold>1.4</bold>
</td>
<td>
<bold>1.3</bold>
</td>
<td>
<bold>1.6</bold>
</td>
<td>3.9</td>
<td>6.2</td>
<td>3.9</td>
<td>3.7</td>
<td>4.5</td>
<td>3.4</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>0.0</bold>
</td>
<td>
<bold>2.1</bold>
</td>
<td>
<bold>2.1</bold>
</td>
<td>
<bold>2.2</bold>
</td>
<td>2.5</td>
<td>4.4</td>
<td>5.8</td>
<td>4.4</td>
<td>3.5</td>
<td>4.5</td>
<td>3.4</td>
</tr>
<tr>
<td rowspan="3">10:10</td>
<td>1</td>
<td>4.2</td>
<td>3.8</td>
<td>3.8</td>
<td>3.7</td>
<td>4.1</td>
<td>7.0</td>
<td>
<bold>8.5</bold>
</td>
<td>7.0</td>
<td>5.6</td>
<td>6.9</td>
<td>5.9</td>
</tr>
<tr>
<td>2.5</td>
<td>5.6</td>
<td>4.3</td>
<td>4.3</td>
<td>4.3</td>
<td>4.6</td>
<td>6.3</td>
<td>
<bold>8.7</bold>
</td>
<td>6.3</td>
<td>5.4</td>
<td>6.7</td>
<td>5.3</td>
</tr>
<tr>
<td>4</td>
<td>7.0</td>
<td>5.1</td>
<td>5.1</td>
<td>5.5</td>
<td>5.7</td>
<td>5.9</td>
<td>
<bold>8.3</bold>
</td>
<td>5.9</td>
<td>5.2</td>
<td>6.4</td>
<td>4.7</td>
</tr>
<tr>
<td rowspan="8">(0, 3)</td>
<td rowspan="3">4:16</td>
<td>1</td>
<td>4.2</td>
<td>3.3</td>
<td>3.3</td>
<td>3.7</td>
<td>3.7</td>
<td>3.5</td>
<td>5.9</td>
<td>3.5</td>
<td>4.7</td>
<td>5.3</td>
<td>3.1</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>17.9</bold>
</td>
<td>3.4</td>
<td>3.4</td>
<td>4.9</td>
<td>4.1</td>
<td>5.4</td>
<td>
<bold>9.2</bold>
</td>
<td>5.4</td>
<td>7.5</td>
<td>
<bold>8.1</bold>
</td>
<td>5.1</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>24.3</bold>
</td>
<td>3.3</td>
<td>3.3</td>
<td>5.2</td>
<td>4.3</td>
<td>57.0</td>
<td>
<bold>9.5</bold>
</td>
<td>57.0</td>
<td>8.2</td>
<td>
<bold>8.7</bold>
</td>
<td>5.6</td>
</tr>
<tr>
<td rowspan="2">16:4</td>
<td>2.5</td>
<td>
<bold>0.4</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>
<bold>1.6</bold>
</td>
<td>
<bold>1.8</bold>
</td>
<td>
<bold>2.3</bold>
</td>
<td>3.3</td>
<td>
<bold>2.3</bold>
</td>
<td>
<bold>1.8</bold>
</td>
<td>
<bold>2.4</bold>
</td>
<td>
<bold>1.6</bold>
</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>0.2</bold>
</td>
<td>
<bold>1.9</bold>
</td>
<td>
<bold>1.9</bold>
</td>
<td>
<bold>2.3</bold>
</td>
<td>2.6</td>
<td>
<bold>2.3</bold>
</td>
<td>3.4</td>
<td>
<bold>2.3</bold>
</td>
<td>
<bold>1.9</bold>
</td>
<td>
<bold>2.2</bold>
</td>
<td>
<bold>2.0</bold>
</td>
</tr>
<tr>
<td rowspan="3">10:10</td>
<td>1</td>
<td>5.2</td>
<td>4.4</td>
<td>4.4</td>
<td>4.4</td>
<td>4.7</td>
<td>4.1</td>
<td>5.9</td>
<td>4.1</td>
<td>3.3</td>
<td>4.2</td>
<td>3.3</td>
</tr>
<tr>
<td>2.5</td>
<td>6.2</td>
<td>5.2</td>
<td>5.2</td>
<td>4.9</td>
<td>5.7</td>
<td>4.7</td>
<td>6.3</td>
<td>4.9</td>
<td>4.1</td>
<td>5.1</td>
<td>4.3</td>
</tr>
<tr>
<td>4</td>
<td>7.2</td>
<td>5.2</td>
<td>5.2</td>
<td>5.7</td>
<td>6.2</td>
<td>5.0</td>
<td>7.0</td>
<td>5.0</td>
<td>4.7</td>
<td>6.0</td>
<td>4.4</td>
</tr>
<tr>
<td rowspan="8">(.75, 0)</td>
<td rowspan="3">4:16</td>
<td>1</td>
<td>
<bold>2.4</bold>
</td>
<td>3.3</td>
<td>3.3</td>
<td>4.3</td>
<td>3.9</td>
<td>6.3</td>
<td>
<bold>8.8</bold>
</td>
<td>6.3</td>
<td>6.8</td>
<td>7.5</td>
<td>5.6</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>21.8</bold>
</td>
<td>2.7</td>
<td>2.7</td>
<td>5.8</td>
<td>3.4</td>
<td>6.4</td>
<td>
<bold>11.3</bold>
</td>
<td>6.4</td>
<td>
<bold>8.9</bold>
</td>
<td>
<bold>10.0</bold>
</td>
<td>5.9</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>31.4</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>4.6</td>
<td>3.5</td>
<td>6.4</td>
<td>
<bold>11.4</bold>
</td>
<td>6.4</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>9.9</bold>
</td>
<td>5.9</td>
</tr>
<tr>
<td rowspan="2">16:4</td>
<td>2.5</td>
<td>
<bold>0.3</bold>
</td>
<td>
<bold>1.8</bold>
</td>
<td>
<bold>1.8</bold>
</td>
<td>
<bold>2.0</bold>
</td>
<td>
<bold>2.2</bold>
</td>
<td>3.7</td>
<td>5.5</td>
<td>3.7</td>
<td>3.7</td>
<td>4.2</td>
<td>3.3</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>0.3</bold>
</td>
<td>3.2</td>
<td>3.2</td>
<td>3.6</td>
<td>4.0</td>
<td>4.2</td>
<td>5.0</td>
<td>4.2</td>
<td>3.5</td>
<td>4.0</td>
<td>3.1</td>
</tr>
<tr>
<td rowspan="3">10:10</td>
<td>1</td>
<td>4.7</td>
<td>3.9</td>
<td>3.9</td>
<td>3.8</td>
<td>4.1</td>
<td>6.6</td>
<td>
<bold>8.3</bold>
</td>
<td>6.6</td>
<td>5.8</td>
<td>6.5</td>
<td>6.0</td>
</tr>
<tr>
<td>2.5</td>
<td>6.8</td>
<td>6.1</td>
<td>6.1</td>
<td>6.1</td>
<td>6.4</td>
<td>6.0</td>
<td>
<bold>7.8</bold>
</td>
<td>6.0</td>
<td>5.5</td>
<td>6.4</td>
<td>5.3</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>8.9</bold>
</td>
<td>6.9</td>
<td>6.9</td>
<td>6.9</td>
<td>7.1</td>
<td>5.5</td>
<td>7.4</td>
<td>5.5</td>
<td>5.4</td>
<td>6.2</td>
<td>5.1</td>
</tr>
<tr>
<td rowspan="24">(3, 21)</td>
<td rowspan="3">4:16</td>
<td>1</td>
<td>
<bold>2.2</bold>
</td>
<td>
<bold>1.4</bold>
</td>
<td>
<bold>1.4</bold>
</td>
<td>
<bold>1.7</bold>
</td>
<td>
<bold>1.5</bold>
</td>
<td>4.5</td>
<td>7.0</td>
<td>4.5</td>
<td>4.7</td>
<td>5.2</td>
<td>4.0</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>30.8</bold>
</td>
<td>2.8</td>
<td>2.8</td>
<td>5.7</td>
<td>3.6</td>
<td>
<bold>9.8</bold>
</td>
<td>
<bold>15.0</bold>
</td>
<td>
<bold>9.8</bold>
</td>
<td>
<bold>12.6</bold>
</td>
<td>
<bold>13.7</bold>
</td>
<td>
<bold>8.6</bold>
</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>44.6</bold>
</td>
<td>
<bold>1.8</bold>
</td>
<td>
<bold>1.8</bold>
</td>
<td>5.4</td>
<td>3.6</td>
<td>
<bold>10.3</bold>
</td>
<td>
<bold>16.8</bold>
</td>
<td>
<bold>10.3</bold>
</td>
<td>
<bold>13.6</bold>
</td>
<td>
<bold>14.4</bold>
</td>
<td>
<bold>9.3</bold>
</td>
</tr>
<tr>
<td rowspan="2">16:4</td>
<td>2.5</td>
<td>
<bold>2.0</bold>
</td>
<td>3.1</td>
<td>3.1</td>
<td>4.1</td>
<td>3.6</td>
<td>
<bold>1.9</bold>
</td>
<td>3.5</td>
<td>
<bold>1.9</bold>
</td>
<td>
<bold>1.8</bold>
</td>
<td>
<bold>2.3</bold>
</td>
<td>
<bold>1.6</bold>
</td>
</tr>
<tr>
<td>4</td>
<td>3.3</td>
<td>
<bold>9.4</bold>
</td>
<td>
<bold>9.4</bold>
</td>
<td>
<bold>10.7</bold>
</td>
<td>10.6</td>
<td>3.1</td>
<td>4.9</td>
<td>3.1</td>
<td>3.3</td>
<td>4.0</td>
<td>2.3</td>
</tr>
<tr>
<td rowspan="3">10:10</td>
<td>1</td>
<td>3.4</td>
<td>
<bold>2.4</bold>
</td>
<td>
<bold>2.4</bold>
</td>
<td>
<bold>2.4</bold>
</td>
<td>2.5</td>
<td>3.7</td>
<td>6.0</td>
<td>3.7</td>
<td>3.0</td>
<td>3.6</td>
<td>2.8</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>15.7</bold>
</td>
<td>
<bold>14.6</bold>
</td>
<td>
<bold>14.6</bold>
</td>
<td>
<bold>14.6</bold>
</td>
<td>
<bold>15.0</bold>
</td>
<td>7.1</td>
<td>9.3</td>
<td>7.1</td>
<td>6.7</td>
<td>7.4</td>
<td>6.3</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>22.4</bold>
</td>
<td>
<bold>9.6</bold>
</td>
<td>
<bold>19.6</bold>
</td>
<td>
<bold>20.0</bold>
</td>
<td>
<bold>20.7</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>11.3</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>8.1</bold>
</td>
<td>
<bold>9.6</bold>
</td>
<td>
<bold>7.9</bold>
</td>
</tr>
<tr>
<td rowspan="3">20:80</td>
<td>1</td>
<td>5.4</td>
<td>5.6</td>
<td>5.6</td>
<td>5.4</td>
<td>5.5</td>
<td>5.1</td>
<td>5.7</td>
<td>5.1</td>
<td>5.1</td>
<td>5.1</td>
<td>4.8</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>42.1</bold>
</td>
<td>
<bold>22.6</bold>
</td>
<td>
<bold>22.6</bold>
</td>
<td>
<bold>22.8</bold>
</td>
<td>
<bold>22.6</bold>
</td>
<td>
<bold>8.7</bold>
</td>
<td>
<bold>9.6</bold>
</td>
<td>
<bold>8.7</bold>
</td>
<td>
<bold>9.3</bold>
</td>
<td>
<bold>9.2</bold>
</td>
<td>
<bold>8.5</bold>
</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>54.5</bold>
</td>
<td>
<bold>27.3</bold>
</td>
<td>
<bold>27.3</bold>
</td>
<td>
<bold>27.5</bold>
</td>
<td>
<bold>27.5</bold>
</td>
<td>
<bold>9.5</bold>
</td>
<td>
<bold>10.5</bold>
</td>
<td>
<bold>9.5</bold>
</td>
<td>
<bold>10.0</bold>
</td>
<td>
<bold>9.9</bold>
</td>
<td>
<bold>9.3</bold>
</td>
</tr>
<tr>
<td rowspan="2">80:20</td>
<td>2.5</td>
<td>
<bold>11.2</bold>
</td>
<td>
<bold>32.2</bold>
</td>
<td>
<bold>32.2</bold>
</td>
<td>
<bold>31.9</bold>
</td>
<td>
<bold>32.3</bold>
</td>
<td>4.4</td>
<td>4.8</td>
<td>4.4</td>
<td>4.3</td>
<td>4.3</td>
<td>4.2</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>11.3</bold>
</td>
<td>
<bold>54.7</bold>
</td>
<td>
<bold>54.7</bold>
</td>
<td>
<bold>54.2</bold>
</td>
<td>
<bold>54.7</bold>
</td>
<td>5.5</td>
<td>5.6</td>
<td>5.5</td>
<td>5.7</td>
<td>5.7</td>
<td>5.5</td>
</tr>
<tr>
<td rowspan="3">50:50</td>
<td>1</td>
<td>3.9</td>
<td>4.3</td>
<td>4.3</td>
<td>4.1</td>
<td>4.3</td>
<td>4.5</td>
<td>4.6</td>
<td>4.5</td>
<td>4.4</td>
<td>4.4</td>
<td>4.4</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>32.7</bold>
</td>
<td>
<bold>32.7</bold>
</td>
<td>
<bold>32.7</bold>
</td>
<td>
<bold>32.7</bold>
</td>
<td>
<bold>32.7</bold>
</td>
<td>6.5</td>
<td>6.9</td>
<td>6.5</td>
<td>6.4</td>
<td>6.4</td>
<td>6.3</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>46.5</bold>
</td>
<td>
<bold>45.9</bold>
</td>
<td>
<bold>45.9</bold>
</td>
<td>
<bold>45.9</bold>
</td>
<td>
<bold>45.9</bold>
</td>
<td>7.1</td>
<td>7.5</td>
<td>7.1</td>
<td>7.1</td>
<td>7.1</td>
<td>7.1</td>
</tr>
<tr>
<td rowspan="3">100:400</td>
<td>1</td>
<td>5.0</td>
<td>5.4</td>
<td>5.4</td>
<td>5.4</td>
<td>5.5</td>
<td>5.4</td>
<td>5.6</td>
<td>5.4</td>
<td>5.4</td>
<td>5.0</td>
<td>5.4</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>77.1</bold>
</td>
<td>
<bold>53.1</bold>
</td>
<td>
<bold>53.1</bold>
</td>
<td>
<bold>53.2</bold>
</td>
<td>
<bold>53.2</bold>
</td>
<td>7.6</td>
<td>
<bold>7.8</bold>
</td>
<td>
<bold>7.6</bold>
</td>
<td>
<bold>7.8</bold>
</td>
<td>7.5</td>
<td>7.5</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>90.0</bold>
</td>
<td>
<bold>68.3</bold>
</td>
<td>
<bold>68.3</bold>
</td>
<td>
<bold>68.3</bold>
</td>
<td>
<bold>68.3</bold>
</td>
<td>7.0</td>
<td>
<bold>7.6</bold>
</td>
<td>7.0</td>
<td>6.3</td>
<td>6.1</td>
<td>7.0</td>
</tr>
<tr>
<td rowspan="2">400:100</td>
<td>2.5</td>
<td>
<bold>57.3</bold>
</td>
<td>
<bold>91.3</bold>
</td>
<td>
<bold>91.3</bold>
</td>
<td>
<bold>91.6</bold>
</td>
<td>
<bold>91.7</bold>
</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>84.0</bold>
</td>
<td>
<bold>99.0</bold>
</td>
<td>
<bold>99.0</bold>
</td>
<td>
<bold>99.0</bold>
</td>
<td>
<bold>99.0</bold>
</td>
<td>6.5</td>
<td>6.5</td>
<td>6.5</td>
<td>6.4</td>
<td>6.4</td>
<td>6.5</td>
</tr>
<tr>
<td rowspan="3">250:250</td>
<td>1</td>
<td>4.2</td>
<td>4.3</td>
<td>4.3</td>
<td>4.4</td>
<td>4.4</td>
<td>6.2</td>
<td>6.2</td>
<td>6.2</td>
<td>6.2</td>
<td>6.1</td>
<td>6.2</td>
</tr>
<tr>
<td>2.5</td>
<td>
<bold>83.1</bold>
</td>
<td>
<bold>83.8</bold>
</td>
<td>
<bold>83.8</bold>
</td>
<td>
<bold>84.0</bold>
</td>
<td>
<bold>84.0</bold>
</td>
<td>6.2</td>
<td>6.6</td>
<td>6.2</td>
<td>6.2</td>
<td>6.1</td>
<td>6.2</td>
</tr>
<tr>
<td>4</td>
<td>
<bold>94.9</bold>
</td>
<td>
<bold>95.0</bold>
</td>
<td>
<bold>95.0</bold>
</td>
<td>
<bold>95.0</bold>
</td>
<td>
<bold>95.0</bold>
</td>
<td>6.3</td>
<td>6.3</td>
<td>6.3</td>
<td>6.3</td>
<td>6.1</td>
<td>6.3</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1076998610396897">
<p>
<italic>Note:</italic> Bolded values indicate rejection rates outside of Bradley’s robustness interval.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section19-1076998610396897">
<title>Elliptically symmetric nonnormal distribution (0,3)</title>
<p>When <italic>K</italic> = 2, the Type I error rates tended to fall below the robustness range for the Welch-type statistics and most RMM approaches when sample sizes were small and under positive conditions; they were robust at all other conditions. The <italic>T</italic>
<sub>ADF</sub> and <italic>T</italic>
<sub>YB2</sub> test statistics, on the other hand, provided inflated Type I error rates with small sample sizes under negative conditions; the <italic>T</italic>
<sub>ADF</sub> test statistic was robust at all other conditions. Finally, when <italic>K =</italic> 3 and <italic>K =</italic> 4, the Welch-type statistics and the RMM approaches were robust across virtually all of the sample size and variance ratio conditions. Select results are displayed in <xref ref-type="table" rid="table1-1076998610396897">Table 1</xref> for the <italic>K</italic> = 2 case.</p>
</sec>
<sec id="section20-1076998610396897">
<title>Asymmetric nonnormal distribution (.75, 0)</title>
<p>When <italic>K</italic> = 2, the RMM approaches clearly outperformed the Welch-type statistics when the distribution by providing significantly fewer nonrobust cells. When sample sizes were moderate or large, the RMM approaches provided the Type I error rates all falling within the robustness interval, while the Welch-type statistics were nonrobust for most heterogeneous variance conditions. When sample sizes were small, the <italic>T</italic>
<sub>ML</sub>, <italic>T</italic>
<sub>SB</sub>, and <italic>T</italic>
<sub>BC</sub> test statistics again controlled Type I error rates well across all <italic>K</italic> = 2 conditions. The remaining RMM statistics, <italic>T</italic>
<sub>ADF</sub>, <italic>T</italic>
<sub>YB1</sub>, and <italic>T</italic>
<sub>YB2</sub>, provided a couple of cells of inflated Type I error rates under negative conditions, while the Welch-type statistics tended to provide Type I error rates falling below the robustness interval for both negative and positive conditions. Select results are displayed in <xref ref-type="table" rid="table1-1076998610396897">Table 1</xref>.</p>
<p>When <italic>K</italic> = 3 and <italic>K</italic> = 4, it was again clear that the RMM approaches outperformed all the Welch-type statistics by showing robust cells across different sample sizes and variances conditions. The Welch-type statistics provided inflated Type I error rates for all negative conditions, for conditions of equal sample sizes with heterogeneous variances, and for positive conditions even when sample sizes became moderate and large. Select results are displayed in <xref ref-type="table" rid="table2-1076998610396897">Tables 2</xref> and <xref ref-type="table" rid="table3-1076998610396897">3</xref>

.</p>
<table-wrap id="table2-1076998610396897" position="float">
<label>Table 2.</label>
<caption>
<p>Type I Error Rates (%) for .05-Level Tests When K = 3, Under Asymmetric Nonnormal Distributions</p>
</caption>
<graphic alternate-form-of="table2-1076998610396897" xlink:href="10.3102_1076998610396897-table2.tif"/>
<table>
<thead>
<tr>
<th>Distribution</th>
<th>
<italic>n</italic>
<sub>1</sub>:<italic>n</italic>
<sub>2</sub>:<italic>n</italic>
<sub>3</sub>
</th>
<th>σ<sub>1</sub>:σ<sub>2</sub>:σ<sub>3</sub>
</th>
<th>
<italic>F</italic>
</th>
<th>W</th>
<th>BF</th>
<th>AG</th>
<th>U</th>
<th>
<italic>T</italic>
<sub>ML</sub>
</th>
<th>
<italic>T</italic>
<sub>ADF</sub>
</th>
<th>
<italic>T</italic>
<sub>SB</sub>
</th>
<th>
<italic>T</italic>
<sub>YB1</sub>
</th>
<th>
<italic>T</italic>
<sub>YB2</sub>
</th>
<th>
<italic>T</italic>
<sub>BC</sub>
</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="15">(.75, 0)</td>
<td rowspan="2">30:30:30</td>
<td>1:2.5:4</td>
<td>
<bold>9.7</bold>
</td>
<td>
<bold>10.3</bold>
</td>
<td>
<bold>9.4</bold>
</td>
<td>
<bold>10.0</bold>
</td>
<td>
<bold>10.3</bold>
</td>
<td>5.5</td>
<td>6.7</td>
<td>5.5</td>
<td>5.8</td>
<td>5.9</td>
<td>5.5</td>
</tr>
<tr>
<td>1:1:1</td>
<td>5.4</td>
<td>6.3</td>
<td>5.3</td>
<td>6.4</td>
<td>6.5</td>
<td>6.0</td>
<td>6.5</td>
<td>6.0</td>
<td>5.9</td>
<td>5.9</td>
<td>5.9</td>
</tr>
<tr>
<td rowspan="3">12:30:48</td>
<td>4:2.5:1</td>
<td>
<bold>27.3</bold>
</td>
<td>
<bold>11.7</bold>
</td>
<td>
<bold>10.8</bold>
</td>
<td>
<bold>11.1</bold>
</td>
<td>
<bold>11.6</bold>
</td>
<td>6.2</td>
<td>8.0</td>
<td>6.2</td>
<td>7.4</td>
<td>7.4</td>
<td>6.1</td>
</tr>
<tr>
<td>1:2.5:4</td>
<td>
<bold>2.4</bold>
</td>
<td>7.0</td>
<td>
<bold>8.7</bold>
</td>
<td>6.9</td>
<td>7.1</td>
<td>4.8</td>
<td>6.0</td>
<td>4.8</td>
<td>4.8</td>
<td>4.8</td>
<td>4.7</td>
</tr>
<tr>
<td>1:1:1</td>
<td>3.3</td>
<td>5.8</td>
<td>4.6</td>
<td>6.6</td>
<td>5.9</td>
<td>4.9</td>
<td>7.1</td>
<td>4.9</td>
<td>6.0</td>
<td>6.0</td>
<td>4.9</td>
</tr>
<tr>
<td rowspan="2">60:60:60</td>
<td>1:2.5:4</td>
<td>
<bold>9.9</bold>
</td>
<td>
<bold>11.1</bold>
</td>
<td>
<bold>9.8</bold>
</td>
<td>
<bold>12.0</bold>
</td>
<td>
<bold>11.9</bold>
</td>
<td>4.4</td>
<td>4.8</td>
<td>4.4</td>
<td>4.3</td>
<td>4.4</td>
<td>4.3</td>
</tr>
<tr>
<td>1:1:1</td>
<td>4.1</td>
<td>4.5</td>
<td>4.1</td>
<td>4.3</td>
<td>4.5</td>
<td>4.4</td>
<td>4.7</td>
<td>4.4</td>
<td>4.2</td>
<td>4.3</td>
<td>4.2</td>
</tr>
<tr>
<td rowspan="3">24:60:96</td>
<td>4:2.5:1</td>
<td>
<bold>27.1</bold>
</td>
<td>
<bold>11.4</bold>
</td>
<td>
<bold>9.2</bold>
</td>
<td>
<bold>11.3</bold>
</td>
<td>
<bold>11.4</bold>
</td>
<td>5.1</td>
<td>6.1</td>
<td>5.1</td>
<td>5.3</td>
<td>5.5</td>
<td>4.8</td>
</tr>
<tr>
<td>1:2.5:4</td>
<td>3.7</td>
<td>
<bold>9.8</bold>
</td>
<td>
<bold>10.3</bold>
</td>
<td>
<bold>9.7</bold>
</td>
<td>
<bold>9.8</bold>
</td>
<td>5.7</td>
<td>6.1</td>
<td>5.7</td>
<td>5.7</td>
<td>5.7</td>
<td>5.5</td>
</tr>
<tr>
<td>1:1:1</td>
<td>3.9</td>
<td>4.5</td>
<td>4.3</td>
<td>4.8</td>
<td>4.5</td>
<td>5.7</td>
<td>5.9</td>
<td>5.7</td>
<td>5.6</td>
<td>5.8</td>
<td>5.7</td>
</tr>
<tr>
<td rowspan="2">300:300:300</td>
<td>1:2.5:4</td>
<td>
<bold>21.3</bold>
</td>
<td>
<bold>30.6</bold>
</td>
<td>
<bold>21.3</bold>
</td>
<td>
<bold>31.1</bold>
</td>
<td>
<bold>31.2</bold>
</td>
<td>4.5</td>
<td>4.6</td>
<td>4.5</td>
<td>4.5</td>
<td>4.3</td>
<td>4.5</td>
</tr>
<tr>
<td>1:1:1</td>
<td>4.9</td>
<td>4.7</td>
<td>4.9</td>
<td>5.2</td>
<td>5.2</td>
<td>5.0</td>
<td>5.2</td>
<td>5.0</td>
<td>5.0</td>
<td>4.9</td>
<td>5.0</td>
</tr>
<tr>
<td rowspan="3">120:300:480</td>
<td>4:2.5:1</td>
<td>
<bold>42.7</bold>
</td>
<td>
<bold>25.6</bold>
</td>
<td>
<bold>16.9</bold>
</td>
<td>
<bold>25.4</bold>
</td>
<td>
<bold>25.7</bold>
</td>
<td>4.4</td>
<td>4.6</td>
<td>4.4</td>
<td>4.5</td>
<td>4.2</td>
<td>4.4</td>
</tr>
<tr>
<td>1:2.5:4</td>
<td>7.1</td>
<td>
<bold>33.0</bold>
</td>
<td>
<bold>21.0</bold>
</td>
<td>
<bold>33.3</bold>
</td>
<td>
<bold>33.3</bold>
</td>
<td>4.4</td>
<td>4.5</td>
<td>4.4</td>
<td>4.4</td>
<td>4.2</td>
<td>4.4</td>
</tr>
<tr>
<td>1:1:1</td>
<td>4.7</td>
<td>4.8</td>
<td>4.2</td>
<td>5.0</td>
<td>4.9</td>
<td>4.7</td>
<td>5.0</td>
<td>4.7</td>
<td>4.8</td>
<td>4.6</td>
<td>4.7</td>
</tr>
<tr>
<td rowspan="15">(3,21)</td>
<td rowspan="2">30:30:30</td>
<td>1:2.5:4</td>
<td>
<bold>26.2</bold>
</td>
<td>
<bold>39.1</bold>
</td>
<td>
<bold>25.4</bold>
</td>
<td>
<bold>39.1</bold>
</td>
<td>
<bold>39.1</bold>
</td>
<td>
<bold>8.5</bold>
</td>
<td>
<bold>9.2</bold>
</td>
<td>
<bold>8.5</bold>
</td>
<td>
<bold>8.4</bold>
</td>
<td>
<bold>8.6</bold>
</td>
<td>
<bold>8.3</bold>
</td>
</tr>
<tr>
<td>1:1:1</td>
<td>3.8</td>
<td>4.4</td>
<td>3.7</td>
<td>4.4</td>
<td>4.7</td>
<td>5.2</td>
<td>5.9</td>
<td>5.2</td>
<td>5.0</td>
<td>5.0</td>
<td>5.1</td>
</tr>
<tr>
<td rowspan="3">12:30:48</td>
<td>4:2.5:1</td>
<td>
<bold>50.6</bold>
</td>
<td>
<bold>38.0</bold>
</td>
<td>
<bold>29.3</bold>
</td>
<td>
<bold>38.7</bold>
</td>
<td>
<bold>37.8</bold>
</td>
<td>
<bold>10.5</bold>
</td>
<td>
<bold>13.2</bold>
</td>
<td>
<bold>10.5</bold>
</td>
<td>
<bold>12.3</bold>
</td>
<td>
<bold>12.3</bold>
</td>
<td>
<bold>10.1</bold>
</td>
</tr>
<tr>
<td>1:2.5:4</td>
<td>
<bold>12.0</bold>
</td>
<td>
<bold>33.3</bold>
</td>
<td>
<bold>24.3</bold>
</td>
<td>
<bold>33.2</bold>
</td>
<td>
<bold>33.5</bold>
</td>
<td>5.4</td>
<td>6.5</td>
<td>5.3</td>
<td>5.5</td>
<td>5.5</td>
<td>5.3</td>
</tr>
<tr>
<td>1:1:1</td>
<td>2.9</td>
<td>5.3</td>
<td>4.1</td>
<td>5.7</td>
<td>5.3</td>
<td>5.7</td>
<td>7.3</td>
<td>5.7</td>
<td>5.5</td>
<td>5.6</td>
<td>5.5</td>
</tr>
<tr>
<td rowspan="2">60:60:60</td>
<td>1:2.5:4</td>
<td>
<bold>41.3</bold>
</td>
<td>
<bold>57.8</bold>
</td>
<td>
<bold>41.6</bold>
</td>
<td>
<bold>59.1</bold>
</td>
<td>
<bold>58.7</bold>
</td>
<td>6.8</td>
<td>6.9</td>
<td>6.8</td>
<td>6.7</td>
<td>6.8</td>
<td>6.7</td>
</tr>
<tr>
<td>1:1:1</td>
<td>3.3</td>
<td>4.2</td>
<td>3.5</td>
<td>4.2</td>
<td>4.2</td>
<td>4.7</td>
<td>5.0</td>
<td>4.7</td>
<td>4.7</td>
<td>4.7</td>
<td>4.7</td>
</tr>
<tr>
<td rowspan="3">24:60:96</td>
<td>4:2.5:1</td>
<td>
<bold>57.1</bold>
</td>
<td>
<bold>45.9</bold>
</td>
<td>
<bold>33.5</bold>
</td>
<td>
<bold>46.2</bold>
</td>
<td>
<bold>45.9</bold>
</td>
<td>
<bold>9.3</bold>
</td>
<td>
<bold>10.7</bold>
</td>
<td>
<bold>9.3</bold>
</td>
<td>
<bold>10.0</bold>
</td>
<td>
<bold>10.0</bold>
</td>
<td>
<bold>9.2</bold>
</td>
</tr>
<tr>
<td>1:2.5:4</td>
<td>
<bold>25.7</bold>
</td>
<td>
<bold>59.3</bold>
</td>
<td>
<bold>44.0</bold>
</td>
<td>
<bold>58.9</bold>
</td>
<td>
<bold>59.7</bold>
</td>
<td>5.3</td>
<td>5.8</td>
<td>5.3</td>
<td>5.3</td>
<td>5.3</td>
<td>5.2</td>
</tr>
<tr>
<td>1:1:1</td>
<td>3.9</td>
<td>4.6</td>
<td>3.5</td>
<td>4.6</td>
<td>4.7</td>
<td>5.9</td>
<td>6.4</td>
<td>5.9</td>
<td>5.9</td>
<td>6.0</td>
<td>5.9</td>
</tr>
<tr>
<td rowspan="2">300:300:300</td>
<td>1:2.5:4</td>
<td>
<bold>96.8</bold>
</td>
<td>
<bold>99.6</bold>
</td>
<td>
<bold>91.0</bold>
</td>
<td>
<bold>99.7</bold>
</td>
<td>
<bold>99.7</bold>
</td>
<td>4.4</td>
<td>4.5</td>
<td>4.4</td>
<td>4.4</td>
<td>4.1</td>
<td>4.3</td>
</tr>
<tr>
<td>1:1:1</td>
<td>4.8</td>
<td>4.9</td>
<td>5.3</td>
<td>5.0</td>
<td>5.0</td>
<td>4.5</td>
<td>4.6</td>
<td>4.5</td>
<td>4.4</td>
<td>3.9</td>
<td>4.3</td>
</tr>
<tr>
<td rowspan="3">120:300:480</td>
<td>4:2.5:1</td>
<td>
<bold>97.5</bold>
</td>
<td>
<bold>96.6</bold>
</td>
<td>
<bold>87.0</bold>
</td>
<td>
<bold>95.7</bold>
</td>
<td>
<bold>96.8</bold>
</td>
<td>6.9</td>
<td>6.9</td>
<td>6.7</td>
<td>6.7</td>
<td>6.7</td>
<td>6.7</td>
</tr>
<tr>
<td>1:2.5:4</td>
<td>
<bold>85.8</bold>
</td>
<td>
<bold>99.8</bold>
</td>
<td>
<bold>97.9</bold>
</td>
<td>
<bold>99.8</bold>
</td>
<td>
<bold>99.8</bold>
</td>
<td>5.4</td>
<td>5.5</td>
<td>5.4</td>
<td>5.4</td>
<td>5.0</td>
<td>5.4</td>
</tr>
<tr>
<td>1:1:1</td>
<td>4.1</td>
<td>4.4</td>
<td>3.8</td>
<td>4.3</td>
<td>4.4</td>
<td>5.9</td>
<td>5.9</td>
<td>5.9</td>
<td>5.9</td>
<td>5.4</td>
<td>5.9</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1076998610396897">
<p>
<italic>Note:</italic> Bolded values indicate rejection rates outside of Bradley’s robustness interval.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table3-1076998610396897" position="float">
<label>Table 3.</label>
<caption>
<p>Type I Error Rates (%) for .05-Level Tests When K = 4, Under Asymmetric Nonnormal Distributions</p>
</caption>
<graphic alternate-form-of="table3-1076998610396897" xlink:href="10.3102_1076998610396897-table3.tif"/>
<table>
<thead>
<tr>
<th>Distribution</th>
<th>
<italic>n</italic>
<sub>1</sub>:<italic>n</italic>
<sub>2</sub>:<italic>n</italic>
<sub>3</sub>:<italic>n</italic>
<sub>4</sub>
</th>
<th>
<italic>σ</italic>
<sub>1</sub>:<italic>σ</italic>
<sub>2</sub>:<italic>σ</italic>
<sub>3</sub>:<italic>σ</italic>
<sub>4</sub>
</th>
<th>
<italic>F</italic>
</th>
<th>W</th>
<th>BF</th>
<th>AG</th>
<th>U</th>
<th>
<italic>T</italic>
<sub>ML</sub>
</th>
<th>
<italic>T</italic>
<sub>ADF</sub>
</th>
<th>
<italic>T</italic>
<sub>SB</sub>
</th>
<th>
<italic>T</italic>
<sub>YB1</sub>
</th>
<th>
<italic>T</italic>
<sub>YB2</sub>
</th>
<th>
<italic>T</italic>
<sub>BC</sub>
</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="15">(.75, 0)</td>
<td rowspan="2">30:30:30:30</td>
<td>1:2:3:4</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>9.2</bold>
</td>
<td>
<bold>8.5</bold>
</td>
<td>
<bold>9.2</bold>
</td>
<td>
<bold>9.3</bold>
</td>
<td>5.8</td>
<td>7.2</td>
<td>5.8</td>
<td>5.8</td>
<td>5.8</td>
<td>5.7</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.7</td>
<td>5.0</td>
<td>4.5</td>
<td>5.2</td>
<td>5.5</td>
<td>6.2</td>
<td>7.2</td>
<td>6.2</td>
<td>6.1</td>
<td>6.1</td>
<td>5.9</td>
</tr>
<tr>
<td rowspan="3">10:20:40:50</td>
<td>4:3:2:1</td>
<td>
<bold>23.6</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>7.0</td>
<td>
<bold>9.2</bold>
</td>
<td>
<bold>8.8</bold>
</td>
<td>5.6</td>
<td>7.4</td>
<td>5.6</td>
<td>6.6</td>
<td>6.6</td>
<td>5.4</td>
</tr>
<tr>
<td>1:2:3:4</td>
<td>
<bold>2.1</bold>
</td>
<td>
<bold>7.7</bold>
</td>
<td>
<bold>8.5</bold>
</td>
<td>7.0</td>
<td>
<bold>7.9</bold>
</td>
<td>5.6</td>
<td>6.8</td>
<td>5.6</td>
<td>5.7</td>
<td>5.7</td>
<td>5.5</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.0</td>
<td>6.5</td>
<td>4.4</td>
<td>6.1</td>
<td>6.3</td>
<td>5.5</td>
<td>7.9</td>
<td>5.5</td>
<td>6.2</td>
<td>6.2</td>
<td>5.4</td>
</tr>
<tr>
<td rowspan="2">60:60:60:60</td>
<td>1:2:3:4</td>
<td>
<bold>8.0</bold>
</td>
<td>
<bold>11.7</bold>
</td>
<td>
<bold>7.7</bold>
</td>
<td>
<bold>12.0</bold>
</td>
<td>
<bold>12.0</bold>
</td>
<td>4.8</td>
<td>5.6</td>
<td>4.8</td>
<td>4.7</td>
<td>4.7</td>
<td>4.8</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>3.9</td>
<td>4.7</td>
<td>3.9</td>
<td>4.7</td>
<td>4.7</td>
<td>5.5</td>
<td>5.9</td>
<td>5.5</td>
<td>5.4</td>
<td>5.4</td>
<td>5.1</td>
</tr>
<tr>
<td rowspan="3">20:40:80:100</td>
<td>4:3:2:1</td>
<td>
<bold>28.7</bold>
</td>
<td>
<bold>11.7</bold>
</td>
<td>
<bold>10.2</bold>
</td>
<td>
<bold>11.3</bold>
</td>
<td>
<bold>11.7</bold>
</td>
<td>5.5</td>
<td>7.1</td>
<td>5.5</td>
<td>6.9</td>
<td>6.9</td>
<td>5.4</td>
</tr>
<tr>
<td>1:2:3:4</td>
<td>2.9</td>
<td>
<bold>11.0</bold>
</td>
<td>
<bold>10.6</bold>
</td>
<td>
<bold>11.2</bold>
</td>
<td>
<bold>11.3</bold>
</td>
<td>5.7</td>
<td>6.4</td>
<td>5.7</td>
<td>5.7</td>
<td>5.7</td>
<td>5.6</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.5</td>
<td>6.7</td>
<td>4.1</td>
<td>6.6</td>
<td>6.9</td>
<td>6.0</td>
<td>6.7</td>
<td>6.0</td>
<td>6.2</td>
<td>6.2</td>
<td>6.0</td>
</tr>
<tr>
<td rowspan="2">300:300:300:300</td>
<td>1:2:3:4</td>
<td>
<bold>24.3</bold>
</td>
<td>
<bold>32.9</bold>
</td>
<td>
<bold>24.4</bold>
</td>
<td>
<bold>34.5</bold>
</td>
<td>
<bold>34.5</bold>
</td>
<td>5.0</td>
<td>5.5</td>
<td>5.0</td>
<td>5.2</td>
<td>5.2</td>
<td>5.0</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.3</td>
<td>4.3</td>
<td>4.3</td>
<td>4.5</td>
<td>4.5</td>
<td>5.7</td>
<td>5.7</td>
<td>5.7</td>
<td>5.7</td>
<td>5.7</td>
<td>5.7</td>
</tr>
<tr>
<td rowspan="3">100:200:400:500</td>
<td>4:3:2:1</td>
<td>
<bold>45.3</bold>
</td>
<td>
<bold>28.4</bold>
</td>
<td>
<bold>17.2</bold>
</td>
<td>
<bold>29.2</bold>
</td>
<td>
<bold>29.0</bold>
</td>
<td>6.1</td>
<td>6.4</td>
<td>6.1</td>
<td>6.2</td>
<td>6.2</td>
<td>6.1</td>
</tr>
<tr>
<td>1:2:3:4</td>
<td>5.4</td>
<td>
<bold>29.9</bold>
</td>
<td>
<bold>17.3</bold>
</td>
<td>
<bold>31.0</bold>
</td>
<td>
<bold>31.5</bold>
</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
<td>6.0</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.2</td>
<td>4.5</td>
<td>4.3</td>
<td>4.6</td>
<td>4.7</td>
<td>5.9</td>
<td>6.1</td>
<td>5.9</td>
<td>6.0</td>
<td>6.0</td>
<td>5.9</td>
</tr>
<tr>
<td rowspan="15">(3, 21)</td>
<td rowspan="2">30:30:30:30</td>
<td>1:2:3:4</td>
<td>
<bold>26.4</bold>
</td>
<td>
<bold>41.2</bold>
</td>
<td>
<bold>25.8</bold>
</td>
<td>
<bold>41.1</bold>
</td>
<td>
<bold>41.6</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>10.7</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>9.3</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>9.0</bold>
</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>3.9</td>
<td>3.5</td>
<td>3.8</td>
<td>4.2</td>
<td>4.0</td>
<td>6.0</td>
<td>6.9</td>
<td>6.0</td>
<td>5.7</td>
<td>5.6</td>
<td>5.7</td>
</tr>
<tr>
<td rowspan="3">10:20:40:50</td>
<td>4:3:2:1</td>
<td>
<bold>39.8</bold>
</td>
<td>
<bold>34.9</bold>
</td>
<td>
<bold>17.0</bold>
</td>
<td>
<bold>35.2</bold>
</td>
<td>
<bold>33.9</bold>
</td>
<td>
<bold>10.4</bold>
</td>
<td>
<bold>13.9</bold>
</td>
<td>
<bold>10.4</bold>
</td>
<td>
<bold>12.5</bold>
</td>
<td>
<bold>12.4</bold>
</td>
<td>
<bold>10.2</bold>
</td>
</tr>
<tr>
<td>1:2:3:4</td>
<td>
<bold>8.9</bold>
</td>
<td>
<bold>33.4</bold>
</td>
<td>
<bold>20.5</bold>
</td>
<td>
<bold>32.6</bold>
</td>
<td>
<bold>33.3</bold>
</td>
<td>5.4</td>
<td>6.8</td>
<td>5.4</td>
<td>5.3</td>
<td>5.2</td>
<td>5.1</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.7</td>
<td>5.0</td>
<td>3.2</td>
<td>5.2</td>
<td>4.8</td>
<td>6.3</td>
<td>
<bold>8.2</bold>
</td>
<td>6.3</td>
<td>7.1</td>
<td>6.9</td>
<td>5.9</td>
</tr>
<tr>
<td rowspan="2">60:60:60:60</td>
<td>1:2:3:4</td>
<td>
<bold>39.2</bold>
</td>
<td>
<bold>61.6</bold>
</td>
<td>
<bold>38.8</bold>
</td>
<td>
<bold>62.3</bold>
</td>
<td>
<bold>62.0</bold>
</td>
<td>6.8</td>
<td>7.1</td>
<td>6.8</td>
<td>6.8</td>
<td>6.8</td>
<td>6.8</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>3.4</td>
<td>4.0</td>
<td>3.9</td>
<td>4.1</td>
<td>4.0</td>
<td>5.1</td>
<td>5.9</td>
<td>5.1</td>
<td>5.1</td>
<td>5.1</td>
<td>5.1</td>
</tr>
<tr>
<td rowspan="3">20:40:80:100</td>
<td>4:3:2:1</td>
<td>
<bold>57.4</bold>
</td>
<td>
<bold>52.6</bold>
</td>
<td>
<bold>28.7</bold>
</td>
<td>
<bold>54.2</bold>
</td>
<td>
<bold>52.9</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>11.4</bold>
</td>
<td>
<bold>9.1</bold>
</td>
<td>
<bold>10.0</bold>
</td>
<td>
<bold>9.9</bold>
</td>
<td>
<bold>8.9</bold>
</td>
</tr>
<tr>
<td>1:2:3:4</td>
<td>
<bold>17.4</bold>
</td>
<td>
<bold>55.3</bold>
</td>
<td>
<bold>34.6</bold>
</td>
<td>
<bold>55.0</bold>
</td>
<td>
<bold>56.4</bold>
</td>
<td>6.2</td>
<td>6.7</td>
<td>6.2</td>
<td>6.2</td>
<td>6.2</td>
<td>6.1</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.8</td>
<td>6.1</td>
<td>2.8</td>
<td>6.1</td>
<td>6.2</td>
<td>6.6</td>
<td>
<bold>7.6</bold>
</td>
<td>6.6</td>
<td>6.8</td>
<td>6.7</td>
<td>6.5</td>
</tr>
<tr>
<td rowspan="2">300:300:300:300</td>
<td>1:2:3:4</td>
<td>
<bold>98.2</bold>
</td>
<td>
<bold>100.0</bold>
</td>
<td>
<bold>98.2</bold>
</td>
<td>
<bold>100.0</bold>
</td>
<td>
<bold>100.0</bold>
</td>
<td>5.1</td>
<td>5.2</td>
<td>5.1</td>
<td>5.1</td>
<td>5.0</td>
<td>5.1</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.0</td>
<td>4.1</td>
<td>4.0</td>
<td>4.7</td>
<td>4.5</td>
<td>5.3</td>
<td>5.5</td>
<td>5.3</td>
<td>5.3</td>
<td>4.9</td>
<td>5.3</td>
</tr>
<tr>
<td rowspan="3">100:200:400:500</td>
<td>4:3:2:1</td>
<td>
<bold>98.3</bold>
</td>
<td>
<bold>98.5</bold>
</td>
<td>
<bold>87.7</bold>
</td>
<td>
<bold>98.7</bold>
</td>
<td>
<bold>98.6</bold>
</td>
<td>7.5</td>
<td>7.5</td>
<td>7.5</td>
<td>7.5</td>
<td>7.1</td>
<td>7.5</td>
</tr>
<tr>
<td>1:2:3:4</td>
<td>
<bold>84.9</bold>
</td>
<td>
<bold>99.9</bold>
</td>
<td>
<bold>97.3</bold>
</td>
<td>
<bold>100.0</bold>
</td>
<td>
<bold>100.0</bold>
</td>
<td>5.1</td>
<td>5.3</td>
<td>5.1</td>
<td>5.2</td>
<td>5.0</td>
<td>5.1</td>
</tr>
<tr>
<td>1:1:1:1</td>
<td>4.2</td>
<td>4.8</td>
<td>4.7</td>
<td>4.9</td>
<td>4.9</td>
<td>7.1</td>
<td>7.1</td>
<td>7.1</td>
<td>71.0</td>
<td>6.7</td>
<td>7.0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-1076998610396897">
<p>
<italic>Note:</italic> Bolded values indicate rejection rates outside of Bradley’s robustness interval.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section21-1076998610396897">
<title>Asymmetric nonnormal distribution (3, 21)</title>
<p>With <italic>K</italic> = 2 groups, the results for this asymmetric nonnormal distribution yielded many more nonrobust cells. Notwithstanding, the advantage of the RMM approaches over the Welch-type statistics was still quite clear. The Welch-type statistics provided many non-robust cells across sample sizes, including highly inflated Type I error rates for cells of heterogeneous variances with moderate and large sample sizes, inflated Type I error rates for negative conditions with small sample size, and Type I error rates below the robustness interval for homogeneous variances with small sample sizes. The only robust cells provided by the Welch-type statistics when <italic>K</italic> = 2 were conditions when the variances were equal with moderate or large sample sizes. On the other hand, the performance of RMM approaches when <italic>K</italic> = 2 was far superior, especially with increases in sample size. Even with these extreme nonnormal data, the RMM approaches provided robust cells at the conditions of homogeneous variance across sample sizes, conditions of balanced groups with moderate or large sample sizes, and under all positive conditions with moderate or large sample sizes. For the RMM approaches, the Type I error rates were inflated under negative conditions when sample sizes were moderate and were sporadically outside the robustness interval when sample sizes were small and variances were heterogeneous. Overall, when <italic>K</italic> = 2 the adjusted <italic>T</italic>
<sub>ADF</sub> test statistic, <italic>T</italic>
<sub>YB2</sub>, provided the most robust cells among the examined methods, followed by the <italic>T</italic>
<sub>YB1</sub>, <italic>T</italic>
<sub>BC</sub>, <italic>T</italic>
<sub>ML</sub>, and <italic>T</italic>
<sub>SB</sub> test statistics. Select results are displayed in <xref ref-type="table" rid="table1-1076998610396897">Table 1</xref>.</p>
<p>When <italic>K</italic> = 3 and <italic>K</italic> = 4, all ANOVA-based methods delivered inflated Type I error rates at all negative conditions, positive conditions, and conditions with equal sample sizes but heterogeneous variance. That is, the ANOVA-based methods were affected by the heterogeneous group variances, delivering extremely inflated Type I error rates across all sample size conditions. The RMM approaches, on the other hand, provided only slightly inflated Type I error rates in the negative conditions when sample sizes were small or moderate, or under conditions with balanced small groups but heterogeneous variances. Thus, the RMM approaches seemed to be a much better choice in terms of controlling the Type I error rates across conditions when the data distributions were asymmetric nonnormal. Select results are displayed in <xref ref-type="table" rid="table2-1076998610396897">Tables 2</xref> and <xref ref-type="table" rid="table3-1076998610396897">3</xref>.</p>
</sec>
</sec>
<sec id="section22-1076998610396897">
<title>Empirical Power</title>
<p>Based on the results of Type I error rates, all test statistics were studied for the normal distribution (0, 0) and the elliptically symmetric nonnormal distribution (0,3) under conditions of variance, sample size, number of groups, and effect size conditions described previously. For the asymmetric nonnormal distributions, (.75, 0) and (3, 21), only the six RMM approaches were included in the power analysis due to the unsatisfactory performance of the ANOVA-based methods. Results are summarized below by distribution. Because of the volume of output, power tables are not provided here in but are available from the authors upon request.</p>
<sec id="section24-1076998610396897">
<title>Normal distribution (0, 0)</title>
<p>The RMM test statistics generally yielded higher power estimates than the ANOVA-based methods across sample sizes, variance ratios, number of groups, and the levels of significance. The advantage, however, did decrease with increased sample size. Among all of the methods, the <italic>T</italic>
<sub>ADF</sub> test statistic generally provided the best empirical power estimates, followed by the RMM test statistics of <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub>. The rest of the RMM test statistics also produced reasonably good empirical power estimates. Among all the ANOVA-based methods, the Alexander–Govern statistic often yielded the highest value for the empirical power estimates across most of the conditions; conversely, the Brown–Forsythe statistic usually provided much lower empirical power estimates across all conditions relative to the rest of the Welch-type statistics. The proposed RMM methods and the Welch-type statistics all perform comparatively much better than the ANOVA <italic>F</italic> test, which generally yielded much smaller empirical power estimates at most of the conditions when the effect size was small and only provided comparatively good power estimates with unequal sample sizes but homogenous variances. When the effect size increased to be large, the empirical power estimates were all above 90%, approaching 100%.</p>
</sec>
<sec id="section25-1076998610396897">
<title>Elliptically symmetric nonnormal distribution (0, 3)</title>
<p>The ANOVA <italic>F</italic> test provided comparable empirical power estimates for equal sample sizes across variance ratios, but much lower values under positive conditions (recall that because the ANOVA <italic>F</italic> test usually provided inflated Type I error rates under negative conditions, those power estimates were not generated.) The Welch-type statistics and the RMM test statistics yielded much higher power estimates under the positive conditions. When the effect size was small, statistics by Welch, Alexander and Govern, and James tended to provide slightly higher empirical power estimates than the RMM test statistics, especially for large sample size; however, the difference was typically less than 5%. Among all methods, the <italic>T</italic>
<sub>ADF</sub> statistic and the Alexander–Govern statistic provided the best empirical power estimates, followed by statistics of Welch and James and the RMM test statistics <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub>. Once again, the Brown–Forsythe statistic usually provided considerably lower empirical power estimates across all conditions.</p>
</sec>
<sec id="section26-1076998610396897">
<title>Asymmetric nonnormal distributions (.75, 0) and (3, 21)</title>
<p>Generally, the power estimates for the six RMM statistics behaved as expected, increasing with increased sample size and being higher in positive conditions than in negative conditions. The power estimates provided by the RMM methods were all quite close to each other across conditions, with the <italic>T</italic>
<sub>ADF</sub> test statistic yielding slightly higher power estimates and the <italic>T</italic>
<sub>BC</sub> test statistic yielding slightly lower power estimate values. However, these differences were typically less than 2%.</p>
</sec>
</sec>
</sec>
<sec id="section27-1076998610396897">
<title>Discussion and Conclusion</title>
<p>Methods such as ANOVA have been used for the last half century by researchers in many fields for inference in experimental, quasi-experimental, and nonexperimental designs. However, while foundational to the advancement of research in many fields, these methods rest upon assumptions that are frequently not met in practice. As a result, the inferences they suggest may mistakenly declare population differences (e.g., treatment effects) that do not exist, and perhaps worse, fail to detect those effects that do exist. Addressing homogeneity of variance violations has traditionally involved the incorporation of weights and degree of freedom adjustments, while normality violations have been met with suggestions to eliminate offending portions of sample data. More desirable would seem to be a paradigm that makes neither assumption in the first place; to this end, we proposed the RMM approaches and sought to understand their behavior empirically.</p>
<p>Based on our simulation results, the RMM approaches did provide comparatively robust Type I error rates across different distributional shapes and different sample sizes and variance conditions. When the distribution was normal, the Type I error rates for both the Welch-type statistics and the RMM test statistics were robust across all conditions of sample size and variance ratios when <italic>K</italic> = 3 and <italic>K</italic> = 4 groups; but when <italic>K</italic> = 2, the Welch-type statistics tend to provide Type I error rates below the robustness interval with small sample sizes under positive conditions. Thus, the RMM test statistics controlled the Type I error rates better than the Welch-type statistics when <italic>K</italic> = 2 and the distribution was normal. When the distribution was elliptically symmetric nonnormal, results were similar to those from the normal distribution, indicating that the change of kurtosis does not affect the control of Type I error rates much for either the Welch or RMM test statistics. When the distribution was asymmetric nonnormal, the Welch-type statistics were not robust with heterogeneous variances; RMM approaches, on the other hand, provided much better control of Type I error rates, especially with increased sample size, thereby being much less affected by skewness in the data. These results for Type I error rate, along with RMM methods' general comparability or superiority in terms of relative power, suggests fairly clearly that the RMM approaches outperform the best that the ANOVA type statistics have to offer for between-subjects designs.</p>
<p>Among the RMM methods, both ADF- and ML-based statistics performed well overall, generally providing control over Type I error rate and comparatively high power even under the extreme nonnormal distribution with skewness and kurtosis of (3, 21). Most worthy of recommendation among the RMM test statistics are <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub>, which seemed to yield slightly better Type I error rates than other RMM statistics by providing robust cells across most conditions, as well as comparatively high values for power estimates.</p>
<p>Although the current investigation could not study all possible conditions, it was fairly comprehensive in conditions addressed relative to previous investigations in the ANOVA literature. Hence we feel comfortable that the RMM methods, with the <italic>T</italic>
<sub>YB1</sub> and <italic>T</italic>
<sub>YB2</sub> statistics as their leading representatives, have tremendous potential as an alternative to the more traditional and assumption-laden test statistics, and do deserve expanded investigation. First, the methods themselves may be examined for further improvement, such as with the incorporation of a bootstrap component that may further enhance distributional robustness (<xref ref-type="bibr" rid="bibr6-1076998610396897">Bollen &amp; Stine, 1992</xref>; <xref ref-type="bibr" rid="bibr19-1076998610396897">Ichikawa &amp; Konishi, 1995</xref>). Second, the RMM approaches may be expanded to incorporate a wider class of test statistics to further improve the accuracy of inferences, such as the three likelihood-based methods that are explicitly proposed to handle nonnormal missing data (<xref ref-type="bibr" rid="bibr44-1076998610396897">Yuan &amp; Bentler, 2000</xref>), and the two simple approximations to the distributions of quadratic forms that are widely used in mean and covariance structure analysis (<xref ref-type="bibr" rid="bibr45-1076998610396897">Yuan &amp; Bentler, 2010</xref>). Third, the RMM approaches can be extended to repeated measures (within-subjects) scenarios, thereby accommodating designs with measures over time or conditions, or across dependent groups. Whereas traditional analytical methods for these designs rest on the assumption of compound symmetry, RMM approaches would make no constraints on the groups' correlational or variance structure and thereby would seem to offer great potential in such designs as well. Future research in this area is eagerly anticipated.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-1076998610396897">
<title>Appendix</title>
<sec id="section28-1076998610396897">
<title>Sample EQS Programming Code for <italic>T<sub>ML</sub></italic>, <italic>T<sub>ADF</sub></italic>, and <italic>T<sub>SB</sub></italic></title>
<p>
<bold>[</bold>
<bold>
<italic>k</italic>
</bold>
<bold>]</bold> = number of groups</p>
<p>
<bold>[</bold>
<bold>
<italic>n</italic>
</bold>
<bold>1]</bold> = sample size for Group 1</p>
<p>
<bold>[</bold>
<bold>
<italic>nk</italic>
</bold>
<bold>]</bold> = sample size for Group <italic>k</italic>
</p>
<p>
<bold>[</bold>
<bold>
<italic>File_1</italic>
</bold>
<bold>]</bold> = data file for Group 1 (e.g., d:\EQS61\rt1.txt)</p>
<p>
<bold>[</bold>
<bold>
<italic>File_k</italic>
</bold>
<bold>]</bold> = data file for Group <italic>k</italic> (e.g., d:\EQS61\rtk.txt)</p>
<p>
<bold>[</bold>
<bold>
<italic>Estimation</italic>
</bold>
<bold>]</bold> = estimation method.  ML yields <italic>T</italic>
<sub>ML</sub>; AGLS yields <italic>T</italic>
<sub>ADF</sub>;</p>
<p>                 ML,ROBUST yields <italic>T</italic>
<sub>SB</sub>.</p>
<p>/SPECIFICATIONS</p>
<p>VAR=1; cases= [<italic>n</italic>1]; ME=[<italic>Estimation</italic>]; MA=RAW; DATA=‘[<italic>File_1</italic>]’;</p>
<p>ANAL=MOMENT; GROUPS=[<italic>k</italic>];</p>
<p>/EQUATIONS</p>
<p>V1 = *V999 + 1.000 E1;</p>
<p>/VAR</p>
<p>E1= *;</p>
<p>/PRINT</p>
<p>FIT=ALL; COV=YES;</p>
<p>/END</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>(Repeat the first part for each of the middle groups.)</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>/SPECIFICATIONS</p>
<p>VAR=1; cases= [<italic>nk</italic>]; ME= [<italic>Estimation</italic>]; MA= RAW; DATA=’[<italic>File</italic>_<italic>k</italic>]’;</p>
<p>ANAL=MOMENT;</p>
<p>/EQUATIONS</p>
<p>V1 = *V999 + 1.000 E1;</p>
<p>/VAR</p>
<p>E1= *;</p>
<p>/Constraints</p>
<p>(1,V1,V999)=…=([<italic>k</italic>],V1,V999);</p>
<p>/PRINT</p>
<p>FIT=ALL; COV=YES;</p>
<p>/END</p>
</sec>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Alexander</surname>
<given-names>R. A.</given-names>
</name>
<name>
<surname>Govern</surname>
<given-names>D. M.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>A new and simpler approximation for ANOVA under variance heterogeneity</article-title>. <source>Journal of Educational Statistics</source>, <volume>19</volume>, <fpage>91</fpage>–<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr2-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Algina</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Oshima</surname>
<given-names>T. C.</given-names>
</name>
<name>
<surname>Lin</surname>
<given-names>W.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Type I error rates for the Welch’s test and James's second-order test under nonnormality and inequality of variance when there are two groups</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>19</volume>, <fpage>275</fpage>–<lpage>291</lpage>.</citation>
</ref>
<ref id="bibr3-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bartlett</surname>
<given-names>M. S.</given-names>
</name>
</person-group> (<year>1950</year>). <article-title>Tests of significance in factor analysis</article-title>. <source>British Journal of Psychology: Statistical Section</source>, <volume>3</volume>, <fpage>77</fpage>–<lpage>85</lpage>.</citation>
</ref>
<ref id="bibr4-1076998610396897">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bentler</surname>
<given-names>P. M.</given-names>
</name>
</person-group> (<year>2004</year>). <source>EQS structural equations program manual</source>. <publisher-loc>Encino, CA</publisher-loc>: <publisher-name>Multivariate Software</publisher-name>.</citation>
</ref>
<ref id="bibr5-1076998610396897">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bollen</surname>
<given-names>K. A.</given-names>
</name>
</person-group> (<year>1989</year>). <source>Structural equations with latent variables</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr6-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bollen</surname>
<given-names>K. A.</given-names>
</name>
<name>
<surname>Stine</surname>
<given-names>R. A.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>Bootstrapping goodness-of-fit measures in structural equation models</article-title>. <source>Sociological Methods &amp; Research</source>, <volume>21</volume>, <fpage>205</fpage>–<lpage>229</lpage>.</citation>
</ref>
<ref id="bibr7-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bradley</surname>
<given-names>J. V.</given-names>
</name>
</person-group> (<year>1978</year>). <article-title>Robustness?</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>34</volume>, <fpage>144</fpage>–<lpage>152</lpage>.</citation>
</ref>
<ref id="bibr8-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brown</surname>
<given-names>M. B.</given-names>
</name>
<name>
<surname>Forsythe</surname>
<given-names>A. B.</given-names>
</name>
</person-group> (<year>1974</year>). <article-title>The small sample behavior of some statistics which test the equality of means</article-title>. <source>Technometrics</source>, <volume>16</volume>, <fpage>129</fpage>–<lpage>132</lpage>.</citation>
</ref>
<ref id="bibr9-1076998610396897">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Browne</surname>
<given-names>M. W.</given-names>
</name>
</person-group> (<year>1982</year>). <article-title>Covariance structures</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Hawkins</surname>
<given-names>D. M.</given-names>
</name>
</person-group> (Ed.), <source>Topics in applied multivariate analysis</source> (pp. <fpage>72</fpage>–<lpage>141</lpage>). <publisher-loc>Cambridge, England</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Browne</surname>
<given-names>M. W.</given-names>
</name>
</person-group> (<year>1984</year>). <article-title>Asymptotic distribution free methods in the analysis of covariance structures</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>37</volume>, <fpage>127</fpage>–<lpage>141</lpage>.</citation>
</ref>
<ref id="bibr11-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Clinch</surname>
<given-names>J. J.</given-names>
</name>
<name>
<surname>Keselman</surname>
<given-names>H. J.</given-names>
</name>
</person-group> (<year>1982</year>). <article-title>Parametric alternatives to the analysis of variance</article-title>. <source>Journal of Educational Statistics</source>, <volume>7</volume>, <fpage>207</fpage>–<lpage>214</lpage>.</citation>
</ref>
<ref id="bibr12-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Curran</surname>
<given-names>P. J.</given-names>
</name>
<name>
<surname>West</surname>
<given-names>S. G.</given-names>
</name>
<name>
<surname>Finch</surname>
<given-names>J. F.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>The robustness of test statistics to nonnormality and specification error in confirmatory factor analysis</article-title>. <source>Psychological Methods</source>, <volume>1</volume>, <fpage>16</fpage>–<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr13-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fleishman</surname>
<given-names>A. I.</given-names>
</name>
</person-group> (<year>1978</year>). <article-title>A method for simulating non-normal distributions</article-title>. <source>Psychometrika</source>, <volume>43</volume>, <fpage>521</fpage>–<lpage>532</lpage>.</citation>
</ref>
<ref id="bibr14-1076998610396897">
<citation citation-type="confproc">
<person-group person-group-type="author"><name><surname>Fouladi</surname><given-names>R. T.</given-names></name></person-group> (<year>1999</year>, <month>April</month>). <source>Model fit in covariance structure analysis under small sample conditions—Modified maximum likelihood and asymptotically distribution free generalized least squares procedures</source>. <conf-name>Paper presented at the annual meeting of the American Educational Research Association</conf-name>, <conf-loc>Montreal, Canada</conf-loc>.
</citation>
</ref>
<ref id="bibr15-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Glass</surname>
<given-names>G. V.</given-names>
</name>
<name>
<surname>Peckham</surname>
<given-names>P. D.</given-names>
</name>
<name>
<surname>Sanders</surname>
<given-names>J. R.</given-names>
</name>
</person-group> (<year>1972</year>). <article-title>Consequences of failure to meet assumptions underlying the analysis of variance and covariance</article-title>. <source>Review of Educational Research</source>, <volume>42</volume>, <fpage>237</fpage>–<lpage>288</lpage>.</citation>
</ref>
<ref id="bibr16-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hancock</surname>
<given-names>G. R.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Fortune cookies, measurement error, and experimental design</article-title>. <source>Journal of Modern Applied Statistical Methods</source>, <volume>2</volume>, <fpage>293</fpage>–<lpage>305</lpage>.</citation>
</ref>
<ref id="bibr17-1076998610396897">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Hancock</surname>
<given-names>G. R.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Experimental, quasi-experimental, and nonexperimental design and analysis with latent variables</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Kaplan</surname>
<given-names>D.</given-names>
</name>
</person-group> (Ed.), <source>The SAGE handbook of quantitative methodology for the social sciences <italic>(pp. 317–334)</italic>
</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr18-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author"><name><surname>Hsiung</surname><given-names>T.</given-names></name><name><surname>Olejnik</surname><given-names>S.</given-names></name></person-group> (<year>1996</year>). <article-title>Type I error rates and statistical power for the James second-order test and the univariate <italic>F</italic> test in two-way fixed-effects ANOVA models under heteroscedasticity and/or nonnormality</article-title>. <source>The Journal of Experimental Education</source>, <volume>65</volume>, <fpage>57</fpage>–<lpage>71</lpage>.
</citation>
</ref>
<ref id="bibr19-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ichikawa</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Konishi</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Application of the bootstrap methods in factor analysis</article-title>. <source>Psychometrika</source>, <volume>60</volume>, <fpage>77</fpage>–<lpage>93</lpage>.</citation>
</ref>
<ref id="bibr20-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>James</surname>
<given-names>G. S.</given-names>
</name>
</person-group> (<year>1951</year>). <article-title>The comparison of several groups of observations when the ratios of the population variances are unknown</article-title>. <source>Biometrika</source>, <volume>38</volume>, <fpage>324</fpage>–<lpage>329</lpage>.</citation>
</ref>
<ref id="bibr21-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Keselman</surname>
<given-names>H. J.</given-names>
</name>
<name>
<surname>Algina</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wilcox</surname>
<given-names>R. R.</given-names>
</name>
<name>
<surname>Kowalchuk</surname>
<given-names>R. K.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Testing repeated measures hypotheses when covariance matrices are heterogeneous: Revisiting the robustness of the Welch-James test again</article-title>. <source>Educational and Psychological Measurement</source>, <volume>60</volume>, <fpage>925</fpage>–<lpage>938</lpage>.</citation>
</ref>
<ref id="bibr22-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Keselman</surname>
<given-names>H. J.</given-names>
</name>
<name>
<surname>Kowalchuk</surname>
<given-names>R. K.</given-names>
</name>
<name>
<surname>Lix</surname>
<given-names>L. M.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Robust nonorthogonal analyses revisited: An update based on trimmed means</article-title>. <source>Psychometrika</source>, <volume>63</volume>, <fpage>145</fpage>–<lpage>163</lpage>.</citation>
</ref>
<ref id="bibr23-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lix</surname>
<given-names>L. M.</given-names>
</name>
<name>
<surname>Keselman</surname>
<given-names>H. J.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>To trim or not to trim: Tests of mean equality under heteroscedasticity and nonnormality</article-title>. <source>Educational and Psychological Measurement</source>, <volume>58</volume>, <fpage>409</fpage>–<lpage>429</lpage>.</citation>
</ref>
<ref id="bibr24-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lix</surname>
<given-names>L. M.</given-names>
</name>
<name>
<surname>Keselman</surname>
<given-names>J. C.</given-names>
</name>
<name>
<surname>Keselman</surname>
<given-names>H. J.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Consequences of assumption violations revisited: A quantitative review of alternatives to the one-way analysis of variance <italic>F</italic> test</article-title>. <source>Review of Educational Research</source>, <volume>66</volume>, <fpage>579</fpage>–<lpage>619</lpage>.</citation>
</ref>
<ref id="bibr25-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Muthén</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1989</year>). <article-title>Multiple-group structural modeling with non-normal continuous variables</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>42</volume>, <fpage>55</fpage>–<lpage>62</lpage>.</citation>
</ref>
<ref id="bibr26-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nevitt</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Hancock</surname>
<given-names>G. R.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>PWRCOEFF &amp; NNORMULT: A set of programs for simulating multivariate nonnormal data</article-title>. <source>Applied Psychological Measurement</source>, <volume>23</volume>, <fpage>54</fpage>.</citation>
</ref>
<ref id="bibr27-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nevitt</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Hancock</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Evaluating small sample approaches for model test statistics in structural equation modeling</article-title>. <source>Multivariate Behavioral Research</source>, <volume>39</volume>, <fpage>439</fpage>–<lpage>478</lpage>.</citation>
</ref>
<ref id="bibr28-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Oshima</surname>
<given-names>T. C.</given-names>
</name>
<name>
<surname>Algina</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>Type I error rates for James's second-order test and Wilcoxon’s Hm test under heteroscedasticity and non-normality</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>45</volume>, <fpage>255</fpage>–<lpage>263</lpage>.</citation>
</ref>
<ref id="bibr29-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Overall</surname>
<given-names>J. E.</given-names>
</name>
<name>
<surname>Atlas</surname>
<given-names>R. S.</given-names>
</name>
<name>
<surname>Gibson</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Tests that are robust against variance heterogeneity in <italic>k</italic> × 2 designs with unequal cell frequencies</article-title>. <source>Psychological Reports</source>, <volume>76</volume>, <fpage>1011</fpage>–<lpage>1017</lpage>.</citation>
</ref>
<ref id="bibr30-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ramberg</surname>
<given-names>J. S.</given-names>
</name>
<name>
<surname>Schmeiser</surname>
<given-names>B. W.</given-names>
</name>
</person-group> (<year>1974</year>). <article-title>An approximate method for generating asymmetric random variables</article-title>. <source>Communications of the ACM</source>, <volume>17</volume>, <fpage>78</fpage>–<lpage>82</lpage>.</citation>
</ref>
<ref id="bibr31-1076998610396897">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>SAS Institute</surname>
<given-names>Inc.</given-names>
</name>
</person-group> (<year>2008</year>). <source>SAS/IML® 9.2 User’s Guide</source>. <publisher-loc>Cary, NC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr32-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Satorra</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>Asymptotic robust inferences in the analysis of mean and covariance structures</article-title>. <source>Sociological Methodology</source>, <volume>22</volume>, <fpage>249</fpage>–<lpage>278</lpage>.</citation>
</ref>
<ref id="bibr33-1076998610396897">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Satorra</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Bentler</surname>
<given-names>P. M.</given-names>
</name>
</person-group> (<year>1988</year>). <article-title>Scaling corrections for chi-square statistics in covariance structure analysis</article-title>. In <source>American Statistical Association 1988 Proceedings of the Business and Economics Sections</source> (pp. <fpage>308</fpage>–<lpage>313</lpage>). <publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>American Statistical Association</publisher-name>.</citation>
</ref>
<ref id="bibr34-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schneider</surname>
<given-names>P. J.</given-names>
</name>
<name>
<surname>Penfield</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Alexander and Govern’s approximation: Providing an alternative to ANOVA under variance</article-title>. <source>The Journal of Experimental Education</source>, <volume>65</volume>, <fpage>271</fpage>–<lpage>286</lpage>.</citation>
</ref>
<ref id="bibr35-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sörbom</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1974</year>). <article-title>A general method for studying differences in factor means and factor structure between groups</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>27</volume>, <fpage>229</fpage>–<lpage>239</lpage>.</citation>
</ref>
<ref id="bibr36-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tomarken</surname>
<given-names>A. J.</given-names>
</name>
<name>
<surname>Serlin</surname>
<given-names>R. C.</given-names>
</name>
</person-group> (<year>1986</year>). <article-title>Comparison of ANOVA alternatives under variance heterogeneity and specific noncentrality structures</article-title>. <source>Psychological Bulletin</source>, <volume>99</volume>, <fpage>90</fpage>–<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr37-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Welch</surname>
<given-names>B. L.</given-names>
</name>
</person-group> (<year>1951</year>). <article-title>On the comparison of several means: An alternative approach</article-title>. <source>Biometrika</source>, <volume>38</volume>, <fpage>330</fpage>–<lpage>336</lpage>.</citation>
</ref>
<ref id="bibr38-1076998610396897">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>West</surname>
<given-names>S. G</given-names></name>
<name><surname>Finch</surname><given-names>J. F.</given-names>
</name>
<name>
<surname>Curran</surname>
<given-names>P. J.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Structural equation models with nonnormal variables: Problems and remedies</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Hoyle</surname>
<given-names>R. H.</given-names>
</name>
</person-group> (Ed.), <source>Structural equation modeling: Concepts, issues, and applications</source> (pp. <fpage>56</fpage>–<lpage>75</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr39-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wilcox</surname>
<given-names>R. R.</given-names>
</name>
</person-group> (<year>1988</year>). <article-title>A new alternative to the ANOVA <italic>F</italic> and new results on James's second-order method</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>41</volume>, <fpage>109</fpage>–<lpage>117</lpage>.</citation>
</ref>
<ref id="bibr40-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author"><name><surname>Wilcox</surname><given-names>R. R.</given-names></name><name><surname>Charlin</surname><given-names>V. L.</given-names></name><name><surname>Thompson</surname><given-names>K. L.</given-names></name></person-group> (<year>1986</year>). <article-title>New Monte Carlo results on the robustness of the ANOVA <italic>F</italic>, <italic>W</italic>, and <italic>F</italic>* statistics</article-title>. <source>Communications in Statistics-Simulation</source>, <volume>15</volume>, <fpage>933</fpage>–<lpage>943</lpage>.
</citation>
</ref>
<ref id="bibr41-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wilcox</surname>
<given-names>R. R.</given-names>
</name>
<name>
<surname>Keselman</surname>
<given-names>H. J.</given-names>
</name>
<name>
<surname>Muska</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Cribbie</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Repeated measures ANOVA: Some new results on comparing trimmed means and means</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>53</volume>, <fpage>69</fpage>–<lpage>82</lpage>.</citation>
</ref>
<ref id="bibr42-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yuan</surname>
<given-names>K. H.</given-names>
</name>
<name>
<surname>Bentler</surname>
<given-names>P.M.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Mean and covariance structure analysis: Theoretical and practical improvements</article-title>. <source>Journal of the American Statistical Association</source>, <volume>92</volume>, <fpage>767</fpage>–<lpage>774</lpage>.</citation>
</ref>
<ref id="bibr43-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yuan</surname>
<given-names>K. H.</given-names>
</name>
<name>
<surname>Bentler</surname>
<given-names>P. M.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>
<italic>F</italic> tests for mean and covariance structure analysis</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>3</volume>, <fpage>225</fpage>–<lpage>243</lpage>.</citation>
</ref>
<ref id="bibr44-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yuan</surname>
<given-names>K.-H.</given-names>
</name>
<name>
<surname>Bentler</surname>
<given-names>P. M.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Three likelihood-based methods for mean and covariance structure analysis with nonnormal missing data</article-title>. <source>Sociological methodology</source>, <volume>30</volume>, <fpage>165</fpage>–<lpage>200</lpage>.</citation>
</ref>
<ref id="bibr45-1076998610396897">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yuan</surname>
<given-names>K.-H.</given-names>
</name>
<name>
<surname>Bentler</surname>
<given-names>P. M.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Two simple approximations to the distributions of quadratic forms</article-title>. <source>British Journal of Mathematical and Statistical Psychology</source>, <volume>63</volume>, <fpage>273</fpage>–<lpage>291</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>