<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SSS</journal-id>
<journal-id journal-id-type="hwp">spsss</journal-id>
<journal-title>Social Studies of Science</journal-title>
<issn pub-type="ppub">0306-3127</issn>
<issn pub-type="epub">1460-3659</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0306312712458478</article-id>
<article-id pub-id-type="publisher-id">10.1177_0306312712458478</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Communicating and compromising on disciplinary expertise in the peer review of research proposals</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Huutoniemi</surname><given-names>Katri</given-names></name>
</contrib>
<aff id="aff1-0306312712458478">Department of Social Research, University of Helsinki, Finland</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0306312712458478">Katri Huutoniemi, Department of Social Research, University of Helsinki, PO Box 18, 00014, Helsinki, Finland Email: <email>katri.huutoniemi@helsinki.fi</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>42</volume>
<issue>6</issue>
<fpage>897</fpage>
<lpage>921</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This paper analyses peer review deliberations in four evaluation panels that differ in terms of scope and disciplinary heterogeneity. Based on evaluation reports and discussions with panel members, it illustrates a variety of ways in which reviewers bridge their epistemological differences and achieve consensus on the quality of research proposals. The analysis demonstrates that peer review panels are forums in which communication across disciplinary boundaries occurs and interdisciplinary judgments arise. At the same time, disciplinary gate-keeping and incommensurabilities may set limits on such communication. The comparison of deliberative processes sheds light on how collective judgments are shaped and constrained by the disciplinary set-up of the panels in which the reviewers operate and in which the intersubjective dynamics of the deliberations unfold. Based on these findings, the paper considers conditions that may enhance disciplinary interaction and complementary judgments in the peer review of proposals, and thereby expands the prospects for interdisciplinary research.</p>
</abstract>
<kwd-group>
<kwd>criteria</kwd>
<kwd>discipline</kwd>
<kwd>evaluation</kwd>
<kwd>expert judgment</kwd>
<kwd>interdisciplinarity</kwd>
<kwd>peer review</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>This study is concerned with the internal functioning of peer review, the practice through which scholarly work is evaluated by those with demonstrated competence. It analyses the ways in which peer review panels produce consensual judgments on the quality of research proposals and how reviewers are able to bridge their epistemological differences in this process. The topic is of interest, especially for those of us who are concerned with the status and fate of less established forms of inquiry – most typically, interdisciplinary research in its various forms. Thus, the aim of this paper is to investigate the disciplinary interaction that occurs in peer review deliberations, and to consider its effects on the evaluation of research proposals. The practical goal is to find a way to compensate for implicit biases in peer evaluation and to broaden the criteria used in research assessments. Despite the widespread concern about interdisciplinary research and the uncertainties in its evaluation, it is rarely asked how disciplinary interaction in peer review can be strengthened.</p>
<p>‘Discipline’, understood as an intellectual structure, denotes a set of codified rules, beliefs, perceptions and procedures with regard to producing and evaluating knowledge. These often tacit standards determine the criteria for admission into a community of scholars, the range of problems considered important, the approaches considered appropriate and the criteria for new knowledge (<xref ref-type="bibr" rid="bibr43-0306312712458478">Russell, 1983</xref>). The existence of rules is claimed to provide clear indicators for assessing performance within a discipline (<xref ref-type="bibr" rid="bibr13-0306312712458478">Feller, 2006</xref>). Whenever research expands, integrates with, or challenges the disciplinary canon, the problem arises that the epistemological standards of the disciplinary community may prove insufficient or even counterproductive (<xref ref-type="bibr" rid="bibr23-0306312712458478">Huutoniemi, 2010</xref>; <xref ref-type="bibr" rid="bibr41-0306312712458478"><italic>Research Evaluation</italic>, 2006</xref>). Interdisciplinary research is, by definition, a hybrid compound of more than one discipline, and is thus a form of scholarship that is not easily amenable to evaluation.</p>
<p>Not only does interdisciplinary research present difficulties for evaluation, studies of researchers’ conceptions of quality also have shown that quality criteria even <italic>within</italic> disciplines are rarely expressed in unambiguous terms (<xref ref-type="bibr" rid="bibr20-0306312712458478">Hemlin, 1991</xref>). Interviews with experienced evaluators suggest that good research is something that they ‘feel’ or ‘experience’ as much as ‘analyse’, and many experts state, ‘I know good research when I see it’ when asked to explain how they go about identifying quality (<xref ref-type="bibr" rid="bibr18-0306312712458478">Gulbransen, 2000</xref>; <xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>; <xref ref-type="bibr" rid="bibr27-0306312712458478">Lamont et al., 2007</xref>; <xref ref-type="bibr" rid="bibr30-0306312712458478">Langfeldt, 2004</xref>; <xref ref-type="bibr" rid="bibr45-0306312712458478">Thorngate et al., 2009</xref>). It is thus crucial to look beyond the criteria defined in methodological textbooks in order to explicate how quality is assessed in concrete evaluation settings. A study of evaluation practices in general, and the practices of peer review panels in particular, can inform us about how standards are intersubjectively constructed and how they determine what is prized in research.</p>
<p>Of prime concern in the literature of peer review has been the norm of universalism, as opposed to different forms of particularism, and the extent to which universalism is realized in practice (<xref ref-type="bibr" rid="bibr8-0306312712458478">Cole, 1992</xref>; <xref ref-type="bibr" rid="bibr7-0306312712458478">Cole and Cole, 1981</xref>; <xref ref-type="bibr" rid="bibr10-0306312712458478">Cole et al., 1978</xref>; <xref ref-type="bibr" rid="bibr16-0306312712458478">General Accounting Office, 1994</xref>; <xref ref-type="bibr" rid="bibr36-0306312712458478">Merton, 1996</xref>; <xref ref-type="bibr" rid="bibr42-0306312712458478">Roy, 1985</xref>). The questions posed by most researchers imply that a unified and fair process of evaluating knowledge can be put in place once particularistic considerations are eliminated. A smaller branch of the literature has, however, revealed various intrinsic ‘biases’ in peer review, such as ‘cognitive homophily’ (<xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>), ‘cognitive particularism’ (<xref ref-type="bibr" rid="bibr46-0306312712458478">Travis and Collins, 1991</xref>), ‘favoritism for the familiar’ (<xref ref-type="bibr" rid="bibr40-0306312712458478">Porter and Rossini, 1985</xref>), ‘professional bias’ (<xref ref-type="bibr" rid="bibr30-0306312712458478">Langfeldt, 2004</xref>) or just ‘peer bias’ (<xref ref-type="bibr" rid="bibr5-0306312712458478">Chubin and Hackett, 1990</xref>; <xref ref-type="bibr" rid="bibr15-0306312712458478">Fuller, 2002</xref>). In short, these studies suggest that the particular cognitive and professional platforms that are the basis for authoritative evaluations are, at the same time, sources of potential bias. In addition, professional expertise is often accompanied by embodied knowledge, views and expectations, which are extremely difficult if not impossible to differentiate from well considered, ‘unbiased’ judgments. It is thus debatable whether one can talk about ‘bias’ with respect to these ‘emotional-cognitive’ (in the term used by <xref ref-type="bibr" rid="bibr45-0306312712458478">Thorngate et al., 2009</xref>: 16) aspects of judgment, and rather than being ‘controlled away’, they should be considered as an inherent part of evaluation.</p>
<p>In the light of the latter discussion, the evaluation of interdisciplinary research concerns a more complex issue than merely a lack of explicit criteria or established procedures (cf. <xref ref-type="bibr" rid="bibr1-0306312712458478">Boix Mansilla et al., 2006</xref>; <xref ref-type="bibr" rid="bibr44-0306312712458478">Shimada et al., 2007</xref>). While a more transparent formalization of the evaluation procedure would serve to institutionalize interdisciplinary criteria (e.g. <xref ref-type="bibr" rid="bibr24-0306312712458478">Huutoniemi et al., 2010</xref>; <xref ref-type="bibr" rid="bibr25-0306312712458478">Klein, 2008</xref>; <xref ref-type="bibr" rid="bibr33-0306312712458478">Laudel and Origgi, 2006</xref>), it would probably not change the less explicit repertoires scholars employ when identifying quality. What seems most important, and yet understudied in research on interdisciplinary evaluations, is how reviewers’ cognitive affiliations with particular disciplines unfold in the evaluation process. Also important is how such aspects of their judgments can be ‘compensated for’ in order to strengthen interdisciplinary deliberation.</p>
<p>Among the frequently cited cognitive tendencies in peer review, two stand out as particularly prejudicial for interdisciplinary research. First, evaluators are likely to perceive excellence in a way that resonates with the work that they themselves are conducting (‘cognitive homophily’). Second, evaluators tend to be conservative and to protect their own epistemic territory from new perspectives and approaches (‘gate-keeping’). These tendencies imply that the prospects for interdisciplinary proposals are not very good, since they may be regarded as unorthodox examples of disciplinary research (<xref ref-type="bibr" rid="bibr32-0306312712458478">Laudel, 2006</xref>; <xref ref-type="bibr" rid="bibr46-0306312712458478">Travis and Collins, 1991</xref>).</p>
<p>A general consensus prevails that a panel of experts from different fields is better equipped than any individual expert to assess the quality of interdisciplinary endeavours. It is also assumed that collective deliberation by such a panel produces a more comprehensive and balanced evaluation than a composite review by several independent evaluators (<xref ref-type="bibr" rid="bibr1-0306312712458478">Boix Mansilla et al., 2006</xref>; <xref ref-type="bibr" rid="bibr31-0306312712458478">Langfeldt, 2006</xref>). On the one hand, in-depth ethnographic research on multidisciplinary peer review panels confirms that a group context may indeed prevent reviewers from institutional bias and encourage methodological pluralism, since panellists often lose their credibility with colleagues if they push their own fields too strongly (<xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>; <xref ref-type="bibr" rid="bibr29-0306312712458478">Lamont et al., 2006</xref>). On the other hand, studies on the decision-making of evaluation panels suggest that collective evaluation processes often involve a clear division of scholarly tasks, little interaction, and tacit compromises (<xref ref-type="bibr" rid="bibr17-0306312712458478">Grigg, 1999</xref>; <xref ref-type="bibr" rid="bibr30-0306312712458478">Langfeldt, 2004</xref>; <xref ref-type="bibr" rid="bibr38-0306312712458478">Olbrecht and Bornmann, 2010</xref>). Thus, while attempts are often made to manage the disciplinary ‘bias’ of peer review by striving for high group diversity, the social or interpersonal context of judgment creates its own challenges.</p>
<p>The present study continues the effort to open the ‘black box’ of quality judgment by analysing the ways in which experts in differently composed groups evaluate research proposals. It focuses on the disciplinary interactions and the bargaining that occur between peer reviewers in different panel compositions. The analysis is meant to contribute to our understanding of variations in evaluation processes across scholarly contexts. In the emerging literature on evaluation practices, there has been very little comparative work (see, however, <xref ref-type="bibr" rid="bibr28-0306312712458478">Lamont and Huutoniemi, 2011</xref>). A particular goal of this study is to consider which conditions may enhance disciplinary interaction and complementary judgments in proposal peer review, thereby enhancing the prospects for interdisciplinary research.</p>
<p>In the section that follows, I describe the materials and methods of this study. I then go on to examine the intersubjective process of evaluation in four different peer review panels. My analysis pays attention to the ways in which the members of the different panels brought together their different areas of expertise and quality criteria, and how they perceived and justified this process. I then discuss my findings in a more theoretical manner and ask what they offer for interdisciplinary evaluation. The concluding section summarizes the findings and raises pragmatic issues about designing the peer review process.</p>
<sec id="section1-0306312712458478" sec-type="methods">
<title>Data and methods</title>
<p>The study draws on a small sample of peer review panels organized in a recent year by the Academy of Finland, a national funding agency in Finland.<sup><xref ref-type="fn" rid="fn1-0306312712458478">1</xref></sup> The panels were put together to evaluate research proposals submitted to one of the Academy’s key funding instruments, ‘Academy Projects’. In the peer review model used by the Academy, recognized scholars from all over the world (but mostly from other European countries) are invited to a panel meeting (usually for one to two full days) where they collectively rate proposals according to their scholarly quality. Although funding officers instruct panellists about evaluation criteria, panellists are given full sovereignty over rating the proposals. Each panel is assigned a subset of proposals emanating from the same research area or concerned with similar subject matter. The proposals are grouped and the panel members selected by the Academy staff, helped by experts of the four Research Councils. Before the meeting, individual panel members are asked to make a preliminary review, including a suggested score (1–5), for several applications. Each application is assigned to two (or sometimes more) panellists by the funding officials. All of the preliminary reports as well as the applications are available online for the whole panel prior to the meeting, but the panellists are not obliged to read them. During face-to-face deliberation, the panellists are required to achieve consensus in their ratings and collectively write an evaluation report of each proposal. The evaluation reports are sent back to the respective applicants and forwarded to one of the four Research Councils of the Academy, and that Council makes funding decisions for a wider range of proposals considered by a number of panels. In the year of this study, approximately every fourth proposal was funded.</p>
<p>I selected four panels for this study; the Academy designated the fields of these panels as <italic>Environmental Ecology, Environmental Sciences, Social Sciences, and Environment and Society</italic>. The panels were selected from the approximately 20 panels that were invited to evaluate the project proposals in the given round. The idea was to include panels with varying degrees of specialization in the research fields I am familiar with. The Environmental Ecology panel (ECO-ENV) was quite unidisciplinary: it operated with a thematically and epistemologically coherent set of submitted applications emanating from one broad field, the ecology of aquatic environments, and all the panellists were ecologists of some sort. The Environmental Sciences panel (ENV) evaluated proposals that dealt with natural processes in various environments, including forests, soils, peat lands and vegetation. It was multidisciplinary, since both the proposals and the panellists spanned multiple fields. The Social Sciences panel (SOC) was also multidisciplinary, and considered proposals from sociology, social psychology, social policy, social theory, social work and cultural studies. It was composed of specialists from these various fields. The Environment and Society panel (ENV-SOC) was clearly interdisciplinary: it included a heterogeneous group of panellists working with a diverse set of proposals. The panellists often had degrees in more than one discipline and were knowledgeable on a wide range of interdisciplinary topics. They considered multi- or interdisciplinary proposals that dealt with environmental issues or with social-environmental interactions from a social, political, economic, technological or other perspective beyond the natural sciences. The scope and composition of each panel is summarized in <xref ref-type="table" rid="table1-0306312712458478">Table 1</xref>.</p>
<table-wrap id="table1-0306312712458478" position="float">
<label>Table 1.</label>
<caption>
<p>The four evaluation panels compared: the number and scope of proposals under evaluation, and the number and expertise of panel members in terms of educational background</p>
</caption>
<graphic alternate-form-of="table1-0306312712458478" xlink:href="10.1177_0306312712458478-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">ECO-ENV Environmental Ecology</th>
<th align="left">ENV Environmental Sciences</th>
<th align="left">SOC Social Sciences</th>
<th align="left">ENV-SOC Environment and Society</th>
</tr>
</thead>
<tbody>
<tr>
<td>Proposals</td>
<td>n=19</td>
<td>n=46</td>
<td>n=22</td>
<td>n=22</td>
</tr>
<tr>
<td/>
<td>Ecology of aquatic environments</td>
<td>Natural processes in various terrestrial environments</td>
<td>Various social phenomena</td>
<td>Social–environmental interactions</td>
</tr>
<tr>
<td>Panel members</td>
<td>n=5</td>
<td>n=11</td>
<td>n=5</td>
<td>n=6</td>
</tr>
<tr>
<td/>
<td>Limnology (2)</td>
<td>Forest ecology (2)</td>
<td>Sociology (2)</td>
<td>Social sciences (2)</td>
</tr>
<tr>
<td/>
<td>Microbiology (1)</td>
<td>Plant physiology (2)</td>
<td>Social policy (1)</td>
<td>Economics (2)</td>
</tr>
<tr>
<td/>
<td>Microbial ecology (1)</td>
<td>Soil ecology (2)</td>
<td>Social work (1)</td>
<td>Ecology (1)</td>
</tr>
<tr>
<td/>
<td>Stream ecology (1)</td>
<td>Mycology (2)</td>
<td>Social sciences (1)</td>
<td>Mathematics–psychology (1)</td>
</tr>
<tr>
<td/>
<td/>
<td>Chemistry (1)</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>Microbiology (1)</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>Micrometeorology (1)</td>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
<p>Various forms of empirical data were collected on each panel, including: the project proposals under review (n=109 – the total for all panels); the preliminary reviews and scoring of the proposals prior to the panel meetings; and the evaluation reports and scores as defined collectively by the panels. Using these data as a guide, I then conducted phone interviews with 18 (out of 27) panel members (designated in this paper as P1–P18) shortly after the panel deliberations. The selection of interviewees included the majority of those panellists who were willing to be interviewed within a few weeks after the panel meeting. As all the panellists could not be interviewed, the views collected do not necessarily represent the group as a whole, and the self-selection of interviewees may have caused a bias in how the group process was represented. However, given the number of commitments and busy schedules of these high-ranking academics, this seemed to be the only conceivable strategy to get first-hand information on the evaluation process. This was the case, especially because I was not allowed personal access to panel meetings by the funding agency, and so direct observation was not possible. Access to these confidential procedures is commonly restricted; unfortunately, the resulting paucity of evidence sets limits on this study.</p>
<p>The interviews lasted approximately an hour, and they loosely followed a schedule structured beforehand. The interviews were conducted in English, although only a fraction of the panellists were native English speakers. During the interviews, the panellists were asked to profile their personal expertise as well as the collective expertise of their panel, to explain what happened in each controversial case, to describe the arguments they themselves made about a range of proposals, to contrast their own arguments with those of other panellists, and to consider the meaning of the deliberation process in general. In addition to conducting the phone interviews with the panellists, I conducted face-to-face interviews with two Finnish funding officers who convened and attended the panel meetings. My main goal with these two interviews was to find out the formal procedures of evaluation in the specific panels. I conducted similar interviews with eight funding officers during an earlier project (<xref ref-type="bibr" rid="bibr4-0306312712458478">Bruun et al., 2005</xref>) that guided the design of this study. All interviews were recorded and transcribed.</p>
<p>I used a qualitative research design to analyse data from the panels and to explore in depth the intersubjective aspects of evaluation that emerged from the deliberations among panel members. The empirical focus was on how each evaluator evaluated proposals and negotiated with the other panellists, and especially how the composition of the panel and the substance of the proposals created constraints on how the negotiation proceeded (see <xref ref-type="bibr" rid="bibr2-0306312712458478">Boltanski and Thévenot, 1999</xref>; <xref ref-type="bibr" rid="bibr37-0306312712458478">Muller-Mirza et al., 2009</xref>). I began by focusing on proposals that had aroused different opinions, in an effort to make sense of those disagreements on the basis of the evaluation reports and the panellists’ own accounts. I then pinned down the strategies through which consensus was reached in each panel as well as the meanings panellists assigned to this process. Throughout this analytical work, I compared and contrasted accounts of each of the four panel’s processes, in order to illustrate how the negotiations were shaped and constrained by the scholarly settings in which the panellists operated.</p>
</sec>
<sec id="section2-0306312712458478">
<title>Perceiving and bridging expertise</title>
<p>Since the task of peer review panels is to produce evaluations of proposals, the panellists’ first effort is to put their collective expertise to efficient use. While most panel members had some knowledge of all of the issues covered in the proposals, they tended to be good at different things, and so each had something unique to bring to the table. Given that the panel members typically did not know one another before the meeting, they came to discover their competences in an incremental way as the work went along. Their eventual views, however, were constrained by the proposals at hand and the discussions that followed. Consequently, substantial differences occurred in how panellists in each committee came to understand, utilize and coordinate their evaluations of the proposals, as well as in how they assigned meanings to this process (for a summary, see <xref ref-type="table" rid="table2-0306312712458478">Table 2</xref>).</p>
<table-wrap id="table2-0306312712458478" position="float">
<label>Table 2.</label>
<caption>
<p>Components of collective judgment in the context of each panel</p>
</caption>
<graphic alternate-form-of="table2-0306312712458478" xlink:href="10.1177_0306312712458478-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">ECO-ENV</th>
<th align="left">ENV</th>
<th align="left">SOC</th>
<th align="left">ENV-SOC</th>
</tr>
</thead>
<tbody>
<tr>
<td><bold>Bridging expertise</bold></td>
<td>Pooling, aggregating</td>
<td>Integrating</td>
<td>Deferring to the best expert(s)</td>
<td>Generalizing</td>
</tr>
<tr>
<td><bold>Agreeing on rating</bold></td>
<td>Equalization</td>
<td>Calibration</td>
<td>Contextualization</td>
<td>Commensuration</td>
</tr>
<tr>
<td><bold>Perceived value of deliberation</bold></td>
<td>Accuracy, consistency</td>
<td>Robustness</td>
<td>Fairness</td>
<td>Empowerment</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Given the abundance and compatibility of expertise in relation to the evaluative tasks at hand, the ECO-ENV panellists did not need to bother themselves much with assessing each other’s competences. When opinions differed, the panellists justified their stances on the basis of what they knew about the topic. In most cases, a panellist with more competence on the topic was able to convince the others that, for example, the proposed methodology was not appropriate, or the research problem was not original. In the following incident, an expert in microbiology explained why he deferred to other panellists who knew more on a topic:
<disp-quote>
<p>I think [the debate] was mainly about the relative importance [of a microbiology proposal] in this field. This sea ice business is a rather specific field that I don’t know so much about, and I considered it an interesting area. But then it turned out that there is already broader background information existing in this area, and from that point of view, the novelty of this approach, or of this project here, was lower than I had assumed. (P1)</p>
</disp-quote>
</p>
<p>In this way, these panellists were able to combine their areas of expertise and fill gaps in each others’ knowledge. Pooling expertise was believed to provide highly refined evaluations and was thus considered a crucial aspect of the panel deliberation. When I asked the panellists to explain what was most important for the evaluation process, they noted unequivocally that the discussions enabled their evaluations to get ‘much more detailed, much fairer’ (P2). A panellist explained that, since reviewers’ judgments were often very similar, ‘the discussion I think is important to validate the views of the individual evaluators. So it’s a sort of a moderation of the quality assurance process, really’ (P4). At the same time, this panellist felt it unnecessary for his evaluations to be ‘validated’ by others when he already was sure of himself: ‘I feel most competent in subjects that are closest to my own. So, for example in terms of molecular work, I feel very confident of my position, <italic>it’s gonna be correct</italic>’ (P4).</p>
<p>The interviewees from the ECO-ENV panel were not convinced that the deliberation itself had anything other than instrumental value; for them, it was important because it made the judgments more accurate. For example, a panellist made the point that having more time for discussion in the panel meeting would have improved the process by allowing the panellists to go into <italic>more detail</italic>. Like other panel members, she highlighted the importance of each panellist’s individual work as the primary source of appropriate judgment: ‘I think that the evaluation you do by reading the applications, the remote evaluation, and that several people are doing that [independently], is the most precise way of doing it’ (P2).</p>
<p>While overlapping competence was perceived as useful for ‘validating’ judgments, ‘blind spots’ in the panel’s collective expertise caused some uneasiness. The panellists I talked with were unequivocal in expressing their view that their decisions could have been improved by having expert opinions about the particular proposals that dealt with areas that were not specifically covered by any panellist: for example, remote sensing techniques, chemical analysis and developmental biology. While they asserted that they ‘felt reasonably comfortable in forming an opinion’ on such proposals by drawing on their discussions with colleagues, they expressed a concern that these evaluations were less valid than the others.</p>
<p>As for the ENV panel, the interviewees’ accounts indicated that the panel’s collective expertise turned out to be more than only a cumulative stock of knowledge. The review committee was composed of so many experts with different skills and specialties that their discussion soon revealed some variance between different panellists’ perspectives. While they found that this variance caused divergent judgments, they also considered it a useful basis for producing robust evaluations. For instance, a mycologist explained that he had a different take on a proposal than an ecologist, because the two panellists had ‘read the application from two completely different points of view’ (P9). As both views were understood to be equally legitimate appraisals of the proposal, the experts framed them as complementary parts of a comprehensive judgment:
<disp-quote>
<p>In this case it was really important that the two evaluators were present …. He considered it from an ecological and physiological standpoint, and also of the importance regarding global change and so on. But my standpoint was more that is it possible to successfully finish the study with these [fungal] organisms with these prerequisites. (P9)</p>
</disp-quote>
</p>
<p>The ENV panellists often attributed divergent opinions to the complexity of the proposals themselves. Interviewees remarked that many proposals were multidisciplinary, ‘bringing in different skills’ or ‘involving contributions to different disciplines’. (P7) The panellists observed that they could complement and reinforce each other’s views and collectively form robust judgments on proposals about which they, as individuals, were not fully competent. In this context, the strength or independence of an individual expert’s opinion was not seen as necessary for making an authoritative judgment: most interviewees from this panel perceived ‘the width of scientific judgment’ (P6) as a more legitimate basis for evaluation than the view of a specialized expert.</p>
<p>For instance, one proposal dealt with a newly developed technique that was highly valued by a specialist in the given field. Some other panel members were doubtful about the proposal because the technique had not been published despite several years of development. I was not able to talk with the specialist, but another panellist had the impression that ‘in the end [the specialist] sort of really changed her mind. … If so many panel members feel that we should be careful about that, then [this specialist] saw you’re right and we will be careful about it’ (P5). In this and other similar cases, the interviewees stressed that ‘you don’t feel a person is less competent if they change their mind’ (P8).</p>
<p>In two comparable incidents, a panellist with fully developed expertise on a proposal’s topic was convinced by other participants that the proposal might be better than he initially thought. This panellist explained such incidents by the reasoning that it is hard not to be critical when a proposal is ‘absolutely and 100% in your field’:
<disp-quote>
<p>If [a proposal] goes to somebody who really does this every day, then from the start, they look at it and they can tell every new problem with it, and secondly, they are less impressed by it, because they do it every day, if you know what I mean. But it was one of those things that, in the discussion it came out when they said, ‘Well, if you agreed with this proposal, would you have done it any differently?’ And you should say, ‘Actually no, it’s a great idea, this is a really valuable proposal.’ And you realized probably that actually you are a lot more critical if you are [very close to the topic]. (P6)</p>
</disp-quote>
</p>
<p>These examples illustrate that the panellists clearly believed the deliberation gave them some additional insight into the proposals. Some interviewees were convinced that only through collective deliberation were they able to produce reliable evaluations:
<disp-quote>
<p>It’s better that two minds think about something together than in isolation. We could have an emergent thing that came out of the discussion that none of us on our own would have checked out. It just made it really clear which ones were the absolute best, and which ones had weaknesses. I think you wouldn’t have gotten the same ranking in the end that you’ve got from the discussion; the discussion made it really clear which ones shouldn’t be funded. (P10)</p>
</disp-quote>
</p>
<p>As was the case with other committees, the SOC panellists said they had considerable collective competence, and especially that each one could ‘add a distinct contribution’ (P16) to the panel’s range of expertise. An interviewee reported that ‘whenever policy questions came about we could, you know, rely on [one panel member]; and whenever questions of technological innovation came in, we could rely on [another panel member]’ (P18). Such deference to expertise was also an important means of dealing with divergent evaluations. In cases where one panellist claimed greater competence in a proposal’s topic, the other panellists were inclined to withhold their own judgments. One panel member strongly supported several cultural studies proposals in her preliminary reviews, but backed down on them after the deliberation:
<disp-quote>
<p>I’m not really a big expert on cultural studies, so when you know [a panellist’s name] or someone said, ‘oh, this is already known in cultural studies, this is not new at all’, I’ve got to say, ‘well, you’re probably right in that case’. So that was certainly happening with several where I disagreed with [another panellist]. (P17)</p>
</disp-quote>
</p>
<p>Such deference and respect for the intellectual turf of each expert was customary in this panel, and lessened potential tensions between the panel members (see <xref ref-type="bibr" rid="bibr34-0306312712458478">Mallard et al., 2009</xref>). At the same time, it seemed to undermine the importance of transparent criteria and shared responsibility for making evaluations. The SOC panellists sometimes adopted an advocate’s role for proposals emanating from their own respective areas of expertise, and they sometimes also abandoned critical appraisal of proposals that represented different specialties from their own. Their discussion of some business school proposals illustrates the latter tendency and the salience of ‘cognitive contextualization’ (<xref ref-type="bibr" rid="bibr34-0306312712458478">Mallard et al., 2009</xref>) more generally. When finding that there was no expert in that field on their panel, the panellists became worried about imposing sociological criteria on those proposals. A sociologist pondered:
<disp-quote>
<p>Obviously we could use a general social science expertise to evaluate the proposals, but … it was quite difficult for us to place them, as it were, academically, because we don’t know what the norms and values of the business school kind of proposal might be. So, for instance, from a sociological point of view, we found them lacking in many ways, but it could be that within that kind of business and critical management studies those kinds of proposals are actually great some time, but we didn’t have anyone with that exact area of expertise to, kind of, give us the kind of key markers. (P18)</p>
</disp-quote>
</p>
<p>Thus, the reviewers did not worry so much about a potential gap in their knowledge of this subject area as they did about not knowing <italic>what criteria</italic> to use. An analogous weakness in their panel was seen to result from the predominant nationality among the members. As the majority of panellists were British, some had questions about their eligibility to evaluate the proposals; they expressed concerns about being ‘too anglocentric’ or having a ‘British bias’ (P17). This did not refer to lack of expertise on, for example, Finnish society, but to their uncertainty about ‘imposing criteria that we would use in our own national context on to this situation’ (P18).</p>
<p>Debates on how to appropriately contextualize the proposals also occurred within the limits of particular fields. For example, a proposal on a minority culture in Finland was rated low by one panellist because ‘it didn’t pay sufficient attention to a particular area of theory which would have completely problematized the basic assumptions of their approach’ (P18). Another panellist accepted this point, but disagreed about its relevance in the given case:
<disp-quote>
<p>That is a kind of way in which cultural studies has moved on in Britain, but they’ve still got a whole tradition of using [the applicant’s] approach to ethnicity. So that’s where we [debated], because I thought it to be unfair to judge it by the kind of standards of British cultural studies, which is you know one country, whereas [the applicant] was coming from a different direction. (P17)</p>
</disp-quote>
</p>
<p>The wide scope of the ENV-SOC panel obliged panellists to take positions on topics outside their core areas of expertise, and they negotiated their judgments with colleagues with whom they had relatively little in common. The chair reported that ‘it was an interdisciplinary panel. I have a feeling that we were all chosen because we were inter[disciplinary], we were broad people’ (P14). Because the proposals, as well as the reviewers, were clearly interdisciplinary, it often happened that the reviewers’ judgments were based on relevant, yet completely different views of a proposal. For example, panellists recalled a proposal that dealt with nature conservation and social dynamics, which had been framed quite differently by the reviewers. An ecologist saw it as addressing a highly relevant problem that had not been properly conceptualized by previous research, and praised the set of case studies that were proposed for investigating the problem. Others focused their reviews more on the social-scientific design provided for the comparison of cases, which they found inadequate. The ecologist explained the disagreement as follows:
<disp-quote>
<p>I acknowledged to the other people where I thought the weaknesses were, and the other people said in the synthesis, that’s fine. But I think that’s a [personal] viewpoint as well as a disciplinary [view], because if that was in the conservation research end people would really value it, whereas I think the people from the social [research end] would say, ‘well this isn’t really giving us a particular new insight’. So I think it is a different disciplinary view: it’s new to the conservation mixture [of disciplines] but may not be that interesting to the social scientists. (P11)</p>
</disp-quote>
</p>
<p>As indicated by this debate, insights from various scholarly traditions did not always mesh together well, and the ecologist characterized their meeting as ‘one of the most difficult panels [on which] I have ever sat’ (P11). Even so, the panellists were not inclined to defer to disciplinary expertise in their evaluation, partly because they acknowledged being invited due to their interdisciplinary competence. Indeed, most panellists were simultaneously involved in several different epistemic communities, which often required an ability to see beyond narrow disciplinary considerations and to compare proposals from a range of disciplines. The interviewees declared that they ‘tended to be not worried too much about the different disciplines’, but looked for more general qualities:
<disp-quote>
<p>What were we looking at was not, you know, particularly disciplinary attributes of the applications. We were looking at things like research design, mostly research design in such a way that is it going to produce useful results, would the results be useful for policy-makers, were … both the methodology and … well-explained and good, was it up to the data that they were thinking of gathering. They were more generic questions, rather than was it good sociology or good economics or good this or good that. And I think we all really took that view. (P14)</p>
</disp-quote>
</p>
<p>Overall, the legitimacy of the evaluation process in this panel was not entirely based on the use of specialized expertise or informed arguments. The panellists explicitly acknowledged that non-experts also had an important role to play in the evaluations. While the role of experts was to judge whether ‘there’s a proper methodology and proper question, because only they know the literature’, a wider group was needed to ‘ask bigger questions … like “why are you doing this”, which can often be a shock to specialists because they haven’t really viewed their own topic from the outside’ (P11).</p>
<p>Given their broad understanding of expertise, these panellists were not concerned about what areas of knowledge they collectively lacked. When I probed for potential blind spots, I received the answer: ‘There was no outlier, there was <italic>nothing</italic> out of the frame of [the panel’s] competence’ (P15). More than the scope of their expertise, they worried about the potential imbalance, since the panel comprised ‘just one poor ecologist against three social scientists’ (P11). In this context, reviewers appeared to feel entitled to rely on their knowledge, even of topics that were less familiar to them. When I inquired about how they had gone about evaluating a network analysis proposal that was given a high rating, a panellist responded:
<disp-quote>
<p>The proposal was very well written, it was very easy to understand. I don’t think anybody in the room really had expertise in network analysis. But because they were clear what they wanted to do, people were very happy to accept that the proposal in this area, that we didn’t have expertise on, was very valid. … If they write in a very clear fashion, and you’re not familiar with the area, you tend to think, ‘oh well that would be okay’. (P11)</p>
</disp-quote>
</p>
<p>In this and other similar cases, judgment was made on the basis of a shared willingness to support a well-written proposal that explained the project clearly for non-specialists.</p>
</sec>
<sec id="section3-0306312712458478">
<title>Agreeing on rating</title>
<p>Peer review panels are engaged in a search, not only for what is qualified, but also for what is valuable or meritorious. What is it that counts, and according to what measures? The evaluative culture of panellists’ own disciplines influences how they define quality, including the relative importance they attach to various values. Differences between evaluative cultures thus pose a particular challenge for reviewers, as panels are expected to produce common judgments on the quality of the proposals assigned to them. The comparison of the evaluative practices of the four panels suggests that while value conflicts are often unavoidable, they can be interpreted and settled in different ways. In fact, the analysis brought out a variety of intersubjective mechanisms used for persuasion and to reach a settlement (for a summary, see <xref ref-type="table" rid="table2-0306312712458478">Table 2</xref>).</p>
<p>When scholars act as judges, they implicitly commit themselves to go beyond their personal preferences and assess quality as defined through more objective standards (<xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>). Hence, an important premise for many panellists is that quality resides in the proposals themselves, rather than being attributed by the observer or by the particular comparative setting. However, as the ECO-ENV panellists were all ecologists, the panel turned out to assume that its epistemological norms were more self-evident than did the other committees, whose broader scope and multidisciplinary structure seemed to support more mutual recognition and exposure of epistemological assumptions (see <xref ref-type="bibr" rid="bibr14-0306312712458478">Fuchs, 2001</xref>). When I asked about their criteria for quality and how different panel members identified a proposal as meritorious, the interviewees assured me that they were ‘looking for exactly the same thing’ (P4). Most of these panellists also did not feel that it was problematic to ‘judge about other fields within science … [because] as an experienced scientist, you <italic>know</italic> what good science is – <italic>even if it’s not in your own field</italic>’ (P3).</p>
<p>The unproblematic status the ECO-ENV panellists gave to quality criteria, together with the fact that they often claimed expertise in the same field, made it hard for them to tolerate different opinions. This became evident in a series of disagreements that had to do with the comparative weighting of different strengths and weaknesses. Proposals that posed ambitious questions but presented somewhat unfocused research strategies were praised by one panellist while being denounced by another. Conversely, projects with modest scientific goals but well-argued, feasible research plans, including clear applications for the results, were not highly prized by the first panellist while the other one gave them high ratings. The first panellist expressed his frustration with these disagreements as follows:
<disp-quote>
<p>These are very highly productive, internationally profiled scientists doing very interesting things. They have very bold ideas, but there may be details in the methods; it’s not very well thought through, and [another panellist] got stuck on that. Especially [an applicant’s name], there was one part of this proposal that [the other panellist] didn’t like at all, and then he wanted to just say no to the whole application. Just because of that little part … . The other parts of it were just splendid, yeah, they were just very good. (P3)</p>
</disp-quote>
</p>
<p>Since neither panellist was able to convince the other, their different criteria caused serious battles over academic authority. It was not easy for these panellists to change their minds as they both felt that, along with their judgment, their identity as experts was at stake. In such incidents, collective deliberation was needed in order to reach a compromise:
<disp-quote>
<p>When there was a clear disagreement, then the whole panel would give their views. But these would be views about the stances taken by the two evaluators, rather than the detailed science in the application. … People would give their views on how much relative weighting should be given to the [applicant’s] track record versus the application. … [It was a] discussion of general principle, rather than the specific application. (P4)</p>
</disp-quote>
</p>
<p>The panel had also set up a routine procedure, in accordance with the guidance of the funding officers, whereby the task of drafting the evaluation report was assigned to a third panellist, who was ‘a little further remote from the respective field’ so that the given proposal was ‘not so close to his personal emotions’. This panellist acted ‘as a kind of independent judge … [who] could look more at the formal aspects, keep things equal, and judge across different cases’ (P1).</p>
<p>These instances indicate that the reviewers did not openly contemplate their different values, but strived to <italic>equalize</italic> their judgments. A key function of this process was to avoid open conflicts while guaranteeing a more or less automatic repetition of the kind of behaviour that would produce consistent judgments. The panellists thus implicitly kept their disciplinary norms ‘sacred’, even though differences between the norms were sometimes obvious. One interviewee commented:
<disp-quote>
<p>It’s just the scientific cultures. He is raised in an applied area of research and wants these applied issues to be funded, whereas I am raised in a more basic science and I want … the nice ideas, the good people, the ones who have shown that they produce good science, I want them to get funded. (P3)</p>
</disp-quote>
</p>
<p>More often than not, a compromise was found, indicated by an average rating, somewhere in the middle of the opposing positions. Of course, such a method of closure was often disappointing for the participants, and those discussions were characterized as ‘meaningless’ by a panellist who had strong opinions on the proposals. Sometimes, however, a compromise took the form of ‘horse-trading’ (<xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>: 121–125), where one panellist enabled another panellist’s objectives to be realized in the hope that he would reciprocate: ‘One case I could win, and in another case, or in several other cases, he won, so to speak. I use these words just to pinpoint the situation that it’s like this – it’s like convincing the other one that your grade is the right one.’ (P3)</p>
<p>While a few instances of horse-trading were evident in the evaluation reports as well, completely missing was evidence of substantial changes in the final judgments due to the deliberation. In the other three panels, the final ratings sometimes went above or below the preliminary ratings (see <xref ref-type="fig" rid="fig1-0306312712458478">Figure 1</xref>).</p>
<fig id="fig1-0306312712458478" position="float">
<label>Figure 1.</label>
<caption>
<p>The effect of deliberation on scores across panels: the final score given to each proposal is compared with the two (or more) preliminary scores.</p>
</caption>
<graphic xlink:href="10.1177_0306312712458478-fig1.tif"/>
</fig>
<p>The experts of the ENV panel also believed that there was agreement in principle as to what constitutes good quality, and they told me of being ‘amazed actually, how much the joint discussion, even for five minutes, really showed you that you have given a right mark to a proposal’ (P6). However, as was the case with the ECO-ENV panel, occasional conflicts arose between different preferences. Pitted against each other were ‘hypothesis-driven’ versus ‘screening’ approaches, ‘creative’ versus ‘feasible’ objectives, and ‘scientific’ versus ‘technological’ relevance. Instead of trying to achieve a balance between the different preferences through some procedural mechanism, the panellists deliberated between their various normative stances. Most disagreements were settled by mutual persuasion without causing anyone to ‘lose face’. In the following disagreement, for example, the relative weighting of different criteria was discussed and the case was settled through consensus:
<disp-quote>
<p>[One reviewer] thought it was really good because of the approach to it, and [another reviewer] thought the approach was fine, but it was undoable, because [the applicant] had written every possible stress combination into the project you can think of. It was brilliant, yes, but impossible to do within the time scale. … [The former reviewer] based her review on the fact that this is really valuable science, and if she only did half of it, it would be worth doing. That was the philosophical question when looking at the proposals in the end. … But you have to review what they’ve written on the page. And if they’ve written something on the page that’s impossible to do, then really and truly, they should rewrite the proposal and submit it next year, shouldn’t they. That’s pretty much the consensus we reached, at the end of this debate. (P6)</p>
</disp-quote>
</p>
<p>In cases of conflict, the ENV panellists often agreed to accommodate to the collective value frame that emerged from their deliberations. The panellists can be thought of as <italic>calibrating</italic> their individual senses of quality to a group standard in order to form a concerted evaluation. An important means of achieving a coherent evaluation was the reciprocal calibration of individual rating scales. At the start of their meeting, panellists had discussed which journals they regarded as most important in their fields of research, and they elaborated ‘in what journals we would have published “outstanding”, “excellent” and “very good” papers, or only “good” papers’ (P9). Whenever they hesitated about giving the highest score, for example, they could ask: ‘Can it, if we are lucky, be published in <italic>Science</italic> or <italic>Nature</italic>?’ (P8). They thus established their collective criteria by setting indirect indicators of different degrees of quality, whereby they avoided quarrelling over disciplinary nuances. Interviewees highlighted the importance of such calibrating activity by referring to a few occasions where the preliminary scores coincided, but due to the calibrated rating scale, reviewers decided to lower their scores (see <xref ref-type="fig" rid="fig1-0306312712458478">Figure 1</xref>).</p>
<p>As for the SOC panel, both the preliminary reports and the interviews suggested systematic differences in how panellists from different fields framed proposals. For example, a panellist who had expertise in empirical sociology, and conducted surveys and secondary analyses of quantitative data, was called ‘technocratic’ by another panel member. This ‘technocrat’ paid a lot of attention to the appropriateness of sampling strategies but also to the ethical dimensions of research methodology, while being less sensitive to proposals’ theoretical ambitions or lack thereof. Another sociologist took the opposite position and defined her expertise in terms of certain theoretical positions. Not surprisingly, she was described by herself and others as operating under ‘traditionally defined academic criteria’ by looking for ‘theoretical coherence, a clearly worked out relationship between method, methodology, and theory’ (P18). She often assigned diminished value, or did not consider at all, whether a proposal included an elaborated research design that took into account various pragmatic constraints. Still another panellist was described by himself and others as a ‘pragmatist’ and explained his standards by explicitly contrasting them to the position taken by his more theoretically oriented colleagues:
<disp-quote>
<p>For somebody like myself, being at the very forefront of theories is a luxury that not everyone can afford and obtain, and so, if you live in very much an applied policy world … you have to do what you can, in a way. So somebody like myself, I’ll be slightly more pragmatic and say, okay, there are a number of theories we can use here, I’m not going to privilege necessarily a particular author or position, I’m more concerned with what appears to be a plausible research design, and does it at least contain … some reliability, and some impact on outcomes that you could anticipate. (P16)</p>
</disp-quote>
</p>
<p>The SOC panellists believed that freely acknowledging their personal standpoints as an inevitable component of evaluation would help them become aware of their individual mindsets and make them more open to rethinking their evaluations. While each expert seemed to favour proposals that somehow spoke to her or his own interests (to ‘technocratic,’ ‘pragmatic’ or ‘academic’ criteria), the panellists were also prepared to alter their positions. An interviewee portrayed such shifts as a conscious choice: ‘Am I going to kind of slightly rethink, or am I going to argue my case? One or the other, really’ (P16). He explained such situations as follows:
<disp-quote>
<p>The panel would have to be explicit about how it understood the criteria in relation to the application, and those discussions would be explicit and substantive. One could then detect different perspectives around the criteria. … I think where positions were very different, I would say, ‘This is my take on it, this is how I saw it’, but, you know, ‘okay, having heard what you said, and looked at some of the other applications where we had some similar discussions, I can see that I was possibly underestimating the importance of x, y, and z.’ (P16)</p>
</disp-quote>
</p>
<p>Such judgmental openness, or awareness of how worldviews affect evaluation, made it easier for these panellists to discuss different points of view ‘without people getting cemented into their position’. In strong contrast to the ECO-ENV panellists, for example, these social scientists often came back to review their positions and to re-examine a proposal, instead of ‘pushing people back into their boxes’ (P16). The resulting compromises did not necessarily indicate agreement on the merits of proposals, but were often the result of conscious moves or academic politeness.<sup><xref ref-type="fn" rid="fn2-0306312712458478">2</xref></sup></p>
<p>In a few cases, however, disagreements clearly prevailed over politeness, and attempts to mediate between different judgments proved unsuccessful. A practical means of reaching compromise was to ‘go through the [evaluation] form bit by bit’ (P17). This involved leaving aside overall positions on proposals and discussing the text of the evaluation report. As explained by a reviewer who came to adjust her marks time after time: ‘That kind of discussion about the different sections of the [evaluation] form was actually very fruitful for arriving at the overall consensus … . By coming to an agreement about the <italic>text</italic> on each section, you actually came to a compromise at the end.’ (P17)</p>
<p>In contrast to the ENV panellists, who strove for a shared understanding on the general quality of each proposal, the members of the SOC committee found it easier to ‘agree on details’. Such a strategy implied that the compromises negotiated in this panel were provisional, and that differences in framing the proposals were taken at face value rather than as discrepancies that should be explained away.</p>
<p>The way in which consensus was negotiated in the ENV-SOC panel produced some interesting similarities and differences when compared with the other panels. Like the ECO-ENV and ENV panellists, these experts noted a ‘surprising amount of agreement’ (P14) prior to any discussion. The fact that the panellists’ ratings tended to coincide (see <xref ref-type="fig" rid="fig1-0306312712458478">Figure 1</xref>), regardless of vast differences in their disciplinary and professional backgrounds, was interpreted as a strong sign that the intrinsic quality of the proposals was evident to every evaluator: ‘When scientists with different backgrounds come to the same grade, there is at least <italic>something</italic> in it’ (P12). At the same time, and like the SOC panellists, these interdisciplinary panellists were fully aware that their evaluative norms necessarily were influenced by their membership in particular cognitive and social networks.</p>
<p>In this evaluation context, the panellists avoided developing strong likes or dislikes towards particular proposals, or debating about epistemological preferences. Instead, they cultivated an appreciation for different kinds of research and tried to settle disagreements through mutual learning, compromising, or simply by trusting in each other’s integrity and intuition. As one of the panellists described:
<disp-quote>
<p>It was quite a lot of looking at the criteria, they were up on the flip chart behind us, you know, what the grade ‘one’ was, what the grade ‘two’ was. And that was very useful, because in every slot you could put your hands on your heart and then say to each other: ‘Do you really, honestly, think that it is a “good” proposal, or an “excellent” proposal? What do you think, really?’ (P14)</p>
</disp-quote>
</p>
<p>The ENV-SOC panellists encouraged each other to downplay epistemological differences between disciplines and strengthen what was shared in their conceptions of quality. They often reached agreement through <italic>commensuration</italic>, a process by which the heterogeneous qualities of proposals were transformed into a standardized form in order to be compared (<xref ref-type="bibr" rid="bibr12-0306312712458478">Espeland and Stevens, 1998</xref>). The analysis of evaluation reports and the discussions among panel members revealed that ‘research design’ emerged as an important criterion and also as a point of comparison between different proposals. An environmental sociologist, for instance, explained that in her evaluations, she used the same logic that guided her when teaching a research design course for incoming PhD students. According to her view, science involves a unified methodology that is recognizable across disciplines:
<disp-quote>
<p>Whether they are remote sensing or feminist analysis or tree physiology or anything, science is science is science, and science is based on methodology and that’s based on community … . If a research proposal can’t be read and understood by another scientist from any discipline in terms of its scientific quality, then there is probably something wrong with it. [It should be clear] what the real question of the research is, what your research design is, how many samples you need and how you’re going to think about the population, and how you’re going to make it so that your research question and your methods and your analysis really do lead to reliable findings. It does not really matter so much whether you’re talking about trees or fish or people. (P13)</p>
</disp-quote>
</p>
<p>However, the process of commensuration was sometimes costly and required thorough discussions of methodological questions. This became evident during a series of disagreements between two panel members whose opinions on several case study proposals were far apart. Both were experts in case-study methodology, but their theoretical backgrounds diverged. During a private discussion over breakfast, they came to an agreement concerning where their criteria of evaluation could overlap. One panellist explained:
<disp-quote>
<p>I had not been as critical on [particular methodological choices], because I’ve read [the proposals] in the context that I worked from, and I didn’t have as much problem with these methodological decisions. But I concurred with his concerns when he went through them in some detail. (P13)</p>
</disp-quote>
</p>
<p>These practices made the ENV-SOC panellists more likely to be convinced by one another to change their initial evaluations of proposals. For example, while an economist was worried that a proposal in bioeconomics was not original or significant within the field of economics, he could be persuaded by the other panel members that the proposal should still score well on the basis of its high pragmatic value. In general, these panellists’ broad understanding of expertise, as well as their belief in generalizable criteria, may have caused them to be less critical in their evaluations: the mean value of their preliminary scores, as well as the final scores they gave to the proposals overall, were higher than those given in the other three panels. Content analysis of evaluation reports suggests that the ENV-SOC panellists paid only slight attention to classical disciplinary criteria such as originality.</p>
</sec>
<sec id="section4-0306312712458478" sec-type="discussion">
<title>Discussion</title>
<p>Expert panel judgments have many intersubjective aspects that play a role in the evaluation of research proposals. In this paper, I have analysed only a fraction of the possible reference points that may shape judgments in an intersubjective evaluative context. The reference points I discussed have to do with panellists’ disciplinary expertise and how it resonates with those of other panellists and with the proposals at hand. By comparing the four panel processes, I have illustrated variations in judgment and consensus making. In this section, I will discuss the theoretical meaning of this variance by addressing questions such as: How does disciplinary expertise operate in the making of appraisals and communicating them to other experts? How may we take these insights into account to improve interdisciplinary evaluations?</p>
<p>Studies of peer review have shown that the negotiation of judgments establishes a sphere of social control and reciprocal accountability: the reviewers judge one another’s standards and behaviour as much as they judge the proposals (<xref ref-type="bibr" rid="bibr22-0306312712458478">Hirschauer, 2010</xref>; <xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>). This necessarily influences the way disciplinary expertise is used, since it makes each reviewer’s disciplinary undertakings visible to the others and forces each to articulate her or his viewpoints in relation to those of the others. What is more, deliberations on individual proposals are more or less constrained by the particular set of proposals on the panel’s table. This focus on local comparisons not only governs an individual panellist’s appraisal of a particular proposal, but also other panellists’ reactions to that appraisal (<xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>). Such collective control reinforces reviewers’ perceptions of legitimacy and thus is an important part of their sense of procedural justice. On some occasions, collectively produced legitimacy may also give panellists more leeway and empower them to judge proposals more boldly or beyond their disciplinary expertise.</p>
<p>There appeared to be relatively tight disciplinary control among the ECO-ENV panellists. Since they occupied the same intellectual turf, they tended to view each other’s appraisals in terms of their validity within the discipline and to compete for authority by ‘spotting more problems’ in proposals. The set of proposals they reviewed also was homogeneous enough to enable each panellist to closely monitor the consistency with others in his or her panel who used the criteria for evaluating proposals. This disciplinary competition may have influenced the panellists to hedge their remarks more than they would have done in other evaluative settings. The panel appeared to be quite selective, and possibly to filter out novel, deviant, interdisciplinary or anti-disciplinary proposals. Such a high degree of disciplinary control was not found among members of the other panels, whose accountability to one another became visible in other ways.</p>
<p>The SOC panellists were protective of their own disciplinary territories, but were prepared to give way to those who claimed better expertise. While their sovereignty in disciplinary issues may thus have been relatively high, they were sensitive to their co-panellists’ suggestions that their own evaluative criteria might be unfair to an applicant. For example, one concern that emerged from this panel’s deliberation was a potential ‘British bias’ in its evaluations.</p>
<p>The ENV panellists, in contrast, struggled to make more complete use of the various skills possessed by each panel member, in order to form majority opinions that combined these skills. This form of reciprocity often resulted in complementarity of judgments, as the value of deliberation became seamlessly intertwined with the extension and enrichment of different panellists’ criteria. This complementarity of different evaluation frameworks sometimes led individual reviewers to recognize merits in proposals that they had not previously seen. Thus, in addition to urging criticism of proposals, panellists also encouraged each other’s enthusiasm about, and support for, proposals, even in cases that they deemed to have ‘undetermined’ merit.</p>
<p>There was relatively little disciplinary control in the ENV-SOC panel, and it was sometimes impossible for its members to assess the expertise of other participants or the validity of their arguments. Moreover, as the proposals also tended to be interdisciplinary, their evaluation was more controversial from the outset. However, as with the other panels, deliberation established strong intersubjective ties that played a crucial role in creating trust. Rather than monitoring others’ appraisals for appropriateness, the panellists often based their own judgments on the deliberation itself and on the shared standards that emerged from it. In the absence of an a priori epistemological framework, they believed that deliberation would lead to optimal decisions, as only it could allow for flexibility and for individual participants to develop a shared sense of merit in each case (<xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>).</p>
<p>Attributes of a successful proposal are likely to be somewhat different in evaluation settings where the authority to judge rests more on panel-wide dialogue, rather than on specialized expertise. A proposal has to speak to different audiences in order to receive a high rating after a discussion among diverse experts. In practice, such a proposal has to be written in a way that is easy for a supportive reviewer to present it in a compelling way to other panel members. To some extent, this appeared to be true in the ENV and ENV-SOC panels, but less so in the ECO-ENV and SOC panels. The proposals that received the highest scores in the ENV panel, for instance, typically dealt with interdisciplinary issues that had broad environmental significance, rather than with specialized questions designed to advance the state of the art in particular fields of environmental research. Moreover, only in the ENV and ENV-SOC panels did I find evidence that participants could convince others of the strengths of proposals that they had not yet recognized (in contrast, in all panels it was easier to make a persuasive argument about weak points in proposals).</p>
<p>How, then, may these findings be taken into consideration to make peer review work more effectively? First, some choices have to be made. As highlighted by <xref ref-type="bibr" rid="bibr5-0306312712458478">Chubin and Hackett (1990</xref>; <xref ref-type="bibr" rid="bibr19-0306312712458478">Hackett and Chubin, 2003</xref>), an optimal peer review procedure can hardly be put in place without some trade-offs between the various values the system is asked to serve. As stated at the outset, a particular concern of this paper is to enhance complementary judgments in peer review and thereby the validity of interdisciplinary evaluations. To meet these goals, drawing from my findings, I will highlight the priority of the intersubjective dynamics that encourage individual panellists to stretch their disciplinary standards in the service of dialog and mutual understanding. The comparison of panel deliberations suggests that a panel that develops good interdisciplinary dynamics does not, at the same time, allow much unidisciplinary discretion for individual panel members. As reviewers adapt their behaviour to take account of the views and arguments of other participants, some features of their own expert judgments gain strength while others are left aside. It seems that ‘good’ interdisciplinary and unidisciplinary judgments are not entirely consistent with one another.</p>
<p>When interdisciplinary considerations are given priority, at least two important choices have to be made:</p>
<list id="list1-0306312712458478" list-type="order">
<list-item><p>One important choice in organizing peer review panels concerns the selection of panellists in terms of their degree of specialized expertise. There is a continuum between ‘specialist’ and ‘interdisciplinary’ (or ‘generalist’) panellists. While some aspects of proposals can be successfully assessed only by specialists who really know the subject (such as the adequacy and completeness of the applicant’s account of the state of research, as well as the originality and the methodological correctness of the proposal), for other aspects specialist knowledge is unnecessary or even prejudicial. Significance and impact, for example, as well as pragmatic and societal utility, are often assessed on the basis of a general or interdisciplinary understanding of the field. The choice of panel members thus partly determines which aspects of proposals become decisive. Recruiting generalist panellists’ from a wider pool of specialties is likely to improve the chances of interdisciplinary proposals, because such proposals typically are stronger in the aspects that non-specialists tend to focus on (such as relevance and pragmatic value), whereas they may fall behind in aspects that specialists examine (methodological correctness, stringency or solidity). The findings also suggest that interdisciplinary or generalist panellists use each others’ experience and views as sources of their own judgments and usually have less difficulty with operating within multiple epistemological regimes.</p></list-item>
<list-item><p>Another choice involves the mix of experts in a panel. There is a continuum between ‘unidisciplinary’ and ‘multidisciplinary’ panels, with varying degrees of overlap in competencies. High overlap in reviewers’ competencies in unidisciplinary panels offers greater ‘reliability’ of evaluations, in the sense that panellists may easily calibrate their standards and validate one another. At the same time, a shared value framework may bring about ‘consensual bias’ by filtering out proposals that do not ‘fit in’ to a disciplinary frame (<xref ref-type="bibr" rid="bibr30-0306312712458478">Langfeldt, 2004</xref>; <xref ref-type="bibr" rid="bibr38-0306312712458478">Olbrecht and Bornmann, 2010</xref>). A multidisciplinary panel design, in contrast, ensures a breadth of expertise and creates a shared sense among panellists that they are accountable to multiple disciplinary communities. This encourages panellists to present their views by drawing on sources other than their own particular fields of knowledge. With an optimal amount of overlap, panellists can debate the strengths and weaknesses of different research approaches in relation to the proposal at hand. Too little overlap, however, may cause panellists to divide responsibilities between panel members and forgo such interdisciplinary deliberation.</p></list-item>
</list>
<p>The above choices also depend on other matters, of course. For example, the grouping of proposals, which also implicates the design of panels, needs to be tailored to the size of research fields and overall scientific activity in a country. In a small country such as Finland, even ‘unidisciplinary’ panels generally are broader than in large countries. On the whole, the disciplinary structure of science is much stronger in countries such as the US, where the sheer volume of scholarship within a field is many times higher than in Finland. Structures, however, can be changed in the long term.</p>
<p>In addition to the design of panels, other ill-defined factors are likely to play a role as well, ranging from a reviewer’s personal wisdom and the norms that prevail in her field, to the procedural rules set by the funding organization (see <xref ref-type="bibr" rid="bibr28-0306312712458478">Lamont and Huutoniemi, 2011</xref>). Many differences identified between, for example, the SOC and ENV panels, very likely pertain to the different institutionalized practices of social scientists versus environmental scientists (<xref ref-type="bibr" rid="bibr48-0306312712458478">Whitley, 1984</xref>). The evaluation rules of the Academy of Finland probably give rise to somewhat different consensual practices than, say, a more unstructured procedure where no preliminary reviewers are nominated (see <xref ref-type="bibr" rid="bibr45-0306312712458478">Thorngate et al., 2009</xref>: 107–122). Various properties of the group, such as sex and age distribution and the number of participants, probably influence the deliberation rules, too.</p>
</sec>
<sec id="section5-0306312712458478" sec-type="conclusions">
<title>Conclusions</title>
<p>‘Peer consensus’ is often believed to be an indicator of ‘inter-rater reliability’, and is typically regarded as the most valuable collective product of panel deliberation (see <xref ref-type="bibr" rid="bibr3-0306312712458478">Brenneis, 1994</xref>; <xref ref-type="bibr" rid="bibr6-0306312712458478">Cicchetti, 1991</xref>; <xref ref-type="bibr" rid="bibr9-0306312712458478">Cole et al., 1981</xref>; <xref ref-type="bibr" rid="bibr21-0306312712458478">Hemlin, 2009</xref>; <xref ref-type="bibr" rid="bibr35-0306312712458478">Marsh et al., 2008</xref>). It indeed results in a clear signal for funding decisions. However, as demonstrated by the present analysis and several other studies on the topic, some variance in reviewers’ judgments is inevitable. This hardly means that the outcome of evaluation depends mainly on chance, as some have suggested (<xref ref-type="bibr" rid="bibr9-0306312712458478">Cole et al., 1981</xref>). The present study has been an attempt to make the variance in panel judgments more understandable. It has considered the reasons offered by panellists for disagreement, how the disagreements are negotiated and how such negotiations influence the evaluation outcome.</p>
<p>This study has also demonstrated that most disagreements can be settled through deliberation. Because reviewers understand proposals through their particular scholarly and professional lenses, communication about epistemological differences is a customary practice and can often lead to an agreement. This does not necessarily mean, however, that reviewers agree in the sense of reaching a shared understanding of a proposal’s merits. Instead, the process is as much about agreeing, more or less tacitly, on the conventions for tolerating divergent views, adjusting initial views, and resolving disagreements. Such intersubjective rules are crucial, as they lead panellists to believe that the process is fair. Participants’ faith in the evaluation process, in turn, has a tremendous influence on how well the process works (<xref ref-type="bibr" rid="bibr26-0306312712458478">Lamont, 2009</xref>).</p>
<p>As is often the case with peer review of grant proposals, the panels analysed in this study were required by the funding agency to produce consensual decisions, and they evidently were able to do so. However, their consensual practices varied. The above comparison of deliberative processes suggests that an important, yet understudied, variable for explaining this variation is the mix of disciplinary specialties in a review panel. The requirement to negotiate one’s own judgments with other panel members establishes a local sphere of reciprocal accountability, which necessarily influences the way in which panellists make their evaluations. Depending on other disciplinary standards and other panellists’ behaviour, a panel member may acknowledge the strengths and weaknesses of a proposal differently when negotiating with colleagues from her or his own field than when doing so with colleagues from other fields.</p>
<p>These findings resonate with broader considerations of the competing functions of peer review and the way it relates to scientific knowledge production. It is often acknowledged that there are two different perspectives on peer review that are at odds with each other. On the one hand, those who are concerned with upholding high standards of technical merit prefer review panels that are composed of more narrowly defined established experts. On the other hand, those who wish to promote innovative approaches and socially relevant research prefer review panels that represent more broadly defined constituencies (<xref ref-type="bibr" rid="bibr11-0306312712458478">Eisenhart, 2002</xref>; <xref ref-type="bibr" rid="bibr15-0306312712458478">Fuller, 2002</xref>; <xref ref-type="bibr" rid="bibr19-0306312712458478">Hackett and Chubin, 2003</xref>; <xref ref-type="bibr" rid="bibr47-0306312712458478">Weinberg, 1962</xref>).</p>
<p>The present study concurs with this two-pole view, and suggests that interdisciplinary goals are better served by adjusting the review process towards the latter pole. However, the study also illuminates why the selection of experts is so important. By highlighting the intersubjective dynamics that emerge during panel deliberations, the study suggests that the relationships between panel members create a temporary accountability environment, which, in turn, plays a major role in the kinds of proposals a panel tends to reward. While there is obviously no static relationship that would give judgments a predictable tendency (<xref ref-type="bibr" rid="bibr22-0306312712458478">Hirschauer, 2010</xref>), the intersubjective context of evaluation could be designed in a way that facilitates interdisciplinary dialog.</p>
<p>The requirement of consensus is itself a strategic choice that does not come without consequences. As indicated in this paper, the consequences for the deliberative process and the decisions that follow from it are not self-evident but depend on context. Settling on an average between two extreme scores, for example, has different implications from, say, deferring to the judgment of the technically best expert or relying on a majority rule. We may need to be more conscious about what kind of consensuses our evaluations produce, and to be responsible about the distributive outcomes that follow. At the same time, considered moves could be made to promote consensual practices that lead to complementary judgments more often than they do to stand-offs between incommensurable viewpoints.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This work was supported by the Finnish Post-Graduate School in Science, Technology and Innovation Studies; the Academy of Finland [decision number 120577]; and the Emil Aaltonen Foundation.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0306312712458478">
<label>1.</label>
<p>The year is not given here in order to better guarantee the anonymity of both applicants and evaluators.</p>
</fn>
<fn fn-type="other" id="fn2-0306312712458478">
<label>2.</label>
<p>See <xref ref-type="bibr" rid="bibr39-0306312712458478">Pomerantz (1984)</xref> for an analysis of how agreements in assessments are calibrated in ordinary conversations.</p>
</fn>
</fn-group>
</notes>
<bio>
<title>Biographical note</title>
<p><bold>Katri Huutoniemi</bold> is a doctoral candidate in environmental policy at the University of Helsinki, Finland. Her dissertation title is <italic>Interdisciplinary Accountability in Research Evaluation: Prospects for Quality Control across Disciplinary Boundaries</italic>. Her publications include: ‘Analyzing interdisciplinarity: typology and indicators’, <italic>Research Policy</italic> 39 (2010); ‘Evaluating interdisciplinary research’, in <italic>The Oxford Handbook of Interdisciplinarity</italic> (Oxford University Press, 2010); and ‘Comparing customary rules of fairness: Evaluative practices in various types of peer review panels’, in <italic>Social Knowledge in the Making</italic> (University of Chicago Press, 2011).</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boix Mansilla</surname><given-names>V</given-names></name>
<name><surname>Feller</surname><given-names>I</given-names></name>
<name><surname>Gardner</surname><given-names>H</given-names></name>
</person-group> (<year>2006</year>) <article-title>Quality assessment in interdisciplinary research and education</article-title>. <source>Research Evaluation</source> <volume>15</volume>(<issue>1</issue>): <fpage>69</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr2-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boltanski</surname><given-names>L</given-names></name>
<name><surname>Thévenot</surname><given-names>L</given-names></name>
</person-group> (<year>1999</year>) <article-title>The sociology of critical capacity</article-title>. <source>European Journal of Social Theory</source> <volume>2</volume>(<issue>3</issue>): <fpage>359</fpage>–<lpage>377</lpage>.</citation>
</ref>
<ref id="bibr3-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brenneis</surname><given-names>D</given-names></name>
</person-group> (<year>1994</year>) <article-title>Discourse and discipline at the National Research Council: A bureaucratic Bildungsroman</article-title>. <source>Cultural Anthropology</source> <volume>9</volume>(<issue>1</issue>): <fpage>23</fpage>–<lpage>36</lpage>.</citation>
</ref>
<ref id="bibr4-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bruun</surname><given-names>H</given-names></name>
<name><surname>Hukkinen</surname><given-names>J</given-names></name>
<name><surname>Huutoniemi</surname><given-names>K</given-names></name>
<name><surname>Klein</surname><given-names>JT</given-names></name>
</person-group> (<year>2005</year>) <article-title>Promoting interdisciplinary research: The case of the Academy of Finland</article-title>. <source>Publications of the Academy of Finland 8/05</source>. <publisher-name>The Academy of Finland</publisher-name>, <publisher-loc>Helsinki</publisher-loc>.</citation>
</ref>
<ref id="bibr5-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chubin</surname><given-names>DE</given-names></name>
<name><surname>Hackett</surname><given-names>EJ</given-names></name>
</person-group> (<year>1990</year>) <source>Peerless Science: Peer Review and U.S. Science Policy</source>. <publisher-loc>Albany, NY</publisher-loc>: <publisher-name>State University of New York Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cicchetti</surname><given-names>DV</given-names></name>
</person-group> (<year>1991</year>) <article-title>The reliability of peer review for manuscript and grant submissions: A cross-disciplinary investigation</article-title>. <source>Behavioral and Brain Sciences</source> <volume>14</volume>(<issue>1</issue>): <fpage>119</fpage>–<lpage>135</lpage>.</citation>
</ref>
<ref id="bibr7-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cole</surname><given-names>JR</given-names></name>
<name><surname>Cole</surname><given-names>S</given-names></name>
</person-group> (<year>1981</year>) <source>Peer Review in the National Science Foundation: Phase Two of a Study</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academy Press</publisher-name>.</citation>
</ref>
<ref id="bibr8-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cole</surname><given-names>S</given-names></name>
</person-group> (<year>1992</year>) <source>Making Science: Between Nature and Society</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr9-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cole</surname><given-names>S</given-names></name>
<name><surname>Cole</surname><given-names>JR</given-names></name>
<name><surname>Simon</surname><given-names>GA</given-names></name>
</person-group> (<year>1981</year>) <article-title>Chance and consensus in peer review</article-title>. <source>Science</source> <volume>214</volume>(<issue>4523</issue>): <fpage>881</fpage>–<lpage>886</lpage>.</citation>
</ref>
<ref id="bibr10-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cole</surname><given-names>S</given-names></name>
<name><surname>Rubin</surname><given-names>L</given-names></name>
<name><surname>Cole</surname><given-names>JR</given-names></name>
</person-group> (<year>1978</year>) <source>Peer Review in the National Science Foundation: Phase One of a Study</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academy Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eisenhart</surname><given-names>M</given-names></name>
</person-group> (<year>2002</year>) <article-title>The paradox of peer review: Admitting too much or allowing too little?</article-title> <source>Research in Science Education</source> <volume>32</volume>(<issue>2</issue>): <fpage>241</fpage>–<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr12-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Espeland</surname><given-names>WN</given-names></name>
<name><surname>Stevens</surname><given-names>ML</given-names></name>
</person-group> (<year>1998</year>) <article-title>Commensuration as a social process</article-title>. <source>Annual Review of Sociology</source> <volume>24</volume>: <fpage>313</fpage>–<lpage>343</lpage>.</citation>
</ref>
<ref id="bibr13-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Feller</surname><given-names>I</given-names></name>
</person-group> (<year>2006</year>) <article-title>Multiple actors, multiple settings, multiple criteria: Issues in assessing interdisciplinary research</article-title>. <source>Research Evaluation</source> <volume>15</volume>(<issue>1</issue>): <fpage>5</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr14-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>S</given-names></name>
</person-group> (<year>2001</year>) <source>Against Essentialism. A Theory of Culture and Society</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr15-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fuller</surname><given-names>S</given-names></name>
</person-group> (<year>2002</year>) <source>Knowledge Management Foundations</source>. <publisher-loc>Boston, MA and Oxford</publisher-loc>: <publisher-name>Butterworth-Heinemann</publisher-name>.</citation>
</ref>
<ref id="bibr16-0306312712458478">
<citation citation-type="book">
<collab>General Accounting Office</collab> (<year>1994</year>) <source>Peer Review Reforms Needed to Ensure Fairness in Federal Agency Grant Selection</source>. Report to the Chairman, Committee on Governmental Activities, US Senate. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>General Accounting Office</publisher-name>.</citation>
</ref>
<ref id="bibr17-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Grigg</surname><given-names>L</given-names></name>
</person-group> (<year>1999</year>) <source>Cross-Disciplinary Research</source>. A Discussion Paper. Commissioned Report No. 61. <publisher-loc>Canberra</publisher-loc>: <publisher-name>Australian Research Council</publisher-name>.</citation>
</ref>
<ref id="bibr18-0306312712458478">
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Gulbrandsen</surname><given-names>JM</given-names></name>
</person-group> (<year>2000</year>) <article-title>Research quality and organisational factors: An investigation of the relationship</article-title>. Doctoral dissertation, <publisher-name>Department of Industrial Economics and Technology Management, Norwegian University of Science and Technology</publisher-name>, <publisher-loc>Trondheim</publisher-loc>.</citation>
</ref>
<ref id="bibr19-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hackett</surname><given-names>EJ</given-names></name>
<name><surname>Chubin</surname><given-names>DE</given-names></name>
</person-group> (<year>2003</year>) <source>Peer Review for the 21st Century: Applications to Education Research</source>. <publisher-name>Prepared for a National Research Council Workshop</publisher-name>, <publisher-loc>Washington, DC</publisher-loc>.</citation>
</ref>
<ref id="bibr20-0306312712458478">
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Hemlin</surname><given-names>S</given-names></name>
</person-group> (<year>1991</year>) <article-title>Quality in science: Researchers’ conceptions and judgments</article-title>. Doctoral Dissertation, <publisher-name>Department of Psychology, University of Göteborg</publisher-name>, <publisher-loc>Göteborg, Sweden</publisher-loc>.</citation>
</ref>
<ref id="bibr21-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hemlin</surname><given-names>S</given-names></name>
</person-group> (<year>2009</year>) <article-title>Peer review agreement or peer review disagreement: Which is better?</article-title> <source>Journal of Psychology of Science and Technology</source> <volume>2</volume>(<issue>1</issue>): <fpage>5</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr22-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hirschauer</surname><given-names>S</given-names></name>
</person-group> (<year>2010</year>) <article-title>Editorial judgments: A praxeology of ‘voting’ in peer review</article-title>. <source>Social Studies of Science</source> <volume>40</volume>(<issue>1</issue>): <fpage>71</fpage>–<lpage>103</lpage>.</citation>
</ref>
<ref id="bibr23-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Huutoniemi</surname><given-names>K</given-names></name>
</person-group> (<year>2010</year>) <article-title>Evaluating interdisciplinary research</article-title>. In: <person-group person-group-type="editor">
<name><surname>Frodeman</surname><given-names>R</given-names></name>
<name><surname>Klein</surname><given-names>JT</given-names></name>
<name><surname>Mitcham</surname><given-names>C</given-names></name>
</person-group> (eds) <source>Oxford Handbook of Interdisciplinarity</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>, <fpage>309</fpage>–<lpage>319</lpage>.</citation>
</ref>
<ref id="bibr24-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Huutoniemi</surname><given-names>K</given-names></name>
<name><surname>Klein</surname><given-names>JT</given-names></name>
<name><surname>Bruun</surname><given-names>H</given-names></name>
<name><surname>Hukkinen</surname><given-names>J</given-names></name>
</person-group> (<year>2010</year>) <article-title>Analyzing interdisciplinarity: Typology and indicators</article-title>. <source>Research Policy</source> <volume>39</volume>: <fpage>79</fpage>–<lpage>88</lpage>.</citation>
</ref>
<ref id="bibr25-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Klein</surname><given-names>JT</given-names></name>
</person-group> (<year>2008</year>) <article-title>Evaluation of interdisciplinary and transdisciplinary research: A literature review</article-title>. <source>American Journal of Preventive Medicine</source> <volume>35</volume>: <fpage>S116</fpage>–<lpage>S123</lpage>.</citation>
</ref>
<ref id="bibr26-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lamont</surname><given-names>M</given-names></name>
</person-group> (<year>2009</year>) <source>How Professors Think: Inside the Curious World of Academic Judgment</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr27-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lamont</surname><given-names>M</given-names></name>
<name><surname>Fournier</surname><given-names>M</given-names></name>
<name><surname>Guetzkow</surname><given-names>J</given-names></name>
<name><surname>Mallard</surname><given-names>G</given-names></name>
<name><surname>Bernier</surname><given-names>R</given-names></name>
</person-group> (<year>2007</year>) <article-title>Evaluating creative minds: The assessment of originality in peer review</article-title>. In: <person-group person-group-type="editor">
<name><surname>Sales</surname><given-names>A</given-names></name>
<name><surname>Fournier</surname><given-names>M</given-names></name>
</person-group> (eds) <source>Knowledge, Communication, and Creativity</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Sage</publisher-name>, <fpage>166</fpage>–<lpage>181</lpage>.</citation>
</ref>
<ref id="bibr28-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lamont</surname><given-names>M</given-names></name>
<name><surname>Huutoniemi</surname><given-names>K</given-names></name>
</person-group> (<year>2011</year>) <article-title>Comparing customary rules of fairness: Evaluative practices in various types of peer review panels</article-title>. In: <person-group person-group-type="editor">
<name><surname>Camic</surname><given-names>C</given-names></name>
<name><surname>Gross</surname><given-names>N</given-names></name>
<name><surname>Lamont</surname><given-names>M</given-names></name>
</person-group> (eds) <source>Social Knowledge in the Making</source>. <publisher-loc>Chicago</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>, <fpage>209</fpage>–<lpage>232</lpage>.</citation>
</ref>
<ref id="bibr29-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lamont</surname><given-names>M</given-names></name>
<name><surname>Mallard</surname><given-names>G</given-names></name>
<name><surname>Guetzkow</surname><given-names>J</given-names></name>
</person-group> (<year>2006</year>) <article-title>Beyond blind faith: Overcoming the obstacles to interdisciplinary evaluation</article-title>. <source>Research Evaluation</source> <volume>15</volume>(<issue>1</issue>): <fpage>43</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr30-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Langfeldt</surname><given-names>L</given-names></name>
</person-group> (<year>2004</year>) <article-title>Expert panels evaluating research: Decision-making and sources of bias</article-title>. <source>Research Evaluation</source> <volume>13</volume>(<issue>1</issue>): <fpage>52</fpage>–<lpage>62</lpage>.</citation>
</ref>
<ref id="bibr31-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Langfeldt</surname><given-names>L</given-names></name>
</person-group> (<year>2006</year>) <article-title>The policy challenges of peer review: Managing bias, conflict of interests and interdisciplinary assessments</article-title>. <source>Research Evaluation</source> <volume>15</volume>(<issue>1</issue>): <fpage>31</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr32-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Laudel</surname><given-names>G</given-names></name>
</person-group> (<year>2006</year>) <article-title>Conclave in the Tower of Babel: How peers review interdisciplinary research proposals</article-title>. <source>Research Evaluation</source> <volume>15</volume>(<issue>1</issue>): <fpage>57</fpage>–<lpage>68</lpage>.</citation>
</ref>
<ref id="bibr33-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Laudel</surname><given-names>G</given-names></name>
<name><surname>Origgi</surname><given-names>G</given-names></name>
</person-group> (<year>2006</year>) <article-title>Introduction to a special issue on the assessment of interdisciplinary research</article-title>. <source>Research Evaluation</source> <volume>15</volume>(<issue>1</issue>): <fpage>2</fpage>–<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr34-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mallard</surname><given-names>G</given-names></name>
<name><surname>Lamont</surname><given-names>M</given-names></name>
<name><surname>Guetzkow</surname><given-names>J</given-names></name>
</person-group> (<year>2009</year>) <article-title>Fairness as appropriateness: Negotiating epistemological differences in peer review</article-title>. <source>Science, Technology, &amp; Human Values</source> <volume>34</volume>(<issue>5</issue>): <fpage>573</fpage>–<lpage>606</lpage>.</citation>
</ref>
<ref id="bibr35-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marsh</surname><given-names>HW</given-names></name>
<name><surname>Jayasinghe</surname><given-names>UW</given-names></name>
<name><surname>Bond</surname><given-names>NW</given-names></name>
</person-group> (<year>2008</year>) <article-title>Improving the peer review process for grant applications: Reliability, validity, bias, and generalizability</article-title>. <source>American Psychologist</source> <volume>63</volume>(<issue>3</issue>): <fpage>160</fpage>–<lpage>168</lpage>.</citation>
</ref>
<ref id="bibr36-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Merton</surname><given-names>RK</given-names></name>
</person-group> (<year>1996</year>) <source>On Social Structure and Science</source>. <publisher-loc>Chicago</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr37-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Muller Mirza</surname><given-names>N</given-names></name>
<name><surname>Perret-Clermont</surname><given-names>A-N</given-names></name>
<name><surname>Tartas</surname><given-names>V</given-names></name>
<name><surname>Iannaccone</surname><given-names>A</given-names></name>
</person-group> (<year>2009</year>) <article-title>Psychosocial processes in argumentation</article-title>. In: <person-group person-group-type="editor">
<name><surname>Muller Mirza</surname><given-names>N</given-names></name>
<name><surname>Perret-Clermont</surname><given-names>A-N</given-names></name>
</person-group> (eds) <source>Argumentation and Education: Theoretical Foundations and Practices</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>, <fpage>67</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr38-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Olbrecht</surname><given-names>M</given-names></name>
<name><surname>Bornmann</surname><given-names>L</given-names></name>
</person-group> (<year>2010</year>) <article-title>Panel peer review of grant applications: What do we know from research in social psychology on judgment and decision-making in groups?</article-title> <source>Research Evaluation</source> <volume>19</volume>(<issue>4</issue>): <fpage>293</fpage>–<lpage>304</lpage>.</citation>
</ref>
<ref id="bibr39-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pomerantz</surname><given-names>A</given-names></name>
</person-group> (<year>1984</year>) <article-title>Agreeing and disagreeing with assessments: Some features of preferred/dispreferred turn shapes</article-title>. In: <person-group person-group-type="editor">
<name><surname>Atkinson</surname><given-names>JM</given-names></name>
<name><surname>Heritage</surname><given-names>J</given-names></name>
</person-group> (eds) <source>Structures of Social Action: Studies in Conversation Analysis</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>, <fpage>57</fpage>–<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr40-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>AL</given-names></name>
<name><surname>Rossini</surname><given-names>FA</given-names></name>
</person-group> (<year>1985</year>) <article-title>Peer review of interdisciplinary research proposals</article-title>. <source>Science, Technology, &amp; Human Values</source><volume>10</volume>(<issue>3</issue>): <fpage>33</fpage>–<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr41-0306312712458478">
<citation citation-type="journal">
<collab>Research Evaluation</collab> (<year>2006</year>) <article-title>Special issue on interdisciplinary research assessment</article-title>. <source>Research Evaluation</source> <volume>15</volume>(<issue>1</issue>).</citation>
</ref>
<ref id="bibr42-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Roy</surname><given-names>R</given-names></name>
</person-group> (<year>1985</year>) <article-title>Funding science: The real defects of peer review and an alternative to it</article-title>. <source>Science, Technology, &amp; Human Values</source> <volume>10</volume>(<issue>3</issue>):<fpage>73</fpage>–<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr43-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>MG</given-names></name>
</person-group> (<year>1983</year>) <article-title>Peer review in interdisciplinary research: Flexibility and responsiveness</article-title>. In: <person-group person-group-type="editor">
<name><surname>Epton</surname><given-names>SR</given-names></name>
<name><surname>Payne</surname><given-names>RL</given-names></name>
<name><surname>Pearson</surname><given-names>AW</given-names></name>
</person-group> (eds) <source>Managing Interdisciplinary Research</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>, <fpage>184</fpage>–<lpage>202</lpage>.</citation>
</ref>
<ref id="bibr44-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shimada</surname><given-names>K</given-names></name>
<name><surname>Akagi</surname><given-names>M</given-names></name>
<name><surname>Kazamaki</surname><given-names>T</given-names></name>
<name><surname>Kobayashi</surname><given-names>S</given-names></name>
</person-group> (<year>2007</year>) <article-title>Designing a proposal review process to facilitate interdisciplinary research</article-title>. <source>Research Evaluation</source> <volume>16</volume>(<issue>1</issue>): <fpage>13</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr45-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thorngate</surname><given-names>W</given-names></name>
<name><surname>Dawes</surname><given-names>RM</given-names></name>
<name><surname>Foddy</surname><given-names>M</given-names></name>
</person-group> (<year>2009</year>) <source>Judging Merit</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Psychology Press</publisher-name>.</citation>
</ref>
<ref id="bibr46-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Travis</surname><given-names>GDL</given-names></name>
<name><surname>Collins</surname><given-names>HM</given-names></name>
</person-group> (<year>1991</year>) <article-title>New light on old boys: Cognitive and institutional particularism in the peer review system</article-title>. <source>Science, Technology, &amp; Human Values</source> <volume>16</volume>(<issue>3</issue>): <fpage>322</fpage>–<lpage>341</lpage>.</citation>
</ref>
<ref id="bibr47-0306312712458478">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weinberg</surname><given-names>AM</given-names></name>
</person-group> (<year>1962</year>) <article-title>Criteria for scientific choice</article-title>. <source>Minerva</source> <volume>1</volume>(<issue>2</issue>): <fpage>158</fpage>–<lpage>171</lpage>.</citation>
</ref>
<ref id="bibr48-0306312712458478">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Whitley</surname><given-names>R</given-names></name>
</person-group> (<year>1984</year>) <source>The Intellectual and Social Organization of the Sciences</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Clarendon Press</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>