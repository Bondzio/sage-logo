<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SAG</journal-id>
<journal-id journal-id-type="hwp">spsag</journal-id>
<journal-title>Simulation &amp; Gaming</journal-title>
<issn pub-type="ppub">1046-8781</issn>
<issn pub-type="epub">1552-826X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1046878112444564</article-id>
<article-id pub-id-type="publisher-id">10.1177_1046878112444564</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Task-Relevant Sound and User Experience in Computer-Mediated Firefighter Training</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Houtkamp</surname><given-names>Joske M.</given-names></name>
<xref ref-type="aff" rid="aff1-1046878112444564">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Toet</surname><given-names>Alexander</given-names></name>
<xref ref-type="aff" rid="aff1-1046878112444564">1</xref>
<xref ref-type="aff" rid="aff2-1046878112444564">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bos</surname><given-names>Frank A.</given-names></name>
<xref ref-type="aff" rid="aff1-1046878112444564">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1046878112444564"><label>1</label>Utrecht University, the Netherlands</aff>
<aff id="aff2-1046878112444564"><label>2</label>TNO Human Factors, Soesterberg, the Netherlands</aff>
<author-notes>
<corresp id="corresp1-1046878112444564">Joske M. Houtkamp, Department of Information and Computing Sciences, Utrecht University, P.O. Box 80.089, 3508TB, Utrecht, the Netherlands Email: <email>Joske.Houtkamp@cs.uu.nl</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>43</volume>
<issue>6</issue>
<fpage>778</fpage>
<lpage>802</lpage>
<permissions>
<copyright-statement>© 2012 SAGE Publications</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The authors added task-relevant sounds to a computer-mediated instructor in-the-loop virtual training for firefighter commanders in an attempt to raise the engagement and arousal of the users. Computer-mediated training for crew commanders should provide a sensory experience that is sufficiently intense to make the training viable and effective. In practice, sound is an important source of information for firefighters. During an evaluation of a soundless computer-mediated and instructor in-the-loop virtual training, both trainees and instructors frequently remarked that the lack of sound made the simulation less convincing and engaging. Research on entertainment games has shown that users may experience higher levels of presence, engagement, and arousal when sound is included in the simulations. The authors therefore hypothesized that the addition of task-relevant (informative) sounds to a virtual training would raise the engagement and arousal of the users, and the overall convincingness of the simulation. In this study, they included verisimilar and task-relevant sounds in an instructor in-the-loop computer-mediated firefighter training and assessed how these sounds affect user experience. In contrast to the common belief of trainees and instructors, the authors find that merely adding task-relevant sounds does not necessarily increase the engagement and arousal of the users. The authors conclude that the physical presence of (and verbal communication with) the instructor probably distracted from the simulation, and an integral sound design involving mediated communication with a remotely present instructor may be required to resolve this problem.</p>
</abstract>
<kwd-group>
<kwd>absence of sound</kwd>
<kwd>arousal</kwd>
<kwd>auditory information</kwd>
<kwd>computer-mediated simulation</kwd>
<kwd>convincing</kwd>
<kwd>crew commanders</kwd>
<kwd>emotion</kwd>
<kwd>emotional involvement</kwd>
<kwd>engagement</kwd>
<kwd>firefighters</kwd>
<kwd>lack of sound</kwd>
<kwd>sound</kwd>
<kwd>training</kwd>
<kwd>user experience</kwd>
<kwd>virtual environment</kwd>
<kwd>virtual training</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The design of virtual environments (VEs) has traditionally focused on the quality of the visual representation. Auditory information is frequently lacking, incomplete, or merely added in afterthought as simple soundtracks. Users typically experience the absence of sounds as a deficiency in the simulation (<xref ref-type="bibr" rid="bibr36-1046878112444564">Rohrmann &amp; Bishop, 2002</xref>; <xref ref-type="bibr" rid="bibr37-1046878112444564">Rohrmann, Palmer, &amp; Bishop, 2000</xref>). During an evaluation of a soundless virtual training for firefighter commanders, we noticed that trainees and instructors frequently remarked that the lack of sound made the simulation less convincing and engaging (<xref ref-type="bibr" rid="bibr19-1046878112444564">Houtkamp &amp; Bos, 2007</xref>). It is known that emotionally compelling VEs provide more effective training (<xref ref-type="bibr" rid="bibr14-1046878112444564">Gratch &amp; Marsella, 2003</xref>; <xref ref-type="bibr" rid="bibr51-1046878112444564">Wilfred et al., 2004</xref>), and result in a high degree of initial learning and subsequent retention of the lessons learned (<xref ref-type="bibr" rid="bibr50-1046878112444564">Ulate, 2002</xref>). However, VEs used for serious gaming and training need not be fully realistic, but should provide a sensory experience that is sufficiently convincing for the task at hand to make the training viable and effective. Even the inclusion of a limited set of sounds can significantly contribute to this purpose (<xref ref-type="bibr" rid="bibr10-1046878112444564">Dodiya &amp; Alexandrov, 2007</xref>; <xref ref-type="bibr" rid="bibr48-1046878112444564">Turner, McGregor, Turner, &amp; Carroll, 2003</xref>). It is, for instance, known that music (<xref ref-type="bibr" rid="bibr12-1046878112444564">Ekman, 2008</xref>; <xref ref-type="bibr" rid="bibr16-1046878112444564">Hébert, Béland, Dionne-Fournelle, Crête, &amp; Lupien, 2005</xref>; <xref ref-type="bibr" rid="bibr44-1046878112444564">Tafalla, 2007</xref>) and hyperreal (i.e., exaggerated, louder than normal, or Foley) sounds that signal discrete events (<xref ref-type="bibr" rid="bibr21-1046878112444564">Krebs, 2002</xref>; <xref ref-type="bibr" rid="bibr44-1046878112444564">Tafalla, 2007</xref>) can effectively heighten tension and induce arousal during entertainment video game playing. It seems likely that veridical sounds may also increase the engagement and arousal of users of a serious game provided that the sounds are relevant for the task. In this study, we performed two empirical pilot field studies, to assess if the inclusion of veridical and task-relevant sounds can enhance the engagement and arousal of firefighters in a computer-mediated virtual training. The term <italic>engagement</italic> is used in this study in its broadest sense, encompassing involvement and spatial presence.</p>
<sec id="section1-1046878112444564">
<title>Sound in Virtual Training</title>
<p>The film industry (<xref ref-type="bibr" rid="bibr4-1046878112444564">Chion, 1994</xref>) and, in its wake, game industry (<xref ref-type="bibr" rid="bibr5-1046878112444564">Collins, 2008</xref>) devote many of their resources to develop soundtracks that evoke emotional responses and allow the viewer or player to be engaged, or immersed in the represented world. In contrast, the developers of serious games still focus on the visual environment (<xref ref-type="bibr" rid="bibr11-1046878112444564">Doornbusch, 2002</xref>). A reason may be that the development of a dedicated soundtrack for a virtual training requires a considerable budget and expert knowledge, while the benefits for the users (e.g., in transfer of training) are not immediately evident.</p>
<p>It is known that humans learn more efficiently if they have an emotional involvement with the training scenario (<xref ref-type="bibr" rid="bibr30-1046878112444564">McGaugh, Ferry, Vazdarjanova, &amp; Roozendaal, 2000</xref>; <xref ref-type="bibr" rid="bibr47-1046878112444564">Tulving &amp; Craik, 2000</xref>; <xref ref-type="bibr" rid="bibr50-1046878112444564">Ulate, 2002</xref>). This can be achieved through a simulation that “feels real” and may therefore trigger memories of similar situations and related emotions. The creation of soundtracks that effectively make virtual training scenarios more similar to the experience users would have when exposed to the corresponding situation in reality (and thus make them more convincing, engaging, and memorable) may therefore contribute to the effectiveness of the training.</p>
<p>Sounds can improve the ecological validity and effectiveness of virtual training scenarios by increasing the naturalness of the experience, by providing task-relevant information, and by affecting the users emotionally. First, users expect that the visual events they perceive in the computer-mediated environment are accompanied by sounds, just like they are in reality. The absence of sounds is therefore experienced as unnatural (i.e., it is a distracting factor) and thus degrades the perceived quality of the simulation. Moreover, since users are increasingly accustomed to computer games with high-quality soundtracks, they probably expect an equivalent experience in a virtual training. Any discrepancy between their expectation and the training experience actually provided may seriously degrade its perceived quality (<xref ref-type="bibr" rid="bibr34-1046878112444564">Pettey, Campanella Bracken, Rubenking, Bunche, &amp; Gress, 2010</xref>). Second, sounds in a virtual training can convey information on significant events on which the trainee should act. In addition, sounds can engage and immerse the trainee, which is thought to increase the effectiveness of the training (<xref ref-type="bibr" rid="bibr38-1046878112444564">Rozendaal, Keyson, de Ridder, &amp; Craig, 2009</xref>). Finally, sounds can be deployed to affect the emotional response of viewers (<xref ref-type="bibr" rid="bibr3-1046878112444564">Bradley &amp; Lang, 2000</xref>). Hence, scenarios can be designed to train professionals to perform in conditions in which they will experience negative emotions, stress, and arousal. This is called “affectively intense learning” (<xref ref-type="bibr" rid="bibr51-1046878112444564">Wilfred et al., 2004</xref>) or “stress exposure training” (<xref ref-type="bibr" rid="bibr46-1046878112444564">Tichon, 2007</xref>) and prepares trainees to perform effectively in stressful environments (<xref ref-type="bibr" rid="bibr31-1046878112444564">C. S. Morris, Hancock, &amp; Shirkey, 2004</xref>).</p>
<p>Two important aspects of the affective response that determine the effectiveness of a computer-mediated training are <italic>arousal</italic> and <italic>engagement</italic>. Sounds that present relevant information in the training make the scenario more convincing. A convincing scenario is thought to trigger memories of similar situations and related emotions, and awareness of the importance of performing well. In scenarios representing dangerous situations, this is expected to lead to higher arousal. By attracting attention, sounds also enhance the experience of presence and the engagement of the trainee. Engagement is a widely used concept in studies of game experience and interaction design, and various definitions currently exist (<xref ref-type="bibr" rid="bibr1-1046878112444564">Bianchi-Berthouze, Kim, &amp; Darshak, 2007</xref>; <xref ref-type="bibr" rid="bibr13-1046878112444564">Garris, Ahlers, &amp; Driskell, 2002</xref>; <xref ref-type="bibr" rid="bibr20-1046878112444564">Jennett et al., 2008</xref>; <xref ref-type="bibr" rid="bibr29-1046878112444564">Mallon &amp; Webb, 2000</xref>; <xref ref-type="bibr" rid="bibr38-1046878112444564">Rozendaal et al., 2009</xref>). Here, it is defined as a state of being involved in the training, without effort, and without extrinsic motivation.</p>
<p>It has been observed that the addition of sound to a VE can significantly enhance the level of presence and engagement (<xref ref-type="bibr" rid="bibr6-1046878112444564">Darken, Bernatovich, Lawson, &amp; Peterson, 1999</xref>; Larsson, Västfjäll, &amp; Kleiner, 2001a,<xref ref-type="bibr" rid="bibr24-1046878112444564">b</xref>; <xref ref-type="bibr" rid="bibr17-1046878112444564">Hendrix &amp; Barfield, 1996</xref>; <xref ref-type="bibr" rid="bibr22-1046878112444564">Larsson, Väljamäe, Västfjäll, &amp; Kleiner, 2008</xref>; <xref ref-type="bibr" rid="bibr26-1046878112444564">Lessiter &amp; Freeman, 2001</xref>; <xref ref-type="bibr" rid="bibr25-1046878112444564">Larsson, Västfjäll, Olsson, &amp; Kleiner, 2007</xref>; <xref ref-type="bibr" rid="bibr7-1046878112444564">Dekker &amp; Champion, 2007</xref>; <xref ref-type="bibr" rid="bibr9-1046878112444564">Dinh, Walker, Hodges, Song, &amp; Kobayashi, 1999</xref>) and induce arousal in the user (<xref ref-type="bibr" rid="bibr7-1046878112444564">Dekker &amp; Champion, 2007</xref>; <xref ref-type="bibr" rid="bibr21-1046878112444564">Krebs, 2002</xref>; <xref ref-type="bibr" rid="bibr39-1046878112444564">Sanders &amp; Scorgie, 2002</xref>). However, previous studies mainly focused on the effects of hyperreal (or Foley) sounds and music on tension and arousal during entertainment video game playing (<xref ref-type="bibr" rid="bibr12-1046878112444564">Ekman, 2008</xref>; <xref ref-type="bibr" rid="bibr16-1046878112444564">Hébert et al., 2005</xref>; <xref ref-type="bibr" rid="bibr21-1046878112444564">Krebs, 2002</xref>). One study observed that the inclusion of both task-relevant and task-irrelevant audio information only improved the performance of the most experienced players, probably because these most effectively used the task-relevant information (<xref ref-type="bibr" rid="bibr45-1046878112444564">Tan, Baxa, &amp; Spackman, 2010</xref>). Here, we hypothesize that the inclusion of veridical sounds that provide information that is critical for a successful task performance in an instructor in-the-loop virtual training scenario will increase the convincingness of the simulation and the engagement and arousal of its users.</p>
</sec>
<sec id="section2-1046878112444564">
<title>Experiment 1: Adding Realistic Sounds to a Computer-Mediated Training Environment</title>
<p>In this experiment, we added realistic and task-relevant sounds to an instructor in-the-loop computer-mediated procedural firefighter training, and we assessed the effects of these modifications on the experience of the users. This application allows procedural training on different virtual scenarios, with a range of emergency assistance tools. It has been used for several years by a large number of trainees, and is generally considered a valid and useful training tool. However, both trainees and instructors consistently claim that they miss the information provided by sounds, and the trainees believe that the absence of sounds degrades their engagement with the virtual world and the events therein (<xref ref-type="bibr" rid="bibr19-1046878112444564">Houtkamp &amp; Bos, 2007</xref>). Since the modifications that were applied in this study will only have practical value if they yield significant effects for a large fraction of the trainees, we only tested a small population in a pilot field test and in actual training conditions.</p>
</sec>
<sec id="section3-1046878112444564" sec-type="materials|methods">
<title>Method and Materials</title>
<sec id="section4-1046878112444564">
<title>Design</title>
<p>Two conditions were compared in a between-subjects design: (a) the original virtual firefighter training without sounds and (b) the same scenario, enhanced with sound events and ambient sounds.</p>
</sec>
<sec id="section5-1046878112444564">
<title>Participants</title>
<p>The participants in the first experiment with the virtual training environment were 21 crew commanders (all males), aged between 27 and 55 years (<italic>M</italic> = 42.7 years, <italic>SD</italic> = 8.6). All participants had 8 to 32 years of firefighting experience (<italic>M</italic> = 17.8 years, <italic>SD</italic> = 7.1) and had been crew commanders for 0 to 22 years (<italic>M</italic> = 8.4 years, <italic>SD</italic> = 6.2). The participants had no previous experience with the virtual training exercise used in this study. Participants were randomly assigned to each of the two conditions: 11 to the sound condition and 10 to the no-sound condition.</p>
</sec>
<sec id="section6-1046878112444564">
<title>Equipment and Setup</title>
<p>The experiment was performed in a dimly lit room at the Lieshout fire station (Lieshout, the Netherlands). The VE was generated using a desktop computer with a Pentium 4 processor and an ATI RADEON 2600 graphics card, and was displayed by means of a beamer on a projection screen subtending 1 m wide by 1.5 m high. The trainee was seated on a high chair that was placed at a distance of 2.5 m in front of the screen. The instructor, the operator, and the experimenter were all seated behind tables that were placed perpendicular to the screen, at a distance of 1.5 m from its right edge (<xref ref-type="fig" rid="fig1-1046878112444564">Figure 1</xref>). This setup allowed them a clear view of the trainee and the screen, and is similar to the setup that is typically used in actual examinations. The trainees used a gamepad to move their avatar through the VE. A 5.1 Creative surround sound system, equipped with five speakers and a subwoofer, was used to present the audio signals. The sound level was such that the sounds of the simulation were clearly audible, while the instructor and trainee could still talk to each other. Galvanic skin response (GSR) was measured by attaching a self-made device, consisting of Velcro straps equipped with small metal plates, to a finger of the participant. The signal was read out and stored by a laptop computer at a rate of 28 Hz. Unfortunately, due to a system malfunction, the GSR data could not be analyzed.</p>
<fig id="fig1-1046878112444564" position="float">
<label>Figure 1.</label>
<caption>
<p>Experimental setup</p>
<p>Note: The trainee is seated at the left side, and the instructor, the operator, and the experimenter are seated behind tables at the right.</p>
</caption>
<graphic xlink:href="10.1177_1046878112444564-fig1.tif"/></fig>
</sec>
<sec id="section7-1046878112444564">
<title>The VE</title>
<p>The computer-mediated scenario training environment was developed by VSTEP B.V. (<ext-link ext-link-type="uri" xlink:href="http://www.vstep.nl">www.vstep.nl</ext-link>) and Artesis B.V. in QUEST 3D version 3.6 (<ext-link ext-link-type="uri" xlink:href="http://www.act-3d.com">www.act-3d.com</ext-link>). This application provides procedural firefighter training on virtual scenarios in several environments, with or without an instructor in-the-loop. It includes a range of tools, equipment, and other emergency services that can be used by the trainee.</p>
<p>During the computer-mediated training, the trainees identify with a character that is initially located inside a fire engine (<xref ref-type="fig" rid="fig2-1046878112444564">Figure 2a</xref>), and later in the vicinity of a house (<xref ref-type="fig" rid="fig2-1046878112444564">Figure 2b</xref>) or inside a house (<xref ref-type="fig" rid="fig2-1046878112444564">Figure 2c</xref>). At that stage in the training, the VE represents a typical modern Dutch suburb and contains all elements that are relevant for the scenario, such as water sources. The buildings and objects are geometrically accurate and textured. The representation of the environment is neutral and has not been designed to elicit arousal or tension. Although the characters featuring in the environment (such as firemen, police, and local residents or bystanders) are easily recognizable, their representation is rather schematic. The trainee has the option to toggle between a first-person view and a tethered view (<xref ref-type="fig" rid="fig3-1046878112444564">Figure 3</xref>).</p>
<fig id="fig2-1046878112444564" position="float">
<label>Figure 2.</label>
<caption>
<p>Screen shots from the computer-mediated firefighter training</p>
</caption>
<graphic xlink:href="10.1177_1046878112444564-fig2.tif"/></fig>
<fig id="fig3-1046878112444564" position="float">
<label>Figure 3.</label>
<caption>
<p>(a) First person and (b) tethered point of view</p>
</caption>
<graphic xlink:href="10.1177_1046878112444564-fig3.tif"/></fig>
</sec>
<sec id="section8-1046878112444564">
<title>Training Scenario</title>
<p>The training scenario selected for this study represents the response to a suburban house fire. A major task of trainees using the computer-mediated training is to optimize their situational awareness. They can achieve this by creating a mental model of the location of the fire, and the areas, objects, and victims in its immediate surroundings (i.e., on all six sides of the fire; the mental model is therefore known in practice as the “cube” model). In the real world, commanders use both the visual and auditory information they perceive directly, and information provided indirectly by crewmembers and other human sources to construct their mental model. In their own words, “crew members are the eyes and ears of the commander.” Commanders dynamically need to update and extend their mental model of the situation with incoming information and in response to events, for instance, to include or update the location of the crewmembers, their equipment, the position of potential victims, and the size or the spread of the fire to adjacent locations. In the virtual training, the trainee adopts the role of the fire commander. The instructor reacts verbally to requests from the trainee and impersonates all other characters that occur in the scenario (dispatch center, crewmembers, local residents, police, etc.). Using a separate command module, the operator responds to changes in the situation or requests from the trainee, by performing the appropriate actions in the VE, such as transporting and adding vehicles and avatars, and manipulating the fire. The operator controls the events and logs the main events and actions of the trainee.</p>
<p>At the start of the session, the trainee’s avatar is situated in a fully equipped fire engine at the fire station. When a dispatch call reports a fire in a house, the fire engine leaves the station on its way to the given address. During transit, the trainee’s task is to request all relevant information, to instruct the other members of the team, and to decide which actions need to be taken upon arrival. The operator lets the vehicle arrive at the location of the fire and places the trainee’s avatar outside the vehicle. From that moment on, the trainee is free to navigate in all directions using a gamepad.</p>
<p>The trainee then performs the required procedures, such as gathering information about possible victims, visually locating the fire from outside the house, saving victims, determining sources of danger in the immediate vicinity, inspecting the house, assessing the extent of the fire, and taking actions such as instructing the crew and ordering a second alarm. Before sending the crew inside, the trainee performs a quick outside inspection of the house. Adopting the role of the other crewmembers, the instructor provides the trainee with the required information about the layout of the house, the location of the fire, and any changes in the situation.</p>
</sec>
<sec id="section9-1046878112444564">
<title>Soundtrack</title>
<p>Next to visual information, auditory information evidently plays a major role in the construction of the trainee’s mental model of the situation. Since the original virtual training environment had no soundtrack, we designed, recorded, and validated a dedicated soundtrack consisting of verisimilar ambient and event-related sounds that are relevant for the task performance of firefighters engaging a suburban house fire. We added this soundtrack to the computer-mediated procedural firefighter training in an attempt to assess its effects on user experience. Our hypothesis was that the inclusion of task-relevant veridical sounds would increase the convincingness of the simulation, and the engagement and arousal of its users.</p>
<p>The design process of the soundtrack was as follows: We used a storyboard to identify important locations and events in the scenario. For each scene, a group of expert firefighters determined the type of required sounds (i.e., either ambient sounds, event-related sounds, or communication sounds). For each sound, we assessed the expected effect (i.e., whether the sound would induce a sense of location, provide information on an event, increase the convincingness of the scenario, and/or increase the perceived danger of the situation). Because the instructor performs a dialogue with the participant, there was no need to include communication sounds in the soundtrack. The sounds we used were either selected from existing databases or recorded at firefighting training centers. If necessary, the recorded sounds were separated from other background sounds, and noise was filtered out. Each individual sound was subjected to pretests, to evaluate whether the sound itself (without its visual counterpart) was evocative of the intended environment or event, whether it conferred the intended affective qualities, and whether it was considered appropriate for the visual environment or event that it represented. Because QUEST 3D does not include a physics engine, sound volume changes that should accompany actions like entering or leaving a house, or opening or closing doors, were hardcoded in the scenario. This workaround successfully simulated a physics sound engine. After implementing the soundtrack, its volume was corrected, and the overall simulation was assessed and fine-tuned with the help of expert users.</p>
<p>The resulting soundtrack consists of a continuous, low, background noise, which is appropriate for an urban environment. All relevant events are accompanied by sounds: the engine of the fire truck, pumps, fire, explosion, extinguishing of the fire (with water); sirens indicate the arrival of ambulances and police, and cries represent a victim in distress. In real emergencies, fire commanders wear helmets and continuously receive messages from other emergency responders, which affects the way they perceive the other sounds. We decided to adhere to the customary training situation in which helmets are not worn and communication with the instructor is not hindered by other (less relevant) information sources.</p>
</sec>
<sec id="section10-1046878112444564">
<title>Measures</title>
<p>A questionnaire was used to assess the engagement and arousal of the trainees and the experienced convincingness of the training (see <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>). The following measures were applied:</p>
<list id="list1-1046878112444564" list-type="bullet">
<list-item><p>Self-reported scores on 9 points Valence and Arousal scales of the SAM (Self-Assessment Manikin; <xref ref-type="bibr" rid="bibr2-1046878112444564">Bradley &amp; Lang, 1994</xref>). The SAM is a nonverbal pictorial assessment technique that directly measures momentary feelings of pleasure, arousal, and dominance on a 9-point scale. It is widely used to measure emotional response (<xref ref-type="bibr" rid="bibr8-1046878112444564">Detenber, Simons, &amp; Reiss, 2000</xref>; <xref ref-type="bibr" rid="bibr32-1046878112444564">J. D. Morris, 1995</xref>; <xref ref-type="bibr" rid="bibr33-1046878112444564">J. D. Morris, Woo, Geason, &amp; Kim, 2002</xref>; <xref ref-type="bibr" rid="bibr35-1046878112444564">Poels &amp; Dewitte, 2006</xref>). The Valence scale was applied to determine effects on the perceived pleasure of the experience.</p></list-item>
<list-item><p>Questions on engagement from the ITC-SOPI (The Independent Television Commission’s Sense of Presence Inventory; <xref ref-type="bibr" rid="bibr27-1046878112444564">Lessiter, Freeman, Keogh, &amp; Davidoff, 2001</xref>). The ITC-SOPI is developed to measure users’ experiences of media, without reference to objective system parameters. It consists of four factors: Engagement, Sense of Physical Space, Ecological Validity, and Negative Effects. Only the questions on Engagement were relevant and usable in the context of this experiment. We made a selection and slightly adapted some questions of the ITC-SOPI to the experimental situation (Items 7-9, 11, 18, and 40 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>).</p></list-item>
<list-item><p>Questions about the perceived danger of the situation and the convincingness of the computer-mediated representation of the scenario, the events and victim in the scenario (Items 1-6 and 21-26 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>).</p></list-item>
<list-item><p>To gather additional information on different aspects of the user experience related to Engagement and Convincingness, we included questions on Spatial Presence (the sense of being physically in the VE: Items 13-16 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>), on Involvement (the attention devoted to the VE and the involvement experienced: Items 10, 12, and 17 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>) from the Igroup Presence Questionnaire (<xref ref-type="bibr" rid="bibr40-1046878112444564">Schubert, Friedmann, &amp; Regenbrecht, 2006</xref>). Some questions were slightly adapted to the situation in this experiment.</p></list-item>
<list-item><p>In the sound condition, questions were included to measure the relative contribution of the sounds to the scenario and their perceived quality and appropriateness (Items 27-34 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>).</p></list-item>
<list-item><p>Questions about the general impression of the training and its relevance for daily practice (Items 35-40 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>). These questions were included to measure the perceived validity of the training.</p></list-item></list>
<table-wrap id="table1-1046878112444564" position="float">
<label>Table 1.</label>
<caption>
<p>Questions Related to the Engagement of the User, The Convincingness of the Simulation, and the Importance of the Sounds in the Simulation</p>
</caption>
<graphic alternate-form-of="table1-1046878112444564" xlink:href="10.1177_1046878112444564-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">No.</th>
<th align="center">Item</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>I felt excited during the training.</td>
</tr>
<tr>
<td>2</td>
<td>I found the situation serious.</td>
</tr>
<tr>
<td>3</td>
<td>I was continuously aware of the seriousness of the situation.</td>
</tr>
<tr>
<td>4</td>
<td>The victim’s life was in danger.</td>
</tr>
<tr>
<td>5</td>
<td>I felt relieved after the victim had been saved.</td>
</tr>
<tr>
<td>6</td>
<td>The situation was dangerous for my crew.</td>
</tr>
<tr>
<td>7</td>
<td>I was involved with the events.</td>
</tr>
<tr>
<td>8</td>
<td>The training appealed to me.</td>
</tr>
<tr>
<td>9</td>
<td>I lost track of time.</td>
</tr>
<tr>
<td>10</td>
<td>I was completely captivated by the virtual world.</td>
</tr>
<tr>
<td>11</td>
<td>I felt sad that the training was over.</td>
</tr>
<tr>
<td>12</td>
<td>My attention was diverted during the training.</td>
</tr>
<tr>
<td>13</td>
<td>I felt present in the virtual environment.</td>
</tr>
<tr>
<td>14</td>
<td>I felt surrounded by the virtual environment.</td>
</tr>
<tr>
<td>15</td>
<td>I felt I was just watching pictures.</td>
</tr>
<tr>
<td>16</td>
<td>I felt more like I was acting in the virtual environment than interacting from outside.</td>
</tr>
<tr>
<td>17</td>
<td>I was not aware of my real environment.</td>
</tr>
<tr>
<td>18</td>
<td>I felt myself being “drawn into” the virtual environment.</td>
</tr>
<tr>
<td>19</td>
<td>The simulation reminded me of situations I have encountered in reality.</td>
</tr>
<tr>
<td>20</td>
<td>The simulated scenario could be real.</td>
</tr>
<tr>
<td>21</td>
<td>The victim was convincing.</td>
</tr>
<tr>
<td>22</td>
<td>The environment was convincing.</td>
</tr>
<tr>
<td>23</td>
<td>The scenario was convincing.</td>
</tr>
<tr>
<td>24</td>
<td>The explosion was convincing.</td>
</tr>
<tr>
<td>25</td>
<td>The fire was convincing.</td>
</tr>
<tr>
<td>26</td>
<td>The extinction of the fire was convincing.</td>
</tr>
<tr>
<td>27</td>
<td>Do you recall the sound of the fire? [yes/vaguely/no]</td>
</tr>
<tr>
<td>28</td>
<td>Do you recall the sound of the explosion? [yes/vaguely/no]</td>
</tr>
<tr>
<td>29</td>
<td>Do you recall the sound of the pump? [yes/vaguely/no]</td>
</tr>
<tr>
<td>30</td>
<td>The sounds agreed with the content of the virtual environment.</td>
</tr>
<tr>
<td>31</td>
<td>The sounds were disturbing.</td>
</tr>
<tr>
<td>32</td>
<td>The sound provided information about the seriousness of the situation.</td>
</tr>
<tr>
<td>33</td>
<td>How were the sounds in the simulation? [appropriate/inappropriate |useful/redundant | realistic/disturbing] (choose one of two words in each case)</td>
</tr>
<tr>
<td>34</td>
<td>What did the sounds add to the scenario? (open question)</td>
</tr>
<tr>
<td>35</td>
<td>How was the scenario? [difficult/easy |boring/exciting | complex/simple | short/long ] (choose one of the two options in each case)</td>
</tr>
<tr>
<td>36</td>
<td>I can apply in practice what I learned in the virtual training.</td>
</tr>
<tr>
<td>37</td>
<td>The virtual training was a useful learning experience.</td>
</tr>
<tr>
<td>38</td>
<td>Did you ever experience a similar situation in reality? [never/once or twice/regularly/quite often]</td>
</tr>
<tr>
<td>39</td>
<td>How often have you encountered a situation in which a victim’s life was in danger? [never/once or twice/regularly/quite often ]</td>
</tr>
<tr>
<td>40</td>
<td>I recommend this training.</td>
</tr>
<tr>
<td>41</td>
<td>To build my mental cube model I used (express in percentages)</td>
</tr>
<tr>
<td/>
<td>Information provided by my crew . . . %</td>
</tr>
<tr>
<td/>
<td>Information provided by the dispatch center . . . %</td>
</tr>
<tr>
<td/>
<td>My view on the virtual environment . . . %</td>
</tr>
<tr>
<td/>
<td>My own knowledge and experience . . . %</td>
</tr>
<tr>
<td/>
<td>Sounds from the simulation . . . %</td>
</tr>
<tr>
<td>42</td>
<td>What was the most risky moment in the scenario? (open question)</td>
</tr>
<tr>
<td>43</td>
<td>I determined the most risky moment in the scenario based on (express in percentages)</td></tr>
<tr>
<td/>
<td>Information provided by my crew . . . %</td>
</tr>
<tr>
<td/>
<td>Information provided by the dispatch center . . . %</td>
</tr>
<tr>
<td/>
<td>My view on the virtual environment . . . %</td>
</tr>
<tr>
<td/>
<td>My own knowledge and experience . . . %</td>
</tr>
<tr>
<td/>
<td>Sounds from the simulation . . . %</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1046878112444564"><p>Note: Items 41 to 43 were only administered in Experiment 2. Unless stated otherwise, the questions were answered on a 7-point scale (ranging from 1 = <italic>completely disagree</italic> to 7 = <italic>completely agree</italic>).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Filling in the questionnaire typically took about 5 minutes.</p>
<p>The experiments were performed during official firefighter training sessions. The training instructor scored the task performance of the trainees on a scorecard, which was based on the official score form of the Netherlands Institute for Safety (NIFV; <ext-link ext-link-type="uri" xlink:href="http://www.nifv.nl">www.nifv.nl</ext-link>) and had been adjusted to this virtual training scenario. For privacy reasons, this information was not available to the experimenters.</p>
</sec>
<sec id="section11-1046878112444564">
<title>Procedure</title>
<p>First, the participants filled in a general questionnaire and scored the SAM Arousal and Valence scales. Then the participants performed the training scenario, which typically took about 10 to 12 minutes. Directly after they had finished the training, the second questionnaire was presented, including the SAM Arousal and Valence scales. Then the instructor finished his assessment of the trainee’s performance by filling in his scorecards. Finally, the instructor evaluated his assessment with the trainee. This evaluation was not part of this experiment.</p>
</sec>
<sec id="section12-1046878112444564">
<title>Results</title>
<p>About half of the participants in the no-sound condition (6 out of 11) noticed that sounds were not provided in the scenario. In the sound condition, the trainees were fairly positive about the appropriateness of the sounds for the visual environment (<italic>M</italic> = 4.82, <italic>SD</italic> = 1.17 on a 7-point bipolar scale, <italic>n</italic> = 11), were not disturbed by the sounds (in the sense that the sounds were unrealistic or inappropriate and distracting from the simulation: <italic>M</italic> = 1.91, <italic>SD</italic> = 1.14 on a 7-point bipolar scale, <italic>n</italic> = 11), and agreed slightly that the sounds provided information about the seriousness of the situation (<italic>M</italic> = 4.55, <italic>SD</italic> = 1.21 on a 7-point bipolar scale, <italic>n</italic> = 11). When presented with two options, 90% of the participants considered the sounds good (vs. bad), 90% thought they were loud (vs. low), and all participants considered them helpful (vs. unhelpful) and realistic (vs. unrealistic). All participants had heard the sound of the fire, 82% had heard the sound of the explosion, and 55% had heard the sound of the pumps.</p>
<p><xref ref-type="table" rid="table2-1046878112444564">Table 2</xref> shows the scores on the Arousal and Valence scales of the SAM, obtained directly before and after the training. For Arousal, a score of 3 to 4 means the participants expressed themselves to be rather calm (1 = <italic>very calm</italic>, 9 = <italic>very excited</italic>, and 5 represents the middle of the scale) before and after the training. A score of 7 for Valence means that the participant feels pleasant or happy (9 is the highest score on this scale). We performed a mixed two-way ANOVA, with the SAM scores as within-subjects factors. No significant main effects of condition, Arousal or Valence difference, and no interaction effect were seen. Thus, the training did not have a significant impact on the self-expressed emotional state of the trainee.</p>
<table-wrap id="table2-1046878112444564" position="float">
<label>Table 2.</label>
<caption>
<p>Mean (and <italic>SD</italic>) of the Scores on the Arousal and Valence Scales of the SAM, Obtained Before and Directly After the Training, in the Sound (<italic>n</italic> = 11) and No-Sound (<italic>n</italic> = 10) Conditions</p>
</caption>
<graphic alternate-form-of="table2-1046878112444564" xlink:href="10.1177_1046878112444564-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">SAM scores</th>
<th align="center">No sound</th>
<th align="center">Sound</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Arousal</td>
</tr>
<tr>
<td> Before</td>
<td>3.60 (1.58)</td>
<td>4.00 (1.34)</td>
</tr>
<tr>
<td> After</td>
<td>3.60 (1.71)</td>
<td>4.27 (1.85)</td>
</tr>
<tr>
<td colspan="3">Valence</td>
</tr>
<tr>
<td> Before</td>
<td>7.30 (1.57)</td>
<td>7.09 (1.58)</td>
</tr>
<tr>
<td> After</td>
<td>7.00 (1.63)</td>
<td>7.18 (1.66)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1046878112444564">
<p>Note: SAM = Self-Assessment Manikin.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The construct “Engagement” was measured (on a 7-point bipolar scale) using six questions (Items 7-9, 11, 18, and 40 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>). Cronbach’s alpha for this construct was .80. There was no significant difference (<italic>p</italic> = .97) in the mean Engagement scores between the sound (<italic>M</italic> = 5.35, <italic>SD</italic> = 0.79, <italic>n</italic> = 10) and the no-sound (<italic>M</italic> = 5.33, <italic>SD</italic> = 1.20, <italic>n</italic> = 11) conditions.</p>
<p>The degree of Involvement was assessed (on a 7-point bipolar scale) by three questions based on the Igroup Presence questionnaire (Items 10, 12, and 17 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>; <xref ref-type="bibr" rid="bibr40-1046878112444564">Schubert et al., 2006</xref>). Cronbach’s alpha remained below .6 for this construct (possible reasons are discussed in the “Conclusion” of this section). There was no significant difference (<italic>p</italic> = .88) in the mean Involvement scores between the sound (<italic>M</italic> = 5.03, <italic>SD</italic> = 0.91, <italic>n</italic> = 11) and the no-sound (<italic>M</italic> = 4.9, <italic>SD</italic> = 1.02, <italic>n</italic> = 10) conditions.</p>
<p>Spatial Presence was measured (on a 7-point bipolar scale) with four questions adapted from the Igroup Presence Questionnaire (Items 13-16 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>). Cronbach’s alpha for this construct was .74. There was no significant difference (<italic>p</italic> = .53) in the mean Presence scores between the sound (<italic>M</italic> = 5.18, <italic>SD</italic> = 0.73, <italic>n</italic> = 11) and the no-sound (<italic>M</italic> = 5.45, <italic>SD</italic> = 1.16, <italic>n</italic> = 10) conditions.</p>
<p>The Perceived Danger for the victim and the overall seriousness of the situation were assessed (on a 7-point bipolar scale) through four questions (Items 1-5 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>). The total of the scores did not show a significant difference between the sound and no-sound conditions (no sound <italic>M</italic> = 4.65, <italic>SD</italic> = 1.19, <italic>n</italic> = 10; sound <italic>M</italic> = 4.84, <italic>SD</italic> = 0.54, <italic>n</italic> = 10). The danger was thus perceived as existent, but not high, in both conditions.</p>
<p>Convincingness was measured using five questions on the representation of the victim, the environment, and the events in the scenario (Items 21-23, 25, and 26 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>). The answers were scored on scales from 1 (<italic>not convincing at all</italic>) to 7 (<italic>extremely convincing</italic>). Cronbach’s alpha for this construct was .70. The mean scores for the convincingness of the elements in the scenario were higher in the condition without sounds (<italic>M</italic> = 5.47, <italic>SD</italic> = 1.0, <italic>n</italic> = 10) than in the condition with sounds (<italic>M</italic> = 4.73, <italic>SD</italic> = 0.62, <italic>n</italic> = 11) and were almost significant (<italic>p</italic> = .051, two-tailed).</p>
<p>The performance of the trainees during the training was assessed by the instructor, using standard scorecards. There was no significant difference between the mean performance of trainees in the no-sound and the sound conditions.</p>
</sec>
<sec id="section13-1046878112444564">
<title>Conclusion</title>
<p>Contrary to our hypothesis, we found no significant effect of sound on the arousal and engagement of the trainees. In a separate evaluation session, expert firefighters judged the added sounds appropriate for the simulated events and of good quality. However, in actual training sessions, trainees judged the convincingness of the virtual scenario with the added sound effects almost significantly lower. Two causes seem probable for this effect. First, from our own observations and from comments by the trainees and the instructor, we inferred that the communication between the trainee and the instructor partly masked the sounds. Second, by coincidence, trainees in the sound condition had significantly more experience with 3D VEs and games than trainees in the no-sound condition (on a scale from 0 to 3, respectively, <italic>M</italic> = 1.45, <italic>SD</italic> = 0.82, <italic>n</italic> = 11, and <italic>M</italic> = 0.36, <italic>SD</italic> = 0.67, <italic>n</italic> = 10; <italic>p</italic> = .003). Using a univariate analysis with 3D VE experience as covariate, to correct for this experience, we found no significant difference in convincingness between both conditions, <italic>F</italic>(1, 19) = 1.42, <italic>p</italic> = .25. Experienced gamers are probably accustomed to high-quality soundtracks, which may have affected the present results. We therefore decided to repeat the experiment at a different location, with an enhanced soundtrack and with participants carefully balanced with respect to game experience.</p>
</sec>
</sec>
<sec id="section14-1046878112444564">
<title>Experiment 2: Enhanced Sounds in a Computer-Mediated Training Environment</title>
<p>From the results of the first experiment, we concluded that some sound events probably required more emphasis to be noticed in the context of a computer-mediated instructor in-the-loop simulation. Since it is known in film industry that important sounds need to be “hyper real” (i.e., they should be played louder than normal) to stand off against other sounds and background noise (<xref ref-type="bibr" rid="bibr15-1046878112444564">Hawkins, 2005</xref>), we adjusted the sound volume of two important (informative) sound events in the computer-mediated training scenario from Experiment 2. These were the sounds of the explosion and the screaming victim, and they were increased by 400% and 200%, respectively, compared with their volume in Experiment 1. The volume of the other sounds was kept the same, to prevent an overall increase in background sound level during the training, which would have interfered with the communication between the instructor and the trainee. We administered three extra questions (Items 41-43 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>) to test if participants indeed perceived the sounds and if they also used them for their task performance.</p>
<sec id="section15-1046878112444564">
<title>Method and Conditions</title>
<p>This experiment was performed in a dimly lit room at the Alblasserdam fire station (Alblasserdam, the Netherlands). The acoustics at this location were better than at the location of Experiment 1. As a result, the overall audibility of the soundtrack was better than in Experiment 1.</p>
<p>In this experiment, we included additional questions (Items 41-43 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>) to assess the importance of sound as a source of information, relative to other information sources that are available to the trainees (dispatch center, crewmember, the VE, etc.).</p>
<p>The other methods and materials, and the experimental design, were the same as in Experiment 1 (<xref ref-type="fig" rid="fig4-1046878112444564">Figure 4</xref>).</p>
<fig id="fig4-1046878112444564" position="float">
<label>Figure 4.</label>
<caption>
<p>Experimental setup.</p> <p>Note: The trainee is seated in the middle, and the instructor, the operator and the experimenter are behind tables at the right</p>
</caption>
<graphic xlink:href="10.1177_1046878112444564-fig4.tif"/></fig>
</sec>
<sec id="section16-1046878112444564">
<title>Participants</title>
<p>The participants in the second experiment were 18 crew commanders (all males), aged between 29 and 51 years (<italic>M</italic> = 42.9 years, <italic>SD</italic> = 6.1). They had 0 to 22 years of firefighting experience (<italic>M</italic> = 21.2 years, <italic>SD</italic> = 7.8) and had been crew commanders for 0 to 22 years (<italic>M</italic> = 6.3 years, <italic>SD</italic> = 3.3). The participants had no previous experience with the virtual training exercise used in this study. Because we suspected an effect of gaming experience on the results in Experiment 1, we divided participants with little and extensive experience with gaming and 3D environments equally over the sound and no-sound conditions, resulting in 9 participants in both conditions.</p>
</sec>
<sec id="section17-1046878112444564">
<title>Results</title>
<p>Again, only half of the participants in the condition without sound noticed that sounds were not provided in the scenario (four out of nine trainees). In the sound condition, the trainees were positive on the appropriateness of the sounds for the visual environment (<italic>M</italic> = 5.78, <italic>SD</italic> = 1.09). However, they were also somewhat disturbed by the sounds (<italic>M</italic> = 2.78, <italic>SD</italic> = 1.64). The sounds provided the trainee with information on the seriousness of the situation (<italic>M</italic> = 5.33, <italic>SD</italic> = 1.12, all on a 7-point bipolar scale). When presented with two options, all trainees (<italic>n</italic> = 8, one missing value) considered the sounds to be good (vs. bad), loud (vs. low) and realistic (vs. unrealistic), and all (<italic>n</italic> = 9) considered them to be helpful (vs. unhelpful). All had noticed the sound of the fire, the sound of the explosion, and 66.7% had heard the sound of the pumps.</p>
<p><xref ref-type="table" rid="table3-1046878112444564">Table 3</xref> presents an overview of the results on the relative contribution of sound to different aspects of the user experience. It appears that sound contributes significantly to situational awareness and the assessment of danger, but not to any of the other aspects of user experience investigated in this study. These results will now be discussed in more detail.</p>
<table-wrap id="table3-1046878112444564" position="float">
<label>Table 3.</label>
<caption>
<p>Relative Contribution of Sound to Different Aspects of the User Experience.</p>
</caption>
<graphic alternate-form-of="table3-1046878112444564" xlink:href="10.1177_1046878112444564-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">User experience</th>
<th align="center">Relative contribution of sound</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arousal and Valence</td>
<td>Not significant<sup><xref ref-type="table-fn" rid="table-fn3-1046878112444564">a</xref></sup></td>
</tr>
<tr>
<td>Engagement</td>
<td>Not significant</td>
</tr>
<tr>
<td>Involvement</td>
<td>Not significant</td>
</tr>
<tr>
<td>Spatial Presence</td>
<td>Not significant</td>
</tr>
<tr>
<td>Perceived Danger</td>
<td>Not significant</td>
</tr>
<tr>
<td>Convincingness</td>
<td>Not significant</td>
</tr>
<tr>
<td>Situational awareness</td>
<td>Sound contributes significantly<sup><xref ref-type="table-fn" rid="table-fn4-1046878112444564">b</xref></sup></td>
</tr>
<tr>
<td>Danger assessment</td>
<td>Sound is a major source of information<sup><xref ref-type="table-fn" rid="table-fn5-1046878112444564">c</xref></sup></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-1046878112444564">
<label>a</label>
<p>See <xref ref-type="table" rid="table4-1046878112444564">Table 4</xref>.</p>
</fn>
<fn id="table-fn4-1046878112444564">
<label>b</label>
<p>See <xref ref-type="table" rid="table5-1046878112444564">Table 5</xref> and <xref ref-type="fig" rid="fig5-1046878112444564">Figure 5</xref>.</p>
</fn>
<fn id="table-fn5-1046878112444564">
<label>c</label>
<p>See <xref ref-type="table" rid="table6-1046878112444564">Table 6</xref> and <xref ref-type="fig" rid="fig6-1046878112444564">Figure 6</xref>.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table4-1046878112444564" position="float">
<label>Table 4.</label>
<caption>
<p>Mean (and <italic>SD</italic>) Scores on the SAM Arousal and Valence Scales, Directly Before and After the Training (<italic>n</italic> = 9)</p>
</caption>
<graphic alternate-form-of="table4-1046878112444564" xlink:href="10.1177_1046878112444564-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">SAM scores</th>
<th align="center">No sound</th>
<th align="center">Sound</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arousal</td>
<td/>
<td/></tr>
<tr>
<td>Before</td>
<td>3.33 (1.00)</td>
<td>3.67 (1.94)</td>
</tr>
<tr>
<td>After</td>
<td>3.22 (0.83)</td>
<td>3.89 (1.62)</td>
</tr>
<tr>
<td>Valence</td>
<td/>
<td/></tr>
<tr>
<td>Before</td>
<td>7.11 (1.45)</td>
<td>6.89 (1.76)</td>
</tr>
<tr>
<td>After</td>
<td>7.11 (1.45)</td>
<td>7.67 (0.71)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-1046878112444564">
<p>Note: SAM = Self-Assessment Manikin.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The scores on the Arousal and Valence scales of the SAM were obtained directly before and after the training. We performed a mixed two-way ANOVA, with the SAM scores as within-subjects factors. It showed no significant main effects of condition, or of the arousal or valence difference, nor an interaction effect.</p>
<p>As in Experiment 1, the reliability was satisfactory for Engagement and Spatial Presence, but low for Involvement.</p>
<p>There was no significant difference (<italic>p</italic> = .79) in the mean (<italic>n</italic> = 9) Engagement scores between the sound (<italic>M</italic> = 5.47, <italic>SD</italic> = 1.10) and the no-sound (<italic>M</italic> = 5.35, <italic>SD</italic> = 0.69) conditions.</p>
<p>There was also no significant difference (<italic>p</italic> = .56) in the mean (<italic>n</italic> = 9) Involvement scores between the sound (<italic>M</italic> = 5.33, <italic>SD</italic> = 1.15) and the no-sound (<italic>M</italic> = 5.04, <italic>SD</italic> = 0.94) conditions.</p>
<p>Finally, there was no significant difference (<italic>p</italic> = .56) in the mean (<italic>n</italic> = 9) Spatial Presence scores between the sound (<italic>M</italic> = 4.69, <italic>SD</italic> = 0.86) and the no-sound (<italic>M</italic> = 4.53, <italic>SD</italic> = 1.09) conditions.</p>
<p>The Perceived Danger for the victim and the overall seriousness of the situation were assessed through five questions. Cronbach’s alpha for this scale was .81. The total mean (<italic>n</italic> = 9) of the scores did not show a significant difference (no sound <italic>M</italic> = 5.04, <italic>SD</italic> = 0.65; sound <italic>M</italic> = 5.2, <italic>SD</italic> = 1.60). The danger was thus perceived as existent, but not very high, in both conditions. The Perceived Danger for the crew did not differ significantly between conditions (no sound <italic>n</italic> = 9, <italic>M</italic> = 3.56, <italic>SD</italic> = 1.67; sound n = 9, <italic>M</italic> = 4.11, <italic>SD</italic> = 2.26).</p>
<p>Again, the mean (<italic>n</italic> = 9) of the scores for the Convincingness of the elements in the scenario were higher in the condition without sounds (<italic>M</italic> = 6.18, <italic>SD</italic> = 0.76) than in the condition with sounds (<italic>M</italic> = 5.51, <italic>SD</italic> = 0.77). However, this time, the difference was not significant (<italic>p</italic> = .085, two-tailed). Cronbach’s alpha was satisfactory: α = .63.</p>
<p>The performance during the training was assessed by the instructor, using standard scorecards. Again, the mean (<italic>n</italic> = 9) performance scores in the condition without sound (<italic>M</italic> = 6.96, <italic>SD</italic> = 0.51) and the condition with sound (<italic>M</italic> = 7.11, <italic>SD</italic> = 0.69) were not significantly different (<italic>p</italic> = .62, two-tailed).</p>
<p>Trainees typically use the virtual environment shown on screen, the sounds, the crew, the dispatch center, or their previous knowledge and experience to construct their mental model or ‘cube’. We asked them to rate what fraction of each of these the information sources they had actually used in the experiment. The answers were given in percentages, totaling 100%. In both conditions, the trainees estimated that they derived less than half of the information from the VE and almost the same fraction from the crewmembers and dispatch center (i.e., the instructor; <xref ref-type="fig" rid="fig5-1046878112444564">Figure 5</xref> and <xref ref-type="table" rid="table5-1046878112444564">Table 5</xref>). Knowledge and experience are also important information sources. The variation in the results also shows that the commanders use different strategies. Some trainees did not notice the absence of sound in the no-sound condition. As a result, even in the no-sound condition, sound is still estimated to contribute a minor fraction (1%) to the mental model.</p>
<fig id="fig5-1046878112444564" position="float">
<label>Figure 5.</label>
<caption>
<p>Information sources used by trainees to complete their mental model or “cube,” in the no-sound and sound conditions (<italic>n</italic> = 9)</p> <p>Note: The scores represent percentages, adding up to 100% for each participant (<italic>n</italic> = 9).</p>
</caption>
<graphic xlink:href="10.1177_1046878112444564-fig5.tif"/></fig>
<table-wrap id="table5-1046878112444564" position="float">
<label>Table 5.</label>
<caption>
<p>Information Sources Used by Trainees to Complete Their Mental Model or “Cube,” in the No-Sound and Sound Conditions (<italic>n</italic> = 9)</p>
</caption>
<graphic alternate-form-of="table5-1046878112444564" xlink:href="10.1177_1046878112444564-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">No Sound <italic>M</italic> (<italic>SD</italic>) (%)</th>
<th align="center">Sound <italic>M</italic> (<italic>SD</italic>) (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Visuals</td>
<td>42.67 (20.0)</td>
<td>31.11 (12.69)</td>
</tr>
<tr>
<td>Sounds</td>
<td>1.11 (3.33)</td>
<td>6.67 (4.33)</td>
</tr>
<tr>
<td>Crew</td>
<td>31.67 (23.18)</td>
<td>33.33 (17.14)</td>
</tr>
<tr>
<td>Dispatch center</td>
<td>6.22 (6.87)</td>
<td>7.44 (4.04)</td>
</tr>
<tr>
<td>Knowledge/experience</td>
<td>18.33 (16.77)</td>
<td>21.67 (12.25)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-1046878112444564"><p>Note: The scores represent percentages, adding up to 100% for each participant (<italic>n</italic> = 9).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>To assess dangerous episodes in the scenario, the trainees used the same sources, but now the sounds were said to be important. Again, the VE itself accounts for only a third of the information (<xref ref-type="fig" rid="fig6-1046878112444564">Figure 6</xref> and <xref ref-type="table" rid="table6-1046878112444564">Table 6</xref>).</p>
<fig id="fig6-1046878112444564" position="float">
<label>Figure 6.</label>
<caption>
<p>Information sources used by trainees to assess dangerous episodes in the scenario, in the no-sound (<italic>n</italic> = 8) and the sound (<italic>n</italic> = 9) conditions</p> <p>Note: The scores represent percentages, adding up to 100% for each participant.</p>
</caption>
<graphic xlink:href="10.1177_1046878112444564-fig6.tif"/></fig>
<table-wrap id="table6-1046878112444564" position="float">
<label>Table 6.</label>
<caption>
<p>Information Sources Used by Trainees to Assess Dangerous Episodes in the Scenario, in the No-Sound (<italic>n</italic> = 8) and the Sound (<italic>n</italic> = 9) Conditions</p>
</caption>
<graphic alternate-form-of="table6-1046878112444564" xlink:href="10.1177_1046878112444564-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">No sound <italic>M</italic> (<italic>SD</italic>) (%)</th>
<th align="center">Sound <italic>M</italic> (<italic>SD</italic>) (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Visuals</td>
<td>36.25 (34.20)</td>
<td>38.89 (28.48)</td>
</tr>
<tr>
<td>Sounds</td>
<td>0.00 (0.00)</td>
<td>28.89 (29.35)</td>
</tr>
<tr>
<td>Crew</td>
<td>30.00 (25.07)</td>
<td>17.78 (16.42)</td>
</tr>
<tr>
<td>Dispatch center</td>
<td>6.88 (17.51)</td>
<td>1.67 (3.54)</td>
</tr>
<tr>
<td>Knowledge/experience</td>
<td>31.88 (31.84)</td>
<td>12.78 (13.49)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn8-1046878112444564">
<p>Note: The scores represent percentages, adding up to 100% for each participant.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The results show that sound was an important source of information, both for completing the mental model or “cube” (<italic>p</italic> = .008) and for assessing dangerous episodes (<italic>p</italic> = .018). A mixed ANOVA, with information sources as a within-subjects factor, shows a significant main effect for condition, <italic>F</italic>(1, 15) = 11.57, <italic>p</italic> = .004, and a significant interaction effect for condition and information source, <italic>F</italic>(1, 15) = 4.49, <italic>p</italic> = .05.</p>
</sec>
<sec id="section18-1046878112444564">
<title>Conclusion</title>
<p>The results of Experiment 2 confirmed the findings of Experiment 1: A soundtrack with realistic ambient and event-related sounds did not increase the engagement and arousal of trainees in a computer-mediated instructor in-the-loop training scenario. In contrast to the realistic soundtrack used in Experiment 1, the enhanced soundtrack used in Experiment 2 did not degrade the convincingness of the computer-mediated environment. The present result agrees with an earlier study, which concluded that, in contrast to commonly used techniques in film industry, exaggerated (hyperreal) sounds do not enhance user engagement in virtual-reality simulations (<xref ref-type="bibr" rid="bibr41-1046878112444564">Serafin &amp; Serafin, 2004</xref>).</p>
<p>Sound appears to be more relevant for the assessment of danger, which is an affective task (involving the appraisal of the environment and the events therein), than for constructing the mental model, which is a procedural and cognitive task. This result agrees with the theory that emotional reactions to risks can diverge from cognitive evaluations of the same risks (<xref ref-type="bibr" rid="bibr28-1046878112444564">Loewenstein, Weber, Hsee, &amp; Welch, 2001</xref>; <xref ref-type="bibr" rid="bibr42-1046878112444564">Slovic, Finucane, Peters, &amp; MacGregor, 2004</xref>; <xref ref-type="bibr" rid="bibr43-1046878112444564">Slovic &amp; Peters, 2006</xref>). Sound may stimulate mental images and associations, which induce a feeling of risk (<xref ref-type="bibr" rid="bibr42-1046878112444564">Slovic et al., 2004</xref>), and thereby determine the overall assessment of danger.</p>
<p>Some trainees estimated that sound contributed slightly (a fraction of 1%; see <xref ref-type="table" rid="table5-1046878112444564">Table 5</xref>) to their mental model in the no-sound condition. Questions related to sound may, in principle, induce the formation of false memories in a no-sound condition. In an attempt to minimize this effect, we first explicitly asked the participants whether they recalled hearing some of the characteristic sounds in the simulation (Items 27-29 in <xref ref-type="table" rid="table1-1046878112444564">Table 1</xref>). In addition, we deliberately kept the questionnaire short so that it could be completed in a minimal amount of time, and we presented it to the participants immediately after finishing their training, while their memories were still fresh.</p>
</sec>
</sec>
<sec id="section19-1046878112444564" sec-type="discussion|conclusions">
<title>Conclusion and Discussion</title>
<p>The addition of a realistic soundtrack with emphasized task-relevant sounds to a computer-mediated instructor in-the-loop firefighter training did not increase the arousal and engagement of the trainees. The original version of this soundtrack even degraded the convincingness of the computer-mediated environment (Experiment 1). It is unlikely that the quality of the sounds or their implementation were insufficient, since the soundtrack was carefully designed and evaluated with the help of expert firefighters, and the trainees judged the sounds as appropriate and adequate.</p>
<p>From our own observations of the training program for firefighters over a longer period of time, and from discussions with expert firefighters, we conclude that the sounds in this study probably did not have an effect on arousal or engagement because the trainees were not fully involved with the virtual world, which is a prerequisite for engagement. The sounds used in the experiments were probably not potent enough to overcome the distractions provided by the physical presence of the instructor and induce an emotional response in the trainees. Further studies in which instructor presence is an independent variable (e.g., by replacing the physical presence of the instructor by mediated communication) are required to investigate this issue.</p>
<p>In addition to visual and auditory information, commanders heavily rely on information provided by fellow crewmembers and other human sources to construct their mental model. The trainees probably focused on the information verbally provided by the instructor and therefore paid less attention to the events that were displayed and represented by the sounds. When the operator and the computer-mediated simulation provided contradictory information (which sometimes happened when the operator made a slight mistake in the scenario), the trainee always relied on the information provided by the crew (i.e., the instructor) and ignored the events that were displayed on the screen or indicated through the soundtrack. About half of the trainees in the soundless condition did not even notice the absence of sounds. They apparently completed their mental model of the events using other information sources. Support for this argument is supplied by the importance that trainees attributed to the crew and the dispatch center, and to their experience, as information sources for completing their mental model (Experiment 2). Further evidence is also provided by the involvement scores, which varied considerably between participants and between questions. This may have contributed to the low internal reliability of the involvement scale. In addition, the fact that some trainees experienced problems with navigation and the use of the controller may also have diminished their engagement with the virtual world (<xref ref-type="bibr" rid="bibr27-1046878112444564">Lessiter et al., 2001</xref>).</p>
<p>We expected that task-relevant sounds would raise the convincingness of the computer-mediated training and thus increase the arousal of the trainees. However, the convincingness was judged lower in the condition with sound in Experiment 1. Although the sounds supported the scenario and served to aggravate the situation, the scenario itself did not involve any extreme risks or unforeseen complications. Participants could therefore rely on the standard procedures, which they had frequently trained before in role-playing games (although not in a VE setting). They did not perceive the scenario as complex or dangerous and were mostly in control of the situation. Also, firefighters are generally self-assured individuals who do not easily get nervous or show strong emotional response to danger. The combination of a typical scenario, an extensive training, and a high degree of self-confidence may have resulted in rather small emotional effects that may require measures that are more sensitive than the SAM.</p>
<p>Factors related to user characteristics and content of the training may also have contributed to the unexpected results on the assessment of the convincingness of the VE. First, as we noted before, sound effects in entertainment games and films often need to be exaggerated (made hyperreal) in order to be noticed or to be perceived as “real” or convincing. A similar effect may occur in computer-mediated trainings. This suggests that trainees experiencing a virtual scenario may expect similar “hyperreal” sounds or may not even be aware of sounds when they are not accentuated. However, in Experiment 2, we found that exaggerated (hyperreal) sounds do not enhance engagement and arousal. This effect has also been observed in the context of panoramic landscape visualizations (<xref ref-type="bibr" rid="bibr41-1046878112444564">Serafin &amp; Serafin, 2004</xref>). Second, in real incidents, communication over the radio, directed to the commander and to the other professionals involved, is an important part of the “soundscape.” In those conditions, several voices may be heard simultaneously and indistinctly, and the commander must filter out the information that is relevant for his task. In the training, the auditory environment is much simpler, since the instructor is the only one speaking, and offers the information clearly audible and sequentially. By splitting event-related sounds from communication sounds, the trainees may have focused only on the communication with the instructor.</p>
<p>We conclude that verisimilar and task-relevant sounds do not affect the arousal and engagement of users of a computer-mediated instructor in-the-loop training when other factors (e.g., the physical presence of and direct communication with an instructor, navigation problems, or operator errors) prevent them from focusing their full attention on the computer-mediated simulation. A previous study indicated that distraction may indeed prevent users from successfully using task-relevant audio information (<xref ref-type="bibr" rid="bibr45-1046878112444564">Tan et al., 2010</xref>). Making the auditory effects hyperreal (exaggerated) does not help to overcome the distractions provided by the training context and fails to achieve “affectively intense training.” Also, enhanced sound effects and nonrealistic sounds cannot be deployed when accurate audio-visual representations of incidents are essential for the training scenario (<xref ref-type="bibr" rid="bibr41-1046878112444564">Serafin &amp; Serafin, 2004</xref>). A better option is probably to increase the realism of the simulation by integrating the communication with a remotely present instructor in the computer-mediated soundscape. In the development of virtual training scenarios, individual and explicit requirements should therefore be made for the learning goals and for the desired affective response of the trainees, to establish and augment relevant sounds (<xref ref-type="bibr" rid="bibr18-1046878112444564">Hoorn, Konijn, &amp; van der Veer, 2003</xref>) that address the different listening modes involved (<xref ref-type="bibr" rid="bibr49-1046878112444564">Tuuri, Mustonen, &amp; Pirhonen, 2007</xref>). Dependent on the relative importance of the affective response of the trainees, these requirements may lead to the identification of different types of training environments.</p>
</sec>
</body>
<back>
<ack>
<p>We thank Per Backlund, Igor Mayer, and Karolien Poels for their initially blind, and subsequently coaching, reviews of this article and for their numerous insightful comments and suggestions. Above all we thank David Crookall for his continuous encouragement and support.</p></ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors declared the following potential conflicts of interest with respect to the research, authorship, and/or publication of this article: This research has been supported by the GATE project, funded by the Netherlands Organization for Scientific Research (NWO) and the Netherlands ICT Research and Innovation Authority (ICT Regie).</p>
</fn>
</fn-group>
<bio>
<title>Bios</title>
<p><bold>Joske M. Houtkamp</bold> is researcher at Alterra (Wageningen University and Research Centre). She is a researcher at Alterra (Wageningen University and Research Centre). Her research interests include virtual environments and serious games, with a focus on users’ affective response to visualizations and virtual training environments.</p>
<p>Contact: <email>joske.houtkamp@wur.nl</email>.</p>
<p><bold>Alexander Toet</bold> is a senior researcher in the perception and simulation department of TNO Human Factors. He is a senior researcher in the perception and simulation department of TNO Human Factors and a guest researcher at the Department of Information and Computing Sciences, University Utrecht. He currently investigates crossmodal perceptual interactions between the visual, auditory, tactile, and olfactory senses, with the aim to deploy these interactions to enhance the perceived quality of serious gaming programs for training and simulation of search and rescue operations.</p>
<p>Contact: <email>lextoet@gmail.com</email>.</p>
<p><bold>Frank A. Bos</bold> is a Utrecht University graduate of the master Content and Knowledge Engineering in Information Sciences. His research interests include learning with modern technologies like serious gaming and e-learning.</p>
<p>Contact: <email>bosf@live.nl</email>.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1046878112444564">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Bianchi-Berthouze</surname><given-names>N.</given-names></name>
<name><surname>Kim</surname><given-names>W. W.</given-names></name>
<name><surname>Darshak</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Does body movement engage you more in digital game play? And Why?</article-title> In <conf-name>Proceedings of the Second International Conference on Affective Computing and Intelligent Interaction ACII 2007</conf-name> (pp. <fpage>102</fpage>-<lpage>113</lpage>). <conf-loc>Berlin/Heidelberg, Germany: Springer</conf-loc>.</citation>
</ref>
<ref id="bibr2-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bradley</surname><given-names>M. M.</given-names></name>
<name><surname>Lang</surname><given-names>P. J.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Measuring emotion: The Self-Assessment Manikin and the semantic differential</article-title>. <source>Journal of Behavior Therapy and Experimental Psychiatry</source>, <volume>25</volume>, <fpage>49</fpage>-<lpage>59</lpage>.</citation>
</ref>
<ref id="bibr3-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bradley</surname><given-names>M. M.</given-names></name>
<name><surname>Lang</surname><given-names>P. J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Affective reactions to acoustic stimuli</article-title>. <source>Psychophysiology</source>, <volume>37</volume>, <fpage>204</fpage>-<lpage>215</lpage>.</citation>
</ref>
<ref id="bibr4-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chion</surname><given-names>M.</given-names></name>
</person-group> (<year>1994</year>). <source>Audio-vision: sound on screen</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Columbia University Press</publisher-name>.</citation>
</ref>
<ref id="bibr5-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Collins</surname><given-names>K.</given-names></name>
</person-group> (<year>2008</year>). <source>Game sound</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Darken</surname><given-names>R. P.</given-names></name>
<name><surname>Bernatovich</surname><given-names>D.</given-names></name>
<name><surname>Lawson</surname><given-names>J.</given-names></name>
<name><surname>Peterson</surname><given-names>B.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Quantitative measures of presence in virtual environments: The role of attention and spatial comprehension</article-title>. <source>CyberPsychology &amp; Behavior</source>, <volume>2</volume>, <fpage>337</fpage>-<lpage>347</lpage>.</citation>
</ref>
<ref id="bibr7-1046878112444564">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Dekker</surname><given-names>A.</given-names></name>
<name><surname>Champion</surname><given-names>E.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Please biofeed the zombies: Enhancing the gameplay and display of a horror game using biofeedback</article-title>. In <conf-name>Situated Play—Proceedings of the Third International Conference of the Digital Games Research Association (DiGRA)</conf-name> (pp. <fpage>550</fpage>-<lpage>558</lpage>). Digital Games Research Association (DiGRA). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.digra.org">www.digra.org</ext-link></citation>
</ref>
<ref id="bibr8-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Detenber</surname><given-names>B.</given-names></name>
<name><surname>Simons</surname><given-names>R. F.</given-names></name>
<name><surname>Reiss</surname><given-names>J. E.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The emotional significance of color in television presentations</article-title>. <source>Media Psychology</source>, <volume>2</volume>, <fpage>331</fpage>-<lpage>355</lpage>.</citation>
</ref>
<ref id="bibr9-1046878112444564">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Dinh</surname><given-names>H. Q.</given-names></name>
<name><surname>Walker</surname><given-names>N.</given-names></name>
<name><surname>Hodges</surname><given-names>L. F.</given-names></name>
<name><surname>Song</surname><given-names>C.</given-names></name>
<name><surname>Kobayashi</surname><given-names>A.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments</article-title>. In <conf-name>Proceedings of the Virtual Reality Annual International Symposium</conf-name> (pp. <fpage>222</fpage>-<lpage>228</lpage>). <conf-loc>Washington, DC: IEEE Press</conf-loc>.</citation>
</ref>
<ref id="bibr10-1046878112444564">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Dodiya</surname><given-names>J.</given-names></name>
<name><surname>Alexandrov</surname><given-names>V. N.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Perspectives on potential of sound in virtual environments</article-title>. In <conf-name>Proceedings of the IEEE International Workshop on Haptic, Audio and Visual Environments and Games (HAVE 2007)</conf-name> (pp. <fpage>15</fpage>-<lpage>20</lpage>). <conf-loc>Washington, DC: IEEE Press</conf-loc>.</citation>
</ref>
<ref id="bibr11-1046878112444564">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Doornbusch</surname><given-names>P.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Sound and reality</article-title>. In <conf-name>Proceedings of the SimTech Conference (CDROM)</conf-name> (pp. <fpage>1</fpage>-<lpage>5</lpage>). <conf-loc>Melbourne, Australia: University of Melbourne</conf-loc>.</citation>
</ref>
<ref id="bibr12-1046878112444564">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>I.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Psychologically motivated techniques for emotional sound in computer games</article-title>. In <source>Proceedings of AudioMostly 2008</source> (pp. <fpage>20</fpage>-<lpage>26</lpage>). Piteå, Sweden. Retrived from <ext-link ext-link-type="uri" xlink:href="http://www.audiomostly.com/">http://www.audiomostly.com/</ext-link></citation>
</ref>
<ref id="bibr13-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Garris</surname><given-names>R.</given-names></name>
<name><surname>Ahlers</surname><given-names>R.</given-names></name>
<name><surname>Driskell</surname><given-names>J. E.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Games, motivation, and learning: A research and practice model</article-title>. <source>Simulation &amp; Gaming</source>, <volume>33</volume>, <fpage>441</fpage>-<lpage>467</lpage>.</citation>
</ref>
<ref id="bibr14-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gratch</surname><given-names>J.</given-names></name>
<name><surname>Marsella</surname><given-names>S.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Fight the way you train: the role and limits of emotions in training for combat</article-title>. <source>Brown Journal of World Affairs</source>, <volume>10</volume>, <fpage>63</fpage>-<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr15-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hawkins</surname><given-names>B.</given-names></name>
</person-group> (<year>2005</year>). <source>Real-time cinematography for games</source>. <publisher-loc>Hingham, MA</publisher-loc>: <publisher-name>Charles River Media</publisher-name>.</citation>
</ref>
<ref id="bibr16-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hébert</surname><given-names>S.</given-names></name>
<name><surname>Béland</surname><given-names>R.</given-names></name>
<name><surname>Dionne-Fournelle</surname><given-names>O.</given-names></name>
<name><surname>Crête</surname><given-names>M.</given-names></name>
<name><surname>Lupien</surname><given-names>S. J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Physiological stress response to video-game playing: The contribution of built-in music</article-title>. <source>Life Sciences</source>, <volume>76</volume>, <fpage>2371</fpage>-<lpage>2380</lpage>.</citation>
</ref>
<ref id="bibr17-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hendrix</surname><given-names>C.</given-names></name>
<name><surname>Barfield</surname><given-names>W.</given-names></name>
</person-group> (<year>1996</year>). <article-title>The sense of presence within auditory virtual environments</article-title>. <source>Presence: Teleoperators and Virtual Environments</source>, <volume>5</volume>, <fpage>290</fpage>-<lpage>301</lpage>.</citation>
</ref>
<ref id="bibr18-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hoorn</surname><given-names>J. F.</given-names></name>
<name><surname>Konijn</surname><given-names>E. A.</given-names></name>
<name><surname>van der Veer</surname><given-names>G. C.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Virtual reality: Do not augment realism, augment relevance</article-title>. <source>UPGRADE, IV</source>, <volume>4</volume>(<issue>1</issue>), <fpage>18</fpage>-<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr19-1046878112444564">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Houtkamp</surname><given-names>J. M.</given-names></name>
<name><surname>Bos</surname><given-names>F. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Evaluation of a virtual scenario training for leading firefighters</article-title>. In <person-group person-group-type="editor">
<name><surname>Van de Walle</surname><given-names>B.</given-names></name>
<name><surname>Burghardt</surname><given-names>P.</given-names></name>
<name><surname>Nieuwenhuis</surname><given-names>C.</given-names></name>
</person-group> (Eds.), <conf-name>Proceedings of the 4th International Conference on Information Systems for Crisis Response and Management ISCRAM2007</conf-name> (pp. <fpage>565</fpage>-<lpage>570</lpage>). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.iscram.org/">http://www.iscram.org/</ext-link></citation>
</ref>
<ref id="bibr20-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jennett</surname><given-names>C.</given-names></name>
<name><surname>Cox</surname><given-names>A. L.</given-names></name>
<name><surname>Caims</surname><given-names>P.</given-names></name>
<name><surname>Dhoparee</surname><given-names>S.</given-names></name>
<name><surname>Epps</surname><given-names>A.</given-names></name>
<name><surname>Tijs</surname><given-names>T.</given-names></name>
<name><surname>Walton</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Measuring and defining the experience of immersion in games</article-title>. <source>International Journal of Human-Computer Studies</source>, <volume>66</volume>, <fpage>641</fpage>-<lpage>661</lpage>.</citation>
</ref>
<ref id="bibr21-1046878112444564">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Krebs</surname><given-names>E. M.</given-names></name>
</person-group> (<year>2002</year>). <source>An audio architecture integrating sound and live voice for virtual environments</source>. (Unpublished doctoral dissertation). <publisher-name>Naval Post Graduate School</publisher-name>, <publisher-loc>Monterey, California</publisher-loc>.</citation>
</ref>
<ref id="bibr22-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Larsson</surname><given-names>P.</given-names></name>
<name><surname>Väljamäe</surname><given-names>A.</given-names></name>
<name><surname>Västfjäll</surname><given-names>D.</given-names></name>
<name><surname>Kleiner</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Auditory induced presence in mediated environments and related technology</article-title>. In <source>Immersed in media experiences: Presence psychology and design (handbook of presence)</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr23-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Larsson</surname><given-names>P.</given-names></name>
<name><surname>Västfjäll</surname><given-names>D.</given-names></name>
<name><surname>Kleiner</surname><given-names>M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Do we really live in a silent world? The (mis-) use of audio in virtual environments</article-title>. In <person-group person-group-type="editor">
<name><surname>Tullberg</surname><given-names>O.</given-names></name>
<name><surname>Dawood</surname><given-names>N.</given-names></name>
<name><surname>Connell</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <source>Proceedings Conference on Applied Virtual Reality in Engineering &amp; Construction Applications of Virtual Reality</source> (AVR II &amp; CONVR 2001) (pp. <fpage>182</fpage>-<lpage>189</lpage>). <publisher-loc>Göteborg, Sweden</publisher-loc>: <publisher-name>Chalmers University of Technology</publisher-name>.</citation>
</ref>
<ref id="bibr24-1046878112444564">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Larsson</surname><given-names>P.</given-names></name>
<name><surname>Västfjäll</surname><given-names>D.</given-names></name>
<name><surname>Kleiner</surname><given-names>M.</given-names></name>
</person-group> (<year>2001b</year>). <article-title>Ecological acoustics and the multi-modal perception of rooms: Real and unreal experiences of auditory-visual virtual environments</article-title>. In <conf-name>Proceedings of the 7th International Conference on Auditory Display</conf-name> (ICAD 2001) (pp. <fpage>245</fpage>-<lpage>259</lpage>). <conf-loc>Espoo, Finland: Helsinki University of Technology. PROCEEDINGS ISBN</conf-loc>: 978-0-9792217-1-2</citation>
</ref>
<ref id="bibr25-1046878112444564">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Larsson</surname><given-names>P.</given-names></name>
<name><surname>Västfjäll</surname><given-names>D.</given-names></name>
<name><surname>Olsson</surname><given-names>P.</given-names></name>
<name><surname>Kleiner</surname><given-names>M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>When what you hear is what you see: Presence and auditory-visual integration in virtual environments</article-title>. In <conf-name>Proceedings of the 10th Annual International Workshop on Presence</conf-name> (Presence 2007) (pp. <fpage>11</fpage>-<lpage>18</lpage>). The International Society for Presence Research (ISPR). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ispr.info/">http://ispr.info/</ext-link></citation>
</ref>
<ref id="bibr26-1046878112444564">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Lessiter</surname><given-names>J.</given-names></name>
<name><surname>Freeman</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <source>Really hear? The effects of audio quality on presence</source>. In <conf-name>Proceedings of the 4th International Workshop on Presence</conf-name> (pp. <fpage>288</fpage>-<lpage>324</lpage>). <conf-loc>Philadelphia, USA: Temple University</conf-loc>.</citation>
</ref>
<ref id="bibr27-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lessiter</surname><given-names>J.</given-names></name>
<name><surname>Freeman</surname><given-names>J.</given-names></name>
<name><surname>Keogh</surname><given-names>E.</given-names></name>
<name><surname>Davidoff</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2001</year>). <article-title>A cross-media presence questionnaire: The ITC Sense of Presence Inventory</article-title>. <source>Presence: Teleoperators and Virtual Environments</source>, <volume>10</volume>, <fpage>282</fpage>-<lpage>297</lpage>.</citation>
</ref>
<ref id="bibr28-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Loewenstein</surname><given-names>G. F.</given-names></name>
<name><surname>Weber</surname><given-names>E. U.</given-names></name>
<name><surname>Hsee</surname><given-names>C. K.</given-names></name>
<name><surname>Welch</surname><given-names>E. S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Risk as feelings</article-title>. <source>Psychological Bulletin</source>, <volume>127</volume>, <fpage>267</fpage>-<lpage>286</lpage>.</citation>
</ref>
<ref id="bibr29-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mallon</surname><given-names>B.</given-names></name>
<name><surname>Webb</surname><given-names>B.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Structure, causality, visibility and interaction: Propositions for evaluating engagement in narrative multimedia</article-title>. <source>International Journal of Human-Computer Studies</source>, <volume>53</volume>, <fpage>269</fpage>-<lpage>287</lpage>.</citation>
</ref>
<ref id="bibr30-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McGaugh</surname><given-names>J. L.</given-names></name>
<name><surname>Ferry</surname><given-names>B.</given-names></name>
<name><surname>Vazdarjanova</surname><given-names>A.</given-names></name>
<name><surname>Roozendaal</surname><given-names>B.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Role in modulation of memory storage</article-title>. In <person-group person-group-type="editor">
<name><surname>Aggleton</surname><given-names>J. P.</given-names></name>
</person-group> (Ed.), <source>The amygdala: a functional analysis</source> (pp. <fpage>391</fpage>-<lpage>423</lpage>). <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr31-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Morris</surname><given-names>C. S.</given-names></name>
<name><surname>Hancock</surname><given-names>P. A.</given-names></name>
<name><surname>Shirkey</surname><given-names>E. C.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Motivational effects of adding context relevant stress in PC-based game training</article-title>. <source>Military Psychology</source>, <volume>16</volume>, <fpage>135</fpage>-<lpage>147</lpage>.</citation>
</ref>
<ref id="bibr32-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Morris</surname><given-names>J. D.</given-names></name>
</person-group> (<year>1995</year>). <article-title>SAM: The Self-Assessment Manikin: An efficient cross-cultural measurement of emotional response</article-title>. <source>Journal of Advertising Research</source>, <volume>35</volume>, <fpage>1</fpage>-<lpage>5</lpage>.</citation>
</ref>
<ref id="bibr33-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Morris</surname><given-names>J. D.</given-names></name>
<name><surname>Woo</surname><given-names>C.</given-names></name>
<name><surname>Geason</surname><given-names>J. A.</given-names></name>
<name><surname>Kim</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>The power of affect: Predicting intention</article-title>. <source>Journal of Advertising Research</source>, <volume>42</volume>, <fpage>7</fpage>-<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr34-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pettey</surname><given-names>G.</given-names></name>
<name><surname>Campanella Bracken</surname><given-names>C.</given-names></name>
<name><surname>Rubenking</surname><given-names>B.</given-names></name>
<name><surname>Bunche</surname><given-names>M.</given-names></name>
<name><surname>Gress</surname><given-names>E.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Telepresence, soundscapes and technological expectation: Putting the observer into the equation</article-title>. <source>Virtual Reality</source>, <volume>14</volume>, <fpage>15</fpage>-<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr35-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Poels</surname><given-names>K.</given-names></name>
<name><surname>Dewitte</surname><given-names>S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>How to capture the heart? Reviewing 20 years of emotion measurement in advertising</article-title>. <source>Journal of Advertising Research</source>, <volume>46</volume>, <fpage>18</fpage>-<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr36-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rohrmann</surname><given-names>B.</given-names></name>
<name><surname>Bishop</surname><given-names>I. D.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Subjective responses to computer simulations of urban environments</article-title>. <source>Journal of Environmental Psychology</source>, <volume>22</volume>, <fpage>319</fpage>-<lpage>331</lpage>.</citation>
</ref>
<ref id="bibr37-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rohrmann</surname><given-names>B.</given-names></name>
<name><surname>Palmer</surname><given-names>S.</given-names></name>
<name><surname>Bishop</surname><given-names>I.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Perceived quality of computer-simulated environments</article-title>. In <person-group person-group-type="editor">
<name><surname>Moore</surname><given-names>G.</given-names></name>
<name><surname>Hunt</surname><given-names>J.</given-names></name>
<name><surname>Trevillon</surname><given-names>L.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the Environment-Behavior Research on the Pacific Rim</source> (pp. <fpage>341</fpage>-<lpage>352</lpage>). <publisher-loc>Sydney, Australia</publisher-loc>: <publisher-name>University of Sydney</publisher-name>.</citation>
</ref>
<ref id="bibr38-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rozendaal</surname><given-names>M. C.</given-names></name>
<name><surname>Keyson</surname><given-names>D. V.</given-names></name>
<name><surname>de Ridder</surname><given-names>H.</given-names></name>
<name><surname>Craig</surname><given-names>P. O.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Game feature and expertise effects on experienced richness, control and engagement in game play</article-title>. <source>AI &amp; Society</source>, <volume>24</volume>, <fpage>123</fpage>-<lpage>133</lpage>.</citation>
</ref>
<ref id="bibr39-1046878112444564">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Sanders</surname><given-names>R. D.</given-names></name>
<name><surname>Scorgie</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2002</year>). <source>The effects of sound delivery methods on a user’s sense of presence in a virtual environment</source> (Unpublished doctoral dissertation). <publisher-name>Naval Post Graduate School</publisher-name>. <publisher-loc>Monterey, California</publisher-loc>.</citation>
</ref>
<ref id="bibr40-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>T.</given-names></name>
<name><surname>Friedmann</surname><given-names>F.</given-names></name>
<name><surname>Regenbrecht</surname><given-names>H.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The experience of presence: Factor analytic insights</article-title>. <source>Presence: Tele-Operators and Virtual Environments</source>, <volume>10</volume>, <fpage>266</fpage>-<lpage>281</lpage>.</citation>
</ref>
<ref id="bibr41-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Serafin</surname><given-names>S.</given-names></name>
<name><surname>Serafin</surname><given-names>G.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Sound design to enhance presence in photorealistic virtual reality</article-title>. In <person-group person-group-type="editor">
<name><surname>Barrass</surname><given-names>S.</given-names></name>
<name><surname>Vickers</surname><given-names>P.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the 2004 International Conference on Auditory Display (ICAD 2004)</source> (pp. <fpage>1</fpage>-<lpage>4</lpage>). <publisher-loc>Sidney, Australia</publisher-loc>: <publisher-name>International Community for Auditory Display</publisher-name>.</citation>
</ref>
<ref id="bibr42-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slovic</surname><given-names>P.</given-names></name>
<name><surname>Finucane</surname><given-names>M. L.</given-names></name>
<name><surname>Peters</surname><given-names>E.</given-names></name>
<name><surname>MacGregor</surname><given-names>D. G.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Risk as analysis and risk as feelings: Some thoughts about affect, reason, risk, and rationality</article-title>. <source>Risk Analysis</source>, <volume>24</volume>, <fpage>311</fpage>-<lpage>322</lpage>.</citation>
</ref>
<ref id="bibr43-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slovic</surname><given-names>P.</given-names></name>
<name><surname>Peters</surname><given-names>E.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Risk perception and affect</article-title>. <source>Current Directions in Psychological Science</source>, <volume>15</volume>, <fpage>322</fpage>-<lpage>325</lpage>.</citation>
</ref>
<ref id="bibr44-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tafalla</surname><given-names>R.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Gender differences in cardiovascular reactivity and game performance related to sensory modality in violent video game play</article-title>. <source>Journal of Applied Social Psychology</source>, <volume>37</volume>, <fpage>2008</fpage>-<lpage>2023</lpage>.</citation>
</ref>
<ref id="bibr45-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tan</surname><given-names>S.-L.</given-names></name>
<name><surname>Baxa</surname><given-names>J.</given-names></name>
<name><surname>Spackman</surname><given-names>M. P.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Effects of built-in audio versus unrelated background music on performance in an adventure role-playing game</article-title>. <source>International Journal of Gaming and Computer-Mediated Simulations</source>, <volume>2</volume>(<issue>3</issue>), <fpage>1</fpage>-<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr46-1046878112444564">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tichon</surname><given-names>J. G.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Using presence to improve a virtual training environment</article-title>. <source>CyberPsychology &amp; Behavior</source>, <volume>10</volume>, <fpage>781</fpage>-<lpage>788</lpage>.</citation>
</ref>
<ref id="bibr47-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tulving</surname><given-names>E.</given-names></name>
<name><surname>Craik</surname><given-names>F. I. M.</given-names></name>
</person-group> (<year>2000</year>). <source>The Oxford handbook of memory</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr48-1046878112444564">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Turner</surname><given-names>P.</given-names></name>
<name><surname>McGregor</surname><given-names>I.</given-names></name>
<name><surname>Turner</surname><given-names>S.</given-names></name>
<name><surname>Carroll</surname><given-names>F.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Evaluating soundscapes as a means of creating a sense of place</article-title>. In <conf-name>Proceedings of the 2003 International Conference on Auditory Display</conf-name>: <fpage>6</fpage>-<lpage>9</lpage> <conf-date>July 2003</conf-date> (pp. ICAD03-1-ICAD03-4). <conf-loc>Boston, MA</conf-loc>.</citation>
</ref>
<ref id="bibr49-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tuuri</surname><given-names>K.</given-names></name>
<name><surname>Mustonen</surname><given-names>M.-S.</given-names></name>
<name><surname>Pirhonen</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Same sound - Different meanings: A novel scheme for modes of listening</article-title>. In <source>Proceedings of AudioMostly 2007, 2nd Conference on Interaction with Sound</source> (pp. <fpage>13</fpage>-<lpage>18</lpage>). <publisher-loc>Ilmenau, Germany</publisher-loc>: <publisher-name>Fraunhofer Institute for Digital Media Technology IDTM</publisher-name>.</citation>
</ref>
<ref id="bibr50-1046878112444564">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Ulate</surname><given-names>S. O.</given-names></name>
</person-group> (<year>2002</year>). <source>The impact of emotional arousal on learning in virtual environments</source> (Unpublished doctoral dissertation). <publisher-name>Naval Post Graduate School</publisher-name>. <publisher-loc>Monterey, California</publisher-loc>.</citation>
</ref>
<ref id="bibr51-1046878112444564">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wilfred</surname><given-names>L. M.</given-names></name>
<name><surname>Hall</surname><given-names>R.</given-names></name>
<name><surname>Hilgers</surname><given-names>M.</given-names></name>
<name><surname>Leu</surname><given-names>M.</given-names></name>
<name><surname>hortenstine</surname><given-names>J.</given-names></name>
<name><surname>Walker</surname><given-names>C.</given-names></name>
<name><surname>Reddy</surname><given-names>M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Training in affectively intense virtual environments</article-title>. In <person-group person-group-type="editor">
<name><surname>Richards</surname><given-names>G.</given-names></name>
</person-group> (Ed.), <source>Proceedings of World Conference on E-Learning in Corporate, Government, Healthcare, and Higher Education 2004</source> (pp. <fpage>2233</fpage>-<lpage>2240</lpage>). <publisher-loc>Chesapeake, VA</publisher-loc>: <publisher-name>AACE</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>