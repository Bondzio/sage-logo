<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ORM</journal-id>
<journal-id journal-id-type="hwp">sporm</journal-id>
<journal-title>Organizational Research Methods</journal-title>
<issn pub-type="ppub">1094-4281</issn>
<issn pub-type="epub">1552-7425</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094428112457829</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094428112457829</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Feature Topic: Advances in Research Methods From Outside the Organizational Sciences</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Time Has Come</article-title>
<subtitle>Bayesian Methods for Data Analysis in the Organizational Sciences</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Kruschke</surname>
<given-names>John K.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094428112457829">1</xref>
<xref ref-type="corresp" rid="corresp1-1094428112457829"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Aguinis</surname>
<given-names>Herman</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094428112457829">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Joo</surname>
<given-names>Harry</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094428112457829">2</xref>
</contrib>
<bio>
<title>Bios</title>
<p>
<bold>John K. Kruschke</bold> is Professor of Psychological and Brain Sciences and Adjunct Professor of Statistics at Indiana University. He is a seven-time winner of Teaching Excellence Recognition Awards from Indiana University, has authored an acclaimed textbook on Bayesian data analysis, and has presented numerous workshops. He guest-edited a section on Bayesian analysis for the journal <italic>Perspectives on Psychological Science</italic>, and is an action editor for the <italic>Journal of Mathematical Psychology</italic>. His current research interests include the science of moral judgment and the education of Bayesian methods. He received the Troland Research Award from the National Academy of Sciences, and is an elected Fellow of the Society of Experimental Psychologists and the Association for Psychological Science and other professional groups.</p>
<p>
<bold>Herman Aguinis</bold> is the Dean's Research Professor, a professor of organizational behavior and human resources, and the founding director of the Institute for Global Organizational Effectiveness in the Kelley School of Business, Indiana University. He has been a visiting scholar at universities in the People's Republic of China (Beijing and Hong Kong), Malaysia, Singapore, Argentina, France, Spain, Puerto Rico, Australia, and South Africa. His research interests span several human resource management, organizational behavior, and research methods and analysis topics. He has published five books and more than 100 articles in refereed journals. He is the recipient of the 2012 Academy of Management Research Methods Division Distinguished Career Award and a former editor-in-chief of <italic>Organizational Research Methods</italic>.</p>
<p>
<bold>Harry Joo</bold> is a doctoral student in organizational behavior and human resource management in the Kelley School of Business, Indiana University. His research interests include performance management and research methods and analysis. His work has appeared in several refereed journals including <italic>Organizational Research Methods, Academy of Management Perspectives</italic>, and <italic>Business Horizons</italic>. He has delivered presentations at the meetings of the Academy of Management and elsewhere.</p>
</bio>
</contrib-group>
<aff id="aff1-1094428112457829">
<label>1</label>Department of Psychological and Brain Sciences, Indiana University, Bloomington, IN, USA</aff>
<aff id="aff2-1094428112457829">
<label>2</label>Department of Management and Entrepreneurship, Kelley School of Business, Indiana University, Bloomington, IN, USA</aff>
<author-notes>
<corresp id="corresp1-1094428112457829">John K. Kruschke, Department of Psychological and Brain Sciences, Indiana University, 1101 E. 10th St., Bloomington, IN 47405-7007, USA Email: <email>JohnKruschke@gmail.com</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>15</volume>
<issue>4</issue>
<fpage>722</fpage>
<lpage>752</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The use of Bayesian methods for data analysis is creating a revolution in fields ranging from genetics to marketing. Yet, results of our literature review, including more than 10,000 articles published in 15 journals from January 2001 and December 2010, indicate that Bayesian approaches are essentially absent from the organizational sciences. Our article introduces organizational science researchers to Bayesian methods and describes why and how they should be used. We use multiple linear regression as the framework to offer a step-by-step demonstration, including the use of software, regarding how to implement Bayesian methods. We explain and illustrate how to determine the prior distribution, compute the posterior distribution, possibly accept the null value, and produce a write-up describing the entire Bayesian process, including graphs, results, and their interpretation. We also offer a summary of the advantages of using Bayesian analysis and examples of how specific published research based on frequentist analysis-based approaches failed to benefit from the advantages offered by a Bayesian approach and how using Bayesian analyses would have led to richer and, in some cases, different substantive conclusions. We hope that our article will serve as a catalyst for the adoption of Bayesian methods in organizational science research.</p>
</abstract>
<kwd-group>
<kwd>quantitative research</kwd>
<kwd>computer simulation procedures (e.g.</kwd>
<kwd>Monte Carlo</kwd>
<kwd>bootstrapping)</kwd>
<kwd>quantitative research</kwd>
<kwd>multilevel research</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The use of Bayesian methods for data analysis is creating a revolution in fields ranging from genetics to marketing. The magnitude of this change from traditional methods is suggested by the following article titles: “Bayesian Computation: A Statistical Revolution” (<xref ref-type="bibr" rid="bibr13-1094428112457829">Brooks, 2003</xref>), “The Bayesian Revolution in Genetics” (<xref ref-type="bibr" rid="bibr9-1094428112457829">Beaumont &amp; Rannala, 2004</xref>), “A Bayesian Revolution in Spectral Analysis” (<xref ref-type="bibr" rid="bibr32-1094428112457829">Gregory, 2001</xref>), and “The Hierarchical Bayesian Revolution: How Bayesian Methods Have Changed the Face of Marketing Research” (<xref ref-type="bibr" rid="bibr6-1094428112457829">Allenby, Bakken, &amp; Rossi, 2004</xref>). A primary reason for this Bayesian revolution is that traditional data analysis methods (e.g., maximum likelihood estimation, MLE) that rely on null hypothesis significance testing (NHST) have several known problems (<xref ref-type="bibr" rid="bibr15-1094428112457829">Cashen &amp; Geiger, 2004</xref>; <xref ref-type="bibr" rid="bibr18-1094428112457829">Cortina &amp; Folger, 1998</xref>; <xref ref-type="bibr" rid="bibr20-1094428112457829">Cortina &amp; Landis, 2011</xref>; <xref ref-type="bibr" rid="bibr44-1094428112457829">Lance, 2011</xref>). Specifically, what researchers want to know is the parameter values that are credible, given the observed data. In particular, researchers may want to know the viability of a null hypothesis (e.g., zero correlation between two variables or zero difference in mean scores across groups) given the data, <italic>p</italic>(<italic>H</italic>o|<italic>D</italic>). However, traditional methods based on NHST, also labeled <italic>frequentist</italic> statistics to distinguish them from <italic>Bayesian</italic> statistics, tell us the probability of obtaining the data in hand, or more extreme unobserved data, if the null hypothesis were true, <italic>p</italic>(<italic>D</italic>|<italic>H</italic>o) (<xref ref-type="bibr" rid="bibr5-1094428112457829">Aguinis, Werner, et al., 2010</xref>). Unfortunately, <italic>p</italic>(<italic>H</italic>o|<italic>D</italic>) ≠ <italic>p</italic>(<italic>D</italic>|<italic>H</italic>o). As noted by <xref ref-type="bibr" rid="bibr16-1094428112457829">Cohen (1994)</xref>, a test of statistical significance “does not tell us what we want to know, and we so much want to know what we want to know that, out of desperation, we nevertheless believe that it does!” (p. 997). In short, what we want to know is the credibility of candidate parameter values given the data that we actually observed.</p>
<p>Bayesian methods provide an alternative to the widely criticized traditional NHST-based methods because they provide the information we seek. Moreover, Bayesian methods are available for numerous data-analytic applications, including those that have been documented to be the most frequently used in organizational science research (<xref ref-type="bibr" rid="bibr4-1094428112457829">Aguinis, Pierce, Bosco, &amp; Muslin, 2009</xref>; <xref ref-type="bibr" rid="bibr66-1094428112457829">Scandura &amp; Williams, 2000</xref>). In addition, Bayesian methods are particularly useful for estimating parameter values in nonlinear hierarchical models that are flexibly adapted to the specifics of research and application contexts (e.g., <xref ref-type="bibr" rid="bibr58-1094428112457829">Pierce &amp; Aguinis, in press</xref>). Bayesian methods thereby open the door to extensive new realms of modeling possibilities that were previously inaccessible. As such, if embraced by organizational researchers, we believe that the use of Bayesian methods is likely to lead to important theoretical advancements and subsequent practical applications.</p>
<p>As noted earlier, Bayesian methods are taking hold across a broad range of scientific disciplines. Before we began writing our article, we suspected that the organizational sciences have not kept abreast of the Bayesian revolution. To formally assess the veracity of our presumption, we conducted a literature review to examine the extent to which Bayesian methods have been used in published articles in the organizational sciences between January 2001 and December 2010. Our review focused on 15 organizational science journals based on their visibility and impact. We used the database Business Source Premier and the keywords <italic>Bayes</italic> and <italic>Bayesian</italic> using the full-text search option.</p>
<p>Results of our review are summarized in <xref ref-type="table" rid="table1-1094428112457829">Table 1</xref> and indicate that Bayesian methods are essentially absent from the organizational science literature. Out of the more than 10,000 articles published in the 15 journals included in our 10-year review, only 42 used Bayesian methods. In other words, fewer than half of 1% of articles in the review period used Bayesian approaches. Only two journals published more than 1% of articles using Bayesian methods: <italic>Management Science</italic> (about 2%) and <italic>Organizational Research Methods</italic> (<italic>ORM</italic>) (about 1.4%). The publication of articles on newer methodological approaches such as Bayesian methods in <italic>ORM</italic> is not entirely surprising given findings based on a content analysis of all <italic>ORM</italic> articles published from 1998 through 2007 (<xref ref-type="bibr" rid="bibr4-1094428112457829">Aguinis et al., 2009</xref>). <xref ref-type="bibr" rid="bibr4-1094428112457829">Aguinis et al. (2009)</xref> concluded that although more traditional methodological approaches are still popular, several novel approaches have become at least as popular in <italic>ORM</italic> in a very short time. Stated differently, researchers focusing on methodological issues and publishing in <italic>ORM</italic> are more likely to become interested in and investigate newer methodological approaches compared with substantive researchers who publish their work in nonmethodological journals. Nevertheless, <xref ref-type="table" rid="table1-1094428112457829">Table 1</xref> shows that, for all practical purposes, Bayesian approaches are not currently in use in the organizational behavior/human resource management (e.g., <italic>Journal of Applied Psychology</italic>), strategy (e.g., <italic>Strategic Management Journal</italic>), and entrepreneurship (e.g., <italic>Journal of Business Venturing</italic>) literatures, to mention a few research domains. In short, the organizational sciences do not seem to be taking advantage of the Bayesian revolution taking place in many other scientific fields. We believe that the time has come for the organizational sciences to consider the use of Bayesian techniques, which yield richer inferences than do traditional methods that rely on NHST, <italic>p</italic> values, and confidence intervals.</p>
<table-wrap id="table1-1094428112457829" position="float">
<label>Table 1.</label>
<caption>
<p>Number of Articles Using Bayesian Methods in Selected Organizational Science Journals (2001-2010)</p>
</caption>
<graphic alternate-form-of="table1-1094428112457829" xlink:href="10.1177_1094428112457829-table1.tif"/>
<table>
<thead>
<tr>
<th>Journal</th>
<th>Total Number of Articles Published</th>
<th>Number (%) of Articles Using Bayesian Methods</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<italic>Academy of Management Journal</italic>
</td>
<td>736</td>
<td>1 (0.14)</td>
</tr>
<tr>
<td>
<italic>Administrative Science Quarterly</italic>
</td>
<td>587</td>
<td>1 (0.17)</td>
</tr>
<tr>
<td>
<italic>Industrial &amp; Labor Relations Review</italic>
</td>
<td>651</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Journal of Applied Psychology</italic>
</td>
<td>1,082</td>
<td>3 (0.28)</td>
</tr>
<tr>
<td>
<italic>Journal of Business Venturing</italic>
</td>
<td>429</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Journal of International Business Studies</italic>
</td>
<td>673</td>
<td>1 (0.15)</td>
</tr>
<tr>
<td>
<italic>Journal of Management</italic>
</td>
<td>486</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Journal of Organizational Behavior</italic>
</td>
<td>589</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Leadership Quarterly</italic>
</td>
<td>642</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Management Science</italic>
</td>
<td>1,407</td>
<td>29 (2.06)</td>
</tr>
<tr>
<td>
<italic>Organization Science</italic>
</td>
<td>544</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Organizational Behavior &amp; Human Decision Processes</italic>
</td>
<td>518</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Organizational Research Methods</italic>
</td>
<td>364</td>
<td>5 (1.37)</td>
</tr>
<tr>
<td>
<italic>Personnel Psychology</italic>
</td>
<td>933</td>
<td>0</td>
</tr>
<tr>
<td>
<italic>Strategic Management Journal</italic>
</td>
<td>731</td>
<td>2 (0.27)</td>
</tr>
<tr>
<td>Total</td>
<td>10,372</td>
<td>42 (0.40)</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The goal of our article is to introduce organizational researchers to Bayesian methods and describe why and how they should be used. Our article is organized as follows. First, we offer a brief description of Bayesian data analysis in general. Second, we use multiple linear regression as the framework to offer a step-by-step demonstration, including the use of software, regarding how to implement Bayesian methods. This demonstration includes complete details regarding how to execute and report the Bayesian analysis, including how to determine the prior distribution, how to compute the posterior distribution, and how to produce a write-up describing the entire Bayesian process, including graphs, results, and their interpretation. Third, we offer a summary of the advantages of using Bayesian analysis and examples of how specific published research based on frequentist analysis-based approaches failed to benefit from the advantages offered by a Bayesian approach and how using Bayesian analyses would have led to richer and, in some cases, different substantive conclusions. We hope that our article will serve as a catalyst for the adoption of Bayesian methods in organizational science research.</p>
<sec id="section1-1094428112457829">
<title>Bayesian Data Analysis: Rationale and Brief Overview</title>
<p>Bayesian analysis determines what can be inferred about parameter values given the actually observed data. Bayesian analysis is the mathematically normative way to reallocate credibility across parameter values as new data arrive.</p>
<p>Reallocation of credibility across possible causes is common in everyday reasoning. For example, suppose there are two unaffiliated suspects for a crime, and strong evidence implicates one suspect. We infer that the other suspect is exonerated. Thus, data that increase culpability of one suspect produce a reallocation of culpability away from the other suspect. The complementary reallocation is also common, as can be paraphrased from the fictional detective Sherlock Holmes: When you have eliminated all other possibilities, then whatever remains, no matter how improbable, must be the truth (<xref ref-type="bibr" rid="bibr25-1094428112457829">Doyle, 1890</xref>). In this case, data that reduce the credibility of some options increase the credibility of the remaining options, even if the prior credibility of those options was small. When the reallocation of credibility is done in the mathematically correct way, then it is Bayesian. <xref ref-type="bibr" rid="bibr24-1094428112457829">Dienes (2011)</xref> showed that scientific intuitions about the interpretation of data match the results from Bayesian analysis.</p>
<p>Formal Bayesian data analysis begins with a descriptive model, just as in classical statistics. The descriptive model has meaningful parameters that describe trends in the data. Unlike classical methods, Bayesian analysis yields a complete distribution over the joint parameter space, revealing the relative credibility of all possible combinations of parameter values. Decisions about parameter values of special interest, such as zero, can be made directly from the derived distribution.</p>
<p>Bayesian analysis is named after Thomas Bayes, who discovered a simple mathematical relation among conditional probabilities that is now known as Bayes’ rule (<xref ref-type="bibr" rid="bibr8-1094428112457829">Bayes &amp; Price, 1763</xref>). When the rule is applied to parameters and data, it can be written as follows:</p>
<p>
<disp-formula id="disp-formula1-1094428112457829">
<label>1</label>
<mml:math id="mml-disp1-1094428112457829">
<mml:mrow><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">θ</mml:mi></mml:mrow></mml:mrow><mml:mfenced close="" open="|"><mml:mi>D</mml:mi></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="true" stretchy="true">⏟</mml:mo></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">=</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mrow><mml:mfenced close="" open="|"><mml:mi mathvariant="italic">θ</mml:mi></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="true" stretchy="true">⏟</mml:mo></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="true" stretchy="true">⏟</mml:mo></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="true" stretchy="true">⏟</mml:mo></mml:munder><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:munder></mml:mrow></mml:mrow><mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula1-1094428112457829" xlink:href="10.1177_1094428112457829-eq1.tif"/>
</disp-formula>
</p>
<p>where <italic>D</italic> is the observed data and θ is a vector of parameters in the descriptive model. The posterior distribution, <italic>p</italic>(θ|<italic>D</italic>), specifies the relative credibility of every combination of parameters given the data. Because the range of parameter values defines the complete space of possible descriptions, the distribution of credibility sums to 1 and is tantamount to a probability distribution. The posterior distribution provides the most complete information that is mathematically possible about the parameter values given the data (unlike the point estimate and confidence interval in classical statistics, which provide no distributional information, as will be explained later). To make this abstraction concrete, we proceed now to the most common data-analytic application in organizational science research over the past three decades (<xref ref-type="bibr" rid="bibr4-1094428112457829">Aguinis et al., 2009</xref>), multiple linear regression.</p>
</sec>
<sec id="section2-1094428112457829">
<title>Bayesian Multiple Linear Regression</title>
<p>Consider a common situation in organizational research, where we are interested in predicting an outcome based on three predictors. For concreteness, the outcome is job performance and the predictors are general mental ability (GMA), conscientiousness, and biodata (i.e., collected using a biographical inventory). To use a realistic illustration, we generated data regarding each of these variables for 346 individual workers from meta-analytically derived population correlations reported by <xref ref-type="bibr" rid="bibr64-1094428112457829">Roth, Switzer, Van Iddekinge, and Oh (2011)</xref> (the <xref ref-type="app" rid="app1-1094428112457829">appendix</xref> provides details for accessing the data file and conducting the analysis). All four variables were generated from a multivariate normal distribution and were then rounded to the nearest Likert scale value from 1 to 7.</p>
<p>Job performance, denoted <italic>y</italic>, is randomly distributed around a linear function of the other three variables, denoted <italic>x</italic>
<sub>1</sub> (GMA), <italic>x</italic>
<sub>2</sub> (conscientiousness), and <italic>x</italic>
<sub>3</sub> (biodata). Formally, the relation is</p>
<p>
<disp-formula id="disp-formula2-1094428112457829">
<label>2</label>
<mml:math id="mml-disp2-1094428112457829">
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula2-1094428112457829" xlink:href="10.1177_1094428112457829-eq2.tif"/>
</disp-formula>
</p>
<p>where <inline-formula id="inline-formula1-1094428112457829">
<mml:math id="mml-inline1-1094428112457829">
<mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub>
</mml:math>
</inline-formula> is the predicted value of <italic>y<sub>i</sub>
</italic> and where the subscript <italic>i</italic> denotes values for the <italic>i</italic>th individual.</p>
<p>Notice that five parameters must be estimated from the data: β<sub>0</sub> (i.e., the intercept), β<sub>1</sub> (i.e., the regression coefficient for GMA), β<sub>2</sub> (i.e., the regression coefficient for conscientiousness), β<sub>3</sub> (i.e., the regression coefficient for biodata), and σ (i.e., the standard deviation for job performance scores). <xref ref-type="disp-formula" rid="disp-formula2-1094428112457829">Equation 2</xref> expresses the probability of observed data values given any candidate values of the parameters and thereby constitutes the likelihood function in Bayes’ rule shown in <xref ref-type="disp-formula" rid="disp-formula1-1094428112457829">Equation 1</xref>.</p>
<sec id="section3-1094428112457829">
<title>Establishing the Prior Distribution</title>
<p>Recall that Bayesian analysis reallocates credibility across candidate parameter values. Therefore, we must establish a prior distribution of credibility on the parameter values, without the newly considered data. When there is little publicly agreed prior knowledge about the parameters, then the prior distribution can be very broad so that no parameter value is given much more credence than any other parameter value. If, however, previous research provides clear guidance regarding plausible parameter values, then the prior distribution can be specified to place more credibility on the plausible parameter values than on the implausible parameter values. Crucially, the prior distribution cannot be set capriciously to favor a researcher’s subjective and idiosyncratic opinion. The prior distribution is explicitly specified and justified for a skeptical scientific audience. When skeptics disagree about the appropriateness of a prior distribution, then a noncommittal broad prior distribution can be used. Another useful procedure is to conduct the analysis with more than one prior distribution to demonstrate that the posterior distribution is essentially invariant under reasonable changes in the prior. In typical analyses, a noncommittal broad prior is used, and therefore, the posterior distribution is very robust.</p>
<p>Even though the prior distribution is often selected to be noncommittal, this does not imply that the prior distribution is an inconvenient nuisance for which a researcher must apologize. To the contrary, in many applications, a well-informed prior distribution can provide inferential leverage. As a simplistic example, consider the goal of estimating the possible bias of a coin. Suppose we flip the coin once and it comes up heads. If we have little prior knowledge about the coin, then the single flip tells us only very <italic>un</italic>certainly that the coin might be somewhat head biased. If, however, we have prior knowledge that the coin came from a magic shop and must be either an extremely head-biased coin or an extremely tail-biased coin, then the single flip tells us with high certainty that the coin is of the extremely head-biased type, because the extremely tail-biased coin would almost never exhibit even a single head. Because Bayesian analysis is able to incorporate prior knowledge when it is appropriate, Bayesian analysis is consistent with the epistemological position that organizational science theories will advance to the extent that we engage in empirical research that relies on the accumulation of knowledge (<xref ref-type="bibr" rid="bibr68-1094428112457829">Schmidt, 2008</xref>).</p>
<p>Not only can prior knowledge be useful, but ignoring it can cause a derailment in the accumulation of knowledge as well as ineffective practical applications. Consider the use of drug screening as part of the preemployment testing process. Suppose we have a drug test that correctly detects drug use 95% of the time and gives false alarms only 5% of the time. Suppose we select a job applicant at random and the test result is positive. What is the probability that the person uses the drug? An answer that ignores the base rate of drug use might be near the detection rate of 95%. But the correct answer takes into account the base rate of drug usage. If the base rate in the population is 5%, then the actual probability of drug use in the randomly tested person is only 50%, not 95%. This follows directly from Bayes’ rule:</p>
<disp-formula id="disp-formula3-1094428112457829">
<mml:math id="mml-disp3-1094428112457829"><mml:mtable columnalign="right left" columnspacing="thickmathspace" displaystyle="true" rowspacing=".5em"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">+</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">+</mml:mo><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">+</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">+</mml:mo><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">+</mml:mo><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">+</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">+</mml:mo><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">θ</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>0.95</mml:mn><mml:mo stretchy="false">×</mml:mo><mml:mn>0.05</mml:mn></mml:mrow><mml:mrow><mml:mn>0.95</mml:mn><mml:mo stretchy="false">×</mml:mo><mml:mn>0.05</mml:mn><mml:mo stretchy="false">+</mml:mo><mml:mn>0.05</mml:mn><mml:mo stretchy="false">×</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">=</mml:mo><mml:mn>0.50.</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math>
<graphic alternate-form-of="disp-formula3-1094428112457829" xlink:href="10.1177_1094428112457829-eq3.tif"/>
</disp-formula>
<p>In this situation, the parameter θ being estimated is the person’s drug use, which has two nominal values, θ = user or θ = nonuser. The base rate is the prior distribution over the two values of the parameter—namely, <italic>p</italic>(θ = user) = .05 and <italic>p</italic>(θ = nonuser) = .95. The prior distribution in this scenario has been established from extensive previous research. Not to use the well-informed prior would be a mistake because it would unfairly deny employment opportunities to job applicants. In any other domain of scientific inference, if we have a well-informed prior distribution, not to use it could also be a costly mistake.</p>
<p>For our specific application to multiple linear regression, although we generated data from an established substantive research domain (i.e., human resource selection), our goal is to provide a generic example without specific prior commitments, and therefore we use a noncommittal, vague prior. For each regression coefficient, the prior distribution is a very broad normal distribution, with a mean of zero and a standard deviation that is extremely large relative to the scale of the data. The same assumption is made for the prior on the intercept. Finally, the prior on the standard deviation of the predicted value is merely a uniform distribution extending from zero to an extremely large value far beyond any realistic value for the scale of the data. In the specific analyses demonstrated in this section of our article, the data were standardized so that the prior would be broad regardless of the original scale of the data. The analysis results were then simply algebraically transformed back to the original scale. For the standardized data, the prior on the intercept and regression coefficients was a normal distribution with mean at zero and standard deviation of 100. This normal distribution is virtually flat over the range of possible intercepts and regression coefficients for standardized data. The prior on the standard deviation parameter (σ in <xref ref-type="disp-formula" rid="disp-formula2-1094428112457829">Equation 2</xref>) was a uniform distribution from zero to 10, which again far exceeds any reasonable value for σ in standardized data. Thus, the prior places essentially no bias on the posterior distribution.</p>
<p>To facilitate our presentation, the full model is illustrated graphically in <xref ref-type="fig" rid="fig1-1094428112457829">Figure 1</xref>. The lower part of the diagram illustrates the likelihood function of <xref ref-type="disp-formula" rid="disp-formula2-1094428112457829">Equation 2</xref>: The arrow to <italic>y<sub>i</sub>
</italic> indicates that the data are normally distributed with mean <inline-formula id="inline-formula2-1094428112457829">
<mml:math id="mml-inline2-1094428112457829">
<mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub>
</mml:math>
</inline-formula> and standard deviation σ. The arrow pointing to <inline-formula id="inline-formula3-1094428112457829">
<mml:math id="mml-inline3-1094428112457829">
<mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub>
</mml:math>
</inline-formula> indicates that the predicted value equals a linear function of the predictors. The upper part of the diagram illustrates the prior distribution. For example, in the upper left, it is shown that the intercept β<sub>0</sub> has a normal prior with mean <italic>M</italic>
<sub>0</sub> and standard deviation <italic>S</italic>
<sub>0</sub>. The prior distribution is a joint distribution across the five-dimensional parameter space, defined here for convenience as the product of five independent distributions on the five parameters.</p>
<fig id="fig1-1094428112457829" position="float">
<label>Figure 1.</label>
<caption>
<p>Hierarchical diagram for multiple linear regression. The prior distribution has histogram bars superimposed to indicate correspondence with posterior distributions shown in subsequent figures.</p>
</caption>
<graphic xlink:href="10.1177_1094428112457829-fig1.tif"/>
</fig>
<p>The prior distribution is a continuous mathematical function indicated by the black curves, but it is illustrated with superimposed histograms because the parameter distribution will be represented by using a very large (e.g., 250,000) representative random sample from the parameter space. Thus, the Bayesian analysis will reallocate the very large set of representative parameter values from the prior distribution to a posterior distribution, illustrated by histograms of the representative values. For any fixed set of data and prior distribution, there is one true posterior distribution, represented by a very large representative sample of parameter values. The larger the representative sample, the higher resolution picture we have of the true posterior.</p>
<p>In summary, Bayesian inference involves a reallocation of credibility across the space of parameter values. The reallocation is based on the actually observed data, not on imagined data that might have been obtained from a null hypothesis if the intended sampling were repeated. The reallocation starts from a prior distribution. As noted earlier, the prior distribution is not capricious and must be explicitly reasonable to a skeptical scientific audience.</p>
</sec>
<sec id="section4-1094428112457829">
<title>Computing the Posterior Distribution</title>
<p>In Bayes’ rule shown in <xref ref-type="disp-formula" rid="disp-formula1-1094428112457829">Equation 1</xref>, the likelihood function and prior distribution have mathematical forms. The two mathematical forms are multiplied together in the numerator. We use the term <italic>evidence</italic> to refer to the denominator of Bayes’ rule, <italic>p</italic>(<italic>D</italic>), which is also known as the marginal likelihood. Computing the value of <italic>p</italic>(<italic>D</italic>) can be difficult because it is actually an integral over the parameter space: <inline-formula id="inline-formula4-1094428112457829">
<mml:math id="mml-inline4-1094428112457829">
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mfenced close="" open="|"><mml:mrow><mml:mrow><mml:mi mathvariant="italic">θ</mml:mi></mml:mrow></mml:mrow></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">θ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="italic">θ</mml:mi></mml:mrow></mml:mrow>
</mml:math>
</inline-formula>. For many years, Bayesian analysis was confined to mathematical forms that could be analytically integrated or analytically approximated. Fortunately, new computer-based methods have emerged that allow Bayesian analysis to be computed flexibly and for very complex models.</p>
<p>The new method is called Markov chain Monte Carlo (MCMC). The idea is to accurately approximate the posterior distribution by a very large representative random sample of parameter values drawn from the posterior distribution. From this very large sample of representative parameter values, we can determine the mean or modal parameter value, the quantiles of the parameter distribution, its detailed shape, and the forms of trade-offs between values of different parameters. What makes this approach possible is that MCMC methods generate the random sample without needing to compute the difficult integral for <italic>p</italic>(<italic>D</italic>). Moreover, recent advances in algorithms and software have made it possible for a researcher merely to specify the form of the likelihood function and prior distribution, and the software is able to apply MCMC methods.</p>
<p>The hierarchical diagram in <xref ref-type="fig" rid="fig1-1094428112457829">Figure 1</xref> contains all the information that must be communicated to a computer program so that MCMC sampling can be conducted. Every arrow in the hierarchical diagram has a corresponding declaration in the software for Bayesian analysis, which is explained in more detail in the <xref ref-type="app" rid="app1-1094428112457829">appendix</xref>. The program has been packaged such that all the user needs to do is type three simple commands. One command loads the data, a second command creates the MCMC chain, and a third command creates graphs of the posterior distribution.</p>
<p>The MCMC chain provides a large sample of credible <italic>combinations</italic> of parameter values in the five-dimensional parameter space. We examine various one-dimensional projections of the posterior. <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> shows the posterior distribution for data that have been standardized, and <xref ref-type="fig" rid="fig3-1094428112457829">Figure 3</xref> shows the posterior distribution on the original data scale. The middle row of <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> shows the three regression coefficients. Notice that the posterior provides an explicit distribution of the credibility (i.e., probability) of each possible value for the regression coefficient. The upper right panel of <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> shows the posterior for the intercept, and the upper middle panel shows the posterior for the standard deviation of the data around the linear prediction. By contrast, results from traditional analysis provide no distribution over parameter values.</p>
<fig id="fig2-1094428112457829" position="float">
<label>Figure 2.</label>
<caption>
<p>Posterior distribution for the multiple linear regression example, showing parameters for standardized data</p>
</caption>
<graphic xlink:href="10.1177_1094428112457829-fig2.tif"/>
<attrib>Note: HDI = highest density interval.</attrib>
</fig>
<fig id="fig3-1094428112457829" position="float">
<label>Figure 3.</label>
<caption>
<p>Posterior distribution for the multiple linear regression example, showing original-scale parameters</p>
</caption>
<graphic xlink:href="10.1177_1094428112457829-fig3.tif"/>
<attrib>Note: HDI = highest density interval.</attrib>
</fig>
<p>
<xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> shows additional information about the regression coefficients. The lower row shows the differences between standardized regression coefficients and, as we describe later in more detail, this information is useful in terms of understanding practical decisions regarding the use of each of the predictors in the model. Although it is well known that standardized regression coefficients must be interpreted with caution because the scales are brought into alignment only in terms of the sample-based standard deviations of the predictors (e.g., <xref ref-type="bibr" rid="bibr37-1094428112457829">King, 1986</xref>), a comparison of standardized coefficients can nevertheless be useful. For example, if it is equally costly to test a job applicant for GMA or for conscientiousness, but we would prefer to avoid the double cost of testing both, then we may want to know which test yields higher predictiveness for job performance. The credible differences are determined by computing the difference between regression coefficients at every step in the MCMC chain and plotting the result.</p>
<p>Finally, the upper left panel of <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> shows an entire distribution of credible values for the proportion of variance accounted for, denoted <italic>R</italic>
<sup>2</sup>. At each step in the MCMC chain, a credible value of <italic>R</italic>
<sup>2</sup> is computed as simply a reexpression of the credible regression coefficients at that step: <inline-formula id="inline-formula5-1094428112457829">
<mml:math id="mml-inline5-1094428112457829">
<mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false" stretchy="false">∑</mml:mo><mml:mi>j</mml:mi></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>.</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow>
</mml:math>
</inline-formula>, where β<italic>
<sub>j</sub>
</italic> is the standardized regression coefficient for the <italic>j</italic>th predictor at that step in the MCMC chain, and <inline-formula id="inline-formula6-1094428112457829">
<mml:math id="mml-inline6-1094428112457829">
<mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>.</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub>
</mml:math>
</inline-formula> is the sample correlation of the criterion values, <italic>y</italic>, with the <italic>j</italic>th predictor values, <italic>x<sub>j</sub>
</italic>. The equation for expressing <italic>R</italic>
<sup>2</sup> in terms of the regression coefficients is used by analogy to least squares regression, in which the equation is exactly true (e.g., <xref ref-type="bibr" rid="bibr33-1094428112457829">Hays, 1994</xref>, Eq. 15.14.2, p. 697). The mean value in the distribution of <italic>R</italic>
<sup>2</sup> is essentially the maximum likelihood estimate when using vague priors and the least squares estimate when using a normal likelihood function. The posterior distribution reveals the entire distribution of credible <italic>R</italic>
<sup>2</sup> values. (The posterior distribution of <italic>R</italic>
<sup>2</sup>, defined this way, can exceed 1.0 or fall below 0.0, because <italic>R</italic>
<sup>2</sup> here is a linear combination of credible regression coefficients, not the singular value that minimizes the squared deviations between predictions and data.)</p>
<p>The panels in <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> show an interval marked as HDI, which stands for <italic>highest density interval</italic>. Points inside an HDI have higher probability density (credibility) than points outside the HDI, and the points inside the 95% HDI include 95% of the distribution. Thus, the 95% HDI includes the most credible values of the parameter. The 95% HDI is useful both as a summary of the distribution and as a decision tool. Specifically, the 95% HDI can be used to help decide which parameter values should be deemed not credible, that is, rejected. This decision process goes beyond probabilistic Bayesian inference per se, which generates the complete posterior distribution, not a discrete decision regarding which values can be accepted or rejected. One simple decision rule is that any value outside the 95% HDI is rejected. In particular, if we want to decide whether the regression coefficients are nonzero, we consider whether zero is included in the 95% HDI. For example, <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> shows that zero is excluded from the 95% HDI for both Predictor 1 and Predictor 3. The lower left panel indicates that the regression coefficients on Predictors 1 and 2 are credibly different (i.e., a difference of zero is not among the 95% most credible values). The upper left panel indicates that <italic>R</italic>
<sup>2</sup> is well above zero.</p>
</sec>
<sec id="section5-1094428112457829">
<title>Accepting the Null Value</title>
<p>A more sophisticated decision rule also has a way of accepting a (null) value, not merely rejecting it. This extended decision rule involves establishing a region of practical equivalence (ROPE) around the value of interest. For example, if we are interested in the null value (i.e., zero) for a particular regression coefficient, we establish slope values that are equivalent to zero for practical purposes in the particular application. Suppose we specify that slopes between –0.05 and +0.05 are practically equivalent to zero. We would decide to <italic>reject</italic> the null value if the 95% HDI falls completely <italic>outside</italic> the ROPE, because none of the 95% most credible values is practically equivalent to the null value. Moreover, we would decide to <italic>accept</italic> the null value if the 95% HDI falls completely <italic>inside</italic> the ROPE, because all of the 95% most credible values are practically equivalent to the null value. The 95% HDI gets narrower as the sample size gets larger.</p>
<p>To illustrate a case of accepting the null value, we simulated a larger sample (<italic>N</italic> = 1,730) of data from the linear regression model using true regression coefficients of .15, .00, and .15. The resulting posterior distribution is shown in <xref ref-type="fig" rid="fig4-1094428112457829">Figure 4</xref> for standardized parameters and in <xref ref-type="fig" rid="fig5-1094428112457829">Figure 5</xref> for original-scale parameters. Regarding <xref ref-type="fig" rid="fig4-1094428112457829">Figure 4</xref>, notice in the middle panel that the posterior distribution for the regression coefficient on the second predictor has its 95% HDI entirely within the ROPE. Because the bulk of the credible values are practically equivalent to zero, we decide to accept the null value. The lower middle panel in <xref ref-type="fig" rid="fig4-1094428112457829">Figure 4</xref> shows that the difference between the first and third regression coefficients is centered on zero, but the 95% HDI of the difference does not quite fall entirely within the ROPE.</p>
<fig id="fig4-1094428112457829" position="float">
<label>Figure 4.</label>
<caption>
<p>Posterior distribution for a large illustrative data set (<italic>N</italic> = 1,730), showing standardized parameters. Notice that the 95% highest density interval (HDI) for the second regression coefficient (middle) falls within the region of practical equivalence (ROPE).</p>
</caption>
<graphic xlink:href="10.1177_1094428112457829-fig4.tif"/>
</fig>
<fig id="fig5-1094428112457829" position="float">
<label>Figure 5.</label>
<caption>
<p>Posterior distribution for a large illustrative data set (<italic>N</italic> = 1,730), showing original-scale parameters</p>
</caption>
<graphic xlink:href="10.1177_1094428112457829-fig5.tif"/>
<attrib>Note: HDI = highest density interval.</attrib>
</fig>
<p>Classical NHST-based analysis has no way of accepting the null hypothesis, which typically consists of specifying the absence of an effect or relationship (cf. <xref ref-type="bibr" rid="bibr18-1094428112457829">Cortina &amp; Folger, 1998</xref>). Indeed, in NHST, because the null hypothesis can only be rejected, a researcher is guaranteed to reject the null hypothesis, even when it is true, if the sample size is allowed to grow indefinitely and the researcher tests with every additional datum collected (e.g., <xref ref-type="bibr" rid="bibr3-1094428112457829">Aguinis &amp; Harden, 2009</xref>; <xref ref-type="bibr" rid="bibr7-1094428112457829">Anscombe, 1954</xref>; <xref ref-type="bibr" rid="bibr17-1094428112457829">Cornfield, 1966</xref>; <xref ref-type="bibr" rid="bibr43-1094428112457829">Kruschke, in press</xref>). This “sampling to reach a foregone conclusion” does not happen in a Bayesian approach. Instead, because the HDI narrows as sample size increases, and therefore the null has greater probability of being accepted when it is true, it is the case that the probability of false alarm asymptotes at a relatively small value (depending on the specific choice of ROPE). For an illustration of rates of false alarms and acceptances in sequential testing, see <xref ref-type="bibr" rid="bibr43-1094428112457829">Kruschke (in press)</xref>. Independently of its use as a decision tool for Bayesian analysis, the use of a ROPE has also been suggested as a way to increase the predictive precision of theories in the organizational sciences (<xref ref-type="bibr" rid="bibr26-1094428112457829">Edwards &amp; Berry, 2010</xref>).</p>
</sec>
<sec id="section6-1094428112457829">
<title>Summary of Rich Information Provided by Bayesian Analysis</title>
<p>As shown in <xref ref-type="fig" rid="fig2-1094428112457829">Figures 2</xref> through 5, the posterior distribution reveals complete distributional information about which values of the parameters are more or less credible, including the stardard deviation of the noise in the data. The posterior distribution simultaneously reveals the differences between standardized regression coefficients. Moreover, a complete distribution of credible <italic>R</italic>
<sup>2</sup> values is provided.</p>
<p>We emphasize that a frequentist sampling distribution of parameter estimates, from an assumed fixed parameter value, is not the same thing as the posterior distribution on the parameter. To create a sampling distribution of estimates, one starts with an assumed fixed parameter value, and then, either analytically or through bootstrapping, creates a sampling distribution of parameter estimates. The process is as follows: (1) Assume a fixed parameter value. (2) From a hypothetical population with that parameter value, repeatedly generate random samples according to an intended sampling design, that is, stopping rule. (3) For each randomly generated sample, compute a sample statistic that estimates the parameter value. (4) From the repeated random samples, create a sampling distribution of the estimator. The result is a distribution that specifies the probability of each estimator value given the assumed parameter value and the sampling design. Notice that the distribution of sample estimates is not about the probability of a parameter value, given the data; indeed, the sampling process starts with a single assumed parameter value. Notice also that the estimator and the parameter are apples and oranges. The distinction between estimator and parameter is especially evident when the parameter of interest is the bias of a coin (e.g., estimating the proportion of left-handers in a population). The sample estimate is the number of “heads” in the sample divided by the sample size, <italic>N</italic>. The estimate can therefore take on any of the discrete values 0/<italic>N</italic>, 1/<italic>N</italic>, 2/<italic>N</italic>,…, <italic>N</italic>/<italic>N</italic> (assuming that fixed <italic>N</italic> was the intended design). The sampling distribution of the estimator is a distribution on those discrete proportions (specifically, a binomial distribution if we assume <italic>N</italic> was fixed by design). In contrast, the posterior distribution on the underlying bias is a continuous distribution on the interval [0,1]. The posterior distribution is explicitly a probability distribution on parameter values, given the actually observed data. A major advantage of the Bayesian approach is that the posterior distribution can be directly understood and interpreted: It reveals the relative credibility of every possible combination of parameter values, given the data. A sampling distribution of parameter estimates, from some assumed parameter value, has only a convoluted interpretation and little direct inferential relevance. (And, as we describe later in this article, there is no unique sampling distribution because it depends on the design intention—a.k.a. the stopping rule.)</p>
<p>From the posterior distribution, decisions can be made about landmark values of interest, such as null values. By using the HDI and ROPE, a reseacher can decide whether to reject or accept a candidate parameter value. And the decision is made without reference to <italic>p</italic> values and sampling distributions as in NHST. Note that one needs a Bayesian approach to implement the decision rule involving the ROPE. In particular, consider the decision to accept a (null) value if the 95% HDI falls entirely within the ROPE around the value. This decision rule only makes sense because the 95% HDI represents the bulk of the credible parameter values. Crucially, the frequentist 95% confidence interval (CI) does not indicate the 95% most credible values of the parameter. The CI is by definition different from the HDI, as we will describe later. One clear illustration that the CI is not an HDI comes from multiple tests: The 95% CI grows much wider when a researcher intends to conduct more tests, but the HDI is unchanged (because the posterior distribution is unchanged).</p>
<p>Although not illustrated by our multiple linear regression example, Bayesian analysis is exceptionally well suited for more complex applications. When the data are skewed or have outliers, it is easy to change the program (see the <xref ref-type="app" rid="app1-1094428112457829">appendix</xref>) to use a nonnormal distribution, and interpretation of the posterior distribution proceeds seamlessly as before. By contrast, generating <italic>p</italic> values for NHST can be challenging, especially for nonnormal distributions in nonlinear hierarchical models. When a researcher is interested in nonlinear trends in data, it is easy to change the program to model the trend of interest, whether it is polynomial, exponential, a power law, or sinusoidal, to name a few. Importantly, a hierarchical structure is easily implemented in Bayesian software. For example, regression curves can be fit to each of many individuals, with higher level parameters describing trends across groups or conditions, even for nonlinear and nonnormal models. Another example of a hierarchical structure is the inclusion of a higher level distribution to describe the distribution of the regression coefficients across predictors. This sort of hierarchical structure allows each regression coefficient to inform the estimates of the other regression coefficients, while all simultaneously informing the higher level estimates of their dispersion across predictors. The resulting shrinkage of estimated regression coefficients helps attenuate artificially extreme estimates caused by noisy data.</p>
</sec>
<sec id="section7-1094428112457829">
<title>Reasons Why a Bayesian Approach Overcomes Deficiencies in the Frequentist Approach</title>
<p>The information provided by the classical frequentist approach is quite different from the information provided by Bayesian analysis. Whereas Bayesian analysis provides the complete posterior distribution of credibility on the joint parameter space, from which multiple decisions can be made, the frequentist approach provides only a point estimate (from least squares or maximum likelihood) without any distributional information. To make decisions in the frequentist approach, the analyst must create sampling distributions to determine <italic>p</italic> values and limits on CIs. Unfortunately, the <italic>p</italic> values and CI limits can change dramatically depending on the intended tests, comparisons, and stopping rules for sampling of data. Moreover, the CIs provide no distributional information about the parameter values. We address these issues in more detail next.</p>
<p>In multiple linear regression, the statistical significance of a regression coefficient is assessed by computing how much the fit of the full model worsens when the coefficient is removed (i.e., set to zero) and then by computing the probability of that magnitude of worsening if the null hypothesis were true. For example, to assess the importance of the regression coefficient β<sub>1</sub>, we consider the residual sum-squared error of the full model, <inline-formula id="inline-formula7-1094428112457829">
<mml:math id="mml-inline7-1094428112457829">
<mml:msub><mml:mi>E</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false" stretchy="false">∑</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup>
</mml:math>
</inline-formula>, versus the residual sum-squared error of the restricted model in which β<sub>1</sub> is set to zero: <inline-formula id="inline-formula8-1094428112457829">
<mml:math id="mml-inline8-1094428112457829">
<mml:msub><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:munder><mml:mo movablelimits="false" stretchy="false">∑</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:mn>0</mml:mn><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="italic">β</mml:mi></mml:mrow></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup>
</mml:math>
</inline-formula>. Note that the best-fitting values of β<sub>0</sub>, β<sub>2</sub>, and β<sub>3</sub> may differ in the full and restricted models. The full model will always fit at least as well as the restricted model, even when the true value of β<sub>1</sub> is zero, because the full model can better fit random sampling noise and the restricted model is nested within the full model. Therefore, the crux is the probability of that amount of worsening if we were to imagine randomly sampling data from the null hypothesis.</p>
<p>Adopting classical frequentist statistics involves using a descriptive statistic, denoted <italic>F</italic>, that measures the average increase in error per parameter restricted, relative to the baseline error per data point in the full model (<xref ref-type="bibr" rid="bibr50-1094428112457829">Maxwell &amp; Delaney, 2004</xref>):</p>
<p>
<disp-formula id="disp-formula4-1094428112457829">
<label>3</label>
<mml:math id="mml-disp4-1094428112457829">
<mml:mi>F</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo stretchy="false">−</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula4-1094428112457829" xlink:href="10.1177_1094428112457829-eq4.tif"/>
</disp-formula>
</p>
<p>where <italic>df</italic> is the degrees of freedom, which is the number of data values <italic>N</italic> minus the number of parameters in the model (other than the “nuisance” parameter, σ). Thus, for testing a single regression coefficient from the set of three predictors, the full model has <italic>df<sub>F</sub>
</italic> = <italic>N</italic> – 4, and the restricted model has <italic>df<sub>R</sub>
</italic> = <italic>N</italic> – 3.</p>
<p>To determine a sampling distribution for <italic>F</italic>, we repeatedly generate random data from the null hypothesis, in which all the (nonnuisance) parameters are zero, and we examine the distribution of <italic>F</italic> when the null hypothesis is true. In the null hypothesis, the simulated data are normally distributed. For each set of randomly generated data, we use the stopping rule intended by the researcher. The stopping rule plays an important role because it determines the space of possible imaginary outcomes when the null hypothesis is true, relative to which the actual outcome is judged. Conventionally, data are assumed to be sampled until <italic>N</italic> reaches a threshold size. Other stopping rules are possible, as will be explained subsequently. When sampling from the null hypothesis, usually <italic>F</italic> will be around 1.0, because the random worsening of fit by a restricted parameter (the numerator of <italic>F</italic>) will be about the same as the background noise per data point (the denominator of <italic>F</italic>). But, by chance, the <italic>F</italic> value will sometimes be larger than 1.0, because the random sample of data will happen to have an accidental arrangement that is much better fit by the full model than by the restricted model. Our goal is to determine exactly the probability that the null hypothesis generates <italic>F<sub>samp</sub>
</italic> values as large as or larger than the particular <italic>F<sub>act</sub>
</italic> value observed in the actual data. This probability is called the <italic>p</italic> value and can be formally specified as</p>
<p>
<disp-formula id="disp-formula5-1094428112457829">
<label>4</label>
<mml:math id="mml-disp5-1094428112457829">
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">≥</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="italic">β</mml:mi><mml:mo stretchy="false">=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula5-1094428112457829" xlink:href="10.1177_1094428112457829-eq5.tif"/>
</disp-formula>
</p>
<p>which means, in words, the probability that any sampled <italic>F</italic> value equals or exceeds the actually observed <italic>F</italic> value, given that the parameter values are set to the null hypothesis values. When the <italic>p</italic> value of <xref ref-type="disp-formula" rid="disp-formula4-1094428112457829">Equation 4</xref> is thought of as a function of <italic>F<sub>act</sub>
</italic> (with β fixed), then we can solve for the value of <italic>F<sub>act</sub>
</italic> that yields <italic>p</italic> = .05 and call that value the critical value, <italic>F<sub>crit</sub>
</italic>, because when <italic>F<sub>act</sub>
</italic> &gt; <italic>F<sub>crit</sub>
</italic>, then <italic>p</italic> &lt; .05. When the <italic>p</italic> value of <xref ref-type="disp-formula" rid="disp-formula4-1094428112457829">Equation 4</xref> is thought of as a function of β (with <italic>F<sub>act</sub>
</italic> fixed), then we can solve for the values of β that yield <italic>p</italic> = .05 and call those values the limits of the confidence interval on β, because values within those limits yield <italic>p</italic> ≥ .05 and hence are not rejected, but values beyond those limits yield <italic>p</italic> &lt; .05 and hence are rejected.</p>
<p>When the frequentist approach is applied to our job performance data introduced earlier in the article, the point estimate and the limits of the conventional 95% CI closely match the mean and 95% HDI of the Bayesian posterior distribution in <xref ref-type="fig" rid="fig3-1094428112457829">Figure 3</xref>. It is crucial to understand, however, that the frequentist result does not provide a continuous distribution over the parameters as the Bayesian analysis does. The frequentist result provides no test or CI on the noise parameter, σ. The frequentist result provides no probability distribution on the proportion of variance accounted for, <italic>R</italic>
<sup>2</sup>, whereas the Bayesian approach does. A frequentist CI can be constructed for <italic>R</italic>
<sup>2</sup> (e.g., <xref ref-type="bibr" rid="bibr72-1094428112457829">Steiger &amp; Fouladi, 1992</xref>), but it provides no distributional information and its limits depend on the sampling intention of the analyst, as explained below. The basic frequentist result also provides no comparative tests of standardized regression coefficients, as is revealed by the posterior distribution in <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref>. There are frequentist comparisons of standardized regression coefficients, but the conclusions from the tests depend on which tests are intended to be conducted.</p>
<p>Indeed, in the frequentist approach, every test incurs an additional opportunity for false alarm, and the primary concern for NHST decisions is keeping the false alarm rate capped at, for example, 5% (<xref ref-type="bibr" rid="bibr5-1094428112457829">Aguinis, Werner, et al., 2010</xref>). When additional tests are conducted, the space of possible <italic>F<sub>samp</sub>
</italic> enlarges because every test contributes <italic>F<sub>samp</sub>
</italic> values to the sampling distribution. Therefore, the <italic>p</italic> value in <xref ref-type="disp-formula" rid="disp-formula4-1094428112457829">Equation 4</xref> increases, even though the actual value <italic>F<sub>act</sub>
</italic> is unchanged. In other words, the <italic>p</italic> value measures how much of the space of imaginary unobserved outcomes exceeds the single actual outcome, and when the analyst’s imagination changes to include additional tests, the <italic>p</italic> value changes.</p>
<p>In particular, the default behavior of most frequentist software packages is to test each regression coefficient as if it were the only test being conducted. But when testing multiple regression coefficients, it is sensible to “correct” the <italic>p</italic> value of each test to take into account the larger sampling space of multiple tests (e.g., <xref ref-type="bibr" rid="bibr53-1094428112457829">Mundfrom, Perrett, Schaffer, Piccone, &amp; Roozeboom, 2006</xref>). Moreover, if additional comparisons of (standardized) regression coefficients are conducted, then the sampling space is again enlarged, and stricter corrections must be applied to the <italic>p</italic> values of each test. Thus, a single observed estimate of a regression coefficient can have many different <italic>p</italic> values and confidence intervals, depending on the testing intentions of the analyst.</p>
<p>A <italic>p</italic> value depends not only on the space of intended tests but also on the intended stopping rule for data sampling, for the same reason: The stopping intention, like the testing intention, determines the space of imaginary <italic>F<sub>samp</sub>
</italic> values relative to which the actual <italic>F<sub>act</sub>
</italic> is judged. The conventional stopping rule is to imagine sampling data from the null hypothesis until the sample size, <italic>N</italic>, meets some fixed threshold. But the data themselves bear no signature of why data collection stopped, because researchers are careful to collect data so that every datum is completely insulated from all other data collected before or after. Therefore, many researchers stop collecting data for reasons other than reaching a threshold sample size. In particular, a common stopping rule is sampling until a temporal threshold is reached (e.g., collect data for two weeks, or until 5 o’clock Friday afternoon). Under this stopping rule, the sampling distribution from the null hypothesis involves samples of various sizes, all of which could be collected in the same temporal window. Some imaginary <italic>F<sub>samp</sub>
</italic> values use sample sizes that are larger or smaller than the sample size in <italic>F<sub>act</sub>
</italic>, but all imaginary <italic>F<sub>samp</sub>
</italic> values use the same temporal threshold as <italic>F<sub>act</sub>
</italic>. Because there are different imaginary <italic>F<sub>samp</sub>
</italic> values, the sampling distribution for stop-at-threshold-sample-size is different from the sampling distribution for stop-at-threshold-time, and therefore the <italic>p</italic> values are different (for specific examples, see <xref ref-type="bibr" rid="bibr39-1094428112457829">Kruschke, 2010a</xref>, in press). Unfortunately, because the data bear no signature to distinguish whether sampling was stopped because of reaching threshold sample size or reaching threshold time, we do not know which <italic>p</italic> value is appropriate for judging the data. The dilemma is not solved by asking the data collector about his or her stopping intention, because identical data could be collected under different intentions, implying that two people with identical data could come to different conclusions about the statistical significance of the data.</p>
<p>There are other plausible stopping rules for data collection, which again yield different <italic>p</italic> values for any one set of data. Consider collecting data until a summary description of the data exceeds a preset threshold, and the variable being measured is how many data values had to be collected before that threshold was exceeded. For example, suppose we set a stopping threshold of <italic>F<sub>stop</sub>
</italic> = 2.0. We collect data until <italic>F<sub>act</sub>
</italic> &gt; <italic>F<sub>stop</sub>
</italic>, and we count how many data values were sampled. If there is a nonnull effect in the data, then it should not take much data to exceed the threshold, but if there is no effect in the data, it will take a long time to exceed the threshold. For the observed count of data values, we compute the probability that so few data would be collected if the null hypothesis were true, and this is the <italic>p</italic> value according to this stopping intention. Importantly, this <italic>p</italic> value differs from the <italic>p</italic> value computed from the same data under the intention of stopping until threshold sample size is reached. For more information about this sort of stopping rule, in the context of dichotomous-scale data instead of metric-scale data, see, for example, <xref ref-type="bibr" rid="bibr10-1094428112457829">Berger and Berry (1988)</xref>. For an example in the context of the traditional <italic>t</italic> test, see <xref ref-type="bibr" rid="bibr43-1094428112457829">Kruschke (in press)</xref>. The situation was succinctly summarized by <xref ref-type="bibr" rid="bibr11-1094428112457829">Berry (2006)</xref> as follows: “The <italic>p</italic>-value depends on the design of the trial and the intentions of the investigator” (p. 31).</p>
<p>The dependency of the <italic>p</italic> value on the stopping intention is distinct from concerns about repeatedly testing data as they are collected. The stopping rules discussed earlier did not repeatedly test data; they merely considered stopping at threshold sample size, at threshold time, or at threshold data-summary value. Once the threshold was reached, a single test was conducted. If, to the contrary, an analyst tests repeatedly as successive data are collected, then the <italic>p</italic> value is inflated because of the repeated tests, just as in the case of multiple comparisons (but the magnitude of inflation is different because of different structural overlap in the tests).</p>
<p>The problem of ill-defined <italic>p</italic> values is not based only on the arguments from the intended stopping rule and intended multiple tests of parameters for a single sample. An analogous argument applies when there are multiple groups. Just as different stopping intentions change the space of possible outcomes from the null hypothesis, different intended comparisons of groups change the space of possible outcomes from the null hypothesis. Effectively, the stopping rule for observing tests affects the <italic>p</italic> value just as the stopping rule for observing data.</p>
<p>Please note that the same logic described previously also applies to bootstrapping and resampling, which is, by definition, the creation of sampling distributions. So, although different from analytically derived sampling distributions in their mechanics and assumptions about the null hypothesis, bootstrapping actually inherits all the problems of <italic>p</italic> values and confidence intervals that apply to analytically derived sampling distributions. In particular, when the sampling intention changes, a bootstrapped or resampled <italic>p</italic> value changes, and the confidence interval changes.</p>
<p>Another problem with the frequentist approach is that it cannot accept a null hypothesis. Consider the large-sample-size example discussed in conjunction with <xref ref-type="fig" rid="fig4-1094428112457829">Figures 4</xref> and <xref ref-type="fig" rid="fig5-1094428112457829">5</xref>. Recall that the Bayesian analysis concluded that the second regression coefficient was equivalent to the null value for practical purposes. When conventional frequentism is applied to the data, the test of the second regression coefficient results in a <italic>p</italic> value close to 1. But this result does not let us accept the null value; it means merely that the null hypothesis would almost always produce <italic>F<sub>samp</sub>
</italic> values greater than <inline-formula id="inline-formula9-1094428112457829">
<mml:math id="mml-inline9-1094428112457829">
<mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">≅</mml:mo><mml:mn>0</mml:mn>
</mml:math>
</inline-formula>. In fact, a frequentist would <italic>reject</italic> the null hypothesis because the probability of getting <italic>F<sub>samp</sub>
</italic> <italic>less than</italic> <italic>F<sub>act</sub>
</italic> is extremely small, and therefore, something about the null hypothesis is probably wrong, such as the assumption of independence in the data.</p>
<p>Importantly, we cannot import a ROPE into frequentist decision making and use the CI like the HDI because of two reasons (although some have explored this approach, e.g., <xref ref-type="bibr" rid="bibr63-1094428112457829">Rogers, Howard, &amp; Vessey, 1993</xref>; <xref ref-type="bibr" rid="bibr77-1094428112457829">Westlake, 1976</xref>, <xref ref-type="bibr" rid="bibr78-1094428112457829">1981</xref>). First, unlike the HDI, the limits of the CI change when the intended stopping rule or comparisons change. Recall that the limits of the 95% CI are the parameter values at which the <italic>p</italic> value in <xref ref-type="disp-formula" rid="disp-formula4-1094428112457829">Equation 4</xref> is .05 (or .05/2 for two-tailed tests). When the <italic>p</italic> value changes, the CI changes. Thus, we simply cannot tell whether “the” 95% CI is within a ROPE because the 95% CI itself is ill-defined. Second, unlike the HDI, the CI tells us nothing about the credibility of the parameters in its bounds. It is tempting to psychologically confer distributional qualities upon the CI that are not actually there. For example, we could plot the <italic>p</italic> value in <xref ref-type="disp-formula" rid="disp-formula4-1094428112457829">Equation 4</xref> as a function of the parameter (e.g., <xref ref-type="bibr" rid="bibr61-1094428112457829">Poole, 1987</xref>; <xref ref-type="bibr" rid="bibr73-1094428112457829">Sullivan &amp; Foster, 1990</xref>). This graph shows the probability of extreme <italic>F</italic> values as a function of the hypothetical parameter value; it does not show the posterior probability of the parameter value, and it is not even a probability distribution (e.g., it does not integrate to 1). More sophisticated variations of the approach build probability distributions on the parameter, such that different intervals under the distribution correspond to different levels of confidence (e.g., <xref ref-type="bibr" rid="bibr70-1094428112457829">Schweder &amp; Hjort, 2002</xref>; <xref ref-type="bibr" rid="bibr71-1094428112457829">Singh, Xie, &amp; Strawderman, 2007</xref>). But these definitions of confidence distributions still assume that sampling proceeds until threshold sample size has been reached, and the confidence distributions change when the sampling intention changes, just as <italic>p</italic> values and confidence intervals change. Another way to impose a distributional interpretation on a confidence interval is by superimposing the sampling distribution of the sample statistic onto the parameter axis centered at the best point estimate of the parameter (<xref ref-type="bibr" rid="bibr22-1094428112457829">Cumming, 2007</xref>; <xref ref-type="bibr" rid="bibr23-1094428112457829">Cumming &amp; Fidler, 2009</xref>). Unfortunately, this approach has limited meaningfulness and applicability. The distribution it produces is not a distribution of parameter values; instead, it is a distribution of sample statistics assuming a fixed parameter value. The distinction between a sample statistic and a parameter value is especially stark when the approach is applied to dichotomous data, for which the sample statistic is a discrete proportion but the parameter is a continuous parameter. The distribution of sample statistics is also subject to the vicissitudes of different sampling intentions and is therefore just as ill-defined.</p>
<p>Some readers may wonder whether the prior distribution in a Bayesian analysis has the same slippery status as a sampling-space intention in the frequentist approach. To the contrary, a prior distribution and a sampling intention are quite different. First, the prior distribution is explicit, and its influence on the posterior distribution is easily checked. With the prior established, the posterior distribution is a fixed entity, unchanging whether or not various comparisons are considered and uninfluenced by the stopping intention of the data collector. On the other hand, stopping and testing intentions in frequentism are usually not explicit and usually rationalized post hoc. Second, and more importantly, the prior distribution <italic>should</italic> influence our interpretation of data, but the sampling-space intentions should <italic>not</italic>. Bayesian analysis indicates exactly how credibility should be reallocated across parameter values, starting from the prior distribution. If there is strong prior knowledge, it can be a blunder not to use it, as was previously explained, for example, in the case of random drug or disease testing. If there is only vague prior knowledge, Bayesian analysis nevertheless provides a complete posterior distribution over parameter values. On the other hand, researchers carefully insulate the data collection process from the intentions of sampling and testing, and therefore it is antithetical to base interpretive conclusions on <italic>p</italic> values and CIs that depend on such intentions. Because research design and scientific reasoning make the sampling and testing intentions irrelevant, a researcher’s intuitions about data analysis match Bayesian interpretation, not frequentism (<xref ref-type="bibr" rid="bibr24-1094428112457829">Dienes, 2011</xref>).</p>
<p>We are not the first to point out flaws of the frequentist approach, but the particular examples presented here are novel. Some previous critiques include those by <xref ref-type="bibr" rid="bibr10-1094428112457829">Berger and Berry (1988)</xref>, <xref ref-type="bibr" rid="bibr16-1094428112457829">Cohen (1994)</xref>, <xref ref-type="bibr" rid="bibr38-1094428112457829">Kline (2004)</xref>, <xref ref-type="bibr" rid="bibr51-1094428112457829">McCloskey (1995)</xref>, <xref ref-type="bibr" rid="bibr55-1094428112457829">Nickerson (2000)</xref>, and <xref ref-type="bibr" rid="bibr75-1094428112457829">Wagenmakers (2007)</xref>. Many of those critiques point out difficulties in correctly understanding NHST (e.g., <xref ref-type="bibr" rid="bibr5-1094428112457829">Aguinis, Werner, et al., 2010</xref>). Our argument is that especially when traditional frequentist analysis is <italic>correctly</italic> understood, including the use of confidence intervals, it is seen to be fundamentally flawed in its foundational logic.</p>
</sec>
</sec>
<sec id="section8-1094428112457829">
<title>Recommendations and Illustration of How to Report Bayesian Analysis Results</title>
<p>Guidelines for reporting a Bayesian data analysis are provided by <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke (2011a</xref>, chap. 23). We offer five recommendations regarding what information to include in a manuscript reporting Bayesian analysis. First, motivate the use of Bayesian analysis if the targeted readership is not familiar with such an approach. Second, describe the model and its parameters because the parameter values bear the meaning of the analysis. Third, describe and justify the prior distribution for a potentially skeptical readership. Fourth, mention the MCMC details, including evidence that the chains are fully representative of the underlying posterior distribution. Finally, interpret the posterior distribution. (Additional optional points are discussed by <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke, 2011a</xref>, chap. 23.) Next, we offer the following paragraphs as an illustration of writing up a realistic data analysis using our job performance example and implementing the aforementioned five recommendations. Before analyzing data, however, we need to describe the data themselves.</p>
<p>We were interested in predicting job performance based on GMA, conscientiousness, and biodata. We initially targeted a random sample of 485 employees of a firm. Thirty-five employees did not have current contact information. Of those we could attempt to contact, 362 responded and, of those, 346 provided complete information about the three predictors, and we also had criterion data available (i.e., supervisory ratings of performance). The predictors and criterion were all measured on 1-to-7 Likert-type scales.</p>
<p>We analyzed the data with Bayesian linear regression. Unlike traditional NHST-based statistics, Bayesian analysis yields complete distributional information regarding the parameters in the regression model. Bayesian analysis uses only the observed data and does not use <italic>p</italic> values and confidence intervals that are based on hypothetical unobserved data that might have been obtained assuming a particular stopping intention about sample size of the researcher. Moreover, traditional analysis assumes that the stopping intention was to cease data collection at a preset sample size—an assumption that does not apply to our data. (Presumably, as Bayesian analyses become routine, this sort of explicit justification will not be needed.)</p>
<p>We used the standard linear regression model, in which the criterion value is described as normally distributed around a linear combination of predictor values:</p>
<p>
<disp-formula id="disp-formula6-1094428112457829">
<mml:math id="mml-disp6-1094428112457829">
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">=</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">+</mml:mo><mml:msub><mml:mi mathvariant="italic">β</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">,</mml:mo><mml:mi mathvariant="italic">σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula6-1094428112457829" xlink:href="10.1177_1094428112457829-eq6.tif"/>
</disp-formula>
</p>
<p>The model has five parameters: β<italic>
<sub>j</sub>
</italic> for <italic>j</italic> ≥ 1 indicates how much the criterion increases when the <italic>j</italic>th predictor increases one unit and the other predictors are held constant, β<sub>0</sub> indicates the value of the criterion when all three predictors are zero, and σ indicates the standard deviation of the residual scores in the criterion around the linearly predicted value. Our primary interest is in the magnitudes of the regression coefficients on the three predictors.</p>
<p>We used a noncommittal broad prior on the parameters so that the prior had minimal influence on the posterior. For the analysis, the data were standardized, and the intercept and slope parameters had normal priors with mean zero and standard deviation of 10, which is very large relative to the scale of the standardized data (because standardized regression coefficients will tend to fall between –1 and +1). The residual-noise parameter had a uniform prior extending from zero to 10 (which is extremely broad and inclusive relative to the standardized noise of 1). The estimated parameters were linearly transformed back to the original scale (see Eq. 17.1, p. 459, of <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke, 2011a</xref>).</p>
<p>The posterior was generated as an MCMC sample using the free software R, rjags, and JAGS (<xref ref-type="bibr" rid="bibr59-1094428112457829">Plummer, 2003</xref>, <xref ref-type="bibr" rid="bibr60-1094428112457829">2011</xref>; <xref ref-type="bibr" rid="bibr62-1094428112457829">R Development Core Team, 2012</xref>). Three chains were initialized at the maximum likelihood values of the parameters and well burned in (for 1,000 steps), and a total of 250,000 steps were saved. There was very little autocorrelation in the well-mixed chains. The resulting MCMC sample is therefore highly representative of the underlying posterior distribution.</p>
<p>As shown in <xref ref-type="fig" rid="fig3-1094428112457829">Figure 3</xref>, the marginal posterior on <italic>b</italic>
<sub>1</sub> had a mean of 0.241 and a 95% HDI that extended from 0.139 to 0.343. Indeed, all of the 250,000 representative values in the posterior were well above zero, so zero was deemed to be not credible. (As these reports become routine in the literature, the language could be compressed; e.g., marginal posterior mean <italic>b</italic>
<sub>1</sub> [GMA] = 0.241, 95% HDI = 0.139, 0.343; zero deemed not credible). Marginal posterior mean <italic>b</italic>
<sub>2</sub> (conscientiousness) = 0.0743, 95% HDI = –0.0385, 0.189; zero among the credible values. Thus, although conscientiousness (<italic>x</italic>
<sub>2</sub>) might have a nonzero predictiveness for job performance, the uncertainty in its estimated influence (<italic>b</italic>
<sub>2</sub>) is large relative to the magnitude of the influence. Marginal posterior mean <italic>b</italic>
<sub>3</sub> (biodata) = 0.205, 95% HDI = 0.0885, 0.321; zero deemed not credible (with 100% of the posterior greater than zero). Marginal posterior mean intercept <italic>b</italic>
<sub>0</sub> (job performance) = 1.94, 95% HDI = 1.36, 2.56. Marginal posterior modal residual-noise σ = 1.37, 95% HDI = 1.28, 1.48. The posterior on <italic>R</italic>
<sup>2</sup> had 95% HDI from 0.122 to 0.137.</p>
<p>We were interested in assessing whether the predictors were differentially predictive of job performance, and therefore we examined the posterior distributions of the differences of standardized regression coefficients, keeping in mind that the comparison is based on standardization using the sample only (e.g., <xref ref-type="bibr" rid="bibr37-1094428112457829">King, 1986</xref>). <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref> shows that for GMA versus conscientiousness, the posterior mean difference of β<sub>1</sub> – β<sub>2</sub> = 0.165, 95% HDI = 0.0183, 0.307, with 98.7% of differences being greater than zero. Therefore, if collecting GMA and conscientiousness data is equally costly, and if we wish to avoid the double cost of measuring both, then it is probably more effective to assess GMA than conscientiousness.</p>
<p>Notice that the preceding summary of the Bayesian analysis never mentioned <italic>p</italic> values, confidence intervals, or corrections for multiple tests. If we had conducted a frequentist analysis, we would have had to correct the <italic>p</italic> values and confidence intervals for the six or more tests we conducted (corresponding to the panels of <xref ref-type="fig" rid="fig2-1094428112457829">Figure 2</xref>), taking into account the structural relations of the tests (e.g., <xref ref-type="bibr" rid="bibr53-1094428112457829">Mundfrom et al., 2006</xref>). With each test, the <italic>p</italic> values increase, and the CIs widen. By contrast, the posterior distribution from the Bayesian analysis is a fixed entity based on the observed data. The posterior distribution highlights explicitly the uncertainty in the estimation of each parameter.</p>
</sec>
<sec id="section9-1094428112457829">
<title>Synthesis of Advantages of Bayesian Data Analysis</title>
<p>In this section, we summarize a number of key advantages of Bayesian methods for data analysis over frequentist analysis. <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref> includes a summary of these advantages. This section also includes several examples of how specific published research that is based on frequentist analysis-based approaches failed to benefit from the advantages offered by a Bayesian approach—and how using Bayesian analyses would have led to richer and, in some cases, different substantive conclusions. In the very few instances when available, we actually illustrate the advantages of Bayesian analysis using published research that did adopt this approach.</p>
<table-wrap id="table2-1094428112457829" position="float">
<label>Table 2.</label>
<caption>
<p>Brief Summary of Selected Advantages of Bayesian Analysis Over Frequentist Analysis</p>
</caption>
<graphic alternate-form-of="table2-1094428112457829" xlink:href="10.1177_1094428112457829-table2.tif"/>
<table>
<thead>
<tr>
<th>Issue</th>
<th>Frequentist</th>
<th>Bayesian</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. Use of prior knowledge</td>
<td>Does not incorporate prior knowledge into estimation of parameters. Instead, only uses the data at hand. As a result, the estimated parameters can be less precise.</td>
<td>Explicitly incorporates prior knowledge into estimation of parameters by expressing the prior knowledge in terms of a prior distribution, which is then combined with the data to produce the posterior.</td>
</tr>
<tr>
<td>2. Joint distribution of parameters</td>
<td>Creates only a local approximation.</td>
<td>Generates accurate joint distribution of parameter estimates.</td>
</tr>
<tr>
<td>3. Assessment of null hypotheses</td>
<td>Null hypothesis cannot be accepted.</td>
<td>Null values can be accepted as well as rejected.</td>
</tr>
<tr>
<td>4. Ability to test complex models</td>
<td>Often difficult to derive<italic> p</italic> values, confidence intervals on parameters, and confidence intervals on predictions for new data.</td>
<td>Flexibility in adapting a model to diverse data structures and distributions (e.g., nonnormal) with no change in inference method.</td>
</tr>
<tr>
<td>5. Unbalanced or small sample sizes</td>
<td>In unbalanced designs in analysis of variance (ANOVA), user must choose “Type I” or “Type III” error terms. In chi-square tests, small sample violates assumptions for obtaining an accurate<italic> p</italic> value.</td>
<td>Posterior distribution reveals greater uncertainty in parameter estimates for cells with smaller sample sizes. Sample size does not affect inference method.</td>
</tr>
<tr>
<td>6. Multiple comparisons</td>
<td>Requires corrections for inflated Type I error rates (i.e., false positives), and correction depends on the comparisons that are intended by the researcher.</td>
<td>No reference to intended or post hoc comparisons. Ease of hierarchical modeling allows rational sharing of information across groups to “shrink” estimates.</td>
</tr>
<tr>
<td>7. Power analysis and replication probability</td>
<td>Virtually unknowable because of lack of distributional information in confidence interval.</td>
<td>Precisely estimated from complete distribution of parameter values.</td>
</tr>
</tbody>
</table>
</table-wrap>
<sec id="section10-1094428112457829">
<title>Use of Prior Knowledge</title>
<p>The first advantage included in <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref> is that Bayesian data analysis incorporates not only the data at hand but also prior accumulated knowledge to generate parameter estimates that tend to be both different and more accurate than those derived under frequentist methods. This is especially true when the prior accumulated knowledge corresponds to a distribution that is substantially different from a noncommittal broad prior distribution.</p>
<p>As an illustration of this advantage in a specific organizational science context, <xref ref-type="bibr" rid="bibr12-1094428112457829">Brannick (2001)</xref> described a situation involving 10 local validation studies of a situation judgment test. The 10 corresponding correlations were calculated using frequentist analysis. Because of the nature of frequentist analysis, the calculation of each frequentist analysis-derived correlation did not involve the use of prior knowledge, which in this example consists of correlations calculated from the other nine studies. As a result, one of the studies yielded a statistically nonsignificant correlation of .15 (<italic>p</italic> &gt; .05). In contrast, when Bayesian analysis was employed such that prior knowledge regarding the other correlations was taken into account, the same study yielded a Bayesian-based correlation of .18. Moreover, the corresponding 95% HDI did not contain the correlation value of .00. In short, the substantive conclusion based on a frequentist approach was that the correlation was not different from zero, but using a Bayesian approach led to the conclusion that the correlation was different from zero.</p>
<p>A difference of less than .05 correlation units—about 33% change— in the estimated magnitude of a correlation might not seem impressive in a number of contexts. At the same time, in many other contexts such as in the case of Brannick’s 10 local validation studies, such a difference was practically significant enough for two local validation studies’ correlations to be judged as different from zero (under Bayesian analysis) but statistically nonsignificant (under frequentist analysis). This is precisely the type of apparently small difference that actually has important practical consequences (<xref ref-type="bibr" rid="bibr19-1094428112457829">Cortina &amp; Landis, 2009</xref>). From a substantive perspective, the situation judgment test is now deemed valid in the local context, whereas the frequentist analysis may have reached the conclusion that the test should not be used because the relationship between predictor and criterion scores was not statistically different from zero. The great potential of Bayesian methods applied specifically to meta-analysis led <xref ref-type="bibr" rid="bibr69-1094428112457829">Schmidt and Raju (2007)</xref> to conclude that “the medical model [i.e., adding new studies to the existing database and recalculating the meta-analysis] and the alternative Bayesian procedure proposed here should be the methods of choice for updating meta-analytic findings in most research areas” (p. 305).</p>
<p>In summary, in practical applications, we benefit from using both prior knowledge and the data at hand to derive accurate parameter estimates. Bayesian data analysis does just that. By translating the prior knowledge into a prior distribution, and then using Bayes’ rule to combine the prior with the current data, the resulting posterior distribution gives the researcher parameter estimates that are the best available lacking further information.</p>
</sec>
<sec id="section11-1094428112457829">
<title>Joint Distribution of Parameters</title>
<p>Mentioned in Issue 2 in <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref>, the second advantage of Bayesian data analysis is that the method produces a <italic>joint</italic> (i.e., simultaneous) distribution of credible parameter estimates across multiple predictors in a multidimensional parameter space, thereby allowing the researcher to examine the trade-offs among values of different parameter estimates across the multiple predictors. The computer program described in the <xref ref-type="app" rid="app1-1094428112457829">appendix</xref> produces scatter plots of jointly credible values for all pairs of parameters. For linear regression with normally distributed noise, the scatter plots tend to be simple oval shapes, but for other models, the scatter plots can be “banana” or S-shaped, indicating interesting trade-offs in credible parameter values. Frequentist analysis can use an asymptotic approximation to the likelihood function, but this approach fails to describe the posterior distribution in general. Although we did not include an illustration of this issue in our article due to space constraints, the programs provide graphical displays of the posterior for all pairs of parameters.</p>
<p>Being able to know the accurate trade-offs among credible parameter estimates across multiple predictors is not a mere technical refinement. Consider our example from <xref ref-type="fig" rid="fig2-1094428112457829">Figures 2</xref> and <xref ref-type="fig" rid="fig3-1094428112457829">3</xref>, where Bayesian data analysis is used to fit a linear regression model to data, and job performance is predicted by GMA, conscientiousness, and biodata. Previous research indicates that conscientiousness and biodata are fairly strongly correlated (<italic>r</italic> = .51; <xref ref-type="bibr" rid="bibr64-1094428112457829">Roth et al., 2011</xref>). If we also have prior knowledge about how strongly either conscientiousness or biodata predicts job performance, then we can use that prior knowledge to leverage more precise estimation of the other predictor (<xref ref-type="bibr" rid="bibr76-1094428112457829">Western &amp; Jackman, 1994</xref>). The inferential leverage derives from the trade-off in parameter estimates: The prior knowledge about one parameter narrows the estimate of the other parameter because of the trade-off.</p>
</sec>
<sec id="section12-1094428112457829">
<title>Assessment of Null Hypothesis</title>
<p>The third advantage summarized in <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref> refers to the assessment of the null hypothesis. Within a Bayesian approach, as described earlier, accepting a null value involves establishing a ROPE around the value of interest. For example, if we are interested in the null value (i.e., zero) for a regression coefficient, we establish slope values that are equivalent to zero for practical purposes in the particular application. On the other hand, the classical frequentist framework has no way of accepting the null hypothesis (cf. <xref ref-type="bibr" rid="bibr18-1094428112457829">Cortina &amp; Folger, 1998</xref>). The frequentist approach cannot incorporate an analogous decision rule involving a ROPE and CI (as opposed to HDI) because the CI, unlike the HDI, does not indicate which parameter values are credible, and the CI changes its size when the sampling and testing intentions change.</p>
<p>Furthermore, in NHST, a researcher is mathematically guaranteed to reject the null hypothesis even when it is true, if the sample size is allowed to grow indefinitely and a test is conducted with every additional datum (e.g., <xref ref-type="bibr" rid="bibr3-1094428112457829">Aguinis &amp; Harden, 2009</xref>; <xref ref-type="bibr" rid="bibr7-1094428112457829">Anscombe, 1954</xref>; <xref ref-type="bibr" rid="bibr17-1094428112457829">Cornfield, 1966</xref>; <xref ref-type="bibr" rid="bibr43-1094428112457829">Kruschke, in press</xref>). This “sampling to reach a foregone conclusion” does not happen in a Bayesian approach. Instead, because the HDI narrows as sample size increases, and therefore the null has greater probability of being accepted when it is true, it is the case that the probability of false alarm asymptotes at a relatively small value (depending on the specific choice of ROPE).</p>
</sec>
<sec id="section13-1094428112457829">
<title>Ability to Test Complex Models</title>
<p>Issue 4 in <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref> refers to another important advantage of Bayesian methods, which is the ease of analyzing complex data structures and models. The model specification language is quite flexible in the Bayesian software JAGS (see the <xref ref-type="app" rid="app1-1094428112457829">appendix</xref>). All the usual forms of the generalized linear model can be specified, including multiple linear regression, analysis of variance (ANOVA), analysis of covariance, logistic regression, ordinal regression, log-linear models for contingency tables, and many other data-analytic approaches that are the most frequently used by organizational science researchers (e.g., <xref ref-type="bibr" rid="bibr4-1094428112457829">Aguinis et al., 2009</xref>; <xref ref-type="bibr" rid="bibr28-1094428112457829">Gelman &amp; Hill, 2007</xref>; <xref ref-type="bibr" rid="bibr36-1094428112457829">Jackman, 2009</xref>; <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke, 2011a</xref>; <xref ref-type="bibr" rid="bibr66-1094428112457829">Scandura &amp; Williams, 2000</xref>). A Bayesian approach can also be used to implement mixed models and nesting of variables in hierarchical models. Bayesian models are especially useful also for nonlinear models. Moreover, a researcher is not limited to normal distributions for describing metric data; other distributions can be used instead to accommodate outliers or skew in robust regression (<xref ref-type="bibr" rid="bibr56-1094428112457829">O’Boyle &amp; Aguinis, 2012</xref>; <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke, 2011a</xref>). Mixture models are handled by Bayesian approaches because the assignment of data to mixture components is probabilistic, and the estimated parameters are jointly distributed with the assignments of data. In other words, instead of only a single assignment of data to mixture components, multiple credible assignments are assessed simultaneously.</p>
<p>In general, a researcher can flexibly create a hierarchical nonlinear model that reflects a structure appropriate to the data. The inference of credible parameter values takes place regardless of the type of model. In all cases, a complete joint distribution of credible parameter values is created, even for dozens or hundreds of parameters, given the single set of actually observed data. Nonlinear models are becoming more important in the organizational sciences, as we gain precision in specifying the functional forms of relations between variables (<xref ref-type="bibr" rid="bibr26-1094428112457829">Edwards &amp; Berry, 2010</xref>; <xref ref-type="bibr" rid="bibr58-1094428112457829">Pierce &amp; Aguinis, in press</xref>). Moreover, hierarchical models are becoming increasingly pervasive in many organizational science domains (<xref ref-type="bibr" rid="bibr4-1094428112457829">Aguinis et al., 2009</xref>). For example, in many applications, researchers measure aspects of individuals within different groups, and the researchers want to estimate the effects of both individual-level variables (e.g., job satisfaction) and group-level variables (e.g., team cohesion) on individual-level outcomes (e.g., performance). Bayesian software allows specification of any number of levels. Within each level, there can be complex models. For example, we might have a multiple regression model for each individual and a higher level model of how individual-level regression coefficients are distributed across groups (for a complete working example, see section 16.3 of <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke, 2011a</xref>, or <xref ref-type="bibr" rid="bibr48-1094428112457829">Lykou &amp; Ntzoufras, 2011</xref>). Hierarchical modeling is also useful for meta-analysis, in which particular studies play the role of individuals, and higher level model structure has parameters that describe overall tendencies across studies (e.g., <xref ref-type="bibr" rid="bibr42-1094428112457829">Kruschke, 2011b</xref>). Frequentist applications can be exceedingly difficult for complex applications, especially for nonlinear or nonnormal models, because generating sampling distributions and confidence intervals is often intractable (<xref ref-type="bibr" rid="bibr56-1094428112457829">O’Boyle &amp; Aguinis, 2012</xref>). Moreover, even in complex hierarchical applications, Bayesian methods straightforwardly generate predictions that accurately incorporate the full distribution of credible parameter values instead of just a single point estimate.</p>
<p>As an example of how the flexibility of Bayesian methods might have affected recent work in the organizational sciences, consider the extensive interest in how creativity and organizational innovation relate to competitiveness (<xref ref-type="bibr" rid="bibr79-1094428112457829">Zhou, 2003</xref>). Research by <xref ref-type="bibr" rid="bibr34-1094428112457829">Hirst, Van Knippenberg, and Zhou (2009)</xref> revealed a complex, polynomial-trend relationship between employee learning orientation and employee creativity that was moderated by a third factor, team learning behavior. The researchers used frequentist hierarchical linear models. Despite the sophistication of their analysis, had the researchers used a Bayesian approach, they would have discovered similar trends but with complete distributional information about the trade-offs between coefficients on different trend components. Moreover, the researchers could have implemented nonnormal distributions at any level in the hierarchical model, to accommodate outliers, skewed data, or alternative assumptions about the distributions of trend parameters across different teams. Bayesian analyses such as these have great potential to be used to critically evaluate the robustness of recent conceptualizations involving nonnormal (<xref ref-type="bibr" rid="bibr56-1094428112457829">O’Boyle &amp; Aguinis, 2012</xref>) and nonlinear (<xref ref-type="bibr" rid="bibr58-1094428112457829">Pierce &amp; Aguinis, in press</xref>) relationships.</p>
</sec>
<sec id="section14-1094428112457829">
<title>Unbalanced or Small Sample Sizes</title>
<p>Regarding Issue 5 in <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref>, Bayesian methods can be used regardless of the overall sample size or relative sample sizes across conditions or groups. Essentially, in Bayesian analysis, there is an updating of the parameter estimates for every individual datum, so it does not matter, computationally, where in the design each datum appears. By contrast, in frequentist ANOVA, when different cells of the design include different sample sizes, a researcher must decide between using Type I or Type III error terms in computing the best estimates and corresponding<italic> p</italic> value. Similarly, in frequentist approaches to moderated multiple regression models with categorical moderator variables, an unequal number of individuals across groups (e.g., more men than women or more Whites than African Americans) leads to important errors in prediction (<xref ref-type="bibr" rid="bibr2-1094428112457829">Aguinis, Culpepper, &amp; Pierce, 2010</xref>). These problems are not relevant in the Bayesian analogues of multiple regression and ANOVA. As another example, in the frequentist chi-square tests of independence, the<italic> p</italic> value is determined by approximating the sampling distribution of the discrete Pearson chi-square value with the continuous chi-square sampling distribution, but that approximation is reasonably good only when the expected cell frequency is about 5 or larger. Therefore, researchers must acknowledge that results and substantive conclusions might be incorrect. There is no such issue in Bayesian analogues of chi-square tests because the analysis estimates parameter values without relying on large-sample approximations.</p>
<p>Dynamic analyses of social networks in organizations are becoming an increasingly important research area in the organizational sciences (<xref ref-type="bibr" rid="bibr54-1094428112457829">Newman, Barabasi, &amp; Watts, 2006</xref>). Recently, <xref ref-type="bibr" rid="bibr65-1094428112457829">Sasovova, Mehra, Borgatti, and Schippers (2010)</xref> used longitudinal data on friendship relations in the department of an organization to find that “high self-monitors were more likely than low self-monitors to attract new friends and to occupy new bridging positions over time” (p. 639). In doing so, <xref ref-type="bibr" rid="bibr65-1094428112457829">Sasovova et al. (2010)</xref> used a frequentist procedure called the quadratic assignment procedure (QAP). However, as explained by <xref ref-type="bibr" rid="bibr14-1094428112457829">Casciaro and Lobo (2008)</xref>, QAP assumes completely balanced sample sizes across distinct respondents, even when it is usually the case that individuals have different numbers of social network ties with other individuals such that the assumption of completely balanced sample sizes is untrue. Thus, it is likely that QAP’s assumption of equal sample sizes biased the parameter estimates and the standard errors reported in <xref ref-type="bibr" rid="bibr65-1094428112457829">Sasovova et al. (2010)</xref>. Had <xref ref-type="bibr" rid="bibr65-1094428112457829">Sasovova et al. (2010)</xref> used a parametric descriptive model and Bayesian estimation, results would not have been susceptible to these problems.</p>
</sec>
<sec id="section15-1094428112457829">
<title>Multiple Comparisons</title>
<p>Issue 6 in <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref> refers to another advantage of Bayesian methods that results from obviating <italic>p</italic> values and confidence intervals. In frequentist analysis, the essential decision criterion is that α (i.e., Type I error rate) = .05 (or some other value such as .01), which means simply that the probability of falsely rejecting the null value is capped at the nominal level (i.e., typically 5%). A new problem arises when applying this decision criterion to situations in which there are multiple tests. The problem is that every additional comparison presents an opportunity for a Type I error (i.e., false positive). Traditional analysts want to keep the overall probability of false positives, across the whole family of tests, at 5%. To do so, each individual comparison must be “corrected” to require a more stringent (i.e., smaller)<italic> p</italic> value for a difference to be deemed statistically significant. This is a scientifically dubious practice because the difference between two groups can be declared to be significant or not entirely on the basis of whether a researcher is inquisitive and decides to conduct many comparisons or feigns disinterest and runs only a few comparisons.</p>
<p>In Bayesian analysis, there is no use of<italic> p</italic> values and confidence intervals and no influence from which comparisons, and how many, a researcher might or might not want to make. Instead, the distribution of credible parameter values is determined purely by the data and the structure of the model. Bayesian methods do not escape the possibility of false positives, of course, because they are caused by coincidences of rogue data that can arise in any random sample. But Bayesian methods do not try to control for inflation in Type I error rates on the basis of a researcher’s explicit or implicit intentions. Bayesian methods can incorporate rational constraints in a model’s structure so that the data from different groups mutually inform each other’s estimates and thereby reduce the extremity of outlying estimates. For example, Bayesian approaches to ANOVA can use a hierarchical model structure such that the estimates of group means shrink toward the overall mean. In other words, the data themselves dictate how much the estimates of outlying groups should be shrunken. This shrinkage in the estimates of group means attenuates false positives (<xref ref-type="bibr" rid="bibr29-1094428112457829">Gelman, Hill, &amp; Yajima, 2009</xref>; <xref ref-type="bibr" rid="bibr39-1094428112457829">Kruschke 2010a</xref>, <xref ref-type="bibr" rid="bibr40-1094428112457829">2010b</xref>). The same approach applies to shrinking estimates of regression coefficients across multiple predictors (e.g., <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke, 2011a</xref>; <xref ref-type="bibr" rid="bibr48-1094428112457829">Lykou &amp; Ntzoufras, 2011</xref>). Hierarchical models are not unique to Bayesian estimation, but they are especially straightforward to implement and evaluate in Bayesian software.</p>
</sec>
<sec id="section16-1094428112457829">
<title>Power Analysis and Replication Probability</title>
<p>The seventh issue summarized in <xref ref-type="table" rid="table2-1094428112457829">Table 2</xref> contrasts traditional and Bayesian approaches regarding statistical power analysis and replication probability. In traditional NHST, statistical power is the probability that a null hypothesis would be rejected if a particular alternative nonnull effect were true and sampling proceeded in a particular way. More generally, statistical power is the probability that an existing population effect will be detected in a sample of observed data. The key prerequisite in any power analysis is to determine a targeted nonzero effect size. In traditional power analysis, there is only one targeted effect size with no distribution of other reasonable values (e.g., <xref ref-type="bibr" rid="bibr67-1094428112457829">Scherbaum &amp; Ferreter, 2009</xref>). Because we have no sense of how uncertain the power estimate is, traditional analysis yields estimates that have little stability and are, according to a recent analysis, “virtually unknowable” (<xref ref-type="bibr" rid="bibr52-1094428112457829">Miller, 2009</xref>; see also <xref ref-type="bibr" rid="bibr30-1094428112457829">Gerard, Smith, &amp; Weerakkody, 1998</xref>; <xref ref-type="bibr" rid="bibr74-1094428112457829">Thomas, 1997</xref>). This may be one of the reasons that published research is still underpowered (<xref ref-type="bibr" rid="bibr49-1094428112457829">Maxwell, 2004</xref>) despite repeated calls that researchers conduct power analyses (e.g., <xref ref-type="bibr" rid="bibr1-1094428112457829">Aguinis, Beaty, Boik, &amp; Pierce, 2005</xref>).</p>
<p>In a Bayesian framework, the estimate of power is robust because the hypothetical model uses the complete distribution of credible parameter values. Power is estimated by using a large number of credible parameter-value combinations and from each generating simulated data. The simulated data are analyzed by Bayesian methods and tallied with respect to whether the effect was detected. Across many simulations, power is thereby estimated while taking into account the full uncertainty of the posterior parameter distribution. This method works for any model, regardless of hierarchical complexity or distributional assumptions. For an example of Bayesian power analysis, see <xref ref-type="bibr" rid="bibr43-1094428112457829">Kruschke (in press)</xref>, and for complete technical details, see <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke (2011a</xref>).</p>
<p>As an example of the use of power analysis in organizational science research, consider a recent study of situational influences on personality states through time (<xref ref-type="bibr" rid="bibr35-1094428112457829">Huang &amp; Ryan, 2011</xref>). Because some of the observed effects trended in the predicted directions but did not reach statistical significance based on a traditional approach, the researchers conducted a power analysis to estimate the sample size that would have been needed to obtain a value of .80. The power and sample size estimates were based on the point estimate of an effect size in the frequentist analysis without considering the uncertainty of that point estimate. Therefore, we do not know whether the estimated power and needed sample size are stable or not. A Bayesian analysis, on the other hand, would take into account the uncertainty in the effect size estimate.</p>
</sec>
</sec>
<sec id="section17-1094428112457829">
<title>Concluding Comments</title>
<p>Although we believe that adopting a Bayesian approach has many benefits and advantages compared with frequentist analysis, we readily acknowledge several potential challenges. First, as is the case with any new methodological approach, researchers interested in adopting Bayesian methods will need to invest the necessary time and effort in the learning process. Given the several textbooks available, as well as introductory courses offered at many universities and professional organization meetings, we believe that the learning curve should not be too steep. However, we also recognize that understanding the benefits of a new approach and adopting it are best accomplished by actually trying it. The program described in the <xref ref-type="app" rid="app1-1094428112457829">appendix</xref> is packaged for easy use with the illustrative data file we used in our article. Second, regarding the issue of implementation, researchers interested in adopting a Bayesian approach will also need to invest time and effort into learning new software tools. Although initially this may seem like a daunting process, packages are becoming increasingly user-friendly, and several textbooks provide step-by-step illustrations as well as data sets to facilitate the learning process (see the <xref ref-type="app" rid="app1-1094428112457829">appendix</xref>; <xref ref-type="bibr" rid="bibr21-1094428112457829">Culpepper &amp; Aguinis, 2011</xref>; <xref ref-type="bibr" rid="bibr41-1094428112457829">Kruschke, 2011a</xref>, in press).</p>
<p>Another issue to consider is that in actual empirical research, the sample of data comes from an unknown underlying process in the world. The researcher chooses a descriptive model based on theoretical motivations prior to the computational analysis. This choice of descriptive model confronts both Bayesian and non-Bayesian analysis, and it is not magically solved by a Bayesian approach. Once candidate models are selected, however, Bayesian methods are excellent for model comparison because they automatically take into account differences in the number of parameters and structural complexity. Bayesian methods therefore provide a formal mechanism for “strong inference” via competitive testing that has recently been advocated for the organizational sciences (<xref ref-type="bibr" rid="bibr31-1094428112457829">Gray &amp; Cooper, 2010</xref>; <xref ref-type="bibr" rid="bibr45-1094428112457829">Leavitt, Mitchell, &amp; Peterson, 2010</xref>).</p>
<p>Finally, it has been argued that Bayesian analysis is an almost magical inferential process that has been “oversold as an all-purpose statistical solution to genuinely hard problems” (<xref ref-type="bibr" rid="bibr27-1094428112457829">Gelman, 2008</xref>, p. 446). To address this concern, it is important to understand that Bayesian methods are a type of data-analytic and inferential procedure and are affected by research design and measurement issues much like any other inferential data-analytic procedure. For example, if the data in hand were collected using a nonexperimental design, then we will not be able to draw conclusions about causality. Also, if the data were collected using unreliable (i.e., “noisy”) measures, the resulting estimates will be imprecise, but the Bayesian analysis will explicitly reveal the uncertainty in the parameter values. In short, Bayesian analysis is an important improvement over existing frequentist methods because it provides information that researchers want to know: the credible parameter values, given the data that have been obtained thus far. However, using Bayesian statistics does not diminish the importance of research design and measurement choices, and it certainly is no substitute for good conceptual thinking.</p>
<p>In closing, Bayesian methods are essentially absent from the organizational science literature. There may be many reasons for why this may be the case, including a lack of clear understanding of advantages as well as a lack of clear guidelines on how to actually conduct and report results of a Bayesian analysis. As recently noted by <xref ref-type="bibr" rid="bibr57-1094428112457829">Orlitzky (2012)</xref>, “Methods training should clearly and explicitly unmask as illusory the belief that NHST is some deus ex machina for instilling objectivity and factuality &amp; in the long run, this would involve a shift toward a Bayesian view of probability” (p. 209). We hope that our article will serve as a catalyst for the adoption of Bayesian methods in the organizational sciences, and we look forward to future Bayesian applications leading to important theoretical advances and organizational practices.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-1094428112457829">
<title>Appendix</title>
<sec id="section18-1094428112457829">
<title>Software for Bayesian Multiple Linear Regression</title>
<p>Complete information for installing the software is provided at <ext-link ext-link-type="uri" xlink:href="http://www.indiana.edu/∼kruschke/BMLR/">http://www.indiana.edu/∼kruschke/BMLR/</ext-link>. “BMLR” stands for Bayesian multiple linear regression. The programs linked in the website provide a complete working example, along with the data used for Figures 2 through 5 in the main text of the article.</p>
<p>After the software has been installed, please do the following. Open the file, BMLRexample.R, in the editor RStudio. Make sure that R’s working directory is the folder with the BMLR programs. The program can be run “as is” to produce graphs such as Figures 2 through 5. The program also produces scatter plots of the posterior for all pairs of parameters, thereby showing the trade-offs in parameter estimates.</p>
<p>An initial step executed by the program BMLRexample.R is loading of the relevant data. The data are formatted in a matrix, such that there is one row per measurement unit, with the first column containing the predicted value (<italic>y</italic>), and the remaining columns containing the predictor values (<italic>x</italic>). The following programming code assumes that the data matrix is loaded into R in a matrix called dataMat. Then, the multiple linear regression functions are loaded into R’s active memory, using the command:</p>
<p>
<monospace>source("BMLR.R")</monospace>
</p>
<p>The MCMC chain for the posterior distribution is generated with the command:</p>
<p>
<monospace>mcmcChain = BMLRmcmc(dataMat)</monospace>
</p>
<p>The posterior distribution is plotted using the command:</p>
<p>
<monospace>BMLRplot(mcmcChain)</monospace>
</p>
<p>The resulting plots can be saved as desired using the interactive menu in R, and the MCMC chain can then be saved, if desired, using R’s save command. The BMLRplot command also outputs a numerical summary of the posterior distribution. The comments in the program, BMLRexample.R, provide more details.</p>
<p>The program is packaged such that the user does not need to delve any deeper into the inner workings of the software. But, one of the benefits of the software is that models can be modified or created for various applications, using, for example, nonnormal distributions or hierarchical structures. Here, we briefly explain how multiple linear regression is implemented, as an illustration of the model specification language. The key to understanding the model specification is that every arrow in the hierarchical diagram of <xref ref-type="fig" rid="fig1-1094428112457829">Figure 1</xref> has a corresponding statement in the model specification. For example, the lowest arrow in <xref ref-type="fig" rid="fig1-1094428112457829">Figure 1</xref>, which points to the data from a normal distribution, is stated in the model specification as follows:</p>
<p>
<monospace>y[i] ∼ dnorm(y.hat[i], tau)</monospace>
</p>
<p>The programming code says that <italic>y<sub>i</sub>
</italic> is distributed as a normal density with mean <inline-formula id="inline-formula10-1094428112457829">
<mml:math id="mml-inline10-1094428112457829">
<mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub>
</mml:math>
</inline-formula> and “precision” τ, where precision is the reciprocal of variance. (Bayesian specifications of normal distributions often use precision instead of standard deviation for historical reasons.) The next arrow up in <xref ref-type="fig" rid="fig1-1094428112457829">Figure 1</xref> indicates that the predicted value, <inline-formula id="inline-formula11-1094428112457829">
<mml:math id="mml-inline11-1094428112457829">
<mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo accent="true" stretchy="false">ˆ</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub>
</mml:math>
</inline-formula>, is a linear combination of the predictors. This is expressed in the model specification as follows:</p>
<p>
<monospace>y.hat[i] &lt;- b0 + inprod(b[1:nPred], x[i,1:nPred])</monospace>
</p>
<p>where “inprod” denotes the inner product of the vector of regression coefficients and the vector of predictors. The complete model specification is as follows:</p>
<p>
<monospace>     model {</monospace>
</p>
<p>
<monospace>       # Likelihood:</monospace>
</p>
<p>
<monospace>       for(i in 1:N) {</monospace>
</p>
<p>
<monospace>        y[i] ∼ dnorm(y.hat[i], tau)</monospace>
</p>
<p>
<monospace>        y.hat[i] &lt;- b0 + inprod(b[1:nPred], x[i,1:nPred])</monospace>
</p>
<p>
<monospace>       }</monospace>
</p>
<p>
<monospace>       # Prior (assumes standardized data):</monospace>
</p>
<p>
<monospace>       tau &lt;- 1/pow(sigma,2)</monospace>
</p>
<p>
<monospace>       sigma ∼ dunif(0, 10)</monospace>
</p>
<p>
<monospace>       b0 ∼ dnorm(0, 1.0E-2)</monospace>
</p>
<p>
<monospace>       for (j in 1:nPred) {</monospace>
</p>
<p>
<monospace>        b[j] ∼ dnorm(0, 1.0E-2)</monospace>
</p>
<p>     }</p>
<p>    }</p>
<p>The model specification language can be modified to express a variety of other assumptions. For example, if the user wants to accommodate outliers in the data, then a <italic>t</italic> distribution can be used instead of a normal distribution. Polynomial trends or interaction terms can also be added to the prediction equation. Moreover, different prior assumptions can be implemented. For example, there might be strong prior knowledge about one of the regression coefficients, and this can be incorporated as well.</p>
<p>The model is built for software called JAGS (Just Another Gibbs Sampler; <xref ref-type="bibr" rid="bibr59-1094428112457829">Plummer, 2003</xref>, <xref ref-type="bibr" rid="bibr60-1094428112457829">2011</xref>), which is written in C++ and usable on most computer platforms. JAGS is an alternative to its predecessor, called BUGS, which stands for Bayesian inference Using Gibbs Sampling. BUGS has a stand-alone version for Windows called WinBUGS (<xref ref-type="bibr" rid="bibr47-1094428112457829">Lunn, Thomas, Best, &amp; Spiegelhalter, 2000</xref>), with a subsequent version called OpenBUGS (<xref ref-type="bibr" rid="bibr46-1094428112457829">Lunn, Spiegelhalter, Thomas, &amp; Best, 2009</xref>). The JAGS modeling language is virtually the same as BUGS. Because BUGS got an earlier start, it is presently more extensively used than JAGS, but JAGS is gaining in popularity because it can be used more easily on non-Windows operating systems and is more robust in operation. Both BUGS and JAGS can be accessed from R and other software environments. All the software is available free of charge.</p>
<p>After the model is specified, there are only four more steps in programming a Bayesian analysis: loading the data, specifying initial values for the MCMC chain, running the MCMC chain, and examining the results. An important aspect of using MCMC methods for representing a distribution is making sure that the random chain is truly representative of the distribution, which means that the chain has smoothly explored the entire posterior distribution and does not have lingering influence from its starting position. To prevent a randomly outlying starting point from influencing the results, we chose the starting point to be in the midst of credible parameter values (via least squares estimation), and the initial 1,000 steps of the chain are excluded from consideration. This is called the <italic>burn-in</italic> <italic>period</italic>. Thanks to the speed of modern personal computers, the chain can be run to great lengths so that a very large sample of representative points is collected. We use 250,000 steps for illustration, but more can be used in research applications as desired. The MCMC sample therefore is robustly representative of the true underlying posterior distribution. In typical applications of linear regression, the Markov chains are sufficiently well behaved that a run of 250,000 steps provides a precise representation of the posterior distribution. The program allows a sophisticated user to check for autocorrelation and convergence of chains in cases with strongly correlated predictors. This issue merely affects the efficiency of obtaining an MCMC representation of the posterior distribution, not the validity of Bayesian analysis.</p>
</sec>
</app>
</app-group>
<ack>
<title>Acknowledgment</title>
<p>We thank Jose M. Cortina and three <italic>Organizational Research Methods</italic> anonymous reviewers for highly constructive and detailed feedback that allowed us to improve our manuscript substantially.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn3-1094428112457829">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn2-1094428112457829">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Beaty</surname>
<given-names>J. C.</given-names>
</name>
<name>
<surname>Boik</surname>
<given-names>R. J.</given-names>
</name>
<name>
<surname>Pierce</surname>
<given-names>C. A.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Effect size and power in assessing moderating effects of categorical variables using multiple regression: A 30-year review</article-title>. <source>Journal of Applied Psychology</source>, <volume>90</volume>, <fpage>94</fpage>–<lpage>107</lpage>.</citation>
</ref>
<ref id="bibr2-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Culpepper</surname>
<given-names>S. A.</given-names>
</name>
<name>
<surname>Pierce</surname>
<given-names>C. A.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Revival of test bias research in preemployment testing</article-title>. <source>Journal of Applied Psychology</source>, <volume>95</volume>, <fpage>648</fpage>–<lpage>680</lpage>.</citation>
</ref>
<ref id="bibr3-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Harden</surname>
<given-names>E. E.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Cautionary note on conveniently dismissing χ<sup>2</sup> goodness-of-fit test results: Implications for strategic management research</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Bergh</surname>
<given-names>D. D.</given-names>
</name>
<name>
<surname>Ketchen</surname>
<given-names>D. J.</given-names>
</name>
</person-group> (Eds.), <source>Research methodology in strategy and management</source> (vol. 5, pp. <fpage>111</fpage>–<lpage>120</lpage>). <publisher-loc>Howard House, England</publisher-loc>: <publisher-name>Emerald Group Publishing</publisher-name>.</citation>
</ref>
<ref id="bibr4-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Pierce</surname>
<given-names>C. A.</given-names>
</name>
<name>
<surname>Bosco</surname>
<given-names>F. A.</given-names>
</name>
<name>
<surname>Muslin</surname>
<given-names>I. S.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>First decade of <italic>Organizational Research Methods</italic>: Trends in design, measurement, and data-analysis topics</article-title>. <source>Organizational Research Methods</source>, <volume>12</volume>, <fpage>69</fpage>–<lpage>112</lpage>.</citation>
</ref>
<ref id="bibr5-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Werner</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Abbott</surname>
<given-names>J. L.</given-names>
</name>
<name>
<surname>Angert</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Park</surname>
<given-names>J. H.</given-names>
</name>
<name>
<surname>Kohlhausen</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Customer-centric science: Reporting research results with rigor, relevance, and practical impact in mind</article-title>. <source>Organizational Research Methods</source>, <volume>13</volume>, <fpage>515</fpage>–<lpage>539</lpage>.</citation>
</ref>
<ref id="bibr6-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Allenby</surname>
<given-names>G. M.</given-names>
</name>
<name>
<surname>Bakken</surname>
<given-names>D. G.</given-names>
</name>
<name>
<surname>Rossi</surname>
<given-names>P. E.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>The hierarchical Bayesian revolution: How Bayesian methods have changed the face of marketing research</article-title>. <source>Marketing Research</source>, <volume>16</volume>, <fpage>20</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr7-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Anscombe</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>1954</year>). <article-title>Fixed sample size analysis of sequential observations</article-title>. <source>Biometrics</source>, <volume>10</volume>, <fpage>89</fpage>–<lpage>100</lpage>.</citation>
</ref>
<ref id="bibr8-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bayes</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Price</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>1763</year>). <article-title>An essay towards solving a problem in the doctrine of chances. By the late Rev. Mr. Bayes, F.R.S. Communicated by Mr. Price, in a letter to John Canton, A.M.F.R.S</article-title>. <source>Philosophical Transactions</source>, <volume>53</volume>, <fpage>370</fpage>–<lpage>418</lpage>. <comment>doi:10.1098/rstl.1763.0053</comment>
</citation>
</ref>
<ref id="bibr9-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Beaumont</surname>
<given-names>M. A.</given-names>
</name>
<name>
<surname>Rannala</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>The Bayesian revolution in genetics</article-title>. <source>Nature Reviews Genetics</source>, <volume>5</volume>, <fpage>251</fpage>–<lpage>261</lpage>.</citation>
</ref>
<ref id="bibr10-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berger</surname>
<given-names>J. O.</given-names>
</name>
<name>
<surname>Berry</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>1988</year>). <article-title>Statistical analysis and the illusion of objectivity</article-title>. <source>American Scientist</source>, <volume>76</volume>, <fpage>159</fpage>–<lpage>165</lpage>.</citation>
</ref>
<ref id="bibr11-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berry</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Bayesian clinical trials</article-title>. <source>Nature Reviews: Drug Discovery</source>, <volume>5</volume>, <fpage>27</fpage>–<lpage>36</lpage>. <comment>doi:10.1038/nrd1927</comment>
</citation>
</ref>
<ref id="bibr12-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brannick</surname>
<given-names>M. T.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Implications of empirical Bayes meta-analysis for test validation</article-title>. <source>Journal of Applied Psychology</source>, <volume>86</volume>, <fpage>468</fpage>–<lpage>480</lpage>.</citation>
</ref>
<ref id="bibr13-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brooks</surname>
<given-names>S. P.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Bayesian computation: A statistical revolution</article-title>. <source>Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences</source>, <volume>361</volume>, <fpage>2681</fpage>–<lpage>2697</lpage>.</citation>
</ref>
<ref id="bibr14-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Casciaro</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Lobo</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>When competence is irrelevant: The role of interpersonal affect in task-related ties</article-title>. <source>Administrative Science Quarterly</source>, <volume>53</volume>, <fpage>655</fpage>–<lpage>684</lpage>.</citation>
</ref>
<ref id="bibr15-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cashen</surname>
<given-names>L. H.</given-names>
</name>
<name>
<surname>Geiger</surname>
<given-names>S. W.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Statistical power and the testing of null hypotheses: A review of contemporary management research and recommendations for future studies</article-title>. <source>Organizational Research Methods</source>, <volume>7</volume>, <fpage>151</fpage>–<lpage>167</lpage>.</citation>
</ref>
<ref id="bibr16-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cohen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>The earth is round (p &lt; 0.05)</article-title>. <source>American Psychologist</source>, <volume>49</volume>, <fpage>997</fpage>–<lpage>1003</lpage>.</citation>
</ref>
<ref id="bibr17-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cornfield</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1966</year>). <article-title>A Bayesian test of some classical hypotheses, with applications to sequential clinical trials</article-title>. <source>Journal of the American Statistical Association</source>, <volume>61</volume>, <fpage>577</fpage>–<lpage>594</lpage>.</citation>
</ref>
<ref id="bibr18-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cortina</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Folger</surname>
<given-names>R. G.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>When is it acceptable to accept a null hypothesis: No way, Jose?</article-title> <source>Organizational Research Methods</source>, <volume>1</volume>, <fpage>334</fpage>–<lpage>350</lpage>.</citation>
</ref>
<ref id="bibr19-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cortina</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Landis</surname>
<given-names>R. S.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>When small effect sizes tell a big story, and when large effect sizes don’t</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Lance</surname>
<given-names>C. E.</given-names>
</name>
<name>
<surname>Vandenberg</surname>
<given-names>R. J.</given-names>
</name>
</person-group> (Eds.), <source>Statistical and methodological myths and urban legends: Doctrine, verity and fable in the organizational and social sciences</source> (pp. <fpage>287</fpage>–<lpage>308</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr20-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cortina</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Landis</surname>
<given-names>R. S.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>The earth is not round (p = .00)</article-title>. <source>Organizational Research Methods</source>, <volume>14</volume>, <fpage>332</fpage>–<lpage>349</lpage>.</citation>
</ref>
<ref id="bibr21-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Culpepper</surname>
<given-names>S. A.</given-names>
</name>
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>R is for revolution: A cutting-edge, free, open source statistical package</article-title>. <source>Organizational Research Methods</source>, <volume>14</volume>, <fpage>735</fpage>–<lpage>740</lpage>.</citation>
</ref>
<ref id="bibr22-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cumming</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Inference by eye: Pictures of confidence intervals and thinking about levels of confidence</article-title>. <source>Teaching Statistics</source>, <volume>29</volume>, <fpage>89</fpage>–<lpage>93</lpage>.</citation>
</ref>
<ref id="bibr23-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cumming</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Fidler</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Confidence intervals: Better answers to better questions</article-title>. <source>Zeitschrift fur Psychologie</source>, <volume>217</volume>, <fpage>15</fpage>–<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr24-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dienes</surname>
<given-names>Z.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Bayesian versus orthodox statistics: Which side are you on?</article-title> <source>Perspectives on Psychological Science</source>, <volume>6</volume>, <fpage>274</fpage>–<lpage>290</lpage>.</citation>
</ref>
<ref id="bibr25-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Doyle</surname>
<given-names>A. C.</given-names>
</name>
</person-group> (<year>1890</year>). <source>The sign of four</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Spencer Blackett</publisher-name>.</citation>
</ref>
<ref id="bibr26-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Edwards</surname>
<given-names>J. R.</given-names>
</name>
<name>
<surname>Berry</surname>
<given-names>J. W.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>The presence of something or the absence of nothing: Increasing theoretical precision in management research</article-title>. <source>Organizational Research Methods</source>, <volume>13</volume>, <fpage>668</fpage>–<lpage>689</lpage>.</citation>
</ref>
<ref id="bibr27-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gelman</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Objections to Bayesian statistics</article-title>. <source>Bayesian Analysis</source>, <volume>3</volume>, <fpage>445</fpage>–<lpage>450</lpage>.</citation>
</ref>
<ref id="bibr28-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gelman</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Hill</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2007</year>). <source>Data analysis using regression and multilevel/hierarchical models</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr29-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gelman</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Hill</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Yajima</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2009</year>). <source>Why we (usually) don't have to worry about multiple comparisons</source> <comment>(Technical report)</comment>. <publisher-loc>New York</publisher-loc>: <publisher-name>Department of Statistics, Columbia University</publisher-name>.</citation>
</ref>
<ref id="bibr30-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gerard</surname>
<given-names>P. D.</given-names>
</name>
<name>
<surname>Smith</surname>
<given-names>D. R.</given-names>
</name>
<name>
<surname>Weerakkody</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Limits of retrospective power analysis</article-title>. <source>Journal of Wildlife Management</source>, <volume>62</volume>, <fpage>801</fpage>–<lpage>807</lpage>.</citation>
</ref>
<ref id="bibr31-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gray</surname>
<given-names>P. H.</given-names>
</name>
<name>
<surname>Cooper</surname>
<given-names>W. H.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Pursuing failure</article-title>. <source>Organizational Research Methods</source>, <volume>13</volume>, <fpage>620</fpage>–<lpage>643</lpage>.</citation>
</ref>
<ref id="bibr32-1094428112457829">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Gregory</surname>
<given-names>P. C.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>A Bayesian revolution in spectral analysis</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Mohammad-Djafari</surname>
<given-names>A.</given-names>
</name>
</person-group> (Ed.), <conf-name>AIP conference proceedings</conf-name> (pp. <fpage>557</fpage>–<lpage>568</lpage>). <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr33-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Hays</surname>
<given-names>W. L.</given-names>
</name>
</person-group> (<year>1994</year>). <source>Statistics</source> (<edition>5th ed</edition>.). <publisher-loc>Fort Worth, TX</publisher-loc>: <publisher-name>Harcourt Brace</publisher-name>.</citation>
</ref>
<ref id="bibr34-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hirst</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Van Knippenberg</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Zhou</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>A cross-level perspective on employee creativity: Goal orientation, team learning behavior, and individual creativity</article-title>. <source>Academy of Management Journal</source>, <volume>52</volume>, <fpage>280</fpage>–<lpage>293</lpage>.</citation>
</ref>
<ref id="bibr35-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Huang</surname>
<given-names>J. L.</given-names>
</name>
<name>
<surname>Ryan</surname>
<given-names>A. M.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Beyond personality traits: A study of personality states and situational contingencies in customer service jobs</article-title>. <source>Personnel Psychology</source>, <volume>64</volume>, <fpage>451</fpage>–<lpage>488</lpage>.</citation>
</ref>
<ref id="bibr36-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Jackman</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2009</year>). <source>Bayesian analysis for the social sciences</source>. <publisher-loc>West Sussex, England</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr37-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>King</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>1986</year>). <article-title>How not to lie with statistics: Avoiding common mistakes in quantitative political science</article-title>. <source>American Journal of Political Science</source>, <volume>30</volume>, <fpage>666</fpage>–<lpage>687</lpage>.</citation>
</ref>
<ref id="bibr38-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kline</surname>
<given-names>R. B.</given-names>
</name>
</person-group> (<year>2004</year>). <source>Beyond significance testing: Reforming data analysis methods in behavioral research</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr39-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kruschke</surname>
<given-names>J. K.</given-names>
</name>
</person-group> (<year>2010a</year>). <article-title>Bayesian data analysis</article-title>. <source>Wiley Interdisciplinary Reviews: Cognitive Science</source>, <volume>1</volume>, <fpage>658</fpage>–<lpage>676</lpage>.</citation>
</ref>
<ref id="bibr40-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kruschke</surname>
<given-names>J. K.</given-names>
</name>
</person-group> (<year>2010b</year>). <article-title>What to believe: Bayesian methods for data analysis</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>14</volume>, <fpage>293</fpage>–<lpage>300</lpage>.</citation>
</ref>
<ref id="bibr41-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kruschke</surname>
<given-names>J. K.</given-names>
</name>
</person-group> (<year>2011a</year>). <source>Doing Bayesian data analysis: A tutorial with R and BUGS</source>. <publisher-loc>Burlington, MA</publisher-loc>: <publisher-name>Academic Press/Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr42-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kruschke</surname>
<given-names>J. K.</given-names>
</name>
</person-group> (<year>2011b, August 1</year>). <article-title>Extrasensory perception (ESP): Bayesian estimation approach to meta-analysis [Blog post]</article-title>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://doingbayesiandataanalysis.blogspot.com/2011/08/extrasensory-perception-esp-bayesian.html">http://doingbayesiandataanalysis.blogspot.com/2011/08/extrasensory-perception-esp-bayesian.html</ext-link>
</citation>
</ref>
<ref id="bibr43-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kruschke</surname>
<given-names>J. K.</given-names>
</name>
</person-group> (<year>In press</year>). <article-title>Bayesian estimation supersedes the <italic>t</italic> test</article-title>. <source>Journal of Experimental Psychology: General</source>.</citation>
</ref>
<ref id="bibr44-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lance</surname>
<given-names>C. E.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>More statistical and methodological myths and urban legends</article-title>. <source>Organizational Research Methods</source>, <volume>14</volume>, <fpage>279</fpage>–<lpage>286</lpage>.</citation>
</ref>
<ref id="bibr45-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leavitt</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Mitchell</surname>
<given-names>T. R.</given-names>
</name>
<name>
<surname>Peterson</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Theory pruning: Strategies to reduce our dense theoretical landscape</article-title>. <source>Organizational Research Methods</source>, <volume>13</volume>, <fpage>644</fpage>–<lpage>667</lpage>.</citation>
</ref>
<ref id="bibr46-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lunn</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Spiegelhalter</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Thomas</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Best</surname>
<given-names>N.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>The BUGS project: Evolution, critique and future directions (with discussion)</article-title>. <source>Statistics in Medicine</source>, <volume>28</volume>, <fpage>3049</fpage>–<lpage>3082</lpage>.</citation>
</ref>
<ref id="bibr47-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lunn</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Thomas</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Best</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Spiegelhalter</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>WinBUGS—A Bayesian modelling framework: Concepts, structure, and extensibility</article-title>. <source>Statistics and Computing</source>, <volume>10</volume>, <fpage>325</fpage>–<lpage>337</lpage>.</citation>
</ref>
<ref id="bibr48-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lykou</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Ntzoufras</surname>
<given-names>I.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>WinBUGS: A tutorial</article-title>. <source>Wiley Interdisciplinary Reviews: Computational Statistics</source>, <volume>3</volume>, <fpage>385</fpage>–<lpage>396</lpage>.</citation>
</ref>
<ref id="bibr49-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Maxwell</surname>
<given-names>S. E.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>The persistence of underpowered studies in psychological research: Causes, consequences, and remedies</article-title>. <source>Psychological Methods</source>, <volume>9</volume>, <fpage>147</fpage>–<lpage>163</lpage>.</citation>
</ref>
<ref id="bibr50-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Maxwell</surname>
<given-names>S. E.</given-names>
</name>
<name>
<surname>Delaney</surname>
<given-names>H. D.</given-names>
</name>
</person-group> (<year>2004</year>). <source>Designing experiments and analyzing data: A model comparison perspective</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr51-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>McCloskey</surname>
<given-names>D. N.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>The insignificance of statistical significance</article-title>. <source>Scientific American</source>, <volume>272</volume>, <fpage>104</fpage>–<lpage>105</lpage>.</citation>
</ref>
<ref id="bibr52-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Miller</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>What is the probability of replicating a statistically significant effect?</article-title> <source>Psychonomic Bulletin &amp; Review</source>, <volume>16</volume>, <fpage>617</fpage>–<lpage>640</lpage>.</citation>
</ref>
<ref id="bibr53-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mundfrom</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Perrett</surname>
<given-names>J. J.</given-names>
</name>
<name>
<surname>Schaffer</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Piccone</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Roozeboom</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Bonferroni adjustments in tests for regression coefficients</article-title>. <source>Multiple Linear Regression Viewpoints</source>, <volume>32</volume>, <fpage>1</fpage>–<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr54-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Newman</surname>
<given-names>M. E. J.</given-names>
</name>
<name>
<surname>Barabasi</surname>
<given-names>A. L.</given-names>
</name>
<name>
<surname>Watts</surname>
<given-names>D. J.</given-names>
</name>
</person-group> (<year>2006</year>). <source>The structure and function of dynamic networks</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr55-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nickerson</surname>
<given-names>R. S.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Null hypothesis significance testing: A review of an old and continuing controversy</article-title>. <source>Psychological Methods</source>, <volume>5</volume>, <fpage>241</fpage>–<lpage>301</lpage>.</citation>
</ref>
<ref id="bibr56-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>O’Boyle</surname>
<given-names>E. H.</given-names>
</name>
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>The best and the rest: Revisiting the norm of normality of individual performance</article-title>. <source>Personnel Psychology</source>, <volume>65</volume>, <fpage>79</fpage>–<lpage>119</lpage>.</citation>
</ref>
<ref id="bibr57-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Orlitzky</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>How can significance tests be deinstitutionalized?</article-title> <source>Organizational Research Methods</source>, <volume>15</volume>, <fpage>199</fpage>–<lpage>228</lpage>.</citation>
</ref>
<ref id="bibr58-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pierce</surname>
<given-names>J. R.</given-names>
</name>
<name>
<surname>Aguinis</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>in press</year>). <article-title>The too-much-of-a-good-thing effect in management</article-title>. <source>Journal of Management</source>.</citation>
</ref>
<ref id="bibr59-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Plummer</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Hornik</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Leisch</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Zeileis</surname>
<given-names>A.</given-names>
</name>
</person-group> (Eds.), <source>Proceedings of the 3ssrd International Workshop on Distributed Statistical Computing, March 20-22</source>. <publisher-loc>Vienna, Austria</publisher-loc>: <publisher-name>DSC</publisher-name>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/">http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Proceedings/</ext-link>
</citation>
</ref>
<ref id="bibr60-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Plummer</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>rjags: Bayesian graphical models using MCMC</article-title>. <comment>R package version 3-5 [Computer software]. Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=rjags">http://CRAN.R-project.org/package=rjags</ext-link>
</citation>
</ref>
<ref id="bibr61-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Poole</surname>
<given-names>C.</given-names>
</name>
</person-group> (<year>1987</year>). <article-title>Beyond the confidence interval</article-title>. <source>American Journal of Public Health</source>, <volume>77</volume>, <fpage>195</fpage>–<lpage>199</lpage>.</citation>
</ref>
<ref id="bibr62-1094428112457829">
<citation citation-type="book">
<collab collab-type="author">R Development Core Team</collab>. (<year>2012</year>). <source>R: A language and environment for statistical computing</source>. <publisher-loc>Vienna, Austria</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link>
</citation>
</ref>
<ref id="bibr63-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rogers</surname>
<given-names>J. L.</given-names>
</name>
<name>
<surname>Howard</surname>
<given-names>K. I.</given-names>
</name>
<name>
<surname>Vessey</surname>
<given-names>J. T.</given-names>
</name>
</person-group> (<year>1993</year>). <article-title>Using significance tests to evaluate equivalence between two experimental groups</article-title>. <source>Psychological Bulletin</source>, <volume>113</volume>, <fpage>553</fpage>–<lpage>565</lpage>.</citation>
</ref>
<ref id="bibr64-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Roth</surname>
<given-names>P. L.</given-names>
</name>
<name>
<surname>Switzer</surname>
<given-names>F. S.</given-names>
</name>
<name>
<surname>Van Iddekinge</surname>
<given-names>C. H.</given-names>
</name>
<name>
<surname>Oh</surname>
<given-names>I. S.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Toward better meta-analytic input matrices: How matrix values change conclusions in human resource management simulations</article-title>. <source>Personnel Psychology</source>, <volume>64</volume>, <fpage>899</fpage>–<lpage>935</lpage>.</citation>
</ref>
<ref id="bibr65-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sasovova</surname>
<given-names>Z.</given-names>
</name>
<name>
<surname>Mehra</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Borgatti</surname>
<given-names>S. P.</given-names>
</name>
<name>
<surname>Schippers</surname>
<given-names>M. C.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Network churn: The effects of self-monitoring personality on brokerage dynamics</article-title>. <source>Administrative Science Quarterly</source>, <volume>55</volume>, <fpage>639</fpage>–<lpage>670</lpage>.</citation>
</ref>
<ref id="bibr66-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scandura</surname>
<given-names>T. A.</given-names>
</name>
<name>
<surname>Williams</surname>
<given-names>E. A.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Research methodology in management: Current practices, trends, and implications for future research</article-title>. <source>Academy of Management Journal</source>, <volume>43</volume>, <fpage>1248</fpage>–<lpage>1264</lpage>.</citation>
</ref>
<ref id="bibr67-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scherbaum</surname>
<given-names>C. A.</given-names>
</name>
<name>
<surname>Ferreter</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Estimating statistical power and required sample sizes for organizational research using multilevel modeling</article-title>. <source>Organizational Research Methods</source>, <volume>12</volume>, <fpage>347</fpage>–<lpage>367</lpage>.</citation>
</ref>
<ref id="bibr68-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schmidt</surname>
<given-names>F. L.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Meta-analysis: A constantly evolving research integration tool</article-title>. <source>Organizational Research Methods</source>, <volume>11</volume>, <fpage>96</fpage>–<lpage>113</lpage>.</citation>
</ref>
<ref id="bibr69-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schmidt</surname>
<given-names>F. L.</given-names>
</name>
<name>
<surname>Raju</surname>
<given-names>N. S.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Updating meta-analytic research findings: Bayesian approaches versus the medical model</article-title>. <source>Journal of Applied Psychology</source>, <volume>92</volume>, <fpage>297</fpage>–<lpage>308</lpage>.</citation>
</ref>
<ref id="bibr70-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schweder</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Hjort</surname>
<given-names>N. L.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Confidence and likelihood</article-title>. <source>Scandinavian Journal of Statistics</source>, <volume>29</volume>, <fpage>309</fpage>–<lpage>332</lpage>.</citation>
</ref>
<ref id="bibr71-1094428112457829">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Singh</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Xie</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Strawderman</surname>
<given-names>W. E.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Confidence distribution (CD)—distribution estimator of a parameter</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Liu</surname>
<given-names>R. Y.</given-names>
</name>
<name>
<surname>Strawderman</surname>
<given-names>W. E.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>C.-H.</given-names>
</name>
</person-group> (Eds.), <source>Complex datasets and inverse problems</source> (Vol. 54, pp. <fpage>132</fpage>–<lpage>150</lpage>). <publisher-loc>Beachwood, OH</publisher-loc>: <publisher-name>Institute of Mathematical Statistics</publisher-name>.</citation>
</ref>
<ref id="bibr72-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Steiger</surname>
<given-names>J. H.</given-names>
</name>
<name>
<surname>Fouladi</surname>
<given-names>R. T.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>R2: A computer program for interval estimation, power calculations, sample size estimation, and hypothesis testing in multiple regression</article-title>. <source>Behavior Research Methods</source>, <volume>24</volume>, <fpage>581</fpage>–<lpage>582</lpage>.</citation>
</ref>
<ref id="bibr73-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sullivan</surname>
<given-names>K. M.</given-names>
</name>
<name>
<surname>Foster</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>1990</year>). <article-title>Use of the confidence interval function</article-title>. <source>Epidemiology</source>, <volume>1</volume>, <fpage>39</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr74-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Thomas</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Retrospective power analysis</article-title>. <source>Conservation Biology</source>, <volume>11</volume>, <fpage>276</fpage>–<lpage>280</lpage>.</citation>
</ref>
<ref id="bibr75-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wagenmakers</surname>
<given-names>E. J.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A practical solution to the pervasive problems of p values</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>14</volume>, <fpage>779</fpage>–<lpage>804</lpage>.</citation>
</ref>
<ref id="bibr76-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Western</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Jackman</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Bayesian inference for comparative research</article-title>. <source>American Political Science Review</source>, <volume>88</volume>, <fpage>412</fpage>–<lpage>423</lpage>.</citation>
</ref>
<ref id="bibr77-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Westlake</surname>
<given-names>W. J.</given-names>
</name>
</person-group> (<year>1976</year>). <article-title>Symmetrical confidence intervals for bioequivalence trials</article-title>. <source>Biometrics</source>, <volume>32</volume>, <fpage>741</fpage>–<lpage>744</lpage>.</citation>
</ref>
<ref id="bibr78-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Westlake</surname>
<given-names>W. J.</given-names>
</name>
</person-group> (<year>1981</year>). <article-title>Response to bioequivalence testing—a need to rethink</article-title>. <source>Biometrics</source>, <volume>37</volume>, <fpage>591</fpage>–<lpage>593</lpage>.</citation>
</ref>
<ref id="bibr79-1094428112457829">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhou</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>When the presence of creative coworkers is related to creativity: Role of supervisor close monitoring, developmental feedback, and creative personality</article-title>. <source>Journal of Applied Psychology</source>, 
<volume>88</volume>, <fpage>413</fpage>–<lpage>422</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>