<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">FMX</journal-id>
<journal-id journal-id-type="hwp">spfmx</journal-id>
<journal-title>Field Methods</journal-title>
<issn pub-type="ppub">1525-822X</issn>
<issn pub-type="epub">1552-3969</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1525822X11419104</article-id>
<article-id pub-id-type="publisher-id">10.1177_1525822X11419104</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Web versus Mail</article-title>
<subtitle>The Influence of Survey Distribution Mode on Employees’ Response</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Saunders</surname>
<given-names>Mark N. K.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1525822X11419104">1</xref>
<xref ref-type="corresp" rid="corresp1-1525822X11419104"/>
</contrib>
</contrib-group>
<aff id="aff1-1525822X11419104"><label>1</label> University of Surrey Business School, Guildford, Surrey, United Kingdom</aff>
<author-notes>
<corresp id="corresp1-1525822X11419104">Mark N. K. Saunders, School of Management, University of Surrey, Guildford, Surrey GU2 7XH, United Kingdom Email: <email>Mark.Saunders@surrey.ac.uk</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>24</volume>
<issue>1</issue>
<fpage>56</fpage>
<lpage>73</lpage>
<permissions>
<copyright-statement>© SAGE Publications 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Despite organizations’ widespread Internet use and ready availability of Web survey software, few studies have examined explicitly the impact on employees’ responses of using the Web as opposed to mail-based surveys (MBS). This article addresses this gap using an employee attitude survey distributed to a 50% systematic sample of 3,338 employees by mail, the remaining employees receiving the survey via a Web link. Although the return rate for the Web (49.1%) was higher than for mail (33.5%), the quality of Web returns was reduced by a higher number of partial responses and abandonments. Taking into account effect size, significant differences in response were small other than for open question content. Recommendations regarding use of Web-based surveys (WBS) are offered and areas for future research suggested.</p>
</abstract>
<kwd-group>
<kwd>survey distribution</kwd>
<kwd>mail survey</kwd>
<kwd>Web survey</kwd>
<kwd>response rate</kwd>
<kwd>organizational survey</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1525822X11419104">
<title>Introduction</title>
<p>Within quantitative empirical studies in organizations, the survey is acknowledged widely as the most frequently used data collection tool (<xref ref-type="bibr" rid="bibr2-1525822X11419104">Baruch and Holtom 2008</xref>). Survey design texts highlight a clear trend to the use of self-completed questionnaires (<xref ref-type="bibr" rid="bibr10-1525822X11419104">Dillman 2009</xref>; <xref ref-type="bibr" rid="bibr15-1525822X11419104">Kraut 2006</xref>), alongside increasing popularity of Web-based surveys (WBS; <xref ref-type="bibr" rid="bibr13-1525822X11419104">Hoonakker and Carayon 2009</xref>; <xref ref-type="bibr" rid="bibr24-1525822X11419104">Thompson and Surface 2007</xref>). Yet, despite rapid growth in organizational Internet use, few recent studies have compared overall and partial response rates, and content of responses for WBS and mail-based surveys (MBS), even fewer considering organizations not at the forefront of implementing Web technology or including all employees (<xref ref-type="bibr" rid="bibr23-1525822X11419104">Shih and Fan 2008</xref>).</p>
<p>For those considering using WBS in organizations, this raises important questions about the extent to which response rates and content now differ between WBS and MBS. Over 80% of organizations in the United States and the European Union now have broadband connection (<xref ref-type="bibr" rid="bibr5-1525822X11419104">Commission of the European Communities 2009</xref>; <xref ref-type="bibr" rid="bibr27-1525822X11419104">U.S. Census Bureau 2009</xref>). Use of the Internet by employees at work has increased rapidly, negating problems of employee IT literacy and coverage indicated in earlier research (<xref ref-type="bibr" rid="bibr6-1525822X11419104">Converse et al. 2008</xref>; <xref ref-type="bibr" rid="bibr23-1525822X11419104">Shih and Fan 2008</xref>). In combination with readily available inexpensive online survey software, WBS may therefore now offer the majority of organizations an alternative to MBS.</p>
<p>This study explores the influence of distribution mode, comparing WBS and MBS responses from employees of a large U.K. public sector organization who are IT literate and who use the Internet in their everyday work. The research furthers understanding of whether WBS now impacts as supporting technology and what its use is in organizations near maturity, providing the first direct comparison with MBS in this environment. I begin with an overview of existing research on WBS and MBS response. A collaborative field experiment comparing a WBS and MBS distributed to an organization’s employees is then outlined and differences in response explored. I conclude with a discussion incorporating recommendations regarding use of WBS and suggestions for future research.</p>
</sec>
<sec id="section2-1525822X11419104">
<title>Existing Research on WBS and MBS Response</title>
<p>Assessment of survey response is critical to establishing whether respondents are representative of the population and thus the validity of findings. Where they differ from nonrespondents, this may distort responses and data collected may be biased (<xref ref-type="bibr" rid="bibr8-1525822X11419104">Couper et al. 1999</xref>). Low response rates increase the likelihood of statistical biases (<xref ref-type="bibr" rid="bibr26-1525822X11419104">Tomaskovic-Devey et al. 1994</xref>), which, where samples are small, are less likely to be detected (<xref ref-type="bibr" rid="bibr22-1525822X11419104">Rogelberg and Stanton 2007</xref>). While any rate of nonresponse can result in bias (<xref ref-type="bibr" rid="bibr12-1525822X11419104">Groves 2006</xref>), there is general consensus that higher response rates lead to a higher probability of the sample being representative (<xref ref-type="bibr" rid="bibr2-1525822X11419104">Baruch and Holtom 2008</xref>).</p>
<sec id="section3-1525822X11419104">
<title>Response Rate and Partial Nonresponse</title>
<p>Research reveals considerable variation in survey response rates. Analysis of 463 academic studies, published in 17 refereed management and behavioral sciences journals in 2000 and 2005, reported mean rates for Web (38.9%) as lower than mail (44.7%), based on a sample of 6 WBS and 309 MBS (<xref ref-type="bibr" rid="bibr2-1525822X11419104">Baruch and Holtom 2008</xref>). A meta-analysis of 39 studies published in 1998–2006, directly comparing WBS and MBS returns for samples drawn from the same population, supports such differences, reporting lower mean response rates for WBS (34%) than MBS (45%; <xref ref-type="bibr" rid="bibr23-1525822X11419104">Shih and Fan 2008</xref>). In contrast, another analysis, including seven WBS and MBS comparative studies published in 1998–2005, records similar mean rates for WBS (50.5%) and MBS (52.4%; <xref ref-type="bibr" rid="bibr13-1525822X11419104">Hoonakker and Carayon 2009</xref>). However, much of this research was undertaken before ready availability of online survey software and widespread use of the Internet. Few (5) study samples were drawn from employees, the majority (31) using IT-literate college or professional populations. Samples recording higher WBS response rates were generally from populations where Internet use was widespread, suggesting literacy increases response. Research questions for this study are therefore constrained to IT-literate employees using the Internet in their everyday work, the first being:</p>
<p>1. To what extent do employee’ response rates now differ between WBS and MBS?</p>
<p>Survey quality is affected by nonresponse. Complete nonresponse occurs when respondents fail to return the survey, while partial response and abandonment occur if a partially completed survey is returned (American Association for Public Opinion Research <xref ref-type="bibr" rid="bibr1-1525822X11419104">[AAPOR] 2008</xref>). Invariably, it is impossible to distinguish partial response and abandonment from complete nonresponse if, after partial completion, a questionnaire is not returned. However, returning a WBS is less onerous, meaning partially completed questionnaires are more likely to be returned (<xref ref-type="bibr" rid="bibr9-1525822X11419104">Crawford et al. 2001</xref>).</p>
<p>For WBS, passive nonresponse is likely to be due to failure to notice the survey or computer problems, the former being most likely to occur where invitations, especially if delivered from the same sender as other “all user” e-mails, are considered organizational spam and deleted (<xref ref-type="bibr" rid="bibr14-1525822X11419104">Kaplowitz et al. 2004</xref>). Concerns about Internet security, in particular that responses can be traced, may result in active nonresponse (<xref ref-type="bibr" rid="bibr17-1525822X11419104">Manfreda and Vehovar 2008</xref>). Respondents will have beliefs regarding how data are likely to be used (<xref ref-type="bibr" rid="bibr22-1525822X11419104">Rogelberg and Stanton 2007</xref>). Where they do not trust organizational authorities, they may perceive repercussions and decline to respond or answer certain questions. While respondents may be willing to answer “crucial” questions relating to the survey purpose (<xref ref-type="bibr" rid="bibr1-1525822X11419104">AAPOR 2008</xref>), they may not answer “demographic” questions if they believe these will enable identification, thereby reducing data quality. Consequently, the second research question is:</p>
<p>2. To what extent do employees’ nonresponse for crucial and for demographic questions differ between WBS and MBS?</p>
</sec>
<sec id="section4-1525822X11419104">
<title>Differences in Response Content</title>
<p>In considering responses, a serious concern is that of nonresponse bias, when nonrespondents differ from respondents in ways that are relevant to the subject being studied (<xref ref-type="bibr" rid="bibr24-1525822X11419104">Thompson and Surface 2007</xref>). Those refusing explicitly to take part in organizational surveys are likely to be less committed to their organizations, less satisfied with their supervisors, and more likely to quit (<xref ref-type="bibr" rid="bibr21-1525822X11419104">Rogelberg et al. 2000</xref>). While the use of WBS rather than MBS may introduce response bias to questions related to technology adoption, online respondents having more developed IT skill sets (<xref ref-type="bibr" rid="bibr7-1525822X11419104">Couper 2000</xref>), it is unclear whether differences will occur regarding other topics. The third research question therefore asks:</p>
<p>3. To what extent do employees’ responses to crucial questions differ between WBS and MBS?</p>
<p>Research reveals that responses to open (write in) questions can be twice as long for WBS as MBS (<xref ref-type="bibr" rid="bibr16-1525822X11419104">Kulesa and Bishop 2006</xref>). The content of such responses appears linked to respondents’ attitudes. In a WBS of military employees, dissatisfied respondents were more likely to answer open questions than those who were satisfied, their responses usually echoing their negative tone (<xref ref-type="bibr" rid="bibr19-1525822X11419104">Poncheri et al. 2008</xref>). This study highlighted a need for research considering factors that may affect the likelihood and tone of such comments. Hence, the final research question is:</p>
<p>4. To what extent do rates, length, and content of open question responses differ between WBS and MBS?</p>
</sec>
</sec>
<sec id="section5-1525822X11419104">
<title>Method</title>
<sec id="section6-1525822X11419104">
<title>Participants</title>
<p>Data were collected as part of an ongoing triennial employee survey in collaboration with a large U.K. public sector organization responsible for the provision of schools and school transport, caring services, roads, libraries, and strategic planning to a largely rural English county. Within this organization, employee surveys were used sparingly, WBS being used rarely. Consequently, oversurveying was not considered a problem.</p>
<p>The survey was designed to gain a clear understanding of employees’ attitudes to the organization, being distributed to all 5,457 employees (excluding school-based staff). In all, 61% of employees had an individual work e-mail account, using this and the Internet as part of their daily duties, the remaining (predominantly manual) employees did not use IT in their work. The subset of 3,338 IT-literate employees, each of whom had their own workstation, form the population. They were distributed across six directorates; the two largest (Adult and Community Services and Children’s Services) comprising 67% of the research population and 72% of all employees. In all, 83% of the research population was female (85% of all employees) and 17% male (15% of all employees). These proportions did not differ between the two samples selected.</p>
</sec>
<sec id="section7-1525822X11419104">
<title>Design</title>
<p>The survey contained 121 Likert-style questions about employees’ attitudes, comprising scales measuring employee commitment (<xref ref-type="bibr" rid="bibr18-1525822X11419104">Meyer and Allen 1997</xref>), perceived organizational support (<xref ref-type="bibr" rid="bibr11-1525822X11419104">Eisenberger et al. 1986</xref>), trust in organizational authorities (<xref ref-type="bibr" rid="bibr3-1525822X11419104">Brockner et al. 1997</xref>), and questions developed for the organization. Grouped in 10 sections under brief descriptive headings, these were “crucial” questions. An additional section contained 10 closed “demographic” questions requesting information including directorate, gender, length of service, and broad salary grade. The final open question provided an option to comment on areas or issues of concern. There were no filter questions.</p>
</sec>
<sec id="section8-1525822X11419104">
<title>Materials</title>
<p>Two versions of the survey were developed and pilot tested. The WBS, designed using the online software SurveyMonkey, was as close to an exact copy of the mail version as possible. Features that could not be replicated using paper, for example, forced response and response validation, were not used. Visual appearance was as similar as possible, allowing for differences between screen and paper. These included: pale shading of alternate Likert-style questions (WBS), clickable response circles (WBS) or response tick boxes (MBS) for closed questions, unlimited text comment box (WBS) or blank half page with a request to continue on separate sheet (MBS) for the open question, and separate sections with scrolled text (WBS), or page breaks (MBS). When completing the WBS, respondents clicked closed question responses, word processing open question answers, clicking “next” to move to the following section. Negative impact on response from scrolling was minimized by dividing the survey into 11 sections, scrolling being required for 3 sections (<xref ref-type="bibr" rid="bibr25-1525822X11419104">Toepoel et al. 2009</xref>).</p>
<p>Respondents could amend responses until the survey was done, final responses and date of submission being recorded automatically. Alternatively, they could “exit this survey” and resume later, these and subsequent responses being recorded only if they clicked “done.” One respondent per work station was allowed, replicating mailing of a paper survey and helping prevent multiple completions. MBS closed question responses were indicated by ticking appropriate boxes, answers to the open question being handwritten. This version was returned in a postage prepaid envelope and date of receipt recorded.</p>
</sec>
<sec id="section9-1525822X11419104">
<title>Procedure</title>
<p>The MBS was distributed to 50% (1,669) of employees selected as a systematic random sample using a sampling frame extracted from the organization’s HR database. The frame was stratified by directorate and employees listed in salary order; ensuring employees were selected from all directorates across all job roles. After removing unreachable noncontacts due to incorrect addresses for 15 home-based employees, the sample consisted of 1,654 employees. The remaining 50% of employees received a Web link to the WBS via e-mail. E-mail addresses were selected individually and the Web link was e-mailed from a named person in the HR department, there being no noncontacts. Both WBS and MBS versions took approximately 20 minutes to complete.</p>
<p>
<xref ref-type="bibr" rid="bibr10-1525822X11419104">Dillman’s (2009)</xref> tailored design method was used within constraints imposed by the organization and the experimental design. General information regarding the forthcoming survey was provided via the staff intranet, the normal method for such communications. Subsequently, each employee received four personal contacts in addition to the survey. The pre-survey notification letter, jointly from the organization’s chief executive and me, was delivered in the same manner as the potential respondent would receive their survey. Assurances of anonymity and my independence were provided, it being stated that some employees would receive the survey by a Web link and others by mail. The WBS or MBS was delivered to the respective samples with a cover e-mail/letter. One week later, all employees received a personal follow-up/reminder designed as an information sheet, reemphasizing the deadline for returns at the end of that week. This did not offer a replacement paper questionnaire or restate the Web link. Two further general reminders (after the deadline for returns) were posted on the staff intranet.</p>
</sec>
<sec id="section10-1525822X11419104">
<title>Data Coding and Transformation</title>
<p>WBS responses to closed questions were coded automatically by SurveyMonkey and imported into statistical package for the social sciences (SPSS). MBS responses to these questions were entered manually using the same codes. WBS and MBS responses to the open question were respectively imported or typed into Microsoft Office verbatim, retaining spelling mistakes, typographical errors, and abbreviations; data sets were linked by respondent identifiers.</p>
<p>To address research question 4, open-ended question responses were transformed. Following <xref ref-type="bibr" rid="bibr19-1525822X11419104">Poncheri et al. (2008)</xref>, I created three variables in SPSS. The first indicated whether a comment had been made. The second recorded comment length in words; this was calculated using Microsoft Office’s word count tool. Finally, the overall tone of each comment was coded independently by me and a colleague as positive, negative, or neutral. Where comments were mixed, the most prevalent tone was coded. Interrater reliability was good (κ = .74), .75 being considered excellent (<xref ref-type="bibr" rid="bibr28-1525822X11419104">Von Eye and Mun 2005</xref>), 91.7% of ratings being identical. Where there were rating disagreements, our understandings were debated, clarified, and final codes agreed.</p>
<p>Comments were prepared for content analysis as an additional variable. Spelling was corrected, hyphens removed, and names were made anonymous. Comments and associated survey responses were imported into the WordStat text analysis module of Simstat (<xref ref-type="bibr" rid="bibr20-1525822X11419104">Provalis Research 2005</xref>) and processed using the WordStat features lemmatization and exclusion. Lemmatization reduced inflected forms of words automatically to a more limited number of canonical forms, for example, “staffing” to “staff.” Reductions were reviewed manually, unsuitable substitutions such as “new” for “news” being corrected. The exclusion dictionary was used to remove words with little semantic value such as pronouns and conjunctions.</p>
</sec>
</sec>
<sec id="section11-1525822X11419104">
<title>Results</title>
<sec id="section12-1525822X11419104">
<title>Response Rates and Partial Nonresponse</title>
<p>The first research question considered the extent response rates differed between WBS and MBS. The proportion of total returns differed significantly between the WBS (49.1%) and MBS (33.5%), <italic>z</italic>(<italic>N</italic> = 3,323) = 9.12, <italic>p</italic> &lt; .001 (two-tailed), WBS recipients being 1.46 times more likely to respond (<xref ref-type="table" rid="table1-1525822X11419104">Table 1</xref>
). Although WBS median return speeds were faster than MBS, the mean return speed for WBS was slower, indicating a positive skew. WBS responses were 1.16 times more likely to be received by the due return date, this difference being significant, <italic>z</italic>(<italic>N</italic> =1,374) = 4.72, <italic>p</italic> &lt; .001 (two-tailed).</p>
<table-wrap id="table1-1525822X11419104" position="float">
<label>Table 1.</label>
<caption>
<p>Returns Analysis</p>
</caption>
<graphic alternate-form-of="table1-1525822X11419104" xlink:href="10.1177_1525822X11419104-table1.tif"/>
<table>
<thead>
<tr>
<th/>
<th align="center" colspan="3">Response</th>
<th align="center" colspan="4">Return Speed (Days)</th>
</tr>
<tr>
<th>Distribution</th>
<th><italic>N</italic></th>
<th><italic>n</italic></th>
<th>%</th>
<th><italic>M</italic></th>
<th>Mdn</th>
<th>Q1</th>
<th>Q3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mail</td>
<td>1,654</td>
<td>554</td>
<td>33.5</td>
<td>9.25</td>
<td>8</td>
<td>5</td>
<td>11</td>
</tr>
<tr>
<td>Web</td>
<td>1,669</td>
<td>820</td>
<td>49.1</td>
<td>9.63</td>
<td>3</td>
<td>2</td>
<td>17</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1525822X11419104">
<p>Note: <italic>N</italic> = total population; <italic>n</italic> = number of surveys returned; <italic>M</italic> = mean; Mdn = median; Q1 = lower quartile; Q2 = upper quartile.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Respondents were classified into two broad groups using salary grade. No significant association was found between distribution mode and whether a respondent was employed on a professional/managerial grade or more junior administrative/technical grade, χ<sup>2</sup>(2, <italic>N</italic> = 1,202) = .31, <italic>p</italic> = .580. Professional and managerial employees consisted of 41.8% of the total population; 43.1% of WBS and 44.7% of MBS respondents were drawn from this group.</p>
<p>Returns grouped using the AAPOR’s (2008) suggested standards for complete response, partial response, and abandonment were associated significantly with distribution mode, χ<sup>2</sup>(2, <italic>N</italic> = 1,374) = 69.06, <italic>p</italic> &lt; .001, <italic>V</italic> = .159 (<xref ref-type="table" rid="table2-1525822X11419104">Table 2</xref>
). Although complete responses were more likely to be associated with MBS while partial responses, particularly abandonments, were more likely to be associated with WBS, Cramer’s <italic>V</italic> indicated the effect size was small (<xref ref-type="bibr" rid="bibr4-1525822X11419104">Cohen 1988</xref>).</p>
<table-wrap id="table2-1525822X11419104" position="float">
<label>Table 2.</label>
<caption>
<p>Completeness of Response</p>
</caption>
<graphic alternate-form-of="table2-1525822X11419104" xlink:href="10.1177_1525822X11419104-table2.tif"/>
<table>
<thead>
<tr>
<th rowspan="2">
</th>
<th rowspan="2">
</th>
<th rowspan="2">
<italic>
</italic>
</th>
<th>Complete</th>
<th>Partial</th>
<th colspan="2">Abandonment</th>
</tr>
<tr>
<th>(&gt;80%–100%)</th>
<th>(50%–80%)</th>
<th>(&lt;50%)<sup><xref ref-type="table-fn" rid="table-fn3-1525822X11419104">a</xref></sup>
</th>
<th>(0%)</th>
</tr>
<tr>
<th>Distribution</th>
<th rowspan="1">Question Type</th>
<th rowspan="1">
<italic>n</italic>
</th>
<th>%</th>
<th>%</th>
<th>%</th>
<th>%</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="3">Mail</td>
<td>Crucial</td>
<td>554</td>
<td>99.1</td>
<td>0.9</td>
<td align="center">–</td>
<td align="center">–</td>
</tr>
<tr>
<td>Demographic</td>
<td>554</td>
<td>95.7</td>
<td>4.3</td>
<td align="center">–</td>
<td align="center">–</td>
</tr>
<tr>
<td>All</td>
<td>554</td>
<td>99.1</td>
<td>0.9</td>
<td align="center">–</td>
<td align="center">–</td>
</tr>
<tr>
<td rowspan="3">Web</td>
<td>Crucial</td>
<td>820</td>
<td>86.6</td>
<td>5.5</td>
<td>7.9</td>
<td align="center">–</td>
</tr>
<tr>
<td>Demographic</td>
<td>820</td>
<td>85.9</td>
<td>1.6</td>
<td>12.6</td>
<td>12.4</td>
</tr>
<tr>
<td>All</td>
<td>820</td>
<td>86.6</td>
<td>4.5</td>
<td>8.9</td>
<td>0.5</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1525822X11419104">
<p>Note: <italic>n</italic> = number of surveys returned.</p>
</fn>
<fn id="table-fn3-1525822X11419104">
<p>
<sup>a</sup>Includes responses in 0% column.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The second research question considered the extent nonresponse differed for crucial and demographic questions. Complete and partial responses and abandonments for crucial questions were significantly associated with distribution mode, χ<sup>2</sup>(2, <italic>N</italic> = 1,374) = 68.66, <italic>p</italic> &lt; .001, <italic>V</italic> = .158, complete responses being more likely to be associated with MBS and partial responses and abandonments with WBS (<xref ref-type="table" rid="table2-1525822X11419104">Table 2</xref>), although Cramer’s <italic>V</italic> indicated the effect size was small. Complete and partial responses and abandonments for demographic questions were also significantly associated with distribution mode, χ<sup>2</sup>(2, <italic>N</italic> = 1,374) = 82.39, <italic>p</italic> &lt; .001, <italic>V</italic> = .173; Cramer’s <italic>V</italic> again indicating the effect size was small. However, unlike with crucial questions, 12.4% of WBS returns included no responses for demographic questions (<xref ref-type="table" rid="table2-1525822X11419104">Table 2</xref>). Independent samples <italic>t</italic>-tests using <xref ref-type="bibr" rid="bibr3-1525822X11419104">Brockner et al.’s (1997)</xref> 3-item scale indicated that there were no significant differences in employees’ trust in organizational authorities between complete and incomplete (partial and abandonment) responses for either crucial questions, <italic>t</italic>(1,232) = −.993, <italic>p</italic> = .321(two-tailed), or demographic questions, <italic>t</italic>(1,224) = −.227, <italic>p</italic> = .821 (two-tailed). Incomplete WBS responses, therefore, seem unlikely to be due to respondents’ low trust in the organizational authorities, perhaps being related to concerns about Internet security outlined earlier.</p>
</sec>
<sec id="section13-1525822X11419104">
<title>Response Content</title>
<p>Research question 3 considered whether employees’ crucial question responses differed between distribution modes. Data have been collected on employees’ affective (6 items), continuance (7 items), and normative commitment (6 items), using <xref ref-type="bibr" rid="bibr18-1525822X11419104">Meyer and Allen’s (1997)</xref> revised scales; their perceived organizational support using <xref ref-type="bibr" rid="bibr11-1525822X11419104">Eisenberger et al.’s (1986)</xref> short version (16 items) scale, and, as outlined earlier, trust in organizational authorities. Reliability analyses for these scales produced values similar to the original studies (<xref ref-type="table" rid="table3-1525822X11419104">Table 3</xref>
). Independent sample <italic>t</italic>-tests indicated that employees’ responses to all but one scale differed significantly between distribution modes. WBS respondents were more likely to have stronger affective and normative commitment, perceive stronger support from the organization, and be more trusting of organizational authorities (<xref ref-type="table" rid="table3-1525822X11419104">Table 3</xref>). However, the effect sizes of these differences were small, suggesting that although respondents to the WBS were more trusting, more emotionally attached (i.e., feeling more obligated to stay), and more likely to perceive strong organizational support, the practical implications were likely to be limited.</p>
<table-wrap id="table3-1525822X11419104" position="float">
<label>Table 3.</label>
<caption>
<p>Difference in Response by Scale</p>
</caption>
<graphic alternate-form-of="table3-1525822X11419104" xlink:href="10.1177_1525822X11419104-table3.tif"/>
<table>
<thead>
<tr>
<th>Scale</th>
<th>Distribution</th>
<th>
<italic>n</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
<th>
<italic>t</italic>
</th>
<th>
<italic>df</italic>
</th>
<th>
<italic>p</italic>
</th>
<th>
<italic>d</italic>
</th>
<th>α</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="2">Trust in organizational authorities</td>
<td>Mail</td>
<td>546</td>
<td>13.2</td>
<td>4.3</td>
<td>−3.60</td>
<td>1302</td>
<td>.000</td>
<td>.200</td>
<td>.76</td>
</tr>
<tr>
<td>Web</td>
<td>758</td>
<td>14.1</td>
<td>4.0</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td rowspan="2">Affective commitment</td>
<td>Mail</td>
<td>544</td>
<td>23.6</td>
<td>8.2</td>
<td>−2.00</td>
<td>1291</td>
<td>.046</td>
<td>.111</td>
<td>.87</td>
</tr>
<tr>
<td>Web</td>
<td>749</td>
<td>24.5</td>
<td>7.7</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td rowspan="2">Continuance commitment</td>
<td>Mail</td>
<td>543</td>
<td>28.1</td>
<td>9.2</td>
<td>−.29</td>
<td>1276</td>
<td>.771</td>
<td>.016</td>
<td>.85</td>
</tr>
<tr>
<td>Web</td>
<td>735</td>
<td>28.2</td>
<td>9.0</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td rowspan="2">Normative commitment</td>
<td>Mail</td>
<td>543</td>
<td>20.3</td>
<td>7.9</td>
<td>−3.15</td>
<td>1265</td>
<td>.002</td>
<td>.177</td>
<td>.86</td>
</tr>
<tr>
<td>Web</td>
<td>724</td>
<td>21.7</td>
<td>7.6</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td rowspan="2">Perceived organizational support</td>
<td>Mail</td>
<td>530</td>
<td>62.2</td>
<td>18.3</td>
<td>−2.91</td>
<td>1203</td>
<td>.004</td>
<td>.168</td>
<td>.95</td>
</tr>
<tr>
<td>Web</td>
<td>675</td>
<td>65.2</td>
<td>17.2</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-1525822X11419104">
<p>Note: <italic>n</italic> = number of surveys returned; <italic>M</italic> = mean; <italic>SD</italic> = standard deviation; <italic>t</italic> = <italic>t</italic> value; <italic>df</italic> = degrees of freedom; <italic>p</italic> = probability; <italic>d</italic> = Cohen’s <italic>d</italic>; α = Cronbach’s alpha.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The final research question considered differences in rate, length, and content of open question responses. About one-fifth (19.2%) of respondents answered the open question, there being no significant association between the distribution mode and whether a comment was provided, χ<sup>2</sup>(1, <italic>N</italic> = 1,374) = 1.08, <italic>p</italic> = .229. The majority of responses were negative (81.1%), reflecting the question’s request for comments on aspects or issues of concern (<xref ref-type="table" rid="table4-1525822X11419104">Table 4</xref>
), there being no significant association between mode and whether tone was negative, χ<sup>2</sup>(1, <italic>N</italic> = 264) = 1.11, <italic>p</italic> = .292). Length of response differed significantly, mean length for WBS being twice that for MBS, <italic>t</italic>(262) = −5.32, <italic>p</italic> &lt; .001 (two-sided) <italic>d</italic> = .657, Cohen’s <italic>d</italic> indicating the effect size of this was medium. Despite this, content analysis of “processed” comments revealed few significant differences in words used by WBS and MBS respondents. About 35.6% of WBS and 40.3% of MBS comment words had semantic value and, while this difference is significant, the phi statistic indicates it is trivial, χ<sup>2</sup>(1, <italic>N =</italic> 20,174) = 31.48, <italic>p</italic> &lt; .001, φ = .002.</p>
<table-wrap id="table4-1525822X11419104" position="float">
<label>Table 4.</label>
<caption>
<p>Open Ended Question Comment Tone and Length</p>
</caption>
<graphic alternate-form-of="table4-1525822X11419104" xlink:href="10.1177_1525822X11419104-table4.tif"/>
<table>
<thead>
<tr>
<th rowspan="3">
</th>
<th colspan="8">“If there are any other areas or issues that concern you, please feel free to comment below”</th>
</tr>
<tr>
<th colspan="3">Tone</th>
<th colspan="3">Response</th>
<th colspan="2">Length</th>
</tr>
<tr>
<th>Positive (+1)</th>
<th>Neutral (0)</th>
<th>Negative (−1)</th>
<th rowspan="1">
<italic>
</italic>
</th>
<th rowspan="1">
<italic>
</italic>
</th>
<th rowspan="1">
</th>
<th colspan="2">(Words)</th>
</tr>
<tr>
<th rowspan="1">Distribution</th>
<th>%</th>
<th>%</th>
<th>%</th>
<th rowspan="1">
<italic>n</italic>
</th>
<th rowspan="1">
<italic>f</italic>
</th>
<th rowspan="1">%</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mail</td>
<td>2.0</td>
<td>20.2</td>
<td>77.8</td>
<td>554</td>
<td>99</td>
<td>17.9</td>
<td>41.3</td>
<td>45.2</td>
</tr>
<tr>
<td>Web</td>
<td>2.4</td>
<td>14.6</td>
<td>83.0</td>
<td>820</td>
<td>165</td>
<td>20.1</td>
<td>98.1</td>
<td>100.1</td>
</tr>
<tr>
<td>Combined</td>
<td>2.3</td>
<td>16.7</td>
<td>81.1</td>
<td>1374</td>
<td>264</td>
<td>19.2</td>
<td>76.8</td>
<td>88.1</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-1525822X11419104">
<p>Note: Positive, neutral, and negative percentage is proportion of total responses to this question. Percentages may not sum to 100 due to rounding.</p>
</fn>
<fn id="table-fn6-1525822X11419104">
<p>
<italic>n</italic> = number of surveys returned; <italic>f</italic> = frequency of response; <italic>M</italic> = mean; <italic>SD</italic> = standard deviation.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Occurrence of individual words of semantic value only differed significantly once for words in at least 10% of responses (<xref ref-type="table" rid="table5-1525822X11419104">Table 5</xref>
). Although the word “staff” was significantly associated with mode, being more likely to be used one or more times by WBS respondents, the phi statistic indicates the effect size is small, χ<sup>2</sup>(1, <italic>N</italic> = 264) = 6.68, <italic>p</italic> = .010, φ = .159. Significant associations between distribution mode and the frequency of individual words of semantic value were more numerous, those words reflecting areas of concern being repeated in the longer WBS responses. Seven such words were significantly associated with distribution mode, occurring more frequently in WBS responses: “feel,” χ<sup>2</sup>(1, <italic>N =</italic> 127) = 10.44, <italic>p</italic> = .001, φ = .287; “staff,” χ<sup>2</sup>(1, <italic>N</italic> = 177) = 20.80, <italic>p &lt;</italic> .001, φ = .343; “management,” χ<sup>2</sup>(1, <italic>N =</italic> 86) = 8.71, <italic>p</italic> = .003, φ = .318; “people,” χ<sup>2</sup>(1, <italic>N</italic> = 73) = 7.56, <italic>p</italic> = .006, φ = .322; “team,” χ<sup>2</sup>(1, <italic>N</italic> = 74) = 6.61, <italic>p</italic> = .010, φ = .300; “time,” χ<sup>2</sup>(1, <italic>N</italic> = 67) = 9.36, <italic>p</italic> = .002, φ = .374; and “decision,” χ<sup>2</sup>(1, <italic>N</italic> = 51) = 10.35, <italic>p</italic> = .001, φ = .451. For all but the first of these, the phi statistic indicates the effect size is medium, suggesting their increased use would be clearly discernable.</p>
<table-wrap id="table5-1525822X11419104" position="float">
<label>Table 5.</label>
<caption>
<p>Content Analysis of Open Question Responses</p>
</caption>
<graphic alternate-form-of="table5-1525822X11419104" xlink:href="10.1177_1525822X11419104-table5.tif"/>
<table>
<thead>
<tr>
<th rowspan="3">
</th>
<th align="center" colspan="6">Occurrence by</th>
</tr>
<tr>
<th colspan="3">Respondent</th>
<th colspan="3">Frequency of Use</th>
</tr>
<tr>
<th>Mail (<italic>n</italic> = 99)</th>
<th>Web (<italic>n</italic> = 165)</th>
<th>All (<italic>n</italic> = 264)</th>
<th>Mail (<italic>n</italic> = 99)</th>
<th>Web (<italic>n</italic> = 165)</th>
<th>All (<italic>n</italic> = 264)</th>
</tr>
<tr>
<th rowspan="1">Word occurring ≥10% responses</th>
<th>%</th>
<th>%</th>
<th>%</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>M</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>Council</td>
<td>32.3</td>
<td>34.5</td>
<td>33.7</td>
<td>1.3</td>
<td>1.4</td>
<td>1.4</td>
</tr>
<tr>
<td>County</td>
<td>31.3</td>
<td>34.5</td>
<td>33.3</td>
<td>1.5</td>
<td>1.4</td>
<td>1.5</td>
</tr>
<tr>
<td>Feel</td>
<td>24.2</td>
<td>35.8</td>
<td>31.4</td>
<td>1.3</td>
<td>1.6</td>
<td>1.5</td>
</tr>
<tr>
<td>Staff</td>
<td>21.2</td>
<td>36.4</td>
<td>30.7</td>
<td>1.8</td>
<td>2.3</td>
<td>2.2</td>
</tr>
<tr>
<td>Manager</td>
<td>22.2</td>
<td>29.1</td>
<td>26.5</td>
<td>1.5</td>
<td>1.7</td>
<td>1.6</td>
</tr>
<tr>
<td>Shirecounty</td>
<td>27.3</td>
<td>24.2</td>
<td>25.4</td>
<td>1.3</td>
<td>1.4</td>
<td>1.3</td>
</tr>
<tr>
<td>Management</td>
<td>16.2</td>
<td>26.7</td>
<td>22.7</td>
<td>1.2</td>
<td>1.5</td>
<td>1.4</td>
</tr>
<tr>
<td>Job</td>
<td>14.1</td>
<td>24.8</td>
<td>20.8</td>
<td>1.7</td>
<td>1.7</td>
<td>1.7</td>
</tr>
<tr>
<td>Service</td>
<td>17.2</td>
<td>22.4</td>
<td>20.5</td>
<td>1.9</td>
<td>2.2</td>
<td>2.1</td>
</tr>
<tr>
<td>People</td>
<td>15.2</td>
<td>23.0</td>
<td>20.1</td>
<td>1.1</td>
<td>1.5</td>
<td>1.4</td>
</tr>
<tr>
<td>Question</td>
<td>12.1</td>
<td>21.2</td>
<td>17.8</td>
<td>1.1</td>
<td>1.2</td>
<td>1.2</td>
</tr>
<tr>
<td>Team</td>
<td>13.1</td>
<td>18.8</td>
<td>16.7</td>
<td>1.3</td>
<td>1.8</td>
<td>1.7</td>
</tr>
<tr>
<td>Time</td>
<td>10.1</td>
<td>19.4</td>
<td>15.9</td>
<td>1.3</td>
<td>1.7</td>
<td>1.6</td>
</tr>
<tr>
<td>Decision</td>
<td>8.1</td>
<td>17.0</td>
<td>13.6</td>
<td>1.0</td>
<td>1.5</td>
<td>1.4</td>
</tr>
<tr>
<td>Difficult</td>
<td>12.1</td>
<td>13.9</td>
<td>13.3</td>
<td>1.0</td>
<td>1.3</td>
<td>1.2</td>
</tr>
<tr>
<td>Lack</td>
<td>7.1</td>
<td>15.8</td>
<td>12.5</td>
<td>1.6</td>
<td>1.2</td>
<td>1.3</td>
</tr>
<tr>
<td>Park</td>
<td>13.1</td>
<td>10.9</td>
<td>11.7</td>
<td>1.5</td>
<td>1.9</td>
<td>1.7</td>
</tr>
<tr>
<td>Survey</td>
<td>7.1</td>
<td>14.5</td>
<td>11.7</td>
<td>1.1</td>
<td>1.3</td>
<td>1.2</td>
</tr>
<tr>
<td>Car</td>
<td>10.1</td>
<td>10.9</td>
<td>10.6</td>
<td>1.1</td>
<td>1.9</td>
<td>1.6</td>
</tr>
<tr>
<td>Issue</td>
<td>8.1</td>
<td>12.1</td>
<td>10.6</td>
<td>1.3</td>
<td>1.5</td>
<td>1.4</td>
</tr>
<tr>
<td>Level</td>
<td>8.1</td>
<td>12.1</td>
<td>10.6</td>
<td>1.3</td>
<td>1.2</td>
<td>1.2</td>
</tr>
<tr>
<td>Answer</td>
<td>6.1</td>
<td>12.7</td>
<td>10.2</td>
<td>1.0</td>
<td>1.1</td>
<td>1.1</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-1525822X11419104">
<p>Note: <italic>n</italic> = number of surveys returned; <italic>M</italic> = mean.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section14-1525822X11419104">
<title>Discussion</title>
<p>In contrast to previous research, this study found mostly small significant or insignificant differences in response between WBS and MBS for a survey distributed to IT-literate employees. These findings are important, indicating quality issues associated with lower response may now be less prevalent for WBS. Earlier research had generally indicated that, other than in organizations at the forefront of implementing Web technology, using WBS would result in lower response rates (<xref ref-type="bibr" rid="bibr2-1525822X11419104">Baruch and Holtom 2008</xref>; <xref ref-type="bibr" rid="bibr23-1525822X11419104">Shih and Fan 2008</xref>). However, this study illustrates this may no longer be the case. Using WBS resulted in significantly greater returns than mail, even after discounting partial returns and abandonments, indicating clear willingness to respond for all nonmanual IT-literate employees.</p>
<p>Response rates for crucial and demographic questions differed significantly between modes; levels of nonresponse being higher for WBS, over 12% of these not answering any demographic questions. Although this was not related to their trust in the organization, actual reasons are unclear and further research is needed; a possible explanation is concerns about Internet security. Notwithstanding issues associated with assessing potential response bias using demographic variables (<xref ref-type="bibr" rid="bibr7-1525822X11419104">Couper 2000</xref>; <xref ref-type="bibr" rid="bibr22-1525822X11419104">Rogelberg and Stanton 2007</xref>) and the small effect size, such nonresponse is likely to be problematic where such assessment relies on benchmarking respondents’ demographic characteristics.</p>
<p>Differences in closed question responses between the two distribution modes were, although significant, generally small. While this finding relates only to responses associated with organizational commitment, perceptions of organizational support, and trust in organizational authorities, it suggests that for such employee attitudes, only small differences in response bias are likely to be attributable to distribution mode. For other topics, particularly those related to distribution mode, using WBS may still lead to response bias (<xref ref-type="bibr" rid="bibr7-1525822X11419104">Couper 2000</xref>).</p>
<p>Mean response length for the open question, as suggested by <xref ref-type="bibr" rid="bibr16-1525822X11419104">Kulesa and Bishop (2006)</xref>, differed significantly, WBS responses being twice as long. While no associations were found between distribution mode and comment tone, words of semantic value (representing concerns) were significantly more likely to be repeated in WBS responses and as a consequence more prominent. Although this is not surprising, given differences in length, where WBS are offered along with MBS alternatives (<xref ref-type="bibr" rid="bibr10-1525822X11419104">Dillman 2009</xref>), there is a need to ensure interpretation does not give unwarranted prominence to WBS responses.</p>
<sec id="section15-1525822X11419104">
<title>Recommendations Regarding the Use of Web Distribution</title>
<p>These findings relate only to IT-literate employees in an organization, so we must be careful about generalizing them across different settings. Overall survey response, although low (41.3%), is not too dissimilar from <xref ref-type="bibr" rid="bibr2-1525822X11419104">Baruch and Holtom’s (2008)</xref> mean rate of 45.8% for similar studies in non-U.S. journals. Despite this, the findings indicate that as employees’ use of Internet becomes more ubiquitous, differences in impact on survey response between WBS and MBS are changing. Based on these, the following recommendations are offered concerning the use of WBS within organizations where employees are IT literate using the Internet as part of their work.</p>
<p>1. WBS should be considered in organizations where potential respondents are IT literate and have ready access to the Internet in their work.</p>
<p>Rates of partial response, in particular abandonment for demographic questions, prompt a second recommendation:</p>
<p>2. WBS design should enable nonresponse bias to be assessed.</p>
<p>High levels of complete nonresponse for demographic questions mean assessment of nonresponse bias will be problematic if the widely adopted approach of benchmarking demographic data collected by the survey with the population is used. Nonresponse may be reduced if, unlike in this research, respondents are compelled to answer using the forced response online survey software feature. Care will still be needed to ensure questions preserve respondents’ anonymity, where this has been offered. In addition, respondents’ concerns about privacy, their beliefs about how the data may be used, or frustration due to compulsion may still lead to partial response or abandonment (<xref ref-type="bibr" rid="bibr17-1525822X11419104">Manfreda and Vehovar 2008</xref>).</p>
<p>My third recommendation relates to the impact of WBS on closed question responses. While differences in responses between WBS and MBS were significant, the effect of these was small for the topics asked. However, as highlighted earlier, WBS may influence response where this is related to the question subject. The third recommendation is therefore:</p>
<p>3. Caution should be exercised regarding including questions on topics directly related to WBS or associated technologies, unless mechanisms to assess response bias can be incorporated.</p>
<p>In such situations, techniques for assessing bias requiring additional data about respondents or nonrespondents are unlikely to be feasible. However, other techniques such as triangulating findings using a different set of methods may be possible (<xref ref-type="bibr" rid="bibr22-1525822X11419104">Rogelberg and Stanton 2007</xref>).</p>
<p>Despite rapid growth in the use of information technology, not all employees will have ready access to IT or the Internet. Following the previous recommendation, my fourth also concerns analysis of responses where WBS are used alongside other survey modes:</p>
<p>4. Care should be taken if aggregating WBS question responses on topics related directly to the distribution mode or associated technologies with those from surveys using other modes.</p>
<p>The study highlighted that, although similar in content, responses to open questions are likely to differ considerably in length, repetition for longer WBS responses giving words greater prominence. My final recommendation is therefore:</p>
<p>5. Care should be taken if aggregating WBS open question responses with those from surveys using other methods, being mindful of the impact of the length of response and repetition on interpretations.</p>
</sec>
<sec id="section16-1525822X11419104">
<title>Concluding Comments</title>
<p>Inevitably, care must be taken in generalizing these findings across different organizational settings where employees are IT literate and have ready access to IT and the Internet, particularly where employees have different job roles. Despite the low response rate, this direct comparison has provided new contrary insights regarding the likely impact on response of using Web rather than mail distribution. These have allowed recommendations to be made regarding the use of WBS. Studies cited indicate that, as more organizations adopt broadband technology and the growth of employees’ use of the Internet reaches maturity, WBS are likely to be used more widely. It is therefore pertinent to conclude with suggestions for future research in the use of WBS.</p>
<p>While this research has indicated WBS and MBS can impact the nature of responses to attitude questions, the underlying reasons are not clear. Further experimental work is also needed to explore differences between WBS and other distribution modes and understand associated reasons. Research reviewed provides useful insights into acceptable responses rates for surveys for different distribution modes and in different settings. However, comparative response data for WBS in organizational and other settings are still sparse. Further research is therefore needed to establish up-to-date benchmark response rates for WBS in different industry settings. Within these benchmarking studies, attention needs to be paid to the use of online survey software features to enhance response and their impact on response (<xref ref-type="bibr" rid="bibr10-1525822X11419104">Dillman 2009</xref>).</p>
</sec>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict" id="fn1-1525822X11419104">
<p>The author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn2-1525822X11419104">
<p>The author received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1525822X11419104">
<citation citation-type="book">
<collab collab-type="author">American Association for Public Opinion Research (AAPOR)</collab>. <year>2008</year>. <source>Standard definitions: Final dispositions of case codes and outcome rates for surveys</source>. (<edition>5th ed</edition>). <publisher-loc>Lenexa, KS</publisher-loc>: <publisher-name>AAPOR</publisher-name>.</citation>
</ref>
<ref id="bibr2-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Baruch</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>Holtom</surname>
<given-names>B. C.</given-names>
</name>
</person-group> <year>2008</year>. <article-title>Survey response rate levels and trends in organizational research</article-title>. <source>Human Relations</source>, <volume>61</volume>, <fpage>1139</fpage>–<lpage>60</lpage>.</citation>
</ref>
<ref id="bibr3-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brockner</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Siegel</surname>
<given-names>P. A.</given-names>
</name>
<name>
<surname>Daly</surname>
<given-names>J. R.</given-names>
</name>
<name>
<surname>Tyler</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Martin</surname>
<given-names>C.</given-names>
</name>
</person-group> <year>1997</year>. <article-title>When trust matters: The moderating effect of outcome favourability</article-title>. <source>Administrative Science Quarterly</source>, <volume>42</volume>, <fpage>558</fpage>–<lpage>83</lpage>.</citation>
</ref>
<ref id="bibr4-1525822X11419104">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cohen</surname>
<given-names>J.</given-names>
</name>
</person-group> <year>1988</year>. <source>Statistical power analysis for the behavioral sciences</source>. (<edition>2nd ed</edition>). <publisher-loc>Hillside, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr5-1525822X11419104">
<citation citation-type="web">
<collab collab-type="author">Commission of the European Communities</collab>. <year>2009</year>. <article-title>Europe’s digital competitiveness report: Volume 1: i2010—Annual Information Society Report 2009</article-title>. <ext-link ext-link-type="uri" xlink:href="http://ec.europa.eu/information_society/eeurope/i2010/docs/annual_report/2009/sec_2009_1103.pdf">http://ec.europa.eu/information_society/eeurope/i2010/docs/annual_report/2009/sec_2009_1103.pdf</ext-link> <comment>(accessed August 24, 2010)</comment>.</citation>
</ref>
<ref id="bibr6-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Converse</surname>
<given-names>P. D.</given-names>
</name>
<name>
<surname>Wolfe</surname>
<given-names>E. W.</given-names>
</name>
<name>
<surname>Huang</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Oswald</surname>
<given-names>F. L.</given-names>
</name>
</person-group> <year>2008</year>. <article-title>Response rates for mixed-mode surveys using mail and e-mail/web</article-title>. <source>American Journal of Evaluation</source>, <volume>29</volume>, <fpage>99</fpage>–<lpage>107</lpage>.</citation>
</ref>
<ref id="bibr7-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Couper</surname>
<given-names>M. P.</given-names>
</name>
</person-group> <year>2000</year>. <article-title>Web surveys: A review of issues and approaches</article-title>. <source>Public Opinion Quarterly</source>, <volume>64</volume>, <fpage>464</fpage>–<lpage>94</lpage>.</citation>
</ref>
<ref id="bibr8-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Couper</surname>
<given-names>M. P.</given-names>
</name>
<name>
<surname>Blair</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Triplett</surname>
<given-names>T.</given-names>
</name>
</person-group> <year>1999</year>. <article-title>A comparison of mail and e-mail for a survey of employees in federal statistical agencies</article-title>. <source>Journal of Official Statistics</source>, <volume>15</volume>, <fpage>39</fpage>–<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr9-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crawford</surname>
<given-names>S. D.</given-names>
</name>
<name>
<surname>Couper</surname>
<given-names>M. P.</given-names>
</name>
<name>
<surname>Lamias</surname>
<given-names>M. J.</given-names>
</name>
</person-group> <year>2001</year>. <article-title>Web surveys: Perceptions of burden</article-title>. <source>Social Science Computer Review</source>, <volume>19</volume>, <fpage>146</fpage>–<lpage>62</lpage>.</citation>
</ref>
<ref id="bibr10-1525822X11419104">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Dillman</surname>
<given-names>D. A.</given-names>
</name>
</person-group> <year>2009</year>. <source>Internet, mail and mixed mode surveys: The tailored design method</source>. (<edition>3rd ed</edition>.). <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr11-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Eisenberger</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Huntington</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Hutchinson</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Sowa</surname>
<given-names>D.</given-names>
</name>
</person-group> <year>1986</year>. <article-title>Perceived organizational support</article-title>. <source>Journal of Applied Psychology</source>, <volume>71</volume>, <fpage>500</fpage>–<lpage>7</lpage>.</citation>
</ref>
<ref id="bibr12-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Groves</surname>
<given-names>R. M.</given-names>
</name>
</person-group> <year>2006</year>. <article-title>Non-response rates and non-response bias in household surveys</article-title>. <source>Public Opinion Quarterly</source>, <volume>70</volume>, <fpage>646</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr13-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hoonakker</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Carayon</surname>
<given-names>P.</given-names>
</name>
</person-group> <year>2009</year>. <article-title>Questionnaire survey nonresponse: A comparison of postal mail and Internet surveys</article-title>. <source>International Journal of Human-Computer Interaction</source>, <volume>25</volume>, <fpage>348</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr14-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kaplowitz</surname>
<given-names>M. D.</given-names>
</name>
<name>
<surname>Hadlock</surname>
<given-names>T. D.</given-names>
</name>
<name>
<surname>Levine</surname>
<given-names>R.</given-names>
</name>
</person-group> <year>2004</year>. <article-title>A comparison of web and mail survey response rates</article-title>. <source>Public Opinion Quarterly</source>, <volume>68</volume>, <fpage>94</fpage>–<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr15-1525822X11419104">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kraut</surname>
<given-names>A. I.</given-names>
</name>
</person-group> <year>2006</year>. <source>Getting action from organizational surveys: New concepts, technologies and applications</source>. <publisher-loc>San Francisco</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr16-1525822X11419104">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kulesa</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Bishop</surname>
<given-names>R. J.</given-names>
</name>
</person-group> <year>2006</year>. <article-title>What did they mean? New and emerging methods for analyzing open-ended comments</article-title>. In <source>Getting action from organizational surveys: New concepts, technologies and applications</source>, eds. <person-group person-group-type="editor">
<name>
<surname>Kraut</surname>
<given-names>A. I.</given-names>
</name>
</person-group> <fpage>238</fpage>–<lpage>63</lpage>. <publisher-loc>San Francisco</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr17-1525822X11419104">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Manfreda</surname>
<given-names>K. L.</given-names>
</name>
<name>
<surname>Vehovar</surname>
<given-names>V.</given-names>
</name>
</person-group> <year>2008</year>. <article-title>Internet surveys</article-title>. In <source>International handbook of survey methodology</source>, eds. <person-group person-group-type="editor">
<name>
<surname>de Leeuw</surname>
<given-names>E. D.</given-names>
</name>
<name>
<surname>Hox</surname>
<given-names>J. J.</given-names>
</name>
<name>
<surname>Dillman</surname>
<given-names>D. A.</given-names>
</name>
</person-group> <fpage>264</fpage>–<lpage>84</lpage>. <publisher-loc>New York</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr18-1525822X11419104">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Meyer</surname>
<given-names>J. P.</given-names>
</name>
<name>
<surname>Allen</surname>
<given-names>W. T.</given-names>
</name>
</person-group> <year>1997</year>. <source>Commitment in the workplace: Theory research and application</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr19-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Poncheri</surname>
<given-names>R. M.</given-names>
</name>
<name>
<surname>Lindberg</surname>
<given-names>J. T.</given-names>
</name>
<name>
<surname>Thompson</surname>
<given-names>L. F.</given-names>
</name>
<name>
<surname>Surface</surname>
<given-names>E. A.</given-names>
</name>
</person-group> <year>2008</year>. <article-title>A comment on employee surveys: Negative bias in open responses</article-title>. <source>Organizational Research Methods</source>, <volume>11</volume>, <fpage>614</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr20-1525822X11419104">
<citation citation-type="web">
<collab collab-type="author">Provalis Research</collab>. <year>2005</year>. <article-title>WordStat Content Analysis Module for SIMSTAT and QDA Miner</article-title>. <ext-link ext-link-type="uri" xlink:href="http://www.provalisresearch.com/Download/Manuals.html">http://www.provalisresearch.com/Download/Manuals.html</ext-link> <comment>(accessed August 26, 2010)</comment>.</citation>
</ref>
<ref id="bibr21-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rogelberg</surname>
<given-names>S. G.</given-names>
</name>
<name>
<surname>Luong</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Sederberg</surname>
<given-names>M. E.</given-names>
</name>
<name>
<surname>Cristol</surname>
<given-names>D. S.</given-names>
</name>
</person-group> <year>2000</year>. <article-title>Employee attitude surveys: Examining the attitudes of noncompliant employees</article-title>. <source>Journal of Applied Psychology</source>, <volume>85</volume>, <fpage>284</fpage>–<lpage>93</lpage>.</citation>
</ref>
<ref id="bibr22-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rogelberg</surname>
<given-names>S. G.</given-names>
</name>
<name>
<surname>Stanton</surname>
<given-names>J. M.</given-names>
</name>
</person-group> <year>2007</year>. <article-title>Introduction: Understanding and dealing with organizational survey non-response</article-title>. <source>Organizational Research Methods</source>, <volume>10</volume>, <fpage>195</fpage>–<lpage>209</lpage>.</citation>
</ref>
<ref id="bibr23-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shih</surname>
<given-names>T. H.</given-names>
</name>
<name>
<surname>Fan</surname>
<given-names>X.</given-names>
</name>
</person-group> <year>2008</year>. <article-title>Comparing response rates from web and mail surveys: A meta-analysis</article-title>. <source>Field Methods</source>, <volume>20</volume>, <fpage>249</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr24-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Thompson</surname>
<given-names>L. F.</given-names>
</name>
<name>
<surname>Surface</surname>
<given-names>E. A.</given-names>
</name>
</person-group> <year>2007</year>. <article-title>Employee surveys administered online: attitudes towards the medium, non-response, and data representativeness</article-title>. <source>Organizational Research Methods</source>, <volume>10</volume>, <fpage>241</fpage>–<lpage>61</lpage>.</citation>
</ref>
<ref id="bibr25-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Toepoel</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Das</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Van Soest</surname>
<given-names>A.</given-names>
</name>
</person-group> <year>2009</year>. <article-title>Design of web questionnaires: The effects of the number of items per screen</article-title>. <source>Field Methods</source>, <volume>21</volume>, <fpage>200</fpage>–<lpage>13</lpage>.</citation>
</ref>
<ref id="bibr26-1525822X11419104">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tomaskovic-Devey</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Leiter</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Thompson</surname>
<given-names>S.</given-names>
</name>
</person-group> <year>1994</year>. <article-title>Organizational survey non-response</article-title>. <source>Administrative Science Quarterly</source>, <volume>39</volume>, <fpage>439</fpage>–<lpage>57</lpage>.</citation>
</ref>
<ref id="bibr27-1525822X11419104">
<citation citation-type="web">
<collab collab-type="author">U.S. Census Bureau</collab>. <year>2009</year>. <article-title>Statistical Abstract of the United States</article-title>: <year>2010</year>. <edition>129th ed</edition>. <ext-link ext-link-type="uri" xlink:href="http://www.census.gov/compendia/statab/2010edition.html">http://www.census.gov/compendia/statab/2010edition.html</ext-link> <comment>(accessed August 24, 2010)</comment>.</citation>
</ref>
<ref id="bibr28-1525822X11419104">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Von Eye</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Mun</surname>
<given-names>E. Y.</given-names>
</name>
</person-group> <year>2005</year>. <source>Analyzing rater agreement: Manifest variable methods</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>