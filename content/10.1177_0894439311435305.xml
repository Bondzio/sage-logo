<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SSC</journal-id>
<journal-id journal-id-type="hwp">spssc</journal-id>
<journal-title>Social Science Computer Review</journal-title>
<issn pub-type="ppub">0894-4393</issn>
<issn pub-type="epub">1552-8286</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0894439311435305</article-id>
<article-id pub-id-type="publisher-id">10.1177_0894439311435305</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Asking Probing Questions in Web Surveys</article-title>
<subtitle>Which Factors have an Impact on the Quality of Responses?</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Behr</surname>
<given-names>Dorothée</given-names>
</name>
<xref ref-type="aff" rid="aff1-0894439311435305">1</xref>
<xref ref-type="corresp" rid="corresp1-0894439311435305"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kaczmirek</surname>
<given-names>Lars</given-names>
</name>
<xref ref-type="aff" rid="aff1-0894439311435305">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bandilla</surname>
<given-names>Wolfgang</given-names>
</name>
<xref ref-type="aff" rid="aff1-0894439311435305">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Braun</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="aff1-0894439311435305">1</xref>
</contrib>
<bio><title>Bios</title><p>
<bold>Dorothée Behr</bold> is a senior researcher in the department of survey design and methodology at GESIS—Leibniz Institute for the Social Sciences, Mannheim (Germany). She holds a doctoral degree from the University of Mainz (2009). Her current research interests include web survey design and comparability of cross-cultural questionnaires. E-mail contact: <email>dorothee.behr@gesis.org</email>.</p>
<p>
<bold>Lars Kaczmirek</bold> is a senior researcher at GESIS—Leibniz Institute for the Social Sciences in Mannheim (Germany). He received his PhD in psychology and specialized in survey design and methodology. His publications focus on online survey methodology, data quality, eyetracking research, data protection, accessibility, and usability. E-mail contact: <email>lars.kaczmirek@gesis.org</email>.</p>
<p>
<bold>Wolfgang Bandilla</bold> is a project consultant and senior researcher in the department of survey design and methodology at GESIS—Leibniz Institute for the Social Sciences, Mannheim (Germany). He has specialized in web survey methodology and mixed-mode surveys. E-mail contact: <email>wolfgang.bandilla@gesis.org</email>.</p>
<p>
<bold>Michael Braun</bold> is a senior project consultant at GESIS—Leibniz Institute for the Social Sciences and adjunct professor at the University of Mannheim (Germany). He has specialized in cross-cultural survey methodology and analysis. He has extensively published both on methodological problems of interculturally comparative research and on international comparisons in the fields of migration, work, and the family. E-mail contact: <email>michael.braun@gesis.org</email>.</p>
</bio>
</contrib-group>
<aff id="aff1-0894439311435305"><label>1</label>GESIS—Leibniz Institute for the Social Sciences, Mannheim, Germany</aff>
<author-notes>
<corresp id="corresp1-0894439311435305">Dorothée Behr, GESIS—Leibniz Institute for the Social Sciences, PO Box 12 21 55, 68072 Mannheim, Germany Email: <email>dorothee.behr@gesis.org</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>30</volume>
<issue>4</issue>
<fpage>487</fpage>
<lpage>498</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Cognitive interviewing is a well-established method for evaluating and improving a questionnaire prior to fielding. However, its present implementation brings with it some challenges, notably in terms of small sample sizes or the possibility of interviewer effects. In this study, the authors test web surveys through nonprobability online panels as a <italic>supplemental</italic> means to implement cognitive interviewing techniques. The overall goal is to tackle the above-mentioned challenges. The focus in this article is on methodological features that pave the way for an eventual successful implementation of category-selection probing in web surveys. The study reports on the results of 1,023 respondents from Germany. In order to identify implementation features that lead to a high number of meaningful answers, the authors explore the effects of (1) different panels, (2) different probing variants, and (3) different numbers of preceding probes on answer quality. The overall results suggest that category-selection probing can indeed be implemented in web surveys. Using data from two panels—a community panel where members can actively get involved, for example, by creating their own polls, and a “conventional” panel where answering surveys is the members' only activity—the authors find that high community involvement does not increase the likelihood to answer probes or produce longer statements. Testing three probing variants that differ in wording and provided context, the authors find that presenting the context of the probe (i.e., the probed item and the respondent’s answer) produces a higher number of meaningful answers. Finally, the likelihood to answer a probe decreases with the number of preceding probes. However, the word count of those who eventually answer the probes slightly increases with an increasing number of probes.</p>
</abstract>
<kwd-group>
<kwd>web survey design</kwd>
<kwd>probing</kwd>
<kwd>open-ended questions</kwd>
<kwd>cognitive interviewing</kwd>
<kwd>nonprobability online panels</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0894439311435305">
<title>Introduction</title>
<p>Cognitive interviewing is a well-established method for assessing a questionnaire prior to its fielding. However, challenges of cognitive interviewing are equally recognized, such as limited sample sizes or potential interviewer effects due to different probing behavior (e.g., <xref ref-type="bibr" rid="bibr2-0894439311435305">Beatty &amp; Willis, 2007</xref>; <xref ref-type="bibr" rid="bibr4-0894439311435305">Blair, Conrad, Ackermann, &amp; Claxton, 2006</xref>; <xref ref-type="bibr" rid="bibr7-0894439311435305">Conrad &amp; Blair, 2009</xref>). Against this background, we propose to conduct a <italic>supplemental</italic> evaluation activity within web surveys that produces data similar to that produced through cognitive interviews. The overall goal of this new evaluation technique is to circumvent the challenges mentioned above.</p>
<p>The integration of cognitive interviewing techniques into a regular survey is not new. Already in 1966, Schuman pioneered “random probes” in a regular face-to-face survey, and Smith followed his example in <xref ref-type="bibr" rid="bibr18-0894439311435305">1989</xref>. In this article, we aim at preparing the methodological ground for integrating cognitive interviewing techniques, particularly probing, into web surveys. We do so by testing which implementation features contribute to a high number of meaningful probe answers. Further work will need to investigate to what extent these responses can indeed answer substantive research questions.</p>
<p>Web surveys are a very cost- and time-efficient means to conduct studies—both pretesting and postsurvey evaluation studies—with large sample sizes. They allow for meaningful quantification of results as well as analysis of smaller subgroups of respondents with potentially diverging response behavior. They also guarantee standardized probing since each respondent receives the same stimulus. They further permit respondents to take their time to reflect on their answers without feeling pressed, elaborate on their answers or modify them, all of which in complete anonymity. This may in fact speak in favor of probing in web surveys, at least if respondents can be motivated to answer probes in the first place.</p>
<p>The rise of web surveys as a data collection method has led to an array of studies on web design features, such as layout effects, interactivity, or screen aesthetics, and their impact on data quality (<xref ref-type="bibr" rid="bibr6-0894439311435305">Christian, Dillman, &amp; Smyth, 2007</xref>; <xref ref-type="bibr" rid="bibr8-0894439311435305">Conrad, Couper, Tourangeau, &amp; Peytchev, 2006</xref>; <xref ref-type="bibr" rid="bibr11-0894439311435305">Ganassali, 2008</xref>; <xref ref-type="bibr" rid="bibr14-0894439311435305">Mahon-Haft &amp; Dillman, 2010</xref>). However, the topic of open-ended questions in web surveys has received attention only recently. The focus to date has been on different answer space sizes (notably for nonnarrative answers), the use of motivational instructions and of follow-ups to open-ended questions or the impact of topic interest and demographic characteristics (<xref ref-type="bibr" rid="bibr9-0894439311435305">Denscombe, 2008</xref>; <xref ref-type="bibr" rid="bibr12-0894439311435305">Holland &amp; Christian, 2009</xref>; <xref ref-type="bibr" rid="bibr19-0894439311435305">Smyth, Dillman, Christian, &amp; McBride, 2009</xref>; <xref ref-type="bibr" rid="bibr15-0894439311435305">Oudejans &amp; Christian, 2010</xref>). We advance this research by integrating cognitive interviewing techniques, particularly category-selection probing, into the web environment. Category-selection probing is a standard cognitive interviewing technique. Respondents are asked to name reasons for having selected a particular scale value for a closed question (<xref ref-type="bibr" rid="bibr16-0894439311435305">Prüfer &amp; Rexroth, 2005</xref>). Category-selection probing can be used to analyze comprehension problems and the differentiations respondents use in the interpretation of items. Results of category-selection probing can be used to improve questions and to obtain background information on how responses have to be interpreted by analysts.</p>
<p>In this article, we explore the effects of different online panels, different probing variants, and different number of preceding probes on the quality of probe answers. The goal is to identify implementation features that lead to a high number of <italic>productive</italic>, that is, meaningful answers to category-selection probes.</p>
<p>Differences in the composition and practices of online access panels can affect survey results (<xref ref-type="bibr" rid="bibr1-0894439311435305">Baker et al., 2010</xref>). A major difference may reside in a community approach where panelists can create their own polls and post their opinions in addition to answering the “usual” surveys. With regard to our research question, we expect community panelists who are particularly active in their panel to be more willing to answer open-ended questions and also to write more words than noncommunity panelists who belong to a “conventional” panel.</p>
<p>To ensure and enhance answer quality among respondents, the probe design should be optimized. Although open-ended questions seem to fare comparably or better in web surveys than in paper-and-pencil surveys and on the whole seem promising (<xref ref-type="bibr" rid="bibr9-0894439311435305">Denscombe, 2008</xref>; <xref ref-type="bibr" rid="bibr12-0894439311435305">Holland &amp; Christian, 2009</xref>; <xref ref-type="bibr" rid="bibr19-0894439311435305">Smyth et al., 2009</xref>), they remain a source of item nonresponse in web surveys. After all, they usually require more effort by respondents than closed-ended questions (<xref ref-type="bibr" rid="bibr10-0894439311435305">Galesic, 2006</xref>; <xref ref-type="bibr" rid="bibr12-0894439311435305">Holland &amp; Christian, 2009</xref>). Category-selection probing may require even more effort than usual open-ended questions depending on how thoroughly respondents processed and answered the questions that are probed. A suitable probe design should therefore keep respondents' effort at a minimum and at the same time encourage rich and descriptive responses. As to design, this may mean providing the context of the probe (i.e., the closed item and the answer) so that no further effort is needed (such as trying to remember what exactly the question and the answer were or even going back to the previous screen to retrieve the relevant information). In addition, the wording of the probe should prevent respondents from satisficing (<xref ref-type="bibr" rid="bibr13-0894439311435305">Krosnick, 1999</xref>). Satisficing, as some argue, may be particularly easy in web surveys where no interviewer is present who could motivate the respondent (<xref ref-type="bibr" rid="bibr1-0894439311435305">Baker et al., 2010</xref>). A usual way of asking category-selection probes is to inquire why the respondent has chosen a particular scale value. However, in web surveys—without an interviewer present—the explicit mentioning of the answer category in the probe might invite respondents to find an easy way out of properly answering the probe (e.g., they might only rephrase the given answer category). However, such an answer would not allow us to identify different frames of interpretation. Theoretically, the satisficing risk may be mitigated by deviating attention from the specific answer category and by simply asking respondents to give reasons for their opinion. We test different probe designs and expect that a probe design that reduces respondents' effort by providing the relevant context and that at the same time is less answer value-specific is most successful in eliciting rich and descriptive answers.</p>
<p>Given the effort required to answer open-ended questions, the number of probes across a survey should carefully be chosen. <xref ref-type="bibr" rid="bibr15-0894439311435305">Oudejans and Christian (2010)</xref>, for instance, show that word count decreases as respondents progress through the survey. We expect that item nonresponse increases and word count decreases, respectively, toward the end of the survey due to increased respondent burden with increasing number of preceding probes.</p>
</sec>
<sec id="section2-0894439311435305" sec-type="methods">
<title>Method</title>
<p>The data in this article come from two web surveys conducted in Germany in June/July 2010 using two different online panels, henceforth called <italic>community panel</italic> and <italic>noncommunity panel</italic>. The target population was defined in each case as German citizens, aged 18–70. Quotas were set according to region, sex, age, and education.<sup>
<xref ref-type="fn" rid="fn1-0894439311435305">1</xref>
</sup> This did certainly not make the survey representative but we tried at least to obtain a broader picture of the population. We targeted 480 completed interviews with the noncommunity panel and 528 with the community panel.<sup>
<xref ref-type="fn" rid="fn2-0894439311435305">2</xref>
</sup> The questionnaire was the same for the two panels.</p>
<p>The community panel—where panelists can create their own polls or write opinions—invited their most active community members (i.e., members with the highest polling activity in the last 3 months). We expected that active community members would be more prone to answer open-ended questions and would also produce longer answers than noncommunity panelists.</p>
<p>Furthermore, we designed three variants for category-selection probing (<xref ref-type="fig" rid="fig1-0894439311435305">Figures 1</xref><xref ref-type="fig" rid="fig2-0894439311435305"/>–<xref ref-type="fig" rid="fig3-0894439311435305">3</xref>).</p>
<fig id="fig1-0894439311435305" position="float">
<caption>
<p>Probe variant A—Closed item and respondent’s answer and answer scale-specific probe (item originally in German).</p>
</caption>
<graphic xlink:href="10.1177_0894439311435305-fig1.tif"/>
</fig>
<fig id="fig2-0894439311435305" position="float">
<caption>
<p>Probe variant B—Closed item and respondent’s answer and less answer scale-specific probe (item originally in German).</p>
</caption>
<graphic xlink:href="10.1177_0894439311435305-fig2.tif"/>
</fig>
<fig id="fig3-0894439311435305" position="float">
<caption>
<p>Probe variant C—Probe and no further context (item originally in German).</p>
</caption>
<graphic xlink:href="10.1177_0894439311435305-fig3.tif"/>
</fig>
<p>In Variant A, the closed item and the respondent’s answer were repeated at the top of the probe screen. The probe itself read: “Please explain why you have chosen [answer category].” Variant B equally had the closed item and the respondent’s answer repeated. The wording of the probe, however, was less scale value-specific: “Could you please give reasons for your opinion.” In Variant C, neither the closed item nor the answer of the respondent was displayed on the probe screen. Only the probe itself was shown: “Could you please give reasons for your opinion on the previous item.” We assumed that Probe variant C would be least successful in producing <italic>productive</italic>, that is, meaningful answers due to the missing context on the screen and, therefore, increased respondent burden. In turn, Probe variants A and B would lead to more productive answers because of the provided context. We expected further that the stronger emphasis on the chosen answer category in Variant A might provoke more nonsubstantive answers in terms of just rephrasing the selected answer categories (e.g., because I fully agree) in Variant A than in Variant B. Three agree–disagree items were probed with category-selection probing. These were “A man and a woman should share housekeeping chores and taking care of the children equally, so that both can combine work and family life” (equal division), “A working mother can establish just as warm and secure a relationship with her children as a mother who does not work” (mother–child relationship), and “Having children interferes too much with the freedom of parents” (children constrain freedom). The answers ranged on a 5-point scale from “strongly agree” to “strongly disagree.” At the beginning of the survey, the respondents were randomly assigned to Probe variants A, B, or C. This variant then remained constant for them across the survey.</p>
<p>The probed items were part of different topical blocks which were rotated. This allowed us to test whether an increasing number of preceding probes had an effect on the quality of the probe answers—independent of item content. The questionnaire in total comprised 33 closed-ended questions and 6 open-ended questions. This article focuses on the three category-selection probes among the six open-ended questions.</p>
<sec id="section3-0894439311435305">
<title>Dependent Variables</title>
<p>To test our assumptions, we used two dependent variables, namely productivity of answers to probes and word count. Productive probe answers were considered to be all answers except the following nonproductive answers: (1) implicit refusals (respondents giving no answer whatsoever), (2) meaningless entries (?, ---, or fgh, etc.), (3) don’t knows, (4) specified don’t knows (respondents providing a reason for their DK such as lack of experience), (5) explicit refusals (respondents answering “n.a.”, “no”, etc.), (6) other nonsubstantive answers (rhetoric questions such as “why not?”, matter-of-fact-statements such as “because it is like that”, rephrasing of answer category, etc.), and (7) nonintelligible answers. Examples for (7) are answers such as “equal shares” or “feeling” for the probe after “A working mother can establish just as warm and secure a relationship with her children as a mother who does not work.” We are aware that we employed a very broad definition of productivity. Depending on the substantive research question, answers beyond these nonproductive answers may also be unusable. We used a dichotomous measure of productivity (0 = <italic>nonproductive answers</italic>, 1 = <italic>productive answers</italic>) in our models. Analyses with word count, our second dependent variable, were restricted to respondents with at least one productive answer across the three probes.</p>
</sec>
<sec id="section4-0894439311435305">
<title>Independent Variables</title>
<p>Among the independent variables included in our analyses were panel (community panel as the baseline) and two dummies for the probing variants (Variant A constituted the baseline). Control variables were region (western Germany as the baseline), sex (women as the baseline), geographical origin (German origin as the baseline), education (university entrance requirement as the baseline), and year of birth. Furthermore, the item “A man’s job is to earn money; a woman’s job is to look after the home and family” which measures traditionality with regard to gender roles was used as an attitudinal control variable. Its answers on a 5-point scale ranged from “strongly agree” to “strongly disagree.” The number of preceding probes was operationalized by a quantitative variable, taking on the values 0–5.</p>
</sec>
<sec id="section5-0894439311435305">
<title>Analytical Procedure</title>
<p>Since the answers to the probes were not independent of each other but nested in respondents, we employed multilevel modeling. Multilevel modeling is appropriate when variables pertain to different levels and when a dependency exists between the elements of the lower level. In our models, the lower level is constituted by the three probes, the higher level by the respondents. We estimated separate multilevel models for productivity and for word count, using an identical set of independent variables.</p>
<p>We started with an empty model containing productivity as our dependent variable and also a differentiation between the three probed items (Model 0). This model allowed us to decompose the variance between the probe level and respondent level and to assess the appropriateness of multilevel modeling. Model 1 included the respondent-level characteristics panel and probe variants. In Model 2, we added the control variables region, sex, geographical origin, education, and year of birth. Model 3 included the attitudinal item as an additional control variable. Model 4 included the number of preceding probes. The hierarchy in models was determined by first introducing the higher-level variables, including the control variables pertaining to this level, and then adding the preceding probe number as our only lower-level variable. The models for the word count followed the same logic. Likelihood-ratio tests were performed to establish difference in fit between the models.</p>
</sec>
</sec>
<sec id="section6-0894439311435305">
<title>Results</title>
<p>485 noncommunity panel members completed the survey; the drop-out rate was 7.8% (41/526). The mean completion time was 14:38 min (median 11:34 min.). From the community panel, 538 members completed the survey; here the drop-out rate was 6.4% (37/575). The mean completion time for the community panel was 12:54 min (median 10:00 min.). Across the panels and across the 3 probed items, between 73.31% and 82.21% of probe answers were coded as productive. Interrater reliability based on two independent codings and 10% of answers for each probed item ranged from 90% to 98%.<sup>
<xref ref-type="fn" rid="fn3-0894439311435305">3</xref>
</sup> The average word count of productive answers ranged from 18.7 to 21.9 words. <xref ref-type="table" rid="table1-0894439311435305">Table 1</xref> presents results separately for probed items and panels.</p>
<table-wrap id="table1-0894439311435305" position="float">
<label>Table 1.</label>
<caption>
<p>Item- and Panel-Specific Outcomes in Productive Answers and Word Count</p>
</caption>
<graphic alternate-form-of="table1-0894439311435305" xlink:href="10.1177_0894439311435305-table1.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Productive Probe Answers (%)</th>
<th>Word Count of Probe Answers</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Item “Equal division”</td>
</tr>
<tr>
<td> Noncommunity panel</td>
<td>83.71</td>
<td>1–152 (mean: 21.1)</td>
</tr>
<tr>
<td> Community panel</td>
<td>80.86</td>
<td>1–162 (mean: 19.0)</td>
</tr>
<tr>
<td> Total</td>
<td>82.21</td>
<td>1–162 (mean: 20.0)</td>
</tr>
<tr>
<td colspan="3">Item “Mother–child relationship”</td>
</tr>
<tr>
<td> Noncommunity panel</td>
<td>78.76</td>
<td>1–103 (mean: 23.3)</td>
</tr>
<tr>
<td> Community panel</td>
<td>68.40</td>
<td>1–136 (mean: 20.4)</td>
</tr>
<tr>
<td> Total</td>
<td>73.31</td>
<td>1–136 (mean: 21.9)</td>
</tr>
<tr>
<td colspan="3">Item “Children constrain freedom”</td>
</tr>
<tr>
<td> Noncommunity panel</td>
<td>80.00</td>
<td>1–116 (mean: 19.5)</td>
</tr>
<tr>
<td> Community panel</td>
<td>78.44</td>
<td>1–189 (mean: 17.9)</td>
</tr>
<tr>
<td> Total</td>
<td>79.18</td>
<td>1–189 (mean: 18.7)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0894439311435305">
<p>
<italic>Note</italic>. <italic>N =</italic> 1,023, but word count excludes nonproductive answers.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>With multilevel modeling of productivity (<xref ref-type="table" rid="table2-0894439311435305">Table 2</xref>), we find the following: Model 0 (not presented in detail) shows that 71% of the total variance in productivity derives from respondent differences and the remainder of 29% from differences on the probe level. This distribution justifies the use of multilevel modeling. Contrary to our expectations, Model 1 shows that noncommunity panelists are more likely to produce productive answers. In addition, Probe variant C, where no further context is provided on the screen, is less likely than Probe variant A, our baseline, to produce productive answers. No differences can be found between Probe variants A and B, though. In Model 2, we control for the sociodemographic background of the respondents. The panel and probe variant effects remain significant. In addition, we find that women and older respondents are more likely to produce a productive answer than men and younger respondents. Education, however, does not impact on the likelihood to produce a productive answer. Upon introducing the attitudinal benchmark item in Model 3, the difference between the panels disappears while the effects of probe variants, sex, and age remain significant. We find that nontraditional respondents are more likely to write productive answers than traditional respondents. Obviously, nontraditional attitudes are related to a higher interest in the topic of the questions. The introduction of the preceding probe number in Model 4 does not alter the conclusions so far. It shows, however, that with increasing number of probes productive answers are less likely. The improvements across all models are statistically significant following likelihood-ratio tests (<italic>p</italic> &lt; .05).</p>
<table-wrap id="table2-0894439311435305" position="float">
<label>Table 2.</label>
<caption>
<p>Mixed-Effects Logistic Regression Models Predicting Productive Answer Behavior (Odds Ratios, in Parentheses: z Values)</p>
</caption>
<graphic alternate-form-of="table2-0894439311435305" xlink:href="10.1177_0894439311435305-table2.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Model 0</th>
<th>Model 1</th>
<th>Model 2</th>
<th>Model 3</th>
<th>Model 4</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Probed items (base: equal division)</td>
</tr>
<tr>
<td> Mother–child relationship</td>
<td>.342*** (−6.70)</td>
<td>.342*** (−6.70)</td>
<td>.340*** (−6.72)</td>
<td>.340*** (−6.72)</td>
<td>.382 (−5.80)***</td>
</tr>
<tr>
<td> Children constrain freedom</td>
<td>.659** (−2.60)</td>
<td>.659** (−2.60)</td>
<td>.657** (−2.61)</td>
<td>.657** (−2.62)</td>
<td>.700 (−2.18)*</td>
</tr>
<tr>
<td>Panel: noncommunity panel (base: community panel)</td>
<td align="center">–</td>
<td>1.765* (2.35)</td>
<td>1.782* (2.46)</td>
<td>1.547 (1.85)</td>
<td>1.532 (1.80)</td>
</tr>
<tr>
<td colspan="6">Probe variants (base: A)</td>
</tr>
<tr>
<td> Probe variant B</td>
<td>
</td>
<td>.869 (−0.47)</td>
<td>.805 (−0.75)</td>
<td>.803 (−0.76)</td>
<td>.800 (−0.77)</td>
</tr>
<tr>
<td> Probe variant C</td>
<td>
</td>
<td>.529* (−2.16)</td>
<td>.519* (−2.29)</td>
<td>.533* (−2.21)</td>
<td>.524 (−2.25)*</td>
</tr>
<tr>
<td colspan="6">Sociodemographic variables</td>
</tr>
<tr>
<td> Region: eastern Germany (base: western Germany)</td>
<td align="center">–</td>
<td align="center">–</td>
<td>1.213 (0.83)</td>
<td>1.113 (0.46)</td>
<td>1.116 (0.47)</td>
</tr>
<tr>
<td> Sex: men (base: women)</td>
<td align="center">–</td>
<td align="center">–</td>
<td>.186*** (−6.84)</td>
<td>.198*** (−6.64)</td>
<td>.192 (−6.68)***</td>
</tr>
<tr>
<td> Geographical origin: non-German origin  (base: German origin)</td>
<td align="center">–</td>
<td align="center">–</td>
<td>.610 (−0.76)</td>
<td>.634 (−0.70)</td>
<td>.642 (−0.67)</td>
</tr>
<tr>
<td> Year of birth</td>
<td align="center">–</td>
<td align="center">–</td>
<td>.959*** (−4.93)</td>
<td>.962*** (−4.55)</td>
<td>.962 (−4.46)***</td>
</tr>
<tr>
<td> Education: less than university entrance requirement  (base: university entrance requirement)</td>
<td>
</td>
<td>
</td>
<td>.717 (−1.43)</td>
<td>.812 (−0.89)</td>
<td>.814 (−0.87)</td>
</tr>
<tr>
<td>Attitudinal item</td>
<td align="center">–</td>
<td align="center">–</td>
<td align="center">–</td>
<td>1.490*** (3.77)</td>
<td>1.502 (3.81)***</td>
</tr>
<tr>
<td>No. of preceding probes (0–5)</td>
<td align="center">–</td>
<td align="center">–</td>
<td align="center">–</td>
<td align="center">–</td>
<td>.883 (−2.80)**</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0894439311435305">
<p>
<italic>Note</italic>. <italic>N =</italic> 1,007.</p>
</fn>
<fn id="table-fn3-0894439311435305">
<p>*<italic>p</italic> &lt; .05. **<italic>p</italic> &lt; .01. ***<italic>p</italic> &lt; .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>With the dependent variable word count (<xref ref-type="table" rid="table3-0894439311435305">Table 3</xref>), we follow the same approach as above in building our models. Our Model 0 decomposes the variance in word count between probe level and respondent level: the respondents account for 58% of variance, the probes for the remaining 42%. In Model 1, we include the probe variants as well as the panel. We find that the noncommunity panelists write more words than the community panelists. The two-word difference between the panels, though, is rather minimal. The probe variant has no impact on the word count as nonproductive answers were not included in the analyses. In Model 2, sociodemographic background variables are added. Women write significantly more words than men; at the same time, increasing age and particularly lower education is related with writing fewer words. Model 3 shows that nontraditional respondents write more words than traditional respondents. When controlling for traditionality, the panel effect found in Model 1, namely that noncommunity panelists write more, weakens although it remains significant. Finally, Model 4 includes the number of preceding probes. With increasing number of probes, respondents seem to write more, although effects are certainly minimal. Among productive respondents at least, we are thus observing a warming-up effect. Likelihood-ratio tests show that the improvements across all models are statistically significant (<italic>p</italic> &lt; .05).</p>
<table-wrap id="table3-0894439311435305" position="float">
<label>Table 3.</label>
<caption>
<p>Mixed-Effects Maximum Likelihood (ML) Regression Predicting Word Count (Coefficients, in Parentheses: z Values)</p>
</caption>
<graphic alternate-form-of="table3-0894439311435305" xlink:href="10.1177_0894439311435305-table3.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Model 0</th>
<th>Model 1</th>
<th>Model 2</th>
<th>Model 3</th>
<th>Model 4</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Probed items (base: equal division)</td>
</tr>
<tr>
<td> Mother–child relationship</td>
<td>1.152 (1.90)</td>
<td>1.140 (1.88)</td>
<td>1.110 (1.84)</td>
<td>1.117 (1.85)</td>
<td>0.730 (1.16)</td>
</tr>
<tr>
<td> Children constrain freedom</td>
<td>−1.662** (−2.80)</td>
<td>−1.657** (−2.79)</td>
<td>−1.634** (−2.75)</td>
<td>−1.631** (−2.75)</td>
<td>−1.817 (−3.03)**</td>
</tr>
<tr>
<td>Panel: noncommunity panel (base: community panel)</td>
<td>—</td>
<td>2.384* (2.32)</td>
<td>2.448* (2.44)</td>
<td>2.006* (1.98)</td>
<td>2.021 (2.00)*</td>
</tr>
<tr>
<td colspan="6">Probe variants (base: A)</td>
</tr>
<tr>
<td> Probe variant B</td>
<td>—</td>
<td>2.201 (1.76)</td>
<td>2.057 (1.68)</td>
<td>2.028 (1.67)</td>
<td>2.044 (1.68)</td>
</tr>
<tr>
<td> Probe variant C</td>
<td>—</td>
<td>.915 (0.72)</td>
<td>.637 (0.51)</td>
<td>.672 (0.54)</td>
<td>0.707 (0.57)</td>
</tr>
<tr>
<td colspan="6">Sociodemographic variables</td>
</tr>
<tr>
<td> Region: eastern Germany (base: western Germany)</td>
<td>—</td>
<td>—</td>
<td>−.006 (−0.01)</td>
<td>−.259 (−0.26)</td>
<td>.283 (0.28)</td>
</tr>
<tr>
<td> Sex: men (base: women)</td>
<td>—</td>
<td>—</td>
<td>−5.803*** (−5.78)</td>
<td>−5.550*** (−5.52)</td>
<td>−5.513 (−5.49)***</td>
</tr>
<tr>
<td> Geographical origin: non-German origin  (base: German origin)</td>
<td>—</td>
<td>—</td>
<td>1.904 (0.64)</td>
<td>1.899 (0.64)</td>
<td>1.841 (0.62)</td>
</tr>
<tr>
<td> Year of birth</td>
<td>
</td>
<td>
</td>
<td>0.084* (2.37)</td>
<td>0.093** (2.60)</td>
<td>0.092 (2.58)*</td>
</tr>
<tr>
<td> Education: less than university entrance requirement (base: university entrance requirement)</td>
<td>
</td>
<td>
</td>
<td>– 3.702*** (−3.68)</td>
<td>−3.341** (−3.30)</td>
<td>−3.339 (−3.30)**</td>
</tr>
<tr>
<td>Attitudinal item</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>1.278** (2.67)</td>
<td>1.272 (2.66)**</td>
</tr>
<tr>
<td>No. of preceding probes (0–5)</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>.383 (2.19)*</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0894439311435305">
<p>
<italic>Note</italic>. <italic>N</italic> = 910.</p>
</fn>
<fn id="table-fn5-0894439311435305">
<p>*<italic>p</italic> &lt; .05. **<italic>p</italic> &lt; .01. ***<italic>p</italic> &lt; .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section7-0894439311435305">
<title>Discussion</title>
<p>Overall, it seems that category-selection probing can successfully be implemented in web surveys for which respondents are drawn from online panels. On the whole, respondents provide productive answers to category-selection probes. In this article, we investigated which implementation features need to be considered in order to obtain a high number of productive probe answers.</p>
<p>Contrary to our assumptions, the panel character <italic>per se</italic>—whether community or noncommunity panel—does not or hardly influence the answer quality. We rather conclude that different panel distributions on a key attitude that is related to the topic of the probes and can be taken as an indicator for interest have an impact on both the likelihood to respond and on word count. On this key attitude, namely the attitudinal item introduced into the third model, the panels differed: the noncommunity panel was more egalitarian in gender role attitudes than the community panel. Different distributions on attitude items can, therefore, make a difference between panels when it comes to answering probe items. The panel composition thus becomes an important factor to take into account when choosing a panel provider for a probing study. In line with one’s research question, a researcher may choose a panel with panel members that predominantly harbor certain attitudes or certain interests (if this is known in advance). Other research questions may call for a panel whose coverage of the general (Internet) population is as broad as possible. A certain level of nonresponse on probe answers among those less interested in a topic will then have to be accepted, however. Regardless of the panel composition, effects of sex, age, or education seem unavoidable. Here, the study backs findings from <xref ref-type="bibr" rid="bibr15-0894439311435305">Oudejans and Christian (2010)</xref> as well as <xref ref-type="bibr" rid="bibr9-0894439311435305">Denscombe (2008)</xref>.</p>
<p>A probe design which had the closed item, the respondent’s answer to it, and the probe on a screen was most successful in eliciting productive answers compared to just the probe without any context (Variant C). Further differentiation in probe wording (Variant A or B) did not produce significantly different results. However, for those who eventually answered the probe, the probe variants did not have an effect on the word count. Efforts should, therefore, be made toward providing respondents with the needed context on the screen in order to reduce nonproductive answers to category-selection probes.</p>
<p>In terms of number of probes, we found that the probes had not yet imposed too heavy a burden on the respondents. The odds of responding in a productive manner decreased with an increasing number of preceding probes. At the same time—among productive respondents—the writing level did not abate, rather the contrary. On the whole, keeping up the motivation and integrating probes sensibly and sparingly is thus an important issue for future studies.</p>
<p>This article provides us with positive results on the willingness of online panelists to answer category-selection probes on the web. We are now more aware of overall design features or respondent characteristics that lead to providing answers to these questions. The steps currently taken are to estimate the usefulness of answers received with regard to answering substantive research questions. <xref ref-type="bibr" rid="bibr3-0894439311435305">Behr, Braun, Kaczmirek, and Bandilla (in press)</xref>, for instance, show that online probe answers help to uncover validity problems with a gender ideology item that is meant to measure a nontraditional stance but falls short of this goal. Further studies currently undertaken involve online probing in the cross-national context as well as the inclusion of other probe types besides category-selection probing. By asking a specific probe, <xref ref-type="bibr" rid="bibr5-0894439311435305">Braun, Behr, and Kaczmirek (2011)</xref>, for instance, explore what type of immigrants respondent in different countries have in mind when answering attitude items on immigrants. While the immigrant groups mentioned differ across countries, on an abstract level they seem to be comparable.</p>
<p>Despite our satisfaction with the overall results of online probing, we caution against substituting this online tool for face-to-face cognitive interviewing. We rather envisage it as a <italic>supplemental</italic> tool for situations where quantification of results and the coverage of hard-to-reach population groups, for example, those identified by contradictory answer behavior, are needed.</p>
<p>We wish to stress that the web implementation cannot offer targeted follow-up probes to incomprehensible or insufficient probe answers nor can it ask respondents to elaborate on an issue if the researcher feels this would be necessary. This interactivity is impossible, unless it is known in advance what exactly constitutes an incomprehensible or insufficient answer. However, the possibilities to add a follow-up probe to nonproductive probe answers, such as “???” or “don’t know,” are available, and also pioneered in research (e.g., <xref ref-type="bibr" rid="bibr15-0894439311435305">Oudejans &amp; Christian, 2010</xref>). This may be a research line worth taking up in order to enhance the chances of getting at least a minimal response to a probe question. Further research could also investigate general differences between face-to-face and online probing as well as the use of different probe types in web surveys.</p>
</sec>
</body>
<back>
<notes>
<fn-group>
<title>Notes</title>
<fn fn-type="other" id="fn1-0894439311435305">
<label>1.</label>
<p>Region included eastern and western Germany. Age groups were defined as 18–30, 31–50, and 51–70. Education differentiated between higher secondary education (university entrance requirement) and lower or no education (less than university entrance requirement). An equal number of cases was targeted for each combination.</p>
</fn>
<fn fn-type="other" id="fn2-0894439311435305">
<label>2.</label>
<p>Learning from the first panel survey, a 10% increase was implemented for the survey with the second panel to compensate for lurkers who quickly clicked through the survey without giving any useful answers to probe questions.</p>
</fn>
<fn fn-type="other" id="fn3-0894439311435305">
<label>3.</label>
<p>Discrepancies almost exclusively pertained to the category “nonintelligible.” If coders had had a particular substantive research question in mind, these discrepancies would possibly have been reduced.</p>
</fn>
</fn-group>
</notes>
<fn-group>
<fn fn-type="conflict" id="fn4-0894439311435305">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<fn-group>
<fn fn-type="financial-disclosure" id="fn5-0894439311435305">
<label>Funding</label>
<p>The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research was funded by the German Research Foundation (DFG) as part of the PPSM Priority Programme on Survey Methodology (SPP 1292) (project # BR 908/3-1).</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Baker</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Blumberg</surname>
<given-names>S. J.</given-names>
</name>
<name>
<surname>Brick</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Couper</surname>
<given-names>M. P.</given-names>
</name>
<name>
<surname>Courtright</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Dennis</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Zahs</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>AAPOR report on online panels</article-title>. <source>Public Opinion Quarterly</source>, <volume>74</volume>, <fpage>711</fpage>–<lpage>781</lpage></citation>
</ref>
<ref id="bibr2-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Beatty</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Willis</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Research synthesis: The practice of cognitive interviewing</article-title>. <source>Public Opinion Quarterly</source>, <volume>71</volume>, <fpage>287</fpage>–<lpage>311</lpage>
</citation>
</ref>
<ref id="bibr3-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Behr</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Braun</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Kaczmirek</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Bandilla</surname>
<given-names>W.</given-names>
</name>
</person-group> (<year>in press</year>). <article-title>Testing the validity of gender ideology items by implementing probing questions in web surveys</article-title>. <source>Field Methods</source>.</citation>
</ref>
<ref id="bibr4-0894439311435305">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Blair</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Conrad</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Ackermann</surname>
<given-names>A. C.</given-names>
</name>
<name>
<surname>Claxton</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>The effect of sample size on cognitive interview findings</article-title>. <conf-name>Paper presented at the AAPOR Conference</conf-name>, <publisher-loc>Montreal, Canada</publisher-loc>, <comment>May 18–21, 2006. Retrieved August 9, 2011, from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.abtassociates.com/presentations/aapor06_sample_size_cognitive_interviews.pdf">http://www.abtassociates.com/presentations/aapor06_sample_size_cognitive_interviews.pdf</ext-link>
</citation>
</ref>
<ref id="bibr5-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Braun</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Behr</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Kaczmirek</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2011</year>). <source>Assessing cross-national equivalence of measures of xenophobia: Evidence from probing in web surveys</source>. <comment>(under review)</comment>.
</citation>
</ref>
<ref id="bibr6-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Christian</surname>
<given-names>L. M.</given-names>
</name>
<name>
<surname>Dillman</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Smyth</surname>
<given-names>J. D.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Helping respondents get it right the first time: The influence of words, symbols, and graphics in web surveys</article-title>. <source>Public Opinion Quarterly</source>, <volume>71</volume>, <fpage>113</fpage>–<lpage>125</lpage>
</citation>
</ref>
<ref id="bibr7-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Conrad</surname>
<given-names>F. G.</given-names>
</name>
<name>
<surname>Blair</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Sources of error in cognitive interviews</article-title>. <source>Public Opinion Quarterly</source>, <volume>73</volume>, <fpage>32</fpage>–<lpage>55</lpage>
</citation>
</ref>
<ref id="bibr8-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Conrad</surname>
<given-names>F. G.</given-names>
</name>
<name>
<surname>Couper</surname>
<given-names>M. P.</given-names>
</name>
<name>
<surname>Tourangeau</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Peytchev</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Use and non-use of clarification features in web surveys</article-title>. <source>Journal of Official Statistics</source>, <volume>22</volume>, <fpage>245</fpage>–<lpage>269</lpage>
</citation>
</ref>
<ref id="bibr9-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Denscombe</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>The length of responses to open-ended questions: A comparison of online and paper questionnaires in terms of a mode effect</article-title>. <source>Social Science Computer Review</source>, <volume>26</volume>, <fpage>359</fpage>–<lpage>368</lpage>
</citation>
</ref>
<ref id="bibr10-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Galesic</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Dropouts on the Web: Effects of interest and burden experienced during an online survey</article-title>. <source>Journal of Official Statistics</source>, <volume>22</volume>, <fpage>313</fpage>–<lpage>328</lpage>
</citation>
</ref>
<ref id="bibr11-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ganassali</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>The influence of the design of web survey questionnaires on the quality of responses</article-title>. <source>Survey Research Methods</source>, <volume>2</volume>, <fpage>21</fpage>–<lpage>32</lpage>
</citation>
</ref>
<ref id="bibr12-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Holland</surname>
<given-names>J. L.</given-names>
</name>
<name>
<surname>Christian</surname>
<given-names>L. M.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>The influence of topic interest and interactive probing on responses to open-ended questions in web surveys</article-title>. <source>Social Science Computer Review</source>, <volume>27</volume>, <fpage>196</fpage>–<lpage>212</lpage>
</citation>
</ref>
<ref id="bibr13-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Krosnick</surname>
<given-names>J. A.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Survey research</article-title>. <source>Annual Review of Psychology</source>, <volume>50</volume>, <fpage>537</fpage>–<lpage>567</lpage>
</citation>
</ref>
<ref id="bibr14-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mahon-Haft</surname>
<given-names>T. A.</given-names>
</name>
<name>
<surname>Dillman</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Does visual appeal matter? Effects of web survey aesthetics on survey quality</article-title>. <source>Survey Research Methods</source>, <volume>4</volume>, <fpage>43</fpage>–<lpage>59</lpage>
</citation>
</ref>
<ref id="bibr15-0894439311435305">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Oudejans</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Christian</surname>
<given-names>L. M.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Using interactive features to motivate and probe responses to open-ended questions</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Das</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Ester</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Kaczmirek</surname>
<given-names>L.</given-names>
</name>
</person-group> (Eds.), <source>Social and behavioral research and the internet: Advances in applied methods and research strategies</source> (pp. <fpage>304</fpage>–<lpage>332</lpage>). <publisher-loc>London; NY</publisher-loc>: <publisher-name>Routledge</publisher-name>
</citation>
</ref>
<ref id="bibr16-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Prüfer</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Rexroth</surname>
<given-names>M.</given-names>
</name>
</person-group> <year>2005</year>. <article-title>Kognitive interviews [cognitive interviews]</article-title>. <source>ZUMA How-to-Reihe 15</source>. <comment>Retrieved August 2, 2011, from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.gesis.org/fileadmin/upload/forschung/publikationen/gesis_reihen/howto/How_to15PP_MR.pdf?download=true">http://www.gesis.org/fileadmin/upload/forschung/publikationen/gesis_reihen/howto/How_to15PP_MR.pdf?download=true</ext-link>
</citation>
</ref>
<ref id="bibr17-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schuman</surname>
<given-names>H.</given-names>
</name>
</person-group> <year>1966</year>. <article-title>The random probe: A technique for evaluating the validity of closed questions</article-title>. <source>American Sociological Review</source>, <volume>31</volume>, <fpage>218</fpage>–<lpage>222</lpage>
</citation>
</ref>
<ref id="bibr18-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname>
<given-names>T. W.</given-names>
</name>
</person-group> <year>1989</year>. <article-title>Random probes of GSS questions</article-title>. <source>International Journal of Public Opinion Research</source>, <volume>1</volume>, <fpage>305</fpage>–<lpage>325</lpage>
</citation>
</ref>
<ref id="bibr19-0894439311435305">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smyth</surname>
<given-names>J. D.</given-names>
</name>
<name>
<surname>Dillman</surname>
<given-names>D. A.</given-names>
</name>
<name>
<surname>Christian</surname>
<given-names>L. M.</given-names>
</name>
<name>
<surname>McBride</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Open-ended questions in web surveys—Can increasing the size of answer boxes and providing extra verbal instructions improve response quality?</article-title> <source>Public Opinion Quarterly</source>, <volume>73</volume>, <fpage>325</fpage>–<lpage>337</lpage>
</citation>
</ref>
</ref-list>
</back>
</article>