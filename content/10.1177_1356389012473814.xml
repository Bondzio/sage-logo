<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="editorial" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EVI</journal-id>
<journal-id journal-id-type="hwp">spevi</journal-id>
<journal-title>Evaluation</journal-title>
<issn pub-type="ppub">1356-3890</issn>
<issn pub-type="epub">1461-7153</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1356389012473814</article-id>
<article-id pub-id-type="publisher-id">10.1177_1356389012473814</article-id>
<article-categories><subj-group subj-group-type="heading">
<subject>Editorial</subject>
</subj-group></article-categories>
<title-group>
<article-title>Editorial</article-title>
</title-group>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2013</year>
</pub-date>
<volume>19</volume>
<issue>1</issue>
<fpage>3</fpage>
<lpage>4</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p>The repertoire that evaluators can draw on to address cause and effect questions is fast expanding. This takes us to the heart of ‘results’, ‘effects’ and ‘impact’ evaluations. In this issue we have examples of the use of ‘programme’ theory and ‘intervention’ (causal) theory; ‘causal pathways’; QCA (Qualitative Comparative Analysis); multi-method triangulation; and process tracing. Designs that were until quite recently considered marginal and exploratory are fast becoming mainstream. This issue also has a strong practice element: in the <italic>Visits to the world of practice</italic> section we have some fascinating reflections by eminent practitioners of the way evaluative processes evolve and are institutionalized internationally.</p>
<p>The planning of interventions and evaluations often follows ‘protocols’ or rubrics that suggest good practice: these are especially prevalent in the healthcare sector. Reflecting on the underlying theory of a programme or intervention is more likely to be advocated than made explicit or transparent. Avril Blamey, Freya MacMillan, Claire Fitzsimons, Rebecca Shaw and Nanette Mutrie reflect on how intervention theory and programme theory can ‘enhance’ both programme implementation and evaluation design. The intervention concerns adults who take insufficient exercise and the efficacy of a walking-based exercise programme. According to the authors, their aim is ‘to improve learning and attribution by more effectively linking data on the process, outcomes, and context of the intervention’. The planned evaluation is to be a full RCT trial. This is a further example of how articulating ‘theory’ is becoming a requirement across many evaluation approaches, including experiments. The article is also a useful step in clarifying what we mean by ‘theory’-based evaluation – one of the most persistently confusing terms in the evaluator’s lexicon (This is a task that was also helpfully advanced by Blamey and Mackenzie in an earlier issue of this journal – see 13(4), 2007).</p>
<p>It is regularly claimed that major sporting events bring with them various benefits − economic, social, environmental and health-related − quite apart from the spectacle of sports. Gerard McCartney, Phil Hanlon and Lyndal Bond examine these claims in the specific case of the forthcoming Commonwealth Games taking place in Glasgow, Scotland in 2014. The authors derived assumed benefits from documents of official sponsors of the Games, public consultations and academic and grey literature. McCartney, Hanlon and Bond adopt a realist frame of reference focusing on the ‘theoretical mechanisms’ that might link the Commonwealth games as an intervention and claimed outcomes. By ‘interrogating’ available evidence possible ‘causal pathways’ are identified through which benefits and indeed negative effects such as increased inequality might occur. Multiple methods are used to prospectively assess likely outcomes of this sporting event, even though the picture that emerges is not a positive one! The authors also suggest the approaches that would be needed to evaluate the actual impacts of the Commonwealth Games given what is known from existing evidence and theory.</p>
<p>Stefan Verweij and Lasse Gerrits consider how best to evaluate major infrastructure projects. These projects share many characteristics such as ‘budget overruns, time delays and public resistance’, but how these come about are not standard. Infrastructure projects are complex systems located in different contexts that shape their ‘emergent’ properties. This limits the possibility of generalization about causes. The disjuncture between ‘generalization’ and ‘contextualization’ is at the heart of the methodological challenge that the authors confront. Building on a growing body of work on qualitative comparative analysis (QCA) in its various forms − including ‘fuzzy set’ QCA − Verweij and Gerrits demonstrate how it is possible to disentangle the causal relationships between contexts and outcomes. Awareness of complexity and systems logics, and of the differences between variable-based and case-based methods, is now widespread in the social sciences. These ideas have been driven by the work of Charles Ragin, David Byrne and Benoit Rihoux. QCA has already featured in the pages of this journal – see Befani, Ledermann and Sager (2007) and more is scheduled for forthcoming issues. (Readers might be interested to note that David Byrne is contributing to a special issue of this journal that explores case-based evaluation later in 2013.)</p>
<p>Staffan Bjurulf, Evert Vedung and C.G. Larsson enter the now familiar impact evaluation debate with a methodological solution that is intended to directly address cause and effect. Their focus of attention is a ‘cluster’ organization made up of many companies as well as public sector and educational agencies. They ask: is the cluster organization responsible for increases in company performance? In answer to this question the authors advance arguments for ‘methodological triangulation’ combining three non-experimental designs: shadow controls, generic controls and process tracing. They suggest that ‘[i]n combination, the three designs provide a progressive and deepening understanding of intervention impacts and the various linkage mechanisms in the causal chain between intervention and outcomes’. It appears that the cluster arrangements ‘had positive effects not only on R&amp;D collaboration and company skills but also in relation to innovations, sales and investments by linking companies, researchers and public actors’. The authors acknowledge that their approach needs further development – for example, by eliminating alternative explanations for improvements in company performance. This should also properly be regarded as a ‘pilot study’ that uses limited data to build a case for the approach. It does, however, illustrate ways in which combinations of methods and designs are able to increase the validity of causal inference.</p>
<p>Capacities to evaluate are critical to manage performance and respond to demands for increased accountability. In <italic>Visits to the world of practice</italic>, two very different aspects of evaluation capacity and capacity building are considered. Santosh Mehrotra provides an authoritative overview of government Monitoring and Evaluation capacity-building across South Asia. Economic growth and donor demands have been among the drivers for improved public management in general and improved performance management in particular. However, Mehrotra suggests that the requisite capacities do not generally exist across the region – and goes on to suggest why this is. Fredrik Korfker looks back to the early stages of establishing evaluation capacities and processes in a multi-lateral development bank, the European Bank of Reconstruction and Development (EBRD). Korfker, who headed up the EBRD’s evaluation department during its first decade, is in a good position to describe and reflect on these early years. His narrative will be recognized by those familiar with similar institutional histories: evaluation’s relations with institutional management; striving to put in place an independent evaluation role; and the way that evaluation products were tied into a ‘lessons learning’ culture.</p>
<p>Finally, in <italic>News from the community</italic> Patricia Rogers provides an update on the ‘Better Evaluation’ website that went live in October 2012. The website is intended to ‘improve programs and projects by helping evaluators and those commissioning evaluations to choose the right evaluation methods for their situation and to use them well’. The ‘News’ section also updates readers on efforts to draw lessons from various ‘comprehensive’ evaluations of international organizations. This effort has been spearheaded by the Evaluation Office of the Global Environment Facility (GEF) and IFAD (International Fund for Agricultural Development).</p>
<p>All in all, much to think about in this first issue of 2013!</p>
<sig-block>
<sig><bold>Elliot Stern</bold></sig>
</sig-block>
</body>
</article>