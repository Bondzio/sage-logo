<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<journal-subtitle>Research, Theory, &amp; Application in Psychology and Related Sciences</journal-subtitle>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612438734</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612438734</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Reports</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Electrophysiological Evidence for the Understanding of Maternal Speech by 9-Month-Old Infants</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Parise</surname><given-names>Eugenio</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Csibra</surname><given-names>Gergely</given-names></name>
</contrib>
<aff id="aff1-0956797612438734">Central European University</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0956797612438734">Eugenio Parise, Cognitive Development Center, Central European University, Hattyú utca 14, 1015 Budapest, Hungary E-mail: <email>eugenioparise@tiscali.it</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2012</year>
</pub-date>
<volume>23</volume>
<issue>7</issue>
<fpage>728</fpage>
<lpage>733</lpage>
<history>
<date date-type="received">
<day>19</day>
<month>7</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>10</day>
<month>1</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>Early word learning in infants relies on statistical, prosodic, and social cues that support speech segmentation and the attachment of meaning to words. It is debated whether such early word knowledge represents mere associations between sound patterns and visual object features, or reflects referential understanding of words. By measuring an event-related brain potential component known as the N400, we demonstrated that 9-month-old infants can detect the mismatch between an object appearing from behind an occluder and a preceding label with which their mother introduces it. Differential N400 amplitudes have been shown to reflect semantic priming in adults, and its absence in infants has been interpreted as a sign of associative word learning. By setting up a live communicative situation for referring to objects, we demonstrated that a similar priming effect also occurs in young infants. This finding may indicate that word meaning is referential from the outset of word learning and that referential expectation drives, rather than results from, vocabulary acquisition in humans.</p>
</abstract>
<kwd-group>
<kwd>infants</kwd>
<kwd>language acquisition</kwd>
<kwd>semantic priming</kwd>
<kwd>ERP</kwd>
<kwd>N400</kwd>
<kwd>infant development</kwd>
<kwd>language development</kwd>
<kwd>priming</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Word learning in infancy is supported by a range of cognitive skills. By at least 8 months of age, infants are able to use statistical information to segment words from continuous speech (<xref ref-type="bibr" rid="bibr24-0956797612438734">Saffran, Aslin, &amp; Newport, 1996</xref>), and they rely on prosody both in word segmentation and in associating novel words with visual referents (<xref ref-type="bibr" rid="bibr27-0956797612438734">Shukla, White, &amp; Aslin, 2011</xref>). Mapping words onto semantic representations by matching the word form to a referent is a nontrivial problem, and there is no agreement on whether young infants are capable of referential word understanding that is genuinely semantic (<xref ref-type="bibr" rid="bibr29-0956797612438734">Waxman &amp; Gelman, 2009</xref>). Evidence suggests that infants conceive deictic signals, such as gazing and pointing, as having referential intent (e.g., <xref ref-type="bibr" rid="bibr25-0956797612438734">Senju &amp; Csibra, 2008</xref>), and they exploit such signals in word learning (e.g., <xref ref-type="bibr" rid="bibr15-0956797612438734">Hollich et al., 2000</xref>). Furthermore, 1-year-olds expect that concurrent verbal and nonverbal expressions from the same source refer to the same object, which suggests that they appreciate the referential nature of both pointing and words (<xref ref-type="bibr" rid="bibr12-0956797612438734">Gliga &amp; Csibra, 2009</xref>). However, some researchers have argued that early word understanding reflects simple associations between auditory and visual stimuli and not an appreciation of the referential and symbolic nature of words (<xref ref-type="bibr" rid="bibr23-0956797612438734">Robinson, Howard, &amp; Sloutsky, 2005</xref>).</p>
<p>Whether infants form stable word-object associations that reflect referential understanding of speech is not yet known. One way to test such understanding in infants is by measuring event-related potentials (ERPs), such as the N400 component. The N400 ERP component—a negative-going waveform that peaks approximately 400 ms after the onset of a stimulus—has been shown to reflect semantic priming in adults (<xref ref-type="bibr" rid="bibr18-0956797612438734">Kutas &amp; Hillyard, 1980</xref>). <xref ref-type="bibr" rid="bibr7-0956797612438734">Friedrich and Friederici (2004</xref>, <xref ref-type="bibr" rid="bibr8-0956797612438734">2005a</xref>, <xref ref-type="bibr" rid="bibr9-0956797612438734">2005b</xref>) found that adults and 19- and 14-month-olds, but not 12-month-olds, produced an enhanced N400 amplitude to words that were incongruous with picture primes. In a follow-up study, only ERPs from 12-month-olds who were assessed as high word producers showed an N400 effect (<xref ref-type="bibr" rid="bibr10-0956797612438734">Friedrich &amp; Friederici, 2010</xref>). The absence of an N400 effect in infants younger than 12 months was interpreted as a sign of merely associative word understanding, which does not entail semantic processing. Word learning requires the learner to form associations between word forms and visual referents. However, temporal associations alone do not imply semantic representation of word meaning. In the present study, we attempted to clarify exactly what infants learn when they form word-object associations: meaningless links or semantically meaningful sign-referent relations.</p>
<p>To shed light on whether young infants interpret words referentially and semantically, we developed a new paradigm that overcomes some shortcomings of earlier ERP studies with infants. In particular, we wanted to ensure that infants realized that the word they heard referred to the object they saw. Because both theoretical arguments (<xref ref-type="bibr" rid="bibr5-0956797612438734">Csibra, 2010</xref>) and empirical findings (e.g., <xref ref-type="bibr" rid="bibr26-0956797612438734">Senju, Csibra, &amp; Johnson, 2008</xref>) suggest that referential expectation is primarily elicited by ostensive communicative contexts in infants, we modified the picture-word priming paradigm (<xref ref-type="bibr" rid="bibr7-0956797612438734">Friedrich &amp; Friederici, 2004</xref>) to provide an optimal stimulus environment for 9-month-olds. First, we presented infants with live rather than prerecorded speech, which was occasionally accompanied by nonverbal referential gestures, such as pointing and gazing toward upcoming objects. Second, we used the words as primes and the objects as probes because a known word is more likely to activate the associated semantic representation in preverbal infants than a picture of an object is (cf. <xref ref-type="bibr" rid="bibr30-0956797612438734">Xu, 2007</xref>). Third, instead of flashing a picture on a monitor, the object appeared in a dynamic fashion from behind an occluder on a computer screen. Thus, on the one hand, live speech and interaction ensured optimal conditions for speech processing of the prime (<xref ref-type="bibr" rid="bibr16-0956797612438734">Kuhl, 2007</xref>), and on the other hand, video presentation of the target objects made it possible to accurately control stimulus variables for measuring ERPs. Finally, because young infants display a preference for maternal speech (e.g., <xref ref-type="bibr" rid="bibr4-0956797612438734">Cooper &amp; Aslin, 1989</xref>), we used the mother as the speaker in one of our two conditions. As auditorily presented words have rarely been used as primes for visual probes in ERP studies (but see <xref ref-type="bibr" rid="bibr19-0956797612438734">Nigam, Hoffman, &amp; Simons, 1992</xref>, for a different paradigm), we also tested whether, using this paradigm, we could find a reliable N400 effect in adults.</p>
<sec id="section1-0956797612438734" sec-type="methods">
<title>Method</title>
<sec id="section2-0956797612438734">
<title>Participants</title>
<p>Twelve adults (6 females, 6 males; average age = 38 years 8 months, range = 25 to 56 years) who were native speakers of Hungarian and 28 infants raised in Hungarian families participated in the study. Infants were assigned to two conditions: 14 to the mother-speech condition (5 females, 9 males; average age = 277 days, range = 269 to 286 days) and 14 to the experimenter-speech condition (6 females, 8 males; average age = 278 days, range = 266 to 285 days). One additional adult was excluded because of poor electroencephalogram (EEG) impedance, and 21 additional infants were excluded because of fussiness (<italic>n</italic> = 11), insufficient number of trials (<italic>n</italic> = 6), extensive body movements (<italic>n</italic> = 2), poor impedance (<italic>n</italic> = 1), and maternal interference (<italic>n</italic> = 1). (See the Supplemental Material available online for further participant details.)</p>
</sec>
<sec id="section3-0956797612438734">
<title>Stimuli</title>
<p>Using data collected from parental reports, we selected 15 object labels that two thirds of Hungarian 1-year-old infants recognize. We used corresponding pictures of objects to create 15 animated video clips showing an occluder dropping to reveal one of the objects (<xref ref-type="fig" rid="fig1-0956797612438734">Fig. 1</xref>; see Table S1 and Stimuli in the Supplemental Material for further details).</p>
<fig id="fig1-0956797612438734" position="float">
<label>Fig. 1.</label>
<caption>
<p>Illustration of the trial sequence for infants. While a moving fixation stimulus was presented in front of an occluder, the infant’s mother or an experimenter spoke a word or phrase that either named the object behind the occluder (congruous trial) or named some other object (incongruous trial). The fixation stimulus then stopped moving, and the display froze for 600 to 800 ms. After this, the fixation stimulus disappeared, and the occluder started to fall forward, revealing an object behind it. After 480 ms, the object was in full view, where it remained for 1,000 ms. Then the occluder began to rise, completely covering the object after 400 ms.</p>
</caption>
<graphic xlink:href="10.1177_0956797612438734-fig1.tif"/></fig>
</sec>
<sec id="section4-0956797612438734">
<title>Procedure</title>
<p>Adults sat 70 cm in front of a CRT monitor. In each trial, they heard a prerecorded word (the name of 1 of the 15 objects) from a loudspeaker behind the monitor while a dynamic fixation stimulus was presented on top of an occluder. The duration of the auditory stimulus was between 419 and 784 ms (average = 559 ms). After the auditory stimulus ended, the fixation stimulus stopped moving, and the display remained frozen for 600 to 800 ms. Then the fixation stimulus disappeared, and the occluder started to fall forward, revealing an object behind it. This phase lasted for 480 ms. The object was fully visible for 1,000 ms before the occluder began to rise, hiding the object again. This was followed by an intertrial interval lasting 1,100 to 1,300 ms. Participants were presented with 240 trials in 4 blocks. In half of the trials, the object corresponded to the preceding auditory word (congruous trials); in the other half, it did not (incongruous trials). Congruous and incongruous trials were presented equiprobably in pseudorandom order.</p>
<p>Infants sat on a high chair 70 cm in front of a CRT monitor. The infant’s mother and an experimenter sat on chairs at either side of the infant. The trial sequence (<xref ref-type="fig" rid="fig1-0956797612438734">Fig. 1</xref>) was the same as for adults, except that at the beginning of each trial, a word was presented over headphones either to the mother (mother-speech condition) or to the experimenter (experimenter-speech condition). She then repeated it (or uttered a phrase containing it) for the infant. Mothers were instructed to speak to the child as they would in everyday life, and they were allowed to gesture toward the monitor on which the occluder was seen. We asked them to utter the word at the very end of the phrase if they wanted to say more than just that word to their infant. In the experimenter-speech condition, the experimenter talked to the infant, attempting to reproduce the words, intonation, and style of a yoked mother from the mother-speech condition. In this design, the experimenter matched each individual mother from the mother-speech condition for an infant in the experimenter-speech condition.</p>
<p>Once the word was uttered and the infant attended the monitor, a second experimenter started a video clip revealing an object behind the occluder. Objects could be congruous or incongruous with the word prime, just as in the procedure for adult participants. Because the auditory stimulus was spoken live, the interstimulus interval between the word prime and the start of the visual stimulus varied across trials, averaging about 2,155 ms. Trials were presented as long as the infants were attentive. The position of the mother was counterbalanced across subjects in both conditions. The behavior of the infants was video-recorded throughout the session for off-line trial-by-trial editing and for additional behavioral scoring. (Further details of the procedure are reported in Experimental Setup in the Supplemental Material.)</p>
</sec>
<sec id="section5-0956797612438734">
<title>EEG recording and analysis</title>
<p>Continuous EEG was recorded by 128-channel Geodesic Sensor Nets at a sampling rate of 500 Hz. The EEG was segmented into 1,700-ms epochs starting 200 ms before the occluder began to fall. We considered Time 0 the frame when the object started to become visible from behind the occluder (160 ms after motion onset). EEG segments were averaged to separate ERPs for word-congruous and word-incongruous objects, baseline-corrected to the first 200 ms of the segments, and rereferenced to the average reference. On the basis of previous reports that found the N400 component over parietal sites (<xref ref-type="bibr" rid="bibr18-0956797612438734">Kutas &amp; Hillyard, 1980</xref>), we identified as regions of interest the electrodes between C3 and P3 and between C4 and P4, over the left and right hemisphere, respectively (<xref ref-type="fig" rid="fig2-0956797612438734">Fig. 2</xref>). On the basis of visual inspection of the grand averages and the existing literature on N400 latencies, we analyzed adults’ mean ERP amplitude between 350 and 500 ms after the object appeared and infants’ mean ERP amplitude between 500 and 650 ms after the object appeared (cf. <xref ref-type="bibr" rid="bibr7-0956797612438734">Friedrich &amp; Friederici, 2004</xref>, <xref ref-type="bibr" rid="bibr9-0956797612438734">2005b</xref>, for N400 time windows in adults and infants). (Further details of the EEG procedure are reported in EEG Acquisition and EEG Data Reduction in the Supplemental Material.)</p>
<fig id="fig2-0956797612438734" position="float">
<label>Fig. 2.</label>
<caption>
<p>Event-related potential (ERP) results. The figure shows grand-average waveforms on congruous and incongruous trials in left and right regions of interest (marked by black contours on the scalp maps). Results are shown separately for adults (top row), infants in the mother-speech condition (middle row), and infants in the experimenter-speech condition (bottom row). The gray shading indicates the time window of the N400 (350–500 ms in adults and 500–650 ms in infants), and the vertical line marks the time at which the object in each trial appeared from behind an occluder. The scalp maps depict the spatial distribution of the difference in ERP amplitude between incongruous and congruous trials in the given time windows.</p>
</caption>
<graphic xlink:href="10.1177_0956797612438734-fig2.tif"/>
</fig>
</sec>
</sec>
<sec id="section6-0956797612438734" sec-type="results">
<title>Results</title>
<p><xref ref-type="fig" rid="fig2-0956797612438734">Figure 2</xref> shows grand-average ERP results for adults and infants. An analysis of variance (ANOVA) on the data from adults, with congruency (congruous object vs. incongruous object) and hemisphere (left vs. right) as within-subjects factors, revealed that incongruous objects elicited a more negative N400 amplitude than did congruous objects, <italic>F</italic>(1, 11) = 7.50, <italic>p</italic> = .02, η<sub><italic>p</italic></sub><sup>2</sup> = .41. Although the effect was bigger on the right side, the interaction between congruity and hemisphere was not significant. This result demonstrates that a semantic congruency effect on the N400 component can be elicited when an auditory word prime precedes an object image.</p>
<p>On average, infants contributed 20.3 congruous trials and 20.3 incongruous trials in the mother-speech condition, and 19.4 congruous trials and 19.9 incongruous trials in the experimenter-speech condition. An ANOVA on the infants’ N400 amplitudes, with congruency and hemisphere as within-subjects factors and condition (mother speech vs. experimenter speech) as a between-subjects factor, revealed an interaction between congruency and condition, <italic>F</italic>(1, 26) = 5.02, <italic>p</italic> = .03, η<sub><italic>p</italic></sub><sup>2</sup> = .16. Two-way follow-up ANOVAs with congruency and hemisphere as factors revealed that this interaction was due to the fact that the N400 amplitude was more negative in response to the incongruous than to the congruous object in the mother-speech condition, <italic>F</italic>(1, 13) = 5.45, <italic>p</italic> = .036, η<sub><italic>p</italic></sub><sup>2</sup> = .30, but not in the experimenter-speech condition, <italic>F</italic>(1, 13) = 0.29, <italic>p</italic> = .598, η<sub><italic>p</italic></sub><sup>2</sup> = .02. A congruency-by-hemisphere interaction in the three-way ANOVA also indicated that the effect was more pronounced over the right hemisphere than over the left hemisphere, <italic>F</italic>(1, 26) = 4.39, <italic>p</italic> = .05, η<sub><italic>p</italic></sub><sup>2</sup> = .15.</p>
<p>To account for the difference in N400 amplitude between the conditions, we took five behavioral measures from infants: the number of times the speaker repeated the object name within a trial, whether the infant was looking to the mother or to the experimenter during the speech, and whether the infant looked to the mother or to the experimenter after seeing the object. Although we tried to match the mothers’ behavior in the experimenter-speech condition, mothers uttered the prime word slightly, but significantly, more than the experimenter did (1.07 times vs. 1.01 times), <italic>F</italic>(1, 26) = 13.67, <italic>p</italic> = .001, η<sub><italic>p</italic></sub><sup>2</sup> = .345. As a control, we reran the statistics on ERPs that included only trials in which the word was uttered once in the mother-speech condition, and we obtained the same pattern of results as before. We did not find a significant difference between conditions in the number of times infants looked to the mother or to the experimenter. However, infants in the mother-speech condition tended to look more toward the mother than those in the other condition did, <italic>F</italic>(1, 26) = 3.74, <italic>p</italic> = .064, η<sub><italic>p</italic></sub><sup>2</sup> = .126. There was no effect of any factor on how many times infants looked at the mother or at the experimenter after having seen the object. (See Additional Analyses in the Supplemental Material for more information.)</p>
</sec>
<sec id="section7-0956797612438734" sec-type="discussion">
<title>Discussion</title>
<p>Our results suggest that infants as young as 9 months have a rudimentary receptive vocabulary. This has been previous- ly suspected (<xref ref-type="bibr" rid="bibr28-0956797612438734">Swingley, 2008</xref>) but difficult to prove. In agreement with our findings, the results of a recent eye-tracker study by <xref ref-type="bibr" rid="bibr3-0956797612438734">Bergelson and Swingley (2012)</xref> showed that 6- to 9-month-olds can follow their mother’s verbal instructions and direct their gaze to objects. Six-month-old infants are also able to match novel words with arbitrary visual referents in a few trials by taking advantage of prosodic cues (<xref ref-type="bibr" rid="bibr11-0956797612438734">Friedrich &amp; Friederici, 2011</xref>; <xref ref-type="bibr" rid="bibr27-0956797612438734">Shukla et al., 2011</xref>). However, the electrophysiological signs of semantic priming disappeared after 24 hr, which suggests strong limitations in memory processes (<xref ref-type="bibr" rid="bibr11-0956797612438734">Friedrich &amp; Friederici, 2011</xref>). Our study shows that word-to-object priming occurs in 9-month-olds with familiar words when there is no requirement to learn new ones. Our method does not allow us to tell which infant understood which word, but infants in the mother-speech condition, as a group, seem to have activated the object features associated with the highly familiar words their mother uttered and matched them with the image that appeared on the screen in front of them. In this sense, infants in our study understood their mother’s speech.</p>
<p>What is the nature of this understanding? In particular, is it possible that the word knowledge that infants displayed in this experiment goes beyond the formation of merely associative links between auditory and visual information, and reflects truly referential and semantic understanding of nouns? The kind of neuronal activation that we demonstrated here is correlational in nature and does not support unambiguous conclusions about the underlying processes. Nevertheless, three aspects of our results suggest that 9-month-olds appreciate the referential nature of words.</p>
<p>First, the N400 component is commonly interpreted to reflect semantic processing by exhibiting lower amplitude to stimuli semantically primed (rather than grammatically or associatively primed) by the context (<xref ref-type="bibr" rid="bibr17-0956797612438734">Kutas &amp; Federmeier, 2011</xref>). The differential N400 responses to congruent and incongruent object images thus indicate that the words activated neural processes in infants that are correlated with the extraction of meaning from stimuli. Second, we found these effects despite the relatively long delay (&gt; 2 s on average) between the uttered word and the appearance of the object. Earlier studies, which used synchronous presentation (with the object being visible during the presentation of the word), should have had a better chance at demonstrating associative audiovisual links, but they failed to do so (e.g., <xref ref-type="bibr" rid="bibr9-0956797612438734">Friedrich &amp; Friederici, 2005b</xref>). Although pure associations could bridge temporal delays, they should work best with contiguous stimuli. In contrast, words can refer to absent referents, as they did in our study. Thus, finding differential brain activations for matches and mismatches between temporally separated stimuli supports the interpretation that a semantic, rather than an associative, link was formed between them.</p>
<p>Third, the success of this study was partly due to our effort to set up a situation in which infants had every reason to expect semantically interpretable referential expressions. Infants pay special attention to ostensive communicative signals, such as eye contact (<xref ref-type="bibr" rid="bibr21-0956797612438734">Parise, Reid, Stets, &amp; Striano, 2008</xref>) and their own name (<xref ref-type="bibr" rid="bibr20-0956797612438734">Parise, Friederici, &amp; Striano, 2010</xref>), as well as deictic referential signals, such as gaze (<xref ref-type="bibr" rid="bibr14-0956797612438734">Hoehl, Wiese, &amp; Striano, 2008</xref>) and pointing (<xref ref-type="bibr" rid="bibr13-0956797612438734">Gredeback &amp; Melinder, 2010</xref>), from early on. Ostensive communication has been suggested to generate referential expectation (<xref ref-type="bibr" rid="bibr5-0956797612438734">Csibra, 2010</xref>) and expectation of coreference for concurrent referential signals (<xref ref-type="bibr" rid="bibr12-0956797612438734">Gliga &amp; Csibra, 2009</xref>). Although it is not clear why ostensive referential signals would facilitate associative processes, they could support the inference that the words infants hear are semantically related to the object that would appear at the location referred to by the speaker. Thus, our paradigm, which closely resembles the natural joint-attention interaction in which infants at this age are regularly engaged with their parents (<xref ref-type="bibr" rid="bibr1-0956797612438734">Bakeman &amp; Adamson, 1984</xref>), provided an optimal environment for measuring the effect of semantic priming in young infants.</p>
<p>Our results suggest that adopting the mother as the speaker also contributed to this optimal environment. Newborns prefer their mother’s voice to a stranger’s voice (<xref ref-type="bibr" rid="bibr6-0956797612438734">DeCasper &amp; Fifer, 1980</xref>), and recent ERP research has confirmed that at the age of 4 months, infants respond faster and allocate more attention to their mother’s voice than to an unfamiliar voice (<xref ref-type="bibr" rid="bibr22-0956797612438734">Purhonen, Kilpeläinen-Lees, Valkonen-Korhonen, Karhu, &amp; Lehtonen, 2005</xref>). In addition, the mother’s voice elicits more activation in language-relevant brain areas in newborns than a stranger’s voice does (<xref ref-type="bibr" rid="bibr2-0956797612438734">Beauchemin et al., 2011</xref>). Our paradigm did not allow us to pinpoint the exact factors that made infants more responsive to the mother’s than to the experimenter’s communication. It could be that the familiar voice, intonation, or verbal expression (e.g., “Look, a duck!” vs. “Here comes the duck!”) helped them to recognize the situation as a naming game. It is also possible that 9-month-old infants had difficulties in recognizing the target word in the slightly different phonetic production by the experimenter. Because the speaker sat next to the infants and did not always make eye contact with them during her speech, they might not have recognized from the experimenter’s intonation alone that they were being addressed; however, the mother’s voice alone could have achieved this effect.</p>
<p>Further studies will be needed to test at what age or under what conditions infants detect semantic violations in a stranger’s speech. Nevertheless, the functional nature of the N400 component, and the similarity of the effects we found in adults and infants, suggests semantic priming by, and referential understanding of, familiar words at 9 months of age. This finding supports the view that the referential nature of speech may not have to be learned by human infants, but it may be expected and exploited by them during language acquisition.</p>
</sec>
</body>
<back>
<ack>
<p>We thank B. Kollod, M. Toth, and A. Volein for assistance, and H. Bortfeld, M. Chen, M. Friederich, A. D. Friederici, T. Gliga, A. M. Kovacs, and O. Mascaro for discussions.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This work was supported by a European Research Council Advanced Investigator Grant (OSTREFCOM) to Gergely Csibra.</p>
</fn>
<fn fn-type="supplementary-material">
<label>Supplemental Material</label>
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bakeman</surname><given-names>R.</given-names></name>
<name><surname>Adamson</surname><given-names>L. B.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Coordinating attention to people and objects in mother-infant and peer-infant interaction</article-title>. <source>Child Development</source>, <volume>55</volume>, <fpage>1278</fpage>–<lpage>1289</lpage>.</citation>
</ref>
<ref id="bibr2-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Beauchemin</surname><given-names>M.</given-names></name>
<name><surname>González-Frankenberger</surname><given-names>B.</given-names></name>
<name><surname>Tremblay</surname><given-names>J.</given-names></name>
<name><surname>Vannasing</surname><given-names>P.</given-names></name>
<name><surname>Martínez-Montes</surname><given-names>E.</given-names></name>
<name><surname>Belin</surname><given-names>P.</given-names></name>
<name><surname>. . . Lassonde</surname><given-names>M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Mother and stranger: An electrophysiological study of voice processing in newborns</article-title>. <source>Cerebral Cortex</source>, <volume>21</volume>, <fpage>1705</fpage>–<lpage>1711</lpage>.</citation>
</ref>
<ref id="bibr3-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bergelson</surname><given-names>E.</given-names></name>
<name><surname>Swingley</surname><given-names>D.</given-names></name>
</person-group> (<year>2012</year>). <article-title>At 6–9 months, human infants know the meanings of many common nouns</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>109</volume>, <fpage>3253</fpage>–<lpage>3258</lpage>.</citation>
</ref>
<ref id="bibr4-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cooper</surname><given-names>R. P.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
</person-group> (<year>1989</year>). <article-title>The language environment of the young infant: Implications for early perceptual development</article-title>. <source>Canadian Journal of Psychology</source>, <volume>43</volume>, <fpage>247</fpage>–<lpage>265</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Csibra</surname><given-names>G.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Recognizing communicative intentions in infancy</article-title>. <source>Mind &amp; Language</source>, <volume>25</volume>, <fpage>141</fpage>–<lpage>168</lpage>.</citation>
</ref>
<ref id="bibr6-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>DeCasper</surname><given-names>A. J.</given-names></name>
<name><surname>Fifer</surname><given-names>W. P.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Of human bonding: Newborns prefer their mothers’ voices</article-title>. <source>Science</source>, <volume>208</volume>, <fpage>1174</fpage>–<lpage>1176</lpage>.</citation>
</ref>
<ref id="bibr7-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Friedrich</surname><given-names>M.</given-names></name>
<name><surname>Friederici</surname><given-names>A. D.</given-names></name>
</person-group> (<year>2004</year>). <article-title>N400-like semantic incongruity effect in 19-month-olds: Processing known words in picture contexts</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>16</volume>, <fpage>1465</fpage>–<lpage>1477</lpage>.</citation>
</ref>
<ref id="bibr8-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Friedrich</surname><given-names>M.</given-names></name>
<name><surname>Friederici</surname><given-names>A. D.</given-names></name>
</person-group> (<year>2005a</year>). <article-title>Lexical priming and semantic integration reflected in the event-related potential of 14-month-olds</article-title>. <source>NeuroReport</source>, <volume>16</volume>, <fpage>653</fpage>–<lpage>656</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Friedrich</surname><given-names>M.</given-names></name>
<name><surname>Friederici</surname><given-names>A. D.</given-names></name>
</person-group> (<year>2005b</year>). <article-title>Phonotactic knowledge and lexical-semantic processing in one-year-olds: Brain responses to words and nonsense words in picture contexts</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>17</volume>, <fpage>1785</fpage>–<lpage>1802</lpage>.</citation>
</ref>
<ref id="bibr10-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Friedrich</surname><given-names>M.</given-names></name>
<name><surname>Friederici</surname><given-names>A. D.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Maturing brain mechanisms and developing behavioral language skills</article-title>. <source>Brain and Language</source>, <volume>114</volume>, <fpage>66</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr11-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Friedrich</surname><given-names>M.</given-names></name>
<name><surname>Friederici</surname><given-names>A. D.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Word learning in 6-month-olds: Fast encoding–weak retention</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>23</volume>, <fpage>3228</fpage>–<lpage>3240</lpage>.</citation>
</ref>
<ref id="bibr12-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gliga</surname><given-names>T.</given-names></name>
<name><surname>Csibra</surname><given-names>G.</given-names></name>
</person-group> (<year>2009</year>). <article-title>One-year-old infants appreciate the referential nature of deictic gestures and words</article-title>. <source>Psychological Science</source>, <volume>20</volume>, <fpage>347</fpage>–<lpage>353</lpage>.</citation>
</ref>
<ref id="bibr13-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gredeback</surname><given-names>G.</given-names></name>
<name><surname>Melinder</surname><given-names>A.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The development and neural basis of pointing comprehension</article-title>. <source>Social Neuroscience</source>, <volume>5</volume>, <fpage>441</fpage>–<lpage>450</lpage>.</citation>
</ref>
<ref id="bibr14-0956797612438734">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Hoehl</surname><given-names>S.</given-names></name>
<name><surname>Wiese</surname><given-names>L.</given-names></name>
<name><surname>Striano</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Young infants’ neural processing of objects is affected by eye gaze direction and emotional expression</article-title>. <source>PLoS ONE</source>, <volume>3</volume>(<issue>6</issue>), <fpage>e2389</fpage>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/article/info:doi%2F10.1371%2Fjournal.pone.0002389">http://www.plosone.org/article/info:doi%2F10.1371%2Fjournal.pone.0002389</ext-link></comment></citation>
</ref>
<ref id="bibr15-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hollich</surname><given-names>G. J.</given-names></name>
<name><surname>Hirsh-Pasek</surname><given-names>K.</given-names></name>
<name><surname>Golinkoff</surname><given-names>R. M.</given-names></name>
<name><surname>Brand</surname><given-names>R. J.</given-names></name>
<name><surname>Brown</surname><given-names>E.</given-names></name>
<name><surname>Chung</surname><given-names>H. L.</given-names></name>
<name><surname>. . . Rocoi</surname><given-names>C.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Breaking the language barrier: An emergentist coalition model for the origins of word learning</article-title> [<comment>Target article and commentary</comment>]. <source>Monographs of the Society for Research in Child Development</source>, <volume>65</volume>, <fpage>i</fpage>–<lpage>vi</lpage>, <fpage>1</fpage>–<lpage>123</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhl</surname><given-names>P. K.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Is speech learning “gated” by the social brain?</article-title> <source>Developmental Science</source>, <volume>10</volume>, <fpage>110</fpage>–<lpage>120</lpage>.</citation>
</ref>
<ref id="bibr17-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kutas</surname><given-names>M.</given-names></name>
<name><surname>Federmeier</surname><given-names>K. D.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Thirty years and counting: Finding meaning in the N400 component of the event-related brain potential (ERP)</article-title>. <source>Annual Review of Psychology</source>, <volume>62</volume>, <fpage>621</fpage>–<lpage>647</lpage>.</citation>
</ref>
<ref id="bibr18-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kutas</surname><given-names>M.</given-names></name>
<name><surname>Hillyard</surname><given-names>S. A.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Reading senseless sentences: Brain potentials reflect semantic incongruity</article-title>. <source>Science</source>, <volume>207</volume>, <fpage>203</fpage>–<lpage>205</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nigam</surname><given-names>A.</given-names></name>
<name><surname>Hoffman</surname><given-names>J.</given-names></name>
<name><surname>Simons</surname><given-names>R.</given-names></name>
</person-group> (<year>1992</year>). <article-title>N400 to semantically anomalous pictures and words</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>4</volume>, <fpage>15</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr20-0956797612438734">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Parise</surname><given-names>E.</given-names></name>
<name><surname>Friederici</surname><given-names>A. D.</given-names></name>
<name><surname>Striano</surname><given-names>T.</given-names></name>
</person-group> (<year>2010</year>). <article-title>“Did you call me?” 5-month-old infants own name guides their attention</article-title>. <source>PLoS One</source>, <volume>5</volume>(<issue>12</issue>), <fpage>e14208</fpage>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0014208">http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0014208</ext-link></comment></citation>
</ref>
<ref id="bibr21-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parise</surname><given-names>E.</given-names></name>
<name><surname>Reid</surname><given-names>V. M.</given-names></name>
<name><surname>Stets</surname><given-names>M.</given-names></name>
<name><surname>Striano</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Direct eye contact influences the neural processing of objects in 5-month-old infants</article-title>. <source>Social Neuroscience</source>, <volume>3</volume>, <fpage>141</fpage>–<lpage>150</lpage>.</citation>
</ref>
<ref id="bibr22-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Purhonen</surname><given-names>M.</given-names></name>
<name><surname>Kilpeläinen-Lees</surname><given-names>R.</given-names></name>
<name><surname>Valkonen-Korhonen</surname><given-names>M.</given-names></name>
<name><surname>Karhu</surname><given-names>J.</given-names></name>
<name><surname>Lehtonen</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Four-month-old infants process own mother’s voice faster than unfamiliar voices: Electrical signs of sensitization in infant brain</article-title>. <source>Cognitive Brain Research</source>, <volume>24</volume>, <fpage>627</fpage>–<lpage>633</lpage>.</citation>
</ref>
<ref id="bibr23-0956797612438734">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Robinson</surname><given-names>C. W.</given-names></name>
<name><surname>Howard</surname><given-names>E. M.</given-names></name>
<name><surname>Sloutsky</surname><given-names>V. M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The nature of early word comprehension: Symbols or associations?</article-title> In <person-group person-group-type="editor">
<name><surname>Bara</surname><given-names>B. G.</given-names></name>
<name><surname>Barsalou</surname><given-names>L.</given-names></name>
<name><surname>Bucciarelli</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the 27th Annual Conference of the Cognitive Science Society</source> (pp. <fpage>1883</fpage>–<lpage>1888</lpage>). <conf-loc>Mahwah, NJ</conf-loc>: <conf-name>Erlbaum</conf-name>.</citation>
</ref>
<ref id="bibr24-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saffran</surname><given-names>J. R.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
<name><surname>Newport</surname><given-names>E. L.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source>, <volume>274</volume>, <fpage>1926</fpage>–<lpage>1928</lpage>.</citation>
</ref>
<ref id="bibr25-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Senju</surname><given-names>A.</given-names></name>
<name><surname>Csibra</surname><given-names>G.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Gaze following in human infants depends on communicative signals</article-title>. <source>Current Biology</source>, <volume>18</volume>, <fpage>668</fpage>–<lpage>671</lpage>.</citation>
</ref>
<ref id="bibr26-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Senju</surname><given-names>A.</given-names></name>
<name><surname>Csibra</surname><given-names>G.</given-names></name>
<name><surname>Johnson</surname><given-names>M. H.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Understanding the referential nature of looking: Infants’ preference for object-directed gaze</article-title>. <source>Cognition</source>, <volume>108</volume>, <fpage>303</fpage>–<lpage>319</lpage>.</citation>
</ref>
<ref id="bibr27-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shukla</surname><given-names>M.</given-names></name>
<name><surname>White</surname><given-names>K. S.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Prosody guides the rapid mapping of auditory word forms onto visual objects in 6-mo-old infants</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>108</volume>, <fpage>6038</fpage>–<lpage>6043</lpage>.</citation>
</ref>
<ref id="bibr28-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Swingley</surname><given-names>D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The roots of the early vocabulary in infants’ learning from speech</article-title>. <source>Current Directions in Psychological Science</source>, <volume>17</volume>, <fpage>308</fpage>–<lpage>311</lpage>.</citation>
</ref>
<ref id="bibr29-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Waxman</surname><given-names>S. R.</given-names></name>
<name><surname>Gelman</surname><given-names>S. A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Early word-learning entails reference, not merely associations</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>13</volume>, <fpage>258</fpage>–<lpage>263</lpage>.</citation>
</ref>
<ref id="bibr30-0956797612438734">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Xu</surname><given-names>F.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Sortal concepts, object individuation, and language</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>11</volume>, <fpage>400</fpage>–<lpage>406</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>