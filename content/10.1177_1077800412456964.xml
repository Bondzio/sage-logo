<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">QIX</journal-id>
<journal-id journal-id-type="hwp">spqix</journal-id>
<journal-title>Qualitative Inquiry</journal-title>
<issn pub-type="ppub">1077-8004</issn>
<issn pub-type="epub">1552-7565</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1077800412456964</article-id>
<article-id pub-id-type="publisher-id">10.1177_1077800412456964</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Weaving a Multimethodology and Mixed Methods Praxis Into Randomized Control Trials to Enhance Credibility</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Hesse-Biber</surname><given-names>Sharlene</given-names></name>
<xref ref-type="aff" rid="aff1-1077800412456964">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1077800412456964"><label>1</label>Boston College, Chestnut Hill, MA, USA</aff>
<author-notes>
<corresp id="corresp1-1077800412456964">Sharlene Hesse-Biber, PhD, Department of Sociology, Boston College, 140 Commonwealth Avenue, 419 McGuinn Hall, Chestnut Hill, MA 02467, USA Email: <email>hesse@bc.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>18</volume>
<issue>10</issue>
<fpage>876</fpage>
<lpage>889</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Most disciplines within the health and social sciences regard randomized control trials (RCTs) as the “gold standard” of evidence-based practice (EBP). The move toward mixed methods within evidence-based research has proven daunting to many researchers, and few best practices for RCT mixed methods studies currently exist. This article provides some strategies for incorporating mixed methods into RCT designs. Furthermore, the author argues for the value of also infusing a multimethodological approach into RCT mixed methods projects to further offer research strategies for enhancing the credibility of RCT research findings through, for example, incorporating participants’ lived experiences and methodological reflexivity into the research process. The bulk of this article presents four case studies that analyze how researchers in diverse fields have taken a multimethodological praxis into account in their RCT mixed methods projects, including the integration of a mixed methods multimethodological component into RCT research designs. The author also addresses the missed opportunities in these studies to maximize the validity of RCT projects by using mixed methods and multimethodological designs.</p>
</abstract>
<kwd-group>
<kwd>randomized</kwd>
<kwd>control trials</kwd>
<kwd>mixed methods</kwd>
<kwd>multimethodology</kwd>
<kwd>credibility</kwd>
<kwd>evidence-based practice</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1077800412456964" sec-type="intro">
<title>Introduction</title>
<p>There is strong faith within the health sciences and increasingly within the social sciences regarding the efficacy of evidence-based practices (EBP) (<xref ref-type="bibr" rid="bibr6-1077800412456964">Donaldson, 2009</xref>, p. 5; <xref ref-type="bibr" rid="bibr27-1077800412456964">Sackett, Rosenberg, Gray, Haynes, &amp; Richardson, 1996</xref>) as the “conscientious, explicit and judicious use of current best evidence in making decisions about the care of individual patients” (<xref ref-type="bibr" rid="bibr6-1077800412456964">Donaldson, 2009</xref>, p. 71). Because the randomized control trial (RCT) design was considered “much more likely to inform us and so much less likely to mislead us, it has become the ‘gold standard’ for judging whether a treatment does more good than harm” (p. 72). Researchers have since widened their definition of current best evidence to include the integration of patient values in the decision-making practice (<xref ref-type="bibr" rid="bibr28-1077800412456964">Sackett, Straus, Richardson, Rosenberg, &amp; Haynes, 1997</xref>). These elements of EBP are depicted in <xref ref-type="fig" rid="fig1-1077800412456964">Figure 1</xref>.</p>
<fig id="fig1-1077800412456964" position="float">
<label>Figure 1.</label>
<caption><p>Components of evidence-based practice (<xref ref-type="bibr" rid="bibr27-1077800412456964">Sackett, Rosenberg, Gray, Haynes, &amp; Richardson, 1996</xref>; <xref ref-type="bibr" rid="bibr28-1077800412456964">Sackett, Straus, Richardson, Rosenberg, &amp; Haynes, 1997</xref>)</p></caption>
<graphic xlink:href="10.1177_1077800412456964-fig1.tif"/>
</fig>
<p>However, over the past several decades, two important elements of EBPs, depicted in <xref ref-type="fig" rid="fig1-1077800412456964">Figure 1</xref>, are increasingly excluded from the EBP praxis model—clinical judgment and expertise, and patient expertise and values—leaving the praxis of EBP hyperfocused on a quantitative/experimental approach. An RCT design allows the researcher to “test” a particular intervention in order to estimate its degree of effectiveness. EBP privileges a positivistic epistemology of knowledge building whose basic foundation is an ontology that assumes social reality is “out there” and that “truth” or “validity” can be captured. EBP’s method of capturing this social reality is through the careful praxis of the scientific method that emphasizes scientific objectivity, a practice whereby the researcher refrains from allowing his or her values and attitudes to enter into the research setting.</p>
<p>The RCT has increasingly become the research design of choice in EBP across disciplines largely because among the research and policy-making communities it is thought to provide the best method for assessing issues of “cause and effect” while at the same time guarding against threats to the internal validity of the research trial through such processes as the statistical randomization of research participants. Policy makers expect researchers to utilize this model because it provides research outcomes that can estimate the extent to which a given intervention is successful by using quantitative measures to track success rates (<xref ref-type="bibr" rid="bibr1-1077800412456964">Bickman &amp; Reich, 2009</xref>).</p>
<p>A standard RCT design consists of setting up one or more “control” (nontreatment group) and “experimental” groups (treatment group) to which individuals are randomly allocated in order to reduce selection bias as well as allow for the measuring of statistical differences using probability theory to determine significant differences not caused by chance. In response to criticisms that RCT designs do not take into account the complexity of research participants’ experiences or clinical expertise, coupled with criticisms that it is difficult to transfer RCT research findings from an experimental laboratory setting into the “real world” of clinical practice, there has been a movement to incorporate a qualitative component into RCT research designs that may assist in addressing these criticisms as well as enhancing the credibility of research findings within a wider social context.</p>
<sec id="section2-1077800412456964">
<title>Enhancing the Credibility of EBP: Moving RCT Toward a Mixed Methods Design</title>
<p>In the mid-1990s <xref ref-type="bibr" rid="bibr29-1077800412456964">Margarete Sandelowski (1996)</xref> suggested that the inclusion of a qualitative component would extend the application of RCT laboratory findings to naturalistic settings by specifically noting how the addition of a qualitative component into RCT could facilitate an understanding of how RCT findings can be applicable in real-world clinical settings. She asserts:<disp-quote>
<p>Qualitative methods can be used to enhance the significance and harness the benefits of clinical trials, and to emphasize the distinctive work and outcomes of nursing care. Qualitative methods are especially useful for further describing or explaining subject variation on outcome variables, verifying outcomes obtained from standardized instruments, and clarifying and evaluating interventions in their real-life contexts. (p. 359)</p>
</disp-quote></p>
<p>There are few studies, however, that analyze exactly how adding a qualitative component and creating a mixed methods RCT design will in fact serve to enhance the overall credibility of the evidence gathered. In a content analysis of mixed methods articles, <xref ref-type="bibr" rid="bibr2-1077800412456964">Bryman (2006)</xref> noted that most often the qualitative and quantitative components of a mixed methods project remain separated and the mixing tends to only take place at the data collection stage of a project. <xref ref-type="bibr" rid="bibr3-1077800412456964">Bryman (2007)</xref> found that researchers gave several reasons for doing so. At the top of the list was that researchers expressed confusion as to how to actually integrate these methods as well as a lack of skills to do so.</p>
<p>Bryman’s findings appear to cast doubt on the early a priori synergistic claims made with regard to the application of mixed methods designs. Synergy has been defined by some as “the idea that two or more options interact so that their combined effect is greater than the sum of their individual effects” (<xref ref-type="bibr" rid="bibr9-1077800412456964">Hall &amp; Howard, 2008</xref>, pp. 250-251). Some researchers suggests that there is an inherent synergy that exists when two methods are combined that allows the researcher “build on the synergy and strength that exists between quantitative and qualitative research methods in order to understand a phenomenon more fully than is possible using either quantitative or qualitative methods alone” (<xref ref-type="bibr" rid="bibr7-1077800412456964">Gay, Mills, &amp; Airasian, 2006</xref>, p. 490).</p>
<p>The early work of <xref ref-type="bibr" rid="bibr8-1077800412456964">Greene, Caracelli, and Graham (1989)</xref> and others (e.g., <xref ref-type="bibr" rid="bibr30-1077800412456964">Sieber, 1973</xref>) provided a useful analytical framework that describes the specific ways in which combining methods could in fact create a synergistic research. Nevertheless, there remain few empirical studies that detail the “how-to’s” of actually integrating mixed methods into research projects. This is especially the case when incorporating a qualitative research component into an experimental research model such as RCT. Those RCT projects that do incorporate qualitative methods into their design treat the qualitative component as marginalized with the subjective approach truncated from the qualitative method used such that the role of qualitative method is one of enhancing the interpretation or confirming the interpretation of RCT (<xref ref-type="bibr" rid="bibr10-1077800412456964">Howe, 2004</xref>).</p>
<p>The popularity of RCT designs in evidence-based practice has primarily rested on the unquestioned belief in a hierarchy of evidence that places RCT research methods at the top as the gold standard in knowledge-building designs. Yet, as RCT designs are expanding within the social sciences and research is increasingly moving out of the laboratory into real-world settings, it becomes more difficult for clinical trial research to assess the effectiveness of clinical interventions or what specific interventions are most appropriate to introduce into a research trial within these settings. In addition, critics of EBP contend that sole reliance on an RCT design strips away a deeper understanding of the social context within which research participants reside and argue that it is not clear just how research findings from an experimental setting translate into everyday life. The method itself is seen to exclude issues of difference among research participants with regard to gender, race, and social class, as well as other differences that are often critical to the understanding of the effects of interventions in diverse clinical populations (<xref ref-type="bibr" rid="bibr23-1077800412456964">Rogers, 2004a</xref>, <xref ref-type="bibr" rid="bibr24-1077800412456964">2004b</xref>). <xref ref-type="bibr" rid="bibr25-1077800412456964">Rogers and Ballantyne (2008)</xref> note that very often the research problems investigated serve to reinforce common stereotypes, especially about women and other oppressed groups. For example, a large majority of clinical trials that focus on women’s health issues center primarily on their reproductive health (i.e., fertility and menopause) but not on other medical conditions that have high mortality rates for women, such as cardiovascular disease. There is no attempt to account for differences among women with regard to how their gender intersects with their race and class, but instead the category of “woman” is treated as an essentialist category, furthering the stereotyping process (<xref ref-type="bibr" rid="bibr26-1077800412456964">Rogers &amp; Ballantyne, 2009</xref>).</p>
<p>RCT designs linked to the growing movement in the social sciences toward more “evidence-based” research practice. In the field of education, for example, several government mandates such as the No Child Left Behind (NCLB) Act of 2001 as well as the Education Sciences Reform Act of 2002 now require federally funded projects mandated by these acts to contain an evidence-based positivistic research design whose systematic approach promises to ensure rigor and objectivity in its findings. Such a definition of scientific inquiry, evidence, and rigor in social research is strongly criticized as very narrow and that in pursuing such a view of science serves to shut down a range of alternative inquiries that are no less rigorous and systematic. Qualitative researchers are challenging the “gold standard” mentality by pointing out how this narrow form of knowledge building is also a way to promote the views of those knowledge brokers whose interests are in maintaining the status quo (see <xref ref-type="bibr" rid="bibr14-1077800412456964">Lather, 2004</xref>; <xref ref-type="bibr" rid="bibr34-1077800412456964">St. Pierre, 2002</xref>).</p>
<p>To this end, we will present a set of case studies that provide strategies for weaving a mixed methods and multimethodology approach into RCT designs that serve to enhance the overall credibility of RCT design evidence.</p>
</sec>
</sec>
<sec id="section3-1077800412456964">
<title>Weaving a Multimethodology and Mixed Methods Praxis Into RCT Research Designs</title>
<p>The overall research process has two important components that are critical to link when assessing the credibility of any research evidence. The first is termed the “context of discovery,” in which the researcher asks specific questions. The second is the “context of justification,” which consists of the project-specific measures and methods tools used by the researcher to answer a question or a set of questions (<xref ref-type="bibr" rid="bibr33-1077800412456964">Sprague &amp; Zimmerman, 1993</xref>).</p>
<p>It is within the problem formation and question-asking stage of research that RCT researchers’ values and attitudes first enter the research process. Being cognizant of our research’s methodological standpoint requires a careful attention to axiological practice. Axiology means being cognizant of our values, attitudes, and biases and acknowledging how these might play out in research praxis in terms of (a) what questions are asked or not asked in our research, (b) what type of data are or are not collected, and (c) the type of methods, measurement, analysis, and interpretation that shape our understanding of the research process (<xref ref-type="bibr" rid="bibr12-1077800412456964">Hesse-Biber, 2010</xref>). For a further discussion of axiology, see <xref ref-type="bibr" rid="bibr12-1077800412456964">Hesse-Biber 2010</xref>. This relationship can be explored with a helpful model. We can see in <xref ref-type="fig" rid="fig2-1077800412456964">Figure 2</xref> that the research question implies a particular worldview or paradigmatic standpoint—the set of assumptions researchers make about the nature of the social world (ontology) and who can know this world (epistemology). This paradigmatic viewpoint lends itself to the selection of particular types of methodologies. These paradigmatic assumptions are either conscious (the solid arrow) or unconscious (the broken arrow). The types of methods a researcher selects and the analysis, and interpretation of data also are shaped by the researcher’s personal values and biases.</p>
<fig id="fig2-1077800412456964" position="float">
<label>Figure 2.</label>
<caption><p>Axiology and research praxis (adapted from <xref ref-type="bibr" rid="bibr12-1077800412456964">Hesse-Biber, 2010</xref>)</p></caption>
<graphic xlink:href="10.1177_1077800412456964-fig2.tif"/>
</fig>
<p>Very often there is a lack of researcher reflexivity with regard to his or her own methodological stance toward his or her research project. It is the methodological perspective that guides what questions are formulated and, with regard to RCT designs, what interventions are selected for a given RCT project. What is often stressed in RCT designs is the power of the experimental method alone in its capacity as a method to obtain credible evidence. This type of thinking tends to delink the context of discovery (methodology) from the context of justification (methods techniques). RCT designs derive their methodology from a positivistic approach whose goal is the pursuit of “the truth” through the practice of the scientific method whose procedures insure a value-free, objective inquiry and a valid outcome. So, for example, the question that is most often asked by RCT researchers is, “Did the intervention work?” This question calls for a more or less a binary answer—“yes or no?”—that is usually followed by a quantitative measurement that states the degree of effectiveness. This answer seems logical if one were taking on a positivistic stance toward the social reality that seeks a known “truth” that is assumed to be “out there.”</p>
<p>We argue, however, that RCT praxis limits the types of questions addressed within the “context of discovery” stage of an RCT project by only focusing on issues that pertain to the efficacy of a given intervention. Although, as we noted earlier, there is a movement underway to incorporate a mixed methods design into RCT research by including a qualitative component (read: a qualitative method) it is not clear whether this inclusion of a qualitative method will serve to expand the range and type of questions RCT researchers ask if in fact these new questions also stem from a positivistic methodological lens. For example, if a researcher applies a positivistic lens to a qualitative component (methods) in his or her mixed methods RCT design, he or she will most likely assign a “secondary” or “embedded” role to that qualitative component and would see its role as primarily validating or confirming the findings gathered from the dominant quantitative component. <xref ref-type="bibr" rid="bibr32-1077800412456964">Song, Sandelowski, and Happ’s (2010)</xref> review of the literature on mixed methods RCT note that although there has been a push within the field of RCT research designs to incorporate qualitative methods, with these methods taking on an “enhancement” and “remediation role,” they conclude that “qualitative research still tends to be depicted not as integral to trial work, but rather as complementary to it” (p. 729). What is most troubling, however, is the lack of understanding among mixed methods RCT researchers of the difference between methodology and method. Song et al. further remark that “qualitative research is essentialized, with no demonstrable recognition that qualitative research itself encompasses a diverse range of methodological approaches suitable for different purposes” (p. 729). This type of mixing of methods in RCT design is akin to “adding and stirring” the qualitative component into what remains a positivistic methodology.</p>
<sec id="section4-1077800412456964">
<title>Weaving and Shifting Methodological Perspectives in RCT Mixed Methods Praxis</title>
<p>Weaving methodological perspectives in RCT mixed- methods praxis to incorporate subjective meaning into RCT research designs provides a new theoretical lens with which researchers can ask a new set of questions that stresses the importance of subjective understanding by including the voices of research participants and health care providers with regard to, for example, their understanding of the efficacy of a given intervention. Furthermore, the inclusion of these voices may suggest the extent to which the intervention may or may not be applied in naturalistic settings. Subjective meanings assigned by research participants and health care experts were, in fact, two important elements that were originally conceived as critical to evidence-based practices (see <xref ref-type="fig" rid="fig1-1077800412456964">Figure 1</xref>). These two components have been removed from current-day RCT research praxis.</p>
<p>Linking a subjectivist methodology to the RCT qualitative component provides the means to answer a new set of research questions and expands the “context of discovery” of RCT mixed methods designs in general. The role that a qualitative component takes on in an RCT mixed methods design depends on the researchers’ methodological (theoretical) lens. If the RCT researcher switches his or her methodological perspective from an objective (positivist) to a subjective methodology, the addition of a qualitative component would now be placed on an equal plane with the quantitative component. The researcher might in fact first utilize a qualitative component to provide him or her with a richer understanding of what types of interventions research participants think are feasible given their specific socialeconomic and cultural context; for example, do participants think the intervention works for them, given their lifestyle and socioeconomic context? Researchers might use a qualitative component to gain a clearer understanding of what specific populations need to be represented with regard to their specific research project.</p>
<p>A researcher who might weave a qualitative component into the “context of justification” phase of an RCT design that asks a different set of questions that derive from a subjectivist methodology would enhance the overall credibility of his or her RCT experiment. This qualitatively driven methodological component would ask about the lived experience of the participants during the RCT trial, and so we might want in fact to ask a set of new questions about how participants experience the clinical trial: First and foremost is during the recruitment of participants: Why do those participants who are qualified for a given trial decide not to participate in it? If during the RCT some participants withdraw from the study there might be a follow-up interview to discover their reasons for withdrawal. To ensure that the RCT procedure is being carried out per specific RCT protocol one might begin to conduct some unobtrusive qualitative data collection to determine whether the clinical trial is proceeding as per protocol. Are participants and researchers following the procedures of the trial? If not, why not? If they are not, it might be important to further develop some additional qualitative methods to ascertain the degree of noncompliance on the part of participants and/or researchers who are administering the RCT.</p>
<p>The researcher might decide to add a qualitatively driven approach postexperiment in order to understand the lived experiences of research participants with regard to their feelings and attitudes about the intervention itself. Did they feel it was successful? In what sense? The researcher might then follow up with disparate results by placing these in conversation with RCT outcome findings, asking such questions as, “Under what conditions did the intervention work for respondents even though the intervention study showed there was no effect?” They might then feed these results back into their original study to perhaps select a different intervention given the feedback received from research participants. Such qualitatively driven participant information may also serve to enhance the researcher’s understanding of how he or she might apply a given intervention outside a laboratory setting, asking such questions as, “Why did the intervention work? From whose perspective did the intervention work?” This set of questions privilege the lived experiences of those most affected by the intervention: Patients and those medical practitioners who are most directly involved in their treatment and care.</p>
<p>Weaving mixed methods RCT designs with a multimethodological standpoint, then, is as an important research strategy for tapping into the synergistic power of RCT mixed methods designs. Using multimethodological strategies encourages the promotion of social justice into the research process as the use of multimethodological perspectives can serve to unearth the subjective experiences of those whose voices have been silent in relation to a given research problem and issue and encourage the asking of new questions aimed at excavating subjugated knowledge. Adding a multimethodological perspective to RCT designs at the context of justification (i.e., decisions about type of data collection design, intervention, and analysis and interpretation) stage of a research project can also enhance the credibility of RCT research evidence by stressing the importance of being cognizant of how research is gathered in an RCT trial, especially with regard to issues of power, authority, and ethics.</p>
<p>Given the positivist framework within which most clinical research trials are embedded, there is already an inherent power imbalance in the researcher–researched relationship in that the participant is treated as an “object” by the researcher, who does not pay attention to the subjective experience of the researched—how they may experience the intervention being tested and how they perceive its effectiveness. Yet, without paying attention to the subjective experience of the participant, the intervention, interpretation, and overall credibility of outcomes are from the researcher’s perspective alone. The researcher has missed an important opportunity to take into account the subjective understandings of those who are directly affected by a clinical trial, namely, the participants. To what extent do they feel the intervention was meaningful? How do they perceive the outcome of the intervention? From whose perspective is the intervention a success? How is success determined? To what extent are the voices of articipants heard and their opinions and concerns factored into subsequent trials?</p>
<p>Ethical discussions usually remain detached from RCT research models, especially with regard to issues of social justice. Yet the ethical standpoint or moral integrity of the researcher is a critically important aspect of ensuring that the research process and a researcher’s findings are trustworthy and valid. The term “ethics” derives from the Greek word <italic>ethos</italic>, which means “character.” Taking a multimethodological perspective can also provide insight into how ethical issues enter into the selection of a research problem, how one conducts research, the design of one’s study, one’s sampling procedure, and the responsibility toward research participants. Ethical issues also come into play in deciding what research findings get published.</p>
<p>We move now to presenting a set of mixed methods RCT case studies that attempt to incorporate a qualitative component into their research, in order to enhance the credibility of their research findings. We analyze each case study presented and critically analyze the extent to which each case study uses multimethodological praxis into its research designs.</p>
</sec>
</sec>
<sec id="section5-1077800412456964">
<title>An Analysis of Mixed Methods and RCT Research Designs Through the Lens of a Multimethodological Praxis Approach: An Extended Case Study</title>
<p>Some argue that the use of multi- and mixed methods in RCT research can capitalize on the inherent “synergy” that is contained within the very process of mixing methods. Some RCT researchers use mixed methods as a means to further the veracity of their quantitative research findings, seeing the qualitative component as a way to validate their quantitative findings. Other researchers use RCT mixed methods to counter the bias that may creep into their research, with the belief that the advantages contained in one method can serve to offset the weaknesses in the other and thereby enhance the overall validity of their study.</p>
<p>We introduce a range of mixed methods RCT studies conducted within both laboratory and naturalistic settings. We examine how RCT researchers integrate a qualitative component into their research design and to what purpose. We analyze how each case study incorporates the qualitative component into its RCT mixed methods project and whether or not in doing so the researchers enhance the overall credibility and application of their research evidence.</p>
<sec id="section6-1077800412456964">
<title>Case Study 1: Using a Multimethodology Mixed Methods RCT to Implement Health Policy Planning at the Primary Care Level</title>
<p><xref ref-type="bibr" rid="bibr20-1077800412456964">Puschel and Thompson’s (2011)</xref> mixed methods study sought to study the lack of increase in the mammography screening compliance rate among women aged 50 and above in Santiago, Chile. The researchers note that in spite of the Chilean national health program’s introduction of free mammography screening at the national level for this age group, the compliance rate hovered at only 12% after 3 years of the national health care system’s implementing free screening. Why?</p>
<p>The authors applied a multimethodologically driven sequential mixed methods design that allowed the researchers to ask different questions in consort, providing the synergy that is promised but often lacking in many mixed methods designs. They first used a qualitatively driven approach that deploys several qualitative data-gathering methods consisting of 7 heterogeneous and homogeneous focus groups with 48 women between the ages of 50 and 70 years. The women also differed with regard to their screening experience: Some had never had a mammogram and some had had a mammogram during the past 2 years, and some who had had breast cancer. The researchers utilized a semistructured interview schedule for the focus groups and collected field notes and field observations. They used a grounded theory approach guided by a theoretical framework known as PRECEDE, which is an acronym for predisposing, enabling, and reinforcing factors, to structure their coding and analysis of these data with the intent of finding both barriers and facilitators to the breast screening process. From these qualitative data they went on develop a set of “intervention strategies” based on women’s lived experiences with mammography screening.</p>
<p>The second component of their research study moved to an RCT quantitatively driven approach whose goal is to ask the question: Did the interventions work? The RCT component’s question was designed to ascertain the efficacy of different interventions types whose outcome measure was whether or not the specific intervention led to an increase in mammography screening over a 6-month period.</p>
<p>They conducted a random sample of 500 women aged 50 to 70 who were registered at a community clinic in Santiago, Chile, who had not had a mammogram within the past 2 years. These women were then randomly assigned to one of three experimental (intervention) groups: (a) the “standard group,” which consisted of women who were only given brief advice on mammography screening by their primary care physician; (b) the “low-intensity” group consisting of women who received brief advice and mail contact; (c) the “high-intensity” group that received brief advice, mail contact, as well as personal outreach. Participants’ screening was tracked by the electronic registry of mammograms performed during the period of the study. (See <xref ref-type="fig" rid="fig3-1077800412456964">Figure 3</xref>) The authors note that after 6 months of the RCT component intervention trial, the control group’s screening percentage was 7%, compared to 52% in the low-intensity group and 70% in the high-intensity group.</p>
<fig id="fig3-1077800412456964" position="float">
<label>Figure 3.</label>
<caption><p>Multimethodology mixed methods sequential design (<xref ref-type="bibr" rid="bibr20-1077800412456964">Puschel &amp; Thompson, 2011</xref>)</p></caption>
<graphic xlink:href="10.1177_1077800412456964-fig3.tif"/>
</fig>
<p>The Chilean mammography screening research project underscores the importance of taking a multimethodological approach to mixed methods RCT, especially when implementing national health care policies at the primary care level. Utilizing a qualitatively driven component prior to the RCT quantitative component provided critical information regarding the type of implementation strategies while taking into account the particular cultural context at the local primary care level. These research strategies resulted in significant increases in the number of mammography screenings.</p>
<p>What this case study also demonstrates is how important it is for researchers to think outside the “gold standard” RCT box. By linking methodology with method and placing the methodologies contained within a study on the same level of importance—that is, not treating the qualitative component as an “add on,” or not mentioning it at all—researchers are able to expand their thinking about how to get at a richer, more complete, and more valid understanding of a particular clinical intervention overall. In addition, what this case study demonstrates is that the researcher can take on a shifting and weaving of methodologies and methods. We saw in this one case study how the researchers’ multimethodological mixed methods design was woven into policy initiatives at the national level “by going back from policy to qualitative research and then forward to quantitative designs.” Such a shifting and weaving of research methodologies and methods can shed light on localized realities that remain subjugated under a national health care social policy agenda, in this case leading to effective localized interventions.</p>
<p>In addition, this case study may be an important template of sorts (being mindful always of different localized conditions; specific geographical location—rural vs. urban settings, for example) that starts the beginning of national and local health care dialogue about how to connect national to local health care initiatives, being mindful of cultural diversity.</p>
</sec>
<sec id="section7-1077800412456964">
<title>Case Study 2: Evaluating the Efficacy of Treatment Interventions</title>
<p><xref ref-type="bibr" rid="bibr21-1077800412456964">Rogers, Day, Randall, and Bentall (2003)</xref> utilized an embedded mixed methods design whereby they weaved two qualitative studies with an RCT that used an intervention designed to assess the effectiveness of different treatments to increase patients’ use of antipsychotic medication. Prior to the clinical trial, researchers began with interviews of 26 schizophrenic patients regarding their lived experiences with taking antipsychotic medications in general (these patients did not participate in the subsequent trial). The goal of the initial qualitative component was to provide the researchers with a more grounded intervention measure based on the patients’ accounts of how they specifically dealt with taking antipsychotic medications.</p>
<p>The second phase of their research used an RCT that assigned patients to three different treatment groups (a control group, which received the standard treatment; a “compliance” treatment model; and a “patient-centered” model of treatment that was developed from their prior interview study). Outcome measures of successful treatment consisted of administering a drug attitudinal scale prior to the intervention, at the end of the intervention, and 1 year after. The higher a patient’s positive attitudes on the drug scale, the more effective the treatment was said to be.</p>
<p>To place the meaning of the attitudinal outcomes from the quantitative measures in context, the researchers then interviewed a sample of 16 patients from 2 different treatment groups (those in the compliance group and those in the patient-centered group), some of whom scored in the positive range and others who scored in the negative range on the quantitative attitudinal component. In <xref ref-type="bibr" rid="bibr21-1077800412456964">Rogers and colleagues’ (2003)</xref> study both the qualitative and quantitative components are positioned on an equal plane.</p>
<p><xref ref-type="fig" rid="fig4-1077800412456964">Figure 4</xref> shows that the researchers started off their research by tending to the context of discovery component of their project design. In fact, Rogers et al. label their work as a “qualitative study” in the title of their article. They use a subjectivist methodology whereby the goal of the qualitative component is to unearth subjugated knowledge by listening to the experiences of research participants. The authors note, “There is a growing recognition of the need to illuminate the processes underlying statistical correlations, in complex trials, and to develop explanatory frameworks, which have applicability beyond the immediate context of the reported study” (<xref ref-type="bibr" rid="bibr21-1077800412456964">Rogers et al., 2003</xref>, p. 721). They began their study by using a qualitative component (QUAL) that centers on in-depth interviews to ascertain the lived experiences of their respondents—particularly their attitudes and feelings regarding the specific types of treatment interventions the researchers were planning to use and whether or not they felt such interventions might be important to the research questions at hand. The researchers used “aspects of grounded theorizing and constant comparison” to extract themes from their in-depth interviews and did so iteratively, before switching to a positivistic methodology that deployed the quantitative clinical trial component. This sequencing design allows the researchers to get at subjugated knowledge that can strengthen the validity of clinical trials by revealing new areas of research and new questions that might be asked. The researchers switch their methodological lens again to ground the validity of a research intervention by adding an additional qualitative component following the RCT clinical trial. Doing so allowed researchers to tend to the differential subjective outcomes of treatment within a broader perspective on the lived experiences of respondents.</p>
<fig id="fig4-1077800412456964" position="float">
<label>Figure 4.</label>
<caption><p><xref ref-type="bibr" rid="bibr21-1077800412456964">Rogers et al.’s (2003)</xref> exploratory sequential mixed methods design</p></caption>
<graphic xlink:href="10.1177_1077800412456964-fig4.tif"/>
</fig>
<p>Such information goes beyond standard clinical trial protocols, which generally seek a much more dichotomous understanding of treatment outcomes (i.e., successful or unsuccessful). By including a qualitative component that also takes account of differential outcomes, the researchers are able to look at those contextual factors within and between treatment groups that can then perhaps serve to influence the context of discovery. So, for example, researchers may be able to better refine their treatment options to enhance a better outcome for those scoring in the negative range of the quantitative component. Researchers working together with clinicians and other health care providers can also reflect on what types of interventions seemed to work from the perspective of their participants, regardless of treatment plan, and also on the basis of their own clinical experiences with working with patients in clinical settings. Such a strategy can perhaps lead to additional research questions of a more contextual nature: Under what conditions can Plan A or Plan B be effective and would a different plan work better?</p>
<p>By opening up dialogue with their participants regarding their experiences with different treatment plans, the researchers can shift their methodological perspective on outcomes from a binary one to a more multifaceted one that gathers a much more nuanced sense of what works and under what conditions outside of the laboratory setting. Such an approach further enhances a more complex understanding of the conditions under which a given intervention and its application is effective outside of a laboratory setting.</p>
</sec>
<sec id="section8-1077800412456964">
<title>Case Study 3: Measuring the Impact of Welfare Rights Interventions</title>
<p>Moffatt et al.’s (Moffatt, White, Mackintosh, &amp; Howel, 2006) article was based on a pilot RCT of individuals aged above 60 that is designed to determine the impact of providing welfare rights advice within primary care setting in the United Kingdom. The authors’ goal was to show how such interventions could tackle age-based health inequities.</p>
<p>The study used a concurrent mixed methods design by incorporating a qualitative component concurrently with an RCT (see <xref ref-type="fig" rid="fig5-1077800412456964">Figure 5</xref>). They first drew a random sample of 126 participants (119 at final follow-up) who were invited from the databases of 4 general practices in the “most deprived wards” of Newcastle upon Tyne. Quantitative data were gathered from structured interviews (4 over 2 years) “using standard scales covering the areas of demographics, mental and physical health (SF36), Hospital Anxiety and Depression Scale, psychosocial descriptors … socioeconomic indicators … [and] the perceived impact of additional resources for those who received them.”</p>
<fig id="fig5-1077800412456964" position="float">
<label>Figure 5.</label>
<caption><p>Moffatt, White, Mackintosh, &amp; Howel’s (2006) concurrent mixed methods design</p></caption>
<graphic xlink:href="10.1177_1077800412456964-fig5.tif"/>
</fig>
<p>The qualitative component consisted of a purposive sample of 25 participants from the quantitative study. Qualitative interviews were conducted to ascertain the impact of the quantitative intervention (the administering of rights advice) on their lived experiences postintervention. Each participant was interviewed twice, at baseline, and 12 to 18 months later.</p>
<p>Both data sets were analyzed and interpreted separately. The quantitative data from the randomized control component of the study revealed no statistically significant impact of interventions on health and social outcome measures. The analysis of their qualitative component, however, revealed that those interviewed viewed the impact of study intervention as positive experience, contrary to their quantitative outcome measure findings. Across the board, interviewees felt that negative outcomes could be attributed to factors that they perceived were outside the purview of the intervention—age, family history, and fate—and could not be affected by the intervention under study.</p>
<p>At this juncture, the researchers could have added more methods components to their study with the goal of validating the efficacy of an intervention, believing that if they did so they might get at the “truth” of the impact of the intervention. Instead, the researchers looked beyond converging their findings and embraced the discrepancies found in the findings from their qualitative and quantitative components as an opportunity to enhance as well as provide a more complex understanding of how a given health intervention can affect research participants.</p>
<p>Moffatt et al. (2006) explored the reasons for divergent findings, assuming at first that they should “triangulate” the data (assuming a more positivistic methodological standpoint of reference). Thus, the researchers first sought to analyze the rigor of each method, by comparing the samples, collecting further data for verification, and addressing unexpected factors that would affect the data that might account for a lack of triangulation.</p>
<p>However, as their inquiry proceeded, the authors sought to think more reflexively about their original positivistic validation approach, seeing how divergence could in fact add to the complexity of understanding the overall impact of their intervention. It is at this point in the study that the authors switch their methodological lens to a subjectivist methodology and in doing so utilize their qualitative component as a tool for the unearthing of subjugated knowledge that revealed a number of dimensions not measured by the quantitative study. They stated:<disp-quote>
<p>Firstly, the qualitative study revealed a number of dimensions not measured by the quantitative study, such as, “maintaining independence” which included affording paid help, increasing and improving access to facilities and managing better within the home. Secondly, some of the measures used with the intention of capturing dimensions of mental health did not adequately encapsulate participants’ accounts of feeling “less stressed” and “less depressed” by financial worries.</p>
</disp-quote></p>
<p>By conducting a careful analysis and empirical investigation of findings from both data sets and placing them in reflective conversation, the authors demonstrate the importance of incorporating a multimethodological perspective into randomized controlled trials in order to enhance the overall understanding of the research problem, treating their research results as “more than the sum of its parts.”</p>
</sec>
<sec id="section9-1077800412456964">
<title>Case Study 4: Taking a Singular Methodological Approach to Mixed Methods RCT: The Problem With “Adding and Stirring” a Qualitative Component Into RCT Trials in Naturalistic Settings</title>
<p><xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al. (2010)</xref> sought to show the significance of mixed methods to a field of research in which the necessity of self-reported data typically makes setting and meeting standards of validity a challenge. Most data in HIV-related research, in this case data on adherence and sexual behavior in microbicide research, is self-reported by participants. However, these data are vital in determining the efficacy of life-saving products and separating the effects of behavior and the products themselves. In the face of this challenge, the Microbicides Development Program executed a mixed methods study at 6 research centers involving 9,385 women. Pool et al. undertook this study to compare the various methods used and determine the accuracy of each method and of a mixed methods model overall.</p>
<p><xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al. (2010)</xref> examined qualitative and quantitative data from a random subsample of 725 women from case record form (CRF) and in-depth interviews, coital diaries, the collection of gel applicators of microbicides, focus group discussions, and ethnography. In this article, they specifically looked at the in-depth interview, coital diary, and CRF data to determine sexual behavior and adherence to microbicide use within an RCT. Data that matched across all three methods for a woman was considered consistent and recorded as the “final, triangulated result.” However, in the case of inconsistencies between the data, Pool et al. used discussions from the in-depth interview to clarify and establish a final figure.</p>
<p>The researchers found high rates of inconsistency (in 54% of the data sets) in reporting frequency of sex and gel and condom use. By comparing the data from each method to the triangulated data, the least accurate source of information was the quantitative CRF interviews, leading <xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al. (2010)</xref> to conclude that the integration of qualitative methods (specifically in-depth interviews) and triangulation in clinical trials is a vital tool for increasing the accuracy of these data. Although 30% of the in-depth interviews were still inaccurate compared to the triangulated data, their role is vital to triangulation and establishing the most accurate results. By simply speaking with participants, inaccuracies are easy to correct and participants are empowered to be an important part of creating and even evaluating the data in an RCT.</p>
<p>Although they incorporated qualitative and quantitative methods in a mixed methods study, <xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al.’s (2010)</xref> provides a cautionary case study example. Their procedure does not address the importance of switching methodological approaches to a subjectivist standpoint in order to understand the lived experiences of respondents. The researchers rely on the positivistic methodology that assumes an RCT in and of itself can deal with difference by assuming that whatever variations exist between the control and experimental group can be negated with a randomized assignment of research participants and an additional layer of analysis.</p>
<p>Solely relying on a participant’s point of view gathered from a survey questionnaire may not be adequate to capture the variety of meanings their participants attribute to such concepts as “sexual intercourse” and “adherence.” The researchers must contend with these differences of perception (between the researcher and researched) on these critical measurement issues and decide to incorporate a qualitative component into their RCT research to get at the subjective experiences of their research participants. Yet incorporating several qualitative components into their research trial is done through the lens of a positivistic methodology that aims at getting at “truth.” The goal of the qualitative findings is to validate their self-report survey measure by comparing their survey data with data gathered from several qualitative measures of adherence (including data from coital diaries, in-depth interviews, and focus groups). They also asked research participants to return any unused gel applicators as an unobtrusive measure of adherence. <xref ref-type="fig" rid="fig6-1077800412456964">Figure 6</xref> depicts the mixed methods triangulated research design.</p>
<fig id="fig6-1077800412456964" position="float">
<label>Figure 6.</label>
<caption><p><xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al.’s (2010)</xref> mixed methods RCT (randomized control trail) triangulated design</p></caption>
<graphic xlink:href="10.1177_1077800412456964-fig6.tif"/>
</fig>
<p>Their singular methodological perspective uses a triangulation RCT mixed methods design. Although this model fits into a positivistic framework of knowledge building in that it seeks to get at “truth,” the authors miss an opportunity to get at a deeper understanding of the underlying rocesses at work, namely, to understand why individuals have different adherence rates depending on the method of inquiry. It is in the taking of a subjective methodological approach, one that provides for the possibility that there may be multiple meanings with regard to adherence to the intervention and differing conditions under which a given intervention is followed or not followed, the researchers are not bound by a positivistic definition of validity of measurement that cannot give them the “truth” outcome they seek. By not looking for a more nuanced and contextual understanding of intervention usage, the authors do not ask a range of questions that might help them better understand the adherence behaviors of their respondents. For example, the following are some of the questions they might ask: Under what conditions do respondents tend to comply with using spermicidal gels during intercourse? How do participants perceive what compliance is? How do they define usage?</p>
<p>To get at “contextual factors” that influence usage, the authors might (in their in-depth interviews) ask about the conditions in participants’ lives that might promote compliance and those conditions where compliance begins to break down. Part of a subjective methodological praxis is to deeply listen to the lived experiences of participants. However, by sticking with a triangulated design, the researchers are left trying to explain the reasons why their research findings did not triangulate, and these explanations often result in “blaming” either their participants (for deception or forgetfulness) and/or the interviewers (who might have lacked interviewing skills). The goal of this line of inquiry is to gain “control” of different outcomes through tighter design procedures.</p>
<p>The researchers do not tend to differences among the women in their study as a possible reason for differences in adherence rates. If the researchers had instead shifted their methodological (theoretical) stance from a positivistic methodology to a subjectivist one, they might also consider the pursuit of truth as multiple and subjective. Switching to a subjectivist methodology might allow them the opportunity to revisit the “context of discovery” component of their research project by expanding on their original set of research questions to also include the goal of seeking (multiple) patterns of adherence among women, that is, a more complex understanding of how women comply with interventions, especially in naturalistic settings.</p>
<p><xref ref-type="fig" rid="fig7-1077800412456964">Figure 7</xref> depicts a suggested alternative research design that is multimethodological and is an exploratory sequential mixed methods RCT design. It begins with the deployment of a subjectivist methodology whose goals are to take into account multiple differences among women and ask both researchers and participants to reflect on these differences. This suggested research design is an exploratory sequential mixed methods study whose goal is to first understand adherence as a process whose findings in turn will serve to enhance the RCT positivistic component whose goals are to ascertain the effectiveness of the study’s intervention (use of spermicidal gel in combating HIV/AIDS).</p>
<fig id="fig7-1077800412456964" position="float">
<label>Figure 7.</label>
<caption><p>An exploratory sequential mixed methods design: A proposed alternative design to <xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al.’s (2010)</xref> triangulated design</p></caption>
<graphic xlink:href="10.1177_1077800412456964-fig7.tif"/>
</fig>
<p>The researcher would begin with a qualitative component consisting of open-ended questions regarding adherence as well as participants’ definitions of important research concepts. Responses from the qualitative interviews would then be used to guide the development of the next component, a survey questionnaire. So, for example, if the respondents were asked open-ended questions using data gathered from initial in-depth interviews (such as, “What were some key challenges preventing you from using condoms/gels during sexual intercourse?”), the thematic analysis of the participants’ answers would then lead to the development of close-ended survey items. When the quantitative data are analyzed and interpreted, the researchers might incorporate another qualitative component consisting of focus group interviews that could deal with any inconsistent results or issues found in the quantitative analysis.</p>
<p>This suggested design then applies a multimethodological lens onto mixed methods RCT whose goals are to go beyond seeking a narrow outcome of knowledge building that seeks to determine the statistical effectiveness of spermicidal gel. By starting off with an exploratory sequential design before launching into a clinical trial on effectiveness, researchers will be in a better position to interpret divergent findings by placing them into a broader context of understanding. In fact, the subjective, methodologically driven qualitative component can also provide researchers with a set of differences to incorporate in selecting participants for the RCT. Thus, the RCT positivistic-driven quantitative results can be analyzed through a number of differences to move beyond a dichotomous understanding of effectiveness of an intervention. If the same respondents are used for the interviews and the clinical trial, having the experience of being listened to in turn might build up rapport with respondents, which may in turn serve to keep individuals highly committed to the study. So the findings from one methodology are integrated into the findings of the other, without compromising the integrity of the philosophical assumptions underlying each methodology.</p>
<p>Perhaps an important insight into this particular case study is that the use of validation through triangulation of research results can cut off inquiry and prevent our understanding of why respondents don’t always say the same thing when different methods are used. Seeing divergent findings as an opportunity to ask new questions (and not just a failure to find agreement) can open up a much more complex understanding of the overall research problem.</p>
</sec>
</sec>
<sec id="section10-1077800412456964" sec-type="conclusions">
<title>Conclusion</title>
<p>The practice model of evidence-based research, especially those studies that use a traditional RCT design, assume a methods-centric research approach that privileges the “context of justification” as the research component that will ensure the credibility of research findings and there is the implicit assumption of a single positivistic methodology. However, as we have demonstrated in our analysis of four case studies that use mixed methods RCT designs, the “context of discovery” is a critical component in determining the overall credibility of research evidence. For example, in <xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al.’s (2010)</xref> case study, the researchers miss important opportunities to reflect on the contribution of the ways in which the “context of discovery” of their research project might contribute to the overall credibility of their research findings. For example, interrogating their research problem begins with such questions as, “How was the research question framed? Who was included/excluded? Whose agenda is served by asking this particular question? Who decided on this particular intervention?” Answers to these questions would be important input to the overall credibility goals of RCT research findings.</p>
<p>The first three case studies presented in this article provide concrete empirical examples of how important it is to tend to the “context of discovery” and its contribution to the overall credibility of RCT evidence. We noted how these first three case studies place “the contexts of discovery and justification” in conversation with one another by using the information from the “context of discovery” to inform the “context of justification.” In all three of these case studies, the researchers’ aim is not to search for a single “truth,” but instead, to capture a more complex understanding of their research problem. The researchers in these three case studies accomplish this by tempering their singular conception of truth through the process of negotiating “truth” to seek the variable conditions under which their research findings apply or are transferable to different social contexts. In effect they use diverse methodological standpoints at different times within their RCT mixed methods design. Never mixing up methodologies, but with an awareness of how specific methodologies are linked to their specific questions and methods choices, they give up their singular view of “truth” by “satisficing” (<xref ref-type="bibr" rid="bibr31-1077800412456964">Simon, 1955</xref>) in the pursuit of “truth,” by seeking “optimizing” truth claims through multimethodological standpoints specific to the mixed methods RCT research process. Herbert Simon coined the tern “satisficing” as an alternative strategy of human decision making that he contrasted with rational choice theory, which depicts humans making decisions that maximize or seek the “best” decision. He felt that most humans may never know what their “best “choice is from among alternative choices, so instead he sought to introduce an alternative decision-making model that moved in the direction of individual decision making that aims for maximizing until individuals had reached a “good enough”/reasonable option.</p>
<p>It is important to point out that by advocating the adoption of multimethodology strategies for mixed methods RCT designs will not only require training on a range of methods practices but also a familiarity with multimethodologies—not all researchers can take on shifting methodological standpoints easily. Such an approach may require an interdisciplinary approach to mixed methods RCT designs where different disciplines may come together across their methodological divides to share their points of view in the service of RCT mixed methods projects. <xref ref-type="bibr" rid="bibr16-1077800412456964">O’Cathain, Nicholl, and Murphy (2009)</xref> note that there are specific “structural” factors that will also need to change with regard to how research praxis (lack of training of researchers to conduct mixed methods studies and the unwillingness of journals to embrace mixed methods designs) will need to change in order to accommodate a mixed methods research as a whole.</p>
<p>The need to apply multimethodology praxis, as outlined in this article, grows increasingly important as EBPs embrace meta-analyses, whose overall aim is to decontextualize and synthesize all research findings with regard to a given intervention in order to provide a set of “guidelines” for treatment praxis. Although these meta-analyses aim for validation, they can in fact lead to a false sense of credibility within an evidence-based research framework. The practice of pooling studies is also inherently driven by a “methods-centric” approach that relies on the research results of studies using a similar method without interrogating their contexts of discovery. Some research, in fact, suggests that many of these studies do not take into account differences among participants (<xref ref-type="bibr" rid="bibr22-1077800412456964">Rogers, 2002</xref>, <xref ref-type="bibr" rid="bibr23-1077800412456964">2004a</xref>, <xref ref-type="bibr" rid="bibr24-1077800412456964">2004b</xref>; <xref ref-type="bibr" rid="bibr25-1077800412456964">Rogers &amp; Ballantyne, 2008</xref>, <xref ref-type="bibr" rid="bibr26-1077800412456964">2009</xref>). The research praxis of evidence-based research at the meta-level is reminiscent of our discussion of <xref ref-type="bibr" rid="bibr18-1077800412456964">Pool et al.’s (2010)</xref> case study, wherein the researchers attempt to nail down the “truth” about adherence rates, pursuing a singular “truth” that does not appear to fit the lived experiences of their research participants. Norman <xref ref-type="bibr" rid="bibr4-1077800412456964">Denzin (2009</xref>, <xref ref-type="bibr" rid="bibr5-1077800412456964">2010</xref>) has been a strong critic of evidence-based meta-analyses that have become critical in determining what qualifies as credible evidence. Denzin notes the fact that this type of analysis is a problem no one wants to talk about—the elephant running around in the research laboratory or in naturalistic research settings that few want to discuss.</p>
<p>Incorporating a multimethodological approach into the process of conducting meta-analysis of intervention studies beyond just RCT studies is an ongoing challenge (see <xref ref-type="bibr" rid="bibr17-1077800412456964">Pearson, 2004</xref>; <xref ref-type="bibr" rid="bibr19-1077800412456964">Popay &amp; Williams, 1998</xref>). As we have seen at in our examination of mixed methods RCT case studies at the micro level, believing there is synergy in adding a qualitative method may not “add value” or “synergy” to a research project and, in the case of meta-analyses, such a praxis may do more harm than good by providing a false sense of security that “more must be better.”</p>
<p>RCT knowledge-building models are often criticized for their lack of transferability of their research findings to real-world practice or “action.” <xref ref-type="bibr" rid="bibr13-1077800412456964">Julnes and Rog (2009)</xref>, for example, suggest that in order for the evidence from RCT studies to be truly credible they must also pass what they term an “actionable” criterion, whereby evidence garnered from RCT can in fact be applied to real-world contexts.</p>
<p>What is often left out of the discussion of what is “credible evidence” is that the definition of just what is credible evidence varies depending on the researcher’s philosophical standpoint or set of assumptions regarding the social world. For a positivist, there is the promise of finding one truth that is out there whose RCT method seeks to “control” for internal and extraneous factors that might bias the seeking out of truth that is assumed to already exist in the social reality. Subjective perspective on what is credible evidence has a far different concept of truth and conceptualizes “evidence” is a singular and fixed entity and truth as multiple and socially constructed and, therefore, the goal of research is to unearth the range of subjective understandings of “what is” “truth” or “credible evidence” from the perspective/experience of the researched. Taking a multimethodological perspective on RCT can garner the synergy of placing these methodologies in conversation with one another.</p>
<p>The multimethodology strategies provided in this article seek to move RCT praxis toward a more inclusive and contextualized knowledge building by interrogating RCT praxis at both the context of discovery stage, asking such questions as follows: What values does the researcher bring to the research process? How was a given intervention selected and why? Who is included in this study and who is excluded and why? To what extent is difference tended to in the research process and why or why not? How was a given intervention selected? What is the range of stakeholders involved in this study? To what extent are the voices of research participants and clinical expertise part of the problem formulation stage? To what extent do these groups provide feedback on the type of intervention selected? To what extent does the research process reflect ethical and social justice practices, especially in terms of randomization procedures and the type of intervention research participants are exposed to? For example, to what extent is the researcher open to including participants’ preferences in assignment to control and experimental groups and why or why not?</p>
<p>Multimethodological strategies also inform the context-of-justification stage of an RCT project by providing specific guidance on how a qualitative component can also be integrated into RCT mixed methods research to provide the researcher with a fuller understanding of the adherence of respondents with regard to the selected intervention prior to the RCT trial component. The qualitative component can also be integrated post-RCT trial to provide clinicians and practitioners with information on participants’ attitudes and values with regard to the efficacy of a given intervention, depending on the researchers’ more quantitative information on the effect of an intervention. This information can also be used to provide feedback to researchers to consider why a specific effect worked or did not work from the perspective of the research participant.</p>
<p>It is possible and fruitful to utilize multimethodologies in RCT research as long as the researcher is cognizant of the particular philosophical standpoint (ontology, epistemology, methodology) he or she is bringing to the set of research questions and methods he or she uses to answer these questions. <xref ref-type="bibr" rid="bibr15-1077800412456964">Lincoln (2005)</xref> notes the importance of what she terms “a healthy mix of paradigms … to guide research (p. 230). Lincoln notes further the importance of the research problems in selection of a given theoretical lens, by noting, “The choice of methods, like the choice of theoretical lens or filters … ought to proceed on the basis of problem fit, not paradigm prejudice” (<xref ref-type="bibr" rid="bibr15-1077800412456964">Lincoln, 2005</xref>, p. 231). What the issue in RCT mixed methods research is one not of mixing methods but being cognizant of the links between methods and their methodologies. It is honoring and feeling comfortable in the tensions of inquiry that lies between methodological divides. To bridge this divide requires dialogue and good communication skills. It means pushing against our singular methodological boundaries—being both an insider and an outsider to a given theoretical perspective—a double consciousness that provides for the unearthing of new knowledge by not allowing some forms of inquiry to be subjugated to any one dominant methodology.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<bio>
<title>Bio</title>
<p><bold>Sharlene Hesse-Biber</bold> is a professor of sociology at Boston College, Chestnut Hill, MA. She is the author of <italic>Mixed Methods Research: Merging Theory With Practice</italic> (Guilford, 2010). She is a codeveloper of the computer-assisted software program, HyperRESEARCH, and HyperTRANSCRIBE, a transcription software program (<ext-link ext-link-type="uri" xlink:href="http://www.researchware.com">www.researchware.com</ext-link>).</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bickman</surname><given-names>L.</given-names></name>
<name><surname>Reich</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Randomized controlled trials: A gold standard with feet of clay?</article-title> In <person-group person-group-type="editor">
<name><surname>Donaldson</surname><given-names>S. I.</given-names></name>
<name><surname>Christie</surname><given-names>C. A.</given-names></name>
<name><surname>Mark</surname><given-names>M. M.</given-names></name>
</person-group> (Eds.), <source>What counts as credible evidence in applied research and evaluation practice?</source> (pp. <fpage>51</fpage>-<lpage>77</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr2-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bryman</surname><given-names>A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Integrating quantitative and qualitative research: How is it done?</article-title> <source>Qualitative Research</source>, <volume>6</volume>, <fpage>97</fpage>-<lpage>113</lpage>.</citation>
</ref>
<ref id="bibr3-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bryman</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Barriers to integrating quantitative and research</article-title>. <source>Journal of Mixed Methods Research</source>, <volume>1</volume>(<issue>8</issue>), <fpage>8</fpage>-<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr4-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Denzin</surname><given-names>N.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The elephant in the living room: Or extending the conversation about the politics of evidence</article-title>. <source>Qualitative Research</source>, <volume>9</volume>(<issue>2</issue>), <fpage>139</fpage>-<lpage>160</lpage>.</citation>
</ref>
<ref id="bibr5-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Denzin</surname><given-names>N.</given-names></name>
</person-group> (<year>2010</year>). <article-title>On elephants and gold standards</article-title>. <source>Qualitative Research</source>, <volume>10</volume>(<issue>2</issue>), <fpage>269</fpage>-<lpage>272</lpage>.</citation>
</ref>
<ref id="bibr6-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Donaldson</surname><given-names>S. I.</given-names></name>
</person-group> (<year>2009</year>). <article-title>In search of the blueprint for an evidence-based global society</article-title>. In <person-group person-group-type="editor">
<name><surname>Donaldson</surname><given-names>S. I.</given-names></name>
<name><surname>Christie</surname><given-names>C. A.</given-names></name>
<name><surname>Mark</surname><given-names>M. M.</given-names></name>
</person-group> (Eds.), <source>What counts as credible evidence in applied research and evaluation practice?</source> (pp. <fpage>2</fpage>-<lpage>18</lpage>). Thousand Oaks, CA. SAGE.</citation>
</ref>
<ref id="bibr7-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gay</surname><given-names>L. R.</given-names></name>
<name><surname>Mills</surname><given-names>G. E.</given-names></name>
<name><surname>Airasian</surname><given-names>P. W.</given-names></name>
</person-group> (<year>2006</year>). <source>Educational research: Competencies for analysis and applications</source>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Merrill/Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr8-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Greene</surname><given-names>J. C.</given-names></name>
<name><surname>Caracelli</surname><given-names>V. J.</given-names></name>
<name><surname>Graham</surname><given-names>W. F.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Toward a conceptual framework for mixed-method evaluation designs</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>11</volume>, <fpage>255</fpage>-<lpage>274</lpage>.</citation>
</ref>
<ref id="bibr9-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hall</surname><given-names>B.</given-names></name>
<name><surname>Howard</surname><given-names>K.</given-names></name>
</person-group> (<year>2008</year>). <article-title>A synergistic approach: Conducting mixed methods research with typological and systemic design considerations</article-title>. <source>Journal of Mixed Methods Research</source>, <volume>2</volume>(<issue>3</issue>), <fpage>248</fpage>-<lpage>269</lpage>.</citation>
</ref>
<ref id="bibr10-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Howe</surname><given-names>K. R.</given-names></name>
</person-group> (<year>2004</year>). <article-title>A critique of experimentalism</article-title>. <source>Qualitative Inquiry</source>, <volume>10</volume>(<issue>1</issue>), <fpage>42</fpage>-<lpage>61</lpage>.</citation>
</ref>
<ref id="bibr11-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hesse-Biber</surname><given-names>S. N.</given-names></name>
</person-group> (<year>2012</year>). <source>Handbook of feminist research: Theory and praxis</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr12-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hesse-Biber</surname><given-names>S. N.</given-names></name>
</person-group> (<year>2010</year>). <source>Mixed methods research: Merging theory with practice</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr13-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Julnes</surname><given-names>G.</given-names></name>
<name><surname>Rog</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Evaluation methods for producing actionable evidence</article-title>. In <person-group person-group-type="editor">
<name><surname>Donaldson</surname><given-names>S. I.</given-names></name>
<name><surname>Christie</surname><given-names>C. A.</given-names></name>
<name><surname>Mark</surname><given-names>M. M.</given-names></name>
</person-group> (Eds.), <source>What counts as credible evidence in applied research and evaluation practice?</source> (pp. <fpage>96</fpage>-<lpage>131</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr14-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lather</surname><given-names>P.</given-names></name>
</person-group> (<year>2004</year>, <month>December</month>). <article-title>Scientific research in education: A critical perspective</article-title>. <source>British Educational Research Journal</source>, <volume>30</volume>(<issue>6</issue>), <fpage>759</fpage>-<lpage>772</lpage>.</citation>
</ref>
<ref id="bibr15-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lincoln</surname><given-names>Y.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Context, lived experience, and qualitative research</article-title>. In <person-group person-group-type="editor">
<name><surname>Swanson</surname><given-names>R. A.</given-names></name>
<name><surname>Holton</surname><given-names>E. F.</given-names><suffix>III</suffix></name>
</person-group> (Eds.), <source>Research in organizations: Foundations and methods of inquiry</source> (pp. <fpage>221</fpage>-<lpage>232</lpage>). <publisher-loc>San Francisco, CA.</publisher-loc>: <publisher-name>Berrett-Koehler</publisher-name>.</citation>
</ref>
<ref id="bibr16-1077800412456964">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>O’Cathain</surname><given-names>A.</given-names></name>
<name><surname>Nicholl</surname><given-names>J.</given-names></name>
<name><surname>Murphy</surname><given-names>E.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Structural issues affecting mixed methods studies in health research: A qualitative study</article-title>. <source>BMC Medical Research Methodology</source>, <volume>9</volume>(<issue>82</issue>). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2288/9/82">http://www.biomedcentral.com/1471-2288/9/82</ext-link></citation>
</ref>
<ref id="bibr17-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pearson</surname><given-names>A.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Balancing the evidence: Incorporating the synthesis of qualitative data into systematic reviews</article-title>. <source>JBI Reports</source>, <volume>2</volume>, <fpage>45</fpage>-<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr18-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pool</surname><given-names>R.</given-names></name>
<name><surname>Montgomery</surname><given-names>C. M.</given-names></name>
<name><surname>Morar</surname><given-names>N. S.</given-names></name>
<name><surname>Mweemba</surname><given-names>O.</given-names></name>
<name><surname>Ssali</surname><given-names>A.</given-names></name>
<name><surname>Gafos</surname><given-names>M.</given-names></name>
<name><surname>…McCormack</surname><given-names>S.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Assessing the accuracy of adherence and sexual behaviour data in the MDP301 Vaginal Microbicides Trial using a mixed methods and triangulation model</article-title>. <source>PLoS ONE</source>, <volume>5</volume>(<issue>7</issue>):<fpage>e11632</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0011632</pub-id></citation>
</ref>
<ref id="bibr19-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Popay</surname><given-names>J.</given-names></name>
<name><surname>Williams</surname><given-names>G.</given-names></name>
</person-group> (<year>1998</year>) <article-title>Qualitative research and evidence-based healthcare</article-title>. <source>Journal of the Royal Society of Medicine</source>, <volume>91</volume>(<supplement>Suppl. 35</supplement>), <fpage>32</fpage>-<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr20-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Puschel</surname><given-names>K.</given-names></name>
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Mammogram screening in Chile: Using mixed methods to implement policy planning at the primary care level</article-title>. <source>The Breast</source>, <volume>20</volume>, <fpage>540</fpage>-<lpage>545</lpage>.</citation>
</ref>
<ref id="bibr21-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>A.</given-names></name>
<name><surname>Day</surname><given-names>J.</given-names></name>
<name><surname>Randall</surname><given-names>F.</given-names></name>
<name><surname>Bentall</surname><given-names>R. P.</given-names></name>
</person-group> (<year>2003</year>) <article-title>Patients’ understanding and participation in a trial designed to improve the management of anti-psychotic medication - A qualitative study</article-title>. <source>Social Psychiatry and Psychiatric Epidemiology</source>, <volume>38</volume> (<issue>12</issue>) <fpage>720</fpage>-<lpage>727</lpage>.</citation>
</ref>
<ref id="bibr22-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>W. A.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Evidence-based medicine in practice: Limiting or facilitating patient choice?</article-title> <source>Health Expectations</source>, <volume>5</volume>, <fpage>95</fpage>-<lpage>103</lpage>.</citation>
</ref>
<ref id="bibr23-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>W. A.</given-names></name>
</person-group> (<year>2004a</year>). <article-title>Evidence-based medicine and justice: A framework for looking at the impact of EBM on vulnerable or disadvantaged groups</article-title>. <source>Journal of Medical Ethics</source>, <volume>30</volume>, <fpage>141</fpage>-<lpage>145</lpage>.</citation>
</ref>
<ref id="bibr24-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>W. A.</given-names></name>
</person-group> (<year>2004b</year>). <article-title>Evidence-based medicine and women: Do the principles and practice of EBM further women’s health?</article-title> <source>Bioethics</source>, <volume>18</volume>(<issue>1</issue>), <fpage>62</fpage>-<lpage>83</lpage>.</citation>
</ref>
<ref id="bibr25-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>W. A.</given-names></name>
<name><surname>Ballantyne</surname><given-names>A. J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Exclusion of women from clinical research: Myth or reality?</article-title> <source>Mayo Clinic Procedures</source>, <volume>83</volume>(<issue>5</issue>), <fpage>536</fpage>-<lpage>542</lpage>.</citation>
</ref>
<ref id="bibr26-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>W. A.</given-names></name>
<name><surname>Ballantyne</surname><given-names>A. J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Justice in health research: What is the role of evidence-based medicine?</article-title> <source>Perspectives in Biology and Medicine</source>, <volume>52</volume>(<issue>2</issue>), <fpage>188</fpage>-<lpage>202</lpage>.</citation>
</ref>
<ref id="bibr27-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sackett</surname><given-names>D. L.</given-names></name>
<name><surname>Rosenberg</surname><given-names>W. M. C.</given-names></name>
<name><surname>Gray</surname><given-names>J. A. M.</given-names></name>
<name><surname>Haynes</surname><given-names>R. B.</given-names></name>
<name><surname>Richardson</surname><given-names>W. S.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Evidence-based medicine: What it is and what it is not</article-title>. <source>BMJ</source>, <volume>312</volume>, <fpage>71</fpage>-<lpage>72</lpage>.</citation>
</ref>
<ref id="bibr28-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sackett</surname><given-names>D. L.</given-names></name>
<name><surname>Straus</surname><given-names>S. E.</given-names></name>
<name><surname>Richardson</surname><given-names>W. S.</given-names></name>
<name><surname>Rosenberg</surname><given-names>W.</given-names></name>
<name><surname>Haynes</surname><given-names>R. B.</given-names></name>
</person-group> (<year>1997</year>). <source>Evidence-based medicine: How to practice and teach EBM</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Churchill Livingstone</publisher-name>.</citation>
</ref>
<ref id="bibr29-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sandelowski</surname><given-names>M.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Using qualitative methods in intervention studies</article-title>. <source>Research in Nursing &amp; Health</source>, <volume>19</volume>, <fpage>359</fpage>-<lpage>364</lpage>.</citation>
</ref>
<ref id="bibr30-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sieber</surname><given-names>S. D.</given-names></name>
</person-group> (<year>1973</year>). <article-title>The integration of fieldwork and survey methods</article-title>. <source>American Journal of Sociology</source>, <volume>78</volume>(<issue>6</issue>), <fpage>1335</fpage>-<lpage>1359</lpage>.</citation>
</ref>
<ref id="bibr31-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Simon</surname><given-names>H. A.</given-names></name>
</person-group> (<year>1955</year>). <article-title>A behavioral model of rational choice</article-title>. <source>Quarterly Journal of Economics</source>, <volume>59</volume>, <fpage>99</fpage>-<lpage>118</lpage>.</citation>
</ref>
<ref id="bibr32-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Song</surname><given-names>M.</given-names></name>
<name><surname>Sandelowski</surname><given-names>M.</given-names></name>
<name><surname>Happ</surname><given-names>M. B.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Current practices and emerging trends in conducting mixed methods intervention studies in the health sciences</article-title>. In <person-group person-group-type="editor">
<name><surname>Tashakkori</surname><given-names>A.</given-names></name>
<name><surname>Teddlie</surname><given-names>C.</given-names></name>
</person-group> (Eds.), <source>SAGE handbook of mixed methods research in social &amp; behavioral research</source> (<edition>2nd ed.</edition>, pp. <fpage>725</fpage>-<lpage>747</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr33-1077800412456964">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sprague</surname><given-names>J.</given-names></name>
<name><surname>Zimmerman</surname><given-names>M.</given-names></name>
</person-group> (<year>1993</year>) <article-title>Overcoming dualisms: A feminist agenda for sociological methodology</article-title>. In <person-group person-group-type="editor">
<name><surname>England</surname><given-names>Paula</given-names></name>
</person-group> (Ed.), <source>Theory on gender/feminism on theory</source> (pp.<fpage>255</fpage>-<lpage>280</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>Aldine DeGruyter</publisher-name>.</citation>
</ref>
<ref id="bibr34-1077800412456964">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>St. Pierre</surname><given-names>E.</given-names></name>
</person-group> (<year>2002</year>, <month>November</month>). <article-title>“Science” rejects postmodernism</article-title>. <source>Educational Researcher</source>, <volume>31</volume>(<issue>8</issue>), <fpage>25</fpage>-<lpage>27</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>