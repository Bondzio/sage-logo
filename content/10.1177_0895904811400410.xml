<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPX</journal-id>
<journal-id journal-id-type="hwp">spepx</journal-id>
<journal-title>Educational Policy</journal-title>
<issn pub-type="ppub">0895-9048</issn>
<issn pub-type="epub">1552-3896</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0895904811400410</article-id>
<article-id pub-id-type="publisher-id">10.1177_0895904811400410</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Measuring Academic Readiness for College</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Porter</surname><given-names>Andrew C.</given-names></name>
<xref ref-type="aff" rid="aff1-0895904811400410">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Polikoff</surname><given-names>Morgan S.</given-names></name>
<xref ref-type="aff" rid="aff2-0895904811400410">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0895904811400410"><label>1</label>University of Pennsylvania, Philadelphia</aff>
<aff id="aff2-0895904811400410"><label>2</label>University of Southern California, Los Angeles</aff>
<author-notes>
<corresp id="corresp1-0895904811400410">Morgan S. Polikoff, University of Southern California, Rossier School of Education, 3470 Trousdale Parkway, Room 904D, Los Angeles, CA 90089. Email: <email>polikoff@usc.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>26</volume>
<issue>3</issue>
<fpage>394</fpage>
<lpage>417</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Recent years have seen increased attention to the high school–college transition. Students are enrolling in college in record numbers, yet they are also taking increasing numbers of remedial courses. How to measure and report on academic readiness for college is an important policy issue receiving attention from the National Assessment Governing Board, Achieve, and several states. The focus of this article is how to create a measure of academic readiness for college, either by building and validating a new assessment or validating and repurposing an existing assessment. After first describing the disjuncture between high school and college and discussing definitions and indicators of academic readiness, the authors identify four strategies that might be used to create a readiness assessment. The pros and cons of each strategy are discussed.</p>
</abstract>
<kwd-group>
<kwd>assessment</kwd>
<kwd>educational policy</kwd>
<kwd>high schools</kwd>
<kwd>higher education</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>There is now an unprecedented level of concern about the transition from high school to college (e.g., <xref ref-type="bibr" rid="bibr24-0895904811400410">Editorial Projects in Education, 2009</xref>; <xref ref-type="bibr" rid="bibr60-0895904811400410">Way, 2010</xref>). On one hand, increasing numbers of high school graduates are seeking to continue their education in some form of postsecondary education, with postsecondary enrollment rates increasing from 51% to 69% between 1975 and 2007 (<xref ref-type="bibr" rid="bibr44-0895904811400410">National Center for Education Statistics, 2007</xref>). On the other hand, frighteningly large percentages of these college-bound high school graduates are being judged not college ready—more than two thirds when judged on high school graduation, high school course taking, and basic reading skills (<xref ref-type="bibr" rid="bibr31-0895904811400410">Greene &amp; Forster, 2003</xref>). How to better understand and remedy the disjuncture between high school and college has become one of America’s pressing education policy concerns (<xref ref-type="bibr" rid="bibr21-0895904811400410">Conley, 2003</xref>; <xref ref-type="bibr" rid="bibr25-0895904811400410">Education Trust, 1999</xref>; <xref ref-type="bibr" rid="bibr35-0895904811400410">Kirst, 2005</xref>).</p>
<p>At the same time, recent years have seen increased attention to developing assessments that could be used as measures of academic readiness for college. For instance, the National Commission on National Assessment of Educational Progress (NAEP) 12th-Grade Assessment and Reporting (<xref ref-type="bibr" rid="bibr47-0895904811400410">2004</xref>, p. 2) recommended that 12th-grade NAEP be changed to (a) provide 12th-grade state-level results and (b) report on readiness for college, training for employment, and entry into the military. These recommendations echoed suggestions from researchers such as <xref ref-type="bibr" rid="bibr34-0895904811400410">Kirst (2003)</xref> and <xref ref-type="bibr" rid="bibr20-0895904811400410">Carnevale and Desrochers (2003)</xref> regarding the use of 12th-grade NAEP as a readiness measure, and they are reflected in recent plans for a series of preparedness studies using the NAEP (<xref ref-type="bibr" rid="bibr41-0895904811400410">National Assessment Governing Board, 2008</xref>). Along with 12th-grade NAEP, the Algebra II end-of-course test being developed by Achieve’s American Diploma Project (ADP) is intended to serve as a measure of college readiness. Algebra II is seen as a gateway course for higher education (<xref ref-type="bibr" rid="bibr3-0895904811400410">Achieve, 2008</xref>; <xref ref-type="bibr" rid="bibr9-0895904811400410">Adelman, Daniel, Berkovits, &amp; Owings, 2003</xref>). Other tests may also be appropriate as measures of college readiness, such as ACT, SAT, college placement tests (e.g., ACCUPLACER, COMPASS), or state end-of-course tests. The current enthusiasm for developing and establishing the validity of measures of college readiness, given the widespread use of ACT and SAT tests, is not clear. Nevertheless, multiple parallel initiatives are underway, and issues of validity, utility, and policy relevance are pressing.</p>
<p>The purpose here is to consider how reliable and valid indicators of academic readiness for college could be created based on new or existing assessments. To meet this purpose, we first summarize the definitions and academic predictors of college readiness. Next, we ask what purposes would be served by the development of an assessment to measure readiness. Finally, we identify four approaches to validating an assessment as an indicator of academic college readiness and explore the feasibility of assessments as indicators of college readiness.</p>
<sec id="section1-0895904811400410">
<title>What Is Known About College Readiness?</title>
<p>The intense and widespread interest in readiness for college should come as no surprise. Source after source cites statistics documenting that huge percentages of U.S. students hope to and actually do go to college. For example, more than 94% of high school seniors in 2004 expected to complete at least some college (<xref ref-type="bibr" rid="bibr33-0895904811400410">Ingels, Dalton, &amp; LoGerfo, 2008</xref>). Seventy percent of high school graduates participate in some form of postsecondary education within 2 years of graduating (<xref ref-type="bibr" rid="bibr25-0895904811400410">Education Trust, 1999</xref>). This is true for students of all races and incomes—89% or more of White, Black, Hispanic, Asian, and lowest income quartile students plan to attend at least some college (<xref ref-type="bibr" rid="bibr33-0895904811400410">Ingels et al., 2008</xref>).</p>
<p>Many Americans go to college, but a large proportion of them are not ready in the sense that they take one or more remedial courses. Not surprisingly, there are large differences among types of institutions; at doctoral degree-granting institutions, approximately 20% of students take one or more remedial courses as compared with 30% at other 4-year institutions, 60% at community colleges, and 50% in other subbaccalaureate institutions (<xref ref-type="bibr" rid="bibr9-0895904811400410">Adelman et al., 2003</xref>). There are also racial and socioeconomic status (SES) differences in remedial course-taking rates—61% of Black students in National Education Longitudinal Study of 1988 (NELS 88) enrolled in remedial courses as compared with 35% of White students; 50% of low-SES (below the median) students enrolled in remedial courses as compared with 32% of high-SES students (<xref ref-type="bibr" rid="bibr14-0895904811400410">Attewell, Lavin, Domina, &amp; Levey, 2006</xref>). <xref ref-type="bibr" rid="bibr5-0895904811400410">ACT (2009)</xref> estimated that only 28% of their high school graduating test takers were prepared for college biology, 42% for college algebra, and 67% for college English composition, with readiness “defined as a 75 percent chance that a student will earn a grade of C or better and approximately a 50 percent chance that a student will earn a grade of B or better” (<xref ref-type="bibr" rid="bibr4-0895904811400410">ACT, 2004</xref>, p. 35). With these findings in mind, it is perhaps unsurprising that approximately half of 1st-year students at community colleges and approximately 25% of 1st-year students at 4-year colleges do not go on to a second year (<xref ref-type="bibr" rid="bibr6-0895904811400410">Adelman, 1994</xref>).</p>
<p>What these statistics make clear is that the vast majority of American high school students plan to attend college—and a hefty 70% actually do attend. At the same time, too many are not ready for the rigors of the postsecondary curriculum. A large percentage of students are required to take one or more remedial courses, and a large percentage do not persist past the 1st year. The situation is not uniform across all institutions of higher education; more selective institutions have smaller problems with lack of readiness.</p>
</sec>
<sec id="section2-0895904811400410">
<title>Defining Academic Readiness for College</title>
<p>Though it is an increasingly popular research topic, there is no commonly accepted definition of “college readiness,” and many researchers simply choose not to define it at all (<xref ref-type="bibr" rid="bibr49-0895904811400410">Olson, 2006</xref>). When readiness is defined in research, definitions themselves vary on multiple dimensions. For one, many definitions of readiness include noncognitive or nonacademic facets, such as student work ethic or determination, parent or family resources, and student personality and persistence. In contrast, when operationalized in the literature, many definitions of readiness focus only on academic performance (e.g., test scores, grades, class rank). Although we certainly agree that noncognitive and nonacademic facets are important in predicting readiness (and success), academic information is more suitable for inclusion in a standardized assessment, and it is the focus here.</p>
<p>A further point of distinction within academic readiness definitions is in terms of the difficulty of the criterion chosen. Perhaps, the most basic definition focuses only on acceptance to college, defining readiness as “the bare minimum qualifications necessary before the college will even consider [the student’s] application” (<xref ref-type="bibr" rid="bibr31-0895904811400410">Greene &amp; Forster, 2003</xref>, p. 3). At the other extreme, other definitions assume that students are only college ready if they <italic>succeed</italic> in college. A middle position is illustrated by definitions that assume a student could be academically ready but still not succeed because of nonacademic factors. In a recent report for the Gates Foundation, David Conley offered an operational definition of college readiness that uses a “success” criterion. Readiness is “the level of preparation a student needs in order to enroll and succeed—without remediation—in a credit-bearing general education course at a postsecondary institution that offers a baccalaureate degree or transfer to a baccalaureate program” (<xref ref-type="bibr" rid="bibr22-0895904811400410">Conley, 2007</xref>, p. 5). To operationalize a definition of academic readiness, we must identify the indicators of “readiness.”</p>
<sec id="section3-0895904811400410">
<title>What Does It Mean to Be Academically Ready for College?</title>
<p>Historically, a primary indicator of academic readiness for college has been freshman grade point average (GPA). Many studies of the predictive validity of various assessments for college success use freshman GPA as their chosen readiness indicator (e.g., <xref ref-type="bibr" rid="bibr17-0895904811400410">Bridgeman, 1991</xref>; <xref ref-type="bibr" rid="bibr30-0895904811400410">Geiser &amp; Studley, 2002</xref>). There are several reasons for the choice of this indicator. Freshman success is critical for success throughout college. More practically, freshman GPA is easy to measure, and it is reported automatically by most institutions in what is perceived to be a common metric (generally 0 to 4.0). Finally, it requires only 1 year of longitudinal tracking. There are important limitations to using freshman GPA as the indicator of readiness. One is that, although more similar than the courses they take during their full college experience, the courses students take during their freshman year can be quite different. Thus, the comparability of grading across courses in an institution can be questioned as faculty within an institution may have no common understanding of what “A” work, for example, entails (<xref ref-type="bibr" rid="bibr37-0895904811400410">Lewis, 2007</xref>). Another is that institutions differ in their grading policies (<xref ref-type="bibr" rid="bibr42-0895904811400410">National Center for Education Statistics, 2002</xref>) so that a 3.0 freshman GPA likely does not mean the same thing across institutions. Finally, using GPA as a measure of readiness requires selecting a level of GPA that is indicative of readiness. Thus, whereas some might say that merely passing one’s classes is indicative of readiness, others might choose a different criterion. Who is “right” in such a scenario?</p>
<p>A second indicator of academic readiness consistent with a middle position is the avoidance of remedial coursework. Being assigned to remedial coursework is an indication that the institution views the student as “unready” to enroll in regular credit-bearing courses that count toward a degree. Remedial course taking is associated with both longer time to degree and decreased likelihood of degree receipt (<xref ref-type="bibr" rid="bibr43-0895904811400410">National Center for Education Statistics, 2004</xref>). Furthermore, remediation takes place at the beginning of college, so it shares the property of freshman grades that it requires relatively little longitudinal tracking.</p>
<p>Avoidance of remediation also has serious limitations as an indicator of readiness. For one, there is no common definition across institutions of what remedial education comprises (<xref ref-type="bibr" rid="bibr39-0895904811400410">Merisotis &amp; Phipps, 2000</xref>). As with grades, care must be taken when comparing remedial course taking across institutions. Second, although students enrolled in remedial courses are more likely to fail to complete their degree than students not enrolled in remedial courses, there are still plenty of students who take no remedial courses and fail to complete their degrees, such as for economic reasons (<xref ref-type="bibr" rid="bibr56-0895904811400410">St. John, Cabrera, Nora, &amp; Asker, 2000</xref>).</p>
<p>Other indicators of academic readiness are longer term. These may include completion of the degree, completion of the degree in the standard 4 years, or cumulative GPA. On one hand, these indicators of readiness make intuitive sense as they take into account actual “success” in completing college. On the other hand, these criteria reflect not only readiness but also the total college experience—completing college successfully obviously takes more than simply being ready when you start. Thus, although these indicators perhaps most fully describe success in college, there are conceptual and methodological difficulties in using them.</p>
<p>Each of these indicators of academic readiness for college involves some form of success in college. Thus, they necessarily omit students who could succeed in college but have not, for one reason or another, enrolled. This is a shortcoming of all of these indicators. Yet without a randomized experiment assigning students to enroll in college or not, there appears to be no way to construct an outcome measure for readiness in college that does not require college enrollment as a precursor.</p>
</sec>
<sec id="section4-0895904811400410">
<title>What Are the Predictors of Academic Readiness for College?</title>
<p>Interest in predicting academic readiness is not a new concern. More than four decades ago, <xref ref-type="bibr" rid="bibr57-0895904811400410">Stanley &amp; Porter (1967)</xref>, studied the correlation of scholastic aptitude scores with college grades, comparing the degree to which aptitude scores were as good a predictor for Black students as White students. There is a substantial industry built around the aptitude assessments of SAT and ACT. A huge literature exists on their use in predicting college success (<xref ref-type="bibr" rid="bibr61-0895904811400410">Zwick, 2004</xref>). <xref ref-type="bibr" rid="bibr29-0895904811400410">Fleming and Garcia (1998)</xref> examined many studies and found that the SAT explained from as high as 25% of the variance in college semester and cumulative GPA to as low as almost none of the variance, but with typical results in the range of 12% to 15%. More recently, a report from the college board found that the three sections of the SAT were correlated .35 with freshman GPA (<xref ref-type="bibr" rid="bibr36-0895904811400410">Kobrin, Patterson, Shaw, Mattern, &amp; Barbuti, 2008</xref>). It is important to recognize that the two tests are different in their stated purposes and original principles. The SAT was designed to measure aptitude, which its authors believed to be positively correlated with subsequent academic attainment. In contrast, the ACT was specifically designed to spur curriculum reform in high school by testing tasks performed in high school and college (<xref ref-type="bibr" rid="bibr19-0895904811400410">Camilli, 2006</xref>).</p>
<p>A second predictor of academic readiness for college is high school GPA or high school class rank. The obvious problem with these two indicators is that they lack a common metric and meaning across high schools. They also lack diagnostic power or leverage over curricular reform. Despite these limitations, high school GPA and class rank are as predictive of freshman GPA as are the aptitude tests built specifically for that purpose (<xref ref-type="bibr" rid="bibr36-0895904811400410">Kobrin et al., 2008</xref>). Generally, however, the findings are that when high school GPA and an aptitude test score are used in combination, the aptitude test has a unique predictive capability above and beyond the predictive value of high school grades (and vice versa; <xref ref-type="bibr" rid="bibr36-0895904811400410">Kobrin et al., 2008</xref>). Furthermore, many schools do not report class rank (<xref ref-type="bibr" rid="bibr28-0895904811400410">Finder, 2006</xref>).</p>
<p>A third possible predictor of students’ academic readiness is the degree to which students have mastered the content that experts say is necessary to be ready for college. This could be accomplished through (a) building an assessment or (b) using existing assessments to measure such content. The former approach would require convening experts and having them identify what content is necessary. An example of this approach is the work of the ADP (<xref ref-type="bibr" rid="bibr12-0895904811400410">American Diploma Project, 2004a</xref>, <xref ref-type="bibr" rid="bibr13-0895904811400410">b</xref>). Then, a test would be built to measure student achievement for that content, and a cut score or cut scores would be set to distinguish college-ready students from college-unready students. The latter approach could be accomplished by using existing college placement tests to make the prediction about readiness (<xref ref-type="bibr" rid="bibr34-0895904811400410">Kirst, 2003</xref>). Colleges use placement tests to decide whether a student has mastered the material sufficiently to place out of a particular remedial course. For example, ACT has COMPASS and SAT has ACCUPLACER. There are three problems with using placement tests as a criterion for college readiness. One is that there is no universal cut point—for both the COMPASS and the ACCUPLACER tests, setting cut scores is a local option. Second, many universities and higher education systems have their own placement tests—nearly 100 different tests in the Southeast alone (<xref ref-type="bibr" rid="bibr2-0895904811400410">Abraham, 1992</xref>). Third, the validity of the institutions’ unique placement tests and their cut scores are unknown as many institutions do not conduct research on the validity of the placement tests they use (<xref ref-type="bibr" rid="bibr59-0895904811400410">Venezia, Kirst, &amp; Antonio, 2003</xref>)</p>
<p>Recently, there has been increasing enthusiasm for using courses taken in high school as a predictor of academic readiness. For instance, the ADP focuses on Algebra II and <xref ref-type="bibr" rid="bibr4-0895904811400410">ACT (2004)</xref> focuses on trigonometry as being especially predictive of college readiness. Using data from two national longitudinal studies, <xref ref-type="bibr" rid="bibr7-0895904811400410">Adelman (1999</xref>, <xref ref-type="bibr" rid="bibr8-0895904811400410">2006</xref>) found that a student’s “academic resources,” a composite measure of the academic content and performance the student brought forward from secondary school into higher education, was an important predictor of college degree completion, even after controlling for other criteria of college success, such as 1st-year college GPA (<xref ref-type="bibr" rid="bibr8-0895904811400410">Adelman, 2006</xref>). <xref ref-type="bibr" rid="bibr23-0895904811400410">DesJardins and Lindsay (2008)</xref> reanalyzed the data and confirmed that high school curriculum and class rank were equally the most important of the academic resources. In short, the research suggests that students who take college preparatory courses are far more likely to get a bachelor’s degree than students who do not, with mathematics courses being the strongest predictors. To be sure, it is not clear from the existing research that requiring all students to take advanced college preparatory courses would result in essentially all students being ready for college. Rather, knowing what courses a student has taken in high school helps predict college success. It could be that the students who succeed in college have taken more college preparatory courses but not necessarily that taking college preparatory courses leads students to success in college. Again, an experiment would be necessary to distinguish these two contrasting possibilities.</p>
</sec>
<sec id="section5-0895904811400410">
<title>Complications</title>
<p>There are additional complications associated with defining and measuring college readiness, no matter the indicator and predictor chosen. The first is that there is no one standard. Depending on where a student goes to college, the student may be seen as ready or not ready in terms of remedial courses they must take and pass, the GPA they earn, or their persistence to a 2nd year. It is probable that if a group of experts were assembled to determine what students must know and be able to do to be ready for college, the findings would vary with the group of experts assembled. In particular, if the experts came from community colleges, the answer might be one thing. If the experts came from selective 4-year institutions, the answer might be quite another. Thus, the predictive value of any college readiness assessment should be investigated by type of institution to account for the differing demands of these types of institutions. It would be sensible to consider the predictive validity of the assessment for students attending (a) community colleges, (b) 4-year institutions that grant doctoral degrees, and (c) other 4-year institutions (perhaps dividing these into two categories of <italic>more</italic> or <italic>less</italic> selective).</p>
<p>The problem of one versus multiple readiness standards is an important issue with regard to the assessments that currently purport to measure readiness. For instance, the ACT and the PLAN report whether students are ready for college coursework in key subject areas. However, each only provides one benchmark, so it is not clear the extent to which their benchmarks are applicable across different college destinations (<xref ref-type="bibr" rid="bibr4-0895904811400410">ACT, 2004</xref>). The <xref ref-type="bibr" rid="bibr13-0895904811400410">ADP (2004b)</xref> was created to identify a core set of content for readiness that extended not only across all institutions of higher education but also to the world of work. Whether the benchmarks they identify indicate college readiness is not clear because no predictive validity evidence in support of their benchmarks has yet been offered. In any event, both logically and from what is known from the research literature, college readiness should not be seen as one universal standard. If there is to be a national indicator of college readiness, there will need to be more than one cut point on the indicator, one for each of several different types of postsecondary institutions.</p>
<p>A second possible complication in defining and measuring college readiness has to do with college major. Certainly, students pursuing different majors need different sets of skills. Thus, depending in part on how readiness is defined, a readiness assessment might need to predict success in different majors, in addition to different institution types. However, constructing an assessment (or multiple assessments) to perform such a task would likely be unfeasible due to the number of potential field-by-institution-type combinations. Still, it would be useful to investigate different readiness standards for success in broad classifications of majors, such as the humanities, social sciences, and science, technology, engineering, and mathematics (STEM) fields. These complications, while making the creation of readiness predictors more challenging, in no way diminish the importance of developing such predictors. Rather, the complications lead us to think about ways to develop predictors that can be useful across the wide range of college-going experiences.</p>
</sec>
</sec>
<sec id="section6-0895904811400410">
<title>Why Create a National Indicator of College Academic Readiness?</title>
<p>A primary motivation for the development of a predictor of college readiness is concern over the quality of the high school diploma (e.g., <xref ref-type="bibr" rid="bibr12-0895904811400410">ADP, 2004a</xref>; <xref ref-type="bibr" rid="bibr46-0895904811400410">National Center on Education and the Economy, 2007</xref>). One manifestation of this concern is the ADP (a partnership of Achieve, the Education Trust, and the Fordham Foundation), which has undertaken analyses and written reports to examine the extent to which, among other things, graduation from high school signifies readiness for higher education and the world of work. They concluded, “While students and their parents may still believe that the diploma reflects adequate preparation for the intellectual demands of adult life, in reality it falls far short of this common sense goal” (<xref ref-type="bibr" rid="bibr13-0895904811400410">ADP, 2004b</xref>, Introduction). They also examined the content tested in high school graduation tests, finding the tests were not very demanding, with cut scores set equivalent to achievement at the seventh-to-ninth-grade-level accomplishment (<xref ref-type="bibr" rid="bibr13-0895904811400410">2004b</xref>). Another manifestation of this concern is the Common Core State Standards Initiative, sponsored by the National Governors Association, the Council of Chief State School Officers, and Achieve (<xref ref-type="bibr" rid="bibr48-0895904811400410">2008</xref>), which has noted that national standards toward college preparedness in other countries provide benchmarks that the United States could emulate. At this point, it is too early to know how the common core standards may help to build predictors of college readiness.</p>
<p>A well-designed and validated readiness assessment could help schools, districts, and states provide more uniform expectations for students across the country. This is an explicit goal of Achieve’s Algebra II exam—bringing about “a consistent level of content and rigor” in Algebra II courses in the participating states (<xref ref-type="bibr" rid="bibr3-0895904811400410">Achieve, 2008</xref>, p. 1). If there was a national college readiness exam, it might help states move their content standards and other assessments in the middle school and early high school years toward the domain of content assessed by the college readiness exam. Furthermore, a national target for college readiness would likely lead to a structuring of goals across grades to ensure readiness by the end of high school, as is being proposed in the Common Core State Standards Initiative, where standards in earlier grades have been developed by backing up from a goal of college readiness for 12th graders. This would likely lead toward a standardization of the college preparatory high school curriculum, at least in the content areas measured on the readiness assessment. Although the goal of universal college access may not be achievable or desirable, giving all students access to a curriculum that would prepare them for success in college, should they choose to enroll, is an important goal that we believe would be furthered by the use of a national college readiness assessment.</p>
<p>In addition to the goal of improving the quality of the high school diploma, a second motivation for a measure of college readiness is that large numbers of American students and their parents desire a postsecondary education. Thus, policy makers need information about college readiness. While potential indicator assessments measure important constructs, either college readiness is not among them or the standards assessed are not national or state representative. For instance, national 12th-grade NAEP reports percentages of U.S. 12th graders that are advanced or proficient but the extent to which high school seniors are ready for college is not reported. The Achieve Algebra II test reports whether students have mastered the ADP’s mathematics benchmarks for Algebra II, and investigations of how to define readiness on the assessments are underway, but the ADP assessment is only useful in those states that have adopted it. In contrast, ACT and PLAN both include college readiness measures in their student reports, but these are not and are unlikely to be national tests in the sense that they are required of representative samples of students in all states. As states are “where the rubber hits the road in standards-based reform” (<xref ref-type="bibr" rid="bibr20-0895904811400410">Carnevale &amp; Desrochers, 2003</xref>, p. 20), being able to report on college readiness state by state would undoubtedly be more important from a policy standpoint than simply reporting on college readiness at the national level.</p>
<p>A third motivation is to improve participation rates and student motivation on existing measures. For 12th-grade NAEP, participation rates fell to 55% in 2002 (National Commission on NAEP 12th-Grade Assessment and Reporting, <xref ref-type="bibr" rid="bibr47-0895904811400410">2004</xref>). Many believe that beyond low participation rates, those students who do take the test have little motivation to do well because there is nothing for students to gain by trying their best (e.g., <xref ref-type="bibr" rid="bibr18-0895904811400410">Brophy &amp; Ames, 2005</xref>). If the test were to become a valid indicator of college academic readiness, there might be greater desire to participate—and with appropriate motivation. Of course, NAEP does not produce student scores, so any increase in motivation would have to come from measuring and reporting academic readiness at aggregate levels. Similarly, making the Achieve Algebra II test a valid measure of college readiness might encourage more states to require Algebra II and the Algebra II assessment so that all students have access to that college academic readiness indicator. Although it is unlikely that making one of these assessments into a valid indicator of college academic readiness would necessarily make all students motivated and convince all states to adopt the instrument, it seems likely that at least some of the motivation problems could be allayed by the move.</p>
<p>Finally, in the case of 12th-grade NAEP, turning it into an assessment of college readiness could help achieve the goal of improving its content. For instance, historically, the 12th-grade NAEP mathematics test has not assessed what most would consider high school mathematics. The argument was that many students take little, if any, high school–level mathematics during their high school years, making it unfair and inappropriate to test them on content they have not had an opportunity to learn. The NAEP mathematics test has moved toward assessing high school mathematics but making the test a measure of college readiness might move it further.</p>
</sec>
<sec id="section7-0895904811400410">
<title>Approaches to Creating a National Predictor of College Readiness</title>
<p>Thus far, we have seen that there are a variety of definitions and indicators of college readiness. There are also a variety of predictors of college readiness, and their predictive power is quite good for at least some definitions and indicators of readiness. Still, there is much enthusiasm for validating a national predictor of college academic readiness. The question addressed here is what approaches can be used to validate national predictors of college readiness. There are several possibilities, and each has advantages and disadvantages. Each approach could stand alone or the approaches could be used in combination with each other. All begin with choosing an assessment such as ACT, an end-of-course test, or 12th-grade NAEP test. The focus here is not on construction of new predictors but rather validation of existing national predictors.</p>
<p>In the case of NAEP, only a probability sample of students is assessed. At present, there is not even state-by-state 12th-grade NAEP. There is, however, work underway at the National Assessment Governing Board (the policy body for NAEP) to build and validate a 12th-grade NAEP that measures college preparedness. The goal of that effort is to use 12th-grade NAEP as an indicator of the percentage of students in 12th grade that are ready for college. Neither would this require reporting NAEP scores at the individual student level nor would it require that all students take 12th-grade NAEP. The effort might, however, lead to a state-by-state 12th-grade NAEP in which the percentage of college-ready 12th-grade students could be estimated at the aggregate level in each state and tracked over time.</p>
<p>The Achieve Algebra II work is also moving forward, and there the intention is to measure readiness at the level of the individual student. However, the likelihood of state- or nationally representative data is much lower than for 12th-grade NAEP. The NAEP focus is on indicators that could be used to monitor systems, and the Algebra II focus is on diagnostic information for individual students, teachers, schools, and states but limited to Algebra II. Ideally, a test could accomplish both—providing feedback to individual students, teachers, and schools as well as providing representative information to state and national policy makers.</p>
<sec id="section8-0895904811400410">
<title>A Longitudinal Study</title>
<p>When one thinks of establishing an indicator of college readiness, probably the first approach that comes to mind is a longitudinal study. The test (e.g., SAT, Achieve’s Algebra II test, NAEP) would be given to high school seniors who would be followed into college with various criterion measures taken, such as number of remedial courses taken, freshman GPA, and persistence to a 2nd year. At least in an initial study, it would be good to take other college readiness indicators into account. Thus, one might investigate the predictive power of the assessment for a specific indicator of college success (e.g., freshman GPA, persistence to graduation) and see the extent to which it has predictive value above and beyond the other indicators used. These additional variables (e.g., high school GPA) would not be needed to establish the assessment as an indicator of readiness, but it would be good to know the extent to which the assessment under investigation has predictive value above and beyond what is typically used.</p>
<p>Clearly, longitudinal studies are expensive as students must be followed over time. Furthermore, studying persistence is complicated; not all students go immediately to college, and not all students that go immediately to college go immediately to a 2nd year. Those that go to a 2nd year do not always go to the same college as they did their 1st year. An ideal study would follow students for up to 5 years so that predictive validity could be studied for both students who go immediately to college and those who go later. For feasibility purposes, however, a useful study might be limited to following students for just a year and a half after they graduate from high school.</p>
<p>Once nationally representative longitudinal data were gathered, cut scores indicating college readiness based on a certain standard of success would be set on the assessment based on the regression equations for each of several types of institutions. Analyses might yield results indicating, for instance, the percentage of a state’s high school students who are predicted to obtain a freshman GPA of at least 2.5 at community colleges, based on their performance on the assessment. Similar analyses could be done for other criteria of success, including avoidance of remediation, persistence to a 2nd year of college, and completion.</p>
<sec id="section9-0895904811400410">
<title>Additional issues for NAEP</title>
<p>Using NAEP as the measure of college readiness would raise additional issues. For one, using NAEP for a longitudinal study would require student identification and tracking procedures to follow students into college. Allowing that would require congressional approval. For another, NAEP data pose several analysis problems for building the regression equations based on the longitudinal samples. However, no new methodological ground would need to be broken; the National Center for Education Statistics (<xref ref-type="bibr" rid="bibr11-0895904811400410">Allen, Carlson, &amp; Zelenak, 1999</xref>) has laid out procedures for dealing with all of these problems. Current versions of the hierarchical linear modeling (HLM) software have built-in options for dealing with NAEP complexities. Thus, although NAEP is not designed to yield reliable and valid scores at the individual student level and results are never reported at the individual student level, researchers have used NAEP data sets to conduct analyses where <italic>student</italic> is the unit of analysis (e.g., <xref ref-type="bibr" rid="bibr16-0895904811400410">Braun, 2004</xref>; <xref ref-type="bibr" rid="bibr32-0895904811400410">Grissmer &amp; Flanagan, 1998</xref>). The complexities of the NAEP matrix sampling design do not preclude the use of NAEP as a predictor of college readiness. Current legislation, however, does not permit tracking NAEP-tested students longitudinally.</p>
</sec>
</sec>
<sec id="section10-0895904811400410">
<title>Using a College Sample</title>
<p>A less difficult and expensive approach to validating a national indicator of college readiness would be to give the readiness test to a national sample of college freshman stratified by type of institution. For instance, <xref ref-type="bibr" rid="bibr15-0895904811400410">Barton (2003)</xref> recommended giving NAEP to admitted students when they take placement tests. He briefly describes how to report results, “NAEP scores could be averaged for the students required and those not required to take remedial courses” (p. 9) or “Alternately, NAEP scores of students who took the placement test and scored in a range above the cut point could be used for comparison” (p. 10).</p>
<p>For a college sample to supply valid information about an indicator of college readiness, we would need to assume that students would do essentially the same on the assessment if taken at the start of their college freshman year as they would if taken in high school. This assumption is probably not justified as there may be a summer drop in achievement, especially for students from low-income families and students of color (<xref ref-type="bibr" rid="bibr51-0895904811400410">Porter, 2005</xref>). Another complication is that getting state representative samples from college freshman would not be straightforward. Certainly, it would not be appropriate to take samples from colleges in each state since students cross state boundaries to go to college.</p>
<p>Even if the readiness assessment were given to college freshman in the fall, the study would need to be longitudinal—at least to the beginning of the next year—unless one was willing to say that the indicator of college readiness is simply the number of remedial courses students were required to take in their first semester. In addition, it might be difficult to get additional predictors of college success such as aptitude scores, high school GPA, and high school courses taken. The same methodological approaches to establishing cut scores as sketched above for the longitudinal study would be applicable. Clearly, although using a college sample would be less expensive, it would provide weaker evidence than the longitudinal study.</p>
</sec>
<sec id="section11-0895904811400410">
<title>Standard Setting</title>
<p>Another approach to establishing a national indicator of college readiness would be to use standard-setting procedures analogous to those states currently use to set proficiency levels on state assessments. The procedure would start by convening panels of experts to define what content students must know to be considered ready for college. These panels might be convened separately by type of institution as described for the longitudinal study. Members would be appropriate faculty from the institutions of higher education. Especially important would be faculty members who teach the introductory courses on which readiness thresholds are to be set. Once these faculty members had written descriptions of what students must know and be able to do to be considered ready for college, a separate set of experts might be convened to complete something like a Bookmark procedure (<xref ref-type="bibr" rid="bibr40-0895904811400410">Mitzel, Lewis, Patz, &amp; Green, 2001</xref>) for setting proficiency levels of college readiness on the readiness predictor using the readiness descriptions (or degrees of college readiness). Multiple panels could be used to improve the stability of the estimates of the cut score.</p>
<p>Experts could also be used to do content analyses of the assessment items to determine the extent to which they are aligned with the content believed necessary for college readiness. The content analyses would determine the extent to which the readiness assessment tests the material that others believe is necessary for college readiness. To have the most leverage over curriculum, the assessment would have to be carefully aligned so that it tests the content deemed necessary for college success. If the content analyses were done at a fine-grained level of detail, performance could be reported at subscale levels (e.g., algebra &amp; number sense within mathematics). This would make the assessment even more valuable as an indicator of specific areas of strength and need in terms of college readiness at the state or national level.</p>
<p>The content analyses would need to be done in ways that were explicit and replicable. One way to accomplish this would be to use already established content analysis tools (e.g., <xref ref-type="bibr" rid="bibr50-0895904811400410">Porter, 2002</xref>). These tools measure content at the intersection of topic and cognitive demand, and they require just four trained coders to achieve highly reliable data (<xref ref-type="bibr" rid="bibr52-0895904811400410">Porter, Polikoff, Zeidner &amp; Smithson, 2008</xref>). The content analyses could be used to determine the degree to which alignment differs across types of colleges. None of the content analysis work of proposed college readiness assessments to date has met these criteria.</p>
</sec>
<sec id="section12-0895904811400410">
<title>Linking the Assessment to Other Predictors</title>
<p>A final approach to validating a national predictor of college readiness would be to link the predictor to the other predictors of academic readiness, such as high school GPA and course taking. Because of the previously noted inadequacies of current predictors of college readiness, we find this approach less satisfactory though probably the least expensive option.</p>
<p>Several researchers have demonstrated how a new predictor of college readiness could be linked to already established predictors. <xref ref-type="bibr" rid="bibr38-0895904811400410">Linn and Kiplinger (1995)</xref> and <xref ref-type="bibr" rid="bibr26-0895904811400410">Ercikan (1997)</xref> investigated linking statewide tests to NAEP. Two reports (<xref ref-type="bibr" rid="bibr53-0895904811400410">Rock &amp; Pollack, 1995</xref>; <xref ref-type="bibr" rid="bibr54-0895904811400410">Scott, Ingels, &amp; Owings, 2008</xref>), using data from NCES longitudinal studies, linked the NAEP exam to the tests of student achievement given in those studies. As these national longitudinal studies follow students over time, the link between the college readiness predictor and their student achievement tests would appear to essentially accomplish much of the longitudinal study described above but without any additional investment other than the linking.</p>
<p>Unfortunately, other evidence suggests this inexpensive solution to investigating validity is not likely to work particularly well. A National Academy of Science, National Research Council committee charged with investigating the feasibility of establishing an equivalency scale to link state achievement tests to one another and NAEP concluded,</p>
<p><disp-quote>
<p>Links between most existing tests and NAEP, for the purpose of reporting individual students’ scores on the NAEP scale, and in terms of the NAEP achievement levels, will be problematic. Unless the test to be linked to NAEP is very similar to NAEP in content, format, and uses, the resulting linkage is likely to be unstable and potentially misleading. (<xref ref-type="bibr" rid="bibr27-0895904811400410">Feuer, Holland, Green, Bertenthal, &amp; Hemphill, 1999</xref>, p. 4)</p>
</disp-quote></p>
<p>Similarly, <xref ref-type="bibr" rid="bibr58-0895904811400410">Thissen (2007)</xref>, in a summary of the studies of linking the NAEP with state assessments, concluded that linkages of the NAEP to state assessments tend to vary over time and across subpopulations. Overall, the literature on linking and NAEP to date suggests that such linking would be difficult if not impossible except in the case of the assessment used in the NCES longitudinal studies. Thus, until methodological advances solve this problem, linking might not be feasible for NAEP. Such problems do not accompany other potential linking assessments, such as the ACT.</p>
<p>When linking is done to, say, the ACT, what we know about performance on the ACT predicting college success can be translated into the metric of the readiness assessment. In the case of the ACT, for instance, the readiness standards they have identified for English Composition, College Algebra, Social Science, and Biology (<xref ref-type="bibr" rid="bibr10-0895904811400410">Allen &amp; Sconing, 2005</xref>) could then be used for the college readiness indicator. What the linking cannot accomplish is to make up for the current inadequacies in college readiness predictors.</p>
</sec>
</sec>
<sec id="section13-0895904811400410">
<title>Reporting</title>
<p>An important component of the utility of a college readiness predictor would be how the results are reported. Results would certainly be reported in the aggregate, such as “80% of the students in Arkansas did sufficiently well on the math assessment that their chances of needing to take one or more remedial courses in mathematics are 1 in 10 if they attend a community college.” The results could be disaggregated by type of student to address the readiness gaps that likely parallel the well-known achievement gaps. These results would be of critical interest to state and national policy makers and could be used to compare policies in terms of high school course taking, for instance.</p>
<p>Depending on the type of assessment used, the reporting could also be done in a diagnostic way for schools or individual students.<sup><xref ref-type="fn" rid="fn1-0895904811400410">1</xref></sup> For this to be the case, the assessment would need to contain reliable subtests. Furthermore, the test could not use a complicated sampling mechanism like NAEP’s. Achieve’s Algebra II test or any end-of-course test would qualify for student- or school-level reporting. Student- or school-level reporting could be diagnostic if feedback were provided in a timely fashion. Students could be informed, for instance, about their readiness for college success and given information about areas on which to focus to increase their likelihood of future success.</p>
</sec>
<sec id="section14-0895904811400410">
<title>Discussion</title>
<p>Several definitions of college readiness have been identified and several different approaches to validating predictors of college academic readiness have been described. Each approach is technically feasible. Each approach has different strengths and weaknesses. Perhaps the most obvious approach is to conduct a longitudinal study of the predictive validity of a chosen assessment for college readiness for each of several definitions of college readiness. Second, the assessment could be given to college freshmen to establish cut scores for various definitions of college readiness. This approach is straightforward but makes the assumption that the achievement of beginning college freshmen is the same as the achievement of high school students. Third, the assessment could be made an indicator of college readiness through a performance standard–setting process much like the process used to set performance standards on state achievement tests. Instead of setting a proficiency standard, panels of experts would set college readiness standards. A fourth approach is to link the assessment to one or more of the commonly used predictors of college readiness. The results would integrate the chosen assessment as a readiness indictor into the larger literature on indicators of college readiness.</p>
<p>Given the hard work and expense of whatever approach is chosen, one must ask the question as to whether it is worth the effort to build and validate another indicator of college readiness. On one hand, we already know a great deal about the readiness of high school graduates for college; they are not as ready as we would like them to be. We already have good predictors of college freshman GPA in the form of aptitude tests, placement tests, high school grades, high school class rank, and courses taken. In fact, it is likely that there would be little unique predictive value added by new readiness assessments. However, none of the current predictors of freshman GPA have been validated for predicting college readiness for different types of higher education institutions. Furthermore, while the ACT was designed as a test to be used for guidance and placement, as well as admissions (<xref ref-type="bibr" rid="bibr19-0895904811400410">Camilli, 2006</xref>), neither it nor the SAT are given to representative samples. Both assessments could be given to representative samples, or they could be required of all 11th or 12th graders, in which case the studies we describe here would be relatively straightforward and no new test construction would need to be done.</p>
<p>Alternatively, the work of Achieve and its partners on Algebra II represents a potentially promising method for building a number of useful readiness predictors. Nevertheless, the validity work described here lies ahead to establish the Algebra II exam as a valid predictor of readiness. If similar decisions were made in other content areas, assessments built around the content deemed to be important, and validity work described here completed, we would have a comprehensive set of predictors of college readiness and tools to help raise and equalize the quality of high school coursework. This would require substantially more work than simply requiring the ACT or SAT of all or a representative sample of students, but it might be more useful in terms of diagnostic information in individual content areas. Furthermore, using assessments based on particular content areas might be more useful if the end goal is predicting readiness by major or at multiple institution types.</p>
<p>A great deal of interest and attention has been placed on all students graduating from high school, ready for college. At present, there are no data routinely available to states or at the national level that tell educators and policy makers the extent to which the goal is being achieved. NAEP tells states what percentage of their students are proficient in each of several subjects at 4th, 8th, and 12th grade, but being proficient does not necessarily mean college ready or being on target to be college ready. Common core standards developed by the National Governors Association and the Council of Chief State School Officers set college readiness content standards in mathematics and English language arts and reading for 12th graders, and these content standards are being extended to earlier grade levels to specify what content is necessary to be on target for college readiness. Still, neither have these content standards been validated nor have measures been built of these standards and college readiness cut scores set.</p>
<p>We have shown how predictors of college readiness could be built or existing predictors validated. The goal is to have predictors of college readiness for state and nationally representative samples and where readiness is distinguished for different kinds of colleges. If the representative samples are of sufficient size, data can be disaggregated to describe college readiness by subgroup, such as ethnicity and gender. If there were a good predictor of college academic readiness that provided results at the state level, states would receive descriptions of their students’ college readiness which they might find useful as they think about state education policy K-16. Some state-by-state indicators currently exist (e.g., <xref ref-type="bibr" rid="bibr45-0895904811400410">National Center for Public Policy and Higher Education, 2008</xref>), but readiness predictors based on specific skills and student performance would undoubtedly be more useful for planning policies to improve instruction and student performance. Knowing what percentage of a state’s students are college ready does not say in any direct way what the state, districts, or schools should do to improve that percentage. Still, monitoring the percentage of students who are college ready on a regular basis would help the nation, state, and districts know the extent to which they are successful in reaching the goal of 100% of high school graduates ready for college. If the percentages are unacceptably low, clearly new solutions must be tried and evaluated. We believe better and better validated predictions of college readiness for large representative samples of students, state by state, would stimulate and guide the process of addressing this important goal.</p>
<p>Finally, in the work done thus far, distinctions among types of postsecondary experiences are either glossed over or are not made explicit in reporting. All but the linking approach sketched above would make these contrasts explicit. It is doubtful whether anyone really believes that college readiness has the same meaning across dramatically different types of postsecondary institutions. Having readiness rates for different types of college-going experiences would be useful for states and the general public.</p>
<p>Of course, there are many important policy issues in higher education that would not be addressed by the readiness assessment strategies discussed here. These include the recent push for accountability in higher education (e.g., <xref ref-type="bibr" rid="bibr55-0895904811400410">Spellings, 2006</xref>) and the development of programs and policies at colleges and universities to remediate student needs. Although the proposed assessment development strategies would lead to assessments that could be useful for addressing these issues, we take higher education for what it is without addressing issues of possible higher education reform. Instead, our purpose is to ask whether students are ready for the higher education system that we have currently. With that in mind, issues of accountability and developing remediation programs to meet student needs based on assessment results, although important, go beyond the scope of this piece.</p>
<p>We conclude that if an assessment were developed (or an existing assessment were used) that is a test of college preparatory content and if it were (a) administered regularly with acceptable participation rates and student motivation and (b) reported at the state and national levels, describing readiness by type of institution of higher education, the result would be new and useful information for policy makers. An assessment with results reported at the district, school, and student levels would add administrators, teachers, parents, and students as users. Both the National Assessment Governing Board for NAEP and Achieve and its partners for Algebra II are moving forward with plans to build and validate new predictors of college readiness (though for NAEP, the term <italic>preparedness</italic> is being used instead of readiness). Some states are moving in the same direction. If they follow the advice presented here, the result will be valuable new indicators of college readiness that serve to guide and monitor important high school and college reforms.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the authorship and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) received no financial support for the research and/or authorship of this article.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0895904811400410">
<label>1.</label>
<p>The idea of student-level reporting has particular implications for NAEP. We do not imagine that results would be reported at the student level for NAEP, as this would require fundamental changes to the way the assessment is designed and conducted. Furthermore, we do not imagine that the timing of NAEP would be changed to make it diagnostic for individuals. Thus, NAEP would not be a suitable readiness assessment if the goal was diagnostic information for this year’s individual students.</p>
</fn>
</fn-group>
</notes>
<bio>
<title>Bios</title>
<p><bold>Andrew C. Porter</bold> is dean of the Graduate School of Education, University of Pennsylvania. He is an applied statistician and psychometrician who studies the measurement of education leadership, student achievement testing, curriculum policies and their effects, and professional development for teachers.</p>
<p><bold>Morgan S. Polikoff</bold> is an assistant professor of K-12 education policy at the University of Southern California’s Rossier School of Education. His research focuses on state and federal curriculum and assessment policies and their impact on teachers’ instruction and student outcomes.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0895904811400410">
<citation citation-type="journal">
<comment>Author</comment>, <year>1998</year>.</citation>
</ref>
<ref id="bibr2-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Abraham</surname><given-names>A. A.</given-names><suffix>Jr.</suffix></name>
</person-group> (<year>1992</year>). <source>College remedial studies: Institutional practices in the SREB States</source>. <publisher-loc>Atlanta, GA</publisher-loc>: <publisher-name>Southern Regional Education Board</publisher-name>.</citation>
</ref>
<ref id="bibr3-0895904811400410">
<citation citation-type="book">
<collab>Achieve, Inc.</collab> (<year>2008</year>). <source>Algebra II end-of-course exam fact sheet</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr4-0895904811400410">
<citation citation-type="book">
<collab>ACT, Inc.</collab> (<year>2004</year>). <source>Crisis at the core: Preparing all students for college and work</source>. <publisher-loc>Iowa City, IA</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr5-0895904811400410">
<citation citation-type="book">
<collab>ACT, Inc.</collab> (<year>2009</year>). <source>The condition of college readiness 2009</source>. <publisher-loc>Iowa City, IA</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr6-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Adelman</surname><given-names>C.</given-names></name>
</person-group> (<year>1994</year>). <source>Lessons of a generation</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr7-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Adelman</surname><given-names>C.</given-names></name>
</person-group> (<year>1999</year>). <source>Answers in the tool box. Academic intensity, attendance patterns, and bachelor’s degree attainment</source> (<comment>Report No. PLLI-1999-8021</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Institute on Postsecondary Education, Libraries, and Lifelong Learning</publisher-name>. (<comment>ERIC Document Reproduction Service No. ED431363</comment>)</citation>
</ref>
<ref id="bibr8-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Adelman</surname><given-names>C.</given-names></name>
</person-group> (<year>2006</year>). <source>The toolbox revisited: Paths to degree completion from high school through college</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Office of Vocational and Adult Education, U.S. Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr9-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Adelman</surname><given-names>C.</given-names></name>
<name><surname>Daniel</surname><given-names>B.</given-names></name>
<name><surname>Berkovits</surname><given-names>I.</given-names></name>
<name><surname>Owings</surname><given-names>J.</given-names></name>
</person-group> (<year>2003</year>). <source>Postsecondary attainment, attendance, curriculum, and performance: Selected results from the NELS:88/2000 Postsecondary Education Transcript Study (PETS), 2000</source> (<comment>Report No. NCES-2003-394</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Center for Education Statistics</publisher-name>. (<comment>ERIC Document Reproduction Service No. ED480959</comment>)</citation>
</ref>
<ref id="bibr10-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Allen</surname><given-names>J.</given-names></name>
<name><surname>Sconing</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <source>Using ACT assessment scores to set benchmarks for college readiness</source> (<comment>ACT Research Report Series 2005-3</comment>). <publisher-loc>Iowa City, IA</publisher-loc>: <publisher-name>ACT</publisher-name>.</citation>
</ref>
<ref id="bibr11-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Allen</surname><given-names>N. L.</given-names></name>
<name><surname>Carlson</surname><given-names>J. E.</given-names></name>
<name><surname>Zelenak</surname><given-names>C. A.</given-names></name>
</person-group> (<year>1999</year>). <source>The NAEP 1996 technical report</source> (<comment>NCES Publication No. 1999-452</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Center for Education Statistics</publisher-name>.</citation>
</ref>
<ref id="bibr12-0895904811400410">
<citation citation-type="book">
<collab>American Diploma Project</collab>. (<year>2004a</year>). <source>Do graduation tests measure up? A closer look at state high school exit exams</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Achieve</publisher-name>.</citation>
</ref>
<ref id="bibr13-0895904811400410">
<citation citation-type="book">
<collab>American Diploma Project</collab>. (<year>2004b</year>). <source>Ready or not: Creating a high school diploma that counts</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Achieve</publisher-name>.</citation>
</ref>
<ref id="bibr14-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Attewell</surname><given-names>P.</given-names></name>
<name><surname>Lavin</surname><given-names>D.</given-names></name>
<name><surname>Domina</surname><given-names>T.</given-names></name>
<name><surname>Levey</surname><given-names>T.</given-names></name>
</person-group> (<year>2006</year>). <article-title>New evidence on college remediation</article-title>. <source>Journal of Higher Education</source>, <volume>77</volume>, <fpage>886</fpage>-<lpage>924</lpage>.</citation>
</ref>
<ref id="bibr15-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Barton</surname><given-names>P. E.</given-names></name>
</person-group> (<year>2003</year>). <source>Grading the twelfth graders: More useful and more used NAEP reporting?</source> <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Assessment Governing Board</publisher-name>.</citation>
</ref>
<ref id="bibr16-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Braun</surname><given-names>H.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Reconsidering the impact of high-stakes testing</article-title>. <source>Education Policy Analysis Archives</source>, <volume>12</volume>(<issue>1</issue>).</citation>
</ref>
<ref id="bibr17-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bridgeman</surname><given-names>B.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Essays and multiple-choice tests as predictors of college freshman GPA</article-title>. <source>Research in Higher Education</source>, <volume>32</volume>, <fpage>319</fpage>-<lpage>331</lpage>.</citation>
</ref>
<ref id="bibr18-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brophy</surname><given-names>J.</given-names></name>
<name><surname>Ames</surname><given-names>C.</given-names></name>
</person-group> (<year>2005</year>). <source>NAEP testing for twelfth graders: Motivational issues</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Assessment Governing Board</publisher-name>.</citation>
</ref>
<ref id="bibr19-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Camilli</surname><given-names>G.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Test fairness</article-title>. In <person-group person-group-type="editor">
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (Ed.), <source>Educational measurement</source> (<edition>4th ed.</edition>, pp. <fpage>221</fpage>-<lpage>256</lpage>). <publisher-loc>Westport, CT</publisher-loc>: <publisher-name>ACE/Praeger</publisher-name>.</citation>
</ref>
<ref id="bibr20-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Carnevale</surname><given-names>A. P.</given-names></name>
<name><surname>Desrochers</surname><given-names>D. M.</given-names></name>
</person-group> (<year>2003</year>). <source>Considerations in using 12th grade NAEP as a prospective indicator of readiness for college and employment</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr21-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Conley</surname><given-names>D. T.</given-names></name>
</person-group> (<year>2003</year>). <source>Understanding university success</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>Pew Charitable Trusts</publisher-name>.</citation>
</ref>
<ref id="bibr22-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Conley</surname><given-names>D. T.</given-names></name>
</person-group> (<year>2007</year>). <source>Toward a more comprehensive conception of college readiness</source>. <publisher-loc>Eugene, OR</publisher-loc>: <publisher-name>Educational Policy Improvement Center</publisher-name>.</citation>
</ref>
<ref id="bibr23-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>DesJardins</surname><given-names>S. L.</given-names></name>
<name><surname>Lindsay</surname><given-names>N. K.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Adding a statistical wrench to the “toolbox.”</article-title> <source>Research in Higher Education</source>, <volume>49</volume>, <fpage>172</fpage>-<lpage>179</lpage>.</citation>
</ref>
<ref id="bibr24-0895904811400410">
<citation citation-type="journal">
<collab>Editorial Projects in Education</collab>. (<year>2009</year>). <article-title>Diplomas count: Broader horizons—The challenge of college readiness for all students</article-title>. <source>Education Week</source>, <volume>28</volume>(<issue>34</issue>).</citation>
</ref>
<ref id="bibr25-0895904811400410">
<citation citation-type="journal">
<collab>Education Trust</collab>. (<year>1999</year>). <article-title>Ticket to nowhere: The gap between leaving high school and entering college and high performance jobs</article-title>. <source>Thinking K-16</source>, <volume>3</volume>(<issue>2</issue>).</citation>
</ref>
<ref id="bibr26-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ercikan</surname><given-names>K.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Linking statewide tests to the National Assessment of Educational Progress: Accuracy of combining test results across states</article-title>. <source>Applied Measurement in Education</source>, <volume>10</volume>(<issue>2</issue>), <fpage>145</fpage>-<lpage>159</lpage>.</citation>
</ref>
<ref id="bibr27-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Feuer</surname><given-names>M. J.</given-names></name>
<name><surname>Holland</surname><given-names>P. W.</given-names></name>
<name><surname>Green</surname><given-names>B. F.</given-names></name>
<name><surname>Bertenthal</surname><given-names>M. W.</given-names></name>
<name><surname>Hemphill</surname><given-names>F. C.</given-names></name>
</person-group> (<year>1999</year>). <source>Uncommon measures: Equivalence and linkage among educational tests</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academy Press</publisher-name>.</citation>
</ref>
<ref id="bibr28-0895904811400410">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Finder</surname><given-names>A.</given-names></name>
</person-group> (<year>2006</year>, <month>March</month> <day>5</day>). <article-title>Schools avoid class ranking, vexing colleges</article-title>. <source>The New York Times</source>. <comment>Retreived from <ext-link ext-link-type="uri" xlink:href="http://www.nytimes.com/2006/03/05/education/05rank.html">http://www.nytimes.com/2006/03/05/education/05rank.html</ext-link></comment> on <access-date>28 February 2011</access-date>.</citation>
</ref>
<ref id="bibr29-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fleming</surname><given-names>J.</given-names></name>
<name><surname>Garcia</surname><given-names>N.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Are standardized tests fair to African Americans?</article-title> <source>Journal of Higher Education</source>, <volume>69</volume>, <fpage>471</fpage>-<lpage>495</lpage>.</citation>
</ref>
<ref id="bibr30-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Geiser</surname><given-names>S.</given-names></name>
<name><surname>Studley</surname><given-names>R.</given-names></name>
</person-group> (<year>2002</year>). <source>UC and the SAT: Predictive validity and differential impact of the SAT I and the SAT II at the University of California</source>. <publisher-loc>Oakland</publisher-loc>: <publisher-name>University of California Press</publisher-name>.</citation>
</ref>
<ref id="bibr31-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Greene</surname><given-names>J. P.</given-names></name>
<name><surname>Forster</surname><given-names>G.</given-names></name>
</person-group> (<year>2003</year>). <source>Public high school graduation and college readiness rates in the United States</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Center for Civic Innovation, Manhattan Institute</publisher-name>.</citation>
</ref>
<ref id="bibr32-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Grissmer</surname><given-names>D.</given-names></name>
<name><surname>Flanagan</surname><given-names>A.</given-names></name>
</person-group> (<year>1998</year>). <source>Exploring rapid achievement gains in North Carolina and Texas</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Education Goals Panel</publisher-name>.</citation>
</ref>
<ref id="bibr33-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ingels</surname><given-names>S. J.</given-names></name>
<name><surname>Dalton</surname><given-names>B. W.</given-names></name>
<name><surname>LoGerfo</surname><given-names>L.</given-names></name>
</person-group> (<year>2008</year>). <source>Trends among high school seniors 1972-2004</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Center for Education Statistics</publisher-name>.</citation>
</ref>
<ref id="bibr34-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kirst</surname><given-names>M. W.</given-names></name>
</person-group> (<year>2003</year>). <source>College preparation and grade 12 NAEP</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Assessment Governing Board</publisher-name>.</citation>
</ref>
<ref id="bibr35-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kirst</surname><given-names>M. W.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Improving preparation for nonselective postsecondary education: Assessment and accountability issues</article-title>. In <person-group person-group-type="editor">
<name><surname>Dwyer</surname><given-names>C. A.</given-names></name>
</person-group> (Ed.), <source>Measurement and research in the accountability era</source> (pp. <fpage>301</fpage>-<lpage>314</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr36-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kobrin</surname><given-names>J. L.</given-names></name>
<name><surname>Patterson</surname><given-names>B. F.</given-names></name>
<name><surname>Shaw</surname><given-names>E. J.</given-names></name>
<name><surname>Mattern</surname><given-names>K. D.</given-names></name>
<name><surname>Barbuti</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2008</year>). <source>Validity of the SAT for predicting first year college grade point average</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr37-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lewis</surname><given-names>H. R.</given-names></name>
</person-group> (<year>2007</year>). <source>Excellence without a soul: Does liberal education have a future?</source> <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Public Affairs</publisher-name>.</citation>
</ref>
<ref id="bibr38-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Linn</surname><given-names>R. L.</given-names></name>
<name><surname>Kiplinger</surname><given-names>V. L.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Linking statewide test to the National Assessment of Educational Progress: Stability of results</article-title>. <source>Applied Measurement in Education</source>, <volume>8</volume>, <fpage>135</fpage>-<lpage>155</lpage>.</citation>
</ref>
<ref id="bibr39-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Merisotis</surname><given-names>J.</given-names></name>
<name><surname>Phipps</surname><given-names>R.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Remedial education in colleges and universities: What’s really going on?</article-title> <source>Review of Higher Education</source>, <volume>24</volume>(<issue>1</issue>), <fpage>67</fpage>-<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr40-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mitzel</surname><given-names>H. C.</given-names></name>
<name><surname>Lewis</surname><given-names>D. M.</given-names></name>
<name><surname>Patz</surname><given-names>R. J.</given-names></name>
<name><surname>Green</surname><given-names>D. R.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The bookmark procedure: Psychological perspectives</article-title>. In <person-group person-group-type="editor">
<name><surname>Cizek</surname><given-names>G. J.</given-names></name>
</person-group> (Ed.), <source>Setting performance standards: Concepts, methods, and perspectives</source> (pp. <fpage>249</fpage>-<lpage>282</lpage>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr41-0895904811400410">
<citation citation-type="book">
<collab>National Assessment Governing Board</collab>. (<year>2008</year>). <source>Technical panel on 12th grade preparedness research</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr42-0895904811400410">
<citation citation-type="book">
<collab>National Center for Education Statistics</collab>. (<year>2002</year>). <source>Profile of undergraduates in U.S. postsecondary institutions: 1999-2000</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr43-0895904811400410">
<citation citation-type="book">
<collab>National Center for Education Statistics</collab>. (<year>2004</year>). <source>Conditions of education 2004</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr44-0895904811400410">
<citation citation-type="book">
<collab>National Center for Education Statistics</collab>. (<year>2007</year>). <source>The condition of education 2007</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr45-0895904811400410">
<citation citation-type="book">
<collab>National Center for Public Policy and Higher Education</collab>. (<year>2008</year>) <source>Measuring up 2008: The national report card on higher education</source>. <publisher-loc>San Jose, CA</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr46-0895904811400410">
<citation citation-type="book">
<collab>National Center on Education and the Economy</collab>. (<year>2007</year>). <source>Tough choices or tough times: The report of the new commission on the skills of the American workplace</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr47-0895904811400410">
<citation citation-type="book">
<collab>National Commission on NAEP 12th Grade Assessment and Reporting</collab>. (<year>2004</year>). <source>12th grade student achievement in America: A new vision for NAEP</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Assessment Governing Board</publisher-name>.</citation>
</ref>
<ref id="bibr48-0895904811400410">
<citation citation-type="book">
<collab>National Governors Association, Council of Chief State School Officers, &amp; Achieve, Inc</collab>. (<year>2008</year>). <source>Benchmarking for success: Ensuring U.S. students receive a world-class education</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Authors</publisher-name>.</citation>
</ref>
<ref id="bibr49-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Olson</surname><given-names>L.</given-names></name>
</person-group> (<year>2006</year>, <month>April</month> <day>26</day>). <article-title>Views differ on defining college prep</article-title>. <source>Education Week</source>, <volume>25</volume>(<issue>33</issue>), <fpage>1</fpage>, <fpage>26</fpage>, <fpage>28</fpage>.</citation>
</ref>
<ref id="bibr50-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>A. C.</given-names></name>
</person-group> (<year>2002</year>, <month>October</month>). <article-title>Measuring the content of instruction: Uses in research and practice</article-title>. <source>Educational Researcher</source>, <volume>31</volume>(<issue>7</issue>), <fpage>3</fpage>-<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr51-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>A. C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Prospects for school reform and closing the achievement gap</article-title>. In <person-group person-group-type="editor">
<name><surname>Dwyer</surname><given-names>C. A.</given-names></name>
</person-group> (Ed.), <source>Measurement and research in the accountability era</source> (pp. <fpage>59</fpage>-<lpage>95</lpage>).<publisher-loc>Mahwah, NJ</publisher-loc>:<publisher-name>Lawrence Erlbaum Associates</publisher-name>.</citation>
</ref>
<ref id="bibr52-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>A.C.</given-names></name>
<name><surname>Polikoff</surname><given-names>M.S.</given-names></name>
<name><surname>Zeidner</surname><given-names>T.</given-names></name>
<name><surname>Smithson</surname><given-names>J.</given-names></name>
</person-group> (<month>December</month> <year>2008</year>). <article-title>The quality of content analyses of state student achievement tests and state content standards</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>27</volume>(<issue>4</issue>), <fpage>2</fpage>-<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr53-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rock</surname><given-names>D. A.</given-names></name>
<name><surname>Pollack</surname><given-names>J. M.</given-names></name>
</person-group> (<year>1995</year>). <source>Psychometric report for the NELS:88 base year through second follow-up</source> (<comment>NCES Publication No. 95-382</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Government Printing Office</publisher-name>.</citation>
</ref>
<ref id="bibr54-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Scott</surname><given-names>L. A.</given-names></name>
<name><surname>Ingels</surname><given-names>S. J.</given-names></name>
<name><surname>Owings</surname><given-names>J. A.</given-names></name>
</person-group> (<year>2008</year>). <source>Interpreting 12th-graders’ NAEP-scaled mathematics performance using high school predictors and postsecondary outcomes from the National Education Longitudinal Study of 1988 (NELS:88)</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Center for Education Statistics</publisher-name>.</citation>
</ref>
<ref id="bibr55-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Spellings</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <source>A test of leadership: Charting the future of U.S. higher education</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr56-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>St. John</surname><given-names>E. P.</given-names></name>
<name><surname>Cabrera</surname><given-names>A. F.</given-names></name>
<name><surname>Nora</surname><given-names>A.</given-names></name>
<name><surname>Asker</surname><given-names>E. H.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Economic influences on persistence reconsidered</article-title>. In <person-group person-group-type="editor">
<name><surname>Braxton</surname><given-names>J. M.</given-names></name>
</person-group> (Ed.), <source>Reworking the student departure puzzle</source> (pp. <fpage>29</fpage>-<lpage>47</lpage>). <publisher-loc>Nashville, TN</publisher-loc>: <publisher-name>Vanderbilt University Press</publisher-name>.</citation>
</ref>
<ref id="bibr57-0895904811400410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stanley</surname><given-names>J. C.</given-names></name>
<name><surname>Porter</surname><given-names>A. C.</given-names></name>
</person-group> (<year>1967</year>). <article-title>Correlation of scholastic aptitude scores with college grades for Negros versus Whites</article-title>. <source>Journal of Educational Measurement</source>, <volume>4</volume>, <fpage>199</fpage>-<lpage>218</lpage>.</citation>
</ref>
<ref id="bibr58-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thissen</surname><given-names>D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Linking assessments based on aggregate reporting: Background and issues</article-title>. In <person-group person-group-type="editor">
<name><surname>Dorans</surname><given-names>N. J.</given-names></name>
<name><surname>Pommerich</surname><given-names>M.</given-names></name>
<name><surname>Holland</surname><given-names>P. W.</given-names></name>
</person-group> (Eds.), <source>Linking and aligning scores and scales</source> (pp. <fpage>287</fpage>-<lpage>312</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr59-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Venezia</surname><given-names>A.</given-names></name>
<name><surname>Kirst</surname><given-names>M. W.</given-names></name>
<name><surname>Antonio</surname><given-names>A. L.</given-names></name>
</person-group> (<year>2003</year>). <source>Betraying the college dream: How disconnected K-12 and postsecondary education systems undermine student aspirations</source>. <publisher-loc>Palo Alto, CA</publisher-loc>: <publisher-name>Stanford University Bridge Project</publisher-name>.</citation>
</ref>
<ref id="bibr60-0895904811400410">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Way</surname><given-names>W. D.</given-names></name>
</person-group> (<year>2010</year>, <month>May</month>). <conf-name>Measuring college readiness: Validity, cut scores, and looking to the future. Symposium conducted at the meeting of the American Education Research Association Conference</conf-name>, <conf-loc>Denver, CO</conf-loc>.</citation>
</ref>
<ref id="bibr61-0895904811400410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Zwick</surname><given-names>R.</given-names></name>
</person-group> (<year>2004</year>). <source>Rethinking the SAT: The future of standardized testing in university admissions</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge Falmer</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>