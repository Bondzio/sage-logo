<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LAS</journal-id>
<journal-id journal-id-type="hwp">splas</journal-id>
<journal-title>Language and Speech</journal-title>
<issn pub-type="ppub">0023-8309</issn>
<issn pub-type="epub">1756-6053</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0023830911434118</article-id>
<article-id pub-id-type="publisher-id">10.1177_0023830911434118</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Lexical Representation of Japanese Vowel Devoicing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Ogasawara</surname><given-names>Naomi</given-names></name>
</contrib>
<aff id="aff1-0023830911434118">National Taiwan Normal University, Taiwan</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0023830911434118">Naomi Ogasawara, Department of English, National Taiwan Normal University, No. 162, Section 1, He-Ping East Road, Daan District, Taipei City 106, Taiwan. Email: <email>naomi703@ntnu.edu.tw</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>56</volume>
<issue>1</issue>
<fpage>5</fpage>
<lpage>22</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Vowel devoicing happens in Japanese when the high vowel is between voiceless consonants. The aim of this study is to investigate the lexical representation of vowel devoicing. A long-term repetition-priming experiment was conducted. Participants shadowed words containing either a devoiced or a voiced vowel in three priming paradigms, and their shadow responses were analyzed. It was found that participants produced the phonologically appropriate allophone most of the time based on the consonantal environments. Shadowing latencies for the voiced stimuli were faster than for the devoiced stimuli in the environment where the vowel should be voiced; while, no significant RT difference was observed between the two forms in the environment where vowel devoicing was expected. In addition, a priming effect between the devoiced and voiced stimuli emerged only in the devoicing environment. The results suggest that since vowel devoicing is very common in spoken Japanese, the devoiced form may be stored in the lexicon. The results also suggest a link between the two forms in the lexicon and a direct access between an input and a lexical representation without going through intermediate levels that usually cost extra processes.</p>
</abstract>
<kwd-group>
<kwd>Japanese</kwd>
<kwd>lexicon</kwd>
<kwd>priming</kwd>
<kwd>shadowing</kwd>
<kwd>vowel devoicing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0023830911434118" sec-type="intro">
<title>1 Introduction</title>
<p>The basis for understanding spoken language is understanding words. Listeners perceive speech signals, decode them into phonetic and phonological information, and map it onto a lexical representation in order to retrieve the meaning of words (<xref ref-type="bibr" rid="bibr4-0023830911434118">Connine &amp; Pinnow, 2006</xref>; <xref ref-type="bibr" rid="bibr7-0023830911434118">Dahan &amp; Magnuson, 2006</xref>; <xref ref-type="bibr" rid="bibr10-0023830911434118">Gaskell &amp; Marslen-Wilson, 1996</xref>; <xref ref-type="bibr" rid="bibr23-0023830911434118">Jusczyk &amp; Luce, 2002</xref>; <xref ref-type="bibr" rid="bibr35-0023830911434118">Massaro &amp; Oden, 1995</xref>, <xref ref-type="bibr" rid="bibr36-0023830911434118">McClelland &amp; Elman, 1986</xref>; <xref ref-type="bibr" rid="bibr39-0023830911434118">McQueen, 2007</xref>). However, speech usually contains some variability, which may cause a mismatch between an input and a canonical lexical form. Variability may be a result of the idiosyncratic nature of pronunciation by individuals, mispronunciation, or some phonological processes such as place assimilation, allophonic variations, reduction or epenthesis of a segment. Generally, the effect of an unsystematic distortion of sounds seems to be inhibitory. An early Cohort model (<xref ref-type="bibr" rid="bibr30-0023830911434118">Marslen-Wilson, 1987</xref>) emphasizes the critical status of word onsets for word recognition, and found that a phonetic deviation of the initial segment inactivates a lexical form even though the rest of the segments match the form (<xref ref-type="bibr" rid="bibr33-0023830911434118">Marslen-Wilson &amp; Zwitserlood, 1989</xref>). Other studies found that sound deviation not only at the phonemic level but also at the feature level disrupts lexical access, and that the word initial segment is not necessarily crucial for word recognition; rather, what matters is the degree of overall acoustic similarity between an input and a lexical representation (<xref ref-type="bibr" rid="bibr3-0023830911434118">Connine, Blasko, &amp; Titone, 1993</xref>; <xref ref-type="bibr" rid="bibr31-0023830911434118">Marslen-Wilson &amp; Gaskell, 1992</xref>).</p>
<p>Unlike the unsystematic distortion, a systematic variability caused by a phonological process may be tolerable for word recognition because retrieving an underlying form through a phonetic or phonological analysis of variants is possible. <xref ref-type="bibr" rid="bibr17-0023830911434118">Gow (2001</xref>, <xref ref-type="bibr" rid="bibr18-0023830911434118">2003)</xref> claims that listeners recover an underlying form from a place-assimilated segment in the phonological contexts by feature-parsing. For example, a place-assimilated alveolar nasal (e.g., a heavily labialized /n/ in <italic>cone bent</italic>) is acoustically different from a pure bilabial nasal because it contains both coronal and labial features, and listeners recover the underlying alveolar nasal by parsing both features. <xref ref-type="bibr" rid="bibr51-0023830911434118">Snoeren, Seguí, and Hallé (2008)</xref> found a similar effect of phonological contexts on voicing assimilation in French. Assimilated voiceless stops (e.g., <italic>une soute</italic> [sud] <italic>bondée</italic>) showed a semantic priming effect but phonetically similar words (<italic>soude</italic>) did not. The underlying feature [-voice] remained in the assimilated segment, and listeners were sensitive to such a subtle acoustic cue to retrieve the underlying form. Besides feature-parsing accounts, <xref ref-type="bibr" rid="bibr10-0023830911434118">Gaskell and Marslen-Wilson (1996)</xref> suggest that the phonological rule of place assimilation infers word recognition. They reported that a phonologically appropriate variant (e.g., <italic>leam bacon</italic>) showed a cross-modal priming effect as well as an underlying form (<italic>lean bacon</italic>) because the underlying form was retrieved through an analysis of the place assimilation rule before accessing the lexicon, and there was no mismatch between the input and the lexical form (<xref ref-type="bibr" rid="bibr11-0023830911434118">Gaskell &amp; Marslen-Wilson, 1998</xref>). However, <xref ref-type="bibr" rid="bibr27-0023830911434118">LoCasto and Connine (2002)</xref> argue that phonological inference is not an automatic mechanism that can apply to any kind of phonological variations. In their study, English words containing a reduced vowel (e.g., [səpoʊz] <italic>suppose</italic>, [kæməɹə] <italic>camera</italic>) were more acceptable and more accurately recognized than the forms with schwa deletion (e.g., [spoʊz], [kæmɹə]). In the lexical decision task with three priming paradigms (match, mismatch, control), three-syllable words in the match condition were responded to faster than in the mismatch condition; while, for two-syllable targets without a schwa there was no significant difference between the match and mismatch conditions, which showed that the reduced and deleted forms primed each other. The authors concluded that the lexical representation of those words contains a schwa, and that the form without a schwa can activate the lexical form by restoration of the missing vowel with the help of the phonological inference; however, phonological inference plays a role under a specific circumstance: when segmental overlap between an input and a lexical representation is insufficient. They suggest that the use of phonological knowledge should be restricted in this way in order to prevent incorrect access to similar words and to block the incorrect restoration of vowels (e.g., [sʊpɹɪŋg] for <italic>spring</italic>).</p>
<p>The existing perception models dealing with speech variability can be roughly divided into two camps: one assuming only an underlying form in the lexicon and the other assuming multiple representations in the lexicon. Mediated access models, such as TRACE (<xref ref-type="bibr" rid="bibr36-0023830911434118">McClelland &amp; Elman, 1986</xref>), feature-parsing (<xref ref-type="bibr" rid="bibr17-0023830911434118">Gow, 2001</xref>, <xref ref-type="bibr" rid="bibr18-0023830911434118">2003</xref>; <xref ref-type="bibr" rid="bibr51-0023830911434118">Snoeren et al., 2008</xref>), and phonological inference (<xref ref-type="bibr" rid="bibr10-0023830911434118">Gaskell &amp; Marslen-Wilson, 1996</xref>, <xref ref-type="bibr" rid="bibr11-0023830911434118">1998</xref>), postulate that a fully specified abstract underlying form is a lexical representation, and some phonetic and phonological mechanism compensates for speech variability before accessing the lexicon. On the other hand, direct access models argue against the abstract representation view and suggest multiple lexical representations. Episodic memory accounts, such as the Exemplar model (<xref ref-type="bibr" rid="bibr12-0023830911434118">Goldinger, 1996</xref>, <xref ref-type="bibr" rid="bibr13-0023830911434118">1997</xref>, <xref ref-type="bibr" rid="bibr14-0023830911434118">1998</xref>; <xref ref-type="bibr" rid="bibr15-0023830911434118">Goldinger &amp; Azuma, 2003</xref>; <xref ref-type="bibr" rid="bibr16-0023830911434118">Goldinger, Pisoni, &amp; Logan, 1991</xref>; <xref ref-type="bibr" rid="bibr22-0023830911434118">Johnson, 1997</xref>; <xref ref-type="bibr" rid="bibr43-0023830911434118">Pierrehumbert, 2001</xref>, <xref ref-type="bibr" rid="bibr44-0023830911434118">2002</xref>; <xref ref-type="bibr" rid="bibr45-0023830911434118">Pisoni, 1990</xref>, <xref ref-type="bibr" rid="bibr46-0023830911434118">1992</xref>, <xref ref-type="bibr" rid="bibr47-0023830911434118">1997</xref>), postulate that the lexicon contains fine phonetic details of every surface form and even speaker information, which supports an extreme view of the episodic account. There is also a hybrid model—“a model in which a degree of normalization takes place, but in which multiple phonological variants of a given word may be stored” (<xref ref-type="bibr" rid="bibr4-0023830911434118">Connine &amp; Pinnow, 2006</xref>, p. 236).</p>
<p>Different from the episodic model, hybrid models consider some conditions under which more than one phonological representation becomes stored in the lexicon. <xref ref-type="bibr" rid="bibr37-0023830911434118">McLennan, Luce, and Charles-Luce (2003</xref>, <xref ref-type="bibr" rid="bibr38-0023830911434118">2005)</xref> propose that sub-lexical ambiguity (e.g., [ɾ] could be /t/ or /d/) allows multiple lexical representations. They used a long-term repetition-priming task and found that a flap form (e.g., [æɾəm]) primed a canonical form [ædəm] and vice versa, but no priming effect appeared between non-flap stimuli (e.g., [beɪkŋ] and [beɪkən]), which were realized as just two different speech styles. A similar priming effect emerged for semantically unambiguous words (e.g., <italic>pretty</italic>) and even for nonsense words (e.g., [poɪdi]). McLennan et al. accounted for the results by asserting that both underlying and surface forms are stored at the lexical level, but the dominance of one form over the other depends on the difficulty of processing. When sub-lexical ambiguity exists or when word discrimination is difficult due to high neighborhood density, an underlying form is dominant; while, when no ambiguity exists or word discrimination is easy, a surface form is dominant.</p>
<p>Another condition for multiple lexical representations can be a high frequency of phonological variants (<xref ref-type="bibr" rid="bibr1-0023830911434118">Bürki, Ernestus, &amp; Frauenfelder, 2010</xref>; <xref ref-type="bibr" rid="bibr2-0023830911434118">Connine, 2004</xref>; <xref ref-type="bibr" rid="bibr4-0023830911434118">Connine &amp; Pinnow, 2006</xref>; <xref ref-type="bibr" rid="bibr49-0023830911434118">Ranbom &amp; Connine, 2007</xref>). <xref ref-type="bibr" rid="bibr2-0023830911434118">Connine (2004)</xref> attests that phonological inference is plausible for low-frequency phonological variants, such as place assimilation or schwa deletion, but not for high-frequency variants, such as flapping in American English. Place assimilation, for instance, is strictly context-dependent and does not happen frequently. It is not conceivable that the lexicon contains all possible assimilated forms, considering the following segment across a morpheme boundary. It is more tenable to posit that only an underlying form is stored in the lexicon, and an assimilated form is decoded to the underlying form through phonological inference. Conversely, this inference mechanism is “time- and resource-consuming” (<xref ref-type="bibr" rid="bibr2-0023830911434118">Connine, 2004</xref>, p. 1085) for frequent variants. Instead, it may be the case that a frequent surface form is stored in the lexicon, and direct mapping between an input and a lexical form can maximally benefit processing. It is not necessary to go through an inference process at the prelexical level, in which phonetic details are lost due to phonological abstraction. English flap, for instance, is an overwhelmingly frequent realization in American English (<xref ref-type="bibr" rid="bibr2-0023830911434118">Connine, 2004</xref>; <xref ref-type="bibr" rid="bibr8-0023830911434118">Eddington &amp; Elzinga, 2008</xref>). <xref ref-type="bibr" rid="bibr2-0023830911434118">Connine (2004)</xref> tested whether phonemic categorization would be influenced by the existence of flap. She found that more real word responses were obtained in the flap carrier than in the underlying form (/t/) carrier. The result indicates that the lexical representation of words like <italic>beetle</italic> contains a flap, which is a highly frequent pronunciation of such words. <xref ref-type="bibr" rid="bibr1-0023830911434118">Bürki et al. (2010)</xref> tested the production lexicon for French words pronounced as one form with a schwa (e.g., [ʀɔkε͂] ‘shark’) and a reduced form without a schwa (e.g., [ʀkε͂]). Participants rated the frequency of realization of both forms for each word, and the reduced variants tended to receive a higher frequency than the schwa variants in a carrier sentence, which indicates the effect of context. In picture naming and symbol–word association learning tasks, when both forms were appropriate in the context (e.g., real words produced with a determiner or pseudo words), a higher variant frequency facilitated the production of both forms; while, when real words were produced in isolation, the schwa form was favored, and naming latencies were shorter for the schwa variants compared with the reduced variants. Under this condition, the effect of variant relative frequency appeared only for the reduced form. These results discovered notable points: the lexicon contains both schwa and reduced forms; the lexicon also records the frequency of each form; and the effect of context modulates the effect of the frequency of the form.</p>
<p>In order to test the effect of frequency and phonological appropriateness of speech variability, the present study investigates the lexical representation of vowel devoicing, which is a highly frequent phonological variability in spoken Japanese. A same line of research has been focusing on Western languages, such as English, German, or French, but not much work has been done on Asian languages. It is worth investigating a phonological variance in a non-Western language in order to have a broader understanding of perception and production of systematic speech variability that occurs in everyday life. Japanese vowel devoicing is a context-dependent phonological process, which occurs nearly 100% of the time, at least in the Tokyo dialect of Japanese (<xref ref-type="bibr" rid="bibr25-0023830911434118">Kondo, 2005</xref>; <xref ref-type="bibr" rid="bibr26-0023830911434118">Kubozono, 1999</xref>; <xref ref-type="bibr" rid="bibr28-0023830911434118">Maekawa &amp; Kikuchi, 2005</xref>), for the high vowels /i/ and /u/ when they are placed between voiceless obstruents or a voiceless obstruent and a pause as in [ki̥tɑ] <italic>north</italic>, [ku̥sɑ] <italic>grass</italic>, or [desu̥] copula-be (<xref ref-type="bibr" rid="bibr21-0023830911434118">Inozuka &amp; Inozuka, 2003</xref>; <xref ref-type="bibr" rid="bibr50-0023830911434118">Sakurai, 1985</xref>; <xref ref-type="bibr" rid="bibr53-0023830911434118">Vance, 1987</xref>, <xref ref-type="bibr" rid="bibr54-0023830911434118">2008</xref>). The main question addressed in this study is whether the canonical voiced vowel is the only lexical form or the devoiced vowel is also a lexical form. If the voiced vowel is the only lexical form, a devoiced surface form must be mediated by a more abstract phonemic representation before accessing the lexicon (<xref ref-type="bibr" rid="bibr36-0023830911434118">McClelland &amp; Elman, 1986</xref>; <xref ref-type="bibr" rid="bibr48-0023830911434118">Pisoni &amp; Luce, 1987</xref>; <xref ref-type="bibr" rid="bibr52-0023830911434118">Studdert-Kennedy, 1974</xref>). Conversely, in the same line of the hybrid models, if the devoiced vowel is also stored in the lexicon, a surface form may directly map onto the lexical form without going through an intermediate level (<xref ref-type="bibr" rid="bibr12-0023830911434118">Goldinger, 1996</xref>; <xref ref-type="bibr" rid="bibr22-0023830911434118">Johnson, 1997</xref>; <xref ref-type="bibr" rid="bibr24-0023830911434118">Klatt, 1989</xref>; <xref ref-type="bibr" rid="bibr32-0023830911434118">Marslen-Wilson &amp; Warren, 1994</xref>). If so, what is the condition that allows the devoiced form to be stored in the lexicon? Japanese vowel devoicing is similar to English flapping in terms of context-dependency, but the devoiced and voiced vowels share one phoneme, in which sub-lexical ambiguity does not exist. Since the two vowel forms are allophones, they are in complementary distribution. The devoiced vowel is the phonologically appropriate and common form in the devoicing contexts, and the voiced vowel is appropriate elsewhere; hence, the frequency of the variants and the phonological context affect processing in the same direction unlike French schwa deletion. Thus, the condition for co-existence of the two lexical forms in Japanese vowel devoicing may be a high frequency of a form in the appropriate phonological context.</p>
<p>In the current study, a long-term repetition-priming experiment with a shadowing task was replicated, following <xref ref-type="bibr" rid="bibr37-0023830911434118">McLennan et al. (2003</xref>, <xref ref-type="bibr" rid="bibr38-0023830911434118">2005)</xref>. Participants heard real words containing either the devoiced or voiced vowel, and repeated the words with their natural pronunciation without imitating the stimuli. Three possible scenarios can be expected. In the first scenario, the devoiced vowel is not in the lexicon, and a devoiced input needs to be decoded to the underlying voiced form before lexical access, and the underlying vowel needs to be encoded to a devoiced vowel as an output. This decoding and encoding would burden processing and must be time- and resource-consuming, which would slow down the whole process for the devoiced variant. In the second scenario, the lexicon contains both voiced and devoiced vowels together with the information about their frequency and the two forms are connected to each other through a lemma. An input would directly map onto one of the lexical forms, and a more strongly activated form is produced as an output. In this case, there would be no disadvantage for the devoiced variant during processing. In the last scenario, both variants exist in the lexicon, but there is no connection between them because they are recognized as two different speech styles. The mismatch between an input and an output may slow down shadowing for both variants. To examine these hypotheses and investigate the nature of the lexical representation of vowel devoicing, the frequency of the two variants in shadow responses and shadowing latencies for each variant will be analyzed, and finally a priming effect between the variants will be examined in the following sections.</p>
</sec>
<sec id="section2-0023830911434118">
<title>2 Experiment</title>
<sec id="section3-0023830911434118">
<title>2.1 Method</title>
<sec id="section4-0023830911434118">
<title>2.1.1 Design and materials</title>
<p>Two phonological environments were created. The devoicing environment was an environment in which the target vowel /i/ was between two voiceless obstruents (e.g., /ɑkikɑɴ/ <italic>empty can</italic>). The devoiced vowel is the phonologically appropriate allophone and the most common realization in this environment. The voicing environment was an environment in which the target vowel was adjacent to at least one voiced consonant. There were two subsets in this environment; 1) the vowel /i/ followed the voiced consonant /dʒ/1 (e.g., /tedʒinɑ/ <italic>magic</italic>) in half of the words; and 2) the vowel was between a voiceless consonant and a voiced consonant (e.g., /takibi/ <italic>bonfire</italic>) in the remaining half. In both subsets, the fully voiced vowel is the phonologically appropriate and the most common realization.</p>
<p>Eighteen common real words were chosen for each environment. All words were 3- to 5-mora long. Two types of stimuli were created from each word across the vowel allophones, one with a fully voiced vowel, and one with a devoiced vowel. <xref ref-type="table" rid="table1-0023830911434118">Table 1</xref> shows the conditions and some examples of stimuli. Besides the experimental items, thirty-five control items and six similar practice items were chosen. These items were real words with 3- to 5-mora length. See the Appendix for the word list.</p>
<table-wrap id="table1-0023830911434118" position="float">
<label>Table 1.</label>
<caption>
<p>Environments and examples.</p></caption>
<graphic alternate-form-of="table1-0023830911434118" xlink:href="10.1177_0023830911434118-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Environment</th>
<th/>
<th align="left" colspan="2">Stimulus<hr/></th>
<th align="left" rowspan="2">Gloss</th>
</tr>
<tr>
<th/>
<th align="left">Devoiced vowel</th>
<th align="left">Voiced vowel</th>
</tr>
</thead>
<tbody>
<tr>
<td>Devoicing:</td>
<td>C<sub>[-voice]</sub> ___ C<sub>[-voice]</sub></td>
<td>[ɑki̥kaɴ]</td>
<td>[ɑkikaɴ]</td>
<td><italic>empty can</italic></td>
</tr>
<tr>
<td>Voicing:</td>
<td>/dʒ/ ___ C<sub>[+/- voice]</sub></td>
<td>[tedʒi̥nɑ]</td>
<td>[tedʒinɑ]</td>
<td><italic>hand magic</italic></td>
</tr>
<tr>
<td/>
<td>C<sub>[-voice]</sub> ___ C<sub>[+voice]</sub></td>
<td>[tɑki̥bi]</td>
<td>[tɑkibi]</td>
<td><italic>bonfire</italic></td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The author, who is a native speaker of the Tokyo dialect of Japanese and phonetically trained, recorded all items in a recording booth in the Douglass Phonetics Lab at the University of Arizona, using a high quality microphone and a CD recorder, which sampled at 44.1 kHz. Some stimuli for the voicing environment had to be recorded multiple times to achieve a natural-sounding production.</p>
<p>The total number of experimental stimuli in each environment was thirty-six (18 words × 2 vowel allophones). Among the stimuli, half were prepared by splicing the target mora (/C(i)/) from the onset of the consonant preceding the target vowel to the onset of the consonant in the following mora, and the other half were left as originally recorded without splicing. That is, nine stimuli were created from the stimuli originally recorded with a fully voiced vowel and spliced with a devoiced vowel; and another nine were created from the stimuli originally recorded with a devoiced vowel and spliced with a voiced vowel. The purpose of splicing was to control the acoustic differences of the segments outside the target mora.</p>
<p>Besides the two environments and two vowel allophones, three priming conditions (match, mismatch, and control) were added, together creating 2 × 2 × 3 conditions. <xref ref-type="table" rid="table2-0023830911434118">Table 2</xref> shows the conditions and some examples of stimuli. The experimental and control stimuli presented in the first block served as primes and only the experimental stimuli presented in the second block served as targets.</p>
<table-wrap id="table2-0023830911434118" position="float">
<label>Table 2.</label>
<caption>
<p>Prime–target conditions and examples.</p></caption>
<graphic alternate-form-of="table2-0023830911434118" xlink:href="10.1177_0023830911434118-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="3" rowspan="2">Condition</th>
<th align="left" colspan="2">Example<hr/></th>
</tr>
<tr>
<th align="left">Block 1: prime</th>
<th align="left">Block 2: target</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">Match</td>
</tr>
<tr>
<td> Devoiced prime</td>
<td>➔</td>
<td>Devoiced target</td>
<td>ɑki̥kɑɴ / tedʒi̥nɑ</td>
<td>ɑki̥kɑɴ / tedʒi̥nɑ</td>
</tr>
<tr>
<td> Voiced prime</td>
<td>➔</td>
<td>Voiced target</td>
<td>ɑkikɑɴ / tedʒinɑ</td>
<td>ɑkikɑɴ / tedʒinɑ</td>
</tr>
<tr>
<td colspan="5">Mismatch</td>
</tr>
<tr>
<td> Voiced prime</td>
<td>➔</td>
<td>Devoiced target</td>
<td>ɑkikɑɴ / tedʒinɑ</td>
<td>ɑki̥kɑɴ / tedʒi̥nɑ</td>
</tr>
<tr>
<td> Devoiced prime</td>
<td>➔</td>
<td>Voiced target</td>
<td>ɑki̥kɑɴ / tedʒi̥nɑ</td>
<td>ɑkikɑɴ / tedʒinɑ</td>
</tr>
<tr>
<td colspan="5">Control</td>
</tr>
<tr>
<td> Unrelated prime</td>
<td>➔</td>
<td>Devoiced target</td>
<td>sɑkɑnɑ</td>
<td>ɑki̥kɑɴ / tedʒi̥nɑ</td>
</tr>
<tr>
<td> Unrelated prime</td>
<td>➔</td>
<td>Voiced target</td>
<td>sɑkɑnɑ</td>
<td>ɑkikɑɴ / tedʒinɑ</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In the match condition, a prime and a target were identical (e.g., prime [ɑki̥kɑɴ], target [ɑki̥kɑɴ]). In the mismatch condition, a prime and a target contained the counterpart allophone (e.g., prime [ɑkikɑɴ], target [ɑki̥kɑɴ], or vice versa). In the control condition, a prime was an unrelated word, and a target was an experimental item (e.g., prime [sɑkɑnɑ], target [ɑki̥kɑɴ], or prime [sɑkɑnɑ], target [ɑkikɑɴ]).</p>
</sec>
<sec id="section5-0023830911434118">
<title>2.1.2 Participants</title>
<p>Thirty-four university students who are native speakers of the Tokyo dialect were recruited. All of them were born and grew up in Tokyo or neighboring prefectures, and studied in a university in the Tokyo area at the time of the experiment. None of them reported speech or hearing disorders. They received a small gift or a small amount of money for their participation.</p>
</sec>
<sec id="section6-0023830911434118">
<title>2.1.3 Procedure</title>
<p>Each participant was tested individually in a quiet location, such as a lab or a room designated for running experiments in universities in Tokyo. Participants were seated in front of a microphone connected to a response box, which sent responses to a laptop computer. They wore headphones to listen to the stimuli, and wore a microphone hooked around the ear, which was connected to a portable digital recorder to record their responses.</p>
<p>The prime–target pairs were counterbalanced across the priming conditions, creating six pairs for each experimental word as shown in <xref ref-type="table" rid="table2-0023830911434118">Table 2</xref> above. Each pair was placed in one of the six item lists with a different order, so that each subject heard each target only in one priming condition. Participants were randomly assigned to one of the six sets of stimuli.</p>
<p>Oral and written instructions in Japanese were given to participants by the author. Participants were asked to listen to real words and to repeat each word out aloud as quickly as possible with their natural pronunciation without imitating the stimuli. The response window was five seconds from the onset of each stimulus. The experiment consisted of three phases: first, participants listened to the practice stimuli and repeated them out aloud; next, they listened to and repeated the thirty-six primes in the first trial block; and finally they listened to and repeated the thirty-six targets in the second trial block. Although they were informed that some words would appear more than once, they were not aware that the experiment consisted of the two trial blocks. E-Prime software (Psychology Software Tools, Inc.) was used to run the experiment and to record the data. Participants were asked to fill out a questionnaire about their own and their parents’ linguistic background at the completion of the experiment.</p>
</sec>
</sec>
<sec id="section7-0023830911434118">
<title>2.2 Results</title>
<p>The reaction times (RTs) were adjusted to measure from the end of each stimulus. All responses were checked for mispronunciation by the experimenter, and nine out of 2,040 responses were regarded as errors due to the mispronunciation of a phoneme. Any RTs outside the range from 0 to 800 ms were treated as errors, which excluded 5.2% of the data from the analysis of RTs. As a result, four subjects’ data were entirely removed from the analyses due to their failure in responding to any items in one condition, and one subject’s data were excluded due to technical problems.</p>
<sec id="section8-0023830911434118">
<title>2.2.1 Frequency of vowel allophones in shadow responses</title>
<p>In order to investigate the frequency of the devoiced and voiced vowels in production, responses to the primes and targets were analyzed acoustically. The realizations of the target vowel /i/ were categorized into four types: 1) deleted—after the release of closure, no acoustic property of the vowel but only frication noise was observed. Coarticulation cues (palatalization) were left behind mostly with the preceding consonant, which indicates the underlying presence of the vowel (<xref ref-type="bibr" rid="bibr42-0023830911434118">Ostreicher &amp; Sharf, 1976</xref>; <xref ref-type="bibr" rid="bibr53-0023830911434118">Vance, 1987</xref>, <xref ref-type="bibr" rid="bibr54-0023830911434118">2008</xref>; <xref ref-type="bibr" rid="bibr57-0023830911434118">Yuen, 2000</xref>); 2) true devoiced—neither voice bar nor periodic waves were observed, but there were faint formants visible enough to characterize the vowel (<xref ref-type="bibr" rid="bibr19-0023830911434118">Han, 1962</xref>; <xref ref-type="bibr" rid="bibr57-0023830911434118">Yuen, 2000</xref>); 3) low amplitude reduced but voiced vowel—a voice bar, formant structures, and a small number of periodic waves with very low amplitude were observed, but the acoustic quality of the vowel was quite different from the fully voiced vowel; and 4) fully voiced vowel.</p>
<p><xref ref-type="table" rid="table3-0023830911434118">Table 3</xref> shows the production rate for the four vowel types in shadow responses in the first and second blocks. Most of the responses contained either a deleted vowel (40.2%) or a fully voiced vowel (57.6%), and true devoiced and low amplitude voiced vowels were rarely produced. This result bears out the claim that Japanese vowel devoicing is acoustically vowel deletion rather than true devoicing (<xref ref-type="bibr" rid="bibr34-0023830911434118">Martin, 1987</xref>; <xref ref-type="bibr" rid="bibr53-0023830911434118">Vance, 1987</xref>, <xref ref-type="bibr" rid="bibr54-0023830911434118">2008</xref>; <xref ref-type="bibr" rid="bibr57-0023830911434118">Yuen, 2000</xref>). Some physiological studies using electromyographic (EMG) and fiberscope observations (<xref ref-type="bibr" rid="bibr9-0023830911434118">Fujimoto, 2005</xref>; <xref ref-type="bibr" rid="bibr20-0023830911434118">Hirose, 1971</xref>; <xref ref-type="bibr" rid="bibr40-0023830911434118">Nakamura, 2003</xref>; <xref ref-type="bibr" rid="bibr55-0023830911434118">Yoshioka, 1981</xref>; <xref ref-type="bibr" rid="bibr56-0023830911434118">Yoshioka, Löfqvist, &amp; Hirose, 1982</xref>) also showed that physiologically there was no discernible adduction of the vocal folds in the sequence of CVC where both consonants were voiceless (<xref ref-type="bibr" rid="bibr20-0023830911434118">Hirose, 1971</xref>). There was only a single peak of glottis opening observed without any attempt to make two openings (<xref ref-type="bibr" rid="bibr9-0023830911434118">Fujimoto, 2005</xref>). However, since the main purpose of this study is not to discuss vowel-devoicing vs. vowel-deletion, the common terms ‘devoiced vowel’ and ‘vowel devoicing’ will be used throughout the paper.</p>
<table-wrap id="table3-0023830911434118" position="float">
<label>Table 3.</label>
<caption>
<p>Frequency of four vowel types in shadow responses.</p></caption>
<graphic alternate-form-of="table3-0023830911434118" xlink:href="10.1177_0023830911434118-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Vowel type</th>
<th align="left">Response to prime</th>
<th align="left">Response to target</th>
<th align="left">Total frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td>Deleted</td>
<td>286 (41.1%)</td>
<td>413 (39.6%)</td>
<td>699 (40.2%)</td>
</tr>
<tr>
<td>True devoiced</td>
<td>11 (1.6%)</td>
<td>19 (1.8%)</td>
<td>30 (1.7%)</td>
</tr>
<tr>
<td>Low amplitude voiced</td>
<td>2 (0.3%)</td>
<td>7 (0.7%)</td>
<td>9 (0.5%)</td>
</tr>
<tr>
<td>Fully voiced</td>
<td>397 (57%)</td>
<td>605 (58%)</td>
<td>1002 (57.6%)</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Owing to their low frequency, both true devoiced and low amplitude voiced vowels (<italic>n</italic> = 39) were excluded from the analyses hereafter. <xref ref-type="table" rid="table4-0023830911434118">Table 4</xref> shows the proportion of voiced vowel responses sorted by the stimulus type (devoiced and voiced) in each environment. It is not surprising that the voiced vowel was produced most of the time in the voicing environment no matter which type of stimulus was presented, and the production rate of the voiced vowel was much lower in the devoicing environment. Interestingly, however, an increase of voiced vowel responses to the voiced stimuli compared with the devoiced stimuli was seen in the devoicing environment.</p>
<table-wrap id="table4-0023830911434118" position="float">
<label>Table 4.</label>
<caption>
<p>Proportion of voiced vowel responses in all responses.</p></caption>
<graphic alternate-form-of="table4-0023830911434118" xlink:href="10.1177_0023830911434118-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left" char="."/>
<col align="left" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Devoiced stimulus</th>
<th align="left">Voiced stimulus</th>
</tr>
</thead>
<tbody>
<tr>
<td>Devoicing environment</td>
<td>3.2%</td>
<td>5.9%</td>
</tr>
<tr>
<td>Voicing environment</td>
<td>24.5%</td>
<td>25.3%</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0023830911434118">
<p><italic>Note</italic>: The proportion was calculated by dividing the number of voiced vowel responses by the sum of deleted vowel responses and voiced vowel responses (<italic>n</italic> = 1701).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Since the variables (vowel types in shadow responses) were categorical, a binary logistic regression analysis was performed in order to examine the effect of the environment (devoicing and voicing) and stimulus (devoiced and voiced) as main factors. <xref ref-type="table" rid="table5-0023830911434118">Table 5</xref> displays the summary of the analysis.table5 The statistical model accounted for 60% of the variance. Both environment and stimulus factors significantly well predicted the vowel type in responses (environment: Wald <italic>x</italic><sup>2</sup> = 51.42, <italic>p</italic> &lt; .001; stimulus: Wald <italic>x</italic><sup>2</sup> = 7.03, <italic>p</italic> &lt; .01). The result confirms that which allophone participants would produce can be predicted based on the environment. As <xref ref-type="table" rid="table3-0023830911434118">Tables 3</xref> and <xref ref-type="table" rid="table4-0023830911434118">4</xref> above show, participants produced the phonologically appropriate vowel most of the time in each environment. In addition, the significant effect of stimulus indicates that what participants heard had an influence on what they produced. Although the proportion of voiced vowel responses was quite high across the stimulus types in the voicing environment, voiced vowel responses to the voiced stimuli even in the devoicing environment increased from 3.2% to 5.9%. Subjects produced the voiced vowel more frequently after they heard the voiced stimulus than after they heard the devoiced stimulus, which seems to have contributed to the significant effect of stimulus. Especially, this tendency was more obvious for two participants. They produced the voiced vowel almost half of the time after hearing the voiced stimuli. This indicates that although vowel devoicing is a quite common phonological process, it is not totally mandatory and there is still some variability across native speakers. More importantly, the occurrence of both voiced and devoiced forms in shadow responses indicates the co-existence of the two allophones in the lexicon for the words containing a devoiceable vowel.</p>
<table-wrap id="table5-0023830911434118" position="float">
<label>Table 5.</label>
<caption>
<p>Results of logistic regression.</p></caption>
<graphic alternate-form-of="table5-0023830911434118" xlink:href="10.1177_0023830911434118-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left" rowspan="2">B (SE)</th>
<th align="left" colspan="3">95% CI for odds ratio<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Lower</th>
<th align="left">Odds ratio</th>
<th align="left">Upper</th>
</tr>
</thead>
<tbody>
<tr>
<td>Constant</td>
<td>6.066 (1.00)</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Environment</td>
<td>-7.226 (1.01)</td>
<td>.000</td>
<td>.001</td>
<td>.005</td>
</tr>
<tr>
<td>Type of stimulus</td>
<td>-2.743 (1.04)</td>
<td>.008</td>
<td>.064</td>
<td>.489</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0023830911434118">
<p><italic>Note: R</italic><sup>2</sup> = .6 (Hosmer &amp; Lemeshow), .55 (Cox &amp; Snell), .75 (Nagelkerke), Model <italic>x</italic><sup>2</sup>(3) = 1373.165, <italic>p</italic> &lt; .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section9-0023830911434118">
<title>2.2.2 RTs for responses in the first block</title>
<p>In this section, RTs for responses to the primes in the first block that contain only phonologically appropriate allophones (devoiced vowel responses in the devoicing environment and voiced vowel responses in the voicing environment) were analyzed in order to investigate the nature of the lexical representation of the two vowel variants and the relationship between an input and an output. The RTs for responses in the second block were excluded to avoid the influence of a priming effect. Data from two subjects were eliminated from the analysis owing to the lack of data in one condition. <xref ref-type="table" rid="table6-0023830911434118">Table 6</xref> shows the number of responses containing the phonologically appropriate vowel and the mean shadowing latencies in each environment sorted by the stimulus type.</p>
<table-wrap id="table6-0023830911434118" position="float">
<label>Table 6.</label>
<caption>
<p>The number of responses with the phonologically appropriate vowel (mean RT).</p></caption>
<graphic alternate-form-of="table6-0023830911434118" xlink:href="10.1177_0023830911434118-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left" char="."/>
<col align="left" char="."/>
<col align="left" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="2">Devoiced vowel responses in devoicing environment</th>
<th align="left" colspan="2">Voiced vowel responses in voicing environment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Devoiced stimulus</td>
<td>146 (323.4 ms)</td>
<td>Devoiced stimulus</td>
<td>165 (343.8 ms)</td>
</tr>
<tr>
<td>Voiced stimulus</td>
<td>132 (315.8 ms)</td>
<td>Voiced stimulus</td>
<td>174 (305.1 ms)</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>An analysis of variances (ANOVA) on RTs was carried out with stimulus (devoiced and voiced) as a within-subject factor, and counterbalanced group as a between-subject factor for each environment. A significant effect of stimulus was found in the voicing environment, <italic>F</italic>(1, 21) = 15.33, <italic>p</italic> = .001. Participants produced voiced vowel responses significantly faster to the voiced stimuli than to the devoiced stimuli. This suggests that for words in the voicing environment, the canonical voiced vowel must be the only lexical representation since vowel devoicing rarely happens in this environment. Because the devoiced vowel is unnatural and an extremely infrequent form, there is little or no chance for the devoiced vowel to get stored in the lexicon. Significantly slower responses to the devoiced stimuli in this analysis indicate the inhibitory effect of the mismatch between the surface form and the lexical form. Since vowel devoicing is not a legitimate phonological process in the environment, the retrieval of the underlying vowel through a phonological inference may not be expected. In this case, shadowers would need to search for the right word in the lexicon by carefully analyzing the acoustic information from the distorted (devoiced) segment and the rest of the segments in the input, which may cause responses to slow down.</p>
<p>Conversely, the effect of stimulus did not reach significance in the devoicing environment, <italic>F</italic>(1, 21) = .53, <italic>p</italic> &gt; .1. Participants produced devoiced vowel responses to the devoiced stimuli as quickly as to the voiced stimuli. This shows that the lexical representation of words containing a devoiceable vowel has both devoiced and voiced forms, and that there is direct mapping of an input to the lexical representation. This view is incompatible with the mediated access accounts, which assume only the underlying voiced vowel is in the lexicon, and a surface devoiced vowel must be converted to the underlying vowel before accessing the lexicon. The mediated access model requires the time- and resource-consuming extra processes of conversion from the devoiced allophone to the fully specified phoneme during perception and another conversion from the phoneme to the devoiced allophone during production. These extra steps must slow down shadowing of the devoiced stimuli in the current experiment, but the result shows that this was not the case.</p>
<p>Additionally, the direct access between a devoiced input and the devoiced form in the lexicon may overcome the acoustic disadvantage of the devoiced vowel during processing. The devoiced vowel is acoustically non-salient as it has little or no acoustic property as a vowel, and it is usually shorter compared with the fully voiced vowel. In this experiment, the average duration of the target mora containing a devoiced or voiced vowel was 134.8 ms and 160.0 ms, respectively. <xref ref-type="bibr" rid="bibr6-0023830911434118">Cutler, van Ooijen, Norris, and Sánchez-Casas (1996)</xref> found that the longer a vowel duration is, the faster listeners recognize it. Thus, the devoiced stimuli must be processed more slowly than the voiced stimuli. Nevertheless, the result shows no significant difference in the RT between the two types of stimuli in the devoicing environment. The devoiced stimuli were processed and repeated as quickly as the voiced stimuli. This eliminates the possibility of a mediated access, and suggests a direct access between an input and a lexical form. Since the devoiced vowel is the phonologically appropriate and the most frequent form, it is likely to become stored in the lexicon, and a surface devoiced form directly maps onto the lexical form. The match between the input and the lexical form would facilitate processing, which is perhaps facilitatory enough to cancel out the acoustic disadvantage of the devoiced vowel. Furthermore, it can be inferred from the results that the voiced and devoiced forms are linked to each other through a lemma in the lexicon because the mismatch between the voiced stimuli and devoiced responses did not slow down shadowing. Devoiced responses to the voiced stimuli were as fast as to the devoiced stimuli. When participants heard the voiced stimuli, the surface form directly mapped onto the voiced form in the lexicon to access the lemma, which is also linked to the devoiced form. Perhaps, the activation of the devoiced form through the lemma must be stronger than the voiced form due to its high frequency, so that when participants repeated the word, the devoiced vowel came out as an output.</p>
<p>One might argue that repeating words without lexical access is possible, and that shadowers produce the phonologically appropriate vowels without considering the devoicing rule simply because it is difficult to articulate the inappropriate vowel. If this is the case, the phonologically inappropriate stimuli must have elicited more erroneous responses. Especially when participants heard the devoiced stimuli in the voicing environment, they must have repeated the item without inserting the right vowel if no lexical access was involved. In fact, errors were few (9 out of 2,040 responses), and subjects produced the right words with the right pronunciation most of the time. It must be impossible to achieve such a high accuracy without lexical access. In addition, the task for subjects in the current study was so easy to achieve that online-lexical processing was possible during the task. <xref ref-type="bibr" rid="bibr29-0023830911434118">Marslen-Wilson (1985)</xref> found lexical access during a more difficult repetition task. He found that some people were able to shadow passages with close latency (about 250 to 300 ms) and their error patterns were similar to distant shadowers (latency around 600 ms). The errors were not only segmental errors, but also errors as a result of online semantic and syntactic analysis. For example, the stimulus “He had heard at the Brigade…” was mistakenly shadowed as “He had heard that the Brigade…”, which was a syntactically correct form. A random word order passage “To daylight the the attack was of in…” was shadowed as “To daylight that the attack was to begin…”, which was a meaningful form. Such error data provide evidence that the higher-level (lexicon, syntax, and semantics) online analysis of incoming speech happens very rapidly in order to comprehend spoken language. In the current experiment, the task for participants was to repeat isolated words, which must be much easier than shadowing 300-word passages with a random word order or jabberwocky as in <xref ref-type="bibr" rid="bibr29-0023830911434118">Marslen-Wilson (1985)</xref>. Thus, it is plausible that lexical access happened when subjects heard the words, and they repeated the words in the form stored in the lexicon. In order to further investigate the lexical representation of vowel devoicing and the relationship between the two variants, the following section presents an analysis of the priming effect between the allophones.</p>
</sec>
<sec id="section10-0023830911434118">
<title>2.2.3 RTs for responses in the second block and priming effect</title>
<p><xref ref-type="fig" rid="fig1-0023830911434118">Figures 1A</xref> and <xref ref-type="fig" rid="fig2-0023830911434118">1B</xref> show the mean RTs for responses to the targets in the second block sorted by the priming conditions in each environment.</p>
<fig id="fig1-0023830911434118" position="float">
<label>Figure 1A.</label>
<caption>
<p>RTs for responses in the second block sorted by the priming conditions (match, mismatch, and control) in the devoicing environment. Error bars show 95% confidence interval.</p>
</caption>
<graphic xlink:href="10.1177_0023830911434118-fig1.tif"/>
</fig>
<fig id="fig2-0023830911434118" position="float">
<label>Figure 1B.</label>
<caption>
<p>RTs for responses in the second block sorted by the priming conditions (match, mismatch, and control) in the voicing environment. Error bars show 95% confidence interval.</p>
</caption>
<graphic xlink:href="10.1177_0023830911434118-fig2.tif"/>
</fig>
<p>By-subjects (<italic>F</italic><sub>1</sub>) and by-items (<italic>F</italic><sub>2</sub>) ANOVAs were carried out on the RTs. Each ANOVA had prime (matching, mismatching, and control), environment (devoicing and voicing), stimulus (devoiced vowel and voiced vowel), and counterbalanced group (for by-subjects analyses only) as main factors. In the by-subjects analysis, prime, environment, and stimulus were within-subjects factors, and counterbalanced group was a between-subjects factor. In the by-items analysis, prime and stimulus were within-items factors, and environment was a between-items factor. The Greenhouse-Geisser correction was applied whenever sphericity was violated.</p>
<p>By-subjects and by-items ANOVAs revealed that the main effect of prime was significant, <italic>F</italic><sub>1</sub>(2, 48) = 5.01, <italic>p</italic> &lt; .02; <italic>F</italic><sub>2</sub>(2, 68) = 4.19, <italic>p</italic> &lt; .05, and stimulus was partially significant, <italic>F</italic><sub>1</sub>(1, 24) = 5.16, <italic>p</italic> &lt; .05; <italic>F</italic><sub>2</sub>(1, 34) = 1.11, <italic>p</italic> &gt; .1, but environment was insignificant, <italic>F</italic><sub>1</sub>(1, 24) = .13, <italic>p</italic> &gt; .5; <italic>F</italic><sub>2</sub>(1, 34) = .002, <italic>p</italic> &gt; .5. There was no significant three-way interaction of environment, prime, and stimulus, <italic>F</italic><sub>1</sub>(2, 48) = .28, <italic>p</italic> &gt; .5; <italic>F</italic><sub>2</sub>(2, 68) = .29, <italic>p</italic> &gt; .5. Also, neither the two-way interaction of prime and environment, <italic>F</italic><sub>1</sub>(2, 48) = 1.96, <italic>p</italic> &gt; .1; <italic>F</italic><sub>2</sub>(2, 68) = .84, <italic>p</italic> &gt; .1, nor that of prime and stimulus, <italic>F</italic><sub>1</sub>(2, 48) = 1.55, <italic>p</italic> &gt; .1; <italic>F</italic><sub>2</sub>(2, 68) = .34, <italic>p</italic> &gt; .5 was significant. Only the two-way interaction of environment and stimulus was partially significant, <italic>F</italic><sub>1</sub>(1, 24) = 5.27, <italic>p</italic> &lt; .05; <italic>F</italic><sub>2</sub>(1, 34) = 2.43, <italic>p</italic> &gt; .1. The data were further split across the environment factor. In the devoicing environment, the mean RTs for the devoiced stimuli and for the voiced stimuli were 332.6 ms and 332.4 ms, respectively, and the simple effect of stimulus type was not significant, <italic>F</italic><sub>1</sub>(1, 24) = .002, <italic>p</italic> &gt; .5; <italic>F</italic><sub>2</sub>(1, 17) = .021, <italic>p</italic> &gt; .5. On the contrary, in the voicing environment, the mean RTs for the devoiced stimuli and for the voiced stimuli were 348.0 ms and 322.9 ms, respectively, and the effect of stimulus type was partially significant, <italic>F</italic><sub>1</sub>(1, 24) = 14.13, <italic>p</italic> = .001; <italic>F</italic><sub>2</sub>(1, 17) = 2.813, <italic>p</italic> &gt; .1. This result replicated the analysis done on the first block. It confirms again that subjects responded to the devoiced stimuli as quickly as to the voiced stimuli in the devoicing environment, but it took them more time to respond to the devoiced stimuli than to the voiced stimuli in the voicing environment. This supports the claim that both devoiced and voiced vowels are acceptable forms in the devoicing environment, but in the voicing environment, the devoiced vowel is not the lexical form; thus, the mismatch between the surface form and the lexical form significantly slowed down shadowing.</p>
<p>Next, planned comparisons were carried out to examine a priming effect. The data were collapsed across the stimulus factor in each environment. In the devoicing environment, the overall priming effect was significant, <italic>F</italic><sub>1</sub>(2, 48) = 7.475, <italic>p</italic> = .001; <italic>F</italic><sub>2</sub>(2, 70) = 3.773, <italic>p</italic> &lt; .03. It was also significant between the match and control conditions, <italic>F</italic><sub>1</sub>(1, 24) = 11.208, <italic>p</italic> = .003; <italic>F</italic><sub>2</sub>(1, 35) = 7.532, <italic>p</italic> &lt; .01, and between the mismatch and control conditions, <italic>F</italic><sub>1</sub>(1, 24) = 8.46, <italic>p</italic> &lt; .01; <italic>F</italic><sub>2</sub>(1, 35) = 4.405, <italic>p</italic> &lt; .05. It was not significant between the match and mismatch conditions, <italic>F</italic><sub>1</sub>(1, 24) = .093, <italic>p</italic> &gt; .5; <italic>F</italic><sub>2</sub>(1, 35) = .179, <italic>p</italic> &gt; .5. In the voicing environment, no priming effect was found. Overall: <italic>F</italic><sub>1</sub>(2, 48) = 1.204, <italic>p</italic> &gt; .3; <italic>F</italic><sub>2</sub>(2, 70) = .466, <italic>p</italic> &gt; .5; between the match and control conditions: <italic>F</italic><sub>1</sub>(1, 24) = 2.373, <italic>p</italic> &gt; .1; <italic>F</italic><sub>2</sub>(1, 35) = 1.076, <italic>p</italic> &gt; .3; between the mismatch and control conditions: <italic>F</italic><sub>1</sub>(1, 24) = .801, <italic>p</italic> &gt; .3; <italic>F</italic><sub>2</sub>(1, 35) = .51, <italic>p</italic> &gt; .3; and between the match and mismatch conditions: <italic>F</italic><sub>1</sub>(1, 24) = .0447, <italic>p</italic> &gt; .5; <italic>F</italic><sub>2</sub>(1, 35) = .043, <italic>p</italic> &gt; .5.</p>
<p>The ANOVAs revealed that both devoiced and voiced vowels primed not only the identical form but also the counterpart variant in the devoicing environment. This illustrates three important points: 1) shadowing involves lexical access; 2) both vowel allophones are stored in the lexicon; and 3) the two forms are linked to each other in the lexicon. <xref ref-type="bibr" rid="bibr37-0023830911434118">McLennan et al. (2003</xref>, <xref ref-type="bibr" rid="bibr38-0023830911434118">2005)</xref> found priming effects between the flapped and underlying [t] stimuli in English when the underlying form of the flap was unambiguous (sub-lexical ambiguity), but no priming effect was found for non-flapping stimuli because there was no such sub-lexical ambiguity. Two different surface forms (e.g., [beɪkŋ] and [beɪkən]) were not linked in the lexicon. They were recognized just as two different speech styles. In the case of Japanese devoiced and voiced allophones, there was no sub-lexical ambiguity because they share the same phoneme; nevertheless, the priming effect emerged between the two allophones in the current study. The result suggests that what conditions a priming effect may be a frequency of variants instead of sub-lexical ambiguity. As <xref ref-type="bibr" rid="bibr2-0023830911434118">Connine (2004)</xref> pointed out, English flapping is so frequent that the flapped form must be stored in the lexicon, but casually articulated non-flap words (e.g., [beɪkŋ] with schwa deletion) are much less frequent and not strictly context-conditioned. Hence, the non-flap variant is not stored in the lexicon, therefore no priming effect emerges between the two forms. Given that Japanese vowel devoicing is a very frequent phonological process, the emergence of a priming effect between the two variants validates the frequency-based account.</p>
<p>Unlike in the devoicing environment, in the voicing environment the statistical analysis found no priming effect between the devoiced and voiced vowels perhaps due to the lack of statistical power. However, <xref ref-type="fig" rid="fig2-0023830911434118">Figure 1B</xref> shows some tendency that there seems to be a priming effect only for the voiced stimuli. The mean RTs to the voiced stimuli in the match and mismatch conditions seem to be faster than that in the control condition. This can draw a conclusion that the voiced vowel is the only lexical form, and when participants heard the voiced stimulus as a target in the second block, it triggered a quick access to the lexical form activated through the devoiced or voiced primes in the first block. As mentioned before, lexical access may be possible even through a devoiced prime by assessing the acoustic similarity of the input to the voiced lexical form even though the vowel was different and the lexical search was time-consuming. Therefore, the target words were already activated in the first block, and the phonologically appropriate voiced stimuli in the second block benefit from the priming effect. On the other hand, when participants heard the devoiced stimulus in the second block, the acoustical assessment of the input in the second block and the lexical search took time to reach the activated target word; thus, the priming effect disappears during this time-consuming search process.</p>
</sec>
</sec>
</sec>
<sec id="section11-0023830911434118" sec-type="discussion">
<title>3 General discussion</title>
<p>Japanese vowel devoicing is a phonologically conditioned process, which occurs nearly 100% of the time. The aim of the present study was to investigate the lexical representation of vowel devoicing. A long-term repetition-priming experiment was conducted using a shadowing task. Participants listened to the devoiced and voiced stimuli in the devoicing and voicing environments and repeated the words in their normal pronunciation without imitating the stimuli.</p>
<p>The analyses revealed different views for the lexical representation of words in the devoicing and voicing environments. First, the acoustic analysis of shadow responses showed that responses contained the phonologically appropriate allophone most of the time based on the environment, but the frequency of the voiced vowel production increased after the voiced stimuli only in the devoicing environment. These results indicate that the production lexicon contains only the voiced vowel that always occurs for words in the voicing environment, but the lexicon seems to contain both vowel forms for words in the devoicing environment. The devoiced vowel is likely to be stored in the lexicon because of its high-frequency conditioned by the contexts. At the same time, it was confirmed that vowel devoicing is not completely mandatory in Japanese. The frequency of voiced vowel production triggered by the voiced stimuli increased in the devoicing environment, which indicates that what listeners hear may influence the activation of variants when multiple representations co-exist in the lexicon.</p>
<p>The analysis on shadowing latencies for the two variants also supports the account of multiple lexical forms for high-frequency variants (<xref ref-type="bibr" rid="bibr1-0023830911434118">Bürki et al., 2010</xref>; <xref ref-type="bibr" rid="bibr2-0023830911434118">Connine, 2004</xref>; <xref ref-type="bibr" rid="bibr4-0023830911434118">Connine &amp; Pinnow, 2006</xref>; <xref ref-type="bibr" rid="bibr49-0023830911434118">Ranbom &amp; Connine, 2007</xref>) and a direct access between an input and a lexical form (<xref ref-type="bibr" rid="bibr12-0023830911434118">Goldinger, 1996</xref>, <xref ref-type="bibr" rid="bibr13-0023830911434118">1997</xref>, <xref ref-type="bibr" rid="bibr14-0023830911434118">1998</xref>; <xref ref-type="bibr" rid="bibr15-0023830911434118">Goldinger &amp; Azuma, 2003</xref>; <xref ref-type="bibr" rid="bibr16-0023830911434118">Goldinger et al., 1991</xref>; <xref ref-type="bibr" rid="bibr22-0023830911434118">Johnson, 1997</xref>; <xref ref-type="bibr" rid="bibr43-0023830911434118">Pierrehumbert, 2001</xref>, <xref ref-type="bibr" rid="bibr44-0023830911434118">2002</xref>; <xref ref-type="bibr" rid="bibr45-0023830911434118">Pisoni, 1990</xref>, <xref ref-type="bibr" rid="bibr46-0023830911434118">1992</xref>, <xref ref-type="bibr" rid="bibr47-0023830911434118">1997</xref>). In the devoicing environment there was no significant difference in RTs between the devoiced and voiced stimuli, which means that the devoiced vowel in the appropriate contexts did not make shadowing difficult. The lexicon seems to contain the devoiced vowel, and when participants heard the devoiced stimulus, the input directly mapped onto the lexical form and came out as an output. Although the devoiced vowel is acoustically weak and short, which usually inhibits the processing (<xref ref-type="bibr" rid="bibr6-0023830911434118">Cutler et al., 1996</xref>; <xref ref-type="bibr" rid="bibr41-0023830911434118">Ogasawara &amp; Warner, 2009</xref>), its phonological appropriateness and high frequency seem to cancel out the acoustic disadvantage. However, if we assume a mediated access (<xref ref-type="bibr" rid="bibr10-0023830911434118">Gaskell &amp; Marslen-Wilson, 1996</xref>, <xref ref-type="bibr" rid="bibr11-0023830911434118">1998</xref>; <xref ref-type="bibr" rid="bibr17-0023830911434118">Gow, 2001</xref>, <xref ref-type="bibr" rid="bibr18-0023830911434118">2003</xref>; <xref ref-type="bibr" rid="bibr36-0023830911434118">McClelland &amp; Elman, 1986</xref>; <xref ref-type="bibr" rid="bibr51-0023830911434118">Snoeren et al., 2008</xref>), the outcome would be different. The lexicon would contain only the underlying voiced form, and the conversion of a devoiced input to the phoneme at the prelexical level and another conversion from the phoneme to a devoiced output at the postlexical level should certainly slow down the whole process. Decoding and encoding between surface and abstract forms would be inefficient for achieving quick online comprehension and production of speech during shadowing. Apparently, the result was incompatible with this view.</p>
<p>The high frequency and phonological appropriateness of the devoiced vowel in the devoicing environment seem to compensate for its structural difficulty as well as its acoustic disadvantage in parsing segments. <xref ref-type="bibr" rid="bibr5-0023830911434118">Cutler, Otake, and McQueen (2009)</xref> found from word-spotting experiments in Japanese that restoration of the underlying vowel from a devoiced input does not happen prelexically. Since spotting a word (e.g., <italic>ani</italic> ‘brother’) in possible vowel devoicing contexts (<italic>anits(u)</italic>) and impossible contexts (<italic>anit(u)</italic>) was equally difficult for native speakers, the authors concluded that the restoration of the vowel by assessing the possible vowel-devoicing environment is not an automatic prelexical process. As the acoustic analysis in the current study and other studies (<xref ref-type="bibr" rid="bibr9-0023830911434118">Fujimoto, 2005</xref>; <xref ref-type="bibr" rid="bibr20-0023830911434118">Hirose, 1971</xref>; <xref ref-type="bibr" rid="bibr34-0023830911434118">Martin, 1987</xref>; <xref ref-type="bibr" rid="bibr40-0023830911434118">Nakamura, 2003</xref>; <xref ref-type="bibr" rid="bibr53-0023830911434118">Vance, 1987</xref>, <xref ref-type="bibr" rid="bibr54-0023830911434118">2008</xref>; <xref ref-type="bibr" rid="bibr55-0023830911434118">Yoshioka, 1981</xref>; <xref ref-type="bibr" rid="bibr56-0023830911434118">Yoshioka et al., 1982</xref>; <xref ref-type="bibr" rid="bibr57-0023830911434118">Yuen, 2000</xref>) found, Japanese vowel devoicing is acoustically vowel deletion, which creates consonant clusters (e.g., [ɑʃtɑ] for /ɑʃitɑ/ ‘tomorrow’). Consonant clusters are illegal in Japanese phonotactics, and universally it is more difficult to parse segments without a vowel (<xref ref-type="bibr" rid="bibr5-0023830911434118">Cutler et al., 2009</xref>). Nevertheless, native speakers in the current study were able to respond to the devoiced stimuli as quickly as to the voiced stimuli. It should be the phonological appropriateness and high frequency of the devoiced variant that compensated for the structural disadvantage. <xref ref-type="bibr" rid="bibr41-0023830911434118">Ogasawara and Warner (2009)</xref> found a similar effect in their lexical decision experiment. They reported that Japanese words containing the devoiced vowel in the appropriate contexts were recognized more quickly and accurately than the same words containing the voiced vowel. They claimed that the advantage of the devoiced vowel was due to its phonological appropriateness and thus high frequency. In daily conversations, words are rarely pronounced with the voiced vowel in the devoicing contexts, and if it happens, it would sound very unnatural. <xref ref-type="bibr" rid="bibr1-0023830911434118">Bürki et al. (2010)</xref> also found that the advantage of the French schwa variant compared with the reduced variant without a schwa in picture-naming tasks was not due to the effect of structural advantage of the schwa variant; rather, it was due to the appropriateness of the schwa variant in the contexts. Along with those previous studies (<xref ref-type="bibr" rid="bibr1-0023830911434118">Bürki et al., 2010</xref>; <xref ref-type="bibr" rid="bibr41-0023830911434118">Ogasawara &amp; Warner, 2009</xref>), the results from the current study support a frequency-based account of multiple lexical representations suggested by Connine and her colleagues (<xref ref-type="bibr" rid="bibr2-0023830911434118">Connine, 2004</xref>; <xref ref-type="bibr" rid="bibr4-0023830911434118">Connine &amp; Pinnow, 2006</xref>; <xref ref-type="bibr" rid="bibr49-0023830911434118">Ranbom &amp; Connine, 2007</xref>).</p>
<p>Another conclusion that can be drawn from the current results is that the two vowel allophones seem to connect to each other in the lexicon, and the degree of activation of each variant may be influenced by variant frequency. The analysis on shadowing latency found that when participants heard the voiced stimulus in the devoicing environment, they still produced devoiced responses, and the mismatch between the input and output did not inhibit processing. It seems that a voiced input mapped onto the voiced form that connected to the devoiced form in the lexicon, and the devoiced form came out as an output because it is the phonological appropriate, high-frequent form. The emergence of a priming effect between the devoiced and voiced stimuli also provides evidence for the link between the two forms in the lexicon. The two variants primed not only the identical form but also their counterpart, which facilitated shadowing in the devoicing environment. This link may compensate for the mismatch between the input and the output. For example, the voiced stimulus maps onto the same form in the lexicon, which connects to the devoiced form perhaps more strongly activated through the lemma, and finally the devoiced form comes out as an output without slowing down shadowing. Correspondingly, no priming effect between the two forms in the voicing environment provides further evidence that only the voiced form is in the lexicon. The mismatch between the devoiced input and the voiced lexical form inhibited shadowing. Since vowel devoicing is not phonologically appropriate in this environment, the conversion of the devoiced vowel to the phoneme before lexical access through phonological inference cannot be expected. Thus, lexical access through the devoiced stimulus is possibly achieved by the acoustic assessment of all the segments in the input and a search for the right word with the closest phonetic match. This process was quite time-consuming, and when a stimulus was devoiced in the second block of the experiment, the same time-consuming process took place to access the lexicon, during which the priming effect had already disappeared.</p>
<p>In conclusion, the current study demonstrated that a frequency-based direct access well accounts for processing highly frequent phonological variability such as English flapping and Japanese vowel devoicing. Phonological appropriateness and high frequency seem to be a key for an allophonic variant to be stored in the lexicon. It is suggested that multiple representations are inter-connected and one form activates another through the lemma. A more strongly activated (usually highly frequent) form is produced during the repetition of a word. This model can easily solve the mismatch problem between an input and an underlying form in perception and mismatch between an input and an output in production. The current results are also compatible with exemplar models (<xref ref-type="bibr" rid="bibr12-0023830911434118">Goldinger, 1996</xref>, <xref ref-type="bibr" rid="bibr13-0023830911434118">1997</xref>, <xref ref-type="bibr" rid="bibr14-0023830911434118">1998</xref>; <xref ref-type="bibr" rid="bibr15-0023830911434118">Goldinger &amp; Azuma, 2003</xref>; <xref ref-type="bibr" rid="bibr16-0023830911434118">Goldinger et al., 1991</xref>; <xref ref-type="bibr" rid="bibr22-0023830911434118">Johnson, 1997</xref>; <xref ref-type="bibr" rid="bibr43-0023830911434118">Pierrehumbert, 2001</xref>, <xref ref-type="bibr" rid="bibr44-0023830911434118">2002</xref>; <xref ref-type="bibr" rid="bibr45-0023830911434118">Pisoni, 1990</xref>, <xref ref-type="bibr" rid="bibr46-0023830911434118">1992</xref>, <xref ref-type="bibr" rid="bibr47-0023830911434118">1997</xref>). <xref ref-type="bibr" rid="bibr1-0023830911434118">Bürki et al. (2010)</xref> suggested that every token with the same pronunciation needs to form an exemplar cloud and the frequency of the exemplars must dominate the cloud as a node. However, exemplar models need to consider one important issue—how much acoustic detail of pronunciation is preserved in each exemplar? If every single piece of acoustic information must be stored as an extreme view of the exemplar model assumes, no pronunciation can be the same acoustically. The question is how are acoustically similar pronunciations considered as the same pronunciation? For example, the current study illustrated that Japanese devoiced vowels have two types, deleted and true devoiced. Both of them are phonologically appropriate in the devoicing environment, but the frequency of the deleted type is much higher than that of the true devoiced type. If each type of vowel forms different exemplars, processing of true devoiced stimuli in the appropriate phonological contexts must be very difficult because of weak activation through a very few exemplars, but it is not certain if this is the case. It may be very unlikely in real life that Japanese listeners categorize the devoiced vowel into the deleted and true devoiced types during processing. This further division of allophones does not seem to bring any benefit to processing; rather it may be inhibitory for processing because it requires more phonetic analysis. As long as this issue remains unclear in exemplar models, a frequency-based hybrid model proposed by Connine and her colleagues seems to be more plausible because the model allows some extent of abstraction of sounds at least to the level of allophones.</p>
</sec>
</body>
<back>
<app-group>
<table-wrap id="table7-0023830911434118" position="float">
<label>Appendix:</label>
<caption>
<p>Experimental materials</p>
</caption>
<graphic alternate-form-of="table7-0023830911434118" xlink:href="10.1177_0023830911434118-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left" char="."/>
<col align="left" char="."/>
</colgroup>
<tbody>
<tr>
<td colspan="3">Devoicing environment</td>
</tr>
<tr>
<td>akikan <italic>empty can</italic></td>
<td>kachiku <italic>domestic animal</italic></td>
<td>kasikoi <italic>clever</italic></td>
</tr>
<tr>
<td>asita <italic>tomorrow</italic></td>
<td>onsitsu <italic>greenhouse</italic></td>
<td>kakikotoba <italic>written language</italic></td>
</tr>
<tr>
<td>hakike <italic>nausea</italic></td>
<td>ochikomu <italic>depressed</italic></td>
<td>tekitō <italic>appropriate</italic></td>
</tr>
<tr>
<td>hasika <italic>measles</italic></td>
<td>akisu <italic>thief</italic></td>
<td>osikakeru <italic>go uninvited</italic></td>
</tr>
<tr>
<td>ekitai <italic>liquid</italic></td>
<td>wasitsu <italic>Japanese-style room</italic></td>
<td>sekitori <italic>sumo wrestler</italic></td>
</tr>
<tr>
<td>yakisoba <italic>fried noodles</italic></td>
<td>sekihan <italic>red-colored rice</italic></td>
<td>machikado <italic>street corner</italic></td>
</tr>
<tr>
<td colspan="3">Voicing environment</td>
</tr>
<tr>
<td>nekojita <italic>sensitive to hot food</italic></td>
<td>tejina <italic>magic</italic></td>
<td>kejime <italic>to draw a line</italic></td>
</tr>
<tr>
<td>mijikai <italic>short</italic></td>
<td>kujikeru <italic>discouraged</italic></td>
<td>ojisan <italic>uncle</italic></td>
</tr>
<tr>
<td>sōjiki <italic>vacuum cleaner</italic></td>
<td>daijiken <italic>big incident</italic></td>
<td>hajiku <italic>to snap</italic></td>
</tr>
<tr>
<td>akirameru <italic>to give up</italic></td>
<td>takibi <italic>bonfire</italic></td>
<td>techigai <italic>mistake</italic></td>
</tr>
<tr>
<td>sasidasu <italic>to stretch out</italic></td>
<td>sekidome <italic>cough syrup</italic></td>
<td>osidasu <italic>to push out</italic></td>
</tr>
<tr>
<td>ichigo <italic>strawberry</italic></td>
<td>tachiba <italic>standpoint</italic></td>
<td>dekigoto <italic>incident</italic></td>
</tr>
<tr>
<td colspan="3">Control items</td>
</tr>
<tr>
<td>sakana <italic>fish</italic></td>
<td>techō <italic>day planner</italic></td>
<td>kaeru <italic>frog</italic></td>
</tr>
<tr>
<td>makura <italic>pillow</italic></td>
<td>sanso <italic>oxygen</italic></td>
<td>sawagu <italic>to make noise</italic></td>
</tr>
<tr>
<td>migaku <italic>to polish</italic></td>
<td>yōsu <italic>state</italic></td>
<td>hirune <italic>nap</italic></td>
</tr>
<tr>
<td>hogosha <italic>guardian</italic></td>
<td>simedasu <italic>to shut out</italic></td>
<td>byōin <italic>hospital</italic></td>
</tr>
<tr>
<td>zeikin <italic>tax</italic></td>
<td>bōeki <italic>trade</italic></td>
<td>yamaneko <italic>wildcat</italic></td>
</tr>
<tr>
<td>natsuyasumi <italic>summer break</italic></td>
<td>kataguruma <italic>ride on shoulder</italic></td>
<td>suzusī <italic>cool</italic></td>
</tr>
<tr>
<td>midori <italic>green</italic></td>
<td>megami <italic>goddess</italic></td>
<td>taido <italic>attitude</italic></td>
</tr>
<tr>
<td>jumon <italic>spell</italic></td>
<td>dantai <italic>group</italic></td>
<td>simauma <italic>zebra</italic></td>
</tr>
<tr>
<td>kōkan <italic>exchange</italic></td>
<td>shokubutsu <italic>plants</italic></td>
<td>tebukuro <italic>gloves</italic></td>
</tr>
<tr>
<td>satsuē <italic>filming</italic></td>
<td>ēyō <italic>nutrition</italic></td>
<td>ryokō <italic>travel</italic></td>
</tr>
<tr>
<td>mizutama <italic>polka dots</italic></td>
<td>mezamasi <italic>eye opener</italic></td>
<td>menseki <italic>area</italic></td>
</tr>
<tr>
<td>nettaigyo <italic>tropical fish</italic></td>
<td>katazukeru <italic>to put in order</italic></td>
<td/>
</tr>
<tr>
<td colspan="3">Practice items</td>
</tr>
<tr>
<td>majime <italic>serious</italic></td>
<td>yakitori <italic>grilled chicken</italic></td>
<td>chīsai <italic>small</italic></td>
</tr>
<tr>
<td>kasutera <italic>pound cake</italic></td>
<td>aruku <italic>to walk</italic></td>
<td>arikui <italic>anteater</italic></td>
</tr>
</tbody>
</table>
</table-wrap>
</app-group>
<ack>
<p>I would like to thank Natasha Warner and James McQueen for discussions and comments on this material, and to thank Meghan Clayards and an anonymous reviewer for their helpful and insightful comments on the earlier version of the manuscript. My gratitude also goes to the Academic Paper Editing Clinic, NTNU, Shannon Bischoff, and Erick Gutierrez Macias. The data in this study came from Chapter 6 of the unpublished doctoral dissertation, Ogasawara, N. (2007) <italic>Processing of Speech Variability: Vowel Reduction in Japanese</italic>, University of Arizona, USA; however, the analyses and theoretical implication were extensively revised for the current study. A portion of the study was presented at Laboratory Phonology 11, Wellington, New Zealand, July 1, 2008.</p>
</ack>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0023830911434118">
<label>1</label>
<p>Only the voiced consonant /dʒ/ whose place of articulation is close to that of the target vowel was adopted as the preceding consonant owing to the ease of articulation.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bürki</surname><given-names>A.</given-names></name>
<name><surname>Ernestus</surname><given-names>M.</given-names></name>
<name><surname>Frauenfelder</surname><given-names>U. H.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Is there only one “fenêtre” in the production lexicon? On-line evidence on the nature of phonological representations of pronunciation variants for French schwa words</article-title>. <source>Journal of Memory and Language</source>, <volume>62</volume>, <fpage>421</fpage>–<lpage>437</lpage>.</citation>
</ref>
<ref id="bibr2-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Connine</surname><given-names>C. M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>It’s not what you hear but how often you hear it: On the neglected role of phonological variant frequency in auditory word recognition</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>11</volume>, <fpage>1084</fpage>–<lpage>1089</lpage>.</citation>
</ref>
<ref id="bibr3-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Connine</surname><given-names>C. M.</given-names></name>
<name><surname>Blasko</surname><given-names>D. G.</given-names></name>
<name><surname>Titone</surname><given-names>D. A.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Do the beginnings of spoken words have a special status in auditory word recognition?</article-title> <source>Journal of Memory and Language</source>, <volume>32</volume>, <fpage>193</fpage>–<lpage>210</lpage>.</citation>
</ref>
<ref id="bibr4-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Connine</surname><given-names>C. M.</given-names></name>
<name><surname>Pinnow</surname><given-names>E.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Phonological variation in spoken word recognition: Episodes and abstractions</article-title>. <source>The Linguistic Review</source>, <volume>23</volume>, <fpage>235</fpage>–<lpage>245</lpage>.</citation>
</ref>
<ref id="bibr5-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cutler</surname><given-names>A.</given-names></name>
<name><surname>Otake</surname><given-names>T.</given-names></name>
<name><surname>McQueen</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Vowel devoicing and the perception of spoken Japanese words</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>125</volume>, <fpage>1693</fpage>–<lpage>1703</lpage>.</citation>
</ref>
<ref id="bibr6-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cutler</surname><given-names>A.</given-names></name>
<name><surname>van Ooijen</surname><given-names>B.</given-names></name>
<name><surname>Norris</surname><given-names>D.</given-names></name>
<name><surname>Sánchez-Casas</surname><given-names>R.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Speeded detection of vowels: A cross-linguistic study</article-title>. <source>Perception and Psychophysics</source>, <volume>58</volume>, <fpage>807</fpage>–<lpage>822</lpage>.</citation>
</ref>
<ref id="bibr7-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dahan</surname><given-names>D.</given-names></name>
<name><surname>Magnuson</surname><given-names>J. S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Spoken-word recognition</article-title>. In <person-group person-group-type="editor">
<name><surname>Traxler</surname><given-names>M. J.</given-names></name>
<name><surname>Gernsbacher</surname><given-names>M. A.</given-names></name>
</person-group> (Eds.), <source>Handbook of psycholinguistics</source> (pp. <fpage>249</fpage>–<lpage>283</lpage>). <publisher-loc>Amsterdam, The Netherlands</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr8-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eddington</surname><given-names>D.</given-names></name>
<name><surname>Elzinga</surname><given-names>D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The phonetic context of American English flapping: Quantitative evidence</article-title>. <source>Language and Speech</source>, <volume>51</volume>, <fpage>245</fpage>–<lpage>266</lpage>.</citation>
</ref>
<ref id="bibr9-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fujimoto</surname><given-names>M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Boin museikaji no kootoochoosetsu: Museikano sukunai Oosaka hoogen washano baai</article-title> [<trans-title xml:lang="en">Glottal opening pattern in devoiced tokens by an Osaka dialect speaker</trans-title>]. <source>Journal of the Phonetic Society of Japan</source>, <volume>9</volume>, <fpage>50</fpage>–<lpage>59</lpage>.</citation>
</ref>
<ref id="bibr10-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gaskell</surname><given-names>G. M.</given-names></name>
<name><surname>Marslen-Wilson</surname><given-names>W. D.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Phonological variation and inference in lexical access</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>22</volume>, <fpage>144</fpage>–<lpage>158</lpage>.</citation>
</ref>
<ref id="bibr11-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gaskell</surname><given-names>G. M.</given-names></name>
<name><surname>Marslen-Wilson</surname><given-names>W. D.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Mechanisms of phonological inference in speech perception</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>24</volume>, <fpage>380</fpage>–<lpage>396</lpage>.</citation>
</ref>
<ref id="bibr12-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldinger</surname><given-names>S. D.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Words and voices: Episodic traces in spoken word identification and recognition memory</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>22</volume>, <fpage>1166</fpage>–<lpage>1183</lpage>.</citation>
</ref>
<ref id="bibr13-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Goldinger</surname><given-names>S. D.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Perception and production in an episodic lexicon</article-title>. In <person-group person-group-type="editor">
<name><surname>Johnson</surname><given-names>K.</given-names></name>
<name><surname>Mullennix</surname><given-names>J. W.</given-names></name>
</person-group> (Eds.), <source>Talker variability in speech processing</source> (pp. <fpage>33</fpage>–<lpage>66</lpage>). <publisher-loc>San Diego</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldinger</surname><given-names>S. D.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Echoes of echoes? An episodic theory of lexical access</article-title>. <source>Psychological Review</source>, <volume>105</volume>, <fpage>251</fpage>–<lpage>279</lpage>.</citation>
</ref>
<ref id="bibr15-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldinger</surname><given-names>S. D.</given-names></name>
<name><surname>Azuma</surname><given-names>T.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Puzzle-solving science: The quixotic quest for units in speech perception</article-title>. <source>Journal of Phonetics</source>, <volume>31</volume>, <fpage>305</fpage>–<lpage>320</lpage>.</citation>
</ref>
<ref id="bibr16-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldinger</surname><given-names>S. D.</given-names></name>
<name><surname>Pisoni</surname><given-names>D. B.</given-names></name>
<name><surname>Logan</surname><given-names>J. S.</given-names></name>
</person-group> (<year>1991</year>). <article-title>On the nature of talker variability effects on recall of spoken word lists</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>17</volume>, <fpage>152</fpage>–<lpage>162</lpage>.</citation>
</ref>
<ref id="bibr17-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gow</surname><given-names>D. W.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Assimilation and anticipation in continuous spoken word recognition</article-title>. <source>Journal of Memory and Language</source>, <volume>45</volume>, <fpage>133</fpage>–<lpage>159</lpage>.</citation>
</ref>
<ref id="bibr18-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gow</surname><given-names>D. W.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Feature parsing: Feature cue mapping in spoken word recognition</article-title>. <source>Perception and Psychophysics</source>, <volume>65</volume>, <fpage>575</fpage>–<lpage>590</lpage>.</citation>
</ref>
<ref id="bibr19-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Han</surname><given-names>M. S.</given-names></name>
</person-group> (<year>1962</year>). <article-title>Unvoicing of vowels in Japanese</article-title>. In <collab>Phonetic Society of Japan</collab> (Ed.), <source>The Study of Sounds</source>, <volume>10</volume>, <fpage>81</fpage>–<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr20-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hirose</surname><given-names>H.</given-names></name>
</person-group> (<year>1971</year>). <article-title>The activity of the adductor laryngeal muscles in respect to vowel devoicing in Japanese</article-title>. <source>Phonetica</source>, <volume>23</volume>, <fpage>156</fpage>–<lpage>170</lpage>.</citation>
</ref>
<ref id="bibr21-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Inozuka</surname><given-names>H.</given-names></name>
<name><surname>Inozuka</surname><given-names>M.</given-names></name>
</person-group> (<year>2003</year>). <source>Nihongo onseigaku no shikumi</source> [<trans-source xml:lang="en">System of Japanese phonetics</trans-source>]. <publisher-loc>Tokyo, Japan</publisher-loc>: <publisher-name>Kenkyuusha</publisher-name>.</citation>
</ref>
<ref id="bibr22-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>K.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Speech perception without speaker normalization</article-title>. In <person-group person-group-type="editor">
<name><surname>Johnson</surname><given-names>K.</given-names></name>
<name><surname>Mullennix</surname><given-names>J. W.</given-names></name>
</person-group> (Eds.), <source>Talker variability in speech processing</source> (pp. <fpage>145</fpage>–<lpage>165</lpage>). <publisher-loc>San Diego</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr23-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jusczyk</surname><given-names>P.</given-names></name>
<name><surname>Luce</surname><given-names>P. A.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Speech perception and spoken word recognition: Past and present</article-title>. <source>Ear &amp; Hearing</source>, <volume>23</volume>, <fpage>1</fpage>–<lpage>40</lpage>.</citation>
</ref>
<ref id="bibr24-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Klatt</surname><given-names>D. H.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Review of selected models of speech perception</article-title>. In <person-group person-group-type="editor">
<name><surname>Marslen-Wilson</surname><given-names>W.</given-names></name>
</person-group> (Ed.), <source>Lexical representation and process</source> (pp. <fpage>169</fpage>–<lpage>226</lpage>). <publisher-loc>London, UK</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr25-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kondo</surname><given-names>M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Syllable structure and its acoustic effects on vowels in devoicing environments</article-title>. In <person-group person-group-type="editor">
<name><surname>van de Weijer</surname><given-names>J.</given-names></name>
<name><surname>Nanjo</surname><given-names>K.</given-names></name>
<name><surname>Nishihara</surname><given-names>T.</given-names></name>
</person-group> (Eds.), <source>Voicing in Japanese</source> (pp. <fpage>229</fpage>–<lpage>246</lpage>). <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>.</citation>
</ref>
<ref id="bibr26-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kubozono</surname><given-names>H.</given-names></name>
</person-group> (<year>1999</year>). <source>Nihongo no onsei</source> [<trans-source xml:lang="en">Japanese phonetics</trans-source>]. <publisher-loc>Tokyo, Japan</publisher-loc>: <publisher-name>Iwanami shoten</publisher-name>.</citation>
</ref>
<ref id="bibr27-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>LoCasto</surname><given-names>P.</given-names></name>
<name><surname>Connine</surname><given-names>C. M.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Rule-governed missing information in spoken word recognition: Schwa vowel deletion</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>64</volume>, <fpage>208</fpage>–<lpage>219</lpage>.</citation>
</ref>
<ref id="bibr28-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Maekawa</surname><given-names>K.</given-names></name>
<name><surname>Kikuchi</surname><given-names>H.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Corpus-based analysis of vowel devoicing in spontaneous Japanese: An interim report</article-title>. In <person-group person-group-type="editor">
<name><surname>van de Weijer</surname><given-names>J.</given-names></name>
<name><surname>Nanjo</surname><given-names>K.</given-names></name>
<name><surname>Nishihara</surname><given-names>T.</given-names></name>
</person-group> (Eds.), <source>Voicing in Japanese</source> (pp. <fpage>205</fpage>–<lpage>228</lpage>). <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>.</citation>
</ref>
<ref id="bibr29-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marslen-Wilson</surname><given-names>W. D.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Speech shadowing and speech comprehension</article-title>. <source>Speech Communication</source>, <volume>4</volume>, <fpage>55</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr30-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marslen-Wilson</surname><given-names>W. D.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Functional parallelism in spoken word-recognition</article-title>. <source>Cognition</source>, <volume>25</volume>, <fpage>71</fpage>–<lpage>102</lpage>.</citation>
</ref>
<ref id="bibr31-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marslen-Wilson</surname><given-names>W. D.</given-names></name>
<name><surname>Gaskell</surname><given-names>G.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Match and mis-match in lexical access [Abstract]</article-title>. <source>International Journal of Psychology</source>, <volume>27</volume>, <fpage>61</fpage>.</citation>
</ref>
<ref id="bibr32-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marslen-Wilson</surname><given-names>W. D.</given-names></name>
<name><surname>Warren</surname><given-names>P.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Levels of perceptual representation and process in lexical access: Words, phonemes, and features</article-title>. <source>Psychology Review</source>, <volume>101</volume>, <fpage>653</fpage>–<lpage>675</lpage>.</citation>
</ref>
<ref id="bibr33-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marslen-Wilson</surname><given-names>W. D.</given-names></name>
<name><surname>Zwitserlood</surname><given-names>P.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Accessing spoken words: On the importance of word onset</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>15</volume>, <fpage>576</fpage>–<lpage>585</lpage>.</citation>
</ref>
<ref id="bibr34-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Martin</surname><given-names>S. E.</given-names></name>
</person-group> (<year>1987</year>). <source>The Japanese language through time</source>. <publisher-loc>New Haven</publisher-loc>: <publisher-name>Yale University Press</publisher-name>.</citation>
</ref>
<ref id="bibr35-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Massaro</surname><given-names>D. W.</given-names></name>
<name><surname>Oden</surname><given-names>G. C.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Independence of lexical context and phonological information in speech perception</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>21</volume>, <fpage>1053</fpage>–<lpage>1064</lpage>.</citation>
</ref>
<ref id="bibr36-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McClelland</surname><given-names>J. L.</given-names></name>
<name><surname>Elman</surname><given-names>J. L.</given-names></name>
</person-group> (<year>1986</year>). <article-title>The TRACE model of speech perception</article-title>. <source>Cognitive Psychology</source>, <volume>18</volume>, <fpage>1</fpage>–<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr37-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McLennan</surname><given-names>C. T.</given-names></name>
<name><surname>Luce</surname><given-names>P. A.</given-names></name>
<name><surname>Charles-Luce</surname><given-names>J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Representation of lexical form</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>29</volume>(<issue>4</issue>), <fpage>539</fpage>–<lpage>553</lpage>.</citation>
</ref>
<ref id="bibr38-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McLennan</surname><given-names>C. T.</given-names></name>
<name><surname>Luce</surname><given-names>P. A.</given-names></name>
<name><surname>Charles-Luce</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Representation of lexical form: Evidence from studies of sublexical ambiguity</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>31</volume>(<issue>6</issue>), <fpage>1308</fpage>–<lpage>1314</lpage>.</citation>
</ref>
<ref id="bibr39-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McQueen</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Eight questions about spoken-word recognition</article-title>. In <person-group person-group-type="editor">
<name><surname>Gaskell</surname><given-names>M. G.</given-names></name>
</person-group> (Ed.), <source>The Oxford handbook of psycholinguistics</source> (pp. <fpage>37</fpage>–<lpage>53</lpage>). <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr40-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nakamura</surname><given-names>M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The articulation of vowel devoicing: A preliminary analysis</article-title>. <source>Phonological Studies</source>, <volume>6</volume>, <fpage>49</fpage>–<lpage>58</lpage>.</citation>
</ref>
<ref id="bibr41-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ogasawara</surname><given-names>N.</given-names></name>
<name><surname>Warner</surname><given-names>N.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Processing missing vowels: Allophonic processing in Japanese</article-title>. <source>Language and Cognitive Processes</source>, <volume>24</volume>, <fpage>376</fpage>–<lpage>411</lpage>.</citation>
</ref>
<ref id="bibr42-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ostreicher</surname><given-names>H. J.</given-names></name>
<name><surname>Sharf</surname><given-names>D. J.</given-names></name>
</person-group> (<year>1976</year>). <article-title>Effects of coarticulation on the identification of deleted consonant and vowel sounds</article-title>. <source>Journal of Phonetics</source>, <volume>4</volume>, <fpage>285</fpage>–<lpage>301</lpage>.</citation>
</ref>
<ref id="bibr43-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pierrehumbert</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Exemplar dynamics: Word frequency, lenition and contrast</article-title>. In <person-group person-group-type="editor">
<name><surname>Bybee</surname><given-names>J. L.</given-names></name>
<name><surname>Hopper</surname><given-names>P.</given-names></name>
</person-group> (Eds.), <source>Frequency and the emergence of linguistic structure</source> (pp. <fpage>137</fpage>–<lpage>157</lpage>). <publisher-loc>Amsterdam, The Netherlands</publisher-loc>: <publisher-name>John Benjamins</publisher-name>.</citation>
</ref>
<ref id="bibr44-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pierrehumbert</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Word-specific phonetics</article-title>. In <person-group person-group-type="editor">
<name><surname>Gussenhoven</surname><given-names>C.</given-names></name>
<name><surname>Warner</surname><given-names>N.</given-names></name>
</person-group> (Eds.), <source>Laboratory Phonology</source> <volume>7</volume> (pp. <fpage>101</fpage>–<lpage>139</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>.</citation>
</ref>
<ref id="bibr45-0023830911434118">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Pisoni</surname><given-names>D. B.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Effects of talker variability on speech perception: Implications for current research and theory</article-title>. In <person-group person-group-type="editor">
<name><surname>Fujisaki</surname><given-names>H.</given-names></name>
</person-group> (Ed.), <conf-name>Proceedings of the 1990 International Conference on Spoken Language Processing</conf-name> (pp. <fpage>1399</fpage>–<lpage>1407</lpage>). <conf-loc>Kobe, Japan</conf-loc>.</citation>
</ref>
<ref id="bibr46-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pisoni</surname><given-names>D. B.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Talker normalization in speech perception</article-title>. In <person-group person-group-type="editor">
<name><surname>Tohkura</surname><given-names>Y.</given-names></name>
<name><surname>Vatikiotis-Bateson</surname><given-names>E.</given-names></name>
<name><surname>Sagisaka</surname><given-names>Y.</given-names></name>
</person-group> (Eds.), <source>Speech perception, production, and linguistic structure</source> (pp. <fpage>143</fpage>–<lpage>151</lpage>). <publisher-loc>Tokyo, Japan</publisher-loc>: <publisher-name>Ohmsha Press</publisher-name>.</citation>
</ref>
<ref id="bibr47-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pisoni</surname><given-names>D. B.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Some thoughts on “normalization” in speech perception</article-title>. In <person-group person-group-type="editor">
<name><surname>Johnson</surname><given-names>K.</given-names></name>
<name><surname>Mullennix</surname><given-names>J. W.</given-names></name>
</person-group> (Eds.), <source>Talker variability in speech processing</source> (pp. <fpage>9</fpage>–<lpage>32</lpage>). <publisher-loc>San Diego</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr48-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pisoni</surname><given-names>D. B.</given-names></name>
<name><surname>Luce</surname><given-names>P. A.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Acoustic-phonetic representations in word recognition</article-title>. <source>Cognition</source>, <volume>25</volume>, <fpage>1</fpage>–<lpage>52</lpage>.</citation>
</ref>
<ref id="bibr49-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ranbom</surname><given-names>L. J.</given-names></name>
<name><surname>Connine</surname><given-names>C. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Lexical representation of phonological variation in spoken word recognition</article-title>. <source>Journal of Memory and Language</source>, <volume>57</volume>, <fpage>273</fpage>–<lpage>298</lpage>.</citation>
</ref>
<ref id="bibr50-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sakurai</surname><given-names>S.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Kyootsuugo no hatsuon de chuui subeki kotogara</article-title> [<trans-title xml:lang="en">Issues on pronunciation of Standard Japanese</trans-title>]. In <collab>Nippon Hoso Kyokai</collab> (Ed.), <source>Nihongo Hatsuon Akusento Jiten</source> (pp. <fpage>128</fpage>–<lpage>134</lpage>). <publisher-loc>Tokyo, Japan</publisher-loc>: <publisher-name>Nippon Hoso Shuppan Kyokai</publisher-name>.</citation>
</ref>
<ref id="bibr51-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Snoeren</surname><given-names>N. D.</given-names></name>
<name><surname>Seguí</surname><given-names>J.</given-names></name>
<name><surname>Hallé</surname><given-names>P. A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Perceptual processing of partially and fully assimilated words in French</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>34</volume>, <fpage>193</fpage>–<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr52-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Studdert-Kennedy</surname><given-names>M.</given-names></name>
</person-group> (<year>1974</year>). <article-title>The perception of speech</article-title>. In <person-group person-group-type="editor">
<name><surname>Sebeok</surname><given-names>T. A.</given-names></name>
</person-group> (Ed.), <source>Current trends in linguistics: Vol. XII</source> (pp. <fpage>2349</fpage>–<lpage>2385</lpage>). <publisher-loc>The Hague, The Netherlands</publisher-loc>: <publisher-name>Mouton</publisher-name>.</citation>
</ref>
<ref id="bibr53-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vance</surname><given-names>T. J.</given-names></name>
</person-group> (<year>1987</year>). <source>An introduction to Japanese phonology</source>. <publisher-loc>Albany</publisher-loc>: <publisher-name>State University of New York Press</publisher-name>.</citation>
</ref>
<ref id="bibr54-0023830911434118">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vance</surname><given-names>T. J.</given-names></name>
</person-group> (<year>2008</year>). <source>The sounds of Japanese</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr55-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yoshioka</surname><given-names>H.</given-names></name>
</person-group> (<year>1981</year>). <article-title>Laryngeal adjustments in the production of the fricative consonants and devoiced vowels in Japanese</article-title>. <source>Phonetica</source>, <volume>38</volume>, <fpage>236</fpage>–<lpage>251</lpage>.</citation>
</ref>
<ref id="bibr56-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yoshioka</surname><given-names>H.</given-names></name>
<name><surname>Löfqvist</surname><given-names>A.</given-names></name>
<name><surname>Hirose</surname><given-names>H.</given-names></name>
</person-group> (<year>1982</year>). <article-title>Laryngeal adjustments in Japanese voiceless sound production</article-title>. <source>Journal of Phonetics</source>, <volume>10</volume>, <fpage>1</fpage>–<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr57-0023830911434118">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yuen</surname><given-names>C. L.-K.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The perception of Japanese devoiced vowels</article-title>. <source>Proceedings of the Chicago Linguistic Society</source>, <volume>36</volume>, <fpage>531</fpage>–<lpage>547</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>