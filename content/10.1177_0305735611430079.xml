<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">POM</journal-id>
<journal-id journal-id-type="hwp">sppom</journal-id>
<journal-title>Psychology of Music</journal-title>
<issn pub-type="ppub">0305-7356</issn>
<issn pub-type="epub">1741-3087</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0305735611430079</article-id>
<article-id pub-id-type="publisher-id">10.1177_0305735611430079</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Reliability issues regarding the beginning, middle and end of continuous emotion ratings to music</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Schubert</surname><given-names>Emery</given-names></name>
</contrib>
<aff id="aff1-0305735611430079">School of English, Media and Performing Arts, University of New South Wales, Australia</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0305735611430079">Emery Schubert, Empirical Musicology Group, School of English, Media and Performing Arts, University of New South Wales, Sydney NSW 2052, Australia. Email: <email>E.Schubert@unsw.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>41</volume>
<issue>3</issue>
<fpage>350</fpage>
<lpage>371</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">Society for Education, Music, and Psychology Research</copyright-holder>
</permissions>
<abstract>
<p>Continuous self-reported emotion expressed by four pieces of music were collected on a two-dimensional (valence and arousal) emotion space in a repeated measures (test-retest conditions) design. Initial orientation time (IOT), test-retest reliability and afterglow were examined. Median IOT was 8 seconds. Valence ratings took up to 25 (median 4), and for arousal up to 35 (median 12) seconds. Slower tempi seemed to require longer IOT. Test-retest reliability examined correlation coefficients, and compared periods of sample-by-sample good agreement in response between Test and Retest condition. About 80% of responses were reliable in both the Test and Retest conditions regardless of response dimension. Pearson correlations demonstrated better test-retest reliability for arousal responses than for valence. Retest condition ratings were within 8% of Test condition rating within participant. Average standard deviations for ratings collapsed across dimension, stimulus and conditions was 12.2% of the ratings scale range. Afterglow effects – large outliers in spread of scores just after the end of a piece – were identified. The reliability of continuous emotional response is therefore considered to be quite good, but caution must be taken as to how to deal with the opening and ending of continuous emotional response data.</p>
</abstract>
<kwd-group>
<kwd>afterglow</kwd>
<kwd>arousal</kwd>
<kwd>continuous response</kwd>
<kwd>emotion in music</kwd>
<kwd>orientation time</kwd>
<kwd>reliability</kwd>
<kwd>time series</kwd>
<kwd>valence</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0305735611430079" sec-type="intro">
<title>Introduction</title>
<p>Both emotional responses to music, and the music which triggers these emotions, unfold in time. With relatively cheap and fast computers, it is possible to collect emotional response to music and other temporally dependent stimuli in real time, using little more than a computer and a mouse (<xref ref-type="bibr" rid="bibr13-0305735611430079">Cowie et al., 2000</xref>; <xref ref-type="bibr" rid="bibr32-0305735611430079">Nagel, Kopiez, Grewe, &amp; Altenmüller, 2007</xref>; <xref ref-type="bibr" rid="bibr42-0305735611430079">Schubert, 1999</xref>, <xref ref-type="bibr" rid="bibr47-0305735611430079">2007b</xref>) or more specialized devices (<xref ref-type="bibr" rid="bibr21-0305735611430079">Geringer, Madsen, &amp; Gregory, 2004</xref>; <xref ref-type="bibr" rid="bibr52-0305735611430079">Stevens et al., 2009</xref>). As a result we have been able to investigate aspects of emotion in music that were not afforded to researchers previously because they were restricted to measuring emotional response to music after the musical stimulus was heard, the so-called ‘postperformance’ or retrospective measure (e.g., <xref ref-type="bibr" rid="bibr5-0305735611430079">Asmus, 1985</xref>; <xref ref-type="bibr" rid="bibr19-0305735611430079">Gabrielsson &amp; Lindström, 2001</xref>; <xref ref-type="bibr" rid="bibr27-0305735611430079">Hevner, 1936</xref>). The continuous approach has allowed us to verify that emotions can vary from moment to moment as a piece of music unfolds, and that these variations can only be captured with postperformance methods in a way that is necessarily reductive: the richness of a time-varying emotional experience is potentially lost.</p>
<p>While the number of papers published on continuous emotional response to music are rising, there are relatively few that focus specifically on analytic issues. For example, when a continuous response is reported, how certain can we be that a statistically identical profile will emerge if the experiment were run again at a different time? That is, replication is rare, and the present study aims to address this. Exceptions are <xref ref-type="bibr" rid="bibr42-0305735611430079">Schubert (1999)</xref> and <xref ref-type="bibr" rid="bibr25-0305735611430079">Grewe et al. (2007a)</xref> but the former only reports general, overall test-retest reliability using Pearson correlations, and the latter took repeated measures to one participant, albeit seven times (see also, <xref ref-type="bibr" rid="bibr26-0305735611430079">Grewe, Nagel, Kopiez, &amp; Altenmüller, 2007b</xref>). Further, there are several complications when dealing with time-series data. One of them is the reliability of the initial responses made to a piece of music – a period of ‘settling-in’ during which responses are unreliable. Another complication occurs at the end of a continuous rating collection period, when the participant’s emotion-expressed rating continues despite the absence of the just completed music. <xref ref-type="bibr" rid="bibr50-0305735611430079">Schubert and Dunsmuir (1999)</xref> catalogued various kinds of outliers in the continuous music-emotion system. The initial response outlier was classified by them as ‘orientation’ time. The variability in response just after the music had ended was called ‘afterglow’. However, no statistical analysis was provided to diagnose what an initial orientation time might be, and afterglow was described in terms of outliers from a musical feature predictive model. The present study aims to revisit these issues and extend recent research by <xref ref-type="bibr" rid="bibr6-0305735611430079">Bachorik et al. (2009)</xref> from the perspective of the variability of multiple participant ratings to the same piece, rather than from a musical feature modelling point of view (for which, see also <xref ref-type="bibr" rid="bibr28-0305735611430079">Korhonen, Clausi, &amp; Jernigan, 2006</xref>; <xref ref-type="bibr" rid="bibr45-0305735611430079">Schubert, 2004</xref>). In addition, this paper will report how reliable emotion ratings are using a test-retest experimental paradigm.</p>
<p>Methods used to analyse time-series data range from pure visual inspection to sophisticated time-series and functional data analysis methods (see <xref ref-type="bibr" rid="bibr9-0305735611430079">Box, Jenkins, &amp; Reinsel, 1994</xref>; <xref ref-type="bibr" rid="bibr17-0305735611430079">Engle, 2001</xref>; <xref ref-type="bibr" rid="bibr22-0305735611430079">Gottman, 1981</xref>; <xref ref-type="bibr" rid="bibr36-0305735611430079">Ramsay &amp; Silverman, 2002</xref>). According to the review of the field by <xref ref-type="bibr" rid="bibr43-0305735611430079">Schubert (2001)</xref>, most analytic techniques applied to emotion in music data lie on the less sophisticated, visual inspection end of the spectrum, and do not always show awareness of the complications to which time-series data are prone, such as serial correlation (<xref ref-type="bibr" rid="bibr44-0305735611430079">Schubert, 2002</xref>). Further, the apparent complexities of the sophisticated analytic techniques discourage some researchers from applying legitimate ways of processing data. In this paper, a simple method of identifying rating agreement in a time series, the Second Order Deviation Threshold, will be applied. It will be described in the analysis section of this paper.</p>
<p>A study by <xref ref-type="bibr" rid="bibr42-0305735611430079">Schubert (1999)</xref> (the data of which is the basis of the present study) presented results of two Pearson correlation analyses comparing the Test and Retest conditions. A correlation coefficient of 0.75 for valence (the positive-negative dimension of emotion) and 0.71 for arousal (the arousal-sleepiness dimension of emotion) were reported. Since then, research into self-report continuous emotional response has developed sufficiently that a more sophisticated analysis is timely. For example, there have been several developments in the comparison of such time-series data (<xref ref-type="bibr" rid="bibr23-0305735611430079">Grewe, Kopiez, &amp; Altenmüller, 2009a</xref>, <xref ref-type="bibr" rid="bibr24-0305735611430079">2009b</xref>; <xref ref-type="bibr" rid="bibr25-0305735611430079">Grewe et al., 2007a</xref>, <xref ref-type="bibr" rid="bibr26-0305735611430079">2007b</xref>; <xref ref-type="bibr" rid="bibr31-0305735611430079">McAdams, Vines, Vieillard, Smith, &amp; Reynolds, 2004</xref>; <xref ref-type="bibr" rid="bibr44-0305735611430079">Schubert, 2002</xref>; <xref ref-type="bibr" rid="bibr57-0305735611430079">Vines, Krumhansl, Wanderley, &amp; Levitin, 2006</xref>). <xref ref-type="bibr" rid="bibr25-0305735611430079">Grewe et al. (2007a)</xref> examined responses of a participant who provided continuous responses to the same pieces of music seven times. In that study moment by moment median responses for the pieces were reported, as well as the differenced (explained in more detail later in this paper) responses, and they used the nonparametric Wilcoxon test to identify whether peak moments were significant. <xref ref-type="bibr" rid="bibr31-0305735611430079">McAdams et al. (2004)</xref> compared two sets of responses using running (moment by moment) <italic>F</italic>-tests. The risk of inflated Type I error detection due to multiple testing (because time-series data typically have many samples) were dealt with by setting the error probability threshold to <italic>p</italic> = 0.01 or 0.001, rather than the more usual <italic>p</italic> = 0.05 found in conventional parametric data analysis. These approaches are generally appropriate for the analyses of interest in those studies. However, none of the more recent studies have specifically examined test-retest reliability with multiple participants. For example, the <italic>F</italic>-test and Wilcoxon test mentioned are typically used to identify significant <italic>differences</italic> between time points, rather than significant <italic>similarities</italic> across time-series. <xref ref-type="bibr" rid="bibr42-0305735611430079">Schubert (1999)</xref> applied Pearson correlation coefficients as a measure of similarity of two time-series data sets, but this only demonstrated dependence, rather than absolute differences, and further, Pearson correlations were reported as producing inflated values unless the data sets were transformed, for example, through differencing (<xref ref-type="bibr" rid="bibr44-0305735611430079">Schubert, 2002</xref>; <xref ref-type="bibr" rid="bibr57-0305735611430079">Vines et al., 2006</xref>) which is what Schubert reported in the 1999 paper. In the absence of an established, absolute correlation coefficient about which test-retest reliability can be quantified, the approach in the present study is to compare two kinds of responses – valence and arousal – and determine which, if any, is more reliable.</p>
<sec id="section2-0305735611430079">
<title>Aim</title>
<p>This aim of the study was to explore the extent to which a continuous emotion rating of music is stable, and to what extent it remains stable when the same piece is rated again at a later date. The focus will be on ratings of expressed emotion made near the beginning of the music, those made at the end of the music, as well as more general test-retest reliability. Further, the study examines whether arousal or valence ratings of emotion (as defined below) are reported with equal reliability.</p>
</sec>
</sec>
<sec id="section3-0305735611430079" sec-type="methods">
<title>Method</title>
<sec id="section4-0305735611430079">
<title>Participants</title>
<p>Fourteen participants completed the test and retest conditions as part of a larger study (<xref ref-type="bibr" rid="bibr42-0305735611430079">Schubert, 1999</xref>). There were six males and eight females. Ages ranged from 19 to 48 years (mean = 29.40, <italic>SD</italic> = 10.36). Ten participants reporting having some music-playing experience, including eight who reported more than ten years of playing. Four reported never playing a musical instrument.</p>
</sec>
<sec id="section5-0305735611430079">
<title>Stimuli</title>
<p>Four pieces were selected with the intention of depicting a wide variety of emotions. They were chosen from a largely Romantic, orchestral idiom. These pieces were <italic>Slavonic Dance</italic> Op. 46, No. 1, by Antonin Dvorak (Slavonic Dance), <italic>Pizzicato Polka</italic> by Johann (Jr) and Josef Strauss (Pizzicato), <italic>Morning Mood</italic> from <italic>Peer Gynt</italic> by Edvard Grieg (Morning) and the Adagio Movement from <italic>Concierto de Aranjuez</italic> for guitar and orchestra by Joaquin Rodrigo (Adagio). Abbreviations used for referring to the pieces are shown in parentheses. Full recording details are provided in the Discography.</p>
</sec>
<sec id="section6-0305735611430079">
<title>Procedure</title>
<p>The study used specially written software that controlled all aspects of participant interaction, interfacing and data collection. The study was conducted twice, first in the ‘Test’ condition, and later in a ‘Retest’ condition, as explained below. The software was run on a Macintosh LC520 computer with one participant tested at a time. In the test condition, participants received training to familiarize themselves with the Two-Dimensional Emotion-Space (2DES), which consisted of valence indicated on the x-axis (positive emotions to the right, negative to the left) and arousal on the y-axis (aroused emotions toward the top, and sleepy emotions towards the bottom), based on the circumplex model and as found on the affect grid of Russell (<xref ref-type="bibr" rid="bibr38-0305735611430079">Russell, 1980</xref>; <xref ref-type="bibr" rid="bibr39-0305735611430079">Russell, Weiss, &amp; Mendelsohn, 1989</xref>). Such a layout implies that the structure of emotion is dimensional, meaning that emotions are thought of as occupying various locations along a limited number of dimensions (in this case valence – positive vs negative, and arousal – high vs low). However, there are other models, such as a discrete units structure (sad, happy, angry etc.) with no necessary link along underlying dimensions. The debate as to which model is suited to which circumstances is to be found elsewhere (<xref ref-type="bibr" rid="bibr8-0305735611430079">Barrett, 1998</xref>; <xref ref-type="bibr" rid="bibr11-0305735611430079">Christie &amp; Friedman, 2004</xref>; <xref ref-type="bibr" rid="bibr16-0305735611430079">Eerola &amp; Vuoskoski, 2011</xref>; <xref ref-type="bibr" rid="bibr29-0305735611430079">Laukka, 2008</xref>; <xref ref-type="bibr" rid="bibr59-0305735611430079">Zentner, Grandjean, &amp; Scherer, 2008</xref>). However, for this research the structure of emotion was based on a dimensional model based on valence and arousal. This model was considered appropriate because it suited the logic of moving a mouse around a two-dimensional surface (computer screen). The x (valence) and y (arousal) axes were presented on the computer screen and mouse movements within the bounds of the axes were recorded on a 201 point scale (−100 to +100), with axes crossing at the origin.</p>
<p>The training consisted of three stages, each involving the presentation of a selection of words which encompassed a wide range of valence and arousal values. In one stage, only the arousal axis was displayed, in another only the valence axis was displayed and in the final stage, both arousal and valence axes were displayed as a 2DES. The order of initial presentation of valence and arousal stages was selected randomly across participants. High arousal words used were Afraid, Angry and Happy, while low arousal words of Relaxed and Sad were used. An animation placed each of these words at an appropriate region of the emotion space. This was followed by a practice session, where participants placed these and other words and pictures of faces (based on <xref ref-type="bibr" rid="bibr15-0305735611430079">Ekman &amp; Friesen, 1975</xref>) on the emotion space by clicking with the computer’s mouse. After a little practice, participants were able to do this reliably. The results of these responses are reported in <xref ref-type="bibr" rid="bibr42-0305735611430079">Schubert (1999)</xref>.</p>
<p>Then the participants were presented with the first of the four music stimuli selected at random for each participant. They moved the mouse to the middle of the emotion space and then the music, played over headphones, commenced. The software controlled the audio, which was all contained on one audio CD (see Discography) via the built-in CD-ROM player of the computer. Participants listened to the audio via headphones in a quiet room at a comfortable listening level. The participant was instructed to move the mouse around the emotion space to reflect the emotion that the music was expressing (and not how the music made the participant feel. See <xref ref-type="bibr" rid="bibr18-0305735611430079">Evans &amp; Schubert, 2008</xref>; <xref ref-type="bibr" rid="bibr20-0305735611430079">Gabrielsson, 2002</xref>; <xref ref-type="bibr" rid="bibr25-0305735611430079">Grewe et al., 2007a</xref>; <xref ref-type="bibr" rid="bibr40-0305735611430079">Salimpoor, Benovoy, Longo, Cooperstock, &amp; Zatorre, 2009</xref>). When no emotion was detected, participants were instructed to move the pointer to the middle of the emotion space. The four pieces were presented in random order, and participants were encouraged to take a short break between each piece.</p>
<p>Participants were not aware that they would be asked to take part in the study again, and six to twelve months elapsed before they returned to complete the Retest condition. This strategy was intended to minimize the possibility that participants would deliberately try to memorize their responses in the test condition. The exact timing of the second data collection phase could not easily be controlled because not all participants were available at the same time. However, it has been argued elsewhere that variations in time in repeating this kind of task of the duration indicated is unlikely to affect results considerably (<xref ref-type="bibr" rid="bibr10-0305735611430079">Cepeda, Pashler, Vul, Wixted, &amp; Rohrer, 2006</xref>; <xref ref-type="bibr" rid="bibr30-0305735611430079">Lucas, Schubert, &amp; Halpern, 2010</xref>). In the Retest condition, the participants received minimal training which was intended to provide a reminder of the way the emotion space worked. The pieces were presented in random order. In all continuous response recording, participant responses were recorded once per second (<xref ref-type="bibr" rid="bibr43-0305735611430079">Schubert, 2001</xref>).</p>
</sec>
<sec id="section7-0305735611430079">
<title>Analysis</title>
<p><xref ref-type="bibr" rid="bibr48-0305735611430079">Schubert (2007c</xref>, <xref ref-type="bibr" rid="bibr49-0305735611430079">2010</xref>; <xref ref-type="bibr" rid="bibr51-0305735611430079">Schubert et al., in press</xref>) proposed a general analytic technique that made minimal assumptions about the distribution characteristics of the continuous self-report emotional data. The technique was developed to provide a simple approach to analysing time-series data such as that of the present study. While the method is flexible and can be optimized for different situations, it can be performed on a conventional spreadsheet program using nothing more sophisticated than mean and standard deviation calculation, though other approaches are mentioned in some of the analyses that follow, and in the Discussion section. Further, the method has a visual orientation for easy identification of regions consisting of reliable ratings of the time series. Consider the general time-series <italic>X</italic>(t, ζ<sub>i</sub>)<sub>p,d,c</sub> where <italic>X</italic> is a matrix representing Arousal or Valence (subscript d) ratings in time t (each increment of this index progresses time by one sample) by i participants (where i varies from 1 to 14 for a given sample, and taken together is referred to an ensemble of ratings) to piece (or stimulus) p in condition c (Test vs Retest) at time t. The sample-by-sample ensemble mean is represented as a new time series, <italic>M</italic>(t)<sub>p,d,c</sub>, the sample-by-sample ensemble median is represented by a new time series <italic>Mdn</italic>(t)<sub>p,d,c</sub> and the sample-by-sample ensemble rating standard deviation time series as <italic>SD1</italic>(t)<sub>p,d,c</sub>. This series will be referred to as the first order standard deviation, and abbreviated <italic>SD1</italic>(t). Lower case subscripts indicate that the series is calculated for each combination (‘p,d,c’ for example, indicates 4 pieces by 2 dimensions by 2 conditions), whereas a capital letter subscript denotes the level of the variable (for example, <italic>M</italic>
<sub>p,d,T</sub>(t) indicates the sample-by-sample ensemble mean score in the test condition for each combination of piece by dimension). Individual elements of a time series are indicate by an index, such as <italic>X</italic>(t<sub>5</sub>, ζ<sub>7</sub>) which is the fifth time sample rating for participant number seven in the time series rating matrix <italic>X</italic>. A single real number evaluation of a time series is represented in lower case. For example, the standard deviation of the mean series, <italic>M</italic>(t), will evaluate to <italic>sd1</italic>.</p>
<p>The mean of <italic>SD1</italic>(t), <inline-formula id="inline-formula1-0305735611430079">
<mml:math display="inline" id="math1-0305735611430079">
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mo stretchy="true">¯</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
</inline-formula> is:</p>
<p><disp-formula id="disp-formula1-0305735611430079">
<label>(1)</label>
<mml:math display="block" id="math2-0305735611430079">
<mml:mrow>
<mml:mover accent="true">
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mo stretchy="true">¯</mml:mo>
</mml:mover>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mi>T</mml:mi>
</mml:mfrac>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>t</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>T</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0305735611430079" xlink:href="10.1177_0305735611430079-eq1.tif"/></disp-formula></p>
<p>The standard deviation of the <italic>SD1</italic>(t) series, which, like its mean, is a real number and will be referred to as the second order standard deviation, abbreviated <italic>sd2</italic>:</p>
<p><disp-formula id="disp-formula2-0305735611430079">
<label>(2)</label>
<mml:math display="block" id="math3-0305735611430079">
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mi>d</mml:mi>
<mml:mn>2</mml:mn>
<mml:mo>=</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mi>T</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>t</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>T</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mover accent="true">
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mo stretchy="true">¯</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
</mml:msqrt>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0305735611430079" xlink:href="10.1177_0305735611430079-eq2.tif"/></disp-formula></p>
<p>where T is the total number of samples recorded for a given piece in one condition and dimension. This total was dependent on the number of seconds of the CD track, and included about three seconds of silence after the stimulus finished playing.</p>
<p>These values were then used to assign a threshold range, or threshold level. The range or level of the threshold is labelled τ and is defined as</p>
<p><disp-formula id="disp-formula3-0305735611430079">
<label>(3), (4)</label>
<mml:math display="block" id="math4-0305735611430079">
<mml:mrow>
<mml:msub>
<mml:mi>τ</mml:mi>
<mml:mrow>
<mml:mo>±</mml:mo>
<mml:mtext>n</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mover accent="true">
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mo stretchy="true">¯</mml:mo>
</mml:mover>
<mml:mo>±</mml:mo>
<mml:mtext>n</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>s</mml:mi>
<mml:mi>d</mml:mi>
<mml:mn>2</mml:mn>
<mml:mspace width="0.25em"/>
<mml:mtext>or</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mi>τ</mml:mi>
<mml:mtext>n</mml:mtext>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mover accent="true">
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>D</mml:mi>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mo stretchy="true">¯</mml:mo>
</mml:mover>
<mml:mo>+</mml:mo>
<mml:mtext>n</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>s</mml:mi>
<mml:mi>d</mml:mi>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-0305735611430079" xlink:href="10.1177_0305735611430079-eq3.tif"/></disp-formula></p>
<p>where n is a real number multiplier of <italic>sd2</italic>
<sup><xref ref-type="fn" rid="fn1-0305735611430079">1</xref></sup>. τ<sub>±n</sub> refers to a range within which elements of the <italic>SD1</italic>(t) series are to fall to be considered in ‘good agreement’, and otherwise they are considered in ‘poor agreement’ (<xref ref-type="disp-formula" rid="disp-formula3-0305735611430079">Equation 3</xref>). For the threshold level τ<sub>n</sub> (<xref ref-type="disp-formula" rid="disp-formula3-0305735611430079">Equation 4</xref>), elements of <italic>SD1</italic>(t) are coded as ‘poor agreement’ when greater than or equal to τ<sub>n</sub> (because the deviation score for element t in <italic>SD1</italic>(t) is relatively large) and otherwise as ‘good agreement’ (because the deviation score of the element t in <italic>SD1</italic>(t) is relatively small). These terms will be applied and further developed in the reporting of the results.</p>
</sec>
</sec>
<sec id="section8-0305735611430079" sec-type="results">
<title>Results</title>
<sec id="section9-0305735611430079">
<title>Initial orientation time</title>
<p>Initial orientation time (IOT) in the present study was defined as the time taken for an emotional rating <italic>SD1</italic>(t) series to settle to within τ<sub>±n</sub> as described above. This definition is analogous to ‘settling time’ used in off-line controller design in engineering (<xref ref-type="bibr" rid="bibr55-0305735611430079">Tay, Mareels, &amp; Moore, 1998</xref>, p. 93).<sup><xref ref-type="fn" rid="fn2-0305735611430079">2</xref></sup> As with settling time, IOT was set as a range rather than a threshold because of the nature of the emotion space procedure used: Participants commenced their emotion ratings at the neutral centre of the emotion space. This means that the start of each piece produces good agreement. However, instructing participants to start in the neutral centre point of the emotion space is a pragmatic decision, and not a reflection of any true underlying agreement. In this case an estimate can be obtained by finding the time taken for the ratings to fall within the ensemble standard deviation range, such as ±n <italic>sd2</italic> of the mean <italic>SD1</italic>(t) value: τ<sub>±n</sub>. <xref ref-type="table" rid="table1-0305735611430079">Table 1</xref> shows the results of the analysis conducted with three range criteria: τ<sub>±1</sub>, τ<sub>±1.5</sub> and τ<sub>±2</sub>.</p>
<table-wrap id="table1-0305735611430079" position="float">
<label>Table 1.</label>
<caption>
<p>Initial orientation time by stimulus and dimension.</p>
</caption>
<graphic alternate-form-of="table1-0305735611430079" xlink:href="10.1177_0305735611430079-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Stimulus</th>
<th align="left">Dimension</th>
<th align="left">Condition</th>
<th align="left" colspan="3">Time (in seconds) to settle to within τ<sub>±n</sub><hr/></th>
</tr>
<tr>
<th/>
<th/>
<th/>
<th align="left">n = 1</th>
<th align="left">n = 1.5</th>
<th align="left">n = 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Adagio</td>
<td>Arousal</td>
<td>Test</td>
<td>18</td>
<td>17</td>
<td>5</td>
</tr>
<tr>
<td>Adagio</td>
<td>Arousal</td>
<td>Retest</td>
<td>35</td>
<td>18</td>
<td>11</td>
</tr>
<tr>
<td>Adagio</td>
<td>Valence</td>
<td>Test</td>
<td>15</td>
<td>13</td>
<td>5</td>
</tr>
<tr>
<td>Adagio</td>
<td>Valence</td>
<td>Retest</td>
<td>25</td>
<td>15</td>
<td>7</td>
</tr>
<tr>
<td>Pizzicato</td>
<td>Arousal</td>
<td>Test</td>
<td>10</td>
<td>9</td>
<td>8</td>
</tr>
<tr>
<td>Pizzicato</td>
<td>Arousal</td>
<td>Retest</td>
<td>14</td>
<td>11</td>
<td>3</td>
</tr>
<tr>
<td>Pizzicato</td>
<td>Valence</td>
<td>Test</td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>Pizzicato</td>
<td>Valence</td>
<td>Retest</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td>Slavonic Dance</td>
<td>Arousal</td>
<td>Test</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>Slavonic Dance</td>
<td>Arousal</td>
<td>Retest</td>
<td>5</td>
<td>5</td>
<td>4</td>
</tr>
<tr>
<td>Slavonic Dance</td>
<td>Valence</td>
<td>Test</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>Slavonic Dance</td>
<td>Valence</td>
<td>Retest</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>Morning</td>
<td>Arousal</td>
<td>Test</td>
<td>5</td>
<td>13</td>
<td>4</td>
</tr>
<tr>
<td>Morning</td>
<td>Arousal</td>
<td>Retest</td>
<td>9</td>
<td>9</td>
<td>5</td>
</tr>
<tr>
<td>Morning</td>
<td>Valence</td>
<td>Test</td>
<td>7</td>
<td>6</td>
<td>4</td>
</tr>
<tr>
<td>Morning</td>
<td>Valence</td>
<td>Retest</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td/>
<td>Overall</td>
<td>Median</td>
<td>8</td>
<td>7.5</td>
<td>4</td>
</tr>
<tr>
<td/>
<td/>
<td>Min</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td/>
<td/>
<td>Max</td>
<td>35</td>
<td>18</td>
<td>11</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>As the interval about gets wider due to the larger τ<sub>±n</sub> criterion, we expect to see the initial response settle into the good agreement region sooner (i.e., the estimate will be more liberal). The most dramatic examples of this are in the Arousal response for Adagio (18 s to 5 s in Test condition and 35 s to 11 s in Retest condition for τ<sub>±1</sub> and τ<sub>±2</sub> respectively – see also <xref ref-type="fig" rid="fig1-0305735611430079">Figure 1</xref>) and for Pizzicato in the Retest condition (for τ<sub>±1</sub> it is 14 s, but τ<sub>±1</sub> drops to 3 s). In some cases the IOT is extremely resilient to different τ criteria, such as the valence times for Pizzicato in both conditions (about 4 s), and for the Slavonic Dance: 2 to 5 seconds regardless of dimension and condition. Within this piece Arousal has the longest initial orientation time at 4 to 5 seconds, depending on the threshold criterion. These ‘resilient’ results may be a reflection of the generally smaller <italic>SD1</italic>(t) movements from the origin throughout the piece: for example, if <italic>SD1</italic>(t) remained very low throughout the stimulus-dimension-condition combination. Inspection of the eight <italic>SD1</italic>(t) time-series (<xref ref-type="fig" rid="fig2-0305735611430079">Figure 2</xref>) indicates quasi-exponential rise, overshoot and decay toward the ‘operating’ region of <italic>SD1</italic>(t) from the beginning in each case, verifying the presence of an initial orientation (start up) time, analogous to settling time. The Adagio Arousal response is presented as a close-up example in <xref ref-type="fig" rid="fig1-0305735611430079">Figure 1</xref>. It was chosen because it allows closer inspection of a result with a large disparity in IOT as a function of τ<sub>±n</sub>, with n = 1, 1.5 and 2. While the τ<sub>±2</sub> range produces a smaller, more liberal estimate of the initial orientation time (5 s in Test condition and 11 s in the Retest condition), the τ<sub>±1</sub> criterion allows us to see the markedly different effect of initial orientation time across the two conditions (17 s in the Test condition, and 35 s in the Retest condition).</p>
<fig id="fig1-0305735611430079" position="float">
<label>Figure 1.</label>
<caption>
<p>Initial orientation time estimates for Adagio Arousal for Test and Retest conditions using two threshold criteria.</p>
<p><italic>Note</italic>: First 30 to 40 seconds shown. Dotted line above x-axis is <italic>SD1</italic>(t). Solid line below x-axis is <italic>M</italic>(t)<sub>A,A,c</sub> (<italic>N</italic> = 14). Shaded region indicates sections of <italic>SD1</italic>(t)<sub>A,A,c</sub> that are within τ<sub>±1</sub> (plots a and c) or τ<sub>±2</sub> (plots b and d). First occurrence of shaded region (in time) is therefore the visually identifiable initial orientation time (IOT, which is the time indicated with an arrow ‘—&gt;’ on the x-axis). Notice that in this case the τ<sub>±2</sub> boundary produces an estimate of the initial orientation time which is markedly earlier than with the τ<sub>±1</sub> boundary. For a summary of orientation time value estimates for different τ, ranges, see <xref ref-type="table" rid="table1-0305735611430079">Table 1</xref>.</p>
</caption>
<graphic xlink:href="10.1177_0305735611430079-fig1.tif"/></fig>
<fig id="fig2-0305735611430079" position="float">
<label>Figure 2.</label>
<caption>
<p><italic>M</italic>(t) and <italic>SD1</italic>(t) time series of each stimulus, dimension and condition combination, showing regions of good agreement at threshold τ<sub>1</sub>.</p>
<p>Notes:</p>
<p>T Test Condition.</p>
<p>R Retest Condition.</p>
<p>x axis is time elapsed from the start of the track (see Discography for track durations) shown in MM:SS.</p>
<p>y axis is a 201 point response scale (±100) for each emotion dimension.</p>
<p>τ<sub>1</sub> based agreement shading extends from x axis to mean time-series (<italic>M</italic>(t)).</p>
<p>First number in parentheses is the Pearson correlation coefficient, <italic>r</italic>, of mean time series T (i.e., <italic>M</italic>(t)<sub>p,d,T</sub>) and R (i.e., <italic>M</italic>(t)<sub>p,d,T</sub>).</p>
<p>Second number in parentheses is the Pearson correlation coefficient (<italic>r</italic>Δ) of differenced mean time series T (i.e., Δ<italic>M</italic>(t)<sub>p,d,T</sub>) and differenced mean time series R (i.e., Δ<italic>M</italic>(t)<sub>p,d,T</sub>).</p>
<p>All correlation analyses are significant and <italic>p</italic> = 0.01.</p>
</caption>
<graphic xlink:href="10.1177_0305735611430079-fig2.tif"/></fig>
<p><xref ref-type="table" rid="table2-0305735611430079">Table 2</xref> provides a summary of these results. It shows that across the four stimuli there is no major difference in median initial orientation time across Test and Retest. At τ<sub>±1</sub>, the arousal ratings oriented one second earlier (11.5 versus 12.5 s) in the Retest, and the valence IOT remained unchanged (4 s) across Test and Retest. It is noted, however, that the maximum IOT tended to be longer in the Retest condition, perhaps reflecting the shorter training session that was performed for that condition regardless of the τ threshold range selected. For arousal, the maximum value increased from 18 s to 35 s, and for valence, from 15 s to 25 s (Test to Retest) for τ<sub>±2</sub>. The trend was the same for the other two threshold criteria reported, but was most consistent for τ<sub>±1.5</sub>, in that the maximum values across conditions changed the least for arousal and for valence.</p>
<table-wrap id="table2-0305735611430079" position="float">
<label>Table 2.</label>
<caption>
<p>Initial orientation time (in seconds) by response dimension collapsed across stimuli.</p>
</caption>
<graphic alternate-form-of="table2-0305735611430079" xlink:href="10.1177_0305735611430079-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="2">τ<sub>±1</sub><hr/></th>
<th/>
<th/>
<th align="left" colspan="2">τ<sub>±1.5</sub><hr/></th>
<th/>
<th/>
<th align="left" colspan="4">τ<sub>±2</sub><hr/></th>
</tr>
<tr>
<th/>
<th/>
<th align="left">A</th>
<th align="left">V</th>
<th/>
<th/>
<th align="left">A</th>
<th align="left">V</th>
<th/>
<th/>
<th align="left">A</th>
<th align="left">V</th>
</tr>
</thead>
<tbody>
<tr>
<td>Test</td>
<td>Median</td>
<td>12.5</td>
<td>4</td>
<td>Test</td>
<td>Median</td>
<td>11</td>
<td>4</td>
<td>Test</td>
<td>Median</td>
<td>4.5</td>
<td>4</td>
</tr>
<tr>
<td/>
<td>Min</td>
<td>3</td>
<td>2</td>
<td/>
<td>Min</td>
<td>2</td>
<td>2</td>
<td/>
<td>Min</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td/>
<td>Max</td>
<td>18</td>
<td>15</td>
<td/>
<td>Max</td>
<td>17</td>
<td>13</td>
<td/>
<td>Max</td>
<td>8</td>
<td>5</td>
</tr>
<tr>
<td>Retest</td>
<td>Median</td>
<td>11.5</td>
<td>4</td>
<td>Retest</td>
<td>Median</td>
<td>10</td>
<td>4</td>
<td>Retest</td>
<td>Median</td>
<td>4.5</td>
<td>3</td>
</tr>
<tr>
<td/>
<td>Min</td>
<td>5</td>
<td>2</td>
<td/>
<td>Min</td>
<td>5</td>
<td>2</td>
<td/>
<td>Min</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td/>
<td>Max</td>
<td>35</td>
<td>25</td>
<td/>
<td>Max</td>
<td>18</td>
<td>15</td>
<td/>
<td>Max</td>
<td>11</td>
<td>7</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0305735611430079"><p>A = Arousal; V = Valence.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section10-0305735611430079">
<title>Test-retest reliability</title>
<p>The reliable responses in each time series (four pieces by two dimensions) were identified by comparing the τ<sub>1</sub> threshold of <italic>SD1</italic>(t)<sub>p,d,T</sub> and <italic>SD1</italic>(t)<sub>p,d,R</sub> conditions. This meant that when elements in an <italic>SD1</italic>(t) series fall below τ<sub>1</sub>, the ratings are considered to be in good agreement with each other, and therefore, in part, reliable. This provided an indication of reliability <italic>within</italic> (i.e., during) the response to each piece and dimension. The τ<sub>1</sub> threshold was chosen to allow a reasonably large number of response samples to be considered in good agreement for reasons that will become clear shortly.</p>
<p>Visual inspection of the mean responses, M(t)<sub>p,d,c</sub>, in <xref ref-type="fig" rid="fig2-0305735611430079">Figure 2</xref> comparing each Test-Retest time series pair shows how similar the profiles appear to be for each stimulus-dimension combination. The conventional Pearson test-retest reliability is indicated in <xref ref-type="table" rid="table3-0305735611430079">Table 3</xref>, with all values at 0.83 or higher. However the serial correlation commonly found in continuous response data can inflate the coefficients generated by Pearson correlations. Each of the participant responses were therefore first-order differenced to reduce the effect of serial correlation (as suggested by <xref ref-type="bibr" rid="bibr44-0305735611430079">Schubert, 2002</xref>). The differencing transformation of the ensemble mean times series, Δ<italic>M</italic>(t)<sub>p,d,c</sub>, produces changes in response made by each participant for a given piece-dimension-condition combination from sample to sample, and is defined in general form as:</p>
<p><disp-formula id="disp-formula4-0305735611430079">
<label>(5)</label>
<mml:math display="block" id="math5-0305735611430079">
<mml:mrow>
<mml:mi>Δ</mml:mi>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>t</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mn>3</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mn>4</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mn>3</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mo>…</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mtext>T</mml:mtext>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mtext>t</mml:mtext>
<mml:mrow>
<mml:mtext>T</mml:mtext>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-0305735611430079" xlink:href="10.1177_0305735611430079-eq4.tif"/></disp-formula></p>
<table-wrap id="table3-0305735611430079" position="float">
<label>Table 3.</label>
<caption>
<p>Comparison of test-retest condition correlation and good agreement match methods for untreated (<italic>X</italic>(t)) and differenced (Δ<italic>X</italic>(t)) ratings.</p>
</caption>
<graphic alternate-form-of="table3-0305735611430079" xlink:href="10.1177_0305735611430079-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Stimulus<sup><xref ref-type="table-fn" rid="table-fn3-0305735611430079">a</xref></sup></th>
<th align="left">Dimension<sup><xref ref-type="table-fn" rid="table-fn4-0305735611430079">b</xref></sup></th>
<th align="left" colspan="2">Pearson’s <italic>r</italic> of<hr/></th>
<th align="left" colspan="2">Spearman’s rho of<hr/></th>
<th align="left" colspan="2">CA/Any based on <sup><xref ref-type="table-fn" rid="table-fn5-0305735611430079">c</xref></sup><hr/></th>
<th align="left" colspan="2">CA/All based on <sup><xref ref-type="table-fn" rid="table-fn6-0305735611430079">d</xref></sup><hr/></th>
</tr>
<tr>
<th/>
<th/>
<th align="left"><italic>M</italic>(t)<sub>T&amp;R</sub></th>
<th align="left">Δ<italic>M</italic>(t)<sub>T&amp;R</sub></th>
<th align="left"><italic>Mdn</italic>(t)<sub>T&amp;R</sub></th>
<th align="left">Δ<italic>Mdn</italic>(t)<sub>T&amp;R</sub></th>
<th align="left"><italic>X</italic>(t)</th>
<th align="left">Δ<italic>X</italic>(t)</th>
<th align="left"><italic>X</italic>(t)</th>
<th align="left">Δ<italic>X</italic>(t)</th>
</tr>
</thead>
<tbody>
<tr>
<td>P</td>
<td>A</td>
<td><bold>0.88</bold></td>
<td><bold>0.70</bold></td>
<td><bold>0.75</bold></td>
<td><bold>0.38</bold></td>
<td><bold>0.94</bold></td>
<td><bold>0.87</bold></td>
<td>0.82</td>
<td><bold>0.81</bold></td>
</tr>
<tr>
<td>P</td>
<td>V</td>
<td>0.83</td>
<td>0.30</td>
<td>0.38</td>
<td>0.19</td>
<td>0.85</td>
<td>0.83</td>
<td>0.82</td>
<td>0.80</td>
</tr>
<tr>
<td>M</td>
<td>A</td>
<td><bold>0.94</bold></td>
<td>0.74</td>
<td><bold>0.78</bold></td>
<td><bold>0.41</bold></td>
<td><bold>0.83</bold></td>
<td><bold>0.84</bold></td>
<td><bold>0.79</bold></td>
<td><bold>0.79</bold></td>
</tr>
<tr>
<td>M</td>
<td>V</td>
<td>0.85</td>
<td><bold>0.76</bold></td>
<td>0.74</td>
<td>0.32</td>
<td>0.76</td>
<td>0.77</td>
<td>0.72</td>
<td>0.74</td>
</tr>
<tr>
<td>S</td>
<td>A</td>
<td><bold>0.96</bold></td>
<td><bold>0.90</bold></td>
<td><bold>0.93</bold></td>
<td><bold>0.68</bold></td>
<td>0.72</td>
<td><bold>0.86</bold></td>
<td>0.69</td>
<td><bold>0.78</bold></td>
</tr>
<tr>
<td>S</td>
<td>V</td>
<td>0.89</td>
<td>0.67</td>
<td>0.73</td>
<td>0.35</td>
<td><bold>0.84</bold></td>
<td>0.82</td>
<td><bold>0.74</bold></td>
<td>0.75</td>
</tr>
<tr>
<td>A</td>
<td>A</td>
<td><bold>0.96</bold></td>
<td><bold>0.61</bold></td>
<td><bold>0.90</bold></td>
<td>0.21</td>
<td>0.79</td>
<td><bold>0.86</bold></td>
<td>0.76</td>
<td><bold>0.83</bold></td>
</tr>
<tr>
<td>A</td>
<td>V</td>
<td>0.79</td>
<td>0.40</td>
<td>0.78</td>
<td>0.21</td>
<td><bold>0.88</bold></td>
<td>0.85</td>
<td><bold>0.79</bold></td>
<td>0.79</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0305735611430079"><p><italic>Notes</italic>:</p></fn>
<fn id="table-fn3-0305735611430079">
<label>a</label>
<p>P Pizzicato, M. Morning, S. Slavonic Dance, A. Adagio</p></fn>
<fn id="table-fn4-0305735611430079">
<label>b</label>
<p>A Arousal, V. Valence</p></fn>
<fn id="table-fn5-0305735611430079">
<label>c</label>
<p>Corresponding sample good agreement (at τ<sub>1</sub>) match count between test and retest as ratio of any (Test or Retest) samples of good agreement (i.e., no agreement at corresponding samples in both test and retest ignored).</p></fn>
<fn id="table-fn6-0305735611430079">
<label>d</label>
<p>Corresponding sample good agreement (at τ<sub>1</sub>) match count between test and retest as a ratio of all samples (T) of the piece.</p></fn>
<fn id="table-fn7-0305735611430079"><p>If coefficient of one of a valence-arousal pair is greater, it is shown in bold font.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The Test and Retest sample-by-sample ensemble average Δ<italic>M</italic>(t)<sub>p,d,T</sub> and Δ<italic>M</italic>(t)<sub>p,d,R</sub> were subjected to a Pearson product–moment correlation analysis. This produced a test-retest reliability statistic that was markedly lower, as shown in <xref ref-type="table" rid="table3-0305735611430079">Table 3</xref>, ranging from 0.40 for Adagio Valence, to 0.90 for Slavonic Dance Arousal. Each of these correlation coefficients still produced large effect sizes, except for Adagio Valence which produced a medium effect size (<xref ref-type="bibr" rid="bibr12-0305735611430079">Cohen, 1992</xref>). The result demonstrates a clearer picture of relatively better reliability for arousal responses in each case. Only Morning Valence produced a lower Pearson’s <italic>r</italic> (= 0.76), but this was relatively close to the Morning Arousal Test-Retest correlation coefficient (<italic>r</italic> = 0.74). To further examine test-retest correlation, a nonparametric approach was applied, where the median of the ensemble ratings were generated to produce the new <italic>Mdn</italic>(t)<sub>p,d,T</sub> and <italic>Mdn</italic>(t)<italic><sub>p,d,R</sub></italic> series. A Spearman’s rho analysis was performed on these pairs, and is reported in <xref ref-type="table" rid="table3-0305735611430079">Table 3</xref>, as is the differenced version of the median series, Δ<italic>Mdn</italic>(t). In general, all rho values are lower than their corresponding Pearson’s <italic>r</italic>. However, the comparison is similar, with arousal ratings producing relatively higher rho values. Only the Spearman rho for Adagio produced a tie between arousal and valence when the median ensemble time series was differenced, but in this case the value was low (0.21) and therefore both dimensions have dubious test-retest reliability according to the Spearman rho criterion. This analysis, too, suggests that arousal responses are more reliable. However this is only an indication of reliability from the perspective of the statistical dependency of mean rating time series. It does not provide information, about differences between rating levels nor about regions in time, which may be spurious noise rather than reliable responses. Therefore, two further analyses were performed, one examining difference in test-retest paired ratings, and the other based on good agreement across test-retest conditions.</p>
<p>Paired differences in ratings between corresponding Test-Retest samples for each stimulus-dimension combination were performed by generating the time series that is a result of subtracting each Test sample element of the series from the corresponding Retest condition sample element, participant by participant, as according to the formula for each sample t, of participant i:</p>
<p><disp-formula id="disp-formula5-0305735611430079">
<label>(6)</label>
<mml:math display="block" id="math6-0305735611430079">
<mml:mrow>
<mml:mtext>X</mml:mtext>
<mml:msub>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>t</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mi>ζ</mml:mi>
<mml:mtext>i</mml:mtext>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mtext>p</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>d</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mtext>X</mml:mtext>
<mml:msub>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>t</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mi>ζ</mml:mi>
<mml:mtext>i</mml:mtext>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mtext>p</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>d</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mtext>R</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-0305735611430079" xlink:href="10.1177_0305735611430079-eq5.tif"/></disp-formula></p>
<p>producing a new time series for each stimulus-dimension combination, for each participant. The resulting Arousal Test-Retest rating difference series for the four stimuli by fourteen participants were pooled, as were the Valence Test-Retest rating difference series for the four stimuli by fourteen participants.</p>
<p>The median of the Test and Retest difference time series was 0.5 for arousal and -1.0 for valence, with 25th and 75th percentile (and 10th to 90th percentile in parentheses) values of −2 (−6) and 4 (7.85) for arousal, and -5 (−10) and 2.5 (5.5) for valence respectively. This is on a rating scale ranging from -100 to +100 – that is 201 units. Positive values indicate that the rating made in the Test condition was higher than that made in the Retest condition. So, while the differences in paired ratings between Test and Retest were on the whole small, valence ratings were slightly higher in the Retest (by a median of 5 units, compared to 2 units for arousal). Considering the 10th to 90th percentile ranges: 4/5ths of the all pairwise Test-Retest difference time-series elements fell within 13.85(= 7.85 + 6)/210 = 6.89% for arousal and 15.5(= 5.5 + 10)/201 = 7.71% for valence of the 201 point rating scale.</p>
<p>Next, samples with good agreement at τ<sub>1</sub> were identified for <italic>X</italic>(t)<sub>p,d,T</sub> and <italic>X</italic>(t)<sub>p,d,R</sub> (for each stimulus-dimension combination) for the purpose of providing a measure of reliability that would take into account the sample-by-sample agreement of ratings. This was performed over a two-stage process. In the first stage the total number of points that fell within the τ<sub>1</sub> threshold were located for each of the sixteen stimulus-dimension-condition combinations. These are shown graphically as the shaded regions in <xref ref-type="fig" rid="fig2-0305735611430079">Figure 2</xref>. As mentioned, this technique identifies the points that are in good agreement compared to other (unshaded) regions. Although standard deviation is not normally distributed, central limit theory predicts that with a sufficiently large number of samples the distribution will begin to resemble normality (<xref ref-type="bibr" rid="bibr41-0305735611430079">Salkind, 2010</xref>), in which case the ratio of points identified would be 0.84 (84%) using a threshold of τ<sub>1</sub>. Across the sixteen stimulus-dimension-condition combinations, the proportion of high-agreement elements selected ranged from 0.79 (for Adagio Valence) to 0.94 (for Pizzicato Valence). The median of these ratios is 0.84, suggesting an effect of the central limit theory with just 16 distribution samplings (albeit with a positive skew). However, what is important is <italic>where</italic> these regions of good agreement occur.</p>
<p>In the second stage the corresponding element numbers in the <italic>SD1</italic>(t)<sub>p,d,T</sub> and <italic>SD1</italic>(t)<sub>p,d.R</sub> series were compared and a new series generated where an element number was set to 1 if both the corresponding element numbers in test and retest condition had a value below τ<sub>1</sub> (good agreement) and otherwise 0 (poor agreement). That is, the new corresponding sample agreement match time series is generated by producing a value of 1 when both an element of <italic>SD1(t)</italic>
<sub>p,d,T</sub> is less than τ<sub>1</sub>, and the corresponding element of <italic>SD1(t)</italic>
<sub>p,d,T</sub> is also less than τ<sub>1</sub>. Otherwise the corresponding element of the new series is set to zero. In <xref ref-type="fig" rid="fig2-0305735611430079">Figure 2</xref> this is equivalent to locating the points in a time series where shaded regions of good agreement overlap (intersect) across test-retest plots. The analysis is summarized using two ratios. One ratio reports the count of 1 elements of the corresponding sample agreement match time series (equivalent of summing the elements) denoted by CA, as a proportion of the number of good agreement samples in <italic>either</italic> condition (number of samples with any shading in the overlapped Test and Retest plot). This means that a sample had to be considered in good agreement in <italic>at least one</italic> condition, and would ignore samples that are in poor agreement in both conditions (CA/Any columns in <xref ref-type="table" rid="table3-0305735611430079">Table 3</xref>). Such a calculation is not biased by the total number of high-agreement samples; poor-agreement samples will not contribute to the ratio. But it also means that a higher ratio of between-condition good-agreement matches would be reported if there were a larger number of poor-agreement matching samples in both conditions. Therefore a second ratio reports CA as a proportion of the <italic>total</italic> number of samples (CA/All columns in <xref ref-type="table" rid="table3-0305735611430079">Table 3</xref>, where ‘all’ here means T the total number of time samples for the rating duration of the stimulus). For the moment, we will restrict our analysis to the matches identified from the undifferenced <italic>SD1</italic>(t) series.</p>
<p>As can be seen in <xref ref-type="table" rid="table3-0305735611430079">Table 3</xref>, the fewest relative matches (regardless of ratio calculation) occur in Slavonic Dance Arousal (72% and 69% of samples with matches for CA/Any and CA/All, respectively). The highest ratio of matches occurred for Pizzicato Arousal, again regardless of the ratio (94% and 82% of any good match and of total samples respectively). It will now become clear why a threshold (τ<sub>1</sub>) was chosen that identified a reasonably large number of samples in good agreement — when comparing across conditions: a more stringent τ<sub>n</sub> criterion would have led to a loss of good-agreement points matching in both conditions. Hence, issuing an overly conservative threshold risked having no Test-Retest matches at all, thus preventing meaningful comparison.</p>
<p>The question at hand is to compare the reliability, according to this analysis, of arousal versus valence. The overall median for arousal match is 81% and for valence 84% as a proportion of any good agreement samples. As a proportion of all samples the overall median is 77% for both. Given the fluctuations of this proportion across stimuli, it is not possible to conclude that there is a difference in corresponding agreement between arousal and valence using this approach. In general, participants agree with each other’s responses in the same regions of a piece when responding on different occasions. According to the present calculations, based on a τ<sub>1</sub> threshold, this occurs at least two thirds of the time (69% worst case), is typically about 80% of the time (average of the median results for all ratios) and up to 94% of the time among the stimuli investigated. However, corresponding agreement in Valence Test-Retest is proportionally higher than Arousal for Slavonic Dance and Adagio. When the same agreement matching analysis is conducted on the differenced data – that is Δ<italic>X(t</italic>)<sub>p,d,T</sub> with Δ<italic>X</italic>(t)<sub>p,d,R</sub> – Arousal consistently has a higher proportion of Test-Retest matches in agreement for all stimuli compared to Valence (<xref ref-type="table" rid="table3-0305735611430079">Table 3</xref>). Since changes in arousal are sensitive to changes in loudness (<xref ref-type="bibr" rid="bibr45-0305735611430079">Schubert, 2004</xref>) it seems reasonable that pieces with many intense changes in dynamics (such as Slavonic Dance and Adagio) will lead to a higher number of changing responses. So, this finding is likely to be related to the Romantic style, orchestral instrumentation of the selected music.</p>
<p>The final analysis concerning test-retest reliability involves an examination of the standard deviation time series across each stimulus-dimension-condition combination: <italic>SD1</italic>(t)<sub>p,d,c</sub>. <xref ref-type="fig" rid="fig3-0305735611430079">Figure 3</xref> shows a comparison of various statistics (box-plot, minimum, maximum, mean and τ<sub>±1</sub>) applied to the <italic>SD1</italic>(t) time series. The height of each median and mean can be thought of as a ‘disagreement’ or ‘confusion’ estimate, because high mean/median indicates larger <italic>SD1</italic>(t) elements in the course of the emotional response dimension for that piece. Mean <italic>SD1</italic>(t) (i.e., ) values ranged from 21 (Slavonic Valence – both test and retest, median also 21 for both), to 28 (Adagio – both test and retest, median also 28 for both) units on a 201 point scale, which translates to 10% and 14% of the response scale range respectively. Median values produce similar results (median line of box plots are close to or overlap the large crosses representing mean, with mean displaying a small positive skew in all cases). Slavonic Dance Arousal Retest <italic>SD1</italic>(t)<sub>S,A,R</sub> had the smallest interquartile range (6.42 units), while Adagio Arousal Test had the largest (11.5 units). The τ<sub>±1</sub> and interquartile range changed little from Test to Retest within stimulus and dimension, however a small trend could be observed with Retest variability being slightly smaller than Test variability (e.g., compare interquartile range of Slavonic Dance Valence, noting that it is smaller in the Retest than the Test condition). No clear trend could be observed across dimension, but overall stimulus-wise differences could be observed, with Slavonic Dance producing higher agreement, with least heteroskedasticity (see the relatively low , and small interquartile and τ<sub>±1</sub> range in <xref ref-type="fig" rid="fig3-0305735611430079">Figure 3</xref>) of the four stimuli, and Adagio producing more poor agreement than the other stimuli (both according to τ<sub>±1</sub> and interquartile distances). Adagio Valence had the highest deviation scores, with a peak (maximum) of 49 units for both Test and Retest conditions (24% of the 201 point scale). Across the entire responses, collapsed across stimulus-dimension-condition, the mean standard deviation was 26.6 units, which is 12.2% of the response scale range (median = 12.6%).</p>
<fig id="fig3-0305735611430079" position="float">
<label>Figure 3.</label>
<caption>
<p>Central tendency (mean and median) and spread (sd and box plot) of <italic>SD1</italic>(t)<sub>p,d,c</sub>.</p><p><italic>Notes</italic>:</p>
<p>Box plots show interquartile range.</p>
<p>Lower and upper extremes of whiskers show 10 and 90 percentile points respectively.</p>
<p>T Test Condition</p>
<p>R Retest Condition</p>
<p>A Arousal</p>
<p>V Valence</p>
<p>y axis is emotion dimension scale units (−100 to +100).</p>
</caption>
<graphic xlink:href="10.1177_0305735611430079-fig3.tif"/></fig>
</sec>
<sec id="section11-0305735611430079">
<title>Afterglow</title>
<p>Afterglow was calculated by examining the <italic>SD1</italic>(t<sub>T</sub>)<sub>p,d,c</sub> values, the ensemble standard deviations of the last sample, calculated for each stimulus-dimension-condition combination, which was approximately three seconds after the offset of the last note of the piece (see Discography for t<sub>T</sub> values of each stimulus). It was not possible to obtain a precise offset time as different recordings had different amounts of reverberation. To put the final sample deviation score into perspective, it was divided by <italic>sd2</italic>. <xref ref-type="table" rid="table4-0305735611430079">Table 4</xref> lists <italic>SD1</italic>(t<sub>T</sub>)<sub>p,d,c</sub> for each stimulus-dimension-condition combination, both in rating scale units and as a multiple of <italic>sd2</italic>. For comparison, the initial sample standard deviation, <italic>SD1</italic>(t<sub>1</sub>)<sub>p,d,c</sub> is shown, which was expected to be small because all participants commenced their response from the same, central region on the emotion space. The peak <italic>SD1</italic>(t) value of each series is also shown. The overall highest peak <italic>SD1</italic>(t)<sub>p,d,c</sub> value was 9.64 <italic>sd2</italic> units which occurred in the last sample of the Slavonic Dance Arousal. This is symptomatic of the final, <italic>SD1</italic>(t<sub>T</sub>)<sub>p,d,c</sub>, values, with values close to or matching the peak <italic>SD1</italic>(t) value. The average of the peak values across stimuli and dimensions was 6.72 <italic>sd2</italic> units, whereas the final, afterglow <italic>SD1</italic>(t<sub>T</sub>) values were 6.00 <italic>sd2</italic> units. For comparison, the initial <italic>SD1</italic>(t<sub>1</sub>)<sub>p,d,c</sub> averaged only 0.3 <italic>sd2</italic> units across stimulus-dimension-condition combinations. No obvious trends could be identified in afterglow for emotion dimension, or for condition (Test versus Retest). But the afterglow effect, indicated by large disagreement in ratings, is clearly present.</p>
<table-wrap id="table4-0305735611430079" position="float">
<label>Table 4.</label>
<caption>
<p><italic>SD1</italic>(t) values at beginning, peak and ending (afterglow) of ensemble ratings and as proportions of <italic>sd2</italic>.</p>
</caption>
<graphic alternate-form-of="table4-0305735611430079" xlink:href="10.1177_0305735611430079-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Morning</th>
<th align="left">arousal test</th>
<th align="left">arousal retest</th>
<th align="left">valence test</th>
<th align="left">valence retest</th>
<th align="left">Mean</th>
<th align="left">Min</th>
<th align="left">Max</th>
</tr>
</thead>
<tbody>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)</td>
<td>0.98</td>
<td>0.96</td>
<td>1.6</td>
<td>1.02</td>
<td>1.14</td>
<td>0.96</td>
<td>1.6</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)</td>
<td>43.44</td>
<td>42.05</td>
<td>40.8</td>
<td>39.49</td>
<td>41.44</td>
<td>39.49</td>
<td>43.44</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)</td>
<td>37.69</td>
<td>30.11</td>
<td>35.44</td>
<td>33.33</td>
<td>34.14</td>
<td>30.11</td>
<td>37.69</td>
</tr>
<tr>
<td>sd2</td>
<td>7.78</td>
<td>6.18</td>
<td>5.45</td>
<td>5.95</td>
<td>6.34</td>
<td>5.45</td>
<td>7.78</td>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)/<italic>sd2</italic></td>
<td>0.13</td>
<td>0.16</td>
<td>0.29</td>
<td>0.17</td>
<td>0.19</td>
<td>0.13</td>
<td>0.29</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)/<italic>sd2</italic></td>
<td>5.58</td>
<td>6.8</td>
<td>7.49</td>
<td>6.63</td>
<td>6.63</td>
<td>5.58</td>
<td>7.49</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)/<italic>sd2</italic></td>
<td>4.84</td>
<td>4.87</td>
<td>6.51</td>
<td>5.6</td>
<td>5.45</td>
<td>4.84</td>
<td>6.51</td>
</tr>
<tr>
<th align="left">Pizzicato</th>
<th align="left">arousal test</th>
<th align="left">arousal retest</th>
<th align="left">valence test</th>
<th align="left">valence retest</th>
<th align="left">Mean</th>
<th align="left">Min</th>
<th align="left">Max</th>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)</td>
<td>0.77</td>
<td>1.42</td>
<td>0.7</td>
<td>0.77</td>
<td>0.92</td>
<td>0.7</td>
<td>1.42</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)</td>
<td>35.93</td>
<td>31.53</td>
<td>34.41</td>
<td>29.91</td>
<td>32.94</td>
<td>29.91</td>
<td>35.93</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)</td>
<td>29.35</td>
<td>25.35</td>
<td>30.37</td>
<td>23.94</td>
<td>27.25</td>
<td>23.94</td>
<td>30.37</td>
</tr>
<tr>
<td>sd2</td>
<td>5.08</td>
<td>4.98</td>
<td>4.1</td>
<td>3.17</td>
<td>4.33</td>
<td>3.17</td>
<td>5.08</td>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)/<italic>sd2</italic></td>
<td>0.15</td>
<td>0.29</td>
<td>0.17</td>
<td>0.24</td>
<td>0.21</td>
<td>0.15</td>
<td>0.29</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)/<italic>sd2</italic></td>
<td>7.07</td>
<td>6.33</td>
<td>8.39</td>
<td>9.44</td>
<td>7.81</td>
<td>6.33</td>
<td>9.44</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)/<italic>sd2</italic></td>
<td>5.78</td>
<td>5.09</td>
<td>7.4</td>
<td>7.56</td>
<td>6.46</td>
<td>5.09</td>
<td>7.56</td>
</tr>
<tr>
<th align="left">Slavonic Dance</th>
<th align="left">arousal test</th>
<th align="left">arousal retest</th>
<th align="left">valence test</th>
<th align="left">valence retest</th>
<th align="left">Mean</th>
<th align="left">Min</th>
<th align="left">Max</th>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)</td>
<td>4.37</td>
<td>1.83</td>
<td>4.13</td>
<td>3.59</td>
<td>3.48</td>
<td>1.83</td>
<td>4.37</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)</td>
<td>43.12</td>
<td>43.86</td>
<td>37.63</td>
<td>36.99</td>
<td>40.4</td>
<td>36.99</td>
<td>43.86</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)</td>
<td>39.13</td>
<td>43.86</td>
<td>37.02</td>
<td>36.99</td>
<td>39.25</td>
<td>36.99</td>
<td>43.86</td>
</tr>
<tr>
<td>sd2</td>
<td>5.71</td>
<td>4.55</td>
<td>5.13</td>
<td>4.08</td>
<td>4.87</td>
<td>4.08</td>
<td>5.71</td>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)/<italic>sd2</italic></td>
<td>0.77</td>
<td>0.4</td>
<td>0.8</td>
<td>0.88</td>
<td>0.71</td>
<td>0.4</td>
<td>0.88</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)/<italic>sd2</italic></td>
<td>7.55</td>
<td>9.64</td>
<td>7.33</td>
<td>9.07</td>
<td>8.4</td>
<td>7.33</td>
<td>9.64</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)/<italic>sd2</italic></td>
<td>6.85</td>
<td>9.64</td>
<td>7.21</td>
<td>9.07</td>
<td>8.19</td>
<td>6.85</td>
<td>9.64</td>
</tr>
<tr>
<th align="left">Adagio</th>
<th align="left">arousal test</th>
<th align="left">arousal retest</th>
<th align="left">valence test</th>
<th align="left">valence retest</th>
<th align="left">Mean</th>
<th align="left">Min</th>
<th align="left">Max</th>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)</td>
<td>0.92</td>
<td>0.96</td>
<td>1.22</td>
<td>0.98</td>
<td>1.02</td>
<td>0.92</td>
<td>1.22</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)</td>
<td>42.74</td>
<td>40.45</td>
<td>49.08</td>
<td>46.92</td>
<td>44.8</td>
<td>40.45</td>
<td>49.08</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)</td>
<td>41.68</td>
<td>36.19</td>
<td>28.03</td>
<td>24.71</td>
<td>32.65</td>
<td>24.71</td>
<td>41.68</td>
</tr>
<tr>
<td>sd2</td>
<td>6.41</td>
<td>4.34</td>
<td>6.67</td>
<td>5.4</td>
<td>5.7</td>
<td>4.34</td>
<td>6.67</td>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)/<italic>sd2</italic></td>
<td>0.14</td>
<td>0.22</td>
<td>0.18</td>
<td>0.18</td>
<td>0.18</td>
<td>0.14</td>
<td>0.22</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)/<italic>sd2</italic></td>
<td>6.67</td>
<td>9.32</td>
<td>7.36</td>
<td>8.69</td>
<td>8.01</td>
<td>6.67</td>
<td>9.32</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)/<italic>sd2</italic></td>
<td>6.5</td>
<td>8.34</td>
<td>4.2</td>
<td>4.58</td>
<td>5.91</td>
<td>4.2</td>
<td>8.34</td>
</tr>
<tr>
<th align="left">Means across stimuli</th>
<th align="left">arousal test</th>
<th align="left">arousal retest</th>
<th align="left">valence test</th>
<th align="left">valence retest</th>
<th align="left">Mean</th>
<th align="left">Min</th>
<th align="left">Max</th>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)</td>
<td>1.76</td>
<td>1.29</td>
<td>1.91</td>
<td>1.59</td>
<td>1.76</td>
<td>0.7</td>
<td>4.37</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)</td>
<td>41.31</td>
<td>39.47</td>
<td>40.48</td>
<td>38.33</td>
<td>41.31</td>
<td>29.91</td>
<td>49.08</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)</td>
<td>36.96</td>
<td>33.88</td>
<td>32.71</td>
<td>29.75</td>
<td>36.96</td>
<td>23.94</td>
<td>43.86</td>
</tr>
<tr>
<td>sd2</td>
<td>6.25</td>
<td>5.01</td>
<td>5.34</td>
<td>4.65</td>
<td>6.25</td>
<td>3.17</td>
<td>7.78</td>
</tr>
<tr>
<td>Opening: <italic>SD1</italic>(t<sub>1</sub>)/<italic>sd2</italic></td>
<td>0.3</td>
<td>0.27</td>
<td>0.36</td>
<td>0.37</td>
<td>0.3</td>
<td>0.13</td>
<td>0.88</td>
</tr>
<tr>
<td>Peak <italic>SD1</italic>(t)/<italic>sd2</italic></td>
<td>6.72</td>
<td>8.02</td>
<td>7.64</td>
<td>8.46</td>
<td>6.72</td>
<td>5.58</td>
<td>9.64</td>
</tr>
<tr>
<td>Afterglow: <italic>SD1</italic>(t<sub>T</sub>)/<italic>sd2</italic></td>
<td>5.99</td>
<td>6.98</td>
<td>6.33</td>
<td>6.7</td>
<td>5.99</td>
<td>4.2</td>
<td>9.64</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
</sec>
<sec id="section12-0305735611430079" sec-type="discussion">
<title>Discussion</title>
<sec id="section13-0305735611430079">
<title>Initial orientation time</title>
<p>Initial orientation time varies from 2 to 35 seconds, with an overall median of 8 seconds for the stimuli investigated, consistent with <xref ref-type="bibr" rid="bibr6-0305735611430079">Bachorik et al. (2009)</xref>. This means that the first seconds of response in a continuous response task must be treated with caution. It is likely that the present estimates are low because substantive training was provided, using examples from all quadrants of the emotion space, with worked examples, and asking the participants to perform the task with words and pictures of faces before commencing continuous responses to music. The reliability of continuous responses can therefore be improved by disregarding an opening period of response. Caution is also required when no training is given. Further studies should explore whether there is a training–orientation-time trade off, for if little instruction is provided, perhaps the participant will find their way around the response space or scale, but to reach a point of reliability will take longer (longer initial orientation time) which can later be diagnosed and discarded if necessary.</p>
<p>The present study does not, however, provide a dictum as to how long the initial orientation time will be in all or even similar continuous emotional response paradigms. The variability of the results provides evidence for this. It seems that initial orientation time is more complex than a participant needing a cognitive adjustment and processing time. It could be that some music facilitates consistent emotional response ratings more rapidly, and it might be that certain musical characteristics or musical style affect this time requirement (<xref ref-type="bibr" rid="bibr6-0305735611430079">Bachorik et al., 2009</xref>). The overall finding that valence had a shorter initial orientation time than arousal (4 s versus 12 s median times) suggests that valence judgements are things with which we are well experienced – evaluating whether something is positive or negative. But this is a dubious conclusion because arousal may be even more privileged in speed of response (e.g., see <xref ref-type="bibr" rid="bibr14-0305735611430079">Cuthbert, Schupp, Bradley, Birbaumer, &amp; Lang, 2000</xref>). A more plausible explanation is that there are more clear and unambiguous indicators of valence, in particular mode, than arousal for the examples used. Morning, Pizzicato and Slavonic Dance all produce a clear expression of major tonality at the very opening of the piece, and hence a clear positive valence (<xref ref-type="bibr" rid="bibr35-0305735611430079">Pittenger, 2003</xref>; <xref ref-type="bibr" rid="bibr58-0305735611430079">Webster &amp; Weir, 2005</xref>), although Morning may produce a slight ambiguity as the melody outlines a descending minor third from the fifth degree until it reaches the end of its descent at the tonic which is clearly in the major mode. Adagio is in a natural minor mode, and this provides a strong sense of negative valence. Yet, it produces long initial orientation times of 15 s in the test and 25 s in the retest condition (though still shorter than the corresponding arousal response). It is therefore somewhat a surprise that Morning produced a median initial orientation time of 4 s in the retest (same as the overall median) instead of something extended as in Adagio. Tempo may provide some explanation as the shortest initial orientation time was for the fastest piece, Slavonic Dance, and the slowest was for Adagio, regardless of emotional response dimension or Test-Retest condition. So, musical characteristics may well have a role to play, and selection of music may therefore have a bearing on the reliability of responses at the start of a continuous emotional response task.</p>
</sec>
<sec id="section14-0305735611430079">
<title>Test-retest reliability</title>
<p>The emotion ratings were examined in different ways to investigate the reliability of continuous emotional ratings. Because there are some moot issues regarding the reporting of test-retest reliability for continuous data, the focus was on comparative reliability, and in particular whether arousal responses are more, less, or equally reliable to valence responses. We have seen, above, that valence response has some advantage in terms of smaller initial orientation time. However, Test-Retest correlation of first-order differenced data suggested that arousal reliability is higher than valence reliability (average <italic>r</italic> = 0.79 and 0.54 respectively, undifferenced <italic>r</italic> = 0.97 and 0.92 respectively). But is this sufficient to conclude that continuous arousal response is more reliable than continuous valence response? Does it mean that when the two are combined onto an emotion-space at right angles that valence responses suffer? Correlations are sensitive to the amount of systematic variance in the variables under comparison. For example, the variability in the longest piece, Adagio – lasting over ten minutes – is relatively small in valence response, certainly compared to the responses to the other pieces (see <xref ref-type="fig" rid="fig2-0305735611430079">Figure 2</xref>). So, it could be that the lack of variability was responsible for the lowest correlation coefficient, of 0.37, for differenced Adagio Valence response, rather than a lack of reliability <italic>per se</italic>. To conclude that arousal responses are more reliable than valence responses would therefore necessarily be tentative.</p>
<p>A method that takes into account the within-participant variability was applied as another way of examining test-retest reliability. By comparing samples of relatively high agreement using the second order standard deviation method between Test-Retest conditions, no major differences could be identified, and no difference could be identified between valence and arousal. While further work is needed to determine what an adequate measure and level of reliability is, the present study proposes a technique that provides an alternative, but more so an addendum, to Pearson correlations in those instances where the correlation coefficient may be susceptible to rogue effects. For the method used, the typical level of reliability was quantified at 80%, meaning that participant responses were in relatively high agreement at the same points in time across test and retest conditions (using a τ<sub>1</sub> threshold criterion). This reliability varied from 69% to 94% of the time across the various stimulus-dimension combinations.</p>
<p>The final comparative analysis of reliability directly examined the mean of the standard deviations for each stimulus-dimension combination for Test and Retest conditions. The results indicate similar standard deviation (agreement) in the Retest condition. This result does not support a cognitive load explanation, which predicts that, because of practice, the participant does not require as much extraneous cognitive load for operating the interface and can focus more on the experimental task (<xref ref-type="bibr" rid="bibr33-0305735611430079">Paas, Tuovinen, van Merrienboer, &amp; Darabi, 2005</xref>; <xref ref-type="bibr" rid="bibr53-0305735611430079">Sweller, 1988</xref>; <xref ref-type="bibr" rid="bibr54-0305735611430079">Sweller, 2006</xref>). It would be interesting to see whether subsequent retests would further change agreement. If emotional ratings of music change due to multiple listenings (<xref ref-type="bibr" rid="bibr7-0305735611430079">Balkwill &amp; Thompson, 1999</xref>; <xref ref-type="bibr" rid="bibr37-0305735611430079">Ritossa &amp; Rickard, 2004</xref>), then it is likely that deviation scores will also change. However, it is more likely that this will be the case when it is the felt emotion that is rated, rather than (as in the present study) that which the music appears to be conveying (<xref ref-type="bibr" rid="bibr26-0305735611430079">Grewe et al., 2007b</xref>; <xref ref-type="bibr" rid="bibr40-0305735611430079">Salimpoor et al., 2009</xref>; <xref ref-type="bibr" rid="bibr46-0305735611430079">Schubert, 2007a</xref>).</p>
</sec>
<sec id="section15-0305735611430079">
<title>Afterglow</title>
<p>Participants are meant to return the cursor to the centre of the emotion space when the piece ends. They should do this because they were instructed to do so, but also, logically, if rating the emotion expressed by the music, and the music stops, the music cannot be expressing further emotion. The analysis provided evidence for the presence of an afterglow effect, caused by some participants not returning to the centre at the conclusion of the music. The lowest relative spread of scores at the end of any stimulus-dimension combination was 4.2 times the <italic>sd2</italic> of Adagio Valence, and the highest relative spread of scores was 9.64 times the <italic>sd2</italic> for Slavonic Dance Arousal which corresponds to the largest ‘outlier’ point for that entire piece. While there were no systematic differences in median afterglow values across dimensions, the values were slightly higher for the median Retest condition. There is, therefore, an unreliable response at the end of the continuous emotion rating task and this presents a message to continuous response researchers that the last portion of a response, like the initial orientation time, should be treated with caution, and possibly deleted from analysis if appropriate.</p>
<p>The explanation of the afterglow effect can only be speculated upon here. Some of the participants seem to become unaware of the requested task – unable to report the emotion that the music is expressing, and confusing the request (expressed emotion) rating with their own feelings. Another (though not independent) explanation is that the serial correlation of the recent responses has bled through to after the end of the piece. If this is the case, it is a further vindication of the differencing technique, which can mathematically reduce the effect of such inertial responses. In non-music literature, a similar effect is referred to as response inhibition – the time it takes to stop performing a task upon presentation of a ‘stop task’, though this inhibition time delay is of a smaller magnitude than the durations found in the present study (see <xref ref-type="bibr" rid="bibr34-0305735611430079">Pessoa, Padmala, Kenzer, &amp; Bauer, 2011</xref>; <xref ref-type="bibr" rid="bibr56-0305735611430079">Verbruggen &amp; Logan, 2009</xref>). Differencing ratings is an alternative to deleting data in cases where it is desirable to use as much of the time series as possible.</p>
</sec>
<sec id="section16-0305735611430079">
<title>The second order deviation threshold method</title>
<p>The second order deviation threshold method was used as a tool to help induce criteria for sample-by-sample agreement in a time series. It is a flexible method, and while the present study used only mean and standard deviation calculations to determine the good agreement threshold criteria, other statistics should be considered in the future. For example, instead of the mean, a less biased central tendency estimator, such as the median, could be used (see also <xref ref-type="bibr" rid="bibr25-0305735611430079">Grewe et al., 2007a</xref>; <xref ref-type="bibr" rid="bibr23-0305735611430079">Grewe et al., 2009a</xref>; <xref ref-type="bibr" rid="bibr28-0305735611430079">Korhonen et al., 2006</xref>). Further, alternative estimates of the spread of scores should be considered, such as the interquartile distance (<xref ref-type="bibr" rid="bibr23-0305735611430079">Grewe et al., 2009a</xref>; <xref ref-type="bibr" rid="bibr25-0305735611430079">Grewe et al., 2007a</xref>) and the median absolute distance (<xref ref-type="bibr" rid="bibr51-0305735611430079">Schubert et al., in press</xref>). So, for example, for the present study, the median, first order interquartile and second order interquartile could be applied in an analogous manner instead of the mean, first order standard deviation and second order deviation respectively. Some examples of these nonparametric statistics were reported in this study.</p>
<p>However the second order deviation threshold does have limitations. It is a relative measure of reliability. Since <italic>SD1</italic>(t) data are presented as a ratio of <italic>sd2</italic> the method is identical in principle to effect size (<xref ref-type="bibr" rid="bibr12-0305735611430079">Cohen, 1992</xref>), and is appropriate for comparing conditions, in the present case arousal and valence responses. In the present study, the technique, used along with other approaches, provides a tool for diagnosing initial orientation time, reliability and afterglow effects in continuous emotional responses to music. It demonstrates the presence of potentially shorter initial orientation time in valence, with respect to arousal, no difference (or equal reliability) across Test-Retest conditions for both arousal and for valence responses, and the presence of an unreliable or misleading ‘afterglow’ at the conclusion of the piece.</p>
</sec>
</sec>
<sec id="section17-0305735611430079" sec-type="conclusions">
<title>Conclusion</title>
<p>Test-retest reliability of four pieces of non-vocal, orchestrals music made by fourteen participants separated by a long period of time (six to twelve months) suggests that there is good agreement in identifying emotion expressed by music. The method of analysis employed confirmed and identified the presence of some important characteristics of time-series data collected from emotional response tasks to music. While it can be concluded that test-retest responses are generally reliable – the current approach reporting agreement across conditions about 80% of the time – the opening moments of the music and the end of the music in general produce unreliable results. Responses to the start of a piece of music are marked by a period of orientation time that, in the present study, ranged from a couple of seconds to over thirty seconds. It was longer for arousal response than for valence response, and was typically eight seconds long. The conclusion drawn is that the initial emotional responses need to be treated with caution and perhaps eliminated from some analyses. It is also speculated that training may reduce the orientation time, but repeated exposures may alter emotional responses, though these issues were not an explicitly manipulated in the present study.</p>
<p>Researchers are aware of the unreliability of the responses to the opening section of a piece of music – for example, <xref ref-type="bibr" rid="bibr25-0305735611430079">Grewe et al. (2007a)</xref> omitted for first 10 seconds of their response data. However, the present study provides a technique for diagnosing the duration of this orientation time. This will increase the amount of ‘useful’ response time that can be analysed, and highlights the variability of initial orientation time, which seems to be a function of musical features, with tempo being speculated as an important factor – slow pieces used in the present study had longer orientation times, and fast pieces had shorter ones.</p>
<p>Afterglow effects were also quantified in this study, and indicate the potential lack of reliability at the end of a continuous rating of expressed emotion by music task. Some of the largest deviation scores were identified at the responses just after the music ended, despite the instruction (returning to the neutral point) which should have produced very small ensemble rating deviations.</p>
<p>Continuous response methods offer exciting insights into how emotion in music works. The present study demonstrated how variations in responses can be identified visually, and can therefore be adapted to more sophisticated problems such as modelling the response as a function of other variables (e.g., <xref ref-type="bibr" rid="bibr28-0305735611430079">Korhonen et al., 2006</xref>; <xref ref-type="bibr" rid="bibr45-0305735611430079">Schubert, 2004</xref>) in the future. Further work is needed to investigate the nature of these various factors that influence reliability, and a wider range of musical styles (<xref ref-type="bibr" rid="bibr6-0305735611430079">Bachorik et al., 2009</xref>) will determine whether the results are limited to Western orchestral music from part of the last two centuries. As continuous response techniques are adopted by emotion in music researchers, the implications of issues such as reliability at the beginning, middle and end of the time dependent data sets need also be considered if the results of the data sets and subsequent conclusions are to be optimized.</p>
</sec>
</body>
<back>
<ack><p>This research was conducted with support from the Australian Research Council (DP0452290 and DP0986153). The author is grateful for the comments of two anonymous reviewers, and for the continuing support of Visiting Associate Eric Sowey and Professor William Dunsmuir.</p></ack>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0305735611430079">
<label>1.</label>
<p>In some statistics texts the pronumeral <italic>c</italic> is used to denote a real number. <italic>n</italic> is used here instead because C is already used to denote ‘experimental condition’.</p></fn>
<fn fn-type="other" id="fn2-0305735611430079">
<label>2.</label>
<p>Settling time in an idealised response that refers to system behaviour resulting from an impulse or step input. However, in continuous responses to music the listener typically experiences a stream of impulses and events (the changing musical features) and so the present system is more complex, and potentially heteroskedastic, well after the IOT. As will be discussed, <italic>SD1</italic>(t) may fluctuate outside τ<sub>±1</sub> during and after orientation time for reasons additional to settling time. The τ range criterion for identifying IOT is necessarily a simplification.</p></fn>
</fn-group>
</notes>
<bio>
<title>Author biography</title>
<p><bold>Emery Schubert</bold> is Associate Professor in the School of the Arts and Media at the University of New South Wales in Sydney, Australia and co-leader of the Empirical Musicology Group. His research interests are concerned with the ‘scientific’ study of aesthetics, including measuring and predicting emotional responses to music continuously. He is on the Editorial Board for Empirical Musicology Review and Journal of New Music Research, and was President of the Australian Music and Psychology Society (AMPS).</p>
</bio>
<ref-list>
<title>Discography</title>
<ref id="bibr1-0305735611430079">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Dvorak</surname><given-names>Anton</given-names></name>
</person-group>. <source>Slavonic Dance, Op.46. No.1</source>. Slovak Philharmonic Orchestra, Zdenek Kosler (conductor). Discover Classical Music, CD2. (1993) HNH International Ltd. 8.550008–9, track 9. Duration 3 minutes 45 seconds (t<sub>T</sub> = 225s).</citation>
</ref>
<ref id="bibr2-0305735611430079">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Strauss</surname><given-names>Johann</given-names><suffix>(Jr.)</suffix></name>
<name><surname>Strauss</surname><given-names>Joseph</given-names></name>
</person-group>. <source>Pizzicato Polka</source>. CSR Symphony Orchestra, Ondrej Lenard (conductor). Discover Classical Music, CD2. (<year>1993</year>) HNH International Ltd. 8.550008–9, track 11. Duration 2 minutes 30 seconds (t<sub>T</sub> = 150s).</citation>
</ref>
<ref id="bibr3-0305735611430079">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Grieg</surname><given-names>Edvard</given-names></name>
</person-group>. <source>Peer Gynt – Morning</source>. Czecho-Slovak Radio Symphony Orchestra, Stephen Gunzenhauser (conductor). Discover Classical Music, CD2. (<year>1993</year>) HNH International Ltd. 8.550008–9, track 13. Duration 3 minutes 38 seconds (t<sub>T</sub> = 218s).</citation>
</ref>
<ref id="bibr4-0305735611430079">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Rodrigo</surname><given-names>Joaquin</given-names></name>
</person-group>, <source>Concierto De Aranjuez – Adagio</source>. Gerald Garcia (guitar), CSSR State Philharmonic Orchestra, Peter Breiner (conductor). Discover Classical Music, CD2. (<year>1993</year>) HNH International Ltd. 8.550008–9, track 17. Duration 10 minutes 52 seconds (t<sub>T</sub> = 652s).</citation>
</ref>
</ref-list>
<ref-list>
<title>References</title>
<ref id="bibr5-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Asmus</surname><given-names>E. P.</given-names></name>
</person-group> (<year>1985</year>). <article-title>The development of a multidimensional instrument for the measurement of affective responses to music</article-title>. <source>Psychology of Music</source>, <volume>13</volume>, <fpage>19</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr6-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bachorik</surname><given-names>J. P.</given-names></name>
<name><surname>Bangert</surname><given-names>M.</given-names></name>
<name><surname>Loui</surname><given-names>P.</given-names></name>
<name><surname>Larke</surname><given-names>K.</given-names></name>
<name><surname>Berger</surname><given-names>J.</given-names></name>
<name><surname>Rowe</surname><given-names>R.</given-names></name><etal/>
</person-group>. (<year>2009</year>). <article-title>Emotion in motion: Investigating the time-course of emotional judgments of musical stimuli</article-title>. <source>Music Perception</source>, <volume>26</volume>(<issue>4</issue>), <fpage>355</fpage>–<lpage>364</lpage>.</citation>
</ref>
<ref id="bibr7-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Balkwill</surname><given-names>L.-L.</given-names></name>
<name><surname>Thompson</surname><given-names>W. F.</given-names></name>
</person-group> (<year>1999</year>). <article-title>A cross-cultural investigation of the perception of emotion in music: Psychophysical and cultural cues</article-title>. <source>Music Perception</source>, <volume>17</volume>(<issue>1</issue>), <fpage>43</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr8-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Barrett</surname><given-names>L. F.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Discrete emotions or dimensions? The role of valence focus and arousal focus</article-title>. <source>Cognition, &amp; Emotion</source>, <volume>12</volume>(<issue>4</issue>), <fpage>579</fpage>–<lpage>599</lpage>.</citation>
</ref>
<ref id="bibr9-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Box</surname><given-names>G. E. P.</given-names></name>
<name><surname>Jenkins</surname><given-names>G. M.</given-names></name>
<name><surname>Reinsel</surname><given-names>G. C.</given-names></name>
</person-group> (<year>1994</year>). <source>Time series analysis: Forecasting and control</source> (3rd ed). <publisher-loc>New Jersey</publisher-loc>: <publisher-name>Prentice-Hall</publisher-name>.</citation>
</ref>
<ref id="bibr10-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cepeda</surname><given-names>N. J.</given-names></name>
<name><surname>Pashler</surname><given-names>H.</given-names></name>
<name><surname>Vul</surname><given-names>E.</given-names></name>
<name><surname>Wixted</surname><given-names>J. T.</given-names></name>
<name><surname>Rohrer</surname><given-names>D.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Distributed practice in verbal recall tasks: A review and quantitative synthesis</article-title>. <source>Psychological bulletin</source>, <volume>132</volume>(<issue>3</issue>), <fpage>354</fpage>–<lpage>380</lpage>.</citation>
</ref>
<ref id="bibr11-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christie</surname><given-names>I. C.</given-names></name>
<name><surname>Friedman</surname><given-names>B. H.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Autonomic specificity of discrete emotion and dimensions of affective space: A multivariate approach</article-title>. <source>International Journal of Psychophysiology</source>, <volume>51</volume>(<issue>2</issue>), <fpage>143</fpage>–<lpage>153</lpage>.</citation>
</ref>
<ref id="bibr12-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1992</year>). <article-title>A power primer</article-title>. <source>Psychological Bulletin</source>, <volume>112</volume>(<issue>1</issue>), <fpage>155</fpage>–<lpage>159</lpage>.</citation>
</ref>
<ref id="bibr13-0305735611430079">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Cowie</surname><given-names>R.</given-names></name>
<name><surname>Douglas-Cowie</surname><given-names>E.</given-names></name>
<name><surname>Savvidou</surname><given-names>S.</given-names></name>
<name><surname>McMahon</surname><given-names>E.</given-names></name>
<name><surname>Sawey</surname><given-names>M.</given-names></name>
<name><surname>Schroeder</surname><given-names>M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>FEELTRACE: An instrument for recording perceived emotion in real time</article-title>. In <person-group person-group-type="editor">
<name><surname>Cowie</surname><given-names>R.</given-names></name>
<name><surname>Douglas-Cowie</surname><given-names>E.</given-names></name>
<name><surname>Schroeder</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <conf-name>Speech and Emotion: Proceedings of the ISCA workshop</conf-name> (pp. <fpage>19</fpage>–<lpage>24</lpage>). <conf-loc>Newcastle, Co. Down, UK</conf-loc>.</citation>
</ref>
<ref id="bibr14-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cuthbert</surname><given-names>B. N.</given-names></name>
<name><surname>Schupp</surname><given-names>H. T.</given-names></name>
<name><surname>Bradley</surname><given-names>M. M.</given-names></name>
<name><surname>Birbaumer</surname><given-names>N.</given-names></name>
<name><surname>Lang</surname><given-names>P. J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Brain potentials in affective picture processing: Covariation with autonomic arousal and affective report</article-title>. <source>Biological Psychology</source>, <volume>52</volume>(<issue>2</issue>), <fpage>95</fpage>–<lpage>111</lpage>.</citation>
</ref>
<ref id="bibr15-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W. V.</given-names></name>
</person-group> (<year>1975</year>). <source>Unmasking the face: A guide to recognizing emotions from facial clues</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Prentice-Hall</publisher-name>.</citation>
</ref>
<ref id="bibr16-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eerola</surname><given-names>T.</given-names></name>
<name><surname>Vuoskoski</surname><given-names>J. K.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A comparison of the discrete and dimensional models of emotion in music</article-title>. <source>Psychology of Music</source>, <volume>39</volume>(<issue>1</issue>), <fpage>18</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr17-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Engle</surname><given-names>R. F.</given-names></name>
</person-group> (<year>2001</year>). <article-title>GARCH 101: The use of ARCH/GARCH model in applied economics</article-title>. <source>Journal of Economic Perspectives</source>, <volume>15</volume>(<issue>4</issue>), <fpage>157</fpage>–<lpage>168</lpage>.</citation>
</ref>
<ref id="bibr18-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Evans</surname><given-names>P.</given-names></name>
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Relationships between expressed and felt emotions in music</article-title>. <source>Musicae Scientiae</source>, <volume>12</volume>(<issue>1</issue>), <fpage>75</fpage>–<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr19-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gabrielsson</surname><given-names>A.</given-names></name>
<name><surname>Lindström</surname><given-names>E.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The influence of musical structure on emotional expression</article-title>. In <person-group person-group-type="editor">
<name><surname>Juslin</surname><given-names>P. N.</given-names></name>
<name><surname>Sloboda</surname><given-names>J. A.</given-names></name>
</person-group> (Eds.), <source>Music and emotion: Theory and research</source> (pp. <fpage>223</fpage>–<lpage>248</lpage>), <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gabrielsson</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Perceived emotion and felt emotion: Same or different?</article-title> <source>Musicae Scientiae</source>, <volume>6</volume>(<issue>1</issue>; Special Issue), <fpage>123</fpage>–<lpage>148</lpage>.</citation>
</ref>
<ref id="bibr21-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Geringer</surname><given-names>J. M.</given-names></name>
<name><surname>Madsen</surname><given-names>C. K.</given-names></name>
<name><surname>Gregory</surname><given-names>D.</given-names></name>
</person-group> (<year>2004</year>). <article-title>A fifteen-year history of the Continuous Response Digital Interface: Issues relating to validity and reliability</article-title>. <source>Bulletin of the Council for Research in Music Education</source>(<issue>160</issue>), <fpage>1</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr22-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gottman</surname><given-names>J. M.</given-names></name>
</person-group> (<year>1981</year>). <source>Time-series analysis: A comprehensive introduction for social scientists</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr23-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grewe</surname><given-names>O.</given-names></name>
<name><surname>Kopiez</surname><given-names>R.</given-names></name>
<name><surname>Altenmüller</surname><given-names>E.</given-names></name>
</person-group> (<year>2009a</year>). <article-title>The chill parameter: Goose bumps and shivers as promising measures in emotion research</article-title>. <source>Music Perception</source>, <volume>27</volume>(<issue>1</issue>), <fpage>61</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr24-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grewe</surname><given-names>O.</given-names></name>
<name><surname>Kopiez</surname><given-names>R.</given-names></name>
<name><surname>Altenmüller</surname><given-names>E.</given-names></name>
</person-group> (<year>2009b</year>). <article-title>Chills as an indicator of individual emotional peaks</article-title>. <source>The Neurosciences and Music III: Disorders and Plasticity</source>, <volume>1169</volume>, <fpage>351</fpage>–<lpage>354</lpage>.</citation>
</ref>
<ref id="bibr25-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grewe</surname><given-names>O.</given-names></name>
<name><surname>Nagel</surname><given-names>F.</given-names></name>
<name><surname>Kopiez</surname><given-names>R.</given-names></name>
<name><surname>Altenmüller</surname><given-names>E.</given-names></name>
</person-group> (<year>2007a</year>). <article-title>Emotions over time: Synchronicity and development of subjective, physiological, and facial affective reactions to music</article-title>. <source>Emotion</source>, <volume>7</volume>(<issue>4</issue>), <fpage>774</fpage>–<lpage>788</lpage>.</citation>
</ref>
<ref id="bibr26-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grewe</surname><given-names>O.</given-names></name>
<name><surname>Nagel</surname><given-names>F.</given-names></name>
<name><surname>Kopiez</surname><given-names>R.</given-names></name>
<name><surname>Altenmüller</surname><given-names>E.</given-names></name>
</person-group> (<year>2007b</year>). <article-title>Listening to music as a re-creative process: Physiological, psychological, and psychoacoustical correlates of chills and strong emotions</article-title>. <source>Music Perception</source>, <volume>24</volume>(<issue>3</issue>), <fpage>297</fpage>–<lpage>314</lpage>.</citation>
</ref>
<ref id="bibr27-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hevner</surname><given-names>K.</given-names></name>
</person-group> (<year>1936</year>). <article-title>Experimental studies of the elements of expression in music</article-title>. <source>American Journal of Psychology</source>, <volume>48</volume>, <fpage>246</fpage>–<lpage>268</lpage>.</citation>
</ref>
<ref id="bibr28-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Korhonen</surname><given-names>M. D.</given-names></name>
<name><surname>Clausi</surname><given-names>D. A.</given-names></name>
<name><surname>Jernigan</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Modeling emotional content of music using system identification</article-title>. <source>IEEE Transactions on Systems Man and Cybernetics Part B-Cybernetics</source>, <volume>36</volume>(<issue>3</issue>), <fpage>588</fpage>–<lpage>599</lpage>.</citation>
</ref>
<ref id="bibr29-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Laukka</surname><given-names>P.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Research on vocal expression of emotion: State of the art and future directions</article-title>. <source>Emotions in the Human Voice</source>, <volume>1</volume>, <fpage>153</fpage>–<lpage>169</lpage>.</citation>
</ref>
<ref id="bibr30-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lucas</surname><given-names>B. J.</given-names></name>
<name><surname>Schubert</surname><given-names>E.</given-names></name>
<name><surname>Halpern</surname><given-names>A. R.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Perception of emotion in sounded and imagined music</article-title>. <source>Music Perception</source>, <volume>27</volume>(<issue>5</issue>), <fpage>399</fpage>–<lpage>412</lpage>.</citation>
</ref>
<ref id="bibr31-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McAdams</surname><given-names>S.</given-names></name>
<name><surname>Vines</surname><given-names>B. W.</given-names></name>
<name><surname>Vieillard</surname><given-names>S.</given-names></name>
<name><surname>Smith</surname><given-names>B. K.</given-names></name>
<name><surname>Reynolds</surname><given-names>R.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Influences of large-scale form on continuous ratings in response to a contemporary piece in a live concert setting</article-title>. <source>Music Perception</source>, <volume>22</volume>(<issue>2</issue>), <fpage>297</fpage>–<lpage>350</lpage>.</citation>
</ref>
<ref id="bibr32-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nagel</surname><given-names>F.</given-names></name>
<name><surname>Kopiez</surname><given-names>R.</given-names></name>
<name><surname>Grewe</surname><given-names>O.</given-names></name>
<name><surname>Altenmüller</surname><given-names>E.</given-names></name>
</person-group> (<year>2007</year>). <article-title>EMuJoy: Software for continuous measurement of perceived emotions in music</article-title>. <source>Behavior Research Methods</source>, <volume>39</volume>(<issue>2</issue>), <fpage>283</fpage>–<lpage>290</lpage>.</citation>
</ref>
<ref id="bibr33-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Paas</surname><given-names>F.</given-names></name>
<name><surname>Tuovinen</surname><given-names>J. E.</given-names></name>
<name><surname>van Merrienboer</surname><given-names>J. J. G.</given-names></name>
<name><surname>Darabi</surname><given-names>A. A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>A motivational perspective on the relation between mental effort and performance: Optimizing learner involvement in instruction</article-title>. <source>Etr&amp;D-Educational Technology Research and Development</source>, <volume>53</volume>(<issue>3</issue>), <fpage>25</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr34-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pessoa</surname><given-names>L.</given-names></name>
<name><surname>Padmala</surname><given-names>S.</given-names></name>
<name><surname>Kenzer</surname><given-names>A.</given-names></name>
<name><surname>Bauer</surname><given-names>A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Interactions between cognition and emotion during response inhibition</article-title>. <source>Emotion</source>. Advance online publication. doi: <pub-id pub-id-type="doi">10.1037/a0024109</pub-id></citation>
</ref>
<ref id="bibr35-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pittenger</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Affective and perceptual judgments of major and minor musical stimuli</article-title>. <source>Dissertation Abstracts International: Section B: the Sciences, &amp; Engineering</source>, <volume>63</volume>(<issue>7-B</issue>), <fpage>3491</fpage>.</citation>
</ref>
<ref id="bibr36-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ramsay</surname><given-names>J. O.</given-names></name>
<name><surname>Silverman</surname><given-names>B. W.</given-names></name>
</person-group> (<year>2002</year>). <source>Applied functional data analysis: Methods and case studies</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr37-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ritossa</surname><given-names>D. A.</given-names></name>
<name><surname>Rickard</surname><given-names>N. S.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The relative utility of ‘pleasantness and liking’ dimensions in predicting the emotions expressed by music</article-title>. <source>Psychology of Music</source>, <volume>32</volume>(<issue>1</issue>), <fpage>5</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr38-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>J. A.</given-names></name>
</person-group> (<year>1980</year>). <article-title>A circumplex model of affect</article-title>. <source>Journal of Social Psychology</source>, <volume>39</volume>, <fpage>1161</fpage>–<lpage>1178</lpage>.</citation>
</ref>
<ref id="bibr39-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>J. A.</given-names></name>
<name><surname>Weiss</surname><given-names>A.</given-names></name>
<name><surname>Mendelsohn</surname><given-names>G. A.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Affect grid: A single-item scale of pleasure and arousal</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>57</volume>(<issue>3</issue>), <fpage>493</fpage>–<lpage>502</lpage>.</citation>
</ref>
<ref id="bibr40-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Salimpoor</surname><given-names>V.</given-names></name>
<name><surname>Benovoy</surname><given-names>M.</given-names></name>
<name><surname>Longo</surname><given-names>G.</given-names></name>
<name><surname>Cooperstock</surname><given-names>J.</given-names></name>
<name><surname>Zatorre</surname><given-names>R.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The rewarding aspects of music listening are related to degree of emotional arousal</article-title>. <source>PloS one</source>, <volume>4</volume>(<issue>10</issue>), <fpage>29</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr41-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Salkind</surname><given-names>N. J.</given-names></name>
</person-group> (Ed.). (<year>2010</year>). <source>Encyclopedia of research design</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage Publications, Inc</publisher-name>.</citation>
</ref>
<ref id="bibr42-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Measuring emotion continuously: Validity and reliability of the two-dimensional emotion-space</article-title>. <source>Australian Journal of Psychology</source>, <volume>51</volume>(<issue>3</issue>), <fpage>154</fpage>–<lpage>165</lpage>.</citation>
</ref>
<ref id="bibr43-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Continuous measurement of self-report emotional response to music</article-title>. In <person-group person-group-type="editor">
<name><surname>Juslin</surname><given-names>P.N.</given-names></name>
<name><surname>Sloboda</surname><given-names>J. A.</given-names></name>
</person-group> (Eds.), <source>Music and emotion: Theory and research</source> (pp. <fpage>393</fpage>–<lpage>414</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr44-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Correlation analysis of continuous emotional response to music: Correcting for the effects of serial correlation</article-title>. <source>Musicae Scientiae, Spec Issue</source>, <fpage>213</fpage>–<lpage>236</lpage>.</citation>
</ref>
<ref id="bibr45-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Modeling perceived emotion with continuous musical features</article-title>. <source>Music Perception</source>, <volume>21</volume>, <fpage>561</fpage>–<lpage>585</lpage>.</citation>
</ref>
<ref id="bibr46-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2007a</year>). <article-title>Locus of emotion: The effect of task order and age on emotion perceived and emotion felt in response to music</article-title>. <source>Journal of Music Therapy</source>, <volume>44</volume>(<issue>4</issue>), <fpage>344</fpage>–<lpage>368</lpage>.</citation>
</ref>
<ref id="bibr47-0305735611430079">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2007b</year>). <article-title>Real time cognitive response recording</article-title>. In <person-group person-group-type="editor">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
<name><surname>Buckley</surname><given-names>K.</given-names></name>
<name><surname>Eliott</surname><given-names>R.</given-names></name>
<name><surname>Koboroff</surname><given-names>B.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Stevens</surname><given-names>C.</given-names></name>
</person-group> (Eds.), <conf-name>Proceedings of the Inaugural International Conference on Music Communication Science</conf-name> (pp. <fpage>139</fpage>–<lpage>142</lpage>). <conf-loc>Sydney, Australia: ARC Research Network in Human Communication Science (HCSNet), University of Western Sydney</conf-loc>.</citation>
</ref>
<ref id="bibr48-0305735611430079">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2007c</year>). <article-title>When is an event in a time-series significant?</article-title> In <person-group person-group-type="editor">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
<name><surname>Buckley</surname><given-names>K.</given-names></name>
<name><surname>Eliott</surname><given-names>R.</given-names></name>
<name><surname>Koboroff</surname><given-names>B.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Stevens</surname><given-names>C.</given-names></name>
</person-group> (Eds.), <conf-name>Proceedings of the Inaugural International Conference on Music Communication Science</conf-name> (pp. <fpage>135</fpage>–<lpage>138</lpage>). <conf-loc>Sydney, Australia: ARC Research Network in Human Communication Science (HCSNet), University of Western Sydney</conf-loc>.</citation>
</ref>
<ref id="bibr49-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Continuous self-report methods</article-title>. In <person-group person-group-type="editor">
<name><surname>Juslin</surname><given-names>P. N.</given-names></name>
<name><surname>Sloboda</surname><given-names>J. A.</given-names></name>
</person-group> (Eds.), <source>Handbook of music and emotion: Theory, research, applications</source> (pp. <fpage>223</fpage>–<lpage>253</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>OUP</publisher-name>.</citation>
</ref>
<ref id="bibr50-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
<name><surname>Dunsmuir</surname><given-names>W.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Regression modelling continuous data in music psychology</article-title>. In <person-group person-group-type="editor">
<name><surname>Yi</surname><given-names>S. W.</given-names></name>
</person-group> (Ed.), <source>Music, mind, and science</source> (pp. <fpage>298</fpage>–<lpage>352</lpage>). <publisher-loc>Seoul</publisher-loc>: <publisher-name>Seoul National University</publisher-name>.</citation>
</ref>
<ref id="bibr51-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schubert</surname><given-names>E.</given-names></name>
<name><surname>Vincs</surname><given-names>K.</given-names></name>
<name><surname>Stevens</surname><given-names>C. J.</given-names></name>
</person-group> (<year>in press</year>). <article-title>Identifying regions of good agreement among responders in engagement with a piece of live dance</article-title>. <source>Empirical Studies of the Arts</source>.</citation>
</ref>
<ref id="bibr52-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stevens</surname><given-names>C.</given-names></name>
<name><surname>Schubert</surname><given-names>E.</given-names></name>
<name><surname>Haszard Morris</surname><given-names>R.</given-names></name>
<name><surname>Frear</surname><given-names>M.</given-names></name>
<name><surname>Chen</surname><given-names>J.</given-names></name>
<name><surname>Healey</surname><given-names>S.</given-names></name><etal/>
</person-group>. (<year>2009</year>). <article-title>The portable Audience Response Facility (pARF): PDAs that record real-time and instantaneous data during live or recorded performance</article-title>. <source>International Journal of Human-Computer Studies</source>, <volume>67</volume>, <fpage>800</fpage>–<lpage>813</lpage>.</citation>
</ref>
<ref id="bibr53-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sweller</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Cognitive load during problem solving: Effects on learning</article-title>. <source>Cognitive Science</source>, <volume>12</volume>(<issue>2</issue>), <fpage>257</fpage>–<lpage>285</lpage>.</citation>
</ref>
<ref id="bibr54-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sweller</surname><given-names>J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Discussion of ‘Emerging topics in cognitive load research: Using learner and information characteristics in the design of powerful learning environments’</article-title>. <source>Applied Cognitive Psychology</source>, <volume>20</volume>(<issue>3</issue>), <fpage>353</fpage>–<lpage>357</lpage>.</citation>
</ref>
<ref id="bibr55-0305735611430079">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tay</surname><given-names>T. T.</given-names></name>
<name><surname>Mareels</surname><given-names>I.</given-names></name>
<name><surname>Moore</surname><given-names>J. B.</given-names></name>
</person-group> (<year>1998</year>). <source>High performance control</source>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Birkhauser</publisher-name>.</citation>
</ref>
<ref id="bibr56-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Verbruggen</surname><given-names>F.</given-names></name>
<name><surname>Logan</surname><given-names>G. D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Models of response inhibition in the stop-signal and stop-change paradigms</article-title>. <source>Neuroscience, &amp; Biobehavioral Reviews</source>, <volume>33</volume>(<issue>5</issue>), <fpage>647</fpage>–<lpage>661</lpage>.</citation>
</ref>
<ref id="bibr57-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vines</surname><given-names>B. W.</given-names></name>
<name><surname>Krumhansl</surname><given-names>C. L.</given-names></name>
<name><surname>Wanderley</surname><given-names>M. M.</given-names></name>
<name><surname>Levitin</surname><given-names>D. J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Cross-modal interactions in the perception of musical performance</article-title>. <source>Cognition</source>, <volume>101</volume>(<issue>1</issue>), <fpage>80</fpage>–<lpage>113</lpage>.</citation>
</ref>
<ref id="bibr58-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Webster</surname><given-names>G. D.</given-names></name>
<name><surname>Weir</surname><given-names>C. G.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Emotional responses to music: Interactive effects of mode, texture, and tempo</article-title>. <source>Motivation and Emotion</source>, <volume>29</volume>(<issue>1</issue>), <fpage>19</fpage>–<lpage>39</lpage>.</citation>
</ref>
<ref id="bibr59-0305735611430079">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zentner</surname><given-names>M.</given-names></name>
<name><surname>Grandjean</surname><given-names>D.</given-names></name>
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Emotions evoked by the sound of music: Characterization, classification, and measurement</article-title>. <source>Emotion</source>, <volume>8</volume>(<issue>4</issue>), <fpage>494</fpage>–<lpage>521</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>