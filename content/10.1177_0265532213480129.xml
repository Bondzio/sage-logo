<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LTJ</journal-id>
<journal-id journal-id-type="hwp">spltj</journal-id>
<journal-title>Language Testing</journal-title>
<issn pub-type="ppub">0265-5322</issn>
<issn pub-type="epub">1477-0946</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0265532213480129</article-id>
<article-id pub-id-type="publisher-id">10.1177_0265532213480129</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The essentials of assessment literacy: Contrasts between testers and users</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Inbar-Lourie</surname><given-names>Ofra</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Malone</surname><given-names>Margaret E</given-names></name>
</contrib>
<aff id="aff1-0265532213480129">Center for Applied Linguistics, USA</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0265532213480129">Margaret E Malone, Center for Applied Linguistics, Associate Vice President World Languages and International Programs, 4646 40th Street NW, Washington, DC 20016-1859, USA. Email: <email>mmalone@cal.org</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>3</issue>
<issue-title>Special Issue on Language Assessment Literacy</issue-title>
<fpage>329</fpage>
<lpage>344</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Language assessment literacy refers to language instructors’ familiarity with testing definitions and the application of this knowledge to classroom practices in general and specifically to issues related to assessing language. While it is widely agreed that classroom teachers need to assess student progress, many teachers and other test users have a limited understanding of assessment fundamentals. To help meet this need, a tutorial for foreign language instructors was developed (CAL, 2009) to describe the basics of language assessment and assist with test selection. In this project, group interviews and surveys were used to elicit feedback from two groups of experts, US language instructors (<italic>N</italic> = 44) and language testers (<italic>N</italic> = 30), on the content of the tutorial. The results of the project revealed the challenges of including the technical information considered essential by testers while meeting the real and practical needs of teachers. This paper investigates efforts to elicit language testers’ beliefs about measurement basics compared with those of language educators and suggests that expert beliefs about what is essential to include in such materials differ depending on the expert perspective.</p>
</abstract>
<kwd-group>
<kwd>Assessment literacy</kwd>
<kwd>language assessment literacy</kwd>
<kwd>language teacher development</kwd>
<kwd>tutorials</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>While there is wide acceptance of the need for assessment literacy as a focus for language testing experts, publications on this topic to date have focused on coursework and on defining the essentials of language assessment literacy. The study described in this paper shows the development of an online tutorial for language instructors on the basics of language assessment which was accompanied by feedback from experts from both the language teaching and testing communities. Reconciling the reactions of the two groups has proved both a challenge and an opportunity to balance the tutorial’s relevance for instructors, while mirroring current language testing theory. The project revealed the practical issues involved in transforming complicated language testing issues into understandable, jargon-free language to communicate a clear message to the target audience, foreign language teachers in the United States.</p>
<p>In discussing the structure and development process of the online tutorial, this paper shows how empirical data from stakeholders, that is, language instructors and language testers, can be used to create and improve assessment literacy resources and add to the growing body of information about the knowledge base of assessment literacy. It also provides an example of incorporating evaluative research into the materials development process and the conflicting, often confusing but important data that emerges from such efforts. The research explores the practical issues involved in the process of transforming complicated language testing issues into a usable, jargon-limited tutorial on the basics of assessment and the test selection process for the target audience, foreign language teachers in the United States.</p>
<sec id="section1-0265532213480129">
<title>Background</title>
<sec id="section2-0265532213480129">
<title>Assessment literacy</title>
<p>Language assessment literacy refers to stakeholders’ (often with a focus on instructors’) familiarity with measurement practices and the application of this knowledge to classroom practices in general and specifically to issues of assessing language (<xref ref-type="bibr" rid="bibr13-0265532213480129">Inbar-Lourie, 2008</xref>; <xref ref-type="bibr" rid="bibr30-0265532213480129">Stiggins, 2001</xref>; <xref ref-type="bibr" rid="bibr32-0265532213480129">Taylor, 2009</xref>). Although classroom instructors must assess student progress (<xref ref-type="bibr" rid="bibr23-0265532213480129">NEA, 1983</xref>; <xref ref-type="bibr" rid="bibr28-0265532213480129">Schafer, 1993</xref>), many teachers and other test users have a limited understanding of assessment fundamentals, such as reliability and validity (<xref ref-type="bibr" rid="bibr25-0265532213480129">Popham, 2009</xref>).</p>
<p>Strong, properly implemented assessment provides teachers, students, and all testing stakeholders with important information about student performance and about the extent to which learning objectives have been attained in the classroom. It can and should integrate with teaching, forming a relationship in which assessment informs and improves teaching and vice versa. However, this reciprocal relationship cannot flourish when language teachers do not have sufficient background or training in assessment to develop, select and use tests and interpret test results. An example of how lack of training in assessment can have an impact on how teachers assess their students emerged in a study of EFL instructors in Colombia; <xref ref-type="bibr" rid="bibr20-0265532213480129">López Mendoza and Bernal Arandia (2009)</xref> identify several trends that have arisen due to the paucity of assessment training among such instructors. First, they found that assessments tend to be summative rather than formative and are frequently not used continuously. Second, they found that assessments are frequently used inappropriately and are subject to abuse. In particular, the authors reported that test scores are frequently used in ways for which they were not designed or intended and that do not “facilitate the learning process” (p. 57). The authors attribute the inability of teachers in Colombia to adequately monitor the consequences of their tests to their lack of adequate and appropriate language assessment training. Among the areas that the authors described as lacking in the assessment literacy of Colombian teachers are the following: a general understanding of assessments as compared to tests and formative versus summative tests; knowledge of different types of language assessments and what information each type provides; how to give more effective feedback to students; how to empower students to take charge of their learning; ethical issues related to test and assessment use and how results are used; the role of the language tester; and concepts such as validity, reliability and fairness. In general, the authors reported a major need for language assessment education in Colombia including instruction in development, use, scoring, and analysis and interpretation of assessments. The needs identified by López Mendoza and Bernal Arandia are relevant to the challenges of developing an online tutorial such as the one described in this paper.</p>
<p><xref ref-type="bibr" rid="bibr4-0265532213480129">Boyles (2005)</xref>, focusing on foreign language education in the United States, describes a set of competencies that foreign language teachers need to acquire in order to develop assessment literacy. These competencies include the ability to understand appropriate testing practices, utilize various means of assessment, interpret and analyze assessment results, respond appropriately to the results and their meanings, and use the results in their teaching. <xref ref-type="bibr" rid="bibr4-0265532213480129">Boyles (2005)</xref> also recommends that teacher development be ongoing, take place through both online and face-to-face formats and occur in a variety of contexts, such as conferences, at language resource centers, and at meetings of various associations and organizations, and be instituted as part of teaching preparation programs. While Boyles specifically focuses on foreign language teachers, Boyle’s recommendations are relevant for other teachers as well.</p>
<p>Like <xref ref-type="bibr" rid="bibr4-0265532213480129">Boyles (2005)</xref>, <xref ref-type="bibr" rid="bibr13-0265532213480129">Inbar-Lourie (2008)</xref> suggests establishing a framework of core competencies of language assessment. This framework would incorporate a “body of knowledge and research grounded in theory and epistemological beliefs and connected to other bodies of knowledge in education, linguistics, and applied linguistics” (p. 396). Inbar-Lourie also suggests adopting a set of standards and proficiency levels for the knowledge base that teachers should have within the language testing and assessment field. To date, however, there has been no development of processes or procedures to measure attainment of different levels of language assessment literacy by instructors, nor identification of whose responsibility it should be to develop such standards and processes: instructors or testers.</p>
<p><xref ref-type="bibr" rid="bibr8-0265532213480129">Davies (2008)</xref> describes language assessment literacy as being composed of three basic elements: skills (the how-to or basic testing expertise), knowledge (information about measurement and about language), and principles (concepts underlying testing such as validity, reliability, and ethics). Davies also chronicles developments in the language testing field by presenting a review of textbooks published about language testing and the evolution of their content.</p>
<p><xref ref-type="bibr" rid="bibr8-0265532213480129">Davies (2008)</xref> identifies two basic trends within the teaching of language testing. First, he states that the field of language testing has undergone a process of growing professionalism, which has led to an expansion of available teaching materials. He then demonstrates, through a detailed analysis of these language testing textbooks, that their scope has expanded from covering mostly skills, to skills and knowledge, to skills and knowledge and principles. Building on Davies, <xref ref-type="bibr" rid="bibr32-0265532213480129">Taylor (2009)</xref> argues that a thorough understanding of all three components is essential for good assessment literacy, and encourages greater development of assessment literacy to help professionals most effectively develop and use tests in the future.</p>
<p>It is important to highlight that <xref ref-type="bibr" rid="bibr8-0265532213480129">Davies (2008)</xref> notes a tendency for language testing textbooks to be created from within the field by language testing experts who may lack contact with related fields, such as general education and those who may ultimately select, develop and use assessments. Davies’s caution on professional insularity applies not only to general education audiences but also to the audience for language testing textbooks: current and future language teachers. <xref ref-type="bibr" rid="bibr11-0265532213480129">Fulcher (2012)</xref> has conducted an international survey of language teachers’ training needs. In addition to specific feedback on the perceived strengths of different language testing textbooks, <xref ref-type="bibr" rid="bibr11-0265532213480129">Fulcher (2012)</xref> concluded that language teachers want a clear, practical text that includes relevant activities for language teachers who also fulfill the role of testers. However, language testing textbooks are not the only way that information on assessment can be conveyed; in recent years, language testing textbooks have been supplemented by other ways to convey information about language testing. Such approaches include traditional as well as face-to-face workshops, online or downloadable tutorials, materials produced by professional language testing associations, reference frameworks, such as the Common European Framework of Reference in Europe, video projects, pre-conference workshops, and series of narrative accounts about developing assessment literacy. However, levels of use and effectiveness of these resources for foreign language instructors have not been extensively studied. Because this paper focuses on the development of online tutorial materials that involved both language teachers and testing experts as reviewers, it raises two questions: how effective such materials are in conveying language assessment information and who decides what information should be included in these materials: teachers, testers or a combination of the two groups.</p>
<p>The next section discusses a variety of approaches to promoting assessment literacy with language instructors and other language assessment stakeholders, including some recent efforts by the Center for Applied Linguistics (CAL).</p>
</sec>
<sec id="section3-0265532213480129">
<title>Initiatives to promote assessment literacy</title>
<p>As highlighted in the introduction, a number of leaders in the fields of language testing and teaching (<xref ref-type="bibr" rid="bibr4-0265532213480129">Boyles, 2005</xref>; <xref ref-type="bibr" rid="bibr13-0265532213480129">Inbar-Lourie, 2008</xref>; <xref ref-type="bibr" rid="bibr32-0265532213480129">Taylor, 2009</xref>) recognize the importance of assessment knowledge among language instructors and suggest that such knowledge must be imparted through more than just pre-service teaching efforts. Even when language instructors have received solid preparation about assessment during pre-service training, there is a need to provide on-going professional development to in-service teachers (<xref ref-type="bibr" rid="bibr22-0265532213480129">Malone, 2008</xref>) as the field of language testing is in a constant state of flux. Providing professional development to in-service foreign language instructors is a challenging effort due to constraints of funding, time and geography. Reaching busy teachers can be accomplished, however, via distance learning, blended learning, and self-access approaches in addition to traditional face-to-face professional development sessions.</p>
<p>Such assessment literacy efforts are not targeted for teachers alone. A number of initiatives have supported such general assessment literacy efforts, from providing information and resources about specific tests, to information regarding the principles underlying language testing. <xref ref-type="bibr" rid="bibr32-0265532213480129">Taylor (2009)</xref> points out that, since its unveiling, the Common European Framework of Reference (CEFR) has served as a source of a number projects that promote, sometimes indirectly, assessment literacy. For example, Dialang provides free online assessment and immediate diagnosis to users on their writing, reading, listening, grammar and vocabulary use (<xref ref-type="bibr" rid="bibr1-0265532213480129">Alderson &amp; Huhta, 2005</xref>; Dialang, 2006–2012). While Dialang was not developed to increase assessment literacy, by participating in an online self-assessment, students learn about their own proficiency and their own ability to assess it.</p>
<p>The Centre for Canadian Language Benchmarks also has an online assessment presence, including guidelines for test development, sample assessment tasks and an online self-assessment. For international and general resources on language testing, the Language Testing Resources Website (developed and operationalized by Glenn Fulcher) has been available since 1995. This website includes not only scholarly articles and news releases from the popular press regarding language testing, but also scenarios pertaining to real-life language testing and videos that feature leaders in the field discussing the basic tenets of language testing, such as validity, reliability and impact. The online availability of these resources presents opportunities for users to increase their knowledge of different aspects of language testing, from helping students assess their own proficiency to working with other stakeholders to explain assessment principles, such as the Center for Advanced Research on Language Acquisition’s Virtual Assessment Center at the University of Minnesota.</p>
<p>While online courses and downloadable tutorials can be helpful to teachers in learning about assessment concepts, many language instructors may not be able to enrol in such courses due to their own technological limitations or time constraints. From 2005 to 2009, CAL staff worked on a US Department of Education funded project to develop an online tutorial to introduce key concepts in language testing and to help foreign language educators select tests and use test results effectively. This online tutorial is the focus of the present study. As part of the project, efforts were made to include feedback from two groups of experts: language instructors and language testers.</p>
<p>When developing the tutorial and related materials, the developers consistently wrestled with the challenge of providing accurate and current information about language testing theory and practice in a way that language teachers would understand and be able to apply to their contexts. To give an account of this challenge, the next section of the paper describes the reactions of both foreign language instructors and language testing experts who reviewed the tutorial during different stages of development and compares the issues each group (instructors and testing experts) identified as most important. Thus, the project research question posed was as follows: <italic>What are the similarities and differences in the reactions of the two groups, the foreign language instructors and the language testing experts, to the online testing tutorial?</italic></p>
</sec>
</sec>
<sec id="section4-0265532213480129">
<title>The research context</title>
<p>Testing is a challenge for US foreign language teachers. Foreign language testing is not mandated by <italic>No Child Left Behind</italic>; therefore, most states and districts do not have uniform tests administered to measure progress in foreign languages. As a result, states, school districts and even individual teachers may have broad leverage to select tests for their students. The tutorial includes examples of real-life language teaching challenges in each test section with scenarios that focused on the specific language needs of the target population featured in each scenario and to discuss ways that the concepts being featured in the scenarios reflect the testing practices specific to that population’s needs.</p>
<p>Developed with input from over 150 experts – language instructors, administrators and testers – the online tutorial includes examples and scenarios, downloadable materials, and photographs to illustrate testing concepts. The process of developing the online tutorial proved complex and iterative, as different groups provided information throughout the period of development. The resulting online tutorial, called <italic>Understanding Assessment: A Guide for Foreign Language Educators</italic> (<ext-link ext-link-type="uri" xlink:href="http://www.cal.org/flad/tutorial/">www.cal.org/flad/tutorial/</ext-link>) includes six major sections as well as references. The purpose of the online tutorial is to focus on the basics of language testing and language test selection for US foreign language teachers rather than how to write and develop test tasks and items. <xref ref-type="table" rid="table1-0265532213480129">Table 1</xref> describes each section of the tutorial.</p>
<table-wrap id="table1-0265532213480129" position="float">
<label>Table 1.</label>
<caption>
<p>Online tutorial contents.</p>
</caption>
<graphic alternate-form-of="table1-0265532213480129" xlink:href="10.1177_0265532213480129-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Module name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Introduction</td>
<td>Provides basic overview to tutorial, including format and content</td>
</tr>
<tr>
<td>Reliability</td>
<td>Explains basic concepts of consistency and how it should be considered in selecting foreign language tests for specific language proficiency and developmental levels</td>
</tr>
<tr>
<td>Validity</td>
<td>Outlines purposes for testing and how these purposes can help users decide about a test’s appropriateness for different languages</td>
</tr>
<tr>
<td>Practicality</td>
<td>Discusses testing resources available in different languages, classroom characteristics, and methods of administration/scoring with a focus on the challenges of developing and rating open-ended tasks</td>
</tr>
<tr>
<td>Impact</td>
<td>Considers the effects of tests and focuses on the relationship between assessment and instruction, particularly on the importance of testing all modalities (e.g. speaking)</td>
</tr>
<tr>
<td>Putting it together</td>
<td>Allows the user to consider all aspects of the situation, from available resources to test reliability, in determining which test to use</td>
</tr>
<tr>
<td>Resources</td>
<td>Provides Internet-based resources and recommends specific print resources for future use</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section5-0265532213480129">
<title>Collection of data and participants</title>
<p><xref ref-type="table" rid="table2-0265532213480129">Table 2</xref> shows the timeline for the online tutorial review, including the experts who reviewed it, the number of experts in each category and the method by which reviews were conducted. It shows the timeline and purposes for which data were collected at different points in the online tutorial development for the two sets of informers: language testers and language instructors. It also shows that multiple methods – group interviews, written surveys and online surveys – were used with the groups. The two groups of experts included high school and university language instructors and language testing specialists.</p>
<table-wrap id="table2-0265532213480129" position="float">
<label>Table 2.</label>
<caption>
<p>Timeline for online tutorial review.</p>
</caption>
<graphic alternate-form-of="table2-0265532213480129" xlink:href="10.1177_0265532213480129-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Version of tutorial</th>
<th align="left">Experts</th>
<th align="left"><italic>N</italic></th>
<th align="left">Method</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pre-release to public (changes made after the group interview)</td>
<td>US high school and university language instructors</td>
<td>44</td>
<td>Group interview and written feedback</td>
</tr>
<tr>
<td>Pre-release to public (changes made after the group interview)</td>
<td>US government language testers</td>
<td>17</td>
<td>Group interview<break/>and written feedback</td>
</tr>
<tr>
<td>Final, online version released spring of 2009</td>
<td>US testing specialists (university faculty, not-for-profit test developers and government language testing expert)</td>
<td>13</td>
<td>Online survey</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The reasons for using a combined group interview and paper survey with most participants were mainly practical. In recruiting instructors to participate, the research team found that obtaining the time of language instructors is challenging. Therefore, two-hour group interviews were scheduled after the school day. The government language professionals were similarly constrained and could not be compensated for their time and therefore group interviews were conducted instead.</p>
<p>Group interviews also provide an opportunity for reviewers to discuss and often resolve issues among themselves. The purpose of the group interview was therefore to maximize discussion among participants. However, <xref ref-type="bibr" rid="bibr21-0265532213480129">Mackey and Gass (2005)</xref> point out that one limitation of focus groups, and by extension, group interviews, is that participants’ attitudes toward others in the group may have an impact on what they choose to share. Similarly, <xref ref-type="bibr" rid="bibr19-0265532213480129">Krueger and Casey (2000)</xref> states that participants in focus groups may influence each other and therefore limit what is expressed in the group. Therefore, the protocol also included a short, two-page sheet that allowed for written comments from each participant.</p>
<p>However, although the first group of testing experts which was composed solely of government language testers participated in a group interview, the second group of language testing experts completed a detailed, online survey in response to the completed tutorial instead. This was because the second group was selected for its geographical diversity and it was impractical to bring them together for a group interview. The following section details the participating language instruction experts and their role in the study.</p>
<sec id="section6-0265532213480129">
<title>Language instruction experts</title>
<p>The language instructors’ reviews were held as group interviews with six sessions attended by a total of 44 participants. Two of the sessions included university-level instructors and administrators, and four sessions included K–12 foreign language teachers and administrators. The participants reported varying experiences in language teaching and testing in Arabic, French, German, Italian, Japanese, Russian and Spanish. <xref ref-type="table" rid="table3-0265532213480129">Table 3</xref> details the languages and grade levels that were the focus of the 44 participating instructors.</p>
<table-wrap id="table3-0265532213480129" position="float">
<label>Table 3.</label>
<caption>
<p>Language instructor participants by grade level and language.</p>
</caption>
<graphic alternate-form-of="table3-0265532213480129" xlink:href="10.1177_0265532213480129-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Language of instructors</th>
<th align="left">K–12</th>
<th align="left">University</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arabic</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>French</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>German</td>
<td>2</td>
<td>0</td>
</tr>
<tr>
<td>Japanese</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Russian</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Spanish</td>
<td>14</td>
<td>4</td>
</tr>
<tr>
<td>Other<xref ref-type="table-fn" rid="table-fn1-0265532213480129">*</xref></td>
<td>9</td>
<td>2</td>
</tr>
<tr>
<td><italic>Total</italic></td>
<td>31</td>
<td>13</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0265532213480129">
<label>*</label>
<p><italic>Note</italic>: This category includes those who work with multiple languages and those whose specialties are in another field.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The group of participants who worked with K–12 students included elementary, middle and high school teachers (<italic>n</italic> = 25) and administrators (<italic>n</italic> = 6). The university-level educators were graduate teaching assistants (<italic>n</italic> = 5), instructors (<italic>n</italic> = 3), professors (<italic>n</italic> = 3), and graduate student researchers with prior experience in language testing in a school or university setting (<italic>n</italic> = 2). Five participants were males and 39 were female. The groups included both native speakers of English and native speakers of other languages. Three of the groups included individuals from different institutions, though within each of those groups several participants knew one another. Three of the K–12 group sessions comprised a single school district.</p>
<p>Those who expressed interest were asked to complete a screening questionnaire and from this pool participants were selected to participate in the research. The purpose of asking interested persons to complete a questionnaire was to help ensure that the groups were as representative of the target users of the online tutorial as possible. Participants had to work with foreign language learners as teachers or administrators, or have expertise in language testing. The 44 participants formed groups that were diverse in terms of schools, languages, and positions and grade levels represented.</p>
<p>Participants were paid for their contributions to the two-hour interview sessions, which included approximately 45 minutes devoted to the tutorial material and over an hour to reviewing an online directory of tests. Participants also completed the written feedback form. Group interviews were recorded for later analysis.</p>
</sec>
<sec id="section7-0265532213480129">
<title>Language testing experts</title>
<p>A group interview (<italic>N</italic> = 17) and a survey (<italic>N</italic> = 13) were conducted independently with two different groups of language testing experts to elicit reactions of language testing experts to the tutorial described above, about whether it supplied sufficient information about assessment to meet the perceived needs of the target audience of language teachers. The language testing specialists were recruited from two groups: US government language testers and academic and not-for-profit language testers who develop tests for large- and small-scale purposes. The US government language testers, who represented approximately eight US government agencies, have extensive experience in selecting, administering and using foreign language test results. The second group was selected from a wider group that included university professors who regularly contribute to the language testing literature, one government language testing professional and a non-profit language testing expert who develops tests for a variety of purposes, including the US government. This group had from six to 35 years of experience in language testing. While they cannot be said to comprise a representative sample and the sample size is small, their experience and employment in language teaching and testing, including representation from the US government, academia and not-for-profit sectors, does reflect that of many US language testing experts. The composition of each group and the method of eliciting reactions are described below.</p>
</sec>
<sec id="section8-0265532213480129">
<title>Data elicitation process</title>
<sec id="section9-0265532213480129">
<title>Group One</title>
<p>The online tutorial was reviewed via group interview by 17 US government language testing professionals during a monthly meeting held near Washington, DC. This group participated voluntarily and none were compensated for their participation. The group interview format was chosen for efficiency in order to elicit feedback from a large number of language testing professionals as well as to provide an environment in which participants could discuss and debate issues among themselves. Because the Internet was not available for the session, moderators presented the tutorial via a slide projector and screen and elicited feedback on each page during the interview. Participants also provided written feedback via the same two-page, constructed response questionnaire used with the instructors. Both the notes from the group interview and the written feedback were coded and provided data for this group.</p>
</sec>
<sec id="section10-0265532213480129">
<title>Group Two</title>
<p>The major changes to the tutorial occurred during the first phase of review from K–12 and university language instructors. After the US government language testers reviewed the tutorial, the few substantive changes made were to the wording of and addition of terms and definitions in the tutorial. Then, 13 language testing professionals reviewed the online tutorial by completing a 16-page online survey. The second group of the language testing experts represented those with six to 30 years in the field as testing specialists and/or graduate-level instructors in language testing. The survey elicited both open-ended reactions to the content as well as Likert-scale questions.</p>
</sec>
</sec>
</sec>
<sec id="section11-0265532213480129">
<title>Coding and analysis</title>
<p>Both the data from the transcribed interviews and the questionnaires were analyzed using an iterative process (<xref ref-type="bibr" rid="bibr10-0265532213480129">Dörnyei, 2007</xref>). During the multi-stage analytic coding process, three researchers used descriptive and thematic codes that emerged from the data. The coding procedures were adapted from established focus group procedures (<xref ref-type="bibr" rid="bibr18-0265532213480129">Krueger, 1998</xref>) as well as approaches to coding qualitative data (<xref ref-type="bibr" rid="bibr12-0265532213480129">Hesse-Biber &amp; Leavy, 2005</xref>). The first coder reviewed the data and identified the major themes, based on the discussions that took place in the interviews. Next, two additional coders independently reviewed each comment and assigned the theme or themes that best corresponded to the comment. Coder 1 then reviewed the results of Coders 2 and 3 to count the number of times each theme emerged in the data and to review and reconcile any discrepancies between the coders. Reconciliation involved a review of the three coders’ results and discussion among the three coders to reconcile discrepancies. The themes that the interview data were coded for could be subsequently divided into three major categories: (1) definitions of testing and testing concepts, indicating the extent to which the tutorial presented definitions and content consistent with theories related to language testing; (2) uses for tests, or the extent to which the tutorial provided a context and information on those procedures and processes that may have an impact on testing administration and results, including administration and scoring; and (3) presentation and delivery, meaning the appearance of the content of the tutorial (color, font, bolding) and functionality of the technology.</p>
</sec>
<sec id="section12-0265532213480129">
<title>Results</title>
<p>The focus of the project was to examine the extent to which the two groups of experts had converging and diverging reactions to the content and presentation of the tutorial. <xref ref-type="table" rid="table4-0265532213480129">Table 4</xref> demonstrates the results of the two groups in terms of number and percentage of comments by theme.</p>
<table-wrap id="table4-0265532213480129" position="float">
<label>Table 4.</label>
<caption>
<p>Comparison of themes by group.</p>
</caption>
<graphic alternate-form-of="table4-0265532213480129" xlink:href="10.1177_0265532213480129-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Theme</th>
<th align="left">Testing experts</th>
<th align="left">Language instructors</th>
</tr>
</thead>
<tbody>
<tr>
<td>Definitions of testing and testing concepts</td>
<td>127 (36%)</td>
<td>48 (30%)</td>
</tr>
<tr>
<td>Uses for tests</td>
<td><bold>222 (64%)</bold></td>
<td>27 (17%)</td>
</tr>
<tr>
<td>Presentation and delivery of information</td>
<td>0</td>
<td><bold>84 (53%)</bold></td>
</tr>
</tbody>
</table>
</table-wrap>
<p>As <xref ref-type="table" rid="table4-0265532213480129">Table 4</xref> shows, there was a difference in the proportion of comments on each theme that emerged from the testing experts compared to the foreign language instructors. Testing experts were split in their comments on definitions of testing and testing concepts in 36% of cases and uses for tests, 64%, while language instructors commented on all themes with the majority of comments relating to presentation and delivery (53%) and slightly less of a focus than testing experts on testing and definitions of testing (30%). To compare more closely the focus of commentary between the groups, each theme was subdivided into common subthemes that emerged. While it is important to note that the testing experts saw a later version of the tutorial, which had addressed some of the presentation issues raised by language instructors, it is striking that the testing experts made no comments about presentation and delivery of information, even when specifically asked to do so.</p>
<sec id="section13-0265532213480129">
<title>Definitions of testing and testing concepts</title>
<p>Comments on definitions of testing and testing concepts were subdivided into five categories:</p>
<list id="list1-0265532213480129" list-type="order">
<list-item><p>Change definition.</p></list-item>
<list-item><p>Expand or extend definition.</p></list-item>
<list-item><p>Clarify definition. This meant that the respondent did not disagree with or wish to expand the definition, but expressed confusion about how it was expressed.</p></list-item>
<list-item><p>Validity. In addition to general comments about definitions, specific comments about validity were made within both the module focused on validity and within other modules.</p></list-item>
<list-item><p>Reliability. In addition to general comments about definitions, specific comments about reliability were made within both the module focused on reliability and within other modules.</p></list-item>
</list>
<p><xref ref-type="table" rid="table5-0265532213480129">Table 5</xref> shows how the theme of testing definitions and concepts was broken down into the subthemes defined above. Percentages are calculated based on the number of comments within a group on a subtheme of theory related to the total number of comments on theory within that group.</p>
<table-wrap id="table5-0265532213480129" position="float">
<label>Table 5.</label>
<caption>
<p>Subthemes within testing definitions and concepts by group.</p>
</caption>
<graphic alternate-form-of="table5-0265532213480129" xlink:href="10.1177_0265532213480129-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="left">Testing experts (127 statements)</th>
<th align="left">Instructional experts (48 statements)</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="5">Testing definitions and concepts</td>
<td>Change definition</td>
<td>42 (33%)</td>
<td>5 (10%)</td>
</tr>
<tr>
<td>Expand definition</td>
<td>39 (31%)</td>
<td>8 (17%)</td>
</tr>
<tr>
<td>Clarify definition</td>
<td>0</td>
<td>35 (73%)</td>
</tr>
<tr>
<td>Understand validity</td>
<td>22 (17%)</td>
<td>0</td>
</tr>
<tr>
<td>Understand reliability</td>
<td>24 (19%)</td>
<td>0</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>As <xref ref-type="table" rid="table5-0265532213480129">Table 5</xref> shows, there was a difference between the language instructors and testing experts not only in volume of comments but also in the nature of comments regarding testing definitions and concepts overall. With regard to definitions, the reviewers suggested that definitions should be either changed, or expanded or clarified. Feedback that posed a completely distinct definition was coded as suggesting a change in definition. Comments about expansion were those that suggested that information is added to an existing definition. Comments in the ‘clarify’ category often used verbs like “clarify” or “explain further” and did not ask that any content be removed, but rather be re-worded.</p>
<p>As mentioned above, comments about definitions were coded by change (meaning to change the definitions), expand the definition (meaning the definition was accurate but required more information) or clarify (to make the information presented clearer but not by expanding or changing the essence of the message). Language testers made proportionally far more comments than language instructors (33% versus 10%) to the effect that a definition should be changed. Similarly, instructional experts only recommended expanding a definition in 17% of comments, while the testing experts requested that a definition be expanded in 31% of their comments. By contrast, instructional experts asked for clarity in definitions in the vast majority of their comments (73%) while testing experts never requested clarity. Additionally, only the testing experts specifically challenged the need for understanding reliability and validity as related to theory while instructional experts did not comment on this issue, except in terms of how it related to classroom practice.</p>
<p>Language testing experts suggested expanded or revised definitions of many terms. For example, one testing expert wrote, “You might consider supplying the technical terms for each of these types of reliability (e.g. inter-rater reliability, test-retest reliability) in parenthesis or some other unobtrusive manner.” By contrast, instructional experts focused on the time the tutorial should take with comments such as, “It should take five minutes. No more.’, and ‘It should be as short as possible.”</p>
<p>The differences between the frequency and content of comments on definitions from each group show the gap between the beliefs of the two groups of experts. Perhaps unsurprisingly, the testing experts preferred detail and fidelity to concepts, while the instructional experts preferred that the tutorial and its definitions should be as succinct and clear as possible. The feedback about how theory is presented in the tutorial highlights clearly differing views between testing and instructional experts on the most important focus of such an assessment literacy resource. For testing experts, there was a focus on accuracy and detail, while for instructional experts, the focus was on clarity and conciseness.</p>
</sec>
<sec id="section14-0265532213480129">
<title>Use of tests</title>
<p>The theme of how tests should be used yielded a great number of comments from language testers and far fewer from instructors. Comments on test use were subdivided into eight categories:</p>
<list id="list2-0265532213480129" list-type="order">
<list-item><p>Assessment tools’ availability and how to use them.</p></list-item>
<list-item><p>Evaluative instrument purposes and uses.</p></list-item>
<list-item><p>Student performance.</p></list-item>
<list-item><p>Program evaluation and assessment.</p></list-item>
<list-item><p>Program needs analyses.</p></list-item>
<list-item><p>Instrument selection process.</p></list-item>
<list-item><p>Establishing baseline criteria and standards.</p></list-item>
<list-item><p>Interpretation and reliability of score results.</p></list-item>
<list-item><p>Assessment administration process.</p></list-item>
</list>
<p><xref ref-type="table" rid="table6-0265532213480129">Table 6</xref> shows how the theme of test use was broken down into the subthemes listed above. As in the previous section, percentages are calculated based on the number of comments within a group on a subtheme of test use related to the total number of comments on test use within that group.</p>
<table-wrap id="table6-0265532213480129" position="float">
<label>Table 6.</label>
<caption>
<p>Subthemes within test use by group.<xref ref-type="table-fn" rid="table-fn2-0265532213480129">*</xref></p>
</caption>
<graphic alternate-form-of="table6-0265532213480129" xlink:href="10.1177_0265532213480129-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Theme</th>
<th align="left">Subtheme</th>
<th align="left">Testing experts (222 statements)</th>
<th align="left">Language instructors<break/>(27 statements)</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="9">Use of tests</td>
<td>Specific assessment tools</td>
<td>16 (7%)</td>
<td>21 (78%)</td>
</tr>
<tr>
<td>Evaluative instrument purposes and uses</td>
<td>50 (23%)</td>
<td>0</td>
</tr>
<tr>
<td>Student performance</td>
<td>24 (11%)</td>
<td>0</td>
</tr>
<tr>
<td>Program evaluation and assessment</td>
<td>11 (5%)</td>
<td>0</td>
</tr>
<tr>
<td>Program needs analysis</td>
<td>22 (10%)</td>
<td>0</td>
</tr>
<tr>
<td>Instrument selection process</td>
<td>44 (20%)</td>
<td>6 (22%)</td>
</tr>
<tr>
<td>Establishing baseline criteria and standards</td>
<td>7 (3%)</td>
<td>0</td>
</tr>
<tr>
<td>Interpretation and reliability of scores and results</td>
<td>29 (13%)</td>
<td>0</td>
</tr>
<tr>
<td>Assessment administration process</td>
<td>19 (9%)</td>
<td>0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0265532213480129">
<label>*</label>
<p><italic>Note</italic>: Due to rounding, percentages exceed 100.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p><xref ref-type="table" rid="table6-0265532213480129">Table 6</xref> shows that the language testing experts commented on many different aspects of test use, while the instructional experts only made limited comments on the selection of instruments and on using particular assessment tools in the classroom. The testing experts’ reactions referred to how tests are used for evaluating student performance, program evaluation, needs assessments, and establishing baseline criteria and standards for student performance, as well as the issue of how test scores should be interpreted and reliability determined. Instructional experts, by contrast, only commented on using specific tests and on how the tutorial helped users to select a test, which was the purpose of the tutorial. This divide shows that testing experts are focused on the fidelity of test use to its intended purpose, while language instructors are concerned about specific assessment tools and how to select these tools.</p>
</sec>
<sec id="section15-0265532213480129">
<title>Presentation and delivery</title>
<p>The theme of presentation and delivery was addressed with many comments by language instructors, but elicited no comment from testing experts. Such an absence is especially striking in the second group of language testing experts who were specifically asked about presentation of information and delivery on each page of the tutorial. Comments on presentation and delivery were subdivided into two categories:</p>
<list id="list3-0265532213480129" list-type="order">
<list-item><p>General clarity and usefulness of tutorial.</p></list-item>
<list-item><p>Visual presentation/mode of delivery.</p></list-item>
</list>
<p><xref ref-type="table" rid="table7-0265532213480129">Table 7</xref> shows how the language instructors’ comments were spread across the subthemes of presentation and delivery. As above, percentages are calculated based on the number of comments within a group on a subtheme of test use related to the total number of comments on test use within that group.</p>
<table-wrap id="table7-0265532213480129" position="float">
<label>Table 7.</label>
<caption>
<p>Subthemes within presentation and delivery by group.</p>
</caption>
<graphic alternate-form-of="table7-0265532213480129" xlink:href="10.1177_0265532213480129-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">Theme</th>
<th align="left">Testing experts<break/>(0 statements)</th>
<th align="left">Instructional experts<break/>(78 statements)</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="2">Presentation and delivery</td>
<td>General clarity and usefulness of tutorial</td>
<td>0</td>
<td>22 (28%)</td>
</tr>
<tr>
<td>Visual presentation or mode of delivery</td>
<td>0</td>
<td>56 (72%)</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>As <xref ref-type="table" rid="table7-0265532213480129">Table 7</xref> shows, the theme of presentation and delivery revealed great differences in the approaches taken by testing experts and language instructors to reviewing the tutorial. The testing specialists, as already noted, did not comment at all on tutorial presentation and delivery. Language instructors, by contrast, made nearly as many comments on usefulness or lack of usefulness of the information provided (22) as they did on test use overall (27). Furthermore, language instructors commented on the visual presentation such as color, font, and bolding 56 times while the testing experts made no comments on these features. This is especially striking in the case of the 13 testing experts who completed the 16-page survey, which specifically asked about aesthetics of the tutorial.</p>
<p>The data from both groups indicate that the concept of how to make the tutorial easy to use differed between the two audiences. The language instructors focused on delivery in their open-ended comments, addressing the tutorial, including definitions that were clear and useful, and could be applied to classroom settings. In response to a question about ease of navigation on a screen, one language testing expert wrote, “Re content: overall, this section needs to be re-thought to better reflect the post-Messick modern paradigm of validity and validation.” This quotation suggests that even when prompted specifically about format, the language testing experts returned to the theme of testing concepts.</p>
</sec>
</sec>
<sec id="section16-0265532213480129">
<title>Discussion and conclusions</title>
<p>There has been recent recognition within the language testing field of the need to develop assessment literacy resources for instructors available outside of pre-service coursework. For such efforts to be effective, resources must include accurate information about language assessment conveyed in a way that resonates with the audience, language instructors. This paper has demonstrated how language assessment and language teaching experts agreed and differed in their reactions to a tutorial on the basics of language assessment.</p>
<p>The analysis of responses from the two groups showed that language testing experts tended to focus on fidelity of definitions and aspects of appropriate test use, while the language teaching experts focused on ease of use with regard to presentation and delivery, as well as clarity of definition. While these differences are hardly surprising, they do highlight a conflict: how can resource developers combine fidelity of definition with succinctness, particularly given the often nuanced and technical nature of language testing definitions? In developing such resources, how can precision be balanced with clarity? Who should review such resources and who should determine how much technicality is sufficient for language instructors?</p>
<p>The implications of this study, then, bring to mind earlier recommendations for assessment literacy resources. <xref ref-type="bibr" rid="bibr13-0265532213480129">Inbar-Lourie (2008)</xref> has recommended the development of a set of standards and proficiency levels to measure educators’ language assessment literacy; <xref ref-type="bibr" rid="bibr32-0265532213480129">Taylor (2009)</xref> points out that such levels will have to adhere to share ideas about the core knowledge, skills and principles of language assessment. As <xref ref-type="bibr" rid="bibr17-0265532213480129">Kern (2000)</xref> points out, such levels of language assessment literacy must take into account the educational culture in which they are to be implemented; in other words, how much language assessment literacy is enough? Like this paper, any such effort would require iterative review and feedback from language testing and teaching experts. While language assessment literacy is an important goal, for such efforts to succeed, they must meet the needs and expectations of the target audience, language instructors.</p>
<p>Perhaps the most interesting aspect of the project presented in this paper was the opportunity to examine, compare and contrast the feedback elicited from the two groups. In promoting assessment literacy, materials developers must be mindful that what interests language testing experts may not be of interest to language instructors.</p>
<p>An additional lesson from this study is that it is feasible to embed evaluative research within a focused materials development process. All reviews conducted for the online tutorial were within the budget and scope of the project and helped to improve the quality of the final product. In addition, the reviews provide interesting information on what the two kinds of reviewers, language instructors and language assessment experts, focus on when asked to review the same content. Such information, in turn, provides some insight into what should be considered when developing such assessment literacy materials. As the existing online tutorial is updated and new materials are created, feedback from the second group of language testers will be incorporated. While the tutorial itself has not yet been updated due to funding restrictions, the specific and global reflections contributed by both groups will serve as a basis for both future revisions and for designing future assessment literacy projects.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Alderson</surname><given-names>J. C.</given-names></name>
<name><surname>Huhta</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The development of a suite of computer-based diagnostic tests based on the Common European Framework</article-title>. <source>Language Testing</source>. <volume>22</volume>(<issue>3</issue>), <fpage>301</fpage>–<lpage>320</lpage>.</citation>
</ref>
<ref id="bibr2-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bachman</surname><given-names>L. F.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Building and supporting a case for test use</article-title>. <source>Language Assessment Quarterly</source>, <volume>2</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr3-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Borg</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>English language teachers’ conceptions of research</article-title>. <source>Applied Linguistics</source>, <volume>30</volume>(<issue>3</issue>), <fpage>358</fpage>–<lpage>388</lpage>.</citation>
</ref>
<ref id="bibr4-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Boyles</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Assessment literacy</article-title>. In <person-group person-group-type="editor">
<name><surname>Rosenbusch</surname><given-names>M.</given-names></name>
</person-group> (Ed.), <source>National assessment summit papers</source> (pp. <fpage>11</fpage>–<lpage>15</lpage>). <publisher-loc>Ames, IA</publisher-loc>: <publisher-name>Iowa State University</publisher-name>.</citation>
</ref>
<ref id="bibr5-0265532213480129">
<citation citation-type="web">
<collab>CAL</collab>, <ext-link ext-link-type="uri" xlink:href="http://www.cal.org/CALWebDB/FLAD/">http://www.cal.org/CALWebDB/FLAD/</ext-link>, <access-date>accessed February 28, 2013</access-date>.</citation>
</ref>
<ref id="bibr6-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cavella</surname><given-names>C.</given-names></name>
<name><surname>Malone</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Teaching principles of assessment online</article-title>. In <person-group person-group-type="editor">
<name><surname>Goertler</surname><given-names>S.</given-names></name>
<name><surname>Winke</surname><given-names>P.</given-names></name>
</person-group> (Eds.), <source>Opening doors through distance language education: Principles, perspectives, and practices</source>. CALICO Monograph 7.</citation>
</ref>
<ref id="bibr7-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chapelle</surname><given-names>C.</given-names></name>
<name><surname>Enright</surname><given-names>M.</given-names></name>
<name><surname>Jamieson</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <source>Building a validity argument for the Test of English as a Foreign Language</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr8-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Davies</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Textbook trends in teaching language testing</article-title>. <source>Language Testing</source>, <volume>25</volume>(<issue>3</issue>): <fpage>327</fpage>–<lpage>347</lpage>.</citation>
</ref>
<ref id="bibr9-0265532213480129">
<citation citation-type="web">
<collab>Dialang</collab>, <ext-link ext-link-type="uri" xlink:href="http://www.lancs.ac.uk/researchenterprise/dialang/about.htm">www.lancs.ac.uk/researchenterprise/dialang/about.htm</ext-link>, <access-date>accessed January 9, 2012</access-date>.</citation>
</ref>
<ref id="bibr10-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dörnyei</surname><given-names>Z.</given-names></name>
</person-group> (<year>2007</year>). <source>Research methods in applied linguistics</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fulcher</surname><given-names>G.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Assessment literacy for the language classroom</article-title>. <source>Language Assessment Quarterly</source>, <volume>9</volume>(<issue>2</issue>), <fpage>113</fpage>–<lpage>132</lpage>.</citation>
</ref>
<ref id="bibr12-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hesse-Biber</surname><given-names>S. N.</given-names></name>
<name><surname>Leavy</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <source>The practice of qualitative research</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE Publications</publisher-name>.</citation>
</ref>
<ref id="bibr13-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Inbar-Lourie</surname><given-names>O.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Constructing a language assessment knowledge base: A focus on language assessment courses</article-title>. <source>Language Testing</source>, <volume>25</volume>, <fpage>328</fpage>–<lpage>402</lpage>.</citation>
</ref>
<ref id="bibr14-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jackson</surname><given-names>F.</given-names></name>
<name><surname>Malone</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <source>Building the foreign language capacity we need: Toward a comprehensive strategy for a national language framework</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Center for Applied Linguistics</publisher-name>.</citation>
</ref>
<ref id="bibr15-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jensen</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>National foreign language policy: A state language coordinator’s perspective</article-title>. <source>Modern Language Journal</source>, <volume>91</volume>(<issue>2</issue>), <fpage>261</fpage>–<lpage>264</lpage>.</citation>
</ref>
<ref id="bibr16-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kenyon</surname><given-names>D. M.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Further research on the efficacy of rater self-training</article-title>. In <person-group person-group-type="editor">
<name><surname>Huhta</surname><given-names>A.</given-names></name>
<name><surname>Kohonen</surname><given-names>V.</given-names></name>
<name><surname>Kurki-Suonio</surname><given-names>L.</given-names></name>
<name><surname>Luoma</surname><given-names>S.</given-names></name>
</person-group> (Eds.), <source>Current developments and alternatives in language assessment: Proceedings of LTRC 96</source> (pp. <fpage>257</fpage>–<lpage>273</lpage>). <publisher-loc>Jyväskylä, Finland</publisher-loc>: <publisher-name>University of Jyväskylä</publisher-name>.</citation>
</ref>
<ref id="bibr17-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kern</surname><given-names>R.</given-names></name>
</person-group> (<year>2000</year>). <source>Literacy and language testing</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr18-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Krueger</surname><given-names>R. A.</given-names></name>
</person-group> (<year>1998</year>). <source>Analyzing and reporting focus group results</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE Publications</publisher-name>.</citation>
</ref>
<ref id="bibr19-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Krueger</surname><given-names>R. A.</given-names></name>
<name><surname>Casey</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2000</year>). <source>Focus groups: A practical guide for applied research</source> (<edition>3rd ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE Publications</publisher-name>.</citation>
</ref>
<ref id="bibr20-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>López Mendoza</surname><given-names>A. A.</given-names></name>
<name><surname>Bernal Arandia</surname><given-names>R</given-names></name>
</person-group>. (<year>2009</year>). <article-title>Language testing in Colombia: A call for more teacher education and teacher training in language assessment</article-title>. <source>PROFILE</source>, <volume>11</volume>(<issue>2</issue>), <fpage>55</fpage>–<lpage>70</lpage>.</citation>
</ref>
<ref id="bibr21-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mackey</surname><given-names>A.</given-names></name>
<name><surname>Gass</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>). <source>Second language research: Methodology and design</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr22-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Malone</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Training in language assessment</article-title>. In <person-group person-group-type="editor">
<name><surname>Shohamy</surname><given-names>E.</given-names></name>
<name><surname>Hornberger</surname><given-names>N.</given-names></name>
</person-group> (Eds.), <source>Encyclopedia of language and education, Vol. 7: Language testing and assessment</source> (<edition>2nd ed.</edition>, pp. <fpage>225</fpage>–<lpage>233</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>Springer Science and Business Media</publisher-name>.</citation>
</ref>
<ref id="bibr23-0265532213480129">
<citation citation-type="book">
<collab>National Education Association</collab>. (<year>1983</year>). <source>Teachers’ views about student assessment</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr24-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nier</surname><given-names>V. C.</given-names></name>
<name><surname>Donovan</surname><given-names>A. E.</given-names></name>
<name><surname>Malone</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Increasing assessment literacy among LCTL instructors through blended learning</article-title>. <source>Journal of the National Council of Less Commonly Taught Languages</source>, <volume>9</volume>, <fpage>105</fpage>–<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr25-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Popham</surname><given-names>W. J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Assessment literacy for teachers: Faddish or fundamental?</article-title> <source>Theory into Practice</source>, <volume>48</volume>(<issue>1</issue>), <fpage>4</fpage>–<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr26-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rea-Dickins</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Learning or measuring? Exploring teacher decision making in planning for classroom-based language assessment</article-title>. In <person-group person-group-type="editor">
<name><surname>Fotos</surname><given-names>S.</given-names></name>
<name><surname>Nassaji</surname><given-names>H.</given-names></name>
</person-group> (Eds.), <source>Form-focused instruction and teacher education: Studies in honour of Rod Ellis</source> (pp. <fpage>193</fpage>–<lpage>210</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr27-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Riestenberg</surname><given-names>K.</given-names></name>
<name><surname>Di Silvio</surname><given-names>F.</given-names></name>
<name><surname>Donovan</surname><given-names>A.</given-names></name>
<name><surname>Malone</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Development of a computer based workshop to foster assessment literacy</article-title>. <source>Journal of the Council of Less Commonly Taught Languages</source>, <volume>9</volume>, <fpage>22</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr28-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schafer</surname><given-names>W. D.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Assessment literacy for teachers</article-title>. <source>Theory into Practice</source>, <volume>32</volume>(<issue>2</issue>), <fpage>118</fpage>–<lpage>126</lpage>.</citation>
</ref>
<ref id="bibr29-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shepard</surname><given-names>L. A.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The role of assessment in a learning culture</article-title>. <source>Educational Researcher</source>, <volume>29</volume>(<issue>7</issue>), <fpage>4</fpage>–<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr30-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stiggins</surname><given-names>R. J.</given-names></name>
</person-group> (<year>1997</year>). <source>Student-centered classroom assessment</source>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr31-0265532213480129">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stoynoff</surname><given-names>S.</given-names></name>
<name><surname>Chapelle</surname><given-names>C. A.</given-names></name>
</person-group> (<year>2005</year>). <source>ESOL tests and testing: A resource for teachers and program administrators</source>. <publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>TESOL Publications</publisher-name>.</citation>
</ref>
<ref id="bibr32-0265532213480129">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Developing assessment literacy</article-title>. <source>Annual Review of Applied Linguistics</source>, <volume>29</volume>, <fpage>21</fpage>–<lpage>36</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>