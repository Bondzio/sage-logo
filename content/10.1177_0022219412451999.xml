<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LDX</journal-id>
<journal-id journal-id-type="hwp">spldx</journal-id>
<journal-id journal-id-type="nlm-ta">J Learn Disabil</journal-id>
<journal-title>Journal of Learning Disabilities</journal-title>
<issn pub-type="ppub">0022-2194</issn>
<issn pub-type="epub">1538-4780</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022219412451999</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022219412451999</article-id>
<title-group>
<article-title>Meta-Analysis and Inadequate Responders to Intervention</article-title>
<subtitle>A Response</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Stuebing</surname><given-names>Karla K.</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff1-0022219412451999">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Fletcher</surname><given-names>Jack M.</given-names></name>
<degrees>MD</degrees>
<xref ref-type="aff" rid="aff1-0022219412451999">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Hughes</surname><given-names>Lisa C.</given-names></name>
<degrees>MA</degrees>
<xref ref-type="aff" rid="aff1-0022219412451999">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022219412451999"><label>1</label>University of Houston, Houston, TX, USA</aff>
<author-notes>
<corresp id="corresp1-0022219412451999">Karla K. Stuebing, Texas Institute for Measurement, Evaluation and Statistics, University of Houston, 2151 W. Holcombe, 222 Texas Medical Center Annex, Houston, TX 77204, USA Email: <email>Karla.stuebing@times.uh.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>45</volume>
<issue>6</issue>
<fpage>565</fpage>
<lpage>569</lpage>
<permissions>
<copyright-statement>© Hammill Institute on Disabilities 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Hammill Institute on Disabilities</copyright-holder>
</permissions>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>November/December 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>In a recently published meta-analysis, <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran, Sanchez, Arellano, and Swanson (2011)</xref> synthesized 13 studies that permitted assessment of characteristics of children who were adequate and inadequate responders to instruction. The authors indicated that “[t]he central question addressed in this review is whether individual differences in reading-related skills at pretest predict responders at posttest across a variety of interventions and sets of criteria for determining responding and low responding” (p. 283). Of specific interest was whether the performance on specific individual difference variables at pretest, such as phonological awareness, better predicted posttest performance than other measures (e.g., rapid automatic naming) (p. 284). It is surprising, given the focus on individual differences, <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> operationalized their research questions with a between-groups model by calculating effect size (ES) differences between groups of responders and nonresponders at pretest and posttest and then predicting the posttest ES from the pretest ES. HLM analyses were performed to investigate whether the class of variable (e.g., reading comprehension vs. rapid naming) or methodological variables explained further variance in the posttest effects. Their analyses lead them to conclude,
<disp-quote>
<p>The validity of RTI procedures, particularly in comparison to other assessment approaches, has not been adequately established in the present synthesis of the literature. The current synthesis suggests that RTI procedures do improve performance as reflected in gain scores, however, the differences (achievement gap) between responding and low responding children were maintained across pretest and posttest conditions. (p. 293)</p>
</disp-quote></p>
<p>In terms of individual predictors, they further concluded that “we did not find that phonological awareness was a significant moderator of ES at posttest when all other measures were entered into the hierarchical linear model” (p. 292). <xref ref-type="bibr" rid="bibr1-0022219412451999">Al Otaiba and Fuchs (2002)</xref> and others (e.g., <xref ref-type="bibr" rid="bibr4-0022219412451999">Fletcher et al., 2011</xref>) found that phonological awareness (PA) was a good predictor of inadequate response.</p>
<p>In this article we consider the following questions: (a) Is the analytic framework in <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> using pretest ESs to predict posttest ESs appropriate for their research question? (b) Are the Cohen guidelines for the use and interpretation of ES <italic>d</italic> meaningful in the context of this meta-analysis, that is, should <italic>d</italic> values from studies where groups are formed by dichotomization be compared to the Cohen benchmarks or to <italic>d</italic> values from experimental studies? (c) Have Tran et al. demonstrated that PA is not a good predictor of response? and (d) Does their analysis and interpretation of these results support the conclusion that RTI does not reduce the postintervention achievement gap?</p>
<sec id="section1-0022219412451999">
<title>Is the pre-post effect size analytic framework appropriate for a meta-analysis of intervention response?</title>
<p><xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> used hierarchical linear modeling (HLM) to predict the posttest ES from the pretest ES, finding that “the magnitude of the ESs increased in some cases from pretest to posttest. Thus the data do not support the notion that posttest scores as a function of RTI provide outcomes independent of pretest scores” (p. 292). It is difficult to see why effective interventions in any service delivery model, including RTI, would lead to predictions of ESs that are not larger at posttest than at pretest. In fact, the larger difference between adequate and inadequate responders at posttest seems consistent with typical RTI studies where students are rather homogenous at pretest (i.e., meet criteria for risk) but heterogeneous after intervention, with some responding adequately and others inadequately. By definition, those designated responders will have higher scores than those designated nonresponders on the measures used to define the groups.</p>
<p>The generally larger posttest ESs can also be explained methodologically. <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> estimated ES on a variety of measures and time points. In most (but not all) of the studies in this meta-analysis, the groups were defined by dichotomizing an outcome (not necessarily the planned posttest) used to indicate instructional response. This outcome is correlated to the posttest both because of construct overlap but also because the outcome and posttest were measured close to the same point in time. Because of time-specific measurement error and differential growth, the correlation of the dichotomized outcome with the pretest will be lower than the correlation of the outcome with the posttest. The result of this pattern of intercorrelations is that the ES will be largest on the variable that is dichotomized (the outcome variable), smaller in the posttest, and smaller still in the pretest. For example, in a study where the posttest was performance on a measure of word identification and dichotomization was done on a measure of reading comprehension, the posttest effects will be smaller on word identification than reading comprehension because they are measuring different but correlated constructs at the same point in time. In contrast, we would expect the pretest word identification ESs to be smaller than the posttest effects on the word identification because they differ from the dichotomizing variable in both construct measured and time. Groups that have no overlap on the dichotomized variable will overlap on other variables as a function of the degree of correlation. We would expect total group overlap, equal means, and an ES of 0 for the two groups on a variable with a correlation of 0 with the dichotomized variable.</p>
<p>For some variables in this synthesis, such as rapid automatized naming (RAN) and PA, the average ES is larger at pretest than it is at posttest. This is an unexpected pattern if responders are defined after an intervention. We located the 13 studies included in this synthesis and coded each one for the timing of the group designation. In three of the studies, each of which could contribute multiple ESs to the meta-analysis (<xref ref-type="bibr" rid="bibr2-0022219412451999">Allor, Fuchs, &amp; Mathes, 2001</xref>; <xref ref-type="bibr" rid="bibr5-0022219412451999">Menzies, Mahdavi, &amp; Lewis, 2008</xref>; <xref ref-type="bibr" rid="bibr10-0022219412451999">Ukrainetz, Ross, &amp; Harm, 2009</xref>), the “responder” group designation was made on the basis of pretest variables. <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> specifically cited <xref ref-type="bibr" rid="bibr2-0022219412451999">Allor et al. (2001)</xref> as a study where the responder groups were defined on the basis of the alternating stimulus RAN, a test given at pretest to assess the risk status of the student. In these situations, we would expect a larger ES at pretest and a smaller effect at posttest. The resulting data set is a heterogeneous mix of ESs where, for the most part, groups are formed at posttest and we would expect larger posttest differences. However, in a substantial number of cases, the designation of “responder” was made on pretest variables. In these studies, we would expect larger pretest effects. This latter approach is on its face inconsistent with the notion of response occurring after the intervention and is logically troublesome. What is more difficult is that the analysis of the relation between the pre- and posttest ESs is highly influenced by the mix, or the proportion of the ESs representing each of these two approaches. If there were mostly posttest designations, we would expect a positive intercept in the HLM model. If there were mostly pretest designations, we might expect a negative intercept. The slope in the analysis tells us whether the pretest difference is informative about the posttest difference. When pretest and posttest correlations are fairly homogeneous across all of the measures being considered, this must be the case. Thus, the analysis and its results confirm what we know about the mix of different approaches to designation in the set of studies and about pre- and posttest correlations and tell us little about which individual characteristics in reading-related skills at pretest allow prediction of response in a group of students who are nearly uniformly at risk prior to an intervention. In individual studies, this question is operationalized via the correlates of slope in growth curve analysis, via the correlates of the gain score in two-wave data collections, and by looking at the effect of baseline characteristics on posttest while controlling for pretest in other studies. We believe that a more comprehensible synthesis of these studies would aggregate these within-person effects.</p>
<p>Because the analytic framework used in this synthesis is novel and complicated, it should be accompanied by a stringent mapping of the analysis onto the research question and should possibly also be accompanied by a simulation demonstrating what the effects would be expected to look like in the presence and absence of effective interventions and individual characteristics that predict response (see Note <xref ref-type="fn" rid="fn1-0022219412451999">1</xref>).</p>
</sec>
<sec id="section2-0022219412451999">
<title>Can Cohen’s heuristic interpretation of <italic>d</italic> be used for dichotomized outcomes?</title>
<p>ESs were originally developed to create a common metric across studies that used different measures and scales. ES <italic>d</italic>, which was created to represent the magnitude of the difference between the means of an experimental group and a control group in randomized experiments, assumes that both groups are selected from the same population of individuals and that prior to intervention, these groups have the same population mean and standard deviation (<italic>SD</italic>). With randomization, the expected difference between the two means is 0 in the absence of a treatment effect. To the extent that the treatment separates the groups and reduces group overlap, <italic>d</italic> will deviate from 0. <xref ref-type="bibr" rid="bibr3-0022219412451999">Cohen (1988)</xref> proposed that in the absence of prior data, when designing experiments for adequate power, <italic>d</italic> of 0.2, 0.6, and 0.8 might represent small, medium, and large effects of treatment. <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> used <xref ref-type="bibr" rid="bibr3-0022219412451999">Cohen’s (1988)</xref> heuristic for interpreting the meaning of all pre- and posttest ESs and found many in the large range (&gt; .80). In observational studies where groups are created by dividing a distribution, there is no differential treatment for the groups, so it is difficult to understand what the “treatment” ES represents. Also, the expected difference between the means is not 0. In fact, it can be shown analytically and via simulation that the ES for a variable being dichotomized will be about <italic>d</italic> = 2.5 if the cut point is between the 5th and 95th percentiles. To understand why, consider <xref ref-type="fig" rid="fig1-0022219412451999">Figure 1</xref>. The cut point at the 16th percentile divides the normal distribution into two nonoverlapping groups. The smaller group’s mean is <italic>z</italic> = −1.52, and the larger group’s mean is <italic>z</italic> = 0.29. The <italic>SD</italic>s for these two groups are 0.46 and 0.79, respectively. When these two <italic>SD</italic>s are pooled and the 1.81 difference in <italic>z</italic>s is divided by the pooled <italic>SD</italic>, the ES is 2.4. Smaller values, such as the benchmarks suggested by Cohen, are possible when there is overlap between the two groups, which is what happens when two groups from the same population are created through randomization. This latter scenario is possible also if the variable in question has a very small correlation with the variable being dichotomized but is not possible when dichotomizing a distribution because the means of the observations above and below the cut point are of necessity quite different from each other: Substantial overlap between the two groups is required to move the means closer together. The ES is also larger because the variance within the two groups is substantially reduced through the dichotomization. ESs calculated in this way cannot be compared to the Cohen benchmarks because they are not on the same scale, that is, they do not have the same expected value and do not represent the same concept (the effect of a treatment versus the effect of being dichotomized).</p>
<fig id="fig1-0022219412451999" position="float">
<label>Figure 1.</label>
<caption>
<p>Demonstration of effect of forming groups by dichotomizing a normal distribution</p>
</caption>
<graphic xlink:href="10.1177_0022219412451999-fig1.tif"/></fig>
<p>In addition, and possibly because of conflating the <italic>d</italic>s from the two types of studies, <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> argued that RTI does not close the gap between students at risk and typical students. Tran et al. reported treatment versus control ESs of 0.45 to 0.79 from randomized studies of children with learning disabilities (LD; <xref ref-type="bibr" rid="bibr7-0022219412451999">Swanson &amp; Hoskyn, 1998</xref>; <xref ref-type="bibr" rid="bibr8-0022219412451999">Swanson &amp; Sachse-Lee, 2000</xref>). They then compared these to the ESs representing the difference between children with LD in treatment groups with nondisabled children of the same grade or age. These groups are formed by dividing a continuum not by random assignment. The average ES in this comparison was <italic>d</italic> = 0.97. Tran et al. interpreted this larger average difference to mean that the intervention is not closing the gap. The problem is that they are comparing the average ESs over some set of variables in treatment-control studies where all ESs are expected to be 0 in the absence of a treatment effect with the average ES over some set of variables in studies where groups are formed by dichotomizing. In these latter studies, the effects are expected to be 0 if the variable of interest is uncorrelated with the dichotomizing variable but approximately 2.5 if the correlation is 1 and between 0 and 2.5 for other correlations. The expected size of the effect for any given variable depends on the correlation of that variable with the dichotomizing variable. The average effect overall then depends on the mix of variables included in the average and the strength of their correlations with the dichotomizing variable. If we assume that most variables reported in studies comparing LD and non-LD students are moderately to highly correlated with the dichotomizing variable, it is not surprising to find an average ES of 0.97. It is also not clear that comparing it to the average ES obtained from randomized trials has any meaningful interpretation, including that made by Tran et al., that RTI is not closing the gap.</p>
</sec>
<sec id="section3-0022219412451999">
<title>Is phonological awareness a good predictor of response to intervention?</title>
<p>In a model where posttest ESs were predicted from pretest ESs as well as dummy-coded vectors representing both the category of posttest variable and methodological variables (such as method used to determine intervention response), <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> reported that the beta weight for the PA coded vector was close to 0 and was nonsignificant. Does this mean that PA is not important for reading acquisition?</p>
<p>The issue is how to interpret the slopes of the centered, dummy coded variables in the context of the whole model. Although HLM was used for this analysis to account for the dependencies of multiple effects within the same studies, the issues in interpretation are the same as in any regression analysis. The magnitudes of the slopes of these variables are affected by issues of centering and the number of pre- and posttests within each category, as well as the number of studies where the groups were created at pretest versus posttest, which makes it difficult to precisely interpret the beta weights. However, we can make an attempt to understand the pattern of results by plotting the regression line from the HLM in the space of the mean pre- and posttest ESs (see <xref ref-type="fig" rid="fig2-0022219412451999">Figure 2</xref>). The slope and intercept are taken from the fullest model (<xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al., 2011</xref>, Table 4), which included the PA vector and a PA value for slope. The results from this model indicate that the predicted value of the posttest is 0.55 (intercept) when the pretest is 0. This finding is consistent with there being more studies in the mix where the designation of responder was made on the basis of a posttest variable so that the posttest effect tends to be larger than the pretest effect when the pretest is small. For each gain of 1 on the pretest ES, an increase of 0.26 on the posttest ES over the intercept is expected. This result is consistent with the pretest and posttest measures having positive correlations, which will result in the pre- and posttest ESs being positively related. On average then, and contingent on all of the other predictors in the model holding constant at their means, if the pretest ES = 1.0, our best prediction of the size of the posttest effect would be the intercept plus 0.26 times 1.0, or 0.81.</p>
<fig id="fig2-0022219412451999" position="float">
<label>Figure 2.</label>
<caption>
<p>Mean effect sizes at pre and post time points. Regression line from HLM superimposed. Size of bubble proportional to number of posttest ESs.</p>
</caption>
<graphic xlink:href="10.1177_0022219412451999-fig2.tif"/></fig>
<p>In <xref ref-type="fig" rid="fig2-0022219412451999">Figure 2</xref>, the regression line implied by the HLM model is plotted, and the mean pre- and posttest values from Tables 2 and 3 (<xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al., 2011</xref>) were overlaid on the plot of the regression line to elucidate the meaning of the beta weights presented in the HLM results. Their analysis indicates that pretest accounts for significant variation in the posttest scores but that there is significant remaining variance. The centered dummy variables represent a comparison of each category of posttest ES with all other categories, and they are included in the model to determine if they account for additional variance in the posttest ESs. To illustrate, the vector coded “reading comprehension” compares the posttest ESs on reading comprehension measures with the combined pool of RAN, PA, word identification, word attack, behavior, and so on. What the HLM weight of 0.80 for reading comprehension indicates is that the average posttest effect was larger for this category than would be predicted by the overall pretest-only model. As the plot shows, the negative weight for RAN indicates that its average posttest ES is smaller than would be predicted based on the entire set of pre- and posttest ESs. Categories with positive weights are above the regression line, indicating that their posttest effects were underpredicted by the pretest, and categories with negative weights are below the regression line and were overpredicted by the pretest. For PA, the beta weight is −.04. Thus, the regression line <italic>slightly</italic> overpredicts the average posttest ES for PA, but this difference was not large enough to meet the level of alpha (<italic>p</italic> &lt; .05) specified for the test of significance. In fact, the predicted posttest ES for PA is 0.85 (or 0.55 + 0.26 × 1.15) when the actual posttest mean effect is 0.81. The regression weight of <italic>b</italic> = −0.04 tells us that the predicted value is higher than the observed value by 0.04. It is coincidental that this beta weight exactly equals the difference between the predicted mean value and the observed mean value. In sum, the nonsignificant finding for PA tells us only that its effect is consistent with the overall set of effects and that allowing it a separate regression line with a different intercept (but the same slope) does not add substantially to the explanation of posttest effects. This result and the results from the other dummy codes entered into the model tell us only whether the effects for variable categories differ in level when the pretest is controlled, not which ones are important in predicting response to intervention.</p>
</sec>
<sec id="section4-0022219412451999">
<title>Does this meta-analysis support the conclusion that RTI is not effective?</title>
<p><xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> conclude that “response to intervention (RTI) conditions were not effective at mitigating learner characteristics related to pre-test conditions” (p. 283). The evidence presented in support of this assertion was that pre- and posttest ESs were substantially correlated. They also conclude that “unfortunately, the validity of RTI procedures, particularly in comparisons to other assessment approaches, has not been adequately established in the present synthesis of the literature” (p. 293). However, there was no place where the researchers established what the results should look like if RTI is “valid.” The synthesis of reading interventions by <xref ref-type="bibr" rid="bibr6-0022219412451999">Scammacca et al. (2007)</xref> showed a narrowing of the gap between responders and <italic>typically developing</italic> children. Most RTI studies compare adequate responders with nonresponders, where <italic>by definition</italic> the gap must increase between these groups if they are defined at posttest, unless a ceiling or floor effect is reached. Thus, these methods are not appropriate for determining the effectiveness of interventions based on RTI approaches, which is more appropriately determined via randomized control trials and syntheses of these studies. This strong conclusion strays beyond the data and the research questions, which involved predictors of intervention response. What is not clear is why <xref ref-type="bibr" rid="bibr9-0022219412451999">Tran et al. (2011)</xref> did not conduct a much simpler meta-analysis of the correlations among pretests, posttests, and other individual characteristics measured at baseline. Moreover, it is not clear what advantage is gained by using this complicated ES model when a more simple approach would actually allow investigation of the research questions that seemed to motivate the study, including whether individual differences in pretest predicted response to intervention. There are substantive reasons to expect greater differences in adequate and inadequate responders after intervention than before, and larger posttest effects could easily be the result of the correlation of posttest variables with the variables used to dichotomize students into responders and nonresponders.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="supplementary-material">
<p>Supplementary material for this article is available on the Journal of Learning Disabilities website at <ext-link ext-link-type="uri" xlink:href="http://ldx.sagepub.com/supplemental">http://ldx.sagepub.com/supplemental</ext-link>.</p>
</fn>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research was supported in part by Grant P50 HD052117, Texas Center for Learning Disabilities, from the Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD). The content is solely the responsibility of the authors and does not necessarily represent the official views of the NICHD or the National Institutes of Health.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0022219412451999">
<label>1.</label>
<p>We would like to acknowledge an anonymous reviewer for this suggestion.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Al Otaiba</surname><given-names>S.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Characteristics of children who are unresponsive to early literacy intervention: A review of the literature</article-title>. <source>Remedial and Special Education</source>, <volume>23</volume>, <fpage>300</fpage>–<lpage>316</lpage>.</citation>
</ref>
<ref id="bibr2-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Allor</surname><given-names>J. H.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
<name><surname>Mathes</surname><given-names>P.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Do children with or without lexical retrieval difficulties respond differently to instruction</article-title>. <source>Journal of Learning Disabilities</source>, <volume>34</volume>, <fpage>264</fpage>–<lpage>275</lpage>.</citation>
</ref>
<ref id="bibr3-0022219412451999">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>). <source>Statistical power analysis for the behavioral sciences</source> (<edition>2nd ed.</edition>). <publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fletcher</surname><given-names>J. M.</given-names></name>
<name><surname>Stuebing</surname><given-names>K. K.</given-names></name>
<name><surname>Barth</surname><given-names>A. E.</given-names></name>
<name><surname>Denton</surname><given-names>C. A.</given-names></name>
<name><surname>Cirino</surname><given-names>P. T.</given-names></name>
<name><surname>Francis</surname><given-names>D. J.</given-names></name>
<name><surname>Vaughn</surname><given-names>S.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Cognitive correlates of inadequate response to intervention</article-title>. <source>School Psychology Review</source>, <volume>40</volume>, <fpage>2</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr5-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Menzies</surname><given-names>H. M.</given-names></name>
<name><surname>Mahdavi</surname><given-names>J. N.</given-names></name>
<name><surname>Lewis</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Early intervention in reading: From research to practice</article-title>. <source>Remedial and Special Education</source>, <volume>29</volume>, <fpage>67</fpage>–<lpage>77</lpage>.</citation>
</ref>
<ref id="bibr6-0022219412451999">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Scammacca</surname><given-names>N.</given-names></name>
<name><surname>Roberts</surname><given-names>G.</given-names></name>
<name><surname>Vaughn</surname><given-names>S.</given-names></name>
<name><surname>Edmonds</surname><given-names>M.</given-names></name>
<name><surname>Wexler</surname><given-names>J.</given-names></name>
<name><surname>Reutebuch</surname><given-names>C. K.</given-names></name>
<name><surname>Torgesen</surname><given-names>J. K.</given-names></name>
</person-group> (<year>2007</year>). <source>Reading interventions for adolescent struggling readers: A meta-analysis with implications for practice</source>. <publisher-loc>Portsmouth, NH</publisher-loc>: <publisher-name>RMC Research Corporation, Center on Instruction</publisher-name>.</citation>
</ref>
<ref id="bibr7-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>H. L.</given-names></name>
<name><surname>Hoskyn</surname><given-names>M.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Experimental intervention research on students with learning disabilities: A meta-analysis of treatment outcomes</article-title>. <source>Review of Educational Research</source>, <volume>68</volume>, <fpage>277</fpage>–<lpage>321</lpage>.</citation>
</ref>
<ref id="bibr8-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>H. L.</given-names></name>
<name><surname>Sachse-Lee</surname><given-names>C.</given-names></name>
</person-group> (<year>2000</year>). <article-title>A meta-analysis of single subject design intervention research for students with LD</article-title>. <source>Journal of Learning Disabilities</source>, <volume>33</volume>(<issue>2</issue>), <fpage>114</fpage>–<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr9-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tran</surname><given-names>L.</given-names></name>
<name><surname>Sanchez</surname><given-names>T.</given-names></name>
<name><surname>Arellano</surname><given-names>B.</given-names></name>
<name><surname>Swanson</surname><given-names>H. L.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A meta-analysis of the RTI literature for children at risk for reading disabilities</article-title>. <source>Journal of Learning Disabilities</source>, <volume>44</volume>(<issue>3</issue>), <fpage>283</fpage>–<lpage>295</lpage>.</citation>
</ref>
<ref id="bibr10-0022219412451999">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ukrainetz</surname><given-names>T. A.</given-names></name>
<name><surname>Ross</surname><given-names>C. L.</given-names></name>
<name><surname>Harm</surname><given-names>H. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>An investigation of treatment scheduling for phonemic awareness with kindergartners who are at risk for reading difficulties</article-title>. <source>Language, Speech, and Hearing Services in Schools</source>, <volume>40</volume>, <fpage>86</fpage>–<lpage>100</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>