<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SMX</journal-id>
<journal-id journal-id-type="hwp">spsmx</journal-id>
<journal-title>Sociological Methodology</journal-title>
<issn pub-type="ppub">0081-1750</issn>
<issn pub-type="epub">1467-9531</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0081175012444860</article-id>
<article-id pub-id-type="publisher-id">10.1177_0081175012444860</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Measurement</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Developing and Assessing Intercoder Reliability in Studies of Group Interaction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Compton</surname><given-names>D’Lane</given-names></name>
<xref ref-type="aff" rid="aff1-0081175012444860">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Love</surname><given-names>Tony P.</given-names></name>
<xref ref-type="aff" rid="aff2-0081175012444860">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Sell</surname><given-names>Jane</given-names></name>
<xref ref-type="aff" rid="aff2-0081175012444860">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-0081175012444860"><label>1</label>University of New Orleans, LA</aff>
<aff id="aff2-0081175012444860"><label>2</label>The University of Texas at Arlington</aff>
<aff id="aff3-0081175012444860"><label>3</label>Texas A&amp;M University, College Station, TX</aff>
<author-notes>
<corresp id="corresp1-0081175012444860">D’Lane Compton, Sociology Department, The University of New Orleans, Milneburg Hall 189, 2000 Lakeshore Drive, New Orleans, LA 70148 Email: <email>dcompton@uno.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2012</year>
</pub-date>
<volume>42</volume>
<issue>1</issue>
<fpage>348</fpage>
<lpage>364</lpage>
<permissions>
<copyright-statement>© American Sociological Association 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Sociological Association</copyright-holder>
</permissions>
<abstract>
<p>Although group interaction is a rich source of data, it offers many challenges for analysis. We discuss how pretesting can solve many coding and reliability issues before the actual data collection and analysis. We also consider how coding schemes can be developed, how coders can be trained, and how coder reliability can be ascertained. We examine the properties of different estimates of intercoder reliability and detail how these relate to decisions about the data.</p>
</abstract>
<kwd-group>
<kwd>Reliability</kwd>
<kwd>intercoder reliability</kwd>
<kwd>interrater reliability</kwd>
<kwd>pretest</kwd>
<kwd>research design</kwd>
<kwd>group interaction studies</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0081175012444860" sec-type="intro">
<title>1. Introduction</title>
<p>In this paper, we explore intercoder reliability and provide suggestions for a systematic approach within the context of group interaction. We focus in particular on the critical importance of pretesting and consider how coding schemes can be developed, how coders can be trained, and how reliability can be ascertained. We view our discussions as adding to the pretesting suggestions offered by <xref ref-type="bibr" rid="bibr32-0081175012444860">Rashotte, Webster, and Whitmeyer (2005)</xref>. Although Rashotte and her colleagues emphasize the importance of pretesting for experimental design, and for manipulations in particular, we delineate how pretesting can be used to design a coding schema, train coders, and provide initial estimates of intercoder reliability and power, all of which are also applicable to both experimental and nonexperimental research.</p>
</sec>
<sec id="section2-0081175012444860">
<title>2. Reliability and Its Meaning</title>
<p>In most cases, reliability involves the extent to which any experiment, test, or systematic procedure yields the same results across replicated trials under identical conditions. Mathematically, reliability is defined as the ratio of the variation between the true score and the observed score (see <xref ref-type="bibr" rid="bibr9-0081175012444860">Carmines and Zeller [1979]</xref> for a more detailed mathematical discussion of reliability).<sup><xref ref-type="fn" rid="fn1-0081175012444860">1</xref></sup> The true score by this account cannot be directly observed and measured.<sup><xref ref-type="fn" rid="fn2-0081175012444860">2</xref></sup> By definition, measurement is not error-free, and consequently any two or more measurements of the same “true” variable will never entirely duplicate each other; however, they can have high consistency. According to <xref ref-type="bibr" rid="bibr9-0081175012444860">Carmines and Zeller (1979)</xref>, “this tendency toward consistency found in repeated measurements of the same phenomenon is referred to as reliability” (p. 12). For example, if an experiment is reliable, it yields consistent results across repeated measures, and it is unreliable if repeated measures yield different results. In addition, more consistent results from repeated measurements suggest higher/stronger reliability of the measuring procedure (<xref ref-type="bibr" rid="bibr9-0081175012444860">Carmines and Zeller 1979</xref>).<sup><xref ref-type="fn" rid="fn3-0081175012444860">3</xref></sup></p>
<p>Our general focus is upon evaluating specific components of interaction among participants. The analysis of interaction is most often directed at analyzing the content of discussions (such as topics or rationales) or at the structure of the interaction in terms of the relative amount of certain kinds of statements (such as commands or directives, compliance or agreement). Interaction is often analyzed in experimental settings, in focus groups, and in naturally occurring settings.</p>
<p>There are many theoretical formulations that draw on the analysis of interactions. Some of these concentrate on analyzing the frequency of particular words or phrases and to whom these are directed (see the discussion by <xref ref-type="bibr" rid="bibr35-0081175012444860">Scholand, Tausczik and Pennebaker [2010]</xref> about social language network analysis). Others analyze the reasoning used in groups exhibiting cognitive complexity (<xref ref-type="bibr" rid="bibr39-0081175012444860">Shelly and Shelly 2009</xref>) or the justifications used by jury members for particular decisions (<xref ref-type="bibr" rid="bibr44-0081175012444860">Tanford and Penrod 2006</xref>). When analysis relies exclusively on counts of words or phrases, or on coding of categories that involve little to no judgment by the analyst, it is usually referred to as manifest coding and analysis. Latent analysis involves more judgment on the part of the analysts or coders (see <xref ref-type="bibr" rid="bibr1-0081175012444860">Babbie 2010</xref>:530–31; <xref ref-type="bibr" rid="bibr30-0081175012444860">Potter and Levine-Donnerstein 1999</xref>). Frequently, investigations employ both manifest and latent coding and analysis, and most of our discussion relates to both. <xref ref-type="bibr" rid="bibr30-0081175012444860">Potter and Levine-Donnerstein (1999)</xref> also point out that the role of theory can vary. At times, the theoretical focus may be clear at the beginning of the analysis. Instances also arise in which there may be little or no initial theory, and in such cases, pure description may be the objective. For example, a researcher might be interested in a descriptive issue such as how many times political parties are mentioned in a particular discussion. At other times, the meaning of the interaction may evolve from initial observations, and in this way, coding schemes could form inductively. Regardless of how the coding schemes evolve, the evaluation of the reliability among coders proceeds in the same manner once a coding scheme is developed.</p>
<p>One of the most influential formulations for interaction analysis was developed by Bales and his colleagues in the 1940s and 1950s (see <xref ref-type="bibr" rid="bibr2-0081175012444860">Bales 1950</xref>; <xref ref-type="bibr" rid="bibr5-0081175012444860">Bales et al. 1951</xref>; <xref ref-type="bibr" rid="bibr4-0081175012444860">Bales and Slater 1955</xref>). This work resulted in a conceptual and measurement framework that emerged and was termed Interaction Process Analysis (IPA). In the 1970s this was modified to Systematic Multiple Level Observation of Groups (SYMLOG). The analysis system was based upon determining a “unit” of interaction. Bales’s basic definition of this is similar to a simple sentence or an act providing similar information to a grammatical sentence.</p>
<p>Berger expanded these initial conceptualizations in his dissertation, which formulated how expectations were transmitted into observable behaviors. This formulation became later known as the observable power and prestige order (<xref ref-type="bibr" rid="bibr6-0081175012444860">Berger 2007</xref>). Observable power and prestige is generally conceptualized as possessing four components: action opportunities, action rates, compliance, and influence. A commonly used standardized setting first developed by Berger and his colleagues (<xref ref-type="bibr" rid="bibr6-0081175012444860">Berger 2007</xref>) designed an experimental way to measure influence, a critical component of observable power and prestige that considers whose opinion is dominant when participants disagree.</p>
<p>Many other formulations within expectation states formulations used the general conceptualization of observable power and prestige. For purposes of illustration and because this is a frequently used framework, we will use observable power and prestige conceptualization and measurement in many of our examples. The research studies most relevant to our interest are those considering open interaction settings. Most often these are studies of problem-solving groups whose members confront each other in face-to-face interaction. In such open interaction settings (<xref ref-type="bibr" rid="bibr31-0081175012444860">Propp 1995</xref>; <xref ref-type="bibr" rid="bibr38-0081175012444860">Shelly and Munroe 1999</xref>; <xref ref-type="bibr" rid="bibr14-0081175012444860">Gallagher et al. 2005</xref>), the measurement questions center upon how to identify observable power and prestige components validly and then to achieve reliability in coding. Studies have routinely documented that observable power and prestige are achieved quickly and, unless interventions occur, stabilize quickly (<xref ref-type="bibr" rid="bibr12-0081175012444860">Fisek and Ofshe 1970</xref>; <xref ref-type="bibr" rid="bibr40-0081175012444860">Shelly and Troyer 2001a</xref>, <xref ref-type="bibr" rid="bibr41-0081175012444860">2001b</xref>). While many of our examples will focus on components of observable power and prestige, the general points regarding intercoder reliability apply to coding issues in any interaction context.</p>
<p>In the next section, we address some of the substantive issues that surround the concept of intercoder reliability. We follow this with a discussion of different measurement concerns that arise when coding open interaction, and we then end with suggestions specific to a systematic approach of coding face-to-face interactions.</p>
</sec>
<sec id="section3-0081175012444860">
<title>3. Conceptual Issues Surrounding Reliability</title>
<p>Reliability in coding means that the biases inherent in the observers/researchers are substantially less than the “true variation” of the behavior being coded. In addition, while a common definition for reliability is the consistency in measures over time, for intercoder reliability, it is consistency in the observations between two or more coders in addition to their consistency in coding over time.</p>
<p>Such a definition carries the presumption that there is an outcome or behavior that exists independent from the observer. While this assumption is generally accepted in research traditions such as the experimental worldview (see <xref ref-type="bibr" rid="bibr47-0081175012444860">Webster and Sell 2007</xref>), it is not universally accepted in all research traditions. In particular, postmodern theorists and standpoint theorists might not accept this assumption and could point to many instances in which characteristics of the observer greatly affected what was observed. As an example, <xref ref-type="bibr" rid="bibr20-0081175012444860">Haraway (1989)</xref> details how the same primate group was observed by an American research group and a Japanese research group. The nature of the observations was quite different, with the Japanese observing much more communal activity than the Americans.</p>
<p>Many would argue that such examples point out the importance of measuring reliability: If such different results obtain, it is an indication of observational problems or “biases” that overwhelm the recording and subsequent evaluation of behavior. In fact, that different observers find different results is a clear indication that reliability <italic>must</italic> be measured. Such a conclusion is an acceptance of the postmodern point that biases are carried by observers, but a commitment that such biases can be “pooled” or neutralized so that what is observed can be agreed upon.</p>
<p>This commitment can also clearly apply to varying kinds of research in different fields of study, including experimental research, focus groups, and field studies. In the field of communication research, <xref ref-type="bibr" rid="bibr15-0081175012444860">Gibson (2003</xref>, <xref ref-type="bibr" rid="bibr16-0081175012444860">2005</xref>) utilized corporate managers and managerial meetings to address participation shifts in group conversations. Data collection for this work involved creating a schema that employed two coders coding while the interaction occurred. In the field of law, the Arizona Filming Project involved filming jury deliberations as they naturally occurred and afforded the opportunity to code many different aspects of decision making (<xref ref-type="bibr" rid="bibr11-0081175012444860">Diamond et al. 2003</xref>, <xref ref-type="bibr" rid="bibr10-0081175012444860">2006</xref>). In management team research, there are examinations of control crews as they engage in routine behaviors. As an illustration, <xref ref-type="bibr" rid="bibr46-0081175012444860">Waller, Gupta, and Giambatista (2004)</xref> had two coders record crew behaviors on activity logs while they watched videos of simulations or training scenarios for nuclear power plant control room crews. In the field of medicine, videos of training or simulation exercises for healthcare teams also hold substantial promise for analysis of group interactions (<xref ref-type="bibr" rid="bibr22-0081175012444860">Hunziker et al. 2011</xref>). Such variety in studies of interaction further exhibits the importance of assessing reliability and the necessity of reporting reliability estimates.</p>
</sec>
<sec id="section4-0081175012444860">
<title>4. The Role of Pretesting in Reliability Assessment</title>
<p>Pretests or pilot studies are essential tools that enable researchers to refine their categories; to adjust wording, presentation, and interaction techniques; and to assess contexts and cultures. In experimental studies, they help determine the success of particular experimental manipulations in the creation or instantiation of the important theoretical variables. <xref ref-type="bibr" rid="bibr32-0081175012444860">Rashotte and colleagues (2005)</xref> emphasize the importance of testing the strength of manipulations and ensuring that the information presented to participants is clear. <xref ref-type="bibr" rid="bibr26-0081175012444860">Kuipers and Hysom (2007)</xref> and <xref ref-type="bibr" rid="bibr37-0081175012444860">Shelly (2007)</xref> also discuss the importance of pretesting for ensuring that the experimental conditions capture the theoretical concepts to be tested.</p>
<p>Pretests can also function as the medium through which coders can be trained and intercoder estimates can be obtained. But in this case, the pretests are second-stage devices—pretests that result after the initial manipulation refinements for experimental research or after initial observations in naturally occurring settings or focus groups. It is obvious that assessment of coding must be conducted in a context similar to the interaction focused upon by the study. Development of coding schemes and reliability estimates cannot be accomplished in the context of the actual study, as this would violate the requirement that final estimates of reliability are independently obtained by different measurements, or in the case of coding, by different independent observers. This suggests that there are three phases by which reliability estimates are obtained: (1) conceptual development, (2) coder training and tentative reliability estimates, and (3) final reliability assessment. The first two phases must be conducted with data that are different from those used in the actual, final reliability assessment. To do otherwise would artificially inflate the estimate of reliability because, as we discuss below, initial reliability estimation and coder training involve a great deal of discussion and refinement that clearly violates independence of observation.</p>
</sec>
<sec id="section5-0081175012444860">
<title>5. The First Pretest Phase: Conceptual Development of the Coding Scheme</title>
<p>Conceptually, there is a clear difference between intercoder agreement and intercoder reliability. Intercoder agreement refers to the number (or percentage) of times that more than one coder has the exact same coding or rating of a phenomenon.<sup><xref ref-type="fn" rid="fn4-0081175012444860">4</xref></sup> It is through measuring agreement that we infer reliability<sup><xref ref-type="fn" rid="fn5-0081175012444860">5</xref></sup> (<xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff 2004b</xref>). The first step in acquiring agreement is the development of a clear theory and the subsequent instantiation of that theory. (For a different view that de-emphasizes the use of theory, see <xref ref-type="bibr" rid="bibr30-0081175012444860">Potter and Levine-Donnerstein [1999]</xref>.) In our illustration, we consider observable power and prestige components of action rates, action opportunities, influence, and compliance. As discussed above, the conceptualization in this case is well established.</p>
<p>The next step involves defining how the conceptualization applies to the group interaction in the particular instance. How is power ordinarily expressed in small task groups for whom interaction and consensus are required? Directives are expressions in which the speaker may straightforwardly tell another what to do: “Put the cards in the envelope,” “write this down.” There are, however, many variations of directives based on the culture of the group members and the length of time the group has interacted. For example, a commonly considered task group is one that is newly formed. In such groups, especially groups that are initially not hierarchically formed, directives are often just as firm but are expressed with somewhat modifying language such as “I think this is what we should do” or “could you write this down?” In medical groups, the directives (or “leadership utterances”) might be more scripted for some things such as checking physical status or administering CPR but less scripted for developing diagnoses (<xref ref-type="bibr" rid="bibr22-0081175012444860">Hunziker et al. 2011</xref>). Since different studies may involve different tasks and different groups of subjects whose language will demonstrate cultural norms, coding schemes will need refinement, and sometimes complete overhauls based on the pretest data. (For additional examples of how directives are coded in various contexts, see <xref ref-type="bibr" rid="bibr36-0081175012444860">Sell et al. [2000]</xref> and <xref ref-type="bibr" rid="bibr17-0081175012444860">Goar and Sell [2005]</xref>.)</p>
<p>If there are pretest recordings or if there are transcripts of recordings from field settings, these can be used to help develop or refine codes. The first step in the process is having the researchers (who are designing the study and consequently are informed about the hypotheses) code together. At this stage there can be modifications and discussion about what does and does not seem to fit. We have found, for example, large differences in the kind and amount of directives delivered in groups in which there is and there is not an appointed leader.</p>
<p>When the informed researchers have reached what they believe to be relatively acceptable coding standards, they can obtain initial estimates of reliability as measured, at that point in time, simply as agreement. This is accomplished by first talking through the coding together as they watch the interaction and then independently coding other segments of the interaction. In this first stage, agreement over a relatively small number of group observations should be close to 100%. When the informed researchers obtain high agreement, training for the coders can begin.</p>
</sec>
<sec id="section6-0081175012444860">
<title>6. The Second Pretest Phase: Training Coders</title>
<p>We have ample evidence that when researchers know the hypotheses they are testing, they attend to information differentially (<xref ref-type="bibr" rid="bibr33-0081175012444860">Rosenthal and Rosnow 1984</xref>; <xref ref-type="bibr" rid="bibr34-0081175012444860">Saks et al. 2003</xref>). This phenomenon is a subset of general expectation effects that includes noninteractional effects. The primary problems with such experimenter or expectancy effects are that they create nonrandom errors that can cause results to be biased in favor of the hypotheses. There are a number of ways to avoid such effects, many of which relate to the availability of technology. First, and most important, group interaction should be recorded whenever possible. Video recording is preferred over audio recording even in instances where only verbal interactions are to be coded. The recorded images make determination of who says what and when much easier, and they enable careful replaying of particular phrases or actions, which decreases observer error. Second, coders can remain uninformed about study hypotheses. Obviously if the observer does not know about the hypotheses, he or she cannot bias observations in favor of the hypotheses. This is accomplished by avoiding any discussion of the specific theory being tested.<sup><xref ref-type="fn" rid="fn6-0081175012444860">6</xref></sup> Additionally, as much as possible, the coders should not be aware of the differing conditions or settings that they are coding. There are different ways to accomplish this. For example, group recordings in experimental research can be identified only by a group number, which is linked to their condition. Another way to do this is to have transcribers eliminate reference to manipulations or settings from any transcripts. If this is done, the transcribers cannot also be the coders.</p>
<p>Using pretest tapes or initial study tapes again, the researchers and the coders can begin the second refinement, at first working together and discussing categories and then working independently. This process will usually involve many hours of work. We address how intercoder reliability estimates are obtained below, but it is important to note that in this preliminary stage the rules for reliability should be very strict—in other words, regardless of the measure used, the reliability estimates should be extremely high. This is in part because the cost of starting over is lower in the beginning than when a study is completed.</p>
<p>In a discussion of these issues, <xref ref-type="bibr" rid="bibr23-0081175012444860">King (2004)</xref> mentions that coders should be homogeneous. On this point, we disagree. The objective is to develop coding that is independent of the particular coder, and it would seem that homogeneity of coders might artificially inflate the reliability measures through shared initial biases. Indeed, sometimes researchers might prefer to have heterogeneous coders. As an example, when <xref ref-type="bibr" rid="bibr17-0081175012444860">Goar and Sell (2005)</xref> coded interaction among black and white women, they employed black and white coders to ensure that whatever kinds of coding occurred was not somehow racially biased. Whenever possible then, we suggest that the characteristics of the coders should at least mirror the heterogeneity of the group interactants.</p>
<p>How many independent coders are desirable? If at all possible, at least two should code all the interaction variables. Additional coders are, of course, desirable because more observations provide better estimates. (Reliability measures that explicitly consider the numbers of coders are discussed below.) Coding is a time-intensive process, and for group-level interaction data, it would not be unusual for two coders to spend over 4 hours each for a 20-minute group interaction. Because paying coders can be expensive in such cases, researchers sometimes have one person code all the data and a second person code only a portion. When the coders are not coding all the data together, the higher the proportion of data overlap, the better the reliability estimate.</p>
<p>It is important that power be estimated before the actual data are collected. If there is an established research protocol within a particular area, then estimates from prior studies can be used. However, if there are no prior studies that used similar variables, then pilot studies (or pretests) are useful. Again the important point is that the estimate is prospective rather than retrospective (<xref ref-type="bibr" rid="bibr27-0081175012444860">Lenth 2001</xref>, <xref ref-type="bibr" rid="bibr28-0081175012444860">2011</xref>). Power estimates for studies are almost always based upon the variable for which the lowest number of events is expected.<sup><xref ref-type="fn" rid="fn7-0081175012444860">7</xref></sup> In the pretests, it is possible to estimate which of the interaction variables will be the lowest, and it is this variable that should be used in power calculations as it will provide the most conservative estimate.</p>
</sec>
<sec id="section7-0081175012444860">
<title>7. Determination of the Correct Reliability Coefficient</title>
<sec id="section8-0081175012444860">
<title>7.1. Agreement Versus Reliability</title>
<p>As mentioned earlier, there is a difference between intercoder agreement and reliability. The key difference between these two measures is that, in some instances, chance agreements are highly probable. If agreement has occurred by random chance, reliability calculations are biased upward unless some modification is made.</p>
<p>There are measurement adjustments that can be used in such cases. For example, if there are only four categories of responses and all responses fit into one of those four categories, then the probability that two coders would have the same code would be .25. In such cases, there should be an adjustment for these chance agreements. Many different types of reliability estimates correct for this. (<xref ref-type="bibr" rid="bibr24-0081175012444860">Krippendorff [2004a]</xref> offers such formulas that correct for the chance-alone probabilities.)</p>
<p>Situations arise where such adjustments do not make sense as well, and this relates to the number of categories and the <italic>a priori</italic> probability of a response or behavior belonging to a given category by chance alone. For example, suppose that coders are attempting to code directives, important concepts in interaction because they signal power advantage. Directives are offered more often by those in power and accepted more often by those with lesser power. In any given 20-minute conversation, any phrase could or could not be a directive. The probability that a given statement is a directive is much lower than the probability that it would fit in another category. Other categories, in fact, include the large category of “no code.” Many statements during interaction do not necessarily fit into our categories of interest for purposes of observable power and prestige. For example, it is not unusual in group interactions for group members to engage in reading, reiterating, or even talking to themselves.</p>
<p>How can researchers assess the probability of coding the same statement in the same way by chance alone? It could technically be possible to derive a probability for each given data set by adding up statements and assuming that all statements might have the same random probability of being one of the categories being considered. In interaction settings, however, such an equal probability assumption is not warranted as the category “no coded category” is more probable than any other. So practically speaking, for interactions longer than a few minutes, the probability of agreement for directives by chance alone would be close to zero. In such cases, the number of agreements over the largest number of any given coder is a relatively good indicator of reliability.</p>
<p>This example illustrates that the choice of reliability measures is not really a “cookbook” kind of decision. Measurement coefficients are simply the result of different mathematical calculations. They should be thought of in the context of their formulation, the data they are being applied to, and the patterns that they are seeking to represent. We cannot merely compare various coefficients. Differences in coefficients may be due to differences in data trends or to the fact that they are considering the same data in different ways (<xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff 2004b</xref>).</p>
<p><xref ref-type="bibr" rid="bibr29-0081175012444860">Lombard, Snyder-Duch, and Braken (2002)</xref> contend that some of the problems in reliability analysis stem from the lack of sufficient guidelines and offer some general rules. In response, <xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff (2004b)</xref> criticizes these rules as lacking precision and sophistication and offers his own reliability analysis for studies employing content analysis. This analysis thoroughly explores and compares the various technical details of reliability measures (<xref ref-type="bibr" rid="bibr24-0081175012444860">Krippendorff 2004a</xref>; <xref ref-type="bibr" rid="bibr25-0081175012444860">2004b</xref>). <xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff (2004b)</xref> concludes with four recommendations for future communication research to consider: (1) that “reliability data must be representative of the data whose reliability is in question” and generated by coders who are theoretically “interchangeable”; (2) that a “measure should indicate the likelihood that conclusions drawn from imperfect data are valid beyond chance”; (3) that “reliable data should not significantly deviate from perfect agreement of 1”; and (4) that “all distinctions that matter should be tested for their reliability.”</p>
<p>There is no coefficient that is the standard for all instances in which intercoder reliability must be assessed. However, it is important to choose a coefficient that does not overestimate agreement. One way to be sure of this is to choose a coefficient that does not assume a level of measurement (nominal, ordinal, interval, or ratio) that is higher than that of the data to be analyzed. If the efficacy of a preferred coefficient is in question, its mathematical formulation should be examined to determine the appropriateness of employing the measure for the particular instance in which agreement is to be examined (<xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff 2004b</xref>).</p>
<p>Alternatively, if data are categorical by nature, the easiest approach to determining intercoder reliability is to judge the number of agreements relative to the total number of possible observations. For example, if researchers wanted to gauge reliability regarding the theme of love, they would add up the total number of times love was recorded by each coder. The larger number would then be divided into the smaller number. So, if Coder 1 coded love 100 times and Coder 2 coded love 50 times, the agreement reliability would be .50, an unacceptably low score. In fact, by chance alone in this example, a score of .50 would be obtained if there are only two categories.</p>
<p>Researchers must also determine the suitable level of agreement for assessing reliability among coders. This cutoff point represents the point above which data are considered to be reliable and below which data are to be rejected as unreliable. This point should be chosen with regard to the impact of study results on human life and “the costs of drawing invalid conclusions” (<xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff 2004b</xref>:429). Krippendorff asserts that content analysis studies within the communication literature generally employ α ≥ .80 but may use α ≥ .667 for drawing tentative conclusions and as the lowest credible limit.</p>
<p>It is also important to recognize that rejecting the null hypothesis, that the observed agreement is due to chance, does not ensure that data are reliable. Rather, reliable data should not significantly differ from perfect agreement (<xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff 2004b</xref>). In studies of group interaction, some behaviors are more common than others and thus some categories will yield higher frequencies. For example, when coding a 20-minute task group interaction, many directives may occur, but there will be few (if any) instances of negative behaviors or statements. If Coder 1 codes 1,000 directives and Coder 2 codes 1,200, their intercoder reliability would be above .90 when Coder 2 identifies 200 more directives than Coder 1. Now assume the same coders are coding for negative statements. Coder 1 identifies 1 negative statement and Coder 2 identifies 0 negative statements. In this situation their intercoder reliability would be .50 even though Coder 1 identified only 1 negative statement more than Coder 2. The real issue here is not the difference between instances coded; rather it is that coders do not significantly differ from perfect agreement. So in the case of directives, even though coders may differ by 200 directives, 1,000 is not that significantly different from 1,200. Conversely, 1 negative statement is significantly different from 0 negative statements. A higher frequency of occurrences coded allows for greater difference in agreement among coders before crossing the reliability threshold within a category, than when there are a low number of occurrences coded.</p>
<p>During the practice of coding pretests, it may become evident that some categories will need to be dropped from the analysis due to either low reliability or infrequent use of the category. Generally, we would recommend a minimum of α ≥ .75 for studies of group interaction.</p>
</sec>
<sec id="section9-0081175012444860">
<title>7.2. Measures of Intercoder Reliability</title>
<p>As previously stated, there are many measures of reliability, which yield quite different results when applied to the same data. <xref ref-type="table" rid="table1-0081175012444860">Table 1</xref> illustrates two sets of observations made by two different paired observers.<sup><xref ref-type="fn" rid="fn8-0081175012444860">8</xref></sup> In the given example of directives as a measure of observable power and prestige, let us suppose that independent observers record the number of directives expressed by each member of a triad. Coders 1 and 2 code one group while Coders 3 and 4 code another. For purposes of discussion, we use the two fabricated data sets in <xref ref-type="table" rid="table1-0081175012444860">Table 1</xref> to examine the properties of different kinds of intercoder reliability measures. By visual inspection, it may seem obvious that Scenario 2 (Coders 3 and 4) has less agreement, and in addition shows a systematic bias because Coder 4 is recording consistently fewer instances than Coder 3. Scenario 1 illustrates some disagreement, but the pattern is not consistent; sometimes Coder 1 is higher than Coder 2 and sometimes it is lower. In the simplest form of a measure, we would want to be able to assess (and then rectify) the kind of error represented by Coders 3 and 4; in addition we would want a measure that can express the difference between two such data sets.</p>
<table-wrap id="table1-0081175012444860" position="float">
<label>Table 1.</label>
<caption>
<p>The Results of Several Commonly Used Intercoder Reliability Estimates on Two Instances of Hypothetical Data</p>
</caption>
<graphic alternate-form-of="table1-0081175012444860" xlink:href="10.1177_0081175012444860-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Scenario 1<hr/></th>
<th align="center" colspan="2">Scenario 2<hr/></th>
</tr>
<tr>
<th align="left">Behavior</th>
<th align="center">Coder 1</th>
<th align="center">Coder 2</th>
<th align="center">Coder 3</th>
<th align="center">Coder 4</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>100</td>
<td>90</td>
<td>110</td>
<td>40</td>
</tr>
<tr>
<td>B</td>
<td>73</td>
<td>80</td>
<td>510</td>
<td>390</td>
</tr>
<tr>
<td>C</td>
<td>78</td>
<td>70</td>
<td>910</td>
<td>740</td>
</tr>
<tr>
<td>D</td>
<td>40</td>
<td>45</td>
<td>95</td>
<td>45</td>
</tr>
<tr>
<td>E</td>
<td>46</td>
<td>40</td>
<td>540</td>
<td>290</td>
</tr>
<tr>
<td>F</td>
<td>67</td>
<td>60</td>
<td>77</td>
<td>23</td>
</tr>
<tr>
<td>G</td>
<td>49</td>
<td>53</td>
<td>343</td>
<td>257</td>
</tr>
<tr>
<td>H</td>
<td>52</td>
<td>47</td>
<td>635</td>
<td>490</td>
</tr>
<tr>
<td>I</td>
<td>27</td>
<td>30</td>
<td>67</td>
<td>27</td>
</tr>
<tr>
<td>J</td>
<td>31</td>
<td>27</td>
<td>410</td>
<td>190</td>
</tr>
<tr>
<td><italic>t</italic>-test</td>
<td colspan="2">1.073 (<italic>p</italic> = 0.16)</td>
<td colspan="2">5.147 (<italic>p</italic> = 0.00)</td>
</tr>
<tr>
<td>Intraclass correlation<xref ref-type="table-fn" rid="table-fn1-0081175012444860">*</xref>
</td>
<td colspan="2">0.979</td>
<td colspan="2">0.932</td>
</tr>
<tr>
<td/>
<td colspan="2">0.922–0.995<xref ref-type="table-fn" rid="table-fn2-0081175012444860">**</xref>
</td>
<td colspan="2">–0.010–0.988<xref ref-type="table-fn" rid="table-fn2-0081175012444860">**</xref>
</td>
</tr>
<tr>
<td>Cronbach’s α</td>
<td colspan="2">0.980</td>
<td colspan="2">0.980</td>
</tr>
<tr>
<td>Krippendorff’s α</td>
<td colspan="2">0.956</td>
<td colspan="2">0.678</td>
</tr>
<tr>
<td/>
<td colspan="2">0.947–0.965<xref ref-type="table-fn" rid="table-fn3-0081175012444860">†</xref>
</td>
<td colspan="2">0.496–0.836<xref ref-type="table-fn" rid="table-fn3-0081175012444860">†</xref>
</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0081175012444860">
<label>a</label>
<p>Type A intraclass correlation coefficients (ICC) using an absolute agreement definition.</p>
</fn>
<fn id="table-fn2-0081175012444860">
<label>b</label>
<p>95% confidence interval of ICC.</p>
</fn>
<fn id="table-fn3-0081175012444860">
<label>c</label>
<p>95% confidence interval of Krippendorff’s α.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>One commonly used method of determining whether coders differ significantly from one another is to conduct a <italic>t</italic>-test between the coders’ observations or an ANOVA when employing more than two coders. In this instance, the coder is the independent variable. Normally, researchers would prefer to obtain high <italic>t</italic> or F-statistic values with resulting low alpha probability levels in order to exhibit differences between groups. However, in this case, the ideal result is to find low values of <italic>t</italic> or F-statistics and the resulting high alpha probability. Low values of <italic>t</italic> or F would mean that the between-group sum of squares is small—that is, there is not much variance based on the coder, and it is smaller than the within sum of squares. In other words, there is more variation across observations of behaviors than there is across coders. In our illustration above, the results for the <italic>t</italic>-test are displayed for our two scenarios in <xref ref-type="table" rid="table1-0081175012444860">Table 1</xref>. For the first set of coders, the <italic>t</italic>-statistic is not significant with a <italic>p</italic>-value above .05, indicating that the individual coders do not show significant statistical differences in their observation frequency—the ideal situation. For the second set of coders, the <italic>t</italic> is significant with a <italic>p</italic>-value below .05 indicating that the coder does make a difference. This method has been shown to be a valid measure of reliability (<xref ref-type="bibr" rid="bibr19-0081175012444860">Gwet 2008</xref>; <xref ref-type="bibr" rid="bibr42-0081175012444860">Shrout and Fleiss 1979</xref>), but the primary drawback to this method is clear: If using ANOVA for multiple coders, a statistically significant finding will not indicate which of at least two coders differ from the others. As such, this method is useful as a check for unreliability but not as a reportable measure of reliability.</p>
<p>The intraclass correlation coefficient (ICC) is another way of accounting for a given amount of variance—and it is specifically related to judgments made by humans (<xref ref-type="bibr" rid="bibr21-0081175012444860">Hays 1973</xref>; <xref ref-type="bibr" rid="bibr42-0081175012444860">Shrout and Fleiss 1979</xref>). According to <xref ref-type="bibr" rid="bibr21-0081175012444860">Hays (1973)</xref>, it is used “to express the fact that observations in the same category are related, or tend on the average to be more like each other than observations in different categories” (p. 535).</p>
<p>In the example above, which is focused on coding categories of behaviors, we expect the coded behavior within classes of behavior categories (and across coders) to be more similar relative to the other coded behavior categories. Larger values of the ICC imply more similarity in observations of the same behavior across coders, relative to observations across different behaviors by the same coder. In this way, it is a measure of homogeneity among coders.</p>
<p>In <xref ref-type="table" rid="table1-0081175012444860">Table 1</xref>, we see that applied to our data, the ICC is .979 for the first set of coders and .932 for the second set. Without the first set of coders for comparison, we might assume that the second set of coders is fairly homogeneous regarding their coding of behaviors and that their reliability is high. However, if we look at the confidence intervals, we can see that the range is much tighter for the first set of coders (.922, .995) compared to the second set of coders (–.01, .988). Furthermore, the ICC may increase by the mere fact that there are numerous judges of the data. While this is “sensical” in application to two homogeneous coders, it can be somewhat misleading if coders are more heterogeneous and if researchers present just the single intraclass correlation coefficient.</p>
<p>Another measure that is occasionally reported for coding data is Cronbach’s alpha. It is perhaps the most common measure reported for assessing the internal consistency of questions for the development of a scale or index, but it is clearly inappropriate for intercoder reliability. Cronbach’s alpha yields exactly the same result, a very high alpha of .98 for both sets of coders. This is a dramatic illustration of why such a measure is inappropriate for estimates of intercoder reliability where the goal for the coders is not only to vary together but also to closely approximate agreement.</p>
<p>An alternative to Cronbach’s alpha is Krippendorff’s alpha. As can be seen in <xref ref-type="table" rid="table1-0081175012444860">Table 1</xref>, it clearly distinguishes between the two data sets by avoiding the standardization of observation frequencies and arithmetically distinguishing between agreements and disagreements, even systematic ones. Krippendorff’s alpha yields a confidence interval as a measure of agreement. The 95% confidence interval estimate for intercoder reliability for Coders 1 and 2 is between .94 and .95 while the same interval estimate for the second data set is very large, between .50 and .84. (It should be noted that because Krippendorff’s alpha uses a bootstrapping technique, slightly different results might obtain from the same data.)</p>
</sec>
</sec>
<sec id="section10-0081175012444860">
<title>8. Reporting of Reliability</title>
<p>Assuming that coding has occurred, how is reliability to be reported? Many, perhaps even most, recent discussions concerning reliability appear in the communication literature and relate to content analysis. Even though this is the case, there exist gaps both in reporting and analysis. In a critique of communications research, <xref ref-type="bibr" rid="bibr29-0081175012444860">Lombard et al. (2002)</xref> analyzed different content analysis studies and reported that only 69% of the articles reported reliability measures. In an examination of coding of city and regional plans, <xref ref-type="bibr" rid="bibr7-0081175012444860">Berke and Godschalk (2009)</xref> note that reliability is rarely considered and reported. The lack of reliability reporting inhibits more precise assessment of studies and the accumulation of research.</p>
<p>Sometimes reliability reports are given for an entire data set, and as Krippendorff states this would most likely be inappropriate unless the researchers specifically took the lowest reliability score. In studies of interaction contexts, we find that it is helpful to examine not only the reliability scores for one variable (like directives) but also the reliabilities for various subsets of the variables (like directives for procedures or directives for problem solving). In this way, it is possible to ascertain where the stability or instability in agreement occurs. It is possible at this point in the data analysis to drop certain categories if their reliability is low or to “lump” certain categories together. However, there should be no further attempts to “resolve” disagreement among coders in order to obtain higher estimates of reliability. Krippendorff is especially critical (as are we) about a relatively common practice of resolving disagreements among coders by, in effect, “recoding” by majority decision. This is not a resolution of any problem related to reliability—and such resolutions, if incorporated into reliability estimates, clearly inflate estimates artificially. Although such decision making can play an important role in initial development of coding schemes, it plays no role in the final reliability estimate.</p>
</sec>
<sec id="section11-0081175012444860" sec-type="conclusions">
<title>9. Summary and Conclusions</title>
<p>The use of different types of measures is related to the theory being tested, the number of observations, and the variance involved in those observations. We want to emphasize, in particular, the value of pretests in determining how the theory relates to the specific context, the likely number of observations and variance involved, and the training of coders and estimation of intercoder reliability. In this regard, we join <xref ref-type="bibr" rid="bibr32-0081175012444860">Rashotte et al. 2005</xref> in their discussion of the importance of careful attention to design details before a study of group interaction is conducted.</p>
<p>We also posit some general guidelines for examining the important issue of reliability of coding in interaction context. As with any guidelines, it is important to assess how the coefficients of reliability, as well as the coding structure itself, relate to theoretical progress. However, we do advocate the explicit consideration and discussion of how issues of reliability relate to the interpretation of empirical results for the theories being tested. With such discussion, it becomes easier to judge the appropriateness and adequacy of the concepts under investigation.</p>
</sec>
</body>
<back>
<ack>
<p>This work was supported in part by National Science Foundation Grant 0961940. The authors are listed in alphabetical order; authorship is equal. We presented an earlier version of this paper at the Annual Group Process meetings in San Francisco in 2009. We thank all participants for their comments.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0081175012444860">
<label>1.</label>
<p>This definition is applicable to measurement variables essentially and does not apply to nonmeasurement variables.</p>
</fn>
<fn fn-type="other" id="fn2-0081175012444860">
<label>2.</label>
<p>In order to obtain a true score, we would have to remeasure the variable infinitely. “No single measurement would pinpoint the true score exactly but the average of an infinite number of repeated measurements would be equal to the true score” (Carmine and Zeller 1979:30).</p>
</fn>
<fn fn-type="other" id="fn3-0081175012444860">
<label>3.</label>
<p>It is also important to note that reliability does not assume validity and measures are sample dependent.</p>
</fn>
<fn fn-type="other" id="fn4-0081175012444860">
<label>4.</label>
<p>It is possible that coders could agree by chance alone and researchers should consider the likelihood of this occurring when choosing which reliability measures to draw on. This is further discussed in Section 7 of this paper.</p>
</fn>
<fn fn-type="other" id="fn5-0081175012444860">
<label>5.</label>
<p>This is because reliability is a measure of consensus via agreement. If coders do not agree then the coding schema is defective and should be reconceptualized, or the coders need to be retrained.</p>
</fn>
<fn fn-type="other" id="fn6-0081175012444860">
<label>6.</label>
<p>The negative aspect of this is that coders may feel less committed to the research. To ensure commitment, we think it is important to involve coders in the process and discuss methodology and the importance of adhering to the tenets of science.</p>
</fn>
<fn fn-type="other" id="fn7-0081175012444860">
<label>7.</label>
<p>The variance must, of course, also be considered, and it is used in the power calculation. In practice, it is frequently the case that the variable having the lowest mean is the one used for the most conservative estimate for power.</p>
</fn>
<fn fn-type="other" id="fn8-0081175012444860">
<label>8.</label>
<p>Cohen’s Kappa is excluded because it does not suit count variables in the way they are portrayed in the example data. In addition, it should be restricted to reliability studies in which one pair of coders judge all units of analysis and unequal coder preferences are not problematic (<xref ref-type="bibr" rid="bibr13-0081175012444860">Fleiss 1978</xref>). <xref ref-type="bibr" rid="bibr25-0081175012444860">Krippendorff (2004b)</xref> notes that Cohen’s Kappa fails to recognize that the two coders’ unequal uses of categories could be a reliability problem.</p>
</fn>
</fn-group>
</notes>
<bio>
<title>Bios</title>
<p><bold>D’Lane Compton</bold> is an assistant professor of sociology at the University of New Orleans. Her areas of emphasis are social psychology and gender and sexuality. Her current research investigates the demography of sexual orientation and theoretical properties of stigma development and dissolution.</p>
<p><bold>Tony P. Love</bold> is an assistant professor of criminology at the University of Texas at Arlington. He currently studies general victimization, intimate partner violence, and role-taking behavior and perception.</p>
<p><bold>Jane Sell</bold> is a professor of sociology at Texas A&amp;M University. Along with Murray Webster, Jr., she had edited, Laboratory Experiments in the Social Sciences (Elsevier). She is presently conducting research focusing on two issues: the dynamics of racial inequality in task groups (with Carla Goar) and the processes of cooperation in public goods settings undergoing sharp changes.</p></bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Babbie</surname><given-names>Earl R.</given-names></name>
</person-group> <year>2010</year>. <source>The Practice of Social Research</source>, <edition>12th ed</edition>. <publisher-loc>Belmont, CA</publisher-loc>: <publisher-name>Wadsworth</publisher-name>.</citation>
</ref>
<ref id="bibr2-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bales</surname><given-names>R. F.</given-names></name>
</person-group> <year>1950</year>. “<article-title>A Set of Categories for the Analysis of Small Group Interaction</article-title>.” <source>American Sociological Review</source> <volume>15</volume>:<fpage>257</fpage>–<lpage>63</lpage>.</citation>
</ref>
<ref id="bibr3-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bales</surname><given-names>R. F.</given-names></name>
</person-group> <year>1953</year>. “<article-title>The Equilibrium Problem in Small Groups</article-title>.” Pp. <fpage>111</fpage>–<lpage>61</lpage> in <source>Working Papers on the Theory of Action</source>, edited by <person-group person-group-type="editor">
<name><surname>Parsons</surname><given-names>T.</given-names></name>
<name><surname>Bales</surname><given-names>R.</given-names></name>
<name><surname>Shils</surname><given-names>E. H.</given-names></name>
</person-group> <publisher-loc>Glencoe, IL</publisher-loc>: <publisher-name>Free Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bales</surname><given-names>R. F.</given-names></name>
<name><surname>Slater</surname><given-names>P.</given-names></name>
</person-group> <year>1955</year>. “<article-title>Role Differentiation in Small Decision-Making Groups</article-title>.” Pp. <fpage>259</fpage>–<lpage>306</lpage> in <source>Family, Socialization and Interaction Process</source>, edited by <person-group person-group-type="editor">
<name><surname>Parsons</surname><given-names>T.</given-names></name>
<name><surname>Bales</surname><given-names>R. F.</given-names></name>
</person-group> <publisher-loc>Glencoe, IL</publisher-loc>: <publisher-name>Free Press</publisher-name>.</citation>
</ref>
<ref id="bibr5-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bales</surname><given-names>R. F.</given-names></name>
<name><surname>Strodbeck</surname><given-names>F. L.</given-names></name>
<name><surname>Mills</surname><given-names>T. M.</given-names></name>
<name><surname>Roseborough</surname><given-names>M. E.</given-names></name>
</person-group> <year>1951</year>. “<article-title>Channels of Communication in Small Groups</article-title>.” <source>American Sociological Review</source> <volume>16</volume>:<fpage>461</fpage>–<lpage>68</lpage>.</citation>
</ref>
<ref id="bibr6-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Berger</surname><given-names>Joseph</given-names></name>
</person-group> <year>2007</year>. “<article-title>The Standardized Experimental Situation in Expectation States Research: Notes on History, Uses, and Special Features</article-title>.” Pp. <fpage>333</fpage>–<lpage>78</lpage> in <source>Laboratory Experiments in the Social Sciences</source>, edited by <person-group person-group-type="editor">
<name><surname>Webster</surname><given-names>Murray</given-names><suffix>Jr.</suffix></name>
<name><surname>Sell</surname><given-names>Jane</given-names></name>
</person-group>. <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr7-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Berke</surname><given-names>Philip</given-names></name>
<name><surname>Godschalk</surname><given-names>David</given-names></name>
</person-group>. <year>2009</year>. “<article-title>Searching for the Good Plan: A Meta-Analysis of Plan Quality Studies</article-title>.” <source>Journal of Planning Literature</source> <volume>24</volume>:<fpage>227</fpage>–<lpage>40</lpage>.</citation>
</ref>
<ref id="bibr8-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Burke</surname><given-names>Peter</given-names></name>
</person-group>. <year>1974</year>. “<article-title>Participation and Leadership in Small Groups</article-title>.” <source>American Sociological Review</source> <volume>39</volume>:<fpage>832</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr9-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Carmines</surname><given-names>Edward G.</given-names></name>
<name><surname>Zeller</surname><given-names>Richard A.</given-names></name>
</person-group> <year>1979</year>. <source>Reliability and Validity Assessment</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr10-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Diamond</surname><given-names>Shari Seidman</given-names></name>
<name><surname>Rose</surname><given-names>Mary R.</given-names></name>
<name><surname>Murphy</surname><given-names>Beth</given-names></name>
<name><surname>Smith</surname><given-names>Sven</given-names></name>
</person-group>. <year>2006</year>. “<article-title>Juror Questions during Trial: A Window into Juror Thinking</article-title>.” <source>Vanderbilt Law Review</source> <volume>59</volume>:<fpage>1927</fpage>–<lpage>72</lpage>.</citation>
</ref>
<ref id="bibr11-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Diamond</surname><given-names>Shari S.</given-names></name>
<name><surname>Vidmar</surname><given-names>Neil</given-names></name>
<name><surname>Rose</surname><given-names>Mary</given-names></name>
<name><surname>Ellis</surname><given-names>Leslie</given-names></name>
<name><surname>Murphy</surname><given-names>Beth</given-names></name>
</person-group> <year>2003</year>. “<article-title>Juror Discussions during Civil Trials: Studying an Arizona Innovation</article-title>.” <source>Arizona Law Review</source> <volume>45</volume>:<fpage>1</fpage>–<lpage>82</lpage>.</citation>
</ref>
<ref id="bibr12-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fisek</surname><given-names>M. H.</given-names></name>
<name><surname>Ofshe</surname><given-names>R.</given-names></name>
</person-group> <year>1970</year>. “<article-title>The Process of Status Evolution</article-title>.” <source>Sociometry</source> <volume>33</volume>:<fpage>287</fpage>–<lpage>304</lpage>.</citation>
</ref>
<ref id="bibr13-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fleiss</surname><given-names>J. L.</given-names></name>
</person-group> <year>1978</year>. “<article-title>Reply to Klaus Krippendorff’s ‘Reliability of Binary Attribute Data</article-title>.’” <source>Biometrics</source> <volume>34</volume>:<fpage>144</fpage>.</citation>
</ref>
<ref id="bibr14-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gallagher</surname><given-names>Timothy J.</given-names></name>
<name><surname>Gregory</surname><given-names>Stanford W.</given-names><suffix>Jr.</suffix></name>
<name><surname>Bianchi</surname><given-names>Alison J.</given-names></name>
<name><surname>Hartung</surname><given-names>Paul J.</given-names></name>
<name><surname>Harkness</surname><given-names>Sarah</given-names></name>
</person-group>. <year>2005</year>. “<article-title>Examining Medical Interview Asymmetry Using the Expectations States Approach</article-title>.” <source>Social Psychology Quarterly</source> <volume>68</volume>:<fpage>187</fpage>–<lpage>203</lpage>.</citation>
</ref>
<ref id="bibr15-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gibson</surname><given-names>David R.</given-names></name>
</person-group> <year>2003</year>. “<article-title>Participation Shifts: Order and Differentiation in Group Conversation</article-title>.” <source>Social Forces</source> <volume>81</volume>:<fpage>1335</fpage>–<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr16-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gibson</surname><given-names>David R.</given-names></name>
</person-group> <year>2005</year>. “<article-title>Taking Turns and Talking Ties: Networks and Conversational Interaction</article-title>.” <source>American Journal of Sociology</source> <volume>110</volume>:<fpage>1561</fpage>–<lpage>97</lpage>.</citation>
</ref>
<ref id="bibr17-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goar</surname><given-names>Carla</given-names></name>
<name><surname>Sell</surname><given-names>Jane</given-names></name>
</person-group>. <year>2005</year>. “<article-title>Decreasing Racial Inequality in Task Groups through Task Definition</article-title>.” <source>Sociological Quarterly</source> <volume>46</volume>:<fpage>525</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr18-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guetzkow</surname><given-names>Harold</given-names></name>
</person-group>. <year>1950</year>. “<article-title>Unitizing and Categorizing Problems in Coding Qualitative Data</article-title>.” <source>Journal of Clinical Psychology</source> <volume>6</volume>:<fpage>47</fpage>–<lpage>58</lpage>.</citation>
</ref>
<ref id="bibr19-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gwet</surname><given-names>Kilem Li</given-names></name>
</person-group>. <year>2008</year>. “<article-title>Computing Inter-rater Reliability and Its Variance in the Presence of High Agreement</article-title>.” <source>British Journal of Mathematical and Statistical Psychology</source> <volume>61</volume>:<fpage>29</fpage>–<lpage>48</lpage>.</citation>
</ref>
<ref id="bibr20-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Haraway</surname><given-names>Donna</given-names></name>
</person-group>. <year>1989</year>. <source>Primate Visions: Gender, Race and Nature in the World of Modern Science</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr21-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hays</surname><given-names>William L.</given-names></name>
</person-group> <year>1973</year>. <source>Statistics for the Social Sciences</source>. <edition>2nd ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Holt, Rinehart and Winston</publisher-name>.</citation>
</ref>
<ref id="bibr22-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hunziker</surname><given-names>Sabina</given-names></name>
<name><surname>Johansson</surname><given-names>Anna C.</given-names></name>
<name><surname>Tschan</surname><given-names>Franziska</given-names></name>
<name><surname>Semmer</surname><given-names>Norbert K.</given-names></name>
<name><surname>Rock</surname><given-names>Laura</given-names></name>
<name><surname>Howell</surname><given-names>Michael D.</given-names></name>
<name><surname>Marsch</surname><given-names>Stephan</given-names></name>
</person-group>. <year>2011</year>. “<article-title>Teamwork and Leadership in Cardiopulmonary Resuscitation</article-title>.” <source>Journal of the American College of Cardiology</source> <volume>57</volume>:<fpage>2381</fpage>–<lpage>88</lpage>.</citation>
</ref>
<ref id="bibr23-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>King</surname><given-names>Laura A.</given-names></name>
</person-group> <year>2004</year>. “<article-title>Measures and Meanings: The Use of Qualitative Data in Social and Personality Psychology</article-title>.” Pp.<fpage>173</fpage>–<lpage>94</lpage> in <source>Handbook of Methods in Social Psychology</source>, edited by <person-group person-group-type="editor">
<name><surname>Sansone</surname><given-names>C.</given-names></name>
<name><surname>Morf</surname><given-names>C.</given-names></name>
<name><surname>Panter</surname><given-names>A. T.</given-names></name>
</person-group> <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr24-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Krippendorff</surname><given-names>Klus</given-names></name>
</person-group>. <year>2004a</year>. <source>Content Analysis: An Introduction to Its Methodology</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr25-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krippendorff</surname><given-names>Klus</given-names></name>
</person-group>. <year>2004b</year>. “<article-title>Reliability in Content Analysis</article-title>.” <source>Human Communication Research</source> <volume>30</volume>:<fpage>411</fpage>–<lpage>33</lpage>.</citation>
</ref>
<ref id="bibr26-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kuipers</surname><given-names>Kathy J.</given-names></name>
<name><surname>Hysom</surname><given-names>Stuart</given-names></name>
</person-group>. <year>2007</year>. “<article-title>Common Problems and Solutions.” Pp</article-title>. <fpage>289</fpage>–<lpage>324</lpage> in <source>Laboratory Experiments in the Social Sciences</source>, edited by <person-group person-group-type="editor">
<name><surname>Webster</surname><given-names>Murray</given-names><suffix>Jr.</suffix></name>
<name><surname>Sell</surname><given-names>Jane</given-names></name>
</person-group>. <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr27-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lenth</surname><given-names>Russell V.</given-names></name>
</person-group> <year>2001</year>. “<article-title>Some Practical Guidelines for Effective Sample Size Determination</article-title>.” <source>American Statistician</source> <volume>55</volume>:<fpage>187</fpage>–<lpage>93</lpage>.</citation>
</ref>
<ref id="bibr28-0081175012444860">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lenth</surname><given-names>Russell V.</given-names></name>
</person-group> <year>2011</year>. <access-date>Retrieved July 16, 2011</access-date> (<ext-link ext-link-type="uri" xlink:href="http://www.cs.uiowa.edu/~rlenth/Power/">http://www.cs.uiowa.edu/~rlenth/Power/</ext-link>).</citation>
</ref>
<ref id="bibr29-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lombard</surname><given-names>M.</given-names></name>
<name><surname>Snyder-Duch</surname><given-names>J.</given-names></name>
<name><surname>Braken</surname><given-names>C. C.</given-names></name>
</person-group> <year>2002</year>. “<article-title>Content Analysis in Mass Communication Research: An Assessment and Reporting of Intercoder Reliability</article-title>.” <source>Human Communication Research</source> <volume>28</volume>:<fpage>587</fpage>–<lpage>472</lpage>.</citation>
</ref>
<ref id="bibr30-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Potter</surname><given-names>W. James</given-names></name>
<name><surname>Levine-Donnerstein</surname><given-names>Deborah</given-names></name>
</person-group>. <year>1999</year>. “<article-title>Rethinking Validity and Reliability in Content Analysis</article-title>.” <source>Journal of Applied Communications Research</source> <volume>27</volume>:<fpage>258</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr31-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Propp</surname><given-names>Kathleen M.</given-names></name>
</person-group> <year>1995</year>. “<article-title>An Experimental Explanation of Biological Sex as a Status Cue in Decision-Making Groups and Its Influence on Information Use</article-title>.” <source>Small Group Research</source> <volume>26</volume>:<fpage>451</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr32-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rashotte</surname><given-names>Lisa S.</given-names></name>
<name><surname>Webster</surname><given-names>Murray</given-names></name>
<name><surname>Whitmeyer</surname><given-names>Joseph</given-names></name>
</person-group>. <year>2005</year>. “<article-title>Pretesting Experimental Instructions.” Pp</article-title>. <fpage>163</fpage>–<lpage>87</lpage> in <source>Sociological Methodology</source>, <volume>vol. 35</volume>, edited by <person-group person-group-type="editor">
<name><surname>Stolzenberg</surname><given-names>Ross M.</given-names></name>
</person-group> <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Blackwell Publishing</publisher-name>.</citation>
</ref>
<ref id="bibr33-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rosenthal</surname><given-names>Robert</given-names></name>
<name><surname>Rosnow</surname><given-names>Ralph L.</given-names></name>
</person-group> <year>1984</year>. <source>Essentials of Behavioral Research: Methods and Data Analysis</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr34-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saks</surname><given-names>M. J.</given-names></name>
<name><surname>Risenger</surname><given-names>R. M.</given-names></name>
<name><surname>Rosenthal</surname><given-names>R.</given-names></name>
<name><surname>Thompson</surname><given-names>W. C.</given-names></name>
</person-group> <year>2003</year>. “<article-title>Context Effects in Forensic Sciences: Review and Application of the Science of Science to Crime Laboratory Practice in the United States</article-title>. <source>Science and Justice</source> <volume>43</volume>:<fpage>77</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr35-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scholand</surname><given-names>Andrew J.</given-names></name>
<name><surname>Yla</surname><given-names>R.</given-names></name>
<name><surname>Tausczik</surname><given-names>R.</given-names></name>
<name><surname>Pennebaker</surname><given-names>James W.</given-names></name>
</person-group> <year>2010</year>. “<article-title>Assessing Group Interaction with Social Language Network Analysis</article-title>.” <source>Advances in Social Computing</source> <volume>6007</volume>:<fpage>248</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr36-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sell</surname><given-names>Jane</given-names></name>
<name><surname>Knottnerus</surname><given-names>J. David</given-names></name>
<name><surname>Ellison</surname><given-names>Christopher</given-names></name>
<name><surname>Mundt</surname><given-names>Heather</given-names></name>
</person-group> <year>2000</year>. “<article-title>Reproducing Social Structure in Task Groups: The Role of Structural Ritualization</article-title>.” <source>Social Forces</source> <volume>79</volume>:<fpage>1243</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr37-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shelly</surname><given-names>Robert K.</given-names></name>
</person-group> <year>2007</year>. “<article-title>Training Interviewers and Experimenters</article-title>.” Pp. <fpage>267</fpage>–<lpage>88</lpage> in <source>Laboratory Experiments in the Social Sciences</source>, edited by <person-group person-group-type="editor">
<name><surname>Webster</surname><given-names>Murray</given-names><suffix>Jr.</suffix></name>
<name><surname>Sell</surname><given-names>Jane</given-names></name>
</person-group>. <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr38-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shelly</surname><given-names>Robert K.</given-names></name>
<name><surname>Munroe</surname><given-names>Paul T.</given-names></name>
</person-group> <year>1999</year>. “<article-title>Do Women Engage in Less Task Behavior than Men?”</article-title> <source>Sociological Perspectives</source> <volume>42</volume>:<fpage>49</fpage>–<lpage>67</lpage>.</citation>
</ref>
<ref id="bibr39-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shelly</surname><given-names>Robert K.</given-names></name>
<name><surname>Shelly</surname><given-names>Ann C.</given-names></name>
</person-group> <year>2009</year>. “<article-title>Speech Content and the Emergence of Inequality in Task Groups</article-title>.” <source>Journal of Social Issues</source> <volume>65</volume>:<fpage>307</fpage>–<lpage>33</lpage>.</citation>
</ref>
<ref id="bibr40-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shelly</surname><given-names>Robert K.</given-names></name>
<name><surname>Troyer</surname><given-names>Lisa</given-names></name>
</person-group> <year>2001a</year>. “<article-title>Emergence and Completion of Structure in Partially Defined Groups</article-title>.” <source>Social Psychology Quarterly</source> <volume>64</volume>:<fpage>318</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr41-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shelly</surname><given-names>Robert K.</given-names></name>
<name><surname>Troyer</surname><given-names>Lisa</given-names></name>
</person-group> <year>2001b</year>. “<article-title>Speech Duration and Dependencies in Initially Structured and Unstructured Groups</article-title>.” <source>Sociological Perspectives</source> <volume>44</volume>:<fpage>419</fpage>–<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr42-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shrout</surname><given-names>Patrick E.</given-names></name>
<name><surname>Fleiss</surname><given-names>Joseph L.</given-names></name>
</person-group> <year>1979</year>. “<article-title>Intraclass Correlations: Uses in Assessing Rater Reliability</article-title>.” <source>Psychological Bulletin</source> <volume>86</volume>:<fpage>420</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr43-0081175012444860">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Simon</surname><given-names>Steve</given-names></name>
</person-group>. <year>2008</year>. <access-date>Retrieved January 12, 2011</access-date> (<ext-link ext-link-type="uri" xlink:href="http://www.childrens-mercy.org/stats/definitions/kappa.htm">http://www.childrens-mercy.org/stats/definitions/kappa.htm</ext-link>).</citation>
</ref>
<ref id="bibr44-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tanford</surname><given-names>Sarah</given-names></name>
<name><surname>Penrod</surname><given-names>Steven</given-names></name>
</person-group>. <year>2006</year>. “<article-title>Jury Deliberations: Discussion Content and Influence Processes in Jury Decision Making</article-title>.” <source>Journal of Applied Social Psychology</source> <volume>16</volume>:<fpage>322</fpage>–<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr45-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thye</surname><given-names>Shane R.</given-names></name>
</person-group> <year>2000</year>. “<article-title>Reliability in Experimental Sociology</article-title>.” <source>Social Forces</source> <volume>78</volume>:<fpage>1277</fpage>–<lpage>309</lpage>.</citation>
</ref>
<ref id="bibr46-0081175012444860">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Waller</surname><given-names>Mary J.</given-names></name>
<name><surname>Gupta</surname><given-names>Naina</given-names></name>
<name><surname>Giambatista</surname><given-names>Robert C.</given-names></name>
</person-group> <year>2004</year>. “<article-title>Effects of Adaptive Behaviors and Shared Mental Models on Control Crew Performance</article-title>.” <source>Management Science</source> <volume>50</volume>:<fpage>1534</fpage>–<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr47-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Webster</surname><given-names>Murray</given-names><suffix>Jr.</suffix></name>
<name><surname>Sell</surname><given-names>Jane</given-names></name>
</person-group>. <year>2007</year>. “<article-title>Theory and Experimentation in the Social Sciences</article-title>.” Pp. <fpage>190</fpage>–<lpage>207</lpage> in <source>Laboratory Experiments in the Social Sciences</source>, edited by <person-group person-group-type="editor">
<name><surname>Webster</surname><given-names>Murray</given-names><suffix>Jr.</suffix></name>
<name><surname>Sell</surname><given-names>Jane</given-names></name>
</person-group>. <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr48-0081175012444860">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Webster</surname><given-names>Murray</given-names><suffix>Jr.</suffix></name>
<name><surname>Sell</surname><given-names>Jane</given-names></name>
</person-group>. Forthcoming. “<article-title>Institutions, Structures and Processes</article-title>.” In <source>New Blackwell Companion to Sociology</source>, edited by <person-group person-group-type="editor">
<name><surname>Ritzer</surname><given-names>George</given-names></name>
</person-group>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>