<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JTE</journal-id>
<journal-id journal-id-type="hwp">spjte</journal-id>
<journal-title>Journal of Teacher Education</journal-title>
<issn pub-type="ppub">0022-4871</issn>
<issn pub-type="epub">1552-7816</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022487112454437</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022487112454437</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Theme Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Incorporating Teacher Effectiveness Into Teacher Preparation Program Evaluation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Henry</surname><given-names>Gary T.</given-names></name>
<xref ref-type="aff" rid="aff1-0022487112454437">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kershaw</surname><given-names>David C.</given-names></name>
<xref ref-type="aff" rid="aff2-0022487112454437">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Zulli</surname><given-names>Rebecca A.</given-names></name>
<xref ref-type="aff" rid="aff3-0022487112454437">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Smith</surname><given-names>Adrienne A.</given-names></name>
<xref ref-type="aff" rid="aff4-0022487112454437">4</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022487112454437"><label>1</label>University of North Carolina at Chapel Hill, USA</aff>
<aff id="aff2-0022487112454437"><label>2</label>Slippery Rock University, PA, USA</aff>
<aff id="aff3-0022487112454437"><label>3</label>North Carolina State University, Raleigh, USA</aff>
<aff id="aff4-0022487112454437"><label>4</label>Horizon Research, Chapel Hill, NC, USA</aff>
<author-notes>
<corresp id="corresp1-0022487112454437">Gary T. Henry, Carolina Institute for Public Policy and Department of Public Policy, University of North Carolina at Chapel Hill, 122 Abernethy Hall, CB# 3435, Chapel Hill, NC 27599, USA Email: <email>gthenry@unc.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>63</volume>
<issue>5</issue>
<issue-title>Examining the Complexities of Assessment and Accountability in Teacher Education</issue-title>
<fpage>335</fpage>
<lpage>355</lpage>
<permissions>
<copyright-statement>© 2012 American Association of Colleges for Teacher Education</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Association of Colleges for Teacher Education</copyright-holder>
</permissions>
<abstract>
<p>New federal and state policies require that teacher preparation programs (TPP) be held accountable for the effectiveness of their graduates as measured by test score gains of the students they teach. In this article, the authors review the approaches taken in several states that have already estimated TPP effects and analyze the proposals for incorporating students’ test score gains into the evaluations of TPP by states that have received federal Race to the Top funds. The authors organize their review to focus on three types of decisions that are required to implement these new accountability requirements: (a) selection of teachers, students, subjects, and years of data; (b) methods for estimating teachers’ effects on student test score gains; and (c) reporting and interpretation of effects. The purpose of the review is to inform the teacher preparation community on the state of current and near term practice for adding measures of teacher effectiveness to TPP accountability practices.</p>
</abstract>
<kwd-group>
<kwd>teacher preparation</kwd>
<kwd>value-added models</kwd>
<kwd>program evaluation</kwd>
<kwd>quantitative methods</kwd>
</kwd-group>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>November/December 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<sec id="section1-0022487112454437" sec-type="intro">
<title>Introduction</title>
<p>A new era of accountability for teacher preparation programs (TPP) is being ushered in across the country by recent federal and state policies, such as the Race to the Top (RttT) fund, which require that these programs be held accountable for producing effective teachers. New accountability reforms define effective teachers, at least in part, as those who produce higher student test score gains. In response, states are scrambling to create sophisticated databases that are able to link practicing teachers to their preparation programs as well as to the achievement data of the students they teach. Making these linkages is a necessary step for any TPP evaluation seeking to include estimates of the effects of TPP graduates on student test scores. However, the paucity of specific details in the RttT applications of successful states suggests that states are just beginning to grapple with the process of designing and implementing methods to estimate the effects of graduates of TPPs on their of students’ test score gains. Although pioneering work exists in which the effects of TPPs on student test scores gains have been estimated, thereby proving that incorporating these measures into TPP evaluation can be done (see <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al., 2009</xref>; <xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle, Noell, Knox, &amp; Schafer, 2010</xref>; <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, Zulli, &amp; Kershaw, 2010</xref>; <xref ref-type="bibr" rid="bibr30-0022487112454437">Henry et al., 2011</xref>; <xref ref-type="bibr" rid="bibr39-0022487112454437">Noell, 2006</xref>; <xref ref-type="bibr" rid="bibr40-0022487112454437">Noell &amp; Burns, 2006</xref>, <xref ref-type="bibr" rid="bibr41-0022487112454437">2007</xref>; <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell, Porter, &amp; Patt, 2007</xref>; <xref ref-type="bibr" rid="bibr65-0022487112454437">Tennessee State Board of Education [TSBOE], 2009</xref>, <xref ref-type="bibr" rid="bibr66-0022487112454437">2010</xref>), the existing literature does not concisely identify the major decisions that states will face (nor the major options available) when incorporating student test score gains into TPP evaluations.</p>
<p>To begin to overcome this omission, we reviewed the scholarly literature, policy literature, and RttT documentation and identified many of the major decisions involved in producing test score gain estimates of TPP effectiveness. We developed a framework that separates the major decisions into three distinct categories: selection, estimation, and reporting. Selection decisions involve choosing which students, teachers, academic subjects, and academic years will be used to form the basis of TPP evaluations. Estimation decisions involve choosing and implementing methods for calculating TPP effects using student achievement data. Finally, reporting and interpretation decisions involve deciding how to present and aid the interpretation of TPP effectiveness estimates.</p>
<p>This article focuses on the ongoing work to create empirically grounded estimates of TPP effectiveness using student test scores that has been mandated by federal and state policies. However, future efforts also need to be directed toward creating additional measures of student learning and developing more comprehensive TPP evaluation frameworks. To be clear, this manuscript deals exclusively with the production of teacher effectiveness estimates as measured by student test score gains and does not attempt to present the options for combining these measures with other types of measures of TPP quality to form a comprehensive accountability system. We acknowledge the importance of developing additional measures, but it is beyond the scope of this article. Before describing important decisions in the process of using student test scores for TPP evaluation, we briefly discuss the recent policy initiatives for adding such evidence into TPP evaluation.</p>
</sec>
<sec id="section2-0022487112454437">
<title>Emergence of Student Outcome–Based TPP Accountability</title>
<p>Historically, TPPs have been evaluated based primarily on the components of the preparation program itself, including required coursework, faculty who teach the courses, and the nature and types of experiences that preservice teachers receive. Traditionally, the primary vehicle for evaluation of preparation programs’ capacity for these components has been through applying the standards and review processes of national accrediting organizations, such as the National Council for Accreditation of Teacher Education (<xref ref-type="bibr" rid="bibr37-0022487112454437">NCATE, 2010</xref>). The NCATE, like other accrediting organizations, sets minimum quality standards in a number of areas. These areas include candidate knowledge, skills, and professional dispositions; assessment system and unit evaluation; field experiences and clinical practice; diversity; faculty qualifications; and unit governance and resources (<xref ref-type="bibr" rid="bibr37-0022487112454437">NCATE, 2010</xref>). TPPs are only accredited by NCATE if they can demonstrate—through documentation and interviews—that they have developed their standards, policies, curricula, instruction, and assessment tools based on a research-grounded conceptual framework (<xref ref-type="bibr" rid="bibr37-0022487112454437">NCATE, 2010</xref>). Notably, the accreditation process does not explicitly link teacher preparation to actual student achievement (<xref ref-type="bibr" rid="bibr14-0022487112454437">Crowe, 2010</xref>). Accreditation focuses on evaluating the process of preparing teachers, not directly evaluating TPP graduates’ instructional skills in the classroom or their ability to help their students learn.</p>
<p>In contrast, RttT and other calls for reform (<xref ref-type="bibr" rid="bibr14-0022487112454437">Crowe, 2010</xref>) shifted the current policy environment by pushing states to make the achievement test score gains of the students of TPP graduates a central component of TPP accountability. For example, RttT called for
<disp-quote>
<p>(D)(4) Improving the effectiveness of teacher and principal preparation programs.</p>
<p>(i) Link student achievement and student growth . . . data to the students’ teachers and principals, to link this information to the in-State programs where those teachers and principals were prepared for credentialing, and to publicly report the data for each credentialing program in the State; and</p>
<p>(ii) Expand preparation and credentialing options and programs that are successful at producing effective teachers and principals. (<xref ref-type="bibr" rid="bibr68-0022487112454437">U.S. Department of Education, 2012</xref>, p. 19504-5)</p>
</disp-quote></p>
<p>The RttT student achievement–focused model of evaluation and accountability starkly contrasts with the traditional process-oriented TPP model of evaluation. Under RttT, states must develop and implement the capacity to reliably link student test scores to teachers to determine teacher effectiveness, and then to link these measures of teacher effectiveness back to the programs that prepared them to teach.</p>
<p>This recent emergence of the student learning evaluation approach was not necessarily due to a historical lack of interest in linking teacher preparation to student outcomes (<xref ref-type="bibr" rid="bibr69-0022487112454437">Wilson, Floden, &amp; Ferrini-Mundy, 2001</xref>). Rather, the statewide longitudinal data systems and analytical tools that link teachers to student outcomes on a large scale, which are needed to exercise the student learning evaluation approach, simply did not exist (<xref ref-type="bibr" rid="bibr38-0022487112454437">National Research Council [NRC], 2010</xref>; <xref ref-type="bibr" rid="bibr67-0022487112454437">Voorhees, Barnes, &amp; Rothman, 2003</xref>).</p>
<p>Indeed, the recent policy initiatives pushing states to evaluate TPPs using student test scores coincide with advancements in state longitudinal data systems. Over the last two decades, states and school districts increased capacity to evaluate the effectiveness of TPP graduates in the classroom through the investment in longitudinal data systems that contain administrative data on students, teachers, and schools (<xref ref-type="bibr" rid="bibr14-0022487112454437">Crowe, 2010</xref>; <xref ref-type="bibr" rid="bibr38-0022487112454437">NRC, 2010</xref>). Without these necessary investments, states would not have the data necessary to assess how well the students of graduates from different TPPs and other credentialing pathways are performing in terms of increasing test scores.</p>
<p>Equally important for the emergence of incorporating student test score growth into TPP evaluation has been the development of quantitative methods that can be used to estimate the effect of individual TPPs on student achievement. Advances in two methodological areas are of particular value to states preparing to address the increasing accountability demands: the progress in calculating the effectiveness of individual teachers (see <xref ref-type="bibr" rid="bibr1-0022487112454437">Aaronson, Barrow, &amp; Sander, 2003</xref>; <xref ref-type="bibr" rid="bibr34-0022487112454437">Kane, Rockoff, &amp; Staiger, 2006</xref>; <xref ref-type="bibr" rid="bibr47-0022487112454437">Rivkin, Hanushek, &amp; Kain, 2005</xref>; <xref ref-type="bibr" rid="bibr48-0022487112454437">Rockoff, 2004</xref>) and the recent evaluations of TPPs conducted in a handful of school districts and states (<xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al., 2009</xref>; <xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle et al., 2010</xref>; <xref ref-type="bibr" rid="bibr30-0022487112454437">Henry et al., 2011</xref>; <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>; <xref ref-type="bibr" rid="bibr39-0022487112454437">Noell, 2006</xref>; <xref ref-type="bibr" rid="bibr40-0022487112454437">Noell &amp; Burns, 2006</xref>, <xref ref-type="bibr" rid="bibr41-0022487112454437">2007</xref>; <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell et al., 2007</xref>; <xref ref-type="bibr" rid="bibr65-0022487112454437">TSBOE, 2009</xref>, <xref ref-type="bibr" rid="bibr66-0022487112454437">2010</xref>).</p>
<p>These developments are directly linked to prior studies in which researchers assessed the effectiveness of teachers through the application of the education production function (<xref ref-type="bibr" rid="bibr13-0022487112454437">Coleman, 1966</xref>; <xref ref-type="bibr" rid="bibr28-0022487112454437">Greenwald, Hedges, &amp; Laine, 1996</xref>) that focused on calculating the effects of different factors (home, school, and community) affecting student achievement including teachers with different characteristics. Education production function studies provide an estimate of the value of specific teacher characteristics (such as years of experience or certification status) in terms of student test score gains, controlling for other factors that also influence student achievement (<xref ref-type="bibr" rid="bibr15-0022487112454437">Darling-Hammond, Berry, &amp; Thoreson, 2001</xref>; <xref ref-type="bibr" rid="bibr26-0022487112454437">Goldhaber &amp; Brewer, 2000</xref>). Over the last few decades though, researchers have moved from describing the characteristics of effective teachers toward the identification of individual teachers’ effects on student test score gains (<xref ref-type="bibr" rid="bibr44-0022487112454437">Nye, Konstantopoulos, &amp; Hedges, 2004</xref>). In the process, a number of researchers have developed and refined methodologies designed to isolate the effect of individual teachers on student achievement (<xref ref-type="bibr" rid="bibr1-0022487112454437">Aaronson et al., 2003</xref>; <xref ref-type="bibr" rid="bibr34-0022487112454437">Kane et al., 2006</xref>; <xref ref-type="bibr" rid="bibr47-0022487112454437">Rivkin et al., 2005</xref>; <xref ref-type="bibr" rid="bibr48-0022487112454437">Rockoff, 2004</xref>; <xref ref-type="bibr" rid="bibr50-0022487112454437">Sanders &amp; Rivers, 1996</xref>). Specifically, these researchers have developed various statistical models that attempt to estimate the unique contribution of a teacher to student learning above and beyond any learning that would be expected given a student’s prior achievement and other individual student characteristics (e.g., socioeconomic status, mobility, and disability status), classroom context, and school context. Overall, these studies provided the “proof of concept” that researchers could link student achievement to teachers and generate estimates of teacher effectiveness based on student test score gains.</p>
<p>Greater sophistication in data systems and ability to estimate teacher effects has enabled researchers to begin to leverage these methodological advancements and data system improvements to evaluate teachers’ performance and TPP (<xref ref-type="bibr" rid="bibr38-0022487112454437">NRC, 2010</xref>). Although a number of studies attempt to tackle teacher preparation broadly through the analysis of certification or initial preparation of teachers (<xref ref-type="bibr" rid="bibr6-0022487112454437">Boyd et al., 2006</xref>; <xref ref-type="bibr" rid="bibr11-0022487112454437">Clotfelter et al., 2007</xref>, <xref ref-type="bibr" rid="bibr12-0022487112454437">2010</xref>; <xref ref-type="bibr" rid="bibr15-0022487112454437">Darling-Hammond et al., 2001</xref>; <xref ref-type="bibr" rid="bibr26-0022487112454437">Goldhaber &amp; Brewer, 2000</xref>; <xref ref-type="bibr" rid="bibr31-0022487112454437">Henry, Thompson, Bastian, et al., 2010</xref>; <xref ref-type="bibr" rid="bibr34-0022487112454437">Kane, Rockoff, &amp; Staiger, 2006</xref>), few researchers have attempted to generate numerical estimates of individual TPP effectiveness. To date, researchers have generated analyses that address TPP effectiveness for North Carolina (<xref ref-type="bibr" rid="bibr29-0022487112454437">Henry, Bastian, &amp; Smith, 2012</xref>; <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>), New York City (<xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al., 2009</xref>), Louisiana (<xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle et al., 2010</xref>; <xref ref-type="bibr" rid="bibr39-0022487112454437">Noell, 2006</xref>; <xref ref-type="bibr" rid="bibr40-0022487112454437">Noell &amp; Burns, 2006</xref>, <xref ref-type="bibr" rid="bibr41-0022487112454437">2007</xref>; <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell et al., 2007</xref>; <xref ref-type="bibr" rid="bibr43-0022487112454437">Noell, Porter, Patt, &amp; Dahir, 2008</xref>), Tennessee (<xref ref-type="bibr" rid="bibr65-0022487112454437">TSBOE, 2009</xref>, <xref ref-type="bibr" rid="bibr66-0022487112454437">2010</xref>), and Florida (<xref ref-type="bibr" rid="bibr18-0022487112454437">Florida Department of Education [FDOE], 2009a</xref>, <xref ref-type="bibr" rid="bibr19-0022487112454437">2009b</xref>, <xref ref-type="bibr" rid="bibr20-0022487112454437">2009c</xref>). These researchers used a diverse array of methods, models, students, subjects, teachers, and test scores to estimate the TPP effectiveness for their teachers. <xref ref-type="table" rid="table1-0022487112454437">Table 1</xref> provides an overview of how these researchers have chosen to estimate the effectiveness of TPPs, given the data limitations faced in each state.</p>
<table-wrap id="table1-0022487112454437" position="float">
<label>Table 1.</label>
<caption><p>Select Examples of TPP Effectiveness Reports and Studies</p></caption>
<graphic alternate-form-of="table1-0022487112454437" xlink:href="10.1177_0022487112454437-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">State (study year)</th>
<th align="center"><xref ref-type="bibr" rid="bibr22-0022487112454437">Florida (2009)</xref></th>
<th align="center"><xref ref-type="bibr" rid="bibr60-0022487112454437">The State of North Carolina (2010)</xref></th>
<th align="center">NYC (2009)</th>
<th align="center">Louisiana (2007)</th>
<th align="center">Louisiana (2010)</th>
<th align="center"><xref ref-type="bibr" rid="bibr63-0022487112454437">The State of Tennessee (2010)</xref></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Selection decisions</td>
</tr>
<tr>
<td> TPP estimates for academic years</td>
<td>2007-2008</td>
<td>2005-2006 to 2007-2008</td>
<td>2000-2001 to 2005-2006 (varied)</td>
<td>2004-2005, 2005-2006</td>
<td>2005-2006 to 2008-2009</td>
<td>2009-2010</td>
</tr>
<tr>
<td> Grades/levels</td>
<td>3-8</td>
<td>3-12</td>
<td>3-8</td>
<td>4-9</td>
<td>4-9</td>
<td>4-12</td>
</tr>
<tr>
<td> Separate grade level analyses</td>
<td>No</td>
<td>Elementary, middle, and high school</td>
<td>No</td>
<td>No</td>
<td>No</td>
<td>EOG, EOC/gateway</td>
</tr>
<tr>
<td> Test scores assessed</td>
<td>Mathematics and reading</td>
<td>Mathematics, reading, science (HS), social studies (HS)</td>
<td>Mathematics and ELA</td>
<td>Math, science, and social studies</td>
<td>Math, science, English-language arts, reading, and social studies</td>
<td>EOG: Math, reading/language arts, science, and social studies; EOC: Algebra I, Biology I, English I, and English II</td>
</tr>
<tr>
<td> Student selection decisions</td>
<td>Unknown</td>
<td>No limiting criteria</td>
<td>Excludes retained students and grade skipping students</td>
<td>Excludes within-year movers, retained students</td>
<td>Excludes within-year movers, retained students</td>
<td>Students with at least 3 test scores</td>
</tr>
<tr>
<td> Teachers used in analyses</td>
<td>1st-year teachers</td>
<td>Teachers with less than 10 years of experience</td>
<td>1st- and 2nd-year teachers</td>
<td>All teachers</td>
<td>All teachers</td>
<td>All teachers</td>
</tr>
<tr>
<td> Minimum TPP teacher limits</td>
<td>10 teachers</td>
<td>10 teachers</td>
<td>40 teachers</td>
<td>10 teachers each year, 25 teachers all years</td>
<td>10 teachers each year, 25 teachers all years</td>
<td>5 teachers</td>
</tr>
<tr>
<td> Multiple teachers</td>
<td>Unknown</td>
<td>assigned equal weight</td>
<td>unknown</td>
<td>ELA-analysis not performed</td>
<td>Unknown</td>
<td>Weighted</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td/>
<td>Math, science, social studies—unknown</td>
<td/>
<td/>
</tr>
<tr>
<td> Programs analyzed</td>
<td>10 Institutions</td>
<td>15 UNC traditional undergraduate prepared</td>
<td>Program analysis: 26 traditional programs at 18 state institutions; 4 NYC teaching fellows programs; Teach for America</td>
<td>3 alternative programs; 13 (now defunct) traditional programs</td>
<td>8 alternative programs and 8 traditional undergraduate program effects reported, 59 program effects reportedly estimated</td>
<td>41 institutions, Teach for America</td>
</tr>
<tr>
<td colspan="7">Estimation decisions</td>
</tr>
<tr>
<td> Methodological approach</td>
<td>Value table</td>
<td>Value-added model–direct estimation</td>
<td>Value-added model–direct estimation</td>
<td>Value-added model–direct estimation</td>
<td>Value-added model–direct estimation</td>
<td>Value-added model–aggregation of teacher quality estimates</td>
</tr>
<tr>
<td> Value-added model type</td>
<td>NA</td>
<td>Year-to-year with controls</td>
<td>Year-to-year with controls (school fixed effects), year-to-year with controls, and OLS</td>
<td>Year-to-year with controls</td>
<td>Year-to-year with controls</td>
<td>TVAAS/EVAAS</td>
</tr>
<tr>
<td> Teacher level controls</td>
<td>No</td>
<td>Yes (in some models)</td>
<td>Yes (in some models)</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td colspan="7">(continued)</td>
</tr>
<tr>
<td> Tests used to predict current achievement</td>
<td>NA</td>
<td>Reading and mathematics</td>
<td>Math and ELA</td>
<td>Math, science, ELA, and social studies</td>
<td>Math, science, ELA, and social studies</td>
<td>All available (for 5 years)</td>
</tr>
<tr>
<td> Teacher level controls</td>
<td>NA</td>
<td>Yes (in some models)</td>
<td>Yes (in some models)</td>
<td>Absences only</td>
<td>Absences only</td>
<td>No</td>
</tr>
<tr>
<td> Reference group</td>
<td>NA</td>
<td>All non-UNC undergraduate prepared teachers</td>
<td>Unclear</td>
<td>Experienced teachers, statistical comparisons with new teachers made</td>
<td>Experienced teachers, statistical comparisons with new teachers made</td>
<td>NA</td>
</tr>
<tr>
<td colspan="7">Presentation of results</td>
</tr>
<tr>
<td/>
<td>Percentage of TPP beginning teachers’ students making learning gain bench marks</td>
<td>Table highlighting significant TPP effects; effectiveness estimates, days equivalent effects (where possible)</td>
<td>Effectiveness estimates</td>
<td>5 Performance bands group TPP by statistical relationship with veteran teachers and each other; effectiveness estimates</td>
<td>5 Performance bands group TPP by statistical relationship with veteran teachers and each other; effectiveness estimates</td>
<td>Table highlighting significant TPP effects; mean <italic>t</italic>-values; % of teachers in upper and lower quintiles</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0022487112454437">
<p>Note: NYC = New York City; TPP = teacher preparation programs; ELA = English-language arts; UNC = University of North Carolina; TVAAS = Tennessee Value-Added Assessment System; EVAAS = Education Value-Added Assessment System. HS = High school only; EOG = End-Of-Grade; EOC = End-Of-Course; OLS = ordinary least squares.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>A close look at the RttT applications of the RttT recipients reveals that few states provide concrete details about how the states will ultimately judge TPP effectiveness using student test score gains. Most states simply refer to using “student growth” or “student learning gains” to generate TPP effectiveness as part of their application. This reflects the fact that many of these states are at the beginning of the TPP evaluation process. Georgia was very forthright in this regard when they noted that they would “use a portion of RT3 resources, if awarded, to contract with a value-added model (VAM) provider to develop a statewide VAM” (<xref ref-type="bibr" rid="bibr54-0022487112454437">The State of Georgia, 2010</xref>, p. 104). VAMs seek to isolate the effect of individual teachers on their students test scores by adjusting out the influence of other factors, always including the students’ prior test scores and in most cases, also including other characteristics of the students, their families, their classroom context, and school context.</p>
<p>A few states, such as Massachusetts and Ohio, signal that they are further along in their plans to incorporate student test scores into the evaluation of TPPs. These states either mention specific methods for evaluating teacher effectiveness (i.e., student growth percentiles [SGP], <xref ref-type="bibr" rid="bibr57-0022487112454437">The State of Massachusetts, 2010</xref>) or mention groups performing teacher effectiveness analyses in the state with known methodologies (i.e., Battelle for Kids who use a SAS Education Value-Added Assessment System [EVAAS]; <xref ref-type="bibr" rid="bibr61-0022487112454437">The State of Ohio, 2010</xref>). Still, Ohio and Massachusetts provide few concrete details about how they will implement the methods as part of their TPP effectiveness evaluation and, in fact, fall short of committing to use these approaches as part of their TPP evaluations.</p>
<p>Finally, <xref ref-type="bibr" rid="bibr60-0022487112454437">The State of North Carolina (2010</xref>, Section D, Appendix 26, p. 143) and <xref ref-type="bibr" rid="bibr63-0022487112454437">The State of Tennessee (2010</xref>, p. D96) provided greater details about teacher or TPP effectiveness studies that have been done in their states. At the same time, the details provided by North Carolina and Tennessee in their RttT applications still do not sufficiently convey the detailed decisions that researchers must make when analyzing TPP effectiveness.</p>
<p><xref ref-type="table" rid="table2-0022487112454437">Table 2</xref> lays out some of the language in the RttT proposals that the successful states used to describe their plans to evaluate and hold their TPPs accountable. To be sure, this table reflects the state’s intention at the RttT proposal stage and only scratches the surface in terms of the plans to incorporate measures of TPP effectiveness and using these measures to improve teacher preparation in the RttT states. It is, however, instructive to review some of the language in their successful proposals. These proposals collectively indicate that the framework we present could inform the policy conversations as states begin to implement their plans.</p>
<table-wrap id="table2-0022487112454437" position="float">
<label>Table 2.</label>
<caption><p>Race to the Top Proposal Information</p></caption>
<graphic alternate-form-of="table2-0022487112454437" xlink:href="10.1177_0022487112454437-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">States</th>
<th align="center">Snapshot of state’s plan to assess teacher preparation programs</th>
</tr>
</thead>
<tbody>
<tr>
<td><xref ref-type="bibr" rid="bibr52-0022487112454437">The State of Delaware, 2010</xref></td>
<td>“Delaware’s rigorous statewide educator evaluation system is based on the most respected standards for teaching and leading (Danielson’s A Framework for Teaching and the Interstate School Leaders Licensure Consortium’s standards for leaders). The system provides a multi-measure assessment of performance that incorporates student growth as one of five components. Rather than set a specific percentage that student growth must be weighted in the evaluation, these regulations go much further. They say that an educator can only be rated effective if they demonstrate satisfactory levels of student growth.” (p. A-4)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr63-0022487112454437">The State of Tennessee, 2010</xref></td>
<td>“Unlike most states, which likely are setting up their data systems in order to know which teacher preparation programs prepare the highest-achieving graduates, Tennessee can—and does already—perform this analysis considering teacher effect data, placement and retention, and Praxis scores. Our LEAs can, and do, optimize our new teacher supply by using these data to increase recruitment, selection and hiring from preparation programs whose teachers consistently achieve better outcomes.</td>
</tr>
<tr>
<td/>
<td>Tennessee already publicly reports this data for each credentialing program in the state.” (p. 110)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr16-0022487112454437">The District of Columbia, 2010</xref></td>
<td>“DC will use Race to the Top to deliver on the next phase of bold reforms. Specifically, the District will:</td>
</tr>
<tr>
<td/>
<td>1. Identify teacher preparation programs that are not providing effective teachers and hold them accountable for their quality, providing them with specific feedback on the performance of their graduates to support targeted improvements, and revoking program approval after continued ineffectiveness, as necessary. Percentage of teacher preparation programs in the State for which the public can access data on the achievement and growth (as defined in this notice) of the graduates’ students. 100%.” (p. 92-93)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr53-0022487112454437">The State of Florida, 2010</xref></td>
<td>“Supported by substantial data and statewide assessment systems, Florida measures growth and proficiency annually for each student (learning gains) in reading and mathematics in Grades 4 through 10 and reports these data at the school level as part of the state’s accountability system. The state’s longitudinal database links students with their teachers and courses, and teachers to teacher preparation programs and professional development. This linkage is currently used statewide to report individual teacher performance in the aggregate by school type, subgroup, and preparation program, but only cautiously, based on the knowledge that a more sophisticated measure of student growth is needed to further examine individual teacher performance.” (p. 138)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr54-0022487112454437">The State of Georgia, 2010</xref></td>
<td>“ACTIVITY (1): Create a Teacher Effectiveness Measure (TEM) for each teacher in the state and a Leader Effectiveness Measure (LEM) for each principal in the State. The TEM and LEM require linking student achievement and student growth data to the students’ teachers and principals. This is the first necessary step in lining the information back to in-State teacher and principal preparation programs.</td>
</tr>
<tr>
<td/>
<td>ACTIVITY (2): Develop a Teacher Preparation Program Effectiveness Measure (TPPEM) and Leader Preparation Program Effectiveness Measure (LPPEM). The TPPEM and LPPEM include multiple components, including TEM and LEM of graduates aggregated by cohort, which provides the linkage between student growth data to in-State teacher and principal preparation programs.</td>
</tr>
<tr>
<td/>
<td>ACTIVITY (3): Calculate TPPEM and LPPEM and publish preparation program “report cards” (both traditional and alternative routes).</td>
</tr>
<tr>
<td/>
<td>Student growth data will be tracked as early as 2010-2011through value-added models, but the first full year of TEM/LEM implementation will not occur until SY2011-2012 (since the qualitative evaluation tool will be validated in 2010-2011 and launched in participating LEAs in 2011-2012). First TEM/LEM scores will be available in the fall of 2012; the earliest the State would have data to calculate TPPEM and LPPEM would be late 2012.” (p. 143)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr55-0022487112454437">The State of Hawaii, 2010</xref></td>
<td>“The State, HIDOE, and the Hawaii Teacher Standards Board (HTSB) share a vested interest in obtaining data about the effectiveness of teacher preparation programs and then acting on those data to ensure that the State Approved Teacher Education Programs (SATEP) and Administrator Certification for Excellence (ACE) programs are doing the best job possible at preparing Hawaii’s teacher and principal corps. In addition, in response to requirements of the Federal Higher Education Opportunity Act (2008), local teacher preparation programs already have been demanding data linking student achievement to students’ teachers and their respective preparation programs. Additionally, HTSB’s Unit Performance Standards for State Approved Teacher Education Programs requires preparation programs to “collect and analyze data about program completer performance to evaluate and improve” the program. Hawaii leaders are working together to ensure its new data system collects and analyzes more relevant information about how well each program is preparing teachers and principals to be effective; they also are working to make sure this information is more widely distributed and easily understood, so it can be better used by policymakers in reviewing programs, by schools in making hiring decisions and by teacher and principal candidates in deciding on which preparation path will give them the best support.” (p. 141)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr56-0022487112454437">The State of Maryland, 2010</xref></td>
<td>“All teacher preparation programs are evaluated on common performance criteria aligned with State and national outcomes; Maryland has closed one program and placed three others on probation for subpar performance. The State Board of Education adopted professional development standards to ensure quality across all professional development experiences, including induction. LEAs provide a teacher induction plan that follows beginning teachers through the tenure period. Additionally, Maryland will establish partnerships with the University System of Maryland to design a STEM teacher preparation program based on a proven national model, such as the National Math and Science Initiative’s UTeach program. Partner institutions will commit to recruiting college students in their junior years for a specially designed model of instruction co-planned, implemented, and evaluated by the collaborative efforts of both the College of Arts and Sciences and the College of Education.” (p. 51-52)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr58-0022487112454437">The State of Massachusetts, 2011</xref></td>
<td>“Great Teachers and Leaders: The depth and breadth of the initiatives and strategies described in section D necessitate continued and consistent collaboration among ESE, EOE, DHE, UMASS, and other stakeholders, and ESE will coordinate these partnerships both during and beyond the four-year RTTT grant. For example, ESE will continue its partnership with DHE, institutions of higher education, and other partners to develop and embed measures of educator effectiveness into every component of the system; improve the content, quality, and structure of teacher preparation programs; and increase the diversity of the educator workforce. RTTT funding also will be allocated to the Readiness Centers to supplement the capacity of ESE to provide instructional and professional development services and to convene stakeholders to address cross-sector priorities.” (p. 196)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr59-0022487112454437">The State of New York, 2010</xref></td>
<td>“NYSED will partner with higher education institutions as they redesign their teacher preparation programs to align with the Department’s new standards and performance-based assessments for teacher certification . . . NYSED will also publish transparent data profiles for all institutions that prepare teachers and principals that focus on the performance of students their graduates have taught. Leaders in educational policy—including superintendents, school board members, members of Congress and the State legislature, the Governor’s office, and the Board of Regents—will have access to customized reports that provide information regarding K–12 program effectiveness, higher education program effectiveness, and the adequacy of teacher preparation programs. Information will be available to help inform discussions regarding teacher and administrator evaluation, as well as policy decisions regarding student performance and the achievement gap.” (p. 18)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr60-0022487112454437">The State of North Carolina, 2010</xref></td>
<td>“Ground-Breaking Study of UNC Teacher Preparation Programs.</td>
</tr>
<tr>
<td/>
<td>NC links student achievement and growth data to teacher preparation programs. The UNC General Administration (UNC-GA), in close partnership with constituent UNC institutions that prepare teachers and principals, has completed the first phase of a new value added accountability study of educator preparation programs (called <italic>NC Teacher Quality Research</italic>). Results from this first phase are outlined in <italic>The Impact of Teacher Preparation on Student Learning in North Carolina Public Schools</italic> (<xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>). A primary component of the study is a quantitative evaluation of the impact of teacher preparation program graduates on student learning at the elementary, middle, and secondary levels. This initiative—one of the first of its kind in the country—has begun the process of examining program impact across grade levels, content-area subjects, and subpopulations of students, as well as across nearly a dozen different “portals” of entry into the profession (e.g., alternative and out-of-State programs, in addition to traditional in-State routes). Future evaluations also will discern the impact of principals and other school-based professionals on student achievement and provide evaluations of their preparation programs.” (p. 176)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr61-0022487112454437">The State of Ohio, 2010</xref></td>
<td>“As part of ODE’s longitudinal data system, teacher and principal effectiveness data will be computed annually and linked to teacher and principal preparation programs. Data will also shed light on achievement gaps and how such gaps or the absence of gaps connect to educator preparation programs. This information will be publicly reported through an annual Ohio Teacher Education Report Card and shared on the OBR website showing aggregate effectiveness ratings of graduates from Ohio programs and institutions. The reporting system will permit the public to view the aggregate rating distribution for all graduates by program and licensure area, as well as for specific years. The OBR report will highlight successful programs while also calling attention to programs that may consistently produce graduates who are unsuccessful in their positions or who fail to obtain their professional licensure.” (p. D4-5)</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr62-0022487112454437">The State of Rhode Island, 2010</xref></td>
<td>“The state will be equally aggressive in holding teacher preparation programs accountable for the effectiveness of their graduates.</td>
</tr>
<tr>
<td/>
<td>Rhode Island will publicly report on the effectiveness of each educator preparation program’s graduates. RIDE will use Race to the Top funds to create new educator preparation program report cards that include information on:</td>
</tr>
<tr>
<td/>
<td>The impact of the program’s graduates on student growth and academic achievement, as compared with all other teacher or principal (as appropriate) preparation programs in the state; The rate at which each program’s graduates earn full Professional Certification, which under the new certification system (described in D (2)(iv)) will require evidence of effectiveness, by the end of their first three years of teaching; and The number of preparation programs’ graduates working in Rhode Island schools, disaggregated by LEA and high/low-poverty and high/low-minority schools. These report cards will use a consumer-friendly format and will be available on the RIDE website to provide preparation programs, prospective teachers and employers, and the public a comprehensive, objective picture of the effectiveness of each preparation program’s graduates. RIDE will also publish an annual statewide educator preparation report card that aggregates information on the performance of all preparation programs in the state.” (D49-D50)</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Because the RttT applications are short on details—and because collecting, coding, and analyzing teacher preparation content data generates a substantial number of key decisions—the remainder of this article turns to existing policy and scholarly research to identify and understand the key decisions that must be faced when estimating TPP effectiveness.</p>
</sec>
<sec id="section3-0022487112454437">
<title>Identifying Major Decisions in TPP Evaluation</title>
<p>Presently, many RttT-funded states and other states hoping to incorporate measures of effectiveness into the evaluation of TPPs are struggling with undertaking the first steps of estimating TPP effectiveness. Fortunately, the empirical research cited above provides evidence that estimating a teacher’s effectiveness is possible and can help to inform future efforts to incorporate student test score gains into the evaluation of TPPs. However, this literature also highlights limitations and challenges that will be faced when incorporating student test scores into TPP evaluations. The shortcomings identified from past and current efforts to link student test scores back to TPP can shed light on the major decisions, both methodological and policy-oriented, that will influence the ultimate nature and robustness of any such evaluation effort. This article will provide a synthesis of the relevant methodological, research, and RttT-related literature to carefully identify (as well as explain) the importance of many of the key decisions states and researchers have faced when attempting to generate a quantitative estimate of TPP effectiveness. We classify these decisions as falling into one of three domains: (a) selection, (b) estimation, and (c) reporting and interpretation.</p>
<p>Selection decisions refer to the choices that states that wish to implement an assessment of TPP effects on student test scores will need to make about the students, teachers, and subjects they will include in the evaluation. Estimation decisions involve the choices associated with selecting an analysis method that will be used to quantify TPP effectiveness. Each commonly used approach has a specific set of processes that generate a set of decisions. Reporting decisions refer to the choices that states who have undertaken TPP will need to make about what specific information is released and the manner in which the results of the evaluation (effect estimates) are presented. The following sections of this article will separately address each of these three types of decisions describing the choices researchers have made when faced with these decisions and identifying some of the consequences of the different choices.</p>
</sec>
<sec id="section4-0022487112454437">
<title>Selection Decisions</title>
<p>Selection decisions involve deciding which students and teachers will be used to analyze TPP effectiveness as well as which content areas will be used to form the evaluation of the TPPs. Specifically, states must select (a) which subjects, grades, and academic years will be used to evaluate TPP effectiveness; (b) which students will be used to evaluate TPP effectiveness; and (c) which teachers will be used to evaluate TPP effectiveness. The choices made at the selection decision points ultimately define the comprehensiveness of the assessment of the TPP effects on student test scores. With the exception of including teachers too far removed from their initial training, researchers gain a more complete (unbiased) picture of TPP effectiveness by including more students, teachers, and subjects in their analyses.</p>
<sec id="section5-0022487112454437">
<title>Subjects, Grades, and Academic Years</title>
<p>Although RttT directly calls for connecting TPPs to student performance, the ability to generate teacher and TPP estimates are limited by the nature of the existing student assessment programs operating in the state. Obviously, if a state does not test students in a certain academic year, grades, or subjects, the state cannot use those years, grades, or subjects to estimate TPP effectiveness. For many states, requirements of No Child Left Behind (NCLB) established the assessment system parameters. NCLB required each state test students in mathematics and reading in Grades 3 through 8, and at least once in either Grade 10, 11, or 12 (PL 107-110). States were also required (by academic year 2007-2008) to test students in science one time in Grades 3 through 5, 6 through 9, and 10 through 12 (PL 107-110). Although all states should meet these basic requirements, states vary in the assessments they administer.</p>
<p>For example, Florida’s Comprehensive Assessment Tests (FCAT) annually tests students learning in mathematics and reading in Grades 3 through 10, science in Grades 5, 8, and 11, and writing in Grades 4 and 8 (<xref ref-type="bibr" rid="bibr17-0022487112454437">FDOE, 2005</xref>). Assessments for specific high school courses are not included. Consequently, analyses of TPPs that focus on training high school teachers would largely be limited to teachers of students in Grades 9 through 11 and would not be tied to learning standards and objectives for specific high school courses (<xref ref-type="bibr" rid="bibr17-0022487112454437">FDOE, 2005</xref>). In contrast, the California Standards Tests (CST) includes an English-language arts test for Grades 2 through 11, but the general mathematics test is only given to students between Grades 2 and 7 (<xref ref-type="bibr" rid="bibr10-0022487112454437">California Department of Education [CDE], 2011</xref>). Beginning in Grade 7, students may end up taking additional mathematics CSTs, including Geometry, Algebra I, and Algebra II. High school geometry teachers and algebra teachers in California can be evaluated based on the specific course objectives, whereas all math teachers would be lumped together in any Florida teacher analysis. Note that Florida brought end-of-course (EOC) tests for high school students online in May 2011 (<xref ref-type="bibr" rid="bibr21-0022487112454437">FDOE, n.d.</xref>).</p>
<p>The greater number of tested grades and subject areas tested, the greater the flexibility a state will have in creating detailed TPP analyses. For example, researchers in North Carolina took advantage of a diverse battery of tests designed to evaluate students’ knowledge of the core curriculum at the elementary, middle, and high school levels. <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al. (2010)</xref> used North Carolina end-of-grade tests to evaluate TPP effectiveness in preparing teachers of elementary and middle grades students in reading and mathematics classes, and EOC tests to estimate TPP effectiveness at promoting overall, mathematics, science, and English achievement for high school students.<sup><xref ref-type="fn" rid="fn1-0022487112454437">1</xref></sup> Henry, Thompson, Fortner, et al. found that programs that were better than all other sources of teachers in one subject at one level of schooling often fell back in the pack—or even fell behind—at other levels and in other subjects. It is worth noting that other researchers have also found that TPPs housed in a single institution can have variable impacts on student achievement across different grade levels and subjects (<xref ref-type="bibr" rid="bibr65-0022487112454437">TSBOE, 2009</xref>, <xref ref-type="bibr" rid="bibr6-0022487112454437">2010</xref>). <xref ref-type="table" rid="table3-0022487112454437">Table 3</xref> presents a list of the student assessments administered during academic year 2009-2010 in RttT recipient states.</p>
<table-wrap id="table3-0022487112454437" position="float">
<label>Table 3.</label>
<caption><p>Statewide Assessments of the General Student Population During Academic Year 2009-2010</p></caption>
<graphic alternate-form-of="table3-0022487112454437" xlink:href="10.1177_0022487112454437-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Delaware</th>
<th align="center">District of Columbia</th>
<th align="center">Florida</th>
<th align="center">Georgia</th>
<th align="center">Hawaii</th>
<th align="center">Maryland</th>
<th align="center">Massachusetts</th>
<th align="center">New York</th>
<th align="center">North Carolina</th>
<th align="center">Ohio</th>
<th align="center">Rhode Island</th>
<th align="center">Tennessee<sup><xref ref-type="table-fn" rid="table-fn3-0022487112454437">a</xref></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="13">End-of-grade exams (cell values are grades the test is offered)</td>
</tr>
<tr>
<td> Writing<sup><xref ref-type="table-fn" rid="table-fn4-0022487112454437">b</xref></sup></td>
<td/>
<td>4, 7, 10</td>
<td>4, 8, 10</td>
<td>3, 5, 8, 11</td>
<td/>
<td/>
<td>4, 7, 10</td>
<td/>
<td/>
<td/>
<td>5, 8, 11</td>
<td>5, 8, 11</td>
</tr>
<tr>
<td> Reading/ELA</td>
<td>3-10</td>
<td>3-8, 10</td>
<td>3-10</td>
<td>1-8</td>
<td>3-8, 10</td>
<td>3-8</td>
<td>3-8, 10</td>
<td>3-8</td>
<td>3-8</td>
<td>3-8</td>
<td>3-8, 11</td>
<td>3-8</td>
</tr>
<tr>
<td> Mathematics</td>
<td>3-10</td>
<td>3-8, 10</td>
<td>3-10</td>
<td>1-8</td>
<td>3-8, 10</td>
<td>3-8</td>
<td>3-8, 10</td>
<td>3-8, 9-12</td>
<td>3-8</td>
<td>3-8</td>
<td>3-8, 11</td>
<td>3-8</td>
</tr>
<tr>
<td> Science–technology–engineering</td>
<td>4, 6, 8, and 11</td>
<td>5, 8</td>
<td>5, 8, 11</td>
<td>3-8</td>
<td>4, 6, 10</td>
<td>5, 8</td>
<td>5,8, 9/10 (multiple HS science/technology subject tests)</td>
<td>4, 8</td>
<td>5, 8</td>
<td>5, 8</td>
<td>4, 8, 11</td>
<td>3-8</td>
</tr>
<tr>
<td> Social studies</td>
<td>4, 6, 8, and 11</td>
<td/>
<td/>
<td>3-8</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>3-8</td>
</tr>
<tr>
<td colspan="13">End-of-course exams (generally offered Grades 9-12, but sometimes earlier)</td>
</tr>
<tr>
<td> Math</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Science</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Geography</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> U.S. history</td>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td>X</td>
<td>X</td>
<td/>
<td/>
<td>X</td>
</tr>
<tr>
<td> Global history</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> English</td>
<td/>
<td/>
<td/>
<td>Ninth-grade literature and composition/American literature and composition</td>
<td/>
<td>X</td>
<td/>
<td>X</td>
<td>English I</td>
<td/>
<td/>
<td>English I, II, III</td>
</tr>
<tr>
<td> Economics-civics</td>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td/>
<td>Civics and economics</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Geometry</td>
<td/>
<td/>
<td/>
<td>Mathematics I and II</td>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Algebra I</td>
<td/>
<td/>
<td/>
<td>Mathematics I and II</td>
<td>X</td>
<td>X</td>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td>X</td>
</tr>
<tr>
<td> Algebra II</td>
<td/>
<td/>
<td/>
<td>Mathematics I and II</td>
<td>X</td>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td>X</td>
</tr>
<tr>
<td> Chemistry</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
</tr>
<tr>
<td> Physics</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
</tr>
<tr>
<td> Physical science</td>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Biology</td>
<td/>
<td>X</td>
<td/>
<td>X</td>
<td/>
<td>X</td>
<td/>
<td/>
<td>X</td>
<td/>
<td/>
<td>X</td>
</tr>
<tr>
<td> Government</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>X</td>
<td/>
<td>X</td>
<td/>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0022487112454437">
<p>Note: ELA = English-language arts.</p>
</fn>
<fn id="table-fn3-0022487112454437">
<label>a</label>
<p>Tennessee measures mathematics, reading, science, and social studies with a single assessment in Grades 3 to 8.</p>
</fn>
<fn id="table-fn4-0022487112454437">
<label>b</label>
<p>Writing/composition may be subsumed in an ELA test in states without an explicit writing assessment.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Similarly, greater content in testing programs across grades and subject matter will yield a richer data set, which allows evaluators to obtain effectiveness estimates for a larger number of teachers in multiple areas of teacher preparation. The size of the pool of educators whose effectiveness can be assessed is directly related to the decisions concerning grade and content selection. Increasing the pool of educators (by maximizing grades and subjects tested) also means that the evaluation of TPP effectiveness would be based on a larger proportion of graduates. States with minimal assessment programs will end up estimating the program’s effects on fewer program graduates. Reducing the number of teachers used in an analysis can reduce reliability (year-to-year stability in the numerical assessment of a TPP’s effectiveness), can bias TPP effectiveness estimates (produce systematic differences between the estimated and actual effect of a TPP), and result in the omission of TPP that prepare teachers for untested grades and subjects such as early childhood education (Pre-K-3) or social studies.</p>
<p>In addition, although the availability of state testing in a particular academic year is a prerequisite for the academic year to be included in the analysis, researchers may not want to evaluate TPPs using all available data. After all, TPPs undergo leadership, faculty, and structural changes over time. For these reasons, researchers have either limited or varied the academic years used to evaluate TPP effectiveness. For example, <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd and colleagues (2009)</xref> separately analyzed TPP effectiveness using different combinations of academic years to test the stability of the estimates. <xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle and colleagues (2010)</xref> refused to generate reports for many teacher-preparing institutions in Louisiana due to complete overhauls of TPPs at particular institutions in the state. Their concern was that many of Louisiana’s current teachers who received training at these institutions in the past will not adequately represent the current state of teacher preparation at these institutions.</p>
</sec>
<sec id="section6-0022487112454437">
<title>Students</title>
<p>Naturally, the students used to capture TPP effectiveness for RttT will exclude those students who did not take the state assessments selected for use in the TPP analyses. Similarly, the student pool will also exclude students who are unable to be matched to their teacher or (for most analyses) prior test score(s) (e.g., students in the first tested grade within the state or new public school students). The exclusion of these students is due to the need for prior test scores from which gains (or losses) can be calculated. However, some states require that students be matched to their test scores from more than 1 year. For example, Tennessee requires that all students included in the analysis have at least three prior test scores, which eliminates fourth-grade students (and their teachers) because these students only took two tests (third-grade reading and mathematics assessments) prior to the fourth grade and therefore lack a third test score.</p>
<p>In addition, some researchers have deliberately chosen to further limit the pool of students used to evaluate TPPs. <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd and colleagues (2009)</xref> excluded students who skipped a grade as well as retained students. Similarly, <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell and colleagues (2007</xref>; <xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle et al., 2010</xref>) excluded retained students and students who moved within the school year. Researchers support their decisions to exclude students who skip a grade or are retained because they believe the difference between the prior year’s test score and the current year’s test score should be interpreted as qualitatively different from test scores of students on a normal grade progression. Other researchers opt to retain as many students in the analysis as possible. For example, <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al. (2010)</xref> included grade skipping, retained, and within-year moving students in their analyses and adjusted for these students by including control variables for students who are overage, underage, those who changed schools within year, and those who changed schools between years.</p>
<p>Overall, the decision to limit the students included in a TPP student outcome evaluation may have important consequences, if the excluded students are disproportionately more or less likely to be taught by graduates of some TPPs or if the graduates of some TPP are either more or less effective with the excluded students. Unfortunately, existing research does not specify what the impact, if any, these different decisions have had on TPP estimates in the studies reported above.</p>
</sec>
<sec id="section7-0022487112454437">
<title>Teachers</title>
<p>Once a state decides which grades, subject areas, and students will be used to evaluate TPPs, their policy makers will need to decide which teachers will be included in the analyses. Researchers face several additional decision points that affect which teachers are included in TPP analyses: These decisions involve (a) choosing whether teachers of all experience levels are included in the analysis, (b) deciding how to handle students with multiple teachers, and (c) deciding which TPPs will be used in analyzing TPP effectiveness.</p>
<p>To address the notion that the influence of teachers’ university preparation diminishes as teachers gain experience, researchers have often limited their analyses to teachers with a limited number of years of experience. For instance, North Carolina initially estimated effectiveness of teachers with less than 10 years of experience (<xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>) and subsequently limited the analysis to teachers with less than 5 years of experience (<xref ref-type="bibr" rid="bibr30-0022487112454437">Henry et al., 2011</xref>) as the number of years of data in the longitudinal database increased. Using an even more restrictive approach, <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd and colleagues (2009)</xref> based their estimates of TPP effectiveness on New York City teachers with 1 or 2 years of experience. Florida provided estimates of student learning for 1st-year teachers from virtually all TPPs within its borders (<xref ref-type="bibr" rid="bibr17-0022487112454437">FDOE, 2005</xref>, <xref ref-type="bibr" rid="bibr20-0022487112454437">2009c</xref>). Restricting TPP analyses to teachers with limited experience increases the confidence that any significant program effect reflects the current TPP process and practices. However, this decision must be balanced by the requirement to include a sufficient number of teachers who graduated from the states’ TPPs and teach tested grades and subjects in the state’s public schools.</p>
<p>Note that not all TPP researchers restrict the pool of teachers used in the analysis. Researchers in Louisiana (<xref ref-type="bibr" rid="bibr40-0022487112454437">Noell &amp; Burns, 2006</xref>; <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell et al., 2007</xref>; <xref ref-type="bibr" rid="bibr43-0022487112454437">Noell et al., 2008</xref>) and Tennessee (<xref ref-type="bibr" rid="bibr66-0022487112454437">TSBOE, 2010</xref>) use teachers at all experience levels in the process of estimating TPP effectiveness for recent TPP graduates. In Louisiana, experienced teachers are used as the reference group and TPP effects are directly estimated for teachers in their first 2 years. The statistical model used in Tennessee allows the state to calculate teacher effects for teachers with all levels of experience (<xref ref-type="bibr" rid="bibr66-0022487112454437">TSBOE, 2010</xref>). The teacher effects for Tennessee’s beginning teachers from TPPs are compared with the effectiveness of more experienced teachers. More information on these approaches is provided in the section on estimation decisions.</p>
<p>Although focusing TPP estimates on a subset of novice teachers has advantages, this approach can substantially reduce the number of teachers used in the analysis of TPP effectiveness. In general, evaluators should be wary of interpreting estimates of programs when the number of teachers included in the analyses is small. Researchers have used different teacher count criteria for reporting purposes. Tennessee required a minimum of 5 teachers (<xref ref-type="bibr" rid="bibr66-0022487112454437">TSBOE, 2010</xref>), North Carolina set the minimum teacher counts for each TPP to 10 (<xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>), Louisiana set the minimum at 25 (<xref ref-type="bibr" rid="bibr40-0022487112454437">Noell &amp; Burns, 2006</xref>; <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell et al., 2007</xref>; <xref ref-type="bibr" rid="bibr43-0022487112454437">Noell et al., 2008</xref>), and <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd and colleagues (2009)</xref> set 40 as the limit in their analysis of New York City TPPs.</p>
<p>In addition, the TPP evaluators must decide how to handle students with multiple teachers. For a variety of reasons, a student may experience multiple teachers within the same subject during an academic year. For example, some students have “pullout” teachers who provide additional help in certain subjects, whereas other students experience coteaching (<xref ref-type="bibr" rid="bibr23-0022487112454437">Fuchs, Compton, Fuchs, Bryant, &amp; Davis, 2008</xref>; <xref ref-type="bibr" rid="bibr36-0022487112454437">Murawski &amp; Lochner, 2011</xref>). In addition, some students take multiple classes assessed by the same test (e.g., English and Literature). All of a student’s teachers in a particular subject area who serve as instructors during a particular year may impact a student’s test performance to some degree. The key decision facing TPP evaluators is, “Should a teacher, linked to a TPP, be equally responsible for student achievement on a test when she is responsible for only a part of a student’s instruction in a particular subject?”</p>
<p>Researchers have taken different approaches to dealing with students with multiple teachers that influence a single student test score. <xref ref-type="bibr" rid="bibr31-0022487112454437">Henry, Thompson, Bastian, et al. (2010)</xref> weighted student test scores across all teachers who share influence of a particular student test score. For example, if a student had two reading teachers, each was credited with half of the student’s reading achievement gains. The statistical models used by Tennessee (<xref ref-type="bibr" rid="bibr66-0022487112454437">TSBOE, 2010</xref>)—and scheduled to be used by Ohio as part of RttT (<xref ref-type="bibr" rid="bibr61-0022487112454437">State of Ohio, 2010</xref>)—similarly assign each teacher a weight equal to the proportion of a student’s instructional time claimed by the teacher (<xref ref-type="bibr" rid="bibr70-0022487112454437">Wright, White, Sanders, &amp; Rivers, 2010</xref>).</p>
<p>In contrast, other researchers have gotten around this problem by avoiding estimating TPP effectiveness in subjects where a large number of students have multiple teachers. <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell and colleagues (2007)</xref> did not perform an English/language arts analyses in Louisiana in the 2004-2005 and 2005-2006 TPP evaluation due to the high prevalence of multiple teachers in English/language arts. The Louisiana report suggests the multiple-teacher problem was small enough as to not warrant special attention in mathematics, science, and social studies analyses (<xref ref-type="bibr" rid="bibr42-0022487112454437">Noell et al., 2007</xref>). <xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle and colleagues (2010)</xref> subsequently circumvented this problem by generating separate analyses for English-language arts and reading. Under an approach such as the Florida (<xref ref-type="bibr" rid="bibr18-0022487112454437">FDOE, 2009a</xref>, <xref ref-type="bibr" rid="bibr20-0022487112454437">2009c</xref>) value table approach, each teacher would be fully accountable for a student’s growth even if they share teaching responsibilities for a student with other teachers. It appears that this approach would effectively double-count students with multiple teachers in state estimates of TPP effectiveness.</p>
<p>Finally, researchers may also be faced with deciding which TPP programs will be used to estimate TPP effectiveness. RttT structures this decision for many states, as RttT requires states to evaluate traditional and alternative preparation programs at both public and private institutions within their borders (see section D(4) of RttT application). However, what is less clear is whether RttT requires states to evaluate TPP effectiveness using student achievement from students taught by teachers trained by <italic>all</italic> teacher preparation sources. In theory, a state might decide to estimate effectiveness for in-state TPPs using only students taught by in-state TPP graduates. Again, this is an important decision because the estimates of TPP effectiveness are relative rather than absolute and therefore will vary if teachers that are prepared in-state are more or less effective than those from out-of-state.</p>
<p>Although in-state TPPs often supply the majority of teachers to the labor pool, out-of-state sources often contribute substantial numbers. Teachers prepared in out-of-state TPPs make up nearly 30% of teachers in North Carolina (<xref ref-type="bibr" rid="bibr31-0022487112454437">Henry, Thompson, Bastian, et al., 2010</xref>). Similarly, the Florida Committee on Pre-K-12 Education reported that out-of-state teachers made up approximately 46% of new teachers in Florida (<xref ref-type="bibr" rid="bibr22-0022487112454437">The Florida Senate, Committee on Education Pre-K-12, 2009</xref>). Importantly, when out-of-state prepared teachers were included and identified in a NC teacher preparation entry portal (pathway) analysis, they performed worse than traditional, public state institution–trained teachers in several grades and subjects (<xref ref-type="bibr" rid="bibr31-0022487112454437">Henry, Thompson, Bastian, et al., 2010</xref>). Furthermore, <xref ref-type="bibr" rid="bibr31-0022487112454437">Henry, Thompson, Bastian, et al. (2010)</xref> found out-of-state teachers were particular less effective in elementary school reading and mathematics (Grades 3-5), where they are the largest source of teachers with less than 5 years of experience in elementary school models. Consequently, the omission of the out-of-state prepared teachers who were, on average, poorer performing teachers from the calculation of TPP effectiveness in North Carolina would, at minimum, make relative effectiveness of in-state TPPs appear worse. Omitting the out-of-state TPPs, which might be the source of additional teachers if in-state TPPs produced fewer teachers, could be an unanticipated negative side effect if the estimates of TPP effectiveness are used to make programmatic decisions.</p>
<p>Similarly, decisions must be made about including out-of-state alternative preparation programs, such as Teach for America, which supplies teachers who, according to some recent research, provide greater gains in student achievement than other sources of teachers (<xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al., 2009</xref>; <xref ref-type="bibr" rid="bibr25-0022487112454437">Glazerman, Mayer, &amp; Decker, 2006</xref>; <xref ref-type="bibr" rid="bibr29-0022487112454437">Henry et al., 2012</xref>; <xref ref-type="bibr" rid="bibr71-0022487112454437">Xu, Hannaway, &amp; Taylor, 2011</xref>). Teach for America teachers only make up a tiny fraction of all teachers in any given state. For instance, Teach for America teachers only accounted for 0.3% of North Carolina teachers in academic year 2007-2008 (<xref ref-type="bibr" rid="bibr31-0022487112454437">Henry, Thompson, Bastian, et al., 2010</xref>). Consequently, their omission would not likely greatly alter TPP estimates of effectiveness. However, for completeness and a sense of fairness, it may be considered advisable to include all other TPP, large and small, to provide a comprehensive view of traditional and alternative preparation programs from in-state and out-of-state rather than just comparing the in-state TPP with each other.</p>
<p>Selection decisions form the foundation of incorporating student test scores into TPP evaluations. There are many small decisions that can have significant impacts of the comprehensiveness and fairness of the TPP evaluation. The decisions concerning how much and which of the available data to be used will affect the choices for estimation, discussed next.</p>
</sec>
</sec>
<sec id="section8-0022487112454437">
<title>Estimation Decisions</title>
<p>With the selection decisions made, those interested in incorporating student outcome measures into the evaluation of TPPs will need to decide what type of analytical models will be used to produce quantitative estimates of TPP effectiveness. The goal for the estimation process is to isolate the effects of individual teachers on the test scores of their students. A central guiding criterion should be to choose an analytic approach that neither benefits nor adversely affects the TPP due to forces beyond the control of the programs, such as the choice of the type of schools in which their graduates choose to teach or the students assigned to the classes that they teach. However, to hold the programs accountable for the effectiveness of their graduates, it is important that the TPP be evaluated on both of the processes that they control that can affect teacher effectiveness: (a) the selection of candidates into the TPP and (b) the preparation provided to the teacher candidates. For this reason and consistent with the existing literature evaluating teacher effectiveness, we will not propose parsing the effects of selection into the TPP and preparation by the TPP, but, rather, focus this section on the decisions necessary to estimate the combined effects of selection and preparation.</p>
<p>The two estimation decisions require (a) choosing the analytic approach that will be used to calculate the estimates of teacher effectiveness and (b) selecting the specific statistical model that will be used to produce the estimates of TPP effectiveness. All the analytical models that are currently in use or under discussion use the differences between students’ current test scores and prior test scores rather than the levels of their test scores to measure effectiveness. Focusing on student test score gains rather than the students’ current scores concentrates attention on the influence of a particular teacher during the year that she or he teaches the students and avoids penalizing teachers who teach students who have lower test scores when they enter her or his classroom. Although far from inclusive of all possible approaches, researchers have typically taken one of three broad analytic approaches to estimating TPP effectiveness (explanations to follow): (a) student growth model, (b) Value-Added Models (VAMs) that estimate the effectiveness of individual teachers and then averages individual teacher estimates for each TPP, or (c) VAMs that estimate each TPP’s effectiveness directly. In the remainder of this section, we will present an overview of the three types of models and the associated estimation decisions. In the appendix in the online supplement located on the website referenced in the abstract, you will find a more detailed explanation of some of the approaches presented.</p>
<sec id="section9-0022487112454437">
<title>Student Growth Models</title>
<p>Student growth models generally refer to fairly straightforward models that attribute the total difference between a student’s current test score and a prior test score to the teacher who taught the student the particular subject being tested during that academic year (<xref ref-type="bibr" rid="bibr2-0022487112454437">Auty et al., 2008</xref>). In practice, student growth models typically use student test scores to classify students into performance categories using predetermined proficiency standards (<xref ref-type="bibr" rid="bibr27-0022487112454437">Goldschmidt et al., 2005</xref>). Generally speaking, in these models, a numerical student growth estimate based on the change in each student’s performance category is generated for each student, and these estimates are then aggregated by the teacher’s TPP to form a comparison of TPPs. To date, two types of student growth models have been or are planned to be used to evaluate TPPs: Value Table/Transition growth models and SGP Models.<sup><xref ref-type="fn" rid="fn2-0022487112454437">2</xref></sup> Florida is the only state known to have used a student growth model to estimate TPP effectiveness. Florida used a value table model (see the appendix in the online supplement located on the website referenced in the abstract for more detail on Florida’s use of non-value-added growth models to estimate TPP effectiveness).</p>
<p>The attraction of using the value table approach for evaluating teachers or TPPs is its simplicity and transparency. Because it does not involve highly technical or sophisticated statistical modeling, virtually anyone can calculate and track student growth (<xref ref-type="bibr" rid="bibr9-0022487112454437">Buzick &amp; Laitusis, 2010</xref>). Still, this approach has some potentially serious limitations. In particular, this growth model does not account for student, classroom, or school characteristics that influence students’ test score gains. If these characteristics are not balanced across teachers from different TPPs, there is a real danger that TPP effectiveness estimates will, for instance, incorrectly attribute differences in the types of students and schools in which graduates of a certain TPP teach to the TPP. If a particular TPP’s graduates serve students less likely to make gains whereas another TPP’s graduates serve students more likely to make higher gains, the differences in students taught by the graduates of these TPPs can be attributed to the TPP incorrectly (<xref ref-type="bibr" rid="bibr33-0022487112454437">Horner, 2009</xref>; see for additional information on the limitation of value tables for estimating student growth as well as for estimating teacher and school effectiveness).</p>
<p>To date, the Florida results are the only widely known use of a non-value-added student growth model to estimate TPP effectiveness. However, Massachusetts’ RttT Phase 2 application (section (D)(2)) explicitly proposed using another non-value-added growth model, a SGP model, to evaluate teacher effectiveness as part of the state’s evaluation of teacher effectiveness using student growth (<xref ref-type="bibr" rid="bibr57-0022487112454437">The State of Massachusetts, 2010</xref>). Importantly, the language Massachusetts uses to describe their plan to link “student growth” to TPPs (see section (D)(4)) suggests the state will use the results from the SGP as part of the state’s TPP evaluation (see the appendix in the online supplement located on the website referenced in the abstract for more details about SGP).<sup><xref ref-type="fn" rid="fn3-0022487112454437">3</xref></sup></p>
<p>The SGP approach starts to address some of the limitations of the value table approach. By estimating student growth relative to their “academic peers,” this approach may control for some of the student background and other contextual conditions that may influence the likelihood that a student makes learning gains. However, the research on the SGP approach does not explicitly test the degree to which the student characteristics or context influence TPP effectiveness estimates. If TPPs will ultimately be held accountable for the quality of their teachers based on these estimates, these issues must be addressed, as otherwise, a TPP may be held accountable for factors beyond their control (<xref ref-type="bibr" rid="bibr64-0022487112454437">Tekwe et al., 2004</xref>). It is also important to point out that the main proponent of the SGP method recommends that states use this approach descriptively, not for causal attribution or punitive actions (<xref ref-type="bibr" rid="bibr4-0022487112454437">Betebenner, 2009a</xref>, <xref ref-type="bibr" rid="bibr5-0022487112454437">2009b</xref>).</p>
</sec>
<sec id="section10-0022487112454437">
<title>VAMs</title>
<p>A VAM is a growth model that uses a student’s prior test performance and, in most cases, other student, classroom, and school variables to estimate student learning gains. The difference between non-VAM and VAMs is in how they attempt to divvy up responsibility for gains and losses in student test scores. Student growth models attribute all of the change in each student test scores or classification categories (proficient or below expectations, for example) to the student’s teacher. VAMs attempt to apportion the total growth between the different factors that have been shown to influence student test scores, including the students and their families, the make up of their classes, their schools, and their teachers. These models aim to directly estimate the unique contribution of teachers, schools, and TPP to student achievement (<xref ref-type="bibr" rid="bibr8-0022487112454437">Braun, 2005</xref>). These models often include additional factors that influence student achievement in an attempt to disentangle the unique effect attributable to teachers or their TPPs. As noted earlier, there are two types of value-added modeling approaches common in practice: (a) VAMs that directly estimate a TPP’s effectiveness and (b) VAMs that first estimate individual teacher effectiveness, then aggregate these effects to the TPP level.</p>
<p>Next, we lay out the aggregation and direct estimation of TPPs approaches. Before those explanations, it is important to mention the second layer of estimation decision making: the structure of the statistical model. Numerous variations in the structure of the VAM have been developed in recent years. Researchers have frequently implemented three distinct types of value-added statistical models to assess the effects of teachers on student achievement: (a) year-to-year VAMs with controls for student, classroom, and school characteristics, (b) year-to-year value-added fixed effects models, and (c) multiple-year VAMs (<xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al., 2009</xref>; <xref ref-type="bibr" rid="bibr49-0022487112454437">Sanders &amp; Horn, 1994</xref>, <xref ref-type="bibr" rid="bibr51-0022487112454437">1997</xref>). In theory, each of these models can be used with either the aggregation approach or the direct estimation approach. The year-to-year VAMs with controls for student, classroom, and school characteristics are often implemented using multilevel models to apportion the variability in test scores to three levels: students, classrooms, and schools. The year-to-year fixed effects models can be implemented with fixed effects at the student, teacher, or school levels. For example, school fixed effects models allow the analyst to control for all non-time-varying characteristics of the school because the teachers are compared only with other teachers within the school. However, what this means substantively is that school fixed effects models set teaching within the school as the standard for judging a teacher’s effectiveness, rather than teaching within the state as the standard. This may distort the effectiveness estimates between teachers at higher performing schools and those at lower performing schools, deflate the effectiveness of teachers at the former and inflate the effectiveness at the later. To the best of our knowledge, the year-to-year VAM with student fixed effects has not been used to evaluate TPP effectiveness, but the year-to-year VAM with school fixed effects has been used for TPP evaluation.</p>
</sec>
<sec id="section11-0022487112454437">
<title>VAMs: Aggregation Approach</title>
<p>In the aggregation approach to value-added modeling, the VAM is used to generate a teacher effectiveness estimate for each teacher. Then, these teacher estimates are averaged for all the teachers from each TPP to estimate the effectiveness of that TPP.</p>
<p>The process of estimating TPP effectiveness using the aggregation approach begins with selection of one of the three specific statistical model types listed above. Although a variety of VAMs could be used with the aggregation approach, in practice, states have only used <xref ref-type="bibr" rid="bibr49-0022487112454437">Sanders and Horn’s (1994)</xref> multiple-year value-added (mixed) model with the aggregation approach for estimating TPP effectiveness, which is commonly referred to EVAAS and formerly as TVAAS (Tennessee Value-Added Assessment System). The EVAAS value-added statistical model is a repeated measures, mixed model that uses all available tests scores from the past 5 years to estimate each teacher’s contribution to growth in a student’s test scores (<xref ref-type="bibr" rid="bibr3-0022487112454437">Ballou, Sanders, &amp; Wright, 2004</xref>; <xref ref-type="bibr" rid="bibr66-0022487112454437">TSBOE, 2010</xref>; <xref ref-type="bibr" rid="bibr70-0022487112454437">Wright et al., 2010</xref>). EVAAS estimates of teachers’ effectiveness are based on the extent to which their students consistently exceed or fall below the district average gains for their grade and subject (<xref ref-type="bibr" rid="bibr3-0022487112454437">Ballou et al., 2004</xref>). Once calculated, the teacher effects are averaged for all the teachers from a TPP and the averages are used to compare the performance of TPPs. Similar to some of the non-VAMs discussed previously, a major criticism of the EVAAS model is that it does not include other variables such as student, classroom, or school characteristics that may also affect student test scores. The extent to which these variables are adequately controlled by the EVAAS model which requires at least three prior test scores for every student in the database is only beginning to be empirically investigated. The complexity of the model also has led some to raise concerns about transparency (see the appendix in the online supplement located on the website referenced in the abstract for more information about the EVAAS model).</p>
</sec>
<sec id="section12-0022487112454437">
<title>VAMs: Direct Estimation Approach</title>
<p>The final approach used by researchers to estimate TPP effectiveness is the VAM which directly estimates TPP effectiveness. This direct estimation approach departs from previously described value-added approaches by adding a TPP indicator variable for each TPP in the analysis to the model. The TPP indicator variable designates whether a teacher was or was not prepared by a particular TPP.<sup><xref ref-type="fn" rid="fn4-0022487112454437">4</xref></sup> In this way, the effect of each TPP is estimated just like any other dichotomous independent variable in the model. That is, the coefficients on each TPP indicator variables provide estimates of the magnitude of the average gain (or loss) that students received on their test scores when taught by teachers from a particular TPP, adjusted for all other variables in the model. The magnitude of gain (or loss) is compared with students of teachers who were not prepared by any of the TPPs for which there is an indicator variable. For example, the models can be estimated with only one TPP indicator variable included in the analysis, which would compare teachers from the included TPP with all other teachers of the students with test scores included in the analysis. Researchers in North Carolina (<xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>), New York City (<xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al., 2009</xref>), and Louisiana (<xref ref-type="bibr" rid="bibr39-0022487112454437">Noell, 2006</xref>) have used this approach using year-to-year VAMs with controls (and in some cases with fixed effects).</p>
<p>The choice of the omitted or reference group for the analysis poses some challenges. To calculate TPP effectiveness for each program in a state, the TPPs must each be compared with each other, teachers from other preparation routes, or some other group such as novice teachers from the TPPs to more experienced teachers. Regardless, the choice of reference group has implications for interpretation of the quantitative estimates of TPP effectiveness. Consequently, researchers who use a VAM approach with direct estimation must thoughtfully choose a reference group against which the other TPPs are compared and think carefully about how to interpret the results—See the appendix in the online supplement located on the website referenced in the abstract for a detailed analysis of how <xref ref-type="bibr" rid="bibr43-0022487112454437">Noell and colleagues (2008</xref>; see also <xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle et al., 2010</xref>; <xref ref-type="bibr" rid="bibr40-0022487112454437">Noell &amp; Burns, 2006</xref>; <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell et al., 2007</xref>; and <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>) handled these issues.</p>
<p>Unlike the EVAAS model described previously, the work evaluating TPPs in North Carolina and Louisiana, and New York City which used VAM with direct estimation incorporated a wide array of controls at the student, classroom, and school levels (<xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al., 2009</xref>; <xref ref-type="bibr" rid="bibr30-0022487112454437">Henry et al., 2011</xref>; <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al., 2010</xref>; <xref ref-type="bibr" rid="bibr39-0022487112454437">Noell, 2006</xref>).<sup><xref ref-type="fn" rid="fn5-0022487112454437">5</xref></sup> These researchers use rich sets of covariates to isolate the unique contribution of each TPP on student achievement. Most of the variables used in the research on Louisiana, North Carolina, and New York City reflect a common set of student demographics and background characteristics (e.g., free lunch, disability, and limited English proficiency status), classroom peer characteristics, and school characteristics derived from data frequently found in state data systems (see the appendix in the online supplement located on the website referenced in the abstract for more details). As <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd and colleagues (2009)</xref> noted and as we pointed out above, programs “supply high quality teachers by a combination of recruitment and selection of potentially excellent teaching candidates and by adding value to teaching ability” (p. 429). The analytical models attempt to hold TPPs accountable for both selection into the program and preparation by the program and therefore, the models omit most teacher characteristics other than the teachers’ TPP to avoid bias in the estimates of effectiveness. In other words, the models do not include variables that have been used in other studies to examine teacher quality such as a teacher’s SAT score or holding a graduate degree because all factors that may influence or be influenced by a teacher’s selection into, and graduation from, a particular TPP should be captured in the TPP indicator variable. <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell and colleagues (2007)</xref> included a control for teacher absences, whereas <xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle and colleagues (2010)</xref> and <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, et al. (2010)</xref> controlled for teacher experience and in the latter case, for teaching out-of-field. These variables, if uncontrolled, could bias the estimates of TPP effectiveness due to factors beyond the control of the programs, such as teachers being assigned to teach out-of-field.<sup><xref ref-type="fn" rid="fn6-0022487112454437">6</xref></sup></p>
<p>Hopefully, the description of estimation decisions that are required has illuminated the fact that these technical decisions have important policy implications that bear directly on what evaluators intend to hold TPPs accountable for. The choice of non-VAMs including the Value Table/Transition growth model or the SGP model implies that teachers and TPP are accountable for all the changes in student test scores from one year to the next. The VAMs using aggregation and VAMs using direct estimation are efforts to isolate the effects of teachers from other factors affecting student test score gains, including measurement error. In the most common VAM using aggregation, EVAAS, the full complement of available test scores for each student are used to isolate teachers’ effects. In the VAMs using direct estimation, additional student, classroom, school, and some teacher characteristics, specifically, those deemed to influence student test scores and lie beyond the control of a TPP, are included. In truth, either the aggregation or the direct estimation of TPP effects can incorporate covariates or fixed effects, but the choices involve assumptions about the influences on student test scores, what TPPs are to be held accountable for, and to what the effects of TPPs are to be compared. Does a state want all TPPs held to a common, statewide standard or should the teachers be compared only with other teachers within their same schools (school fixed effects) to control for unmeasured school influences on student test scores and the nonrandom pattern of graduates of TPP programs taking positions in different types of schools? Does a state want to compare novice graduates of each TPP with more experienced teachers or with other novice teachers? If the latter is preferred, should all in-state TPP programs be compared with the same group (e.g., all novice teachers not prepared by in-state TPP programs), to each other, or to all teachers not prepared by the TPP? Ultimately, decisions made in answering these questions will define how the quantitative estimates of the TPP effects on student test scores should be interpreted. Reporting decisions, discussed next, should be sensitive to the implications of the estimation decisions to accurately convey the meaning of estimates of TPP effects on student test scores.</p>
</sec>
</sec>
<sec id="section13-0022487112454437">
<title>Reporting Decisions</title>
<p>As part of their RttT accountability responsibilities, RttT recipient states plan to publicly release the results of their TPP evaluations in the form of report cards. This implies that the reports releasing the information should be easily digestible by the public at large yet also provide sufficient detail to stakeholders about the magnitude of differences in TPP effectiveness. Prior reports vary in the way they present the findings, with some researchers reporting the actual numerical estimates of TPP effectiveness, others grouping programs into performance bands, and still others focused on whether each TPP meets some performance threshold (e.g., Are the effects of a TPP significantly different from the “comparison group” using traditional tests of statistical significance?).</p>
<p>TPP assessments in Louisiana, North Carolina, and New York City directly report the TPP effectiveness estimates. The Louisiana research typically presents the effect estimates along with a 68% confidence interval (<xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle et al., 2010</xref>; <xref ref-type="bibr" rid="bibr40-0022487112454437">Noell &amp; Burns, 2006</xref>; <xref ref-type="bibr" rid="bibr42-0022487112454437">Noell et al., 2007</xref>; <xref ref-type="bibr" rid="bibr43-0022487112454437">Noell et al., 2008</xref>). <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd and colleagues (2009)</xref>, who have examined New York City TPPs, simply present a graph that plots effect sizes along with confidence intervals; however, they do not identify individual TPPs.</p>
<p><xref ref-type="bibr" rid="bibr30-0022487112454437">Henry and colleagues (2011)</xref> also reported quantitative estimates for North Carolina TPPs. However, because the tests used in these analyses were standardized to have a mean of zero and standard deviation of one, the TPP estimates are in standard deviation units and not easily interpretable. For this reason, they use information about test score standard deviations and average annual test score gains to convert some of these effects into “additional days of instruction” that assume learning occurs at the same daily rate throughout the school year. For example, an elementary mathematics student taught by a recent graduate of University of North Carolina-Asheville’s TPP was estimated to have the equivalent of approximately 7.5 days of additional schooling than she or he would have had if taught by another novice teacher prepared outside the University of North Carolina system (<xref ref-type="bibr" rid="bibr30-0022487112454437">Henry et al., 2011</xref>).</p>
<p>Because of the difficultly in interpreting the magnitude of an effect when presented as just a statistical estimate, TPP evaluations often report effect sizes or other metrics to aid in interpretation. Louisiana provides effect sizes that indicate how different new teachers from a specific TPP are from more experienced teachers. As stakeholders may also want to know how new teachers from one TPP compare with new teachers from other TPPs, Louisiana also classifies each TPP into one of five performance band levels based on the results of the analysis. These levels classify the programs based on their relative (to new and experienced teachers) outcomes. For example, a program “for which there is evidence that new teachers are more effective than experienced teachers” is coded to Level 1, whereas Level 5 is restricted to TPPs “whose effect estimate is statistically significantly below the mean for new teachers” (<xref ref-type="bibr" rid="bibr24-0022487112454437">Gansle et al., 2010</xref>). This approach provides lots of ease to interpret information. However, important information can be lost using this approach, namely, “Are some of the programs whose effect estimate is below the mean for new teachers better or worse than others?”<sup><xref ref-type="fn" rid="fn7-0022487112454437">7</xref></sup></p>
<p>Reporting the effects of TPPs involves many decisions. What is the best metric for reporting the magnitude of the estimated effects? Are “equivalent days of schooling,” standard deviation units, or the original scale scores most informative? Should programs be grouped into performance categories, tested for differences with one another, or tested for differences with a particular reference group, such as novice teachers who did not come from traditional in-state preparation programs at public institutions of higher education? As better methods of reporting information are developed, the formats and amount of information may be targeted to different audiences. Detailed graphical displays, perhaps like the “thermometer graphs,” used in North Carolina may be informative for the faculty and staff of individual TPP (<xref ref-type="bibr" rid="bibr30-0022487112454437">Henry et al., 2011</xref>), but state policy makers may value summary lists of TPPs by performance category more than detailed displays.</p>
</sec>
<sec id="section14-0022487112454437" sec-type="conclusions">
<title>Conclusion</title>
<p>The complexity behind estimating and reporting TPP effects on student test scores goes beyond setting up the sophisticated data systems that link program graduates to practicing teachers to student test scores. Although the RttT Fund and other federal initiatives urge greater accountability for publicly funded TPPs, the decisions required to tie TPP graduates to the achievement of their students must be made with an understanding of the consequences of each of the myriad decisions involved. A series of selection, estimation, and reporting decisions are required, including determining what tests teachers can reasonably be held accountable for, which teachers should be included and which are excluded, whether highly technical VAMs will produce the most accurate estimates of the effects of TPPs on student performance or whether more transparent and simpler methods should be preferred, and how much information to report. Understanding the options as well as their strengths and weaknesses along with open dialogue on the insights and consequences of certain decisions will lead to better estimates of TPP effectiveness.</p>
<p>From the analysis of the descriptions of the various state systems in place and under development, we can discern a few important conclusions about the current efforts to include measures of TPP effects on student test scores. First, the overall goal of most of these efforts to date has been to obtain unbiased estimates of the overall effects of TPP on student achievement. The goal is causal inference, but without random assignment of teachers and students to schools and classes, numerous assumptions must be made (<xref ref-type="bibr" rid="bibr46-0022487112454437">Reardon &amp; Raudenbush, 2009</xref>) for us to believe that the causal effects of TPP have been isolated. However, as the purpose of the exercise is not to deliberatively manipulate (or assign) the “treatment,” in this case the TPP, to be able to estimate the effect on a specified study population, but rather to study the effects in situ better and worse choices can be made. In place of the strict standard of random assignment, we can establish criteria that guide the selection of better choices. We believe that several criteria should be considered as states move forward with TPP effect estimates: (a) accuracy, (b) fairness, (c) transparency, and (d) inclusiveness.</p>
<sec id="section15-0022487112454437">
<title>Accuracy</title>
<p>First, we must accept that we do not have any means to measure or estimate a TPP program’s <italic>true effect</italic> on student test scores. Therefore, we must rely on traditional criteria for our efforts such as reliability and validity. Effect estimates that are unreliable can produce year-to-year swings in program effect estimates that would provide little guidance about where performance problems exist. Although the true effect is illusive, it will be important to validate the effects on student test scores estimated for TPP evaluations with other, independent measures of high quality instruction by TPP graduates, such as direct observation of teachers using observation instruments with desirable psychometric properties and student surveys that have been shown to measure high quality instruction or other positive attributes of effective teachers such as building and maintaining warm relationship with students.</p>
</sec>
<sec id="section16-0022487112454437">
<title>Fairness</title>
<p>One way of viewing fairness is that the TPP are neither advantaged nor penalized by decisions of their graduates who are beyond their control. Preparing teachers who choose to teach in challenging schools or challenging students should not be a factor that affects the TPP effect estimates. All VAMs that are being used for the evaluation of TPPs attempt to isolate the effects of teachers and remove the influence on student test scores that are beyond the control of TPP. Some VAMs attempt to do this directly with student, classroom, and school covariates, and others do it with extensive student test scores, but it does not seem reasonable to expect that differences in students or schools have no bearing on teacher or TPP effectiveness. It is more difficult to discern whether the non-VAMs deal with these choices in a way that is fair to TPPs whose graduates choose more challenging students or schools and should along with the VAMs be investigated further in this regard.</p>
</sec>
<sec id="section17-0022487112454437">
<title>Transparency</title>
<p>Transparency is an obvious, but difficult, criterion to satisfy because the most transparent approaches may fail the tests of accuracy and fairness, whereas approaches that seem accurate and fair require more complex modeling procedures that are opaque to many teachers and policy makers. Clear and understandable explanations will need to be offered when complex and sophisticated estimation approaches are used. Perhaps this suggests that the reporting burdens in terms of clarity and ease of interpretation will be greater when using VAMs than the non-VAMs. Ultimately, there may be a trade-off between accuracy and fairness on one hand and transparency on the other.</p>
</sec>
<sec id="section18-0022487112454437">
<title>Inclusiveness</title>
<p>Finally, the inclusiveness criterion suggests that the current testing regime that has been put into place largely to meet NCLB requirements may fall short in the longer term and omit significant subprograms of larger traditional TPP, such as early education programs or special education. In addition, incorporating student test scores into the evaluation of TPP is only one of many measures that could be used to measure the effects of the graduates of these programs on student outcomes. Student engagement, graduation rates, and direct measures of high quality instruction are but a few of the potential measures that could be added to TPP evaluation for a more inclusive and comprehensive perspective on the program’s effects on students. Although we believe it is reasonable to first attempt to evaluate TPPs utilizing the tests that are currently available, attention should be given to incorporating a broader set of teacher performance measures.</p>
<p>It is also important to note that the current efforts to incorporate student test scores into TPP evaluation partially fulfill one of the four purposes for evaluation, accountability/oversight, and do not directly or necessarily address the other three purposes for which evaluations are conducted: program improvement, assessment of merit and worth, or knowledge development (<xref ref-type="bibr" rid="bibr35-0022487112454437">Mark, Henry, &amp; Julnes, 2000</xref>). A TPP’s overall effect on student test scores does not directly provide information concerning program improvement. To begin to perform this function, the effects of selection and retention processes will need to be distinguished from effects of the actual preparation processes to determine which of these processes are responsible for TPPs achieving their effects. Is the selection of high quality candidates or the preparation program itself that has the greatest effect on teachers’ effectiveness or a combination of both? In addition, the extent to which variation in the programmatic components of TPP systematically relate to variations in teacher effectiveness will need to be explored. This has begun with research by <xref ref-type="bibr" rid="bibr7-0022487112454437">Boyd et al. (2009</xref>; see also <xref ref-type="bibr" rid="bibr6-0022487112454437">Boyd et al., 2006</xref>), who found that student test score gains associated with specific TPP components, such as exposure to specific content or the extent of clinical experiences. This type of research is in its nascent stage and, unfortunately, the data that will be needed to expand this effort is not currently available in many longitudinal data systems.</p>
<p>The overall assessment of merit and worth of TPPs requires much broader information than the effects of public in-state TPPs on student test scores. Assessing the true merit of TPP and their full worth to society requires understanding the counterfactual: What would student outcomes be in the absence of the TPPs being evaluated? How would teachers be prepared in the absence of these programs? How much would students learn and be able to do if the TPPs being evaluated did not exist? At a minimum, this suggests that equal attention be given to the effectiveness of all alternative TPP such as Teach For America, Visiting International Faculty, each of the various alternative or lateral entry programs supplying teachers in the state (whether they are located in the state), and state-to-state reciprocal licensing arrangements. However, this by itself will not be entirely adequate for assessing merit and worth of the traditional TPP at public and private institutions because we do not know how the teacher labor market would respond in the absence of these programs or about other outcomes of public schools such as graduation or participation as a citizen that are not attended to in the test scores.</p>
<p>Finally, we have much ground to cover for knowledge development, some of which has already been alluded to above. The empirical work to date has established “proof of concept” for incorporating student test scores into the evaluation of TPP. It shows that a sufficient signal can be found in student test scores to reliably attribute effects to individual TPP even with the “noise” of confounding variables and nonrandom sorting of teachers and students into schools and classrooms. However, we have just begun to understand some of the differences in the approaches and models. Much technical work remains for assessing the accuracy and fairness of current methods and improving their transparency as well as developing measures that will increase the inclusiveness of TPP effects on valued student outcomes that go beyond test scores.</p>
</sec>
</sec>
</body>
<back>
<ack><p>The authors gratefully acknowledge helpful comments and support from Alisa Chapman, Alan Mabe, Erskine Bowles, the Council of Education Deans of the University of North Carolina, Stephanie L. Knight, and two anonymous reviewers.</p></ack>
<fn-group>
<fn fn-type="other">
<label>Authors’ Note</label>
<p>The authors take full responsibility for the research, interpretations, and conclusions included in the manuscript.</p>
</fn>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0022487112454437">
<label>1.</label>
<p>North Carolina offers end-of-course tests in Algebra I, Algebra II, biology, chemistry, English I, geometry, physical science, physics, U.S. history, and civics and economics courses. Student performance in English I, Algebra I and II, geometry, biology, chemistry, physics, and physical science were used in these analyses.</p>
</fn>
<fn fn-type="other" id="fn2-0022487112454437">
<label>2.</label>
<p>In response to No Child Left Behind and other initiatives, states have developed several types of growth models, including growth to proficiency models, value table/transition models, projection models, and student growth percentile (SGP) models. For more information on these approaches, see <xref ref-type="bibr" rid="bibr45-0022487112454437">O’Malley et al., 2009</xref>.</p>
</fn>
<fn fn-type="other" id="fn3-0022487112454437">
<label>3.</label>
<p>Whether, or to what extent, Massachusetts will use the SGP scores to estimate teacher preparation program effectiveness is unknown by the authors at this time.</p>
</fn>
<fn fn-type="other" id="fn4-0022487112454437">
<label>4.</label>
<p>An indicator variable is a variable that takes on a value of 1 when, for instance, a person has a characteristic of interest and 0 when a person does not have a characteristic. For example, men would be coded 1 on a male indicator variable and women 0.</p></fn>
<fn fn-type="other" id="fn5-0022487112454437">
<label>5.</label>
<p>Proponents of the Education Value-Added Assessment System/Tennessee Value-Added Assessment System (EVAAS/TVAAS) argue and demonstrate (<xref ref-type="bibr" rid="bibr3-0022487112454437">Ballou, Sanders, &amp; Wright, 2004</xref>) that the use of multiple student test scores in the EVAAS/TVAAS models effectively subsumes many of these unobserved factors.</p>
</fn>
<fn fn-type="other" id="fn6-0022487112454437">
<label>6.</label>
<p>In an additional analysis, <xref ref-type="bibr" rid="bibr32-0022487112454437">Henry, Thompson, Fortner, Zulli, and Kersham (2010)</xref> also incorporated teacher level covariates into some of their analyses, including SAT scores, high school rank, and high school grade point average, but these are initial attempts to parse the effects of preparation from selection and are not considered estimates of the overall effects of TPPs.</p>
</fn>
<fn fn-type="other" id="fn7-0022487112454437">
<label>7.</label>
<p>Note that in the Louisiana approach, placement in these levels is not always tied to statistical significance. In other words, TPP can be labeled as comparatively over or under achieving although they are not reliably different from the average TPP.</p>
</fn>
</fn-group>
</notes>
<bio>
<title>About the Authors</title>
<p><bold>Gary T. Henry</bold> holds the Duncan MacRae ’09 and Rebecca Kyle MacRae Professorship of Public Policy in the Department of Public Policy and serves as a fellow with the Carolina Institute for Public Policy and the Frank Porter Graham Institute for Child Development at the University of North Carolina at Chapel Hill. He specializes in education policy, educational evaluation, teacher quality research, and quantitative research methods.</p>
<p><bold>David C. Kershaw</bold> is an assistant professor in the Department of Political Science at Slippery Rock University of Pennsylvania. Dr. Kershaw’s research areas include education policy, political behavior, and research methods.</p>
<p><bold>Rebecca A. Zulli</bold>, PhD, is project manager for the ATOMS Project in the Department of Elementary Education at NC State University. Her research focuses on the evaluation of educational innovation and intervention in the areas of teacher preparation, professional development, and student achievement.</p>
<p><bold>Adrienne A. Smith</bold>, PhD, is a research associate at Horizon Research, Inc. Her research focuses on educational policy, program evaluation, and research methods.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Aaronson</surname><given-names>D.</given-names></name>
<name><surname>Barrow</surname><given-names>L.</given-names></name>
<name><surname>Sander</surname><given-names>W.</given-names></name>
</person-group> (<year>2003</year>). <source>Teachers and student achievement in the Chicago Public High Schools</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Federal Reserve Bank of Chicago</publisher-name>. (Working Paper Series: WP-02–28)</citation>
</ref>
<ref id="bibr2-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Auty</surname><given-names>W.</given-names></name>
<name><surname>Bielawski</surname><given-names>P.</given-names></name>
<name><surname>Deeter</surname><given-names>T.</given-names></name>
<name><surname>Hirata</surname><given-names>G.</given-names></name>
<name><surname>Hovanetz-Lassila</surname><given-names>C.</given-names></name>
<name><surname>Rheim</surname><given-names>J.</given-names></name>
<name><surname>. . .Williams</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <source>Implementer’s guide to growth models</source>. Retrieved from The Council of Chief of State School Officers website: <ext-link ext-link-type="uri" xlink:href="http://www.ccsso.org/Documents/2008/Implementers_Guide_to_Growth_2008.pdf">http://www.ccsso.org/Documents/2008/Implementers_Guide_to_Growth_2008.pdf</ext-link></citation>
</ref>
<ref id="bibr3-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ballou</surname><given-names>D.</given-names></name>
<name><surname>Sanders</surname><given-names>W.</given-names></name>
<name><surname>Wright</surname><given-names>P.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Controlling for student background in value-added assessment for teachers</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>29</volume>(<issue>1</issue>), <fpage>37</fpage>-<lpage>65</lpage>.</citation>
</ref>
<ref id="bibr4-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Betebenner</surname><given-names>D.</given-names></name>
</person-group> (<year>2009a</year>). <article-title>Norm- and criterion-referenced student growth</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>24</volume>, <fpage>42</fpage>-<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr5-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Betebenner</surname><given-names>D.</given-names></name>
</person-group> (<year>2009b</year>). <source>A primer on student growth percentiles</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.cde.state.co.us/cdedocs/Research/PDF/Aprimeronstudentgrowthpercentiles.pdf">http://www.cde.state.co.us/cdedocs/Research/PDF/Aprimeronstudentgrowthpercentiles.pdf</ext-link></citation>
</ref>
<ref id="bibr6-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boyd</surname><given-names>D. J.</given-names></name>
<name><surname>Grossman</surname><given-names>P.</given-names></name>
<name><surname>Lankford</surname><given-names>H.</given-names></name>
<name><surname>Loeb</surname><given-names>S.</given-names></name>
<name><surname>Michelli</surname><given-names>N. M.</given-names></name>
<name><surname>Wyckoff</surname><given-names>J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Complex by design: Investigating pathways into teaching in New York City schools</article-title>. <source>Journal of Teacher Education</source>, <volume>57</volume>, <fpage>155</fpage>-<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr7-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boyd</surname><given-names>D. J.</given-names></name>
<name><surname>Grossman</surname><given-names>P.</given-names></name>
<name><surname>Lankford</surname><given-names>H.</given-names></name>
<name><surname>Loeb</surname><given-names>S.</given-names></name>
<name><surname>Michelli</surname><given-names>N. M.</given-names></name>
<name><surname>Wyckoff</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Teacher preparation and student achievement</article-title>. <source>Education Evaluation and Policy Analysis</source>, <volume>31</volume>, <fpage>416</fpage>-<lpage>440</lpage>.</citation>
</ref>
<ref id="bibr8-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Braun</surname><given-names>H. I.</given-names></name>
</person-group> (<year>2005</year>). <source>Using student progress to evaluate teachers: A primer on value-added models</source> (Tech. Rep.). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr9-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Buzick</surname><given-names>H. M.</given-names></name>
<name><surname>Laitusis</surname><given-names>C. C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Using growth for accountability: Measurement challenges for students with disabilities and recommendations for research</article-title>. <source>Educational Researcher</source>, <volume>39</volume>, <fpage>537</fpage>-<lpage>544</lpage>.</citation>
</ref>
<ref id="bibr10-0022487112454437">
<citation citation-type="gov">
<collab>California Department of Education</collab>. (<year>2011</year>). <source>2010 STAR Test Results: About 2010 STAR</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://star.cde.ca.gov/star2010/aboutSTAR.asp">http://star.cde.ca.gov/star2010/aboutSTAR.asp</ext-link></citation>
</ref>
<ref id="bibr11-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clotfelter</surname></name>
<name><surname>Ladd</surname><given-names>H.</given-names></name>
<name><surname>Vigdor</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Teacher credentials and student achievement: Longitudinal analysis with student fixed effects</article-title>. <source>Economics of Education Review</source>, <volume>26</volume>(<issue>6</issue>), <fpage>673</fpage>-<lpage>682</lpage>.</citation>
</ref>
<ref id="bibr12-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clotfelter</surname></name>
<name><surname>Ladd</surname><given-names>H.</given-names></name>
<name><surname>Vigdor</surname><given-names>J.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Teacher credentials and student achievement in high school: A cross-subject analysis with student fixed effects</article-title>. <source>The Journal of Human Resources</source>, <volume>45</volume>(<issue>3</issue>), <fpage>655</fpage>-<lpage>681</lpage>.</citation>
</ref>
<ref id="bibr13-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Coleman</surname><given-names>J. S.</given-names></name>
<name><surname>Campbell</surname><given-names>E. Q.</given-names></name>
<name><surname>Hobson</surname><given-names>C. J.</given-names></name>
<name><surname>McPartland</surname><given-names>J.</given-names></name>
<name><surname>Mood</surname><given-names>A. M.</given-names></name>
<name><surname>Weinfeld</surname><given-names>F. D.</given-names></name>
<name><surname>York</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1966</year>). <source>Equality of educational opportunity</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>USGPO</publisher-name>.</citation>
</ref>
<ref id="bibr14-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Crowe</surname><given-names>E.</given-names></name>
</person-group> (<year>2010</year>). <source>Measuring what matters: A stronger accountability model for teacher education</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Center for American Progress</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.americanprogress.org/issues/2010/07/pdf/teacher_accountability.pdf">http://www.americanprogress.org/issues/2010/07/pdf/teacher_accountability.pdf</ext-link></citation>
</ref>
<ref id="bibr15-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
<name><surname>Berry</surname><given-names>B.</given-names></name>
<name><surname>Thoreson</surname><given-names>A.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Does teacher certification matter? Evaluating the evidence</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>23</volume>(<issue>1</issue>), <fpage>55</fpage>-<lpage>77</lpage>.</citation>
</ref>
<ref id="bibr16-0022487112454437">
<citation citation-type="gov">
<collab>The District of Columbia</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr17-0022487112454437">
<citation citation-type="book">
<collab>Florida Department of Education</collab>. (<year>2005</year>). <source>FCAT Handbook: A resource for educators</source>. <publisher-loc>Tallahassee, FL</publisher-loc>: <publisher-name>Florida Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr18-0022487112454437">
<citation citation-type="web">
<collab>Florida Department of Education</collab>. (<year>2009a</year>). <source>20072008 FCAT learning gains results</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tampabay.com/blogs/gradebook/sites/tampabay.com.blogs.gradebook/files/images/typepad-legacy-files/54957.june-2009-teacher-quality-data.pdf">http://www.tampabay.com/blogs/gradebook/sites/tampabay.com.blogs.gradebook/files/images/typepad-legacy-files/54957.june-2009-teacher-quality-data.pdf</ext-link></citation>
</ref>
<ref id="bibr19-0022487112454437">
<citation citation-type="web">
<collab>Florida Department of Education</collab>. (<year>2009b</year>). <source>Memorandum: Data on program completers’ impact on K-12 student learning to meet continued approval requirements</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tampabay.com/blogs/gradebook/sites/tampabay.com.blogs.gradebook/files/images/typepad-legacy-files/54991.11-20-09teacher-preparation.pdf">http://www.tampabay.com/blogs/gradebook/sites/tampabay.com.blogs.gradebook/files/images/typepad-legacy-files/54991.11-20-09teacher-preparation.pdf</ext-link></citation>
</ref>
<ref id="bibr20-0022487112454437">
<citation citation-type="web">
<collab>Florida Department of Education</collab>. (<year>2009c</year>). <source>Overall performance of 2007-08 teacher preparation program completers teaching reading and mathematics Grade 4-10 during 2008-09</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tampabay.com/blogs/gradebook/sites/tampabay.com.blogs.gradebook/files/images/typepad-legacy-files/54991.tq_deans_list_0809-kh.pdf">http://www.tampabay.com/blogs/gradebook/sites/tampabay.com.blogs.gradebook/files/images/typepad-legacy-files/54991.tq_deans_list_0809-kh.pdf</ext-link></citation>
</ref>
<ref id="bibr21-0022487112454437">
<citation citation-type="web">
<collab>Florida Department of Education</collab>. (n.d.). <article-title>Florida End-of-Course (EOC) Assessments</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://fcat.fldoe.org/eoc/">http://fcat.fldoe.org/eoc/</ext-link></citation>
</ref>
<ref id="bibr22-0022487112454437">
<citation citation-type="gov">
<collab>The Florida Senate, Committee on Education Pre-K-12</collab>. (<year>2009</year>). <source>Teacher quality: Issue Brief 2010-313</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://archive.flsenate.gov/data/Publications/2010/Senate/reports/interim_reports/pdf/2010-313ed.pdf">http://archive.flsenate.gov/data/Publications/2010/Senate/reports/interim_reports/pdf/2010-313ed.pdf</ext-link></citation>
</ref>
<ref id="bibr23-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
<name><surname>Compton</surname><given-names>D. L.</given-names></name>
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Bryant</surname><given-names>J.</given-names></name>
<name><surname>Davis</surname><given-names>G. N.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Making “secondary intervention” work in a three-tiered responsiveness-to-intervention model: Findings from the first-grade longitudinal reading study of the national research center on learning disabilities</article-title>. <source>Reading and Writing</source>, <volume>21</volume>(<issue>4</issue>), <fpage>413</fpage>-<lpage>436</lpage>.</citation>
</ref>
<ref id="bibr24-0022487112454437">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Gansle</surname><given-names>K. A.</given-names></name>
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Knox</surname><given-names>R. M.</given-names></name>
<name><surname>Schafer</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2010</year>). <source>Value added assessment of teacher preparation in Louisiana: 2005-2006 to 2008-2009</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://regents.louisiana.gov/assets/docs/TeacherPreparation/2010VATechnical082610.pdf">http://regents.louisiana.gov/assets/docs/TeacherPreparation/2010VATechnical082610.pdf</ext-link></citation>
</ref>
<ref id="bibr25-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Glazerman</surname><given-names>S.</given-names></name>
<name><surname>Mayer</surname><given-names>D.</given-names></name>
<name><surname>Decker</surname><given-names>P.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Alternative routes to teaching: The impacts of Teach for America on student achievement and other outcomes</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>25</volume>, <fpage>75</fpage>-<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr26-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldhaber</surname><given-names>D. D.</given-names></name>
<name><surname>Brewer</surname><given-names>D. J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Does teacher certification matter? High school teacher certification status and student achievement</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>22</volume>(<issue>2</issue>), <fpage>129</fpage>-<lpage>145</lpage>.</citation>
</ref>
<ref id="bibr27-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Goldschmidt</surname><given-names>P.</given-names></name>
<name><surname>Roschewski</surname><given-names>P.</given-names></name>
<name><surname>Choi</surname><given-names>K.</given-names></name>
<name><surname>Auty</surname><given-names>W.</given-names></name>
<name><surname>Hebbler</surname><given-names>S.</given-names></name>
<name><surname>Blank</surname><given-names>R.</given-names></name>
<name><surname>Williams</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <source>Policymakers’ guide to growth models for school accountability: How do accountability models differ?</source> <collab>The Council of Chief of State School Officers</collab>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.ccsso.org/Documents/2009/Guide_to_United_States_2009.pdf">http://www.ccsso.org/Documents/2009/Guide_to_United_States_2009.pdf</ext-link></citation>
</ref>
<ref id="bibr28-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Greenwald</surname><given-names>R.</given-names></name>
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
<name><surname>Laine</surname><given-names>R. D.</given-names></name>
</person-group> (<year>1996</year>). <article-title>The effect of school resources on student achievement</article-title>. <source>Review of Educational Research</source>, <volume>66</volume>, <fpage>361</fpage>-<lpage>396</lpage>.</citation>
</ref>
<ref id="bibr29-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Henry</surname><given-names>G. T.</given-names></name>
<name><surname>Bastian</surname><given-names>K. C.</given-names></name>
<name><surname>Smith</surname><given-names>A. A.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Scholarships to recruit the “best and brightest” into teaching: Who is recruited, where do they teach, how effective are they, and how long do they stay?</article-title> <source>Educational Research</source>, <volume>41</volume>(<issue>3</issue>), <fpage>83</fpage>-<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr30-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Henry</surname><given-names>G. T.</given-names></name>
<name><surname>Thompson</surname><given-names>C. L.</given-names></name>
<name><surname>Bastian</surname><given-names>K. C.</given-names></name>
<name><surname>Fortner</surname><given-names>C. K.</given-names></name>
<name><surname>Kershaw</surname><given-names>D. C.</given-names></name>
<name><surname>Marcus</surname><given-names>J. V.</given-names></name>
<name><surname>Zulli</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2011</year>). <source>UNC Teacher Preparation Program Effectiveness Report</source>. <publisher-loc>Chapel Hill</publisher-loc>: <publisher-name>Carolina Institute for Public Policy, University of North Carolina at Chapel Hill</publisher-name>.</citation>
</ref>
<ref id="bibr31-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Henry</surname><given-names>G. T.</given-names></name>
<name><surname>Thompson</surname><given-names>C. L.</given-names></name>
<name><surname>Bastian</surname><given-names>K. C.</given-names></name>
<name><surname>Fortner</surname><given-names>C. K.</given-names></name>
<name><surname>Kershaw</surname><given-names>D. C.</given-names></name>
<name><surname>Purtell</surname><given-names>K. M.</given-names></name>
<name><surname>Zulli</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2010</year>). <source>Portal report: Teacher preparation and student test scores in North Carolina</source>. <publisher-loc>Chapel Hill</publisher-loc>: <publisher-name>Carolina Institute for Public Policy, University of North Carolina at Chapel Hill</publisher-name>.</citation>
</ref>
<ref id="bibr32-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Henry</surname><given-names>G. T.</given-names></name>
<name><surname>Thompson</surname><given-names>C. L.</given-names></name>
<name><surname>Fortner</surname><given-names>C. K.</given-names></name>
<name><surname>Zulli</surname><given-names>R. A.</given-names></name>
<name><surname>Kershaw</surname><given-names>D. C.</given-names></name>
</person-group> (<year>2010</year>). <source>The impact of teacher preparation on student learning in North Carolina public schools</source>. <publisher-loc>Chapel Hill</publisher-loc>: <publisher-name>Carolina Institute for Public Policy, University of North Carolina at Chapel Hill</publisher-name>.</citation>
</ref>
<ref id="bibr33-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Horner</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <source>Quantifying student growth: Analysis of the validity of applying growth models to the California Standards Test</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://digitallibrary.usc.edu/assetserver/controller/item/etd-Horner-2920.pdf;jsessionid=81F14E4D9DD8AB29986304A535E41942">http://digitallibrary.usc.edu/assetserver/controller/item/etd-Horner-2920.pdf;jsessionid=81F14E4D9DD8AB29986304A535E41942</ext-link></citation>
</ref>
<ref id="bibr34-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Kane</surname><given-names>T.</given-names></name>
<name><surname>Rockoff</surname><given-names>J.</given-names></name>
<name><surname>Staiger</surname><given-names>D.</given-names></name>
</person-group> (<year>2006</year>). <source>What does certification tell us about teacher effectiveness? Evidence from New York City</source>. NBER Working Paper 12155. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.nber.org/papers/w12155">http://www.nber.org/papers/w12155</ext-link></citation>
</ref>
<ref id="bibr35-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mark</surname><given-names>M. M.</given-names></name>
<name><surname>Henry</surname><given-names>G.T.</given-names></name>
<name><surname>Julnes</surname><given-names>G.</given-names></name>
</person-group> (<year>2000</year>). <source>Evaluation: an integrated framework for understanding, guiding, and improving public and nonprofit policies and programs</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr36-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Murawski</surname><given-names>W. W.</given-names></name>
<name><surname>Lochner</surname><given-names>W. W.</given-names></name>
</person-group> (<year>2011</year>).<article-title>Observing co-teaching: What to ask for, look for, and listen for</article-title>. <source>Intervention in School and Clinic</source>, <volume>46</volume>, <fpage>174</fpage>-<lpage>183</lpage>.</citation>
</ref>
<ref id="bibr37-0022487112454437">
<citation citation-type="book">
<collab>National Council for Accreditation of Teacher Education</collab>. (<year>2010</year>). <source>Professional Standards for the Accreditation of Teacher Preparation Institutions</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Council for the Accreditation of Teachers</publisher-name>.</citation>
</ref>
<ref id="bibr38-0022487112454437">
<citation citation-type="book">
<collab>National Research Council</collab>. (<year>2010</year>). <source>Preparing teachers: Building evidence for sound policy</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academies Press</publisher-name>.</citation>
</ref>
<ref id="bibr39-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
</person-group> (<year>2006</year>). <source>Technical report of: Assessing teacher preparation program effectiveness: A pilot examination of value added approaches (2006)</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.laregentsarchive.com/Academic/TE/technical_report.pdf">http://www.laregentsarchive.com/Academic/TE/technical_report.pdf</ext-link></citation>
</ref>
<ref id="bibr40-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Burns</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Value-added assessment of teacher preparation: An illustration of emerging technology</article-title>. <source>Journal of Teacher Education</source>, <volume>57</volume>, <fpage>37</fpage>-<lpage>50</lpage>.</citation>
</ref>
<ref id="bibr41-0022487112454437">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Burns</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2007</year>). <source>Value added teacher preparation assessment overview of 2006-07 study</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://regents.louisiana.gov/assets/docs/TeacherPreparation/NarrativeDescriptionof2006-07ValueAddedStudy10.24.07.pdf">http://regents.louisiana.gov/assets/docs/TeacherPreparation/NarrativeDescriptionof2006-07ValueAddedStudy10.24.07.pdf</ext-link></citation>
</ref>
<ref id="bibr42-0022487112454437">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Porter</surname><given-names>B. A.</given-names></name>
<name><surname>Patt</surname><given-names>R. M.</given-names></name>
</person-group> (<year>2007</year>). <source>Value added assessment of teacher preparation in Louisiana: 2004–2006</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://regents.louisiana.gov/assets/docs/TeacherPreparation/VAATPPTechnicalReport10-24-2007.pdf">http://regents.louisiana.gov/assets/docs/TeacherPreparation/VAATPPTechnicalReport10-24-2007.pdf</ext-link></citation>
</ref>
<ref id="bibr43-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Porter</surname><given-names>B. A.</given-names></name>
<name><surname>Patt</surname><given-names>R. M.</given-names></name>
<name><surname>Dahir</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <source>Value added assessment of teacher preparation in Louisiana: 2004-2005 to 2006-2007</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.laregentsarchive.com/Academic/TE/2008/Final%20Value-Added%20Report%20(12.02.08).pdf">http://www.laregentsarchive.com/Academic/TE/2008/Final%20Value-Added%20Report%20(12.02.08).pdf</ext-link></citation>
</ref>
<ref id="bibr44-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nye</surname><given-names>B.</given-names></name>
<name><surname>Konstantopoulos</surname><given-names>S.</given-names></name>
<name><surname>Hedges</surname><given-names>L.</given-names></name>
</person-group> (<year>2004</year>). <article-title>How large are teacher effects?</article-title> <source>Educational Evaluation and Policy Analysis</source>, <volume>26</volume>, <fpage>237</fpage>-<lpage>257</lpage>.</citation>
</ref>
<ref id="bibr45-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>O’Malley</surname><given-names>K.</given-names></name>
<name><surname>Auty</surname><given-names>W.</given-names></name>
<name><surname>Bielawski</surname><given-names>P.</given-names></name>
<name><surname>Bernstein</surname><given-names>R.</given-names></name>
<name><surname>Boatman</surname><given-names>T.</given-names></name>
<name><surname>Deeter</surname><given-names>T.</given-names></name>
<name><surname>. . .Blank</surname><given-names>R.</given-names></name>
</person-group> (<year>2009</year>). <source>Guide to United States Department of Education Growth Model Pilot Program 2005-2008</source>. <collab>The Council of Chief of State School Officers</collab>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.ccsso.org/Documents/2009/Guide_to_United_States_2009.pdf">http://www.ccsso.org/Documents/2009/Guide_to_United_States_2009.pdf</ext-link></citation>
</ref>
<ref id="bibr46-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reardon</surname><given-names>S. F.</given-names></name>
<name><surname>Raudenbush</surname><given-names>S. W.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Assumptions of value-added models for estimating school effects</article-title>. <source>Education Finance and Policy</source>, <volume>4</volume>(<issue>4</issue>), <fpage>492</fpage>-<lpage>519</lpage>.</citation>
</ref>
<ref id="bibr47-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rivkin</surname><given-names>S.</given-names></name>
<name><surname>Hanushek</surname><given-names>E.</given-names></name>
<name><surname>Kain</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Teachers, schools, and academic achievement</article-title>. <source>Econometrica</source>, <volume>73</volume>(<issue>2</issue>), <fpage>417</fpage>-<lpage>458</lpage>.</citation>
</ref>
<ref id="bibr48-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rockoff</surname><given-names>J.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The impact of individual teachers on student achievement: Evidence from panel data</article-title>. <source>American Economic Review</source>, <volume>94</volume>(<issue>2</issue>), <fpage>247</fpage>-<lpage>252</lpage>.</citation>
</ref>
<ref id="bibr49-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sanders</surname><given-names>W. L.</given-names></name>
<name><surname>Horn</surname><given-names>S.</given-names></name>
</person-group> (<year>1994</year>). <article-title>The Tennessee value-added assessment system (TVAAS): Mixed-model methodology in educational assessment</article-title>. <source>Journal of Personnel Evaluation in Education</source>, <volume>8</volume>, <fpage>299</fpage>-<lpage>311</lpage>.</citation>
</ref>
<ref id="bibr50-0022487112454437">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Sanders</surname><given-names>W. L.</given-names></name>
<name><surname>Rivers</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1996</year>). <source>Cumulative and residual effects of teachers on future student academic achievement</source>. Retrieved from the University of Tennessee Value-Added Research and Assessment Center website: <ext-link ext-link-type="uri" xlink:href="http://heartland.org/sites/all/modules/custom/heartland_migration/files/pdfs/3048.pdf">http://heartland.org/sites/all/modules/custom/heartland_migration/files/pdfs/3048.pdf</ext-link></citation>
</ref>
<ref id="bibr51-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sanders</surname><given-names>W. L.</given-names></name>
<name><surname>Saxton</surname><given-names>A. M.</given-names></name>
<name><surname>Horn</surname><given-names>S. P.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The Tennessee value-added assessment system: A quantitative, outcomes-based approach to educational assessment</article-title>. In <person-group person-group-type="editor">
<name><surname>Millman</surname><given-names>J.</given-names></name>
</person-group> (Ed.), <source>Grading teachers, grading schools</source> (pp. <fpage>137</fpage>-<lpage>162</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin Press</publisher-name>.</citation>
</ref>
<ref id="bibr52-0022487112454437">
<citation citation-type="gov">
<collab>The State of Delaware</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 1</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase1-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase1-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr53-0022487112454437">
<citation citation-type="gov">
<collab>The State of Florida</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr54-0022487112454437">
<citation citation-type="gov">
<collab>The State of Georgia</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr55-0022487112454437">
<citation citation-type="gov">
<collab>The State of Hawaii</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr56-0022487112454437">
<citation citation-type="gov">
<collab>The State of Maryland</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr57-0022487112454437">
<citation citation-type="gov">
<collab>The State of Massachusetts</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr58-0022487112454437">
<citation citation-type="web">
<collab>The State of Massachusetts</collab>. (<year>2011</year>). <source>MCAS student growth percentiles: Interpretive guide</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www.doe.mass.edu/mcas/growth/InterpretiveGuide.pdf">http://www.doe.mass.edu/mcas/growth/InterpretiveGuide.pdf</ext-link></citation>
</ref>
<ref id="bibr59-0022487112454437">
<citation citation-type="gov">
<collab>The State of New York</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr60-0022487112454437">
<citation citation-type="gov">
<collab>The State of North Carolina</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr61-0022487112454437">
<citation citation-type="gov">
<collab>The State of Ohio</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr62-0022487112454437">
<citation citation-type="gov">
<collab>The State of Rhode Island</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 2</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr63-0022487112454437">
<citation citation-type="gov">
<collab>The State of Tennessee</collab>. (<year>2010</year>). <source>Race to the Top Proposal, Phase 1</source>. Retrieved from the U.S. Department of Education website: <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html">http://www2.ed.gov/programs/racetothetop/phase2-applications/index.html</ext-link></citation>
</ref>
<ref id="bibr64-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tekwe</surname><given-names>C. D.</given-names></name>
<name><surname>Carter</surname><given-names>R. L.</given-names></name>
<name><surname>Ma</surname><given-names>C.</given-names></name>
<name><surname>Algina</surname><given-names>J.</given-names></name>
<name><surname>Lucas</surname><given-names>M. E.</given-names></name>
<name><surname>Roth</surname><given-names>J.</given-names></name>
<name><surname>. . .Resnick</surname><given-names>M. B.</given-names></name>
</person-group> (<year>2004</year>). <article-title>An empirical comparison of statistical models for value-added assessment of school performance</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>29</volume>, <fpage>11</fpage>-<lpage>36</lpage>.</citation>
</ref>
<ref id="bibr65-0022487112454437">
<citation citation-type="gov">
<collab>Tennessee State Board of Education</collab>. (<year>2009</year>). <source>Report card on the effectiveness of teacher training programs</source>. <month>November</month> Report. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tn.gov/sbe/TeacherReportCard/2009/2009%20Report%20Card%20on%20Teacher%20Effectiveness.pdf">http://www.tn.gov/sbe/TeacherReportCard/2009/2009%20Report%20Card%20on%20Teacher%20Effectiveness.pdf</ext-link></citation>
</ref>
<ref id="bibr66-0022487112454437">
<citation citation-type="gov">
<collab>Tennessee State Board of Education</collab>. (<year>2010</year>). <source>Report card on the effectiveness of teacher training programs</source>. <month>December</month> <article-title>Report</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tn.gov/sbe/Teacher%20Report%20Card%202010/2010%20Report%20Card%20on%20the%20Effectiveness%20of%20Teacher%20Training%20Programs.pdf">http://www.tn.gov/sbe/Teacher%20Report%20Card%202010/2010%20Report%20Card%20on%20the%20Effectiveness%20of%20Teacher%20Training%20Programs.pdf</ext-link></citation>
</ref>
<ref id="bibr67-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Voorhees</surname><given-names>R. A.</given-names></name>
<name><surname>Barnes</surname><given-names>G.</given-names></name>
<name><surname>Rothman</surname><given-names>R.</given-names></name>
</person-group> (<year>2003</year>). <source>Data systems to enhance teacher quality</source>. <publisher-loc>Denver, CO</publisher-loc>: <publisher-name>State Higher Education Executive Officers</publisher-name>.</citation>
</ref>
<ref id="bibr68-0022487112454437">
<citation citation-type="journal">
<collab>U.S. Department of Education</collab>. (<year>2010</year>). <article-title>Overview information; Race to the Top Fund</article-title>. <source>Federal Register</source>, <volume>75</volume>(<issue>71</issue>), <fpage>19496</fpage>-<lpage>19531</lpage>.</citation>
</ref>
<ref id="bibr69-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wilson</surname><given-names>S.</given-names></name>
<name><surname>Floden</surname><given-names>R.</given-names></name>
<name><surname>Ferrini-Mundy</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <source>Teacher preparation research: Current knowledge, gaps, and recommendations</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Center for the Study of Teaching and Policy</publisher-name>.</citation>
</ref>
<ref id="bibr70-0022487112454437">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wright</surname><given-names>S. P.</given-names></name>
<name><surname>White</surname><given-names>J. T.</given-names></name>
<name><surname>Sanders</surname><given-names>W. L.</given-names></name>
<name><surname>Rivers</surname><given-names>J. C.</given-names></name>
</person-group> (<year>2010</year>). <source>SAS® EVAAS® Statistical Models (white paper)</source>. <publisher-loc>Cary, NC</publisher-loc>: <publisher-name>SAS Institute</publisher-name>.</citation>
</ref>
<ref id="bibr71-0022487112454437">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Xu</surname><given-names>Z.</given-names></name>
<name><surname>Hannaway</surname><given-names>J.</given-names></name>
<name><surname>Taylor</surname><given-names>C.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Making a difference? The effects of Teach For America in high school</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>30</volume>(<issue>3</issue>): <fpage>447</fpage>-<lpage>469</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>