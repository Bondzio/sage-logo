<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JTE</journal-id>
<journal-id journal-id-type="hwp">spjte</journal-id>
<journal-title>Journal of Teacher Education</journal-title>
<issn pub-type="ppub">0022-4871</issn>
<issn pub-type="epub">1552-7816</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022487112439894</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022487112439894</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Theme Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Do Student Achievement Outcomes Differ Across Teacher Preparation Programs? An Analysis of Teacher Education in Louisiana</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Gansle</surname><given-names>Kristin A.</given-names></name>
<xref ref-type="aff" rid="aff1-0022487112439894">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Noell</surname><given-names>George H.</given-names></name>
<xref ref-type="aff" rid="aff1-0022487112439894">1</xref>
<xref ref-type="aff" rid="aff2-0022487112439894">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Burns</surname><given-names>Jeanne M.</given-names></name>
<xref ref-type="aff" rid="aff3-0022487112439894">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022487112439894"><label>1</label>Louisiana State University, Baton Rouge, USA</aff>
<aff id="aff2-0022487112439894"><label>2</label>Louisiana State Department of Education, Baton Rouge, USA</aff>
<aff id="aff3-0022487112439894"><label>3</label>Louisiana Board of Regents, Baton Rouge, USA</aff>
<author-notes>
<corresp id="corresp1-0022487112439894">Kristin A. Gansle, Louisiana State University, 223 Peabody Hall, Baton Rouge, LA 70803, USA Email: <email>kgansle@lsu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>63</volume>
<issue>5</issue>
<issue-title>Examining the Complexities of Assessment and Accountability in Teacher Education</issue-title>
<fpage>304</fpage>
<lpage>317</lpage>
<permissions>
<copyright-statement>© 2012 American Association of Colleges for Teacher Education</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Association of Colleges for Teacher Education</copyright-holder>
</permissions>
<abstract>
<p>Achievement outcomes for students taught by recent program completers of Louisiana’s teacher preparation programs (TPPs) are examined using hierarchical linear modeling of State student achievement data in English language arts, reading, mathematics, science, and social studies. The current year’s achievement in each content area is predicted using previous achievement data, student characteristics, classroom characteristics (e.g., percentage of students with disabilities), school characteristics, and attendance of teachers and students. The contribution of a teacher having recently completed a specific TPP is modeled at the classroom level as an indicator variable for each TPP. Results for programs with 25 or more new teachers are reported. Results demonstrate substantial overlap in confidence intervals (CI) among programs. In some instances, 68% and/or 95% CI for programs in specific content areas did not overlap results for the average new teacher or experienced teachers (i.e., they were lower than average new teachers or higher than average experienced certified teachers). Results varied across content areas for some programs.</p>
</abstract>
<kwd-group>
<kwd>teacher preparation</kwd>
<kwd>value-added assessment</kwd>
<kwd>student outcomes</kwd>
<kwd>multivariate longitudinal analysis</kwd>
</kwd-group>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>November/December 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>The extent to which students are prepared for postsecondary education and/or to effectively participate in the global economy after completing their primary education is linked to the quality and quantity of educational experiences they have while they are in school, most of which may be mediated by their teachers. When students leave school underprepared for postsecondary study or for the workplace, educators and schools have commonly been indicted as having failed those students (e.g., <xref ref-type="bibr" rid="bibr2-0022487112439894">Anderson, 2011</xref>; <xref ref-type="bibr" rid="bibr9-0022487112439894">DeWeese, 2007</xref>; <xref ref-type="bibr" rid="bibr35-0022487112439894">Postal &amp; Roth, 2011</xref>). Although data clarifying the direct impact of teacher preparation on student achievement have been limited (<xref ref-type="bibr" rid="bibr19-0022487112439894">Koretz, 2002</xref>), some have cited the underpreparation of new teachers who may not be effective at the point they enter the profession as an important contributor to poor achievement (<xref ref-type="bibr" rid="bibr5-0022487112439894">Boyd, Grossman, Lankford, Loeb, &amp; Wykoff, 2009</xref>; <xref ref-type="bibr" rid="bibr40-0022487112439894">Sawchuk, 2010</xref>). This argument and others have led to concerns regarding new teachers’ readiness for the workforce, to calls for improving teacher preparation, and more recently, to interest in examining the achievement of students who are taught by new teachers who enter the profession through different programs or pathways (<xref ref-type="bibr" rid="bibr5-0022487112439894">Boyd et al., 2009</xref>).</p>
<p>Although there is a clear need and desire for teachers and schools to help students overcome challenges to their success, the extent to which this is possible as well as the specific teacher characteristics and practices that might contribute to this end remain an active area of professional discourse and inquiry (<xref ref-type="bibr" rid="bibr6-0022487112439894">Cochran-Smith &amp; Zeichner, 2005a</xref>, <xref ref-type="bibr" rid="bibr7-0022487112439894">2005b</xref>; <xref ref-type="bibr" rid="bibr8-0022487112439894">Darling-Hammond &amp; Bransford, 2005</xref>; <xref ref-type="bibr" rid="bibr39-0022487112439894">Rice, 2003</xref>; <xref ref-type="bibr" rid="bibr43-0022487112439894">Wilson, Floden, &amp; Ferrini-Mundy, 2001</xref>). In addition, policies such as No Child Left Behind Act (<xref ref-type="bibr" rid="bibr28-0022487112439894">NCLB; 2001</xref>) and the Individuals With Disabilities Education Improvement Act (<xref ref-type="bibr" rid="bibr18-0022487112439894">IDEA; 2004</xref>) that place considerable emphasis on test-based accountability increase pressure on educators to identify factors contributing to teacher effectiveness. Teacher preparation programs (TPPs) are an obvious potential source of variability in teacher effectiveness; however, their impact is poorly understood and their impact on student achievement has received limited attention (<xref ref-type="bibr" rid="bibr5-0022487112439894">Boyd et al., 2009</xref>; <xref ref-type="bibr" rid="bibr7-0022487112439894">Cochran-Smith &amp; Zeichner, 2005b</xref>). Part of the challenge in studying the impact of teacher preparation is that it is a complex enterprise that includes many potentially operative elements. For example, TPPs are engaged to varying degrees in the recruitment of potential teachers, selecting who will be trained, providing new educators content knowledge, transmitting professional knowledge, conveying professional values, teaching new educators professional skills, and selecting who has shown enough promise in preparation to be recommended for licensure. Despite the intuitive appeal that these activities should matter either in isolation and/or in how they are aggregated within programs, limited evidence exists that TPPs produce different student achievement results (<xref ref-type="bibr" rid="bibr10-0022487112439894">Duckworth, Quinn, &amp; Seligman, 2009</xref>; <xref ref-type="bibr" rid="bibr12-0022487112439894">Goldhaber &amp; Brewer, 1997</xref>).</p>
<p>Historically, one of the most important barriers to studying the impact of TPPs, in this case indicating the ensemble of activities above, is that data systems did not exist that made the necessary connections to complete these kinds of analysis. At a minimal level, data are needed that link students to themselves to provide longitudinal achievement histories, students to teachers, and new teachers to the programs that prepared them. In addition, other key data about students may be needed such as attendance data and/or disability status. Until fairly recently, data systems providing this web of longitudinal linkages have not been available. As these data become available, they provide researchers, teacher educators, and policy makers the opportunity to examine a dimension of TPP effects that it has not been possible to study previously: the extent to which programs, pathways, or practices in teacher education influence student outcomes as measured by state-administered standardized tests. In addition, TPPs prepare educators to work in content areas that are not tested, prepare educators who contribute to outcomes that are not assessed by state-administered standardized tests, and may contribute to other outcomes such as teacher persistence within the profession.</p>
<p>In addition to the challenges associated with data availability, examining whether there is variability across TPPs in the achievement of students taught by their graduates presents consequential methodological challenges (<xref ref-type="bibr" rid="bibr12-0022487112439894">Goldhaber &amp; Brewer, 1997</xref>). The interpretation of student achievement scores that are administered annually as reflecting the quality of their teachers’ work is not practical because current year achievement may primarily reflect prior achievement as well as intervening influences such as disability, attendance, or poverty (<xref ref-type="bibr" rid="bibr1-0022487112439894">Anderman, Anderman, Yough, &amp; Gimbert, 2010</xref>; <xref ref-type="bibr" rid="bibr4-0022487112439894">Ballou, Sanders, &amp; Wright, 2004</xref>; <xref ref-type="bibr" rid="bibr17-0022487112439894">Hershberg, Simon, &amp; Lea-Kruger, 2004</xref>; <xref ref-type="bibr" rid="bibr24-0022487112439894">McCaffrey, Lockwood, Koretz, &amp; Hamilton, 2003</xref>). In response to these challenges, value-added analysis methods have been developed to examine current educational inputs such as teachers or educational services after controlling for prior achievement and other key variables (<xref ref-type="bibr" rid="bibr1-0022487112439894">Anderman et al., 2010</xref>; <xref ref-type="bibr" rid="bibr21-0022487112439894">Lasley, Siedentop, &amp; Yinger, 2006</xref>). Research using value-added methods for measuring teacher performance has grown dramatically (<xref ref-type="bibr" rid="bibr34-0022487112439894">Papay, 2010</xref>), and these methods are increasingly used in the development of public policy (<xref ref-type="bibr" rid="bibr16-0022487112439894">Heitin, 2011</xref>; <xref ref-type="bibr" rid="bibr27-0022487112439894">McNeil, 2009</xref>; <xref ref-type="bibr" rid="bibr33-0022487112439894">Olson, 2007</xref>).</p>
<p>The use and misuse of value-added assessments in evaluating the work of individual teachers is the source of an active professional and policy debate (<xref ref-type="bibr" rid="bibr3-0022487112439894">Baker et al., 2010</xref>; <xref ref-type="bibr" rid="bibr11-0022487112439894">Glazerman et al., 2010</xref>; <xref ref-type="bibr" rid="bibr13-0022487112439894">Hanson, 1988</xref>; <xref ref-type="bibr" rid="bibr14-0022487112439894">Harris, 2009</xref>; <xref ref-type="bibr" rid="bibr36-0022487112439894">Raudenbush, 2004</xref>; <xref ref-type="bibr" rid="bibr41-0022487112439894">Tekwe et al., 2004</xref>). Although space limitations preclude an inclusive review of the arguments supporting and challenging the use of value-added assessment to evaluate teacher effectiveness, a few critical points warrant review. Concerns have been raised regarding the precision and error rates in differentiating between teachers; variation across statistical models; the narrow focus of test-based, value-added assessments; narrowing of the curriculum; the impact of unmeasured school/community factors; and teacher demoralization (<xref ref-type="bibr" rid="bibr3-0022487112439894">Baker et al., 2010</xref>; <xref ref-type="bibr" rid="bibr24-0022487112439894">McCaffrey et al., 2003</xref>). Other authors have argued that the issue is not value-added per se, but how the information is used. They suggest that value added provides additional outcomes that have not been available, that design features/policy can be used to provide mitigation against errors, and that the use of value-added data can improve current teacher evaluation practices (<xref ref-type="bibr" rid="bibr11-0022487112439894">Glazerman et al., 2010</xref>). Although some of these concerns may be equally relevant to assessing TPPs (e.g., unmeasured variables), it is important to note that examining TPPs differs from teacher assessment in at least two critical ways. First, it is based on the assessment of multiple and potentially many teachers, and second, those teachers can be observed across many schools and potentially many school years. The availability of data across teachers and contexts provides some opportunities for mitigation against some of the concerns regarding value-added assessment in schools.</p>
<sec id="section1-0022487112439894">
<title>Evaluating Teacher Preparation in Louisiana</title>
<p>Beginning in 1999-2000, Louisiana’s Blue Ribbon Commission for Teacher Quality identified 60 recommendations to recruit, select, prepare, and support quality teachers. The Louisiana Board of Regents (BoR) approved new policies (see <xref ref-type="bibr" rid="bibr29-0022487112439894">Noell &amp; Burns, 2006</xref>, for further information) that required all universities to redesign their TPPs, and the Board of Elementary and Secondary Education (BESE) approved new policies that created greater rigor for teachers to become certified to teach in Louisiana. Universities were required to create teams involving college of education, college of arts/sciences, school/district, and other representatives who were responsible for creating redesigned programs that contained more rigorous content courses that were aligned with state/national content standards, more rigorous pedagogy courses that were aligned with state and national teacher standards, and more clinical experiences that allowed teacher candidates to practice skills earlier in their programs and as a result, over a longer period of time. Concurrent with their recommendations for improving teacher quality, the Blue Ribbon Commission recommended that a Teacher Preparation Accountability System be implemented that examined (a) institutional performance (i.e., Praxis passage rates, new teacher satisfaction ratings), (b) quantity (i.e., quantity of program completers and quantity of program completers in teacher shortage areas), and (c) achievement of students taught by TPPs’ new teachers. Although evaluation of TPPs in terms of the first two items had been ongoing, a data-based system for linking student achievement to teachers and new teachers to TPPs had not previously existed in Louisiana. As the professionals engaged in this study examined methods to link student achievement and teacher preparation, interest in the viability of value-added analyses emerged and was reinforced by the emergence of data systems that would support this type of analysis. Around the year 2003, interest in value-added methods as an additional source of evaluative information regarding TPPs in Louisiana emerged as a result of the State’s ongoing work to strengthen teacher preparation.</p>
<p>The initial issue examined within the value-added analyses in Louisiana was whether there was sufficient variability in teacher effectiveness between TPPs after providing extensive controls for student history, classroom composition, and school composition to make its use desirable in the evaluation of TPPs. In addition, propensity matching studies were undertaken as one test to examine the possibility of bias emerging based on the classes and schools new teachers served (<xref ref-type="bibr" rid="bibr31-0022487112439894">Noell, Porter, &amp; Patt, 2007</xref>; <xref ref-type="bibr" rid="bibr32-0022487112439894">Noell, Porter, Patt, &amp; Dahir, 2008</xref>). Analyses finding no or very limited variability across TPPs would suggest that focusing analytic and policy work on the quality of teacher preparation would have limited merit. The value-added work within Louisiana also provided a context for examining variability in achievement patterns across content areas within TPP. Whether a TPP’s effectiveness estimates differed among content areas or whether the effects were generalized to the program could provide information that would assist in evaluation of programs. The intent of the work was to examine whether differences could be demonstrated between TPPs that would then set the occasion for subsequent work to examine the potential differential impact of the numerous features around selection, preparation, and endorsement for certification. The remainder of this article describes the procedures that were employed under the sponsorship of the Louisiana BoR to examine the relationship between TPP completion and student achievement outcomes.</p>
</sec>
<sec id="section2-0022487112439894" sec-type="methods">
<title>Method</title>
<sec id="section3-0022487112439894">
<title>Participants</title>
<p>The data sets describing students, teachers, classes, and schools were merged using procedures described in the following sections. Standardized achievement test data for <italic>all</italic> students in the state who took the assessments were in the original data set, after which eligibility for the analysis was determined. Based on prior research examining the ratio of variability within programs to variability between programs, a minimum threshold of 25 teacher-by-year results were required for results to be reported for TPPs (see <xref ref-type="bibr" rid="bibr31-0022487112439894">Noell et al., 2007</xref>). A combination of 3 years of data was used to develop the estimates reported here. For example, a TPP might have had only 10 new teachers (i.e., in their first or second year of teaching) who were teaching in tested grades in a tested subject in their areas of certification in a single year. However, each year, achievement data, new teacher data, classroom data, and school data were merged to determine how many new teachers that year could contribute to the TPPs effect estimate in the overall analysis. When TPPs’ total number of new teachers in a given content over 3 years of data was 25 or more, that TPP was included in the analysis.</p>
<sec id="section4-0022487112439894">
<title>Eligibility for analysis</title>
<p>Each year’s records were limited to students who completed at least one content area assessment in Grades 4 through 9. The state collects achievement data in Grades 3 through 9, and this allowed for the use of 1 year of prior achievement scores for each student in each content area. In state databases, teachers are linked to their students at only the beginning of the academic year each year. As a result, it was necessary to determine which students were taught by their beginning of the year teachers for the bulk of the year. Students and teachers linked in the analyses were enrolled or teaching at the same school, based on employment (teacher attendance) or student attendance records from September 15 to March 15. The Louisiana school year begins in mid- to late August, and preliminary analyses revealed significant shifting in enrollment records during August, leading to the choice of September 15. For the years reported herein, standardized assessment was typically conducted in early to mid-April.</p>
<p>It was also necessary for a student to have been promoted 1 grade ahead to be included in the analyses. The meaning of their achievement scores is clearly different for a student who took the same test 2 years in a row versus one who took two different tests over the same 2 years. Following all of the linking and application of eligibility rules, almost 80% of the achievement records with which the analyses began remained in the database. Students were excluded predominantly due to grade retention and changing schools, which prevented linking them to teachers. For each content area, between 162,500 and 237,000 students contributed to the analyses per year. These students were taught by between 5,100 and 7,300 teachers, in 1,050 to 1,250 schools. <xref ref-type="table" rid="table1-0022487112439894">Table 1</xref> shows how two representative databases were attenuated as a result of cleaning and matching.</p>
<table-wrap id="table1-0022487112439894" position="float">
<label>Table 1.</label>
<caption><p>Cases Available for 2008 Analysis in Mathematics and Science</p></caption>
<graphic alternate-form-of="table1-0022487112439894" xlink:href="10.1177_0022487112439894-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Mathematics</th>
<th align="center">Science</th>
</tr>
</thead>
<tbody>
<tr>
<td>Assessed students Grades 4 to 9 in 2008</td>
<td>295,810</td>
<td>248,175</td>
</tr>
<tr>
<td>Matched to 2007 data</td>
<td>272,134 (92.0%)</td>
<td>231,069 (93.1%)</td>
</tr>
<tr>
<td>Consecutive grades assessed</td>
<td>250,188 (84.6%)</td>
<td>212,644 (85.7%)</td>
</tr>
<tr>
<td>Single primary school of attendance in curriculum database</td>
<td>233,336 (78.9%)</td>
<td>198,085 (79.8%)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0022487112439894">
<p>Note: The percentage in parentheses within each cell is the percentage of the total records available for analysis in that content area at that stage of database construction.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section5-0022487112439894">
<title>Research Design</title>
<p>Given the nature of the study question and data, an experimental design was not possible. This study examined the degree to which coefficients for recent program completers from specific TPPs varied across institutions and content domains. The data were fit within a hierarchical linear model (HLM; <xref ref-type="bibr" rid="bibr37-0022487112439894">Raudenbush &amp; Bryk, 2002</xref>); students were nested within teachers who were nested within schools (illustrated within <xref ref-type="fig" rid="fig1-0022487112439894">Figure 1</xref>). Predictor equations were developed for the student, teacher, and school levels of the model based on procedures described in the <xref ref-type="app" rid="app1-0022487112439894">appendix</xref>.</p>
<fig id="fig1-0022487112439894" position="float">
<label>Figure 1.</label>
<caption><p>Nesting structure of students within teachers/classrooms and teachers/classrooms within schools</p></caption>
<graphic xlink:href="10.1177_0022487112439894-fig1.tif"/>
</fig>
<sec id="section6-0022487112439894">
<title>Assignment of teacher level predictors</title>
<p>Teachers were designated as members of specific teacher groups depending on several factors. First, teacher experience data and certification data were used to designate teachers with less than 2 years teaching experience as new teachers as long as they possessed full certification status and were certified in the relevant content area. Based on prior work looking at gains in teaching efficacy over time, for purposes of these analyses, new teachers were defined as teachers in their first 2 years teaching post program completion (<xref ref-type="bibr" rid="bibr31-0022487112439894">Noell et al., 2007</xref>; <xref ref-type="bibr" rid="bibr32-0022487112439894">Noell et al., 2008</xref>). In addition, teachers had to be teaching within 5 years of completing their TPP to be included.</p>
<p>New teachers were coded as a completer of a specific TPP based on the Title II report data that TPPs submit to the BoR. For these analyses, programs within institutions are differentiated. For example, some universities have an undergraduate and a practitioner pathway to certification. TPPs in Louisiana consist of (a) Undergraduate Programs, (b) Practitioner Teacher Programs, (c) Master’s Degree Programs, and (d) Non-Master’s Certification-Only Programs (described in the following). The TPPs represented in the analysis are listed according to their program and institution in <xref ref-type="table" rid="table3-0022487112439894">Table 3</xref>.</p>
<p>All Louisiana pathways to teacher certification require candidates to address the same content and teacher standards, albeit in differing structures. They offer courses in the following major areas: knowledge of the learner and learning environment, methodology, and internship/student teaching. All programs or pathways also require completers to pass the same Praxis examinations to become certified to teach. The undergraduate pathway to teaching requires a minimum of 180 hr of direct teaching experience in clinical settings prior to student teaching and a minimum of 270 clock hours in student teaching with at least 180 hr spent actually teaching. Credit hours must be earned at a regionally accredited college or university. To be admitted to alternate pathways (i.e., Practitioner Teacher Program, Master of Arts in Teaching, and Certification-Only Program), candidates are required to pass all Praxis I Basic Skills examinations (or an equivalent), pass Praxis II Content examinations, and possess a baccalaureate degree from a regionally accredited university. The Practitioner Program is a fast-track program that can be completed in 1 year after completing 21 to 33 credit hours. The program begins with an intensive summer preparation program followed by acting as the teacher of record as candidates complete their training. There is no degree awarded at its completion. The Master of Arts in Teaching often requires candidates to meet the requirements for admission into a graduate program at that university and consists of 33 to 39 credit hours. The Certification-Only Program requires 27 to 33 credit hours for certification as a teacher and does not result in a degree. Most Master’s and Certification-Only Programs permit candidates to serve as teachers of record at schools while being paid as full-time teachers and meeting the requirements for their internships under the supervision of school and university personnel.</p>
<p>Teachers who had full certification status in a content area and had been teaching more than 2 years were designated as <italic>experienced, certified teachers</italic>. This code was omitted, serving as the reference group. <italic>Practitioner teachers</italic> were teachers of record who have not yet completed their TPP and are not yet certified to teach. Teachers who either had no certification or were certified but teaching out of their certification area were designated as <italic>not certified</italic>.</p>
</sec>
<sec id="section7-0022487112439894">
<title>Student achievement</title>
<p>Test databases included scores for the <italic>i</italic>LEAP and the LEAP in five content areas: English-language arts (ELA), reading, mathematics, science, and social studies. The <italic>i</italic>LEAP is a version of the Iowa Tests of Basic Skills (ITBS) that has been modified to include Louisiana curriculum-specific items. The LEAP is the Louisiana Educational Assessment Program, and is used for high-stakes testing in Grades 4 and 8. All other included grades take the <italic>i</italic>LEAP. The <italic>i</italic>LEAP and LEAP were reviewed against content standards by a committee of experts who confirmed alignment with state standards and content validity. Cronbach’s alpha was used to assess reliability for all grades and content areas and was described as excellent (<italic>i</italic>LEAP α ranging from .81 to .95; <xref ref-type="bibr" rid="bibr22-0022487112439894">Louisiana Department of Education, 2008a</xref>; LEAP α ranging from .86 to .92; <xref ref-type="bibr" rid="bibr23-0022487112439894">Louisiana Department of Education, 2008b</xref>). Scores for all students who took the tests at the spring administration were included in the databases and raw scores were converted to <italic>z</italic> scores by year, grade, and content area to create consistent scaling across years, grades, and content areas.</p>
<p>For purposes of reporting results, the outcome variable was scaled up to a standard deviation of 50. This standard deviation was chosen because it is the most common approximate standard deviation of scaled scores across content areas, grades, and years; it places the TPP outcomes in a metric that is familiar to educators in Louisiana. In all analyses, experienced certified teachers were the reference (omitted indicator) group, so that results for TPPs are relative to experienced certified teachers.</p>
</sec>
<sec id="section8-0022487112439894">
<title>Student demographic data</title>
<p>State school records for students’ gender, race, special education status, attendance, and free lunch status were included in the analysis. All student variables are listed in <xref ref-type="table" rid="table2-0022487112439894">Table 2</xref>.</p>
<table-wrap id="table2-0022487112439894" position="float">
<label>Table 2.</label>
<caption><p>Student, Teacher, and School-Level Variables</p></caption>
<graphic alternate-form-of="table2-0022487112439894" xlink:href="10.1177_0022487112439894-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Student-level variables</th>
<th align="center">Teacher- or classroom-level variables</th>
<th align="center">School-level variables</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gender (male)</td>
<td>Percentage of students who were male</td>
<td>Percentage of students who were male</td>
</tr>
<tr>
<td>African American</td>
<td>Percentage of students who were minorities</td>
<td>Percentage of students who were minorities</td>
</tr>
<tr>
<td colspan="3">Asian American</td>
</tr>
<tr>
<td colspan="3">Native American</td>
</tr>
<tr>
<td colspan="3">Hispanic</td>
</tr>
<tr>
<td>Emotionally disturbed</td>
<td>Percentage of students who were in special education</td>
<td>Percentage of students who were in special education</td>
</tr>
<tr>
<td colspan="3">Speech and language</td>
</tr>
<tr>
<td colspan="3">Mild mental retardation</td>
</tr>
<tr>
<td colspan="3">Specific learning disability</td>
</tr>
<tr>
<td colspan="3">Other health impaired</td>
</tr>
<tr>
<td colspan="3">Special education—Other</td>
</tr>
<tr>
<td>Gifted</td>
<td>Percentage of students who were identified as gifted</td>
<td>Percentage of students who were identified as gifted</td>
</tr>
<tr>
<td>Limited English proficiency</td>
<td>Percentage of students who exhibited limited English proficiency</td>
<td>Percentage of students who exhibited limited English proficiency</td>
</tr>
<tr>
<td>Section 504</td>
<td>Percentage of students identified as protected by Section 504</td>
<td>Percentage of students identified as protected by Section 504</td>
</tr>
<tr>
<td>Free lunch</td>
<td>Percentage of students who received free lunch</td>
<td>Percentage of students who received free lunch</td>
</tr>
<tr>
<td>Reduced price lunch</td>
<td>Percentage of students who received reduced price lunch</td>
<td>Percentage of students who received reduced price lunch</td>
</tr>
<tr>
<td>Prior year mathematics score</td>
<td>Class mean prior achievement in mathematics</td>
<td>School mean prior achievement in mathematics</td>
</tr>
<tr>
<td>Prior year reading score</td>
<td>Class mean prior achievement in reading</td>
<td>School mean prior achievement in reading</td>
</tr>
<tr>
<td>Prior year science score</td>
<td>Class mean prior achievement in science</td>
<td>School mean prior achievement in science</td>
</tr>
<tr>
<td>Prior year social studies score</td>
<td>Class mean prior achievement in social studies</td>
<td>School mean prior achievement in social studies</td>
</tr>
<tr>
<td>Prior year English-language arts score</td>
<td>Class mean prior achievement in English-language arts</td>
<td>School mean prior achievement in English-language arts</td>
</tr>
<tr>
<td>Student absences</td>
<td>Teacher absences</td>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section9-0022487112439894">
<title>Course and class data</title>
<p>Course codes were reviewed and categorized based on their primary content. For example, seventh-grade English was categorized as ELA and algebra was categorized as mathematics. Courses such as jazz band did not receive a content category and were not included in the analyses as these courses are not aligned to the five areas assessed by the standardized assessments.</p>
<p>In addition, summary demographic data regarding teachers’ class composition were included in the database. For example, the percentage of their class receiving special education services and class prior year mean achievement in all content areas were included in the analyses. The number of absences each teacher had during the year was included as well. These variables are listed in the second column of <xref ref-type="table" rid="table2-0022487112439894">Table 2</xref>.</p>
</sec>
<sec id="section10-0022487112439894">
<title>School measures</title>
<p>Summary school demographic data such as mean prior year achievement in each content area and the percentage of students receiving free or reduced price lunch were included in the models. These variables are listed in the third column of <xref ref-type="table" rid="table2-0022487112439894">Table 2</xref>.</p>
</sec>
</sec>
<sec id="section11-0022487112439894">
<title>Procedures</title>
<sec id="section12-0022487112439894">
<title>Linking databases</title>
<p>Multiple distinct annual data systems were linked connecting new teachers to TPP, teachers to students, teachers to their descriptive data, test data to demographic data, students to themselves across years, and students to teachers within years. Due to the number of data sources, years, and the complexity of the merge process, substantive collaboration was necessary with the Louisiana Department of Education and the Louisiana BoR.</p>
<p>Once teachers, students, courses, and schools were linked, each content area database was checked for students who had multiple teachers in a subject. If one student had two different mathematics courses taught by two different teachers, that student contributed to the estimated classroom result at one half the weight of students for whom that was their only course in mathematics. This convention was also used for situations in which two teachers were instructors of record for one course due to a team-teaching situation.</p>
</sec>
<sec id="section13-0022487112439894">
<title>Analytic models</title>
<p>The analysis is based on HLM (<xref ref-type="bibr" rid="bibr26-0022487112439894">McCulloch &amp; Searle, 2001</xref>; <xref ref-type="bibr" rid="bibr37-0022487112439894">Raudenbush &amp; Bryk, 2002</xref>) using achievement scores from the State’s mandated testing programs in Grades 3 through 9; demographic data for the students, teachers, and schools; attendance data for teachers and students; teachers’ certification information; and the TPP they completed. The result estimates the degree to which students taught by new teachers from specific TPPs achieve more or less than would be predicted based on an extensive set of student, class, and school predictors in ELA, reading, mathematics, science, and social studies. Additional technical information concerning the analytic procedures may be found in the <xref ref-type="app" rid="app1-0022487112439894">appendix</xref>.</p>
</sec>
</sec>
</sec>
<sec id="section14-0022487112439894" sec-type="results">
<title>Results</title>
<p>There were sufficient data to provide estimates for between 7 and 10 programs per content area. The number of programs for which results are available was reduced due to a historical artifact resulting from the State’s redesign of TPPs (see <xref ref-type="bibr" rid="bibr29-0022487112439894">Noell &amp; Burns, 2006</xref>, for a description). This analysis was completed for a period during which the State was transitioning from its original programs to redesigned programs. The decision was made by the BoR to provide data only for currently operational programs (i.e., post-redesign), which meant that all programs had not yet completed the transition and were producing sufficient graduates from their post-redesign programs to be included in the analysis.</p>
<p>The transition from the original programs to the redesigned programs created another issue. Within each content area, six to eight programs were alternate certification programs and only one or two were undergraduate programs. This occurred because alternate programs generally went through the redesign process first and required less time to complete. The imbalance between undergraduate and alternate programs in these data is a result of the reality that it took longer to transition undergraduate programs and longer to complete those programs. Data from a period further removed from program redesign should not evidence this imbalance.</p>
<p>For all data discussed, the results represent student achievement in Grades 4 through 9. For each content area, a mean new teacher effect estimate was calculated. Data are reported herein using the same 68% confidence intervals (CI) that were included in the original reports to the Louisiana BoR. The 68% CI was adopted in that work for several reasons. First, the estimate obtained for each institution was the <italic>population</italic> of all program completers in the tested grades and subjects. As a result, typical uncertainty deriving from sampling issues was attenuated. Second, because the intent of the analyses was to develop data for program improvement (formative data), a more aggressive CI was adopted because not acting when improvement was needed was weighed as a more serious error than acting somewhat too often. Third, the measurement was repeated annually (repeated observation) providing an opportunity for any aberrant results to be identified and interpretations corrected. Fourth, prior experience with these measures over annual reports demonstrated a degree of stability that exceeded the level that would be expected based on even the 68% CI.</p>
<p>Effects are reported relative to average student achievement for each content area for experienced certified teachers, which is the <italic>x</italic> axis or 0 points. In other words, if a program’s effect estimate in mathematics was 1.0, this would indicate that students taught by new teachers from that program scored 1 point higher than would be predicted for those students based on prior achievement, demographics, and attendance in an experienced certified teacher’s class, based on an achievement test whose mean is 300 and standard deviation is 50. TPP coefficient estimates may be found in <xref ref-type="table" rid="table3-0022487112439894">Table 3</xref>. They are represented graphically in <xref ref-type="fig" rid="fig2-0022487112439894">Figure 2</xref>.</p>
<table-wrap id="table3-0022487112439894" position="float">
<label>Table 3.</label>
<caption><p>Teacher Preparation Program Coefficients with 68% Confidence Intervals in All Content Areas</p></caption>
<graphic alternate-form-of="table3-0022487112439894" xlink:href="10.1177_0022487112439894-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Abbreviation</th>
<th align="center">ELA</th>
<th align="center">Mathematics</th>
<th align="center">Reading</th>
<th align="center">Science</th>
<th align="center">Social studies</th>
</tr>
</thead>
<tbody>
<tr>
<td>Average new teacher</td>
<td/>
<td>−2.9</td>
<td>−2.7</td>
<td>−2.8</td>
<td>−1.4</td>
<td>−2.1</td>
</tr>
<tr>
<td rowspan="2">Undergraduate Program 1</td>
<td>UG 1</td>
<td>−4.7 (−6.2, −3.2)</td>
<td>−4.3 (−6.4, −2.2)</td>
<td>−2.8 (−4.1, −1.5)</td>
<td>−0.8 (−2.5, 0.9)</td>
<td>−3.1 (−4.6, −1.6)</td>
</tr>
<tr>
<td>n</td>
<td>68</td>
<td>59</td>
<td>49</td>
<td>39</td>
<td>55</td>
</tr>
<tr>
<td rowspan="2">Undergraduate Program 2</td>
<td>UG 2</td>
<td>−3.7 (−6.0, −1.4)</td>
<td>−2.5 (−4.3, −0.7)</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>n</td>
<td>25</td>
<td>25</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td rowspan="2">Master’s Alternate Certification Program 1</td>
<td>M Alt C 1</td>
<td>2.6 (−0.2, 5.4)</td>
<td>−1.0 (−2.9, 0.9)</td>
<td>0.2 (−2.3, 2.7)</td>
<td>2.2 (0.0, 4.4)</td>
<td>1.4 (−0.6, 3.4)</td>
</tr>
<tr>
<td>n</td>
<td>41</td>
<td>46</td>
<td>30</td>
<td>39</td>
<td>42</td>
</tr>
<tr>
<td rowspan="2">Master’s Alternate Certification Program 2</td>
<td>M Alt C 2</td>
<td>1.9 (−0.7, 4.5)</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>n</td>
<td>25</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td rowspan="2">Non-Master’s/Certification Only Program 1</td>
<td>NM/CO 1</td>
<td>−4.9 (−6.7, −3.1)</td>
<td>−2.2 (−3.9, −0.6)</td>
<td>−2.9 (−4.8, −0.9)</td>
<td>−3.1 (−5.2, −1.0)</td>
<td>−2.8 (−5.3, −0.3)</td>
</tr>
<tr>
<td>n</td>
<td>69</td>
<td>59</td>
<td>58</td>
<td>43</td>
<td>50</td>
</tr>
<tr>
<td rowspan="2">Non-Master’s/Certification Only Program 2</td>
<td>NM/CO 2</td>
<td>2.4 (−0.8, 5.6)</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>n</td>
<td>26</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td rowspan="2">Practitioner TPP 1</td>
<td>U Pract 1</td>
<td>1.6 (−0.9, 4.1)</td>
<td>−3.4 (−5.4, −1.4)</td>
<td>1.2 (−0.9, 3.5)</td>
<td>−1.4 (−3.7, 0.9)</td>
<td>−2.8 (−5.4, −0.2)</td>
</tr>
<tr>
<td>n</td>
<td>36</td>
<td>43</td>
<td>35</td>
<td>29</td>
<td>40</td>
</tr>
<tr>
<td rowspan="2">Practitioner TPP 2</td>
<td>U Pract 2</td>
<td>−0.4 (−2.6, 1.8)</td>
<td>−0.2 (−3.2, 2.8)</td>
<td>0.4 (−1.8, 2.6)</td>
<td>3.7 (1.8, 5.6)</td>
<td>−0.4 (−2.0, 1.2)</td>
</tr>
<tr>
<td>n</td>
<td>46</td>
<td>47</td>
<td>44</td>
<td>28</td>
<td>29</td>
</tr>
<tr>
<td rowspan="2">Private practitioner TPP 1</td>
<td>PrivPract 1</td>
<td>−2.7 (−4.4, −1.0)</td>
<td>−2.9 (−4.6, −1.2)</td>
<td>−6.3 (−8.3, −4.3)</td>
<td>−1.8 (−3.2, −0.4)</td>
<td>−3.0 (−5.3, −0.7)</td>
</tr>
<tr>
<td>n</td>
<td>43</td>
<td>47</td>
<td>35</td>
<td>43</td>
<td>28</td>
</tr>
<tr>
<td rowspan="2">Private practitioner TPP 2</td>
<td>PrivPract 2</td>
<td>2.0 (−0.4, 4.4)</td>
<td>5.7 (4.0, 7.4)</td>
<td>4.1 (1.2, 7.0)</td>
<td>0.9 (−1.3, 3.1)</td>
<td>−3.1 (−6.2, 0.1)</td>
</tr>
<tr>
<td>n</td>
<td>41</td>
<td>55</td>
<td>30</td>
<td>37</td>
<td>32</td>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="fig2-0022487112439894" position="float">
<label>Figure 2.</label>
<caption><p>Coefficient of difference between predicted and actual achievement for teacher preparation programs</p></caption>
<graphic xlink:href="10.1177_0022487112439894-fig2.tif"/>
</fig>
<sec id="section15-0022487112439894">
<title>Average New Teacher Effects</title>
<p>All teachers with 2 or fewer years experience were averaged to produce the mean effect for new teachers. Average new Louisiana teachers’ effect estimates in ELA, mathematics, and reading clustered between −2.7 and −2.9 points. Average new teachers scored closer to experienced teachers in science, where the effect estimate was −1.4 points. In social studies, the effect estimate for new teachers was −2.1 points.</p>
</sec>
<sec id="section16-0022487112439894">
<title>TPP Effect Estimates</title>
<p>Results were obtained for exemplars of all four types of TPPs in Louisiana: two undergraduate programs, two Master of Arts in Teaching alternate certification programs, two Non-Master’s Certification-Only alternate certification programs, and four Practitioner Teacher Programs—two at institutions of higher education and two private, nonuniversity providers. <xref ref-type="fig" rid="fig2-0022487112439894">Figure 2</xref> presents the TPP results. It is important to recognize that at least two reasonable standards for comparison are available. One could choose to compare programs to the average experienced certified teacher (the 0 point). Comparison with this referent would determine the extent to which new teachers from specific TPPs obtain student achievement results that are distinct from average veteran teachers. Alternatively, one could choose to compare results to the average new teacher result in that content area. Comparison to this referent essentially asks how program completers fair compared with other new teachers. Choice of the comparison point changes which programs will be identified as outlying.</p>
<p>For programs with results in multiple content areas, TPP coefficients predominantly clustered in ranges that were either similar to average new teachers or they clustered predominantly in a range that was distinct from average new teachers. In the case of programs whose coefficients were generally higher than the average new teacher result, in some content areas, the 68% CI did not overlap with the average experienced certified teacher result either (the 0 point), and in one case, the 95% CI would not have overlapped with the 0 point (private practitioner TPP 2, mathematics).</p>
<p>The three programs whose coefficients were predominantly higher than the average new teacher result and whose 68% CI did not overlap with the average new teacher result were Private Practitioner TPP 2, Masters Alternate Certification Program 1, and University Practitioner Program 2. All three programs had one content area with a result whose CI did overlap the average new teacher effect (Private Practitioner Program 2, social studies; Master’s Alternate Certification Program 1, mathematics; and University Practitioner Program 2, mathematics).</p>
<p>University Practitioner Program 1 had mixed results with three areas whose CI overlapped with the average new teacher estimate and in two content areas in which the 68% CI did not overlap with the average new teacher. Four programs’ coefficients were predominantly similar to other new teachers (CI overlapping the average new teacher result). These were Undergraduate Programs 1 and 2, Non-Master’s/Certification-Only Program 1, and Private Practitioner Program 1. In addition, among these programs, two TPPs had results that were lower than the average new teacher result and for which the 68% CI did not overlap with the average new teacher result. This occurred for Private Practitioner Program 1 in reading and the Non-Master’s/Certification-Only Program 1 in ELA. It is interesting to note that in both of the cases, the result would also fall outside the 95% CI for a comparison to experienced teachers.</p>
</sec>
</sec>
<sec id="section17-0022487112439894" sec-type="discussion">
<title>Discussion</title>
<p>This study describes the output of 1 year’s analyses of a systematic approach to examining student achievement outcomes for recent program completers across TPPs in Louisiana. The analysis was made possible through the integration of distinct data systems from the Louisiana Department of Education and the Louisiana BoR and was built on longitudinal data regarding student achievement as well as demographic variables such as attendance, free lunch status, and disability status. Similarly, completing the analyses modeled here also required data regarding teacher certification and experience in addition to TPP completion data. Analyses were completed using a three-level HLM with students nested within teachers nested within schools and TPP coefficients extracted at the teacher level. Results demonstrated considerable overlap in CI between programs, with some programs having coefficients whose CI did not overlap with substantive anchors such as the average new teacher or the average experienced certified teacher in that content domain with either a 68% or a 95% CI. One of the critical challenges that this sort of data create is arriving at reasoned and reasonable decision-making rules for a context in which measures are repeated annually, potentially provide information for program improvement, and include the population of new teachers from a program in tested grades and subjects. This is a decision-making context that diverges from the typical hypothesis testing research context in a number of important ways.</p>
<p>Although contributing to student achievement is a hallmark of teaching, an inadequate literature base exists directly linking student achievement outcomes to teachers, teaching, or how teachers are prepared (<xref ref-type="bibr" rid="bibr5-0022487112439894">Boyd et al., 2009</xref>; <xref ref-type="bibr" rid="bibr6-0022487112439894">Cochran-Smith &amp; Zeichner, 2005a</xref>). One of the notable gaps in the literature is the extent to which teacher preparation matters. It is certainly within the realm of possibility that individual teachers vary widely in effectiveness, but that this variation at the individual level is not related in a systematic way to the preparation programs that recruited, admitted, prepared, and recommended them for professional licensure. Although it is possible, it seems somewhat improbable. The absence of a compelling literature base regarding variability in teacher preparation may have more to do with the complexity of studying it than with a lack of interest in the topic on the part of researchers. Studying the contribution of teacher preparation to student achievement requires elaborate longitudinal databases that link students, teachers, and preparation across multiple years. It also requires data across multiple programs to make comparisons. The availability of data that permits study based on large representative administrative databases is a relatively recent phenomenon.</p>
<p>Although value-added methods for evaluating educational inputs have existed since at least the 1970s, their complexity, computational challenges, and data demands have limited their application to problems of teacher preparation. Value-added analyses have emerged with dramatically increasing popularity in education over the last two decades as the methods have become more widely understood, computational resources have become more readily available, and the necessary data have become more available (<xref ref-type="bibr" rid="bibr34-0022487112439894">Papay, 2010</xref>). They have used previous achievement, student demographic variables, and contextual variables to estimate the contribution of a range of educational inputs, including individual teachers in its most controversial application (<xref ref-type="bibr" rid="bibr24-0022487112439894">McCaffrey et al., 2003</xref>; <xref ref-type="bibr" rid="bibr25-0022487112439894">McCaffrey, Lockwood, Koretz, Louis, &amp; Hamilton, 2004</xref>; <xref ref-type="bibr" rid="bibr42-0022487112439894">Todd &amp; Wolpin, 2003</xref>). This study illustrates a method for extending this type of analysis to the study of teacher preparation.</p>
<p>One interesting challenge in examining the methods and data is conceptualizing what independent variable is being studied or should be studied. Is the independent variable simply the TPPs that vary in a variety of ways? This conceptualization might be of interest to policy makers who would like to identify programs whose graduates obtain unexpectedly positive or negative contributions to student achievement. Another possibility is that the independent variable should be conceptualized as pathways to certification. For example, are practitioner programs more effective than undergraduate programs in preparing teachers? Although the data presented here do not contain enough information on enough different programs within each pathway to answer that question, they do provide clear cautionary data. Examination of the differences between Private Provider Practitioner Programs 1 and 2 suggests the possibility of considerable variability within pathways. A final possibility is interest in features within preparation programs (e.g., <xref ref-type="bibr" rid="bibr5-0022487112439894">Boyd et al., 2009</xref>). For example, what is the relative importance of the amount of supervised practice with feedback provided to candidates versus the degree of selectivity of program admissions? The challenges underlying this sort of contrast are particularly daunting given the measurement challenges surrounding program features and obtaining sufficient controls to isolate the impact of individual program features. This type of analysis was beyond the scope of the current study.</p>
<p>These data also illustrate a potential process for providing TPPs feedback on the achievement of students taught by new graduates. These types of data provide one key element of continuous improvement models, the ability to obtain repeated measurements of a relevant meaningful outcome of interest (<xref ref-type="bibr" rid="bibr38-0022487112439894">Reusser, Butler, Symonds, Vetter, &amp; Wall, 2007</xref>). This is not to argue that it is the only outcome that should be of interest to teacher educators. However, if sufficient controls are in place that policy makers and teacher educators trust is a fair evaluation and results are stable or have clear trends over time, they provide the potential for a substantive new tool in program improvement by providing feedback in a domain in which it has not been available in the past.</p>
<p>Louisiana’s BoR is using value-added analysis to provide TPPs with information on how students taught by recent program completers are fairing on standardized achievement tests. This has occasioned some programs to examine their programs in domains for which they were dissatisfied with their results either relative to their performance in other content areas or relative to the state to identify potential points of program improvement. The difficulty of this task has highlighted a key challenge underlying this work. As with all value-added data, the results do not answer <italic>why</italic> a particular result occurred or what might be done to improve on it; rather, all it does is provide feedback on performance, focus program improvement efforts, and provide a benchmark that helps sustain a focus on continuous program improvement (<xref ref-type="bibr" rid="bibr15-0022487112439894">Hart &amp; Bogan, 1992</xref>; <xref ref-type="bibr" rid="bibr38-0022487112439894">Reusser et al., 2007</xref>). It is interesting to note that at least two programs that engaged in the sort of self-study occasioned by the data identified plausible hypotheses regarding actions that they could and did take to improve their results.</p>
<sec id="section18-0022487112439894">
<title>Limitations and Future Directions</title>
<p>There are relatively few TPPs that have yet had data sufficient to report publicly. New undergraduate programs have a minimum of 4 years from the time they are redesigned to produce their first graduates, and those graduates have to teach for a year before their data can appear in the analyses. New alternate programs generally take less time to complete, so their graduates appear in the schools earlier than their undergraduate colleagues do. As Louisiana moves further from the redesign process, more of the new programs’ graduates will contribute to the analyses and produce effect estimates for an increasing number of programs. This creates the inevitable tension between disseminating what is known now and withholding information until some later date when more complete information is available.</p>
<p>In addition, the analyses reported herein reflect only those teachers in tested grades and subjects. These analyses do not provide any information regarding early elementary teachers or teachers in subjects such as art, band, or physical education. It is exceedingly unlikely that data will become available to use this sort of framework to examine the preparation of teachers in these domains. Subsequent distinct work is needed to develop practical methods for TPPs to obtain information regarding the impact of these educators on student outcomes of interest.</p>
<p>The absolute magnitudes of the coefficients for TPPs are not large. However, given that the coefficient represents the deviation from predicted achievement for large groups of students taught by new teachers from specific programs in some cases, one could argue they may represent socially significant outcomes. Specifically, although a 5-point (0.1 standard deviation units) difference score for a single student may have limited importance, when that difference emerges in 2,000 students distributed across many schools, it may take on social significance. It is also important to recognize that these differences for current year total achievement emerge after controlling for an extensive series of predictor variables including prior achievement and this year’s result will in turn affect subsequent year’s expected outcomes. Finally, it is worth noting that socioeconomic disadvantage is widely accepted as having a socially significant impact on student achievement (<xref ref-type="bibr" rid="bibr20-0022487112439894">Krovetz, 2008</xref>). In many cases, the TPP coefficients are two times the magnitude of the coefficients for free lunch status as an indicator of socioeconomic disadvantage (see <xref ref-type="table" rid="table4-0022487112439894">Table 4</xref>).</p>
<table-wrap id="table4-0022487112439894" position="float">
<label>Table 4.</label>
<caption><p>Base Hierarchical Linear Model for Mathematics Achievement 2007-2008</p></caption>
<graphic alternate-form-of="table4-0022487112439894" xlink:href="10.1177_0022487112439894-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Model level</th>
<th align="center">Variables entered</th>
<th align="center">Coefficient</th>
<th align="center">CI</th>
</tr>
</thead>
<tbody>
<tr>
<td>Student-level variables</td>
<td>Gender (male)</td>
<td>2.2</td>
<td>[1.9, 2.4]</td>
</tr>
<tr>
<td/>
<td>African American</td>
<td>−4.8</td>
<td>[−5.2, −4.5]</td>
</tr>
<tr>
<td/>
<td>Asian American</td>
<td>5.7</td>
<td>[4.7, 6.8]</td>
</tr>
<tr>
<td/>
<td>Native American</td>
<td>−2.2</td>
<td>[−3.7, −0.8]</td>
</tr>
<tr>
<td/>
<td>Limited English proficiency</td>
<td>3.1</td>
<td>[1.7, 4.4]</td>
</tr>
<tr>
<td/>
<td>Speech and language</td>
<td>−1.7</td>
<td>[−2.6, −0.8]</td>
</tr>
<tr>
<td/>
<td>Mild mental retardation</td>
<td>−14.3</td>
<td>[−17.4, −11.3]</td>
</tr>
<tr>
<td/>
<td>Specific learning disability</td>
<td>−7.3</td>
<td>[−8.2, −6.3]</td>
</tr>
<tr>
<td/>
<td>Other health impaired</td>
<td>−7.4</td>
<td>[−8.6, −6.2]</td>
</tr>
<tr>
<td/>
<td>Special education—Other</td>
<td>−4.5</td>
<td>[−6.6, −2.3]</td>
</tr>
<tr>
<td/>
<td>Gifted</td>
<td>10.3</td>
<td>[9.5, 11.2]</td>
</tr>
<tr>
<td/>
<td>Section 504</td>
<td>−4.0</td>
<td>[−4.8, −3.3]</td>
</tr>
<tr>
<td/>
<td>Free lunch</td>
<td>−1.9</td>
<td>[−2.2, −1.6]</td>
</tr>
<tr>
<td/>
<td>Reduced price lunch</td>
<td>−0.9</td>
<td>[−1.3, −0.4]</td>
</tr>
<tr>
<td/>
<td>Student absences</td>
<td>−0.3</td>
<td>[−0.3, −0.3]</td>
</tr>
<tr>
<td/>
<td>Prior year mathematics test</td>
<td>27.9</td>
<td>[27.5, 28.3]</td>
</tr>
<tr>
<td/>
<td>Prior year reading test</td>
<td>1.2</td>
<td>[1.0, 1.5]</td>
</tr>
<tr>
<td/>
<td>Prior year science test</td>
<td>5.5</td>
<td>[5.2, 5.8]</td>
</tr>
<tr>
<td/>
<td>Prior year social studies test</td>
<td>2.6</td>
<td>[2.3, 2.8]</td>
</tr>
<tr>
<td/>
<td>Prior year English-language arts test</td>
<td>2.9</td>
<td>[2.7, 3.2]</td>
</tr>
<tr>
<td>Classroom variables</td>
<td>Teacher absences</td>
<td>0.0</td>
<td>[−0.1, 0.0]</td>
</tr>
<tr>
<td/>
<td>% Special education</td>
<td>−7.6</td>
<td>[−12.0, −3.1]</td>
</tr>
<tr>
<td/>
<td>% Free lunch</td>
<td>−13.5</td>
<td>[−16.6, −10.4]</td>
</tr>
<tr>
<td/>
<td>% Reduced price lunch</td>
<td>−9.4</td>
<td>[−15.0, −3.7]</td>
</tr>
<tr>
<td/>
<td>% Minority</td>
<td>−0.2</td>
<td>[−2.5, 2.0]</td>
</tr>
<tr>
<td/>
<td>% Section 504</td>
<td>3.0</td>
<td>[−3.6, 9.5]</td>
</tr>
<tr>
<td>Building variables</td>
<td>% Section 504</td>
<td>16.8</td>
<td>[4.5, 29.1]</td>
</tr>
<tr>
<td/>
<td>% Free lunch</td>
<td>16.9</td>
<td>[12.2, 21.5]</td>
</tr>
<tr>
<td/>
<td>Mean prior year mathematics test</td>
<td>8.1</td>
<td>[4.2, 12.1]</td>
</tr>
<tr>
<td/>
<td>Mean prior year science test</td>
<td>−10.7</td>
<td>[−14.5, −6.9]</td>
</tr>
<tr>
<td/>
<td>Mean prior year reading test</td>
<td>7.4</td>
<td>[2.8, 12.1]</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The obvious core future work in Louisiana will be to continue to accumulate data to permit a broader representation of TPPs. The data raise the questions of why Private Practitioner Program 2’s graduates obtained mathematics achievement that was so much higher than predicted or why Private Practitioner Program 1’s graduates’ students did more poorly than predicted in reading. Although the TPPs have been informally engaged in self-appraisal with and without these data, it has become clear that unpacking the elements of TPPs that may link to student achievement is exceedingly challenging. In some ways, it is not much different from developing the value-added analysis of student achievement. TPPs have many moving parts that may be complimentary, compensatory, or conflicting. It is possible that rigor in either admissions or preparation may mask a weakness in the other domain. Similarly, the experiences that candidates have in field placements, student teaching, or internship may reinforce didactic preparation or it may actively encourage students to disregard the didactic preparation they were provided. In addition, the features of a program may play out very differently based on its day-to-day management and faculty, so two programs that have similar design may achieve very different results based on the program faculty. The authors’ point is not to suggest that there is not an urgent need for research on what aspects of TPPs support improved student achievement outcomes in K-12 schools. Much the opposite, we believe it is a critical need. Rather, we raise these concerns to surface the complexity and subtlety of the work needed to generate the sort of broadly applicable knowledge the field needs.</p>
</sec>
</sec>
</body>
<back>
<app-group>
<app id="app1-0022487112439894">
<title>Appendix</title>
<sec id="section19-0022487112439894">
<title>Hierarchical linear modeling for Louisiana’s teacher preparation programs (TPPs)</title>
<p>The analysis was based on hierarchical linear models (HLM; <xref ref-type="bibr" rid="bibr26-0022487112439894">McCulloch &amp; Searle, 2001</xref>; <xref ref-type="bibr" rid="bibr37-0022487112439894">Raudenbush &amp; Bryk, 2002</xref>) using achievement scores from the State’s mandated testing programs in Grades 3 through 9; demographic data for the students, teachers, and schools; attendance data for teachers and students; teachers’ certification information; and the TPP they completed (described previously). The result estimates the degree to which students taught by new teachers from specific TPPs achieve more or less than would be predicted based on an extensive set of student, class, and school predictors in five content areas. HLM allows for correlation of error terms within nested units, which is appropriate to the structure of these data.</p>
<p>Data for teachers who completed redesigned programs in Louisiana and who were in their first or second year of teaching in 2007-2008, 2006-2007, and 2005-2006 were combined and analyzed together. Data for new teachers who completed old programs were considered new teachers when calculating overall new teacher estimates, but were not members of the reported universities’ new teachers. Five sets of analyses were completed: one for each content area (English-language arts [ELA], reading, mathematics, science, and social studies). The dependent variable for each content area was the achievement test score. All 3 years were analyzed within a single equation per content. Predictors for all years were entered separately with predictors for the current year containing data for that year and predictors for the other two year set to 0 (interacted with year). All TPPs had a common identification code for their 3 years of data to allow extraction of cross-year coefficients and standard errors from the pooled data. Schools, teachers, and students were treated as independent observations across years.</p>
</sec>
<sec id="section20-0022487112439894">
<title>Building the base model of student achievement</title>
<p>Model development generally paralleled the sequential approach described by <xref ref-type="bibr" rid="bibr41-0022487112439894">Tekwe et al. (2004)</xref>, but was implemented somewhat differently based on the specific data and research questions. Analysis was completed separately for each content area and for each of three academic years independently: 2005-2006, 2006-2007, and 2007-2008. Further detailed descriptions of the development and results can be found in Noell and colleagues’ reports (<xref ref-type="bibr" rid="bibr31-0022487112439894">Noell, Porter, &amp; Patt, 2007</xref>; <xref ref-type="bibr" rid="bibr32-0022487112439894">Noell, Porter, Patt, &amp; Dahir, 2008</xref>; <xref ref-type="bibr" rid="bibr30-0022487112439894">Noell, Gansle, Patt, &amp; Schafer, 2009</xref>). Prior to building the value-added model for TPPs, student achievement was modeled using a nesting structure of students within teachers/classrooms and teachers/classrooms within schools. This structure was replicated across the five content areas. Please see <xref ref-type="fig" rid="fig1-0022487112439894">Figure 1</xref> for a graphic describing the nesting structure.</p>
<p>At each of the three levels, error was considered normally distributed with a mean of 0 and common variance. For each content area, an initial three-level model was specified for achievement with no prior predictors. The next step entered prior achievement in ELA, mathematics, reading, science, and social studies. For each content area, all effects were significant and were retained in the model for all content areas and years. Next, the demographic data for each student were entered as a block and removed one at a time in the order of the lowest <italic>t</italic> value until all remaining predictors were significant at <italic>p</italic> &lt; .01.</p>
<p>Once the model for student achievement was developed for each content area, several classroom/teacher variables and school variables were examined. As with the student-level factors, the classroom variables were entered as a block and removed one at a time in order of the smallest <italic>t</italic> value for the coefficient. When only those that were significant at <italic>p</italic> &lt; .01 were retained, the same process was repeated across content areas for the school level of the model. All of the variables that were tested at each level are listed in Table 2.</p>
<p>Based on these analyses, variables were retained at each of the three levels that would predict student achievement in each content area prior to consideration of TPP effects. Models were developed for each content area for intercepts as outcomes. At the student level, prior achievement, demographic variables, and attendance were retained as predictors of test performance. At the teacher/classroom level, classroom covariates were entered as predictors of the classroom mean or Level 1 intercept, and this effect was modeled as random. Classroom- and school-level covariates were used to adjust intercepts for students and classrooms, respectively. No covariates were used to predict lower level coefficients; all coefficients were treated as fixed. Error variance was modeled for intercepts only. A presentation of the model appears in <xref ref-type="table" rid="table5-0022487112439894">appendix Table 1</xref>. Only equations for intercepts are presented here; all Level 2 and Level 3 models for Level 1 coefficients were fixed. In these equations, Σ is used to indicate summing across the <italic>p, q</italic>, and <italic>s</italic> coefficients at the student, teacher/classroom, and school levels of the model, respectively.</p>
<table-wrap id="table5-0022487112439894" position="float">
<label>Appendix Table 1.</label>
<caption><p>Equations for Student, Classroom/Teacher, and School Levels of the Model</p></caption>
<graphic alternate-form-of="table5-0022487112439894" xlink:href="10.1177_0022487112439894-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Level 1:</th>
<th align="center">Students</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="2"><inline-formula id="inline-formula1-0022487112439894">
<mml:math display="inline" id="math1-0022487112439894">
<mml:mrow>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mrow>
<mml:mtext>ijk</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>π</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mtext>jk</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mstyle>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:msub>
<mml:mi>π</mml:mi>
<mml:mrow>
<mml:mtext>pjk</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mi>a</mml:mi>
<mml:mrow>
<mml:mtext>pijk</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mstyle>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mtext>ijk</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula></td>
</tr>
<tr>
<td>where</td>
<td/>
</tr>
<tr>
<td><italic>Y</italic><sub>ijk</sub></td>
<td>is the achievement of student <italic>i</italic> in class <italic>j</italic> at school <italic>k</italic> in the target subject</td>
</tr>
<tr>
<td>π<sub>0jk</sub></td>
<td>is the mean achievement for classroom <italic>j</italic> at school <italic>k</italic></td>
</tr>
<tr>
<td>π<sub>pjk</sub></td>
<td>are the <italic>p</italic> coefficients that weight the contribution of the student-level data in the prediction of Y for <italic>p</italic> = 1 to the total number of coefficients</td>
</tr>
<tr>
<td><italic>a</italic><sub>pijk</sub></td>
<td>are the student-level data (prior achievement, demographic variables, and attendance) that predict achievement for <italic>p</italic> = 1 to the total number of data points</td>
</tr>
<tr>
<td><italic>e</italic><sub>ijk</sub></td>
<td>the student-level random effect, the deviation of the predicted score of student i in classroom j in school k from the obtained score</td>
</tr>
<tr>
<td>Level 2:</td>
<td>Classrooms/teachers</td>
</tr>
<tr>
<td colspan="2"><inline-formula id="inline-formula2-0022487112439894">
<mml:math display="inline" id="math2-0022487112439894">
<mml:mrow>
<mml:msub>
<mml:mi>π</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mtext>jk</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mn>00</mml:mn>
<mml:mtext>k</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mstyle>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mtext>q</mml:mtext>
<mml:mn>0</mml:mn>
<mml:mtext>k</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mrow>
<mml:mtext>q</mml:mtext>
<mml:mn>0</mml:mn>
<mml:mtext>jk</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mstyle>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>r</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mtext>jk</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula></td>
</tr>
<tr>
<td>where</td>
<td/>
</tr>
<tr>
<td>π<sub>0jk</sub></td>
<td>is the mean achievement for classroom <italic>j</italic> at school <italic>k</italic></td>
</tr>
<tr>
<td>β<sub>00k</sub></td>
<td>is the mean achievement for school <italic>k</italic></td>
</tr>
<tr>
<td>β<sub>q0k</sub></td>
<td>are the <italic>q</italic> coefficients that weight the weight the relationship between the classroom characteristics and π<sub>0jk</sub>, <italic>q</italic> = 1 to the total number of coefficients</td>
</tr>
<tr>
<td><italic>X</italic><sub>q0jk</sub></td>
<td>are the classroom level data that are used to predict achievement; this is also the location in the model at which codes for recent TPP completers are entered (described below)</td>
</tr>
<tr>
<td><italic>r</italic><sub>0jk</sub></td>
<td>the classroom level random effect, the deviation of classroom jk’s measured classroom mean from its predicted mean</td>
</tr>
<tr>
<td>Level 3:</td>
<td>Schools</td>
</tr>
<tr>
<td colspan="2"><inline-formula id="inline-formula3-0022487112439894">
<mml:math display="inline" id="math3-0022487112439894">
<mml:mrow>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mn>00</mml:mn>
<mml:mtext>k</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>000</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mstyle>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mtext>s</mml:mtext>
<mml:mn>00</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mi>W</mml:mi>
<mml:mrow>
<mml:mtext>s</mml:mtext>
<mml:mn>00</mml:mn>
<mml:mtext>k</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mstyle>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>u</mml:mi>
<mml:mrow>
<mml:mn>00</mml:mn>
<mml:mtext>k</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula></td>
</tr>
<tr>
<td>where</td>
<td/>
</tr>
<tr>
<td>β<sub>00k</sub></td>
<td>is the mean achievement for school <italic>k</italic></td>
</tr>
<tr>
<td>γ<sub>000</sub></td>
<td>is the grand mean achievement in the target subject</td>
</tr>
<tr>
<td>γ<sub>s00</sub></td>
<td>are the s coefficients that weight the weight the relationship between the school characteristics and β<sub>00k</sub> for s = 1 to the total number of coefficients</td>
</tr>
<tr>
<td><italic>W</italic><sub>s00k</sub></td>
<td>are the school level data that are used to predict achievement</td>
</tr>
<tr>
<td><italic>u</italic><sub>00k</sub></td>
<td>the school level random effect, the deviation of school k’s measured classroom mean from its predicted mean</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Using these models, coefficients were determined for each content area that would describe the relationship between each predictor variable and the achievement score in that area for that year for each student. For each content area, the coefficients were scaled to the approximate standard deviation of the Louisiana tests: 50. Generally, the best predictor of the current year’s achievement was the previous year’s achievement in that content area. HLMs for each content area were run to determine the base model for student achievement (<xref ref-type="bibr" rid="bibr30-0022487112439894">Noell, Gansle, Patt, &amp; Schafer, 2009</xref>); the full model for mathematics for 2008 achievement scores is presented in appendix <xref ref-type="table" rid="table5-0022487112439894">Table 1</xref> to illustrate the outcome of this process. Variables are scaled in different ways; these are important to carefully consider their meaning when comparing coefficients across variables. Demographic variables are coded as “1” if present and “0” if absent. Prior achievement is scaled in standard deviation units compared with grand mean prior achievement. Classroom percentages are measured in 10% units; they indicate how the students’ scores would change if the percentage of the indicated group changed by 10%.</p>
<p>Generally, across content areas, the models had much in common. Prior achievement in the predicted content area had the largest coefficient among prior achievement scores, with different loadings from the remainder of the prior content area achievement scores. Having a special education diagnosis was consistently a strong negative predictor of achievement. Student absences and free or reduced lunch status had small but consistent coefficients across content areas. No single ethnicity variable was consistently statistically significant and always loaded in the same direction.</p>
</sec>
</app>
</app-group>
<ack><p>The authors would like to thank Mike Collier; Allen Schulenberg; David Elder; the Division of Planning, Analysis, and Information Resources of the Louisiana Department of Education; and the Louisiana Board of Regents, without whom this work would not have been possible.</p></ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This research has been supported by award CT-06/07-VAA-01 from the Louisiana Board of Regents, which was funded by a grant from the Carnegie Corporation of New York.</p>
</fn>
</fn-group>
<bio>
<title>About the Authors</title>
<p><bold>Kristin A. Gansle</bold>, PhD, is an associate professor of Curriculum &amp; Instruction and Special Education Programs at Louisiana State University, Baton Rouge, Louisiana. Her recent research interests include value-added assessment of teacher preparation programs, correlates of quality teacher preparation programs, and improving teacher preparation.</p>
<p><bold>George H. Noell</bold>, PhD, is a professor of psychology at Louisiana State University, Baton Rouge, Louisiana. His research interests include value-added assessment of teacher preparation, behavioral consultation, intervention implementation, and assessment procedures that have treatment utility. He has worked closely with the Department of Education and Board of Regents in Louisiana to improve all students’ access to effective schools.</p>
<p><bold>Jeanne M. Burns</bold>, PhD, is associate commissioner for teacher and leadership initiatives for the Louisiana Board of Regents. Her work has focused on increasing children’s access to high-quality education and improving educational leaders in Louisiana. Her policy, program development, and research efforts have included K-12 and university environments as well as teaching in both environments.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anderman</surname><given-names>E. M.</given-names></name>
<name><surname>Anderman</surname><given-names>L. H.</given-names></name>
<name><surname>Yough</surname><given-names>M. S.</given-names></name>
<name><surname>Gimbert</surname><given-names>B. G.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Value-added models of assessment: Implications for motivation and accountability</article-title>. <source>Educational Psychologist</source>, <volume>45</volume>, <fpage>123</fpage>-<lpage>137</lpage>. doi:<pub-id pub-id-type="doi">10.1080/00461521003703045</pub-id></citation>
</ref>
<ref id="bibr2-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>N.</given-names></name>
</person-group> (<year>2011</year>, <month>March</month> <day>10</day>). <article-title>Most schools could face “failing” label under No Child Left Behind, Duncan says</article-title>. <source>The Washington Post</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.thewashingtonpost.com/local/education/duncan-most-schools-could-face-failing-label/2011/03/09/AB7L2hP_story.html">http://www.thewashingtonpost.com/local/education/duncan-most-schools-could-face-failing-label/2011/03/09/AB7L2hP_story.html</ext-link></citation>
</ref>
<ref id="bibr3-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Baker</surname><given-names>E. L.</given-names></name>
<name><surname>Barton</surname><given-names>P. E.</given-names></name>
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
<name><surname>Haertel</surname><given-names>E.</given-names></name>
<name><surname>Ladd</surname><given-names>H. F.</given-names></name>
<name><surname>Linn</surname><given-names>R. L.</given-names></name>
<name><surname>. . .Shepard</surname><given-names>L. A.</given-names></name>
</person-group> (<year>2010</year>). <source>Problems with the use of student test scores to evaluate teachers</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>The Economic Policy Institute</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://epi.3cdn.net/b9667271ee6c154195_t9m6iij8k.pdf">http://epi.3cdn.net/b9667271ee6c154195_t9m6iij8k.pdf</ext-link></citation>
</ref>
<ref id="bibr4-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ballou</surname><given-names>D.</given-names></name>
<name><surname>Sanders</surname><given-names>W.</given-names></name>
<name><surname>Wright</surname><given-names>P.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Controlling for student background in value-added assessment of teachers</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>29</volume>, <fpage>37</fpage>-<lpage>65</lpage>. doi:<pub-id pub-id-type="doi">10.3102/10769986029001037</pub-id></citation>
</ref>
<ref id="bibr5-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boyd</surname><given-names>D. J.</given-names></name>
<name><surname>Grossman</surname><given-names>P. L.</given-names></name>
<name><surname>Lankford</surname><given-names>H.</given-names></name>
<name><surname>Loeb</surname><given-names>S.</given-names></name>
<name><surname>Wyckoff</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Teacher preparation and student achievement</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>31</volume>, <fpage>416</fpage>-<lpage>440</lpage>. doi:<pub-id pub-id-type="doi">10.3102/0162373709353129</pub-id></citation>
</ref>
<ref id="bibr6-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cochran-Smith</surname><given-names>M.</given-names></name>
<name><surname>Zeichner</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2005a</year>). <article-title>Executive summary</article-title>. In <source>Studying teacher education: The report of the AERA panel on research and teacher education</source> (pp. <fpage>1</fpage>-<lpage>36</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr7-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cochran-Smith</surname><given-names>M.</given-names></name>
<name><surname>Zeichner</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2005b</year>). <source>Studying teacher education</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr8-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
<name><surname>Bransford</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <source>Preparing teachers for a changing world</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr9-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>DeWeese</surname><given-names>T.</given-names></name>
</person-group> (<year>2007</year>, <month>December</month> <day>25</day>). <source>Why is public education failing?</source> Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.newswithviews.com/DeWeese/tom100.htm">http://www.newswithviews.com/DeWeese/tom100.htm</ext-link></citation>
</ref>
<ref id="bibr10-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Duckworth</surname><given-names>A. L.</given-names></name>
<name><surname>Quinn</surname><given-names>P. D.</given-names></name>
<name><surname>Seligman</surname><given-names>M. E. P.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Positive predictors of teacher effectiveness</article-title>. <source>Journal of Positive Psychology</source>, <volume>4</volume>, <fpage>540</fpage>-<lpage>547</lpage>. doi:<pub-id pub-id-type="doi">10.1080/17439760903157232</pub-id></citation>
</ref>
<ref id="bibr11-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Glazerman</surname><given-names>S.</given-names></name>
<name><surname>Loeb</surname><given-names>S.</given-names></name>
<name><surname>Goldhaber</surname><given-names>D.</given-names></name>
<name><surname>Raudenbush</surname><given-names>S.</given-names></name>
<name><surname>Staiger</surname><given-names>D.</given-names></name>
<name><surname>Whitehurst</surname><given-names>G.</given-names></name>
</person-group> (<year>2010</year>). <source>Evaluating teachers: The important role of value-added</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>The Brown Center on Education Policy at Brookings</publisher-name>.</citation>
</ref>
<ref id="bibr12-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldhaber</surname><given-names>D.</given-names></name>
<name><surname>Brewer</surname><given-names>D.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Why don’t schools and teachers seem to matter? Measuring the impact of unobservables on educational productivity</article-title>. <source>Journal of Human Resources</source>, <volume>32</volume>, <fpage>505</fpage>-<lpage>523</lpage>.</citation>
</ref>
<ref id="bibr13-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hanson</surname><given-names>G. R.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Critical issues in the assessment of value added in education</article-title>. <source>New Directions for Institutional Research</source>, <volume>59</volume>, <fpage>53</fpage>-<lpage>67</lpage>. doi:<pub-id pub-id-type="doi">10.1002/ir.37019885907</pub-id></citation>
</ref>
<ref id="bibr14-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Harris</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The policy uses and policy validity of value-added and other quality measures</article-title>. In <person-group person-group-type="editor">
<name><surname>Gitomer</surname><given-names>D.</given-names></name>
</person-group> (Ed.). <source>Measurement issues and assessment for teaching quality</source> (pp. <fpage>99</fpage>-<lpage>130</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr15-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hart</surname><given-names>C. W. L.</given-names></name>
<name><surname>Bogan</surname><given-names>C. E.</given-names></name>
</person-group> (<year>1992</year>). <source>The Baldrige: What it is, how it’s won, how to use it to improve quality in your company</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr16-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Heitin</surname><given-names>L.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Evaluation system weighing down Tennessee teachers</article-title>. <source>Education Week</source>, <volume>31</volume>(<issue>8</issue>), <volume>1</volume>, <fpage>14</fpage>-<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr17-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hershberg</surname><given-names>T.</given-names></name>
<name><surname>Simon</surname><given-names>V.</given-names></name>
<name><surname>Lea-Kruger</surname><given-names>B.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The revelations of value-added: An assessment model that measures student growth in ways that NCLB fails to do</article-title>. <source>School Administrator</source>, <volume>61</volume>(<issue>11</issue>), <fpage>10</fpage>-<lpage>13</lpage>.</citation>
</ref>
<ref id="bibr18-0022487112439894">
<citation citation-type="other">
<collab>Individuals with Disabilities Education Improvement Act</collab>. (<year>2004</year>). Pub. L. No 108-446, 20 USC 1400.</citation>
</ref>
<ref id="bibr19-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Koretz</surname><given-names>D. M.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Limitations in the use of achievement tests as measures of educators’ productivity</article-title>. <source>Journal of Human Resources</source>, <volume>37</volume>, <fpage>752</fpage>-<lpage>777</lpage>. doi:<pub-id pub-id-type="doi">10.2307/3069616</pub-id></citation>
</ref>
<ref id="bibr20-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Krovetz</surname><given-names>M. L.</given-names></name>
</person-group> (<year>2008</year>). <source>Fostering resilience: Expecting all students to use their minds and hearts well</source> (<edition>2nd ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin Press</publisher-name>.</citation>
</ref>
<ref id="bibr21-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lasley</surname><given-names>T. J.</given-names><suffix>II</suffix></name>
<name><surname>Siedentop</surname><given-names>D.</given-names></name>
<name><surname>Yinger</surname><given-names>R.</given-names></name>
</person-group> (<year>2006</year>). <article-title>A systemic approach to enhancing teacher quality: The Ohio model</article-title>. <source>Journal of Teacher Education</source>, <volume>57</volume>, <fpage>13</fpage>-<lpage>21</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0022487105284455</pub-id></citation>
</ref>
<ref id="bibr22-0022487112439894">
<citation citation-type="web">
<collab>Louisiana Department of Education</collab>. (<year>2008a</year>). <article-title>iLEAP 2008 technical summary</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.louisianaschools.net/lde/uploads/13125.pdf">http://www.louisianaschools.net/lde/uploads/13125.pdf</ext-link></citation>
</ref>
<ref id="bibr23-0022487112439894">
<citation citation-type="web">
<collab>Louisiana Department of Education</collab>. (<year>2008b</year>). <article-title>LEAP GEE 2008 technical summary</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.louisianaschools.net/lde/uploads/13306.pdf">http://www.louisianaschools.net/lde/uploads/13306.pdf</ext-link></citation>
</ref>
<ref id="bibr24-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McCaffrey</surname><given-names>D. F.</given-names></name>
<name><surname>Lockwood</surname><given-names>J. R.</given-names></name>
<name><surname>Kortez</surname><given-names>D. M.</given-names></name>
<name><surname>Hamilton</surname><given-names>L. S.</given-names></name>
</person-group> (<year>2003</year>). <source>Evaluating value-added models for teacher accountability</source>. <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>RAND Corporation</publisher-name>.</citation>
</ref>
<ref id="bibr25-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McCaffrey</surname><given-names>D. F.</given-names></name>
<name><surname>Lockwood</surname><given-names>J. R.</given-names></name>
<name><surname>Kortez</surname><given-names>D. M.</given-names></name>
<name><surname>Louis</surname><given-names>T. A.</given-names></name>
<name><surname>Hamilton</surname><given-names>L. S.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Models for value-added modeling of teacher effects</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>29</volume>, <fpage>67</fpage>-<lpage>102</lpage>. doi:<pub-id pub-id-type="doi">10.3102/10769986029001067</pub-id></citation>
</ref>
<ref id="bibr26-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McCulloch</surname><given-names>C. E.</given-names></name>
<name><surname>Searle</surname><given-names>S. R.</given-names></name>
</person-group> (<year>2001</year>). <source>Generalized, linear, and mixed models</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr27-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McNeil</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Starting gun sounds for “Race to the Top.”</article-title> <source>Education Week</source>, <volume>29</volume>(<issue>12</issue>), <volume>1</volume>, <fpage>18</fpage>-<lpage>19</lpage>.</citation>
</ref>
<ref id="bibr28-0022487112439894">
<citation citation-type="other">
<collab>No Child Left Behind Act</collab>. (<year>2001</year>). Pub. L. No. &gt;107–110, §1601 <italic>et seq</italic>. (2002).</citation>
</ref>
<ref id="bibr29-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Burns</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Value-added assessment of teacher preparation: An illustration of emerging technology</article-title>. <source>Journal of Teacher Education</source>, <volume>57</volume>, <fpage>37</fpage>-<lpage>50</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0022487105284466</pub-id></citation>
</ref>
<ref id="bibr30-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Gansle</surname><given-names>K. A.</given-names></name>
<name><surname>Patt</surname><given-names>R. M.</given-names></name>
<name><surname>Schafer</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2009</year>). <source>Value added assessment of teacher preparation in Louisiana: 2005-2006 to 2007-2008</source>. <publisher-loc>Baton Rouge, LA</publisher-loc>: <publisher-name>Louisiana State University Department of Psychology</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.laregentsarchive.com/Academic/TE/2009/2008-09VATechnical(8.24.09).pdf">http://www.laregentsarchive.com/Academic/TE/2009/2008-09VATechnical(8.24.09).pdf</ext-link></citation>
</ref>
<ref id="bibr31-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Porter</surname><given-names>B. A.</given-names></name>
<name><surname>Patt</surname><given-names>R. M.</given-names></name>
</person-group> (<year>2007</year>). <source>Value added assessment of teacher preparation in Louisiana: 2004-2006</source>. <publisher-loc>Baton Rouge, LA</publisher-loc>: <publisher-name>Louisiana State University Department of Psychology</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.laregentsarchive.com/Academic/TE/2008/Final%20Value-Added%20Report%20(12.02.08).pdf">http://www.laregentsarchive.com/Academic/TE/2008/Final%20Value-Added%20Report%20(12.02.08).pdf</ext-link></citation>
</ref>
<ref id="bibr32-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Porter</surname><given-names>B. A.</given-names></name>
<name><surname>Patt</surname><given-names>R. M.</given-names></name>
<name><surname>Dahir</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>).<source>Value added assessment of teacher preparation in Louisiana: 2004-2005 to 2006-2007</source>. <publisher-loc>Baton Rouge, LA</publisher-loc>: <publisher-name>Louisiana State University Department of Psychology</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.laregentsarchive.com/Academic/TE/2009/2008-09VATechnical(8.24.09).pdf">http://www.laregentsarchive.com/Academic/TE/2009/2008-09VATechnical(8.24.09).pdf</ext-link></citation>
</ref>
<ref id="bibr33-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Olson</surname><given-names>L.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Houston board OKs revamped performance-pay plan</article-title>. <source>Education Week</source>, <volume>27</volume>(<issue>4</issue>), <fpage>11</fpage>.</citation>
</ref>
<ref id="bibr34-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Papay</surname><given-names>J. P.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Different tests, different answers: The stability of teacher value-added estimates across outcome measures</article-title>. <source>American Educational Research Journal</source>, <volume>48</volume>, <fpage>163</fpage>-<lpage>193</lpage>. doi:<pub-id pub-id-type="doi">10.3102/0002831210362589</pub-id></citation>
</ref>
<ref id="bibr35-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Postal</surname><given-names>L.</given-names></name>
<name><surname>Roth</surname><given-names>L.</given-names></name>
</person-group> (<year>2011</year>, <month>July</month> <day>19</day>). <article-title>More than 150 “failing” schools must allow student transfers</article-title>. <source>The Orlando Sentinel</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://articles.orlandosentinel.com">http://articles.orlandosentinel.com</ext-link></citation>
</ref>
<ref id="bibr36-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Raudenbush</surname><given-names>S. W.</given-names></name>
</person-group> (<year>2004</year>). <article-title>What are value-added models estimating and what does this imply for statistical practice?</article-title> <source>Journal of Educational and Behavioral Statistics</source>, <volume>29</volume>, <fpage>121</fpage>-<lpage>129</lpage>. doi:<pub-id pub-id-type="doi">10.3102/10769986029001121</pub-id></citation>
</ref>
<ref id="bibr37-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Raudenbush</surname><given-names>S. W.</given-names></name>
<name><surname>Bryk</surname><given-names>A. S.</given-names></name>
</person-group> (<year>2002</year>). <source>Hierarchical linear models: Applications and data analysis methods</source> (<edition>2nd ed.</edition>). <publisher-loc>London, England</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr38-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reusser</surname><given-names>J.</given-names></name>
<name><surname>Butler</surname><given-names>L.</given-names></name>
<name><surname>Symonds</surname><given-names>M.</given-names></name>
<name><surname>Vetter</surname><given-names>R.</given-names></name>
<name><surname>Wall</surname><given-names>T. J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>An assessment system for teacher education program quality improvement</article-title>. <source>International Journal of Educational Management</source>, <volume>21</volume>, <fpage>105</fpage>-<lpage>113</lpage>. doi:<pub-id pub-id-type="doi">10.1108/09513540710729908</pub-id></citation>
</ref>
<ref id="bibr39-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rice</surname><given-names>J. K.</given-names></name>
</person-group> (<year>2003</year>). <source>Teacher quality: Understanding the effectiveness of teacher attributes</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Economic Policy Institute</publisher-name>.</citation>
</ref>
<ref id="bibr40-0022487112439894">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Sawchuk</surname><given-names>S.</given-names></name>
</person-group> (<year>2010</year>). <article-title>New vigor propelling training</article-title>. <source>Education Week</source>, <volume>30</volume>(<issue>13</issue>), <fpage>1</fpage>-<lpage>20</lpage>. Available from <ext-link ext-link-type="uri" xlink:href="http://search.ebscohost.com">http://search.ebscohost.com</ext-link></citation>
</ref>
<ref id="bibr41-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tekwe</surname><given-names>C. D.</given-names></name>
<name><surname>Carter</surname><given-names>R. L.</given-names></name>
<name><surname>Ma</surname><given-names>C.</given-names></name>
<name><surname>Algina</surname><given-names>J.</given-names></name>
<name><surname>Lucas</surname><given-names>M. E.</given-names></name>
<name><surname>Roth</surname><given-names>J.</given-names></name>
<name><surname>. . .Resnick</surname><given-names>M. B.</given-names></name>
</person-group> (<year>2004</year>). <article-title>An empirical comparison of statistical models for value added assessment of school performance</article-title>. <source>Journal of Educational and Behavioral Statistics</source>, <volume>29</volume>, <fpage>11</fpage>-<lpage>37</lpage>. doi:<pub-id pub-id-type="doi">10.3102/10769986029001011</pub-id></citation>
</ref>
<ref id="bibr42-0022487112439894">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Todd</surname><given-names>P. E.</given-names></name>
<name><surname>Wolpin</surname><given-names>K. I.</given-names></name>
</person-group> (<year>2003</year>).<article-title>On the specification and estimation of the production function for cognitive achievement</article-title>. <source>Economic Journal</source>, <volume>113</volume>, <fpage>3</fpage>-<lpage>33</lpage>. doi:<pub-id pub-id-type="doi">10.1111/1468-0297.00097</pub-id></citation>
</ref>
<ref id="bibr43-0022487112439894">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wilson</surname><given-names>S.</given-names></name>
<name><surname>Floden</surname><given-names>R. Y.</given-names></name>
<name><surname>Ferrini-Mundy</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <source>Teacher preparation research: Content knowledge, gaps and recommendations</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>