<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612460407</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612460407</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Attention Is Spontaneously Biased Toward Regularities</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Zhao</surname><given-names>Jiaying</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612460407">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Al-Aidroos</surname><given-names>Naseem</given-names></name>
<xref ref-type="aff" rid="aff2-0956797612460407">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Turk-Browne</surname><given-names>Nicholas B.</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612460407">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0956797612460407"><label>1</label>Princeton University</aff>
<aff id="aff2-0956797612460407"><label>2</label>University of Guelph</aff>
<author-notes>
<corresp id="corresp1-0956797612460407">Jiaying Zhao, Department of Psychology, Green Hall, Princeton University, Princeton, NJ 08540 E-mail: <email>jiayingz@princeton.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>24</volume>
<issue>5</issue>
<fpage>667</fpage>
<lpage>677</lpage>
<history>
<date date-type="received">
<day>18</day>
<month>3</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>7</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>Knowledge about regularities in the environment can be used to facilitate perception, memory, and language acquisition. Given this usefulness, we hypothesized that statistically structured sources of information receive attentional priority over noisier sources, independent of their intrinsic salience or goal relevance. We report three experiments that support this hypothesis. Experiment 1 shows that regularities bias spatial attention: Visual search was facilitated at a location containing temporal regularities, even though these regularities did not predict target location, timing, or identity. Experiments 2 and 3 show that regularities bias feature attention: Attentional capture doubled in magnitude when singletons appeared, respectively, in a color or dimension with temporal regularities among task-irrelevant stimuli. Prioritization of the locations and features of regularities is not easily accounted for in the conventional dichotomy between stimulus-driven and goal-directed attention. This prioritization may in turn promote further statistical learning, helping the mind to acquire knowledge about stable aspects of the environment.</p>
</abstract>
<kwd-group>
<kwd>statistical learning</kwd>
<kwd>attentional capture</kwd>
<kwd>cognitive control</kwd>
<kwd>feature-based attention</kwd>
<kwd>spatial attention</kwd>
<kwd>visual search</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The environment is highly structured, containing widespread regularities in terms of how objects co-occur over space and time. The mind extracts regularities via statistical learning (<xref ref-type="bibr" rid="bibr13-0956797612460407">Fiser &amp; Aslin, 2001</xref>; <xref ref-type="bibr" rid="bibr27-0956797612460407">Perruchet &amp; Pacton, 2006</xref>; <xref ref-type="bibr" rid="bibr29-0956797612460407">Saffran, Aslin, &amp; Newport, 1996</xref>; <xref ref-type="bibr" rid="bibr38-0956797612460407">Turk-Browne, Scholl, Chun, &amp; Johnson, 2009</xref>) and can use them to perceive and interact with the environment more efficiently. For example, exposure to regularities improves learning of object labels (<xref ref-type="bibr" rid="bibr18-0956797612460407">Graf Estes, Evans, Alibali, &amp; Saffran, 2007</xref>), facilitates object categorization (<xref ref-type="bibr" rid="bibr39-0956797612460407">Turk-Browne, Scholl, Johnson, &amp; Chun, 2010</xref>), and expands visual short-term memory capacity (<xref ref-type="bibr" rid="bibr4-0956797612460407">Brady, Konkle, &amp; Alvarez, 2009</xref>; <xref ref-type="bibr" rid="bibr40-0956797612460407">Umemoto, Scolari, Vogel, &amp; Awh, 2010</xref>). Given the prevalence and usefulness of regularities, we hypothesized that structured sources of information receive attentional priority in the context of other, noisier sources of information.</p>
<p>Attention is known to be controlled by two factors (<xref ref-type="bibr" rid="bibr8-0956797612460407">Chun, Golomb, &amp; Turk-Browne, 2011</xref>; <xref ref-type="bibr" rid="bibr12-0956797612460407">Corbetta &amp; Shulman, 2002</xref>; <xref ref-type="bibr" rid="bibr22-0956797612460407">Jonides, 1981</xref>; <xref ref-type="bibr" rid="bibr25-0956797612460407">Pashler, Johnston, &amp; Ruthruff, 2001</xref>; <xref ref-type="bibr" rid="bibr28-0956797612460407">Posner, 1980</xref>; <xref ref-type="bibr" rid="bibr42-0956797612460407">Yantis, 2000</xref>). First, attention can be driven exogenously by salient external stimuli (<xref ref-type="bibr" rid="bibr20-0956797612460407">Itti &amp; Koch, 2001</xref>; <xref ref-type="bibr" rid="bibr41-0956797612460407">Wolfe &amp; Horowitz, 2004</xref>), such as unique features (<xref ref-type="bibr" rid="bibr32-0956797612460407">Theeuwes, 1992</xref>), abrupt onsets (<xref ref-type="bibr" rid="bibr43-0956797612460407">Yantis &amp; Jonides, 1984</xref>), new and looming motion (<xref ref-type="bibr" rid="bibr1-0956797612460407">Abrams &amp; Christ, 2003</xref>; <xref ref-type="bibr" rid="bibr16-0956797612460407">Franconeri &amp; Simons, 2003</xref>), and novelty (<xref ref-type="bibr" rid="bibr21-0956797612460407">Johnston, Hawley, Plew, Elliott, &amp; DeWitt, 1990</xref>). Second, attention can be directed endogenously by internal goals or task rules (<xref ref-type="bibr" rid="bibr19-0956797612460407">Hopfinger, Buonocore, &amp; Mangun, 2000</xref>; <xref ref-type="bibr" rid="bibr24-0956797612460407">Miller &amp; Cohen, 2001</xref>), which enhances the processing of goal-relevant stimuli irrespective of salience (<xref ref-type="bibr" rid="bibr2-0956797612460407">Bacon &amp; Egeth, 1994</xref>; <xref ref-type="bibr" rid="bibr15-0956797612460407">Folk, Remington, &amp; Johnston, 1992</xref>; cf. <xref ref-type="bibr" rid="bibr33-0956797612460407">Theeuwes, 2004</xref>).</p>
<p>We propose that attention is also biased by statistical regularities, in a way that is not cleanly accounted for by the stimulus-driven vs. goal-directed framework. Prioritization of regularities is not stimulus driven, in the sense that regularities reflect stable relationships learned over time. That is, the presence of regularities depends only on internal representations of prior experience and not on the salience of any given stimulus. Prioritization of regularities is also not goal directed, in the sense that statistical learning can occur incidentally during other tasks (<xref ref-type="bibr" rid="bibr39-0956797612460407">Turk-Browne et al., 2010</xref>). Moreover, this learning can be expressed even when not required by the current task (<xref ref-type="bibr" rid="bibr44-0956797612460407">Zhao, Ngo, McKendrick, &amp; Turk-Browne, 2011</xref>).</p>
<p>This form of prioritization is reminiscent of cases in which learning of regularities helps guide attention to expected locations (<xref ref-type="bibr" rid="bibr9-0956797612460407">Chun &amp; Jiang, 1998</xref>; <xref ref-type="bibr" rid="bibr31-0956797612460407">Summerfield, Lepsien, Gitelman, Mesulam, &amp; Nobre, 2006</xref>) and features (<xref ref-type="bibr" rid="bibr7-0956797612460407">Chalk, Seitz, &amp; Seriès, 2010</xref>; <xref ref-type="bibr" rid="bibr10-0956797612460407">Chun &amp; Jiang, 1999</xref>). The critical difference between such contextual cuing and the hypothesized prioritization of regularities is the idea that attention might be biased by regularities even when they do not provide information that is helpful for task performance (e.g., information about target location or identity). That is, we tested the more radical claim that regularities among task-irrelevant stimuli spontaneously establish implicit attentional biases that influence performance in other tasks.</p>
<p>In three experiments, we examined the attentional prioritization of locations and features containing regularities. Participants viewed multiple streams of objects while monitoring for occasional visual search arrays. The stream in one “structured” location (Experiment 1), color (Experiment 2), or feature dimension (Experiment 3) was generated from temporal regularities. The remaining “random” streams were generated by shuffling the order of objects. The structured and random streams (and regularities) were irrelevant to the primary visual search task, in terms of both when and where the target appeared and which response was needed. Nevertheless, we hypothesized that responses would be faster when targets happened to appear in the location of the structured stream than in the location of a random stream, and that singletons in the structured color or dimension would capture attention more strongly than singletons in a random color or dimension.</p>
<sec id="section1-0956797612460407">
<title>Experiment 1</title>
<p>The goal of this experiment was to test whether visual search is facilitated at a spatial location containing temporal regularities.</p>
<sec id="section2-0956797612460407">
<title>Participants</title>
<p>Twenty-five undergraduates (16 female, 9 male; mean age = 20.7 years) from Princeton University participated in the experiment for course credit. Participants reported normal or corrected-to-normal visual acuity and color vision and provided informed consent. The protocol was approved by the Princeton University Institutional Review Board (IRB).</p>
</sec>
<sec id="section3-0956797612460407">
<title>Stimuli</title>
<p>Displays consisted of black shapes subtending 3.3° on a white background in four locations: top, bottom, left, and right of a central fixation cross. The stimuli in each stream were selected from four separate sets of nine shapes (<xref ref-type="fig" rid="fig1-0956797612460407">Fig. 1b</xref>). The location of each stream was centered 5.1° from fixation and marked by a black outline. The stream in one location (counterbalanced across participants) was structured, whereas the streams in the three remaining locations were random. In the structured stream, the nine shapes were grouped arbitrarily for each participant into three “triplets” that remained constant throughout the experiment (e.g., ABC, DEF, GHI). Each triplet was repeated 50 times, and the order of triplets was randomized, with the constraint that no triplet could repeat back to back (e.g., DEFGHIDEFABCGHI . . .). In each random stream, the nine shapes were sequenced individually (i.e., not in triplets). Each shape was repeated 50 times, and the order of shapes was randomized, with the constraint that no shape could repeat back to back. Shapes from each stream appeared one at a time and synchronously, such that four shapes appeared per trial—one in each of the top, bottom, left, and right positions on the screen (<xref ref-type="fig" rid="fig1-0956797612460407">Fig. 1a</xref>). This procedure resulted in 450 trials (9 unique shapes per stream × 50 repetitions), with the frequency of each individual shape equated within and across streams.</p>
<fig id="fig1-0956797612460407" position="float">
<label>Fig. 1.</label>
<caption>
<p>Sample trial sequence, stimuli, and results from Experiment 1. Streams of shapes were shown at the top, bottom, left, and right of central fixation (a). These streams were interrupted occasionally by a visual search array that contained one <italic>T</italic> target (indicated here by an arrow) and three <italic>L</italic> distractors in the four stream locations. The task was to indicate whether the target pointed to the left or right. Each stream was generated from a different set of nine shapes (b). The stream in one location contained temporal regularities (the structured stream). The nine shapes in this stream were grouped into three triplets, whose members always appeared in the same order. The streams in the other locations did not contain temporal regularities (the random streams), with the nine shapes appearing in a random order. The graph shows mean response time in the visual search task as a function of whether the target appeared at the location of the structured stream or at the location of a random stream (c). Error bars indicate ±1 <italic>SEM</italic>. The asterisk indicates a significant difference between conditions (*<italic>p</italic> &lt; .05).</p>
</caption>
<graphic xlink:href="10.1177_0956797612460407-fig1.tif"/>
</fig>
<p>Shape streams were interrupted occasionally by a task-relevant visual search array. Each array contained one target and three distractors in the four stream locations, with the location of the target determined randomly. The target was a <italic>T</italic> shape rotated 90° (i.e., pointing left) or −90° (i.e., pointing right). The distractors were <italic>L</italic> shapes rotated 0° (i.e., pointing right) or 180° (i.e., pointing left), with the horizontal line offset 20% from either the bottom or top of the vertical line to increase discrimination difficulty. The pointing directions of the target and distractors were counterbalanced in each array. For example, if the target pointed right, then two distractors pointed left and one distractor pointed right. This ensured that any given distractor did not predict target orientation. The target appeared with equal frequency at each of the four locations. Thus, there was no benefit of attending to the structured stream for finding the target, nor was the target location informative about which stream was structured.</p>
</sec>
<sec id="section4-0956797612460407">
<title>Apparatus</title>
<p>Participants were seated 70 cm from a CRT monitor (refresh rate = 100 Hz). Stimuli were presented using MATLAB (The MathWorks, Natick, MA) and the Psychophysics Toolbox (<xref ref-type="bibr" rid="bibr5-0956797612460407">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bibr26-0956797612460407">Pelli, 1997</xref>).</p>
</sec>
<sec id="section5-0956797612460407">
<title>Procedure</title>
<p>The experiment consisted of an exposure phase that included the shape streams and visual search trials, followed by a test phase to assess whether participants learned regularities. During exposure, each shape trial contained one shape from each of the four streams for 750 ms followed by an interstimulus interval (ISI) of 750 ms. Participants were instructed to fixate while attending to the four locations in order to complete occasional visual search trials. There were 80 search trials interspersed randomly into the 450 shape trials, which resulted in an average of 1 search trial for every 5.6 shape trials (no search trials appeared back to back). Search arrays appeared for 750 ms, followed by an ISI that lasted a minimum of 750 ms. Participants indicated as quickly and accurately as possible whether the target pointed left or right (by pressing the “1” or “0” key, respectively). If they did not respond during the array presentation or ISI, the screen remained blank until response. Participants were first shown 5 random-shape trials and 1 search trial for practice and to clarify the instructions. Unlike many studies of statistical learning, in which the task during exposure is passive viewing, the search task in the present experiment provided a useful cover story for participants that helped obscure the purpose of the study.</p>
<p>To verify that participants were sensitive to regularities, we followed the exposure phase with a surprise two-alternative forced-choice test (<xref ref-type="bibr" rid="bibr14-0956797612460407">Fiser &amp; Aslin, 2002</xref>). In each trial, participants viewed two sequences of three shapes presented at fixation. Each shape appeared for 750 ms followed by a 750-ms ISI, and each sequence was separated by a 1,000-ms pause. Participants judged whether the first or second sequence seemed more familiar based on what they saw during exposure. One sequence was a triplet that had appeared repeatedly in the structured stream, and the other sequence was a “foil” composed of three shapes from the structured stream that never appeared sequentially. The foils were constructed by choosing one shape from each of the three triplets, which preserved their position in the original triplets (i.e., AEI, DHC, GBF). Each triplet was tested against each foil twice, which resulted in a total of 18 trials (3 triplets × 3 foils × 2 repetitions). This testing procedure equated the frequency of every triplet and foil at test, as well as the frequency of every individual shape. Thus, to discriminate triplets from foils, participants needed to know which particular shapes followed each other during exposure. The order of trials was randomized, and whether the triplet or foil appeared first was counterbalanced across trials.</p>
<p>After the test, an extensive debriefing was conducted. Participants were told about the triplets in the exposure phase and asked to report whether they had noticed any such regularities. Demand characteristics might encourage an affirmative response, so participants who reported awareness were additionally asked to identify which location contained regularities. Participants were asked several other questions, including how confident they were in the familiarity test, whether they used a strategy of attending to a specific location (and, if so, which location), and what they thought the experiment was testing.</p>
</sec>
<sec id="section6-0956797612460407">
<title>Results and discussion</title>
<p>Triplets were chosen over foils on 54.0% (<italic>SD</italic> = 7.9%) of test trials, which reveals that statistical learning occurred (chance = 50%), <italic>t</italic>(24) = 2.52, <italic>p</italic> = .02, <italic>d</italic> = 0.50. This finding demonstrates, for the first time, that temporal regularities at one spatial location can be learned when embedded among other random sources of input at different locations.</p>
<p>The critical test of our hypothesis was whether regularities were preferentially attended during the exposure phase. If they were, target discrimination should have been facilitated at the structured location compared with the random locations. The task used in this experiment elicited high accuracy, so we focused on response time (RT) as a more sensitive measure (accuracy data are reported in the Supplemental Material available online). Only trials with correct responses were included in RT analyses, and RTs greater than 3 standard deviations from the mean in each condition (1.2% of all trials) were excluded.</p>
<p>Target-discrimination RTs (<xref ref-type="fig" rid="fig1-0956797612460407">Fig. 1c</xref>) were faster for targets in the structured location than in the random locations, <italic>t</italic>(24) = 2.31, <italic>p</italic> = .03, <italic>d</italic> = 0.46. In debriefing, 7 participants reported noticing regularities during the exposure phase, but only 1 of these participants correctly identified which location contained regularities (chance = 25%; binomial test: <italic>p</italic> = .87). Regardless, the RT difference was reliable without these 7 participants, <italic>t</italic>(17) = 2.80, <italic>p</italic> = .01, <italic>d</italic> = 0.66.</p>
<p>Although shape streams were task irrelevant, search items appearing in the location of the structured stream were preferentially attended. These results suggest that attention is biased toward the locations of regularities.</p>
</sec>
</sec>
<sec id="section7-0956797612460407">
<title>Experiment 2</title>
<p>The goal of this experiment was to generalize the findings of Experiment 1 from spatial attention to feature attention by testing whether temporal regularities among shapes in a color enhance attentional capture by that color.</p>
<sec id="section8-0956797612460407">
<title>Participants</title>
<p>Twenty new Princeton University undergraduates (12 female, 8 male; mean age = 20.1 years) participated in the experiment for course credit. Participants reported normal or corrected-to-normal visual acuity and color vision and provided informed consent. The protocol was approved by the Princeton University IRB.</p>
</sec>
<sec id="section9-0956797612460407">
<title>Stimuli</title>
<p>Two sets of shapes were randomly selected for each participant from the sets used in Experiment 1. The shapes in one set were colored red (red, green, blue values: 255, 0, 0, respectively) and the shapes in the other set were colored green (red, green, blue values: 0, 255, 0, respectively). One color was chosen for the structured set and the other color for the random set, and this color assignment was counterbalanced across participants (<xref ref-type="fig" rid="fig2-0956797612460407">Fig. 2b</xref>). Shapes in the structured set were grouped into three triplets, and a temporal stream was generated by pseudorandomly sequencing 40 repetitions of each triplet. Shapes in the random set were sequenced individually by pseudorandomizing 40 repetitions of each shape. The two streams were then interleaved into one temporal stream of 720 shapes (9 shapes × 40 repetitions × 2 colors). This interleaving was done by randomly sampling shapes in each color stream in order and without replacement, with the constraint that the difference in the number of remaining shapes per stream could not exceed 6. Shapes in the interleaved stream appeared one at a time in the center of the display (<xref ref-type="fig" rid="fig2-0956797612460407">Fig. 2a</xref>).</p>
<fig id="fig2-0956797612460407" position="float">
<label>Fig. 2.</label>
<caption>
<p>Sample trial sequence, stimuli, and results from Experiment 2. A single stream of red and green shapes appeared at central fixation (a). This stream was interrupted occasionally by a visual search array that contained one <italic>T</italic> target (indicated here by an arrow) and three <italic>L</italic> distractors. One of the items in the array was a red or green color singleton, which could appear as either the target or one of the distractors. Regardless, the task was to indicate whether the target pointed to the left or right. The central stream was created by randomly interleaving shapes from separate red and green streams (b), each generated from a different set of nine shapes. One of these color streams contained temporal regularities (the structured color). The nine shapes in this stream were grouped into three triplets, whose members always appeared in the same order. The other stream did not contain temporal regularities (the random color), with the nine shapes appearing in a random order. The graph shows mean response time in the visual search task as a function of whether the singleton was the target or a distractor and whether its color matched the structured or random stream. Error bars indicate ±1 <italic>SEM</italic>. Asterisks indicate significant differences between conditions (*<italic>p</italic> &lt; .05, **<italic>p</italic> &lt; .01, ***<italic>p</italic> &lt; .001).</p>
</caption>
<graphic xlink:href="10.1177_0956797612460407-fig2.tif"/>
</fig>
<p>The shape stream was interrupted occasionally by a task-relevant visual search array. Each search array consisted of one rotated <italic>T</italic> target and three <italic>L</italic> distractors (with the horizontal line offset 20% from either the bottom or top of the vertical line) in the four locations from Experiment 1, with the location of the target determined randomly. To assess attention to color, we included one item in every search array (the singleton) that was colored differently than the remaining three black items. This singleton appeared with equal frequency in the color of the structured or random set. The target and each distractor served as the singleton with equal frequency, and the target appeared at each location with equal frequency. Thus, the shapes were not predictive of which item would be the singleton, what color the singleton would be, where the target would appear, or which response was correct.</p>
</sec>
<sec id="section10-0956797612460407">
<title>Apparatus</title>
<p>The apparatus was the same as in Experiment 1.</p>
</sec>
<sec id="section11-0956797612460407">
<title>Procedure</title>
<p>As in Experiment 1, participants completed exposure and test phases. During exposure, each shape trial contained one shape for 500 ms followed by an ISI of 500 ms. Participants were instructed to fixate and passively view shapes while waiting for visual search arrays. The 720 shape trials were randomly interrupted by 128 visual search trials (none back to back), which resulted in an average of 1 search trial for every 5.6 shape trials. The search arrays appeared for 750 ms, followed by a minimum ISI of 750 ms. Participants indicated whether the target pointed left or right (by pressing the “1” or “0” key, respectively)—regardless of its color and location (accuracy data are again reported in the Supplemental Material). Prior to exposure, participants were shown 5 random shape trials and 1 search trial for practice and to clarify the instructions. The test phase was identical to that in Experiment 1, with triplets and foils drawn from the structured color set. After the test phase, an extensive debriefing was conducted.</p>
</sec>
<sec id="section12-0956797612460407">
<title>Results and discussion</title>
<p>Triplets were chosen over foils on 61.1% (<italic>SD</italic> = 14.9%) of test trials, which reveals that statistical learning occurred (chance = 50%), <italic>t</italic>(19) = 3.34, <italic>p</italic> = .003, <italic>d</italic> = 0.75. Thus, temporal regularities of shapes in one color can be learned despite interruptions by random stimuli in another color.</p>
<p>Target-discrimination RTs (<xref ref-type="fig" rid="fig2-0956797612460407">Fig. 2c</xref>) were analyzed with a 2 (singleton type: target, distractor) × 2 (singleton color: structured, random) repeated measures analysis of variance (ANOVA). As a manipulation check that color singletons captured attention, we analyzed the speed of target responses and found that they were indeed faster for target singletons than for distractor singletons: main effect of singleton type, <italic>F</italic>(1, 19) = 17.33, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .48. Planned comparisons revealed that this occurred for singletons in both the structured color, <italic>t</italic>(19) = 4.26, <italic>p</italic> &lt; .001, <italic>d</italic> = 0.95, and the random color, <italic>t</italic>(19) = 2.60, <italic>p</italic> = .02, <italic>d</italic> = 0.58. There was no main effect of singleton color, <italic>F</italic>(1, 19) = 1.61, <italic>p</italic> = .22, η<sub><italic>p</italic></sub><sup>2</sup> = .08.</p>
<p>The critical test of our hypothesis was whether attentional capture (defined as the RT difference between distractor singletons and target singletons) was enhanced for the structured color. Indeed, there was an interaction between singleton type and singleton color, with greater attentional capture for the structured color than for the random color, <italic>F</italic>(1, 19) = 8.34, <italic>p</italic> = .009, η<sub><italic>p</italic></sub><sup>2</sup> = .30. In debriefing, 6 participants reported noticing regularities during exposure, but only 2 of these participants correctly identified which color contained regularities (chance = 50%; binomial test: <italic>p</italic> = .89). Regardless, the interaction remained reliable without these 6 participants, <italic>F</italic>(1, 13) = 4.97, <italic>p</italic> = .04, η<sub><italic>p</italic></sub><sup>2</sup> = .28.</p>
<p>Although shapes were task irrelevant, search items appearing in the color of shapes with temporal regularities were preferentially attended. These results suggest that attention is biased toward the features of regularities.</p>
</sec>
</sec>
<sec id="section13-0956797612460407">
<title>Experiment 3</title>
<p>The goal of this experiment was to extend the findings of Experiment 2 by testing whether temporal regularities in a feature dimension enhance capture by a singleton in that dimension.</p>
<sec id="section14-0956797612460407">
<title>Participants</title>
<p>Thirty new Princeton University undergraduates (20 female, 10 male; mean age = 19.9 years) participated in the experiment for course credit. Participants reported normal or corrected-to-normal visual acuity and color vision and provided informed consent. The protocol was approved by the Princeton University IRB.</p>
</sec>
<sec id="section15-0956797612460407">
<title>Stimuli</title>
<p>Displays consisted of lines varying in two feature dimensions: color (red, green, blue, yellow, orange, or purple) and orientation (15°, 45°, 75°, 105°, 135°, or 165°). We used six values per dimension to ensure that values were discriminable. One dimension was chosen as structured and the other as random (<xref ref-type="fig" rid="fig3-0956797612460407">Fig. 3c</xref>). In the color group (<italic>n</italic> = 15), the six colors were randomly assigned to three “pairs” for each participant, and a temporal stream of colors was generated by pseudorandomly sequencing 60 repetitions of each pair. A temporal stream of orientations was generated by pseudorandomly sequencing 60 repetitions of each of the six individual orientations. These structured (color) and random (orientation) feature sequences were overlaid to create a single stream of 360 colored lines (6 lines × 60 repetitions). This process was reversed for the orientation group (<italic>n</italic> = 15), with orientations being structured (sequenced in pairs) and colors being random (sequenced individually). For both groups, the lines subtended 2.4° and appeared one at a time in the center of the display (<xref ref-type="fig" rid="fig3-0956797612460407">Fig. 3a</xref>).</p>
<fig id="fig3-0956797612460407" position="float">
<label>Fig. 3.</label>
<caption>
<p>Sample trial sequence, conditions, stimuli, and results from Experiment 3. A single stream of oriented colored lines appeared at central fixation (a). This stream was interrupted occasionally by a visual search array that contained one solid-line target (indicated here by an arrow) among 15 broken-line distractors. The task was to indicate whether the target was a thick or thin line. One of the items in the array was a singleton (b), which differed in either color or orientation from all other items and appeared as either the target or one of the distractors. Participants were assigned to two groups (c). In the color group, the colors of the lines contained temporal regularities, but the orientations did not. The six colors were combined into three pairs, whose members always appeared in the same order; in contrast, the six orientations appeared in a random order. In the orientation group, the orientations of the lines contained temporal regularities, but the colors did not. The six orientations were combined into three pairs, whose members always appeared in the same order; in contrast, the six colors appeared in a random order. The graph shows mean response time in the visual search task as a function of group and type of search array (d). Error bars indicate ±1 <italic>SEM</italic>. Asterisks indicate significant differences between conditions and groups (**<italic>p</italic> &lt; .01, ***<italic>p</italic> &lt; .001).</p>
</caption>
<graphic xlink:href="10.1177_0956797612460407-fig3.tif"/>
</fig>
<p>The line stream was interrupted occasionally by a task-relevant visual search array. Each search array contained either a color singleton or an orientation singleton. The four search locations from previous experiments were surrounded by 12 additional locations, which yielded arrays of 16 lines in a diamond arrangement (18.2° × 18.2°). The larger set size was used to enhance pop-out for orientation singletons because a pilot study with four lines did not lead to a baseline capture effect. The search target was a solid line, whereas distractor lines contained a 0.2° gap (creating a form of serial search; <xref ref-type="bibr" rid="bibr34-0956797612460407">Treisman &amp; Souther, 1985</xref>). Because orientation was a dimension of interest, we avoided the orientation-discrimination task from Experiments 1 and 2. Instead, participants judged whether the target line was thick or thin. Half of the lines in each array were thick (0.20°), and the other half of the lines were thin (0.16°). All but one line had the same color and orientation (chosen randomly for each array). The singleton differed in either color or orientation (equated for frequency), with its value in the other dimension matching the other lines (<xref ref-type="fig" rid="fig3-0956797612460407">Fig. 3b</xref>).</p>
<p>The target and singleton appeared randomly in the four locations used in Experiments 1 and 2, although participants were not informed about this. The singleton was the target on 50% of search trials. This change from Experiment 2 (in which 25% of singletons were targets) was made to increase statistical power in light of the smaller overall number of trials in this experiment. It is important to note that although the singleton was partly informative about the target location, this was true for singletons in both the structured and random dimensions. Therefore, this predictiveness cannot explain differences between the color and orientation groups—the search arrays were identical for all participants. The line trials were not predictive of which search item would be the singleton, what dimension the singleton would appear in, what feature value it would have in that dimension, where the target would appear, or which response was correct.</p>
</sec>
<sec id="section16-0956797612460407">
<title>Apparatus</title>
<p>The apparatus was the same as in Experiments 1 and 2.</p>
</sec>
<sec id="section17-0956797612460407">
<title>Procedure</title>
<p>The exposure phase was similar to that used in Experiment 2 (with colored and oriented lines appearing instead of colored shapes), except that search arrays appeared for 1,000 ms with a minimum ISI of 500 ms. The array duration was extended because of the larger set size. The 360 line trials were interrupted by 80 visual search trials (none back to back), which resulted in an average of 1 search trial for every 4.5 line trials (accuracy data are again reported in the Supplemental Material). Prior to exposure, participants were shown 5 random shape trials and 1 search trial for practice and to clarify the instructions. The test phase was similar to that used in Experiments 1 and 2, except that pairs were tested rather than triplets, and test lines appeared with only the structured dimension varying (all horizontal in the color group, and all black in the orientation group). Each pair was tested twice against three two-line foils. After the test phase, an extensive debriefing was conducted.</p>
</sec>
<sec id="section18-0956797612460407">
<title>Results and discussion</title>
<p>Pairs were chosen over foils on 66.3% (<italic>SD</italic> = 13.4%) and 72.6% (<italic>SD</italic> = 15.9%) of test trials by the color and orientation groups, respectively, which revealed that statistical learning occurred in both dimensions (chance = 50%): color group, <italic>t</italic>(14) = 4.73, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.22; orientation group, <italic>t</italic>(14) = 5.50, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.42. These levels did not differ between groups, <italic>t</italic>(28) = 1.17, <italic>p</italic> = .25, <italic>d</italic> = 0.43. This replicates findings that regularities in one feature dimension can be learned despite randomness in another feature dimension (<xref ref-type="bibr" rid="bibr35-0956797612460407">Turk-Browne, Isola, Scholl, &amp; Treat, 2008</xref>).</p>
<p>Target-discrimination RTs (<xref ref-type="fig" rid="fig3-0956797612460407">Fig. 3d</xref>) were analyzed with a 2 (group: color, orientation; between subjects) × 2 (singleton type: target, distractor; within subjects) × 2 (singleton dimension: color, orientation; within subjects) mixed-effects ANOVA. We observed a robust main effect of singleton type across both groups, <italic>F</italic>(1, 28) = 70.01, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .71, and separately within the color group, <italic>F</italic>(1, 14) = 27.01, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .66, and the orientation group, <italic>F</italic>(1, 14) = 68.32, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .83, which confirms that our manipulation of attentional capture was successful. Separate mixed-effects ANOVAs within each singleton dimension across groups revealed this main effect of singleton type for both the color dimension, <italic>F</italic>(1, 28) = 38.17, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .58, and the orientation dimension, <italic>F</italic>(1, 28) = 59.43, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .68. Planned comparisons further revealed this capture effect for both dimensions within both groups (<italic>p</italic>s &lt; .002, <italic>d</italic>s &gt; 1.00).</p>
<p>The critical test of our hypothesis was whether attentional capture was stronger for singletons in the structured dimension than in the random dimension. Indeed, there was a three-way crossover interaction between group, singleton type, and singleton dimension, <italic>F</italic>(1, 28) = 19.18, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .41, with greater attentional capture for color than orientation singletons in the color group, <italic>F</italic>(1, 14) = 9.56, <italic>p</italic> = .008, η<sub><italic>p</italic></sub><sup>2</sup> = .41, and greater attentional capture for orientation than color singletons in the orientation group, <italic>F</italic>(1, 14) = 12.28, <italic>p</italic> = .003, η<sub><italic>p</italic></sub><sup>2</sup> = .47. This interaction was found separately within each singleton dimension, with greater attentional capture in the color than orientation group for color singletons, <italic>F</italic>(1, 28) = 8.20, <italic>p</italic> = .007, η<sub><italic>p</italic></sub><sup>2</sup> = .23, and greater attentional capture in the orientation than color group for orientation singletons, <italic>F</italic>(1, 28) = 6.22, <italic>p</italic> = .02, η<sub><italic>p</italic></sub><sup>2</sup> = .18. No other main effects or interactions reached significance (<italic>p</italic>s &gt; .23, η<sub><italic>p</italic></sub><sup>2</sup>s &lt; .06). In debriefing, 6 participants in each of the color and orientation groups reported noticing regularities, but only 2 participants per group correctly identified which dimension contained regularities (chance = 50%; binomial test: <italic>p</italic> = .93). Regardless, the three-way interaction remained reliable without the inclusion of these 12 participants, <italic>F</italic>(1, 16) = 13.36, <italic>p</italic> = .002, η<sub><italic>p</italic></sub><sup>2</sup> = .46, as did all two-way interactions (<italic>p</italic>s &lt; .04).</p>
<p>Although the line stream was task irrelevant, search items that were unique in the dimension with temporal regularities were preferentially attended. These results suggest that attention is biased toward feature dimensions that contain regularities.</p>
</sec>
</sec>
<sec id="section19-0956797612460407" sec-type="discussion">
<title>General Discussion</title>
<p>Across three studies, we found that the locations and features of objects embedded in temporal regularities received priority in visual search. We interpret this effect as reflecting an implicit attentional bias for regularities during statistical learning—that is, an increased likelihood of attending to the structured stream just before the search array appeared. This bias may combine with other orthogonal cues to determine the allocation of attention at any given moment, such as the salience of particular stimuli and the goal of finding the search target.</p>
<p>One explanation for this bias focuses on randomness. Because participants could not know in advance which stream was structured, they may have initially sampled all locations and features. When attempts to learn in random streams failed, attention may have redirected to other streams, eventually settling on the structured stream. Indeed, encountering randomness prevents subsequent learning about the same input (<xref ref-type="bibr" rid="bibr17-0956797612460407">Gebhart, Aslin, &amp; Newport, 2009</xref>; <xref ref-type="bibr" rid="bibr23-0956797612460407">Jungé, Scholl, &amp; Chun, 2007</xref>).</p>
<p>An alternative explanation focuses on structure. Because regularities in the structured stream matched prior experience, they may have either attracted attention that was initially allocated elsewhere or held attention that was allocated to this stream already on the basis of other cues. Indeed, input that matches prior experience can incidentally cue memory retrieval and reflexively engage attention networks (<xref ref-type="bibr" rid="bibr11-0956797612460407">Ciaramelli, Grady, &amp; Moscovitch, 2008</xref>). Repetition biases often appear during initial learning: Infants look longer at repeated stimuli until fully habituated, and neural responses are enhanced for repeated stimuli that are degraded (<xref ref-type="bibr" rid="bibr37-0956797612460407">Turk-Browne, Scholl, &amp; Chun, 2008</xref>). Thus, extensive exposure to regularities may dissipate or reverse the bias for structure—in other words, having strong expectations after learning might release attention elsewhere. The duration of the current experiments may not have been sufficient for this, though all experiments showed a numerically weaker effect in the second half compared with the first half (<italic>p</italic>s &gt; .11).</p>
<p>On the surface, the notion that regularities might attract attention seems incompatible with demonstrations that attention is necessary for statistical learning (<xref ref-type="bibr" rid="bibr3-0956797612460407">Baker, Olson, &amp; Behrmann, 2004</xref>; <xref ref-type="bibr" rid="bibr6-0956797612460407">Campbell, Zimerman, Healey, Lee, &amp; Hasher, 2012</xref>; <xref ref-type="bibr" rid="bibr36-0956797612460407">Turk-Browne, Jungé, &amp; Scholl, 2005</xref>). However, these findings can be reconciled by suggesting that attention and learning may interact in a closed-loop manner: In contrast to tasks in prior studies that required selective attention to one stream, the tasks in the current experiments left participants free to attend broadly, and they could thus learn a little from all streams. When subsequent input matched what had been learned, attention may have been drawn toward or held on the spatial location or features of this regular information. This more selective attentional focus would help promote further statistical learning, and in turn, more sophisticated learning would lead to better matches and more attention, and so on. Although this possibility requires further investigation, such an interaction would be highly ecological, with prior experience helping shape attentional priorities and these priorities ensuring the acquisition of knowledge about stable aspects of the environment.</p>
<p>In contrast to implicit accounts, our results could in principle be explained if participants became explicitly aware of regularities and intentionally oriented to them. This is unlikely for three reasons. First, most participants reported no awareness, the attentional bias was robust in just these participants, and those who did report awareness performed at chance in identifying the location, color, or dimension of regularities. Second, only 8 out of 75 participants across the three experiments reported using a strategy of selectively attending to one stream; among them, only 1 participant reported attending to the stream with regularities. Third, unlike findings that regularities among task-relevant stimuli can affect performance on the same task (<xref ref-type="bibr" rid="bibr4-0956797612460407">Brady et al., 2009</xref>; <xref ref-type="bibr" rid="bibr9-0956797612460407">Chun &amp; Jiang, 1998</xref>; <xref ref-type="bibr" rid="bibr30-0956797612460407">Smyth &amp; Shanks, 2008</xref>; <xref ref-type="bibr" rid="bibr40-0956797612460407">Umemoto et al., 2010</xref>; <xref ref-type="bibr" rid="bibr44-0956797612460407">Zhao et al., 2011</xref>), the present regularities were defined over task-irrelevant stimuli and did not provide predictive information about the visual search task. Thus, there was no evidence that intentional orienting to regularities occurred, nor would it have been a useful strategy for improving task performance. The discovery of robust orienting based on a completely task-irrelevant “summoning signal” distinguishes our findings from prior work.</p>
<p>To conclude, the current findings are significant in several ways. We uncovered a novel factor in the control of attention driven neither by intrinsic salience (stimuli were randomly assigned to streams) nor by intentional goals (participants were not aware of regularities, and regularities were task irrelevant). Moreover, the current paradigm provides a novel implicit and online measure of statistical learning, moving beyond the conventional use of familiarity to measure a process that is often implicit (<xref ref-type="bibr" rid="bibr38-0956797612460407">Turk-Browne et al., 2009</xref>). Finally, recent work has shown that statistical learning interacts with several other cognitive processes (<xref ref-type="bibr" rid="bibr4-0956797612460407">Brady et al., 2009</xref>; <xref ref-type="bibr" rid="bibr18-0956797612460407">Graf Estes et al., 2007</xref>; <xref ref-type="bibr" rid="bibr39-0956797612460407">Turk-Browne et al., 2010</xref>; <xref ref-type="bibr" rid="bibr40-0956797612460407">Umemoto et al., 2010</xref>; <xref ref-type="bibr" rid="bibr44-0956797612460407">Zhao et al., 2011</xref>), and the current findings show that statistical learning, even of task-irrelevant stimuli, can control the allocation of spatial and feature attention.</p>
</sec>
</body>
<back>
<ack>
<p>For helpful conversations, we thank Ed Awh, Floris de Lange, Dan Osherson, Aaron Seitz, and the Turk-Browne Lab.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This work was supported by a postdoctoral fellowship from the Natural Sciences and Engineering Research Council of Canada (to N. A.-A.) and by National Institutes of Health Grant No. R01-EY021755 (to N. B. T.-B.).</p>
</fn>
<fn fn-type="supplementary-material">
<label>Supplemental Material</label>
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abrams</surname><given-names>R. A.</given-names></name>
<name><surname>Christ</surname><given-names>S. E.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Motion onset captures attention</article-title>. <source>Psychological Science</source>, <volume>14</volume>, <fpage>427</fpage>–<lpage>432</lpage>.</citation>
</ref>
<ref id="bibr2-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bacon</surname><given-names>W. F.</given-names></name>
<name><surname>Egeth</surname><given-names>H.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Overriding stimulus-driven attentional capture</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>55</volume>, <fpage>485</fpage>–<lpage>496</lpage>.</citation>
</ref>
<ref id="bibr3-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Baker</surname><given-names>C. I.</given-names></name>
<name><surname>Olson</surname><given-names>C. R.</given-names></name>
<name><surname>Behrmann</surname><given-names>M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Role of attention and perceptual grouping in visual statistical learning</article-title>. <source>Psychological Science</source>, <volume>15</volume>, <fpage>460</fpage>–<lpage>466</lpage>.</citation>
</ref>
<ref id="bibr4-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brady</surname><given-names>T. F.</given-names></name>
<name><surname>Konkle</surname><given-names>T.</given-names></name>
<name><surname>Alvarez</surname><given-names>G. A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Compression in visual short-term memory: Using statistical regularities to form more efficient memory representations</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>138</volume>, <fpage>487</fpage>–<lpage>502</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brainard</surname><given-names>D. H.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>,<volume>10</volume>,<fpage>433</fpage>–<lpage>436</lpage>.</citation>
</ref>
<ref id="bibr6-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Campbell</surname><given-names>K. L.</given-names></name>
<name><surname>Zimerman</surname><given-names>S.</given-names></name>
<name><surname>Healey</surname><given-names>M. K.</given-names></name>
<name><surname>Lee</surname><given-names>M. M.</given-names></name>
<name><surname>Hasher</surname><given-names>L.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Age differences in visual statistical learning</article-title>. <source>Psychology and Aging</source>, <volume>27</volume>, <fpage>650</fpage>–<lpage>656</lpage>. doi:<pub-id pub-id-type="doi">10.1037/a0026780</pub-id></citation>
</ref>
<ref id="bibr7-0956797612460407">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Chalk</surname><given-names>M.</given-names></name>
<name><surname>Seitz</surname><given-names>A.</given-names></name>
<name><surname>Seriès</surname><given-names>P.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Rapidly learned stimulus expectations alter perception of motion</article-title>. <source>Journal of Vision</source>, <volume>10</volume>(<issue>8</issue>), Article <fpage>2</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/10/8/2">http://www.journalofvision.org/content/10/8/2</ext-link></citation>
</ref>
<ref id="bibr8-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
<name><surname>Golomb</surname><given-names>J. D.</given-names></name>
<name><surname>Turk-Browne</surname><given-names>N. B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A taxonomy of external and internal attention</article-title>. <source>Annual Review of Psychology</source>, <volume>62</volume>, <fpage>73</fpage>–<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Contextual cueing: Implicit learning and memory of visual context guides spatial attention</article-title>. <source>Cognitive Psychology</source>, <volume>36</volume>, <fpage>28</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr10-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
<name><surname>Jiang</surname><given-names>Y.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Top-down attentional guidance based on implicit learning of visual covariation</article-title>. <source>Psychological Science</source>, <volume>10</volume>, <fpage>360</fpage>–<lpage>365</lpage>.</citation>
</ref>
<ref id="bibr11-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ciaramelli</surname><given-names>E.</given-names></name>
<name><surname>Grady</surname><given-names>C. L.</given-names></name>
<name><surname>Moscovitch</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Top-down and bottom-up attention to memory: A hypothesis (AtoM) on the role of the posterior parietal cortex in memory retrieval</article-title>. <source>Neuropsychologia</source>, <volume>46</volume>, <fpage>1828</fpage>–<lpage>1851</lpage>.</citation>
</ref>
<ref id="bibr12-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Corbetta</surname><given-names>M.</given-names></name>
<name><surname>Shulman</surname><given-names>G. L.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>3</volume>, <fpage>215</fpage>–<lpage>229</lpage>.</citation>
</ref>
<ref id="bibr13-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fiser</surname><given-names>J.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Unsupervised statistical learning of higher-order spatial structures from visual scenes</article-title>. <source>Psychological Science</source>, <volume>12</volume>, <fpage>499</fpage>–<lpage>504</lpage>.</citation>
</ref>
<ref id="bibr14-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fiser</surname><given-names>J.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Statistical learning of higher-order temporal structure from visual shape sequences</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>28</volume>, <fpage>458</fpage>–<lpage>467</lpage>.</citation>
</ref>
<ref id="bibr15-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
<name><surname>Remington</surname><given-names>R. W.</given-names></name>
<name><surname>Johnston</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Involuntary covert orienting is contingent on attentional control settings</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>18</volume>, <fpage>1030</fpage>–<lpage>1044</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Franconeri</surname><given-names>S. L.</given-names></name>
<name><surname>Simons</surname><given-names>D. J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Moving and looming stimuli capture attention</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>65</volume>, <fpage>999</fpage>–<lpage>1010</lpage>.</citation>
</ref>
<ref id="bibr17-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gebhart</surname><given-names>A. L.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
<name><surname>Newport</surname><given-names>E. L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Changing structures in midstream: Learning along the statistical garden path</article-title>. <source>Cognitive Science</source>, <volume>33</volume>, <fpage>1087</fpage>–<lpage>1116</lpage>.</citation>
</ref>
<ref id="bibr18-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Graf Estes</surname><given-names>K. M.</given-names></name>
<name><surname>Evans</surname><given-names>J.</given-names></name>
<name><surname>Alibali</surname><given-names>M. W.</given-names></name>
<name><surname>Saffran</surname><given-names>J. R.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Can infants map meaning to newly segmented words? Statistical segmentation and word learning</article-title>. <source>Psychological Science</source>, <volume>18</volume>, <fpage>254</fpage>–<lpage>260</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hopfinger</surname><given-names>J. B.</given-names></name>
<name><surname>Buonocore</surname><given-names>M. H.</given-names></name>
<name><surname>Mangun</surname><given-names>G. R.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The neural mechanisms of top-down attentional control</article-title>. <source>Nature Neuroscience</source>, <volume>3</volume>, <fpage>284</fpage>–<lpage>291</lpage>.</citation>
</ref>
<ref id="bibr20-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Itti</surname><given-names>L.</given-names></name>
<name><surname>Koch</surname><given-names>C.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Computational modelling of visual attention</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>2</volume>, <fpage>194</fpage>–<lpage>203</lpage>.</citation>
</ref>
<ref id="bibr21-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Johnston</surname><given-names>W. A.</given-names></name>
<name><surname>Hawley</surname><given-names>K. J.</given-names></name>
<name><surname>Plew</surname><given-names>S. H.</given-names></name>
<name><surname>Elliott</surname><given-names>J. M.</given-names></name>
<name><surname>DeWitt</surname><given-names>M. J.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Attention capture by novel stimuli</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>119</volume>, <fpage>397</fpage>–<lpage>411</lpage>.</citation>
</ref>
<ref id="bibr22-0956797612460407">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jonides</surname><given-names>J.</given-names></name>
</person-group> (<year>1981</year>). <article-title>Voluntary vs. automatic control over the mind’s eye’s movement</article-title>. In <person-group person-group-type="editor">
<name><surname>Long</surname><given-names>J. B.</given-names></name>
<name><surname>Baddeley</surname><given-names>A. D.</given-names></name>
</person-group> (Eds.), <source>Attention and performance</source> (<volume>Vol. 9</volume>, pp. <fpage>187</fpage>–<lpage>203</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr23-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jungé</surname><given-names>J. A.</given-names></name>
<name><surname>Scholl</surname><given-names>B. J.</given-names></name>
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>How is spatial context learning integrated over signal versus noise? A primacy effect in contextual cueing</article-title>. <source>Visual Cognition</source>, <volume>15</volume>, <fpage>1</fpage>–<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr24-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Miller</surname><given-names>E. K.</given-names></name>
<name><surname>Cohen</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2001</year>). <article-title>An integrative theory of prefrontal cortex function</article-title>. <source>Annual Review of Neuroscience</source>, <volume>24</volume>, <fpage>167</fpage>–<lpage>202</lpage>.</citation>
</ref>
<ref id="bibr25-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pashler</surname><given-names>H.</given-names></name>
<name><surname>Johnston</surname><given-names>J.</given-names></name>
<name><surname>Ruthruff</surname><given-names>E.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Attention and performance</article-title>. <source>Annual Review of Psychology</source>, <volume>52</volume>, <fpage>629</fpage>–<lpage>651</lpage>.</citation>
</ref>
<ref id="bibr26-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pelli</surname><given-names>D. G.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spatial Vision</source>,<volume>10</volume>, <fpage>437</fpage>–<lpage>442</lpage>.</citation>
</ref>
<ref id="bibr27-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Perruchet</surname><given-names>P.</given-names></name>
<name><surname>Pacton</surname><given-names>S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Implicit learning and statistical learning: Two approaches, one phenomenon</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>10</volume>, <fpage>233</fpage>–<lpage>238</lpage>.</citation>
</ref>
<ref id="bibr28-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Posner</surname><given-names>M. I.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Orienting of attention</article-title>. <source>Quarterly Journal of Experimental Psychology</source>, <volume>32</volume>, <fpage>3</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr29-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saffran</surname><given-names>J. R.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
<name><surname>Newport</surname><given-names>E. L.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source>, <volume>274</volume>, <fpage>1926</fpage>–<lpage>1928</lpage>.</citation>
</ref>
<ref id="bibr30-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Smyth</surname><given-names>A. C.</given-names></name>
<name><surname>Shanks</surname><given-names>D. R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Awareness in contextual cuing with extended and concurrent explicit tests</article-title>. <source>Memory &amp; Cognition</source>, <volume>36</volume>, <fpage>403</fpage>–<lpage>415</lpage>.</citation>
</ref>
<ref id="bibr31-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Summerfield</surname><given-names>J. J.</given-names></name>
<name><surname>Lepsien</surname><given-names>J.</given-names></name>
<name><surname>Gitelman</surname><given-names>D. R.</given-names></name>
<name><surname>Mesulam</surname><given-names>M. M.</given-names></name>
<name><surname>Nobre</surname><given-names>A. C.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Orienting attention based on long-term memory experience</article-title>. <source>Neuron</source>, <volume>49</volume>, <fpage>905</fpage>–<lpage>916</lpage>.</citation>
</ref>
<ref id="bibr32-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Perceptual selectivity for color and form</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>51</volume>, <fpage>599</fpage>–<lpage>606</lpage>.</citation>
</ref>
<ref id="bibr33-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Top-down search strategies cannot override attentional capture</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>11</volume>, <fpage>65</fpage>–<lpage>70</lpage>.</citation>
</ref>
<ref id="bibr34-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treisman</surname><given-names>A.</given-names></name>
<name><surname>Souther</surname><given-names>J.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Search asymmetry: A diagnostic for preattentive processing of separable features</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>114</volume>, <fpage>285</fpage>– <lpage>310</lpage>.</citation>
</ref>
<ref id="bibr35-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turk-Browne</surname><given-names>N. B.</given-names></name>
<name><surname>Isola</surname><given-names>P. J.</given-names></name>
<name><surname>Scholl</surname><given-names>B. J.</given-names></name>
<name><surname>Treat</surname><given-names>T. A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Multidimensional visual statistical learning</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>34</volume>, <fpage>399</fpage>–<lpage>407</lpage>.</citation>
</ref>
<ref id="bibr36-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turk-Browne</surname><given-names>N. B.</given-names></name>
<name><surname>Jungé</surname><given-names>J.</given-names></name>
<name><surname>Scholl</surname><given-names>B. J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The automaticity of visual statistical learning</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>134</volume>, <fpage>552</fpage>–<lpage>564</lpage>.</citation>
</ref>
<ref id="bibr37-0956797612460407">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Turk-Browne</surname><given-names>N. B.</given-names></name>
<name><surname>Scholl</surname><given-names>B. J.</given-names></name>
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Babies and brains: Habituation in infant cognition and functional neuroimaging</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>2</volume>, <fpage>16</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.frontiersin.org/Human_Neuroscience/10.3389/neuro.09.016.2008/full">http://www.frontiersin.org/Human_Neuroscience/10.3389/neuro.09.016.2008/full</ext-link></citation>
</ref>
<ref id="bibr38-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turk-Browne</surname><given-names>N. B.</given-names></name>
<name><surname>Scholl</surname><given-names>B. J.</given-names></name>
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
<name><surname>Johnson</surname><given-names>M. K.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Neural evidence of statistical learning: Efficient detection of visual regularities without awareness</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>21</volume>, <fpage>1934</fpage>–<lpage>1945</lpage>.</citation>
</ref>
<ref id="bibr39-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turk-Browne</surname><given-names>N. B.</given-names></name>
<name><surname>Scholl</surname><given-names>B. J.</given-names></name>
<name><surname>Johnson</surname><given-names>M. K.</given-names></name>
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Implicit perceptual anticipation triggered by statistical learning</article-title>. <source>Journal of Neuroscience</source>, <volume>30</volume>, <fpage>11177</fpage>–<lpage>11187</lpage>.</citation>
</ref>
<ref id="bibr40-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Umemoto</surname><given-names>A.</given-names></name>
<name><surname>Scolari</surname><given-names>M.</given-names></name>
<name><surname>Vogel</surname><given-names>E. K.</given-names></name>
<name><surname>Awh</surname><given-names>E.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Statistical learning induces discrete shifts in the allocation of working memory resources</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>36</volume>, <fpage>1419</fpage>–<lpage>1429</lpage>.</citation>
</ref>
<ref id="bibr41-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
<name><surname>Horowitz</surname><given-names>T. S.</given-names></name>
</person-group> (<year>2004</year>). <article-title>What attributes guide the deployment of visual attention and how do they do it?</article-title> <source>Nature Reviews Neuroscience</source>, <volume>5</volume>, <fpage>495</fpage>–<lpage>501</lpage>.</citation>
</ref>
<ref id="bibr42-0956797612460407">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Yantis</surname><given-names>S.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Goal-directed and stimulus-driven determinants of attentional control</article-title>. In <person-group person-group-type="editor">
<name><surname>Monsell</surname><given-names>S.</given-names></name>
<name><surname>Driver</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Attention and performance</source> (<volume>Vol. 18</volume>, pp. <fpage>73</fpage>–<lpage>103</lpage>). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr43-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yantis</surname><given-names>S.</given-names></name>
<name><surname>Jonides</surname><given-names>J.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Abrupt visual onsets and selective attention: Evidence from visual search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>10</volume>, <fpage>601</fpage>–<lpage>621</lpage>.</citation>
</ref>
<ref id="bibr44-0956797612460407">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>J.</given-names></name>
<name><surname>Ngo</surname><given-names>N.</given-names></name>
<name><surname>McKendrick</surname><given-names>R.</given-names></name>
<name><surname>Turk-Browne</surname><given-names>N. B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Mutual interference between statistical summary perception and statistical learning</article-title>. <source>Psychological Science</source>, <volume>22</volume>, <fpage>1212</fpage>–<lpage>1219</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>