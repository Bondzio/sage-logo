<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LTJ</journal-id>
<journal-id journal-id-type="hwp">spltj</journal-id>
<journal-title>Language Testing</journal-title>
<issn pub-type="ppub">0265-5322</issn>
<issn pub-type="epub">1477-0946</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0265532213480338</article-id>
<article-id pub-id-type="publisher-id">10.1177_0265532213480338</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Communicating the theory, practice and principles of language testing to test stakeholders: Some reflections</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Inbar-Lourie</surname><given-names>Ofra</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Taylor</surname><given-names>Lynda</given-names></name>
</contrib>
<aff id="aff1-0265532213480338">University of Bedfordshire, UK</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0265532213480338">Lynda Taylor, Senior Lecturer in Language Assessment, Centre for Research in English Language Learning and Assessment (CRELLA), University of Bedfordshire, 47 Montague Road, Cambridge, CB4 1BU, UK. Email: <email>lynda.taylor@beds.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>3</issue>
<issue-title>Special Issue on Language Assessment Literacy</issue-title>
<fpage>403</fpage>
<lpage>412</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The 33rd Language Testing Research Colloquium (LTRC), held in June 2011 in Ann Arbor, Michigan, included a conference symposium on the topic of <italic>assessment literacy</italic>. This event brought together a group of four presenters from different parts of the world, each of whom reported on their recent research in this area. Presentations were followed by a discussant slot that highlighted some thematic threads from across the papers and raised various questions for the professional language testing community to consider together. One point upon which there was general consensus during the discussion was the need for more research to be undertaken and published in this complex and challenging area. It is particularly encouraging, therefore, to see a coherent set of studies on assessment literacy brought together in this special issue of <italic>Language Testing</italic> and it will undoubtedly make an important contribution to the steadily growing body of literature on this topic, particularly as it concerns the testing of languages. This brief commentary revisits some of the themes originally raised during the LTRC 2011 symposium, considers how these have been explored or developed through the papers in this special issue and reflects on some future directions for our thinking and activity in this important area.</p>
</abstract>
<kwd-group>
<kwd>Assessment literacy</kwd>
<kwd>language assessment literacy</kwd>
<kwd>test stakeholders</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>LTRC 2011 in Michigan celebrated 50 years since the publication in 1961 of the seminal work in our field by Professor Robert Lado, one-time Director of the English Language Institute at the University of Michigan. His <italic>Language Testing</italic> volume helped to inform and shape subsequent research and practice in our field perhaps more than most publications. Although Lado himself made no explicit reference in his book to <italic>assessment literacy</italic> (AL) or <italic>language assessment literacy</italic> (LAL), it could be argued that the publication was in its essence an exercise in developing assessment literacy, particularly as far as the testing of languages was concerned. Though he did not use the term ‘assessment literacy,’ Lado was quite clear about the target readership for his book and its intended function. He was writing for a variety of different audiences who would be involved in language teaching and/or learning, who would be making use of language tests and who would, therefore, need to understand something about language testing. The volume’s subtitle – <italic>The Construction and Use of Foreign Language Tests</italic> – confirms that Lado was keen to target not just ‘test makers’ (though these undoubtedly constituted an important audience for him) but a much wider range of key stakeholder constituencies including: existing teachers of foreign languages; prospective language teachers and, presumably, those in training; linguists and language specialists; teachers; and graduate students. Lado articulated his aims in writing his book as follows: “so that increasingly we may speak on the basis of knowledge rather than from opinions and hypotheses alone valuable as they are at proper stages in the development of man’s [sic] thought” (<xref ref-type="bibr" rid="bibr7-0265532213480338">1961</xref>, pp. vii–viii). His goal seems to have been to disseminate language testing knowledge, skills and competence at a time when “great advances in transportation and communication” were stimulating a need for “more effective teaching of languages” along with the need for “more effective testing of their use” (<xref ref-type="bibr" rid="bibr7-0265532213480338">1961</xref>, pp. 1–2). Reading Lado’s words from half a century ago, it is tempting to think that little has changed in some respects; the debate over what constitutes the effective teaching and testing of languages continues to this day in a world that is still characterized by globalization and technological advances.</p>
<sec id="section1-0265532213480338">
<title>An era of literacy or ‘literacies’</title>
<p>There is, however, one aspect of our world that has changed significantly. We seem to have entered an era in which it is now necessary to develop ‘multiple literacies’ as individuals and as communities, and we find ourselves under pressure to acquire an ever expanding body of knowledge, skills and competence relating to a growing number of domains in everyday life; this includes the language and discourse patterns belonging to these domains. <xref ref-type="bibr" rid="bibr2-0265532213480338">Brumfit (2010</xref>, p. 13) described this contemporary trend as follows: “recent changes in global economics, politics and language use have placed the issue of interpreting multiple linguistic identities in the centre of our concerns.”</p>
<p>The traditional understanding of literacy as ‘the ability to read and write’ expanded many decades ago to embrace not only competency in reading and writing but also the critical and effective use of these skills in people’s lives, as well as the use of language (both written and oral) for specific purposes and in specific domains. Thus the notion of literacy began to take on a sociocultural as well as a functional dimension. Since then, the notion of ‘academic literacy’ has become fairly well-established in the context of studying in higher education. <xref ref-type="bibr" rid="bibr6-0265532213480338">Hyland and Hamp-Lyons (2002</xref>, p. 4) applied the term ‘academic literacy’ to “the complex set of skills (not necessarily only those relating to the mastery of reading and writing) which are increasingly argued to be vital underpinnings or cultural knowledge required for success in academic communities.” (See also the collection of papers on this topic edited by <xref ref-type="bibr" rid="bibr1-0265532213480338">Blue, 2010</xref>.)</p>
<p>Other notions of literacy to have emerged, mainly since the late 20th century, are similarly metaphorical in nature, often associated with education or with society more broadly. Teachers and lecturers in schools and universities, for example, sometimes complain that their students fail to acquire the ‘biblical literacy’ (a form of ‘cultural literacy’) that is fundamental for a full understanding and appreciation of so much Western literature, art and music from earlier centuries. Where once this knowledge and understanding could be generally assumed among students within most Western educational contexts, specially designed resources may be needed nowadays to familiarize students with the great religious themes that once inspired writers, artists, composers and others in Western culture and civilization (see <xref ref-type="bibr" rid="bibr4-0265532213480338">Dyas &amp; Hughes, 2005</xref>).</p>
<p>At a more applied level, notions of ‘technological literacy’ (e.g., to operate standard household appliances) and ‘computer literacy’ are now acknowledged throughout society; and in a technological age, we are sensitive to the reality that those who are not computer-literate risk becoming increasingly marginalized or disadvantaged as a result. Some social commentators express concern at increasingly low levels of ‘civic literacy’ in contemporary society; that is, citizens are seen as lacking the knowledge and skills needed to participate effectively in the community, government and politics, leading to a potential threat to democracy and good governance.</p>
<p>Other metaphorical expressions to have entered common usage in recent years include ‘risk literacy,’ ‘health literacy,’ ‘media literacy’ and ‘emotional literacy.’ In each case, the use of the term ‘literacy’ stands for know-how and awareness linked to the preceding item in the multi-word phrase. The focus is on the ability to understand the content and discourse associated with a given domain or activity and on being able to engage with and express oneself appropriately in relation to this.</p>
<p>It is hardly surprising, then, that ‘assessment literacy’ (AL) has been added to the growing list of ‘literacies’ to be acquired in contemporary life, together with ‘language assessment literacy’ (LAL) as a potentially subordinate or overlapping category. Both are relatively new fields of as far as theoretical and empirical research is concerned, but both are generally acknowledged today as an important focus for attention, debate, policymaking and action in education and wider society. Some writers highlight an imperative for work in this area in response to the huge growth of testing and assessment worldwide and the increasing number of people involved in it; the role of language assessment in particular has expanded in education and wider society as a result of globalization and migration (for a fuller discussion see <xref ref-type="bibr" rid="bibr11-0265532213480338">Taylor, 2009</xref>). Outcomes from empirical research investigating the nature and development of language assessment literacy are urgently needed not just to inform and underpin existing policy and practice but also to inspire and shape new and innovative initiatives for disseminating core knowledge and expertise in language assessment to a growing range of test stakeholders.</p>
</sec>
<sec id="section2-0265532213480338">
<title>Research topics and questions</title>
<p>At the LTRC 2011 symposium, a set of <italic>wh-</italic> questions was used as a convenient springboard for identifying relevant research topics and questions concerning AL/LAL:</p>
<list id="list1-0265532213480338" list-type="bullet">
<list-item><p>Who are the <italic>key stakeholder groups</italic> that need to develop AL/LAL?</p></list-item>
<list-item><p>What sort of <italic>content input</italic> is needed for developing AL/LAL?</p></list-item>
<list-item><p>Where are the specific <italic>domains and contexts</italic> in which this needs to be done?</p></list-item>
<list-item><p>When is the <italic>best time</italic> for this to be done?</p></list-item>
<list-item><p>What <italic>methods or approaches</italic> are likely to be most effective?</p></list-item>
</list>
<p>The papers presented in the LTRC 2011 symposium demonstrated how a broad range of research methodologies (quantitative, qualitative and mixed methods) could be successfully employed to investigate such questions. Subsequent discussion during the symposium highlighted four particular areas in which further research into AL and LAL might generate important insights for our field. The four areas related to the following:</p>
<sec id="section3-0265532213480338">
<title>1) How we understand and define the AL/LAL construct</title>
<p>How should we distinguish assessment literacy from language assessment literacy? Is it possible to think and speak in terms of ‘a core’ relating to the former generic concept, and for the latter some sort of ‘bolt-on’ component relating to the teaching, learning and assessment of language? Or should we be thinking in terms of a more integrated construct? Is language assessment literacy primarily knowledge-based? Is it a skill-based competence? Or is it a set of principles? Or is it something that integrates all three, as <xref ref-type="bibr" rid="bibr3-0265532213480338">Davies (2008)</xref> seemed to suggest?</p>
</sec>
<sec id="section4-0265532213480338">
<title>2) Our language and discourse when engaging with a non-specialist audience</title>
<p>Would it help to define our terms and phraseology more narrowly and in ways that are accessible to a broader and non-specialist audience? Even if the concepts of assessment literacy and language assessment literacy are still evolving – in an emergentist sense – we nevertheless need to develop a discourse and a way of communicating that reflects and meets the needs of differing stakeholder groups and varying professional contexts. In his 1961 volume, Robert Lado was keenly aware of the need for communicative effectiveness if his book was to speak to its target audience and achieve its intended objectives. He wrote as follows in the preface to his volume: “The style has been kept as non-technical as possible because of the variety of backgrounds of the intended readers. Linguistic terminology is not always familiar to the average teacher and test maker; and testing terminology is not often familiar to language teachers and linguists” (<xref ref-type="bibr" rid="bibr7-0265532213480338">1961</xref>, p. vii). Once again, it is tempting to feel that little has changed in this regard over half a century; in fact, it could be argued that the terminology and discourse used in applied linguistics and language testing today risk being even more highly technical and thus inaccessible to the non-specialist than they were 50 years ago.</p>
</sec>
<sec id="section5-0265532213480338">
<title>3) How we identify, evaluate and respond to varying user needs</title>
<p>Growing awareness and understanding of test impact since the early 1990s, particularly its multidimensional nature within education and wider society, has made it clear that language testers and test developers need to be sensitive to many different types of test stakeholder and the varying ways in which they find themselves engaging with and understanding assessment issues. Identifying the range of relevant stakeholders and evaluating their specific needs in relation to what test scores mean in their context and, consequently, how scores can or cannot be used, is becoming a priority in a world where assessment occupies such a central role. Ideally the promotion and development of assessment literacy will be achieved ‘by design’ rather than being a corrective afterthought. This means that engagement with test stakeholder groups needs to take place at the outset of any project to develop a new or revised test or testing system, not simply initiated at a point where the test is ready to be implemented. Similarly, some degree of familiarization with the principles and practice of assessment will ideally be embedded from an early stage in the training of language teachers, not simply offered as a ‘bolt-on’ option. AL/LAL development activities may need to be integrated within professional development programmes or briefing sessions for other stakeholder groups, such as civil servants or politicians. In each case, the primary content relevant to the needs of that stakeholder group (e.g., will it be measurement theory? practical know-how? ethical principles?) has to be extracted and translated into a language that particular group can access and understand.</p>
</sec>
<sec id="section6-0265532213480338">
<title>4) Our understanding of how AL/LAL grows and matures over time</title>
<p>Relatively little is understood still about how AL or LAL develops within individuals or across stakeholder groups who are not trained assessment professionals. Is it possible, for example, to think of someone moving from being a ‘novice’ in AL/LAL to achieving a level of proficiency and, if so, what might that progression curve look like? Are there recognizable and meaningful steps of achievement along the way that can be discerned and even accredited in some formal or informal way? Might there be a place for certificating language assessment literacy in some contexts? And could there be a role in this for a professional language testing organization at national, regional or international level?</p>
</sec>
</sec>
<sec id="section7-0265532213480338">
<title>The contribution of this special issue to the ongoing AL/LAL debate</title>
<p>The five papers published in this special issue echo various aspects of the four areas identified above, and they build upon the earlier body of research reported at the LTRC 2011 assessment literacy symposium. In some cases the studies reported here confirm or replicate previous findings. In other cases they explore new territory and contexts, helping to broaden our understanding and advance our thinking in this area.</p>
<p>The papers by Scarino, by Malone, by Jeong and by O’Loughlin all touch upon the central question of how we understand and define the AL/LAL construct. They highlight the importance of balancing theoretical assessment knowledge with practical know-how, while still allowing some place in the mix for attention to ethical principles. The papers also confirm the impact on assessment literacy of the professional beliefs, perceptions and practices which are invariably located within any local community of practice, for example language teachers, course instructors, university administrators, government policy makers. The authors challenge us to consider more deeply how to take account of these factors and whether they need to be integrated within the AL/LAL construct.</p>
<p>The papers by Jeong, by O’Loughlin and by Pill and Harding touch upon issues associated with the language and discourse of language testing, sometimes in terms of its overly technical nature for a non-specialist audience, or in relation to its tendency to generate misconceptions regarding measurement precision and test score infallibility. They challenge us to seek new and effective ways of communicating the theory and practice of language testing in transparent and accessible terms. This may require innovative modes of presentation tailored for specific audiences and contexts, for example, a specially designed online module for university administrators or immigration personnel on how to use test scores safely as part of complex decision-making processes, or an animated cartoon for teachers, parents and children showing how assessment supports successful teaching and learning. Creative and imaginative approaches using web-based tools are likely to be far more effective at reaching some key stakeholder groups than the print-based approach (e.g., paper leaflets or test handbooks) which language testers and test developers have traditionally adopted with stakeholders. Furthermore, even though much of the relevant information that stakeholders need can be found somewhere on test producers’ websites, its presentation and packaging too often reflects the documentation-based approach of the past, where the burden is on the stakeholders to find, read and understand the information that is relevant to them. <xref ref-type="bibr" rid="bibr2-0265532213480338">Brumfit (2010</xref>, p. 14) speculates that literacy has become another term for ‘interpretation’ “in a world of great and greater communication, we are presented with more and more data to interpret.” This view of the world helps to explain the growing need for assessment literacy, but it also implies that those who need to develop such literacy are likely to have less time and energy to spend seeking out what is relevant and useful to their requirements; the onus of responsibility for making key information more accessible must surely lie with those who already know where it is located.</p>
<p>All five papers alert us to the growing diversity of individuals and collective bodies, typically referred to as stakeholders, all of whom require some degree of AL/LAL though not always at the highest level of expertise. There have been various attempts to list categories of test stakeholder (e.g., <xref ref-type="bibr" rid="bibr8-0265532213480338">Rea-Dickins, 1997</xref>; <xref ref-type="bibr" rid="bibr10-0265532213480338">Taylor, 2000</xref>; <xref ref-type="bibr" rid="bibr9-0265532213480338">Saville, 2009</xref>) and it is self-evident that different stakeholder groups experience differing degrees of interest in a particular test (or in testing more generally) according to their level of assessment involvement. What becomes much clearer from the papers in this special issue, however, is the extent to which these different stakeholder groups need differential levels of assessment literacy according to their specific roles and responsibilities. We might think of some stakeholders as being closer to the heart of assessment – the ‘assessment core’; such groups are likely to require comprehensive training in measurement theory, technical know-how and ethical principles. Other stakeholder groups might place themselves some distance away from the core or closer to the periphery; they may be interested in or concerned with a particular dimension of assessment use, for example a classroom-based learning context, a university admissions system or a public forum such as a parliamentary enquiry. Instead of simply listing a comprehensive range of potential test stakeholders, perhaps we need a more sophisticated model such as that proposed in <xref ref-type="fig" rid="fig1-0265532213480338">Figure 1</xref> where concentric circles expand outwards from an ‘expert core’ of assessment knowledge, skill and principles, with each successive ring (or segment of a ring) representing the level of content/input that is required to meet the needs of a particular set of stakeholders.</p>
<fig id="fig1-0265532213480338" position="float">
<label>Figure 1.</label>
<caption>
<p>Levels of AL/LAL differentiated according to stakeholder constituency.</p>
</caption>
<graphic xlink:href="10.1177_0265532213480338-fig1.tif"/>
</fig>
<p>Taking a more discriminating perspective in this way may assist us in identifying the specific range and depth of testing expertise that is relevant when selecting the content or input that will best match each group’s needs. It should also make the process of developing AL/LAL more manageable and achievable. Not everyone needs to know or be able to do everything to the same level. What is important is that they should be competent in the knowledge, skills and understanding necessary for their context of activity. The resulting profile for each stakeholder group is likely to look somewhat different. For example, a profile for test writers may cover a wide range of content dimensions fairly evenly and in some depth. A profile for classroom language teachers, however, may end up focusing strongly on the practical know-how needed for creating tests but have a much lighter focus on measurement theory or ethical principles; the latter may need to be touched upon only briefly at a surface level. While a profile for university administrators will address those aspects of the assessment literacy construct that relate to understanding the nature of test instruments and the meaning of their scores for decision-making purposes, other aspects such as how to construct and validate tests need not receive much attention.</p>
<p><xref ref-type="fig" rid="fig2-0265532213480338">Figure 2(a</xref>–<xref ref-type="fig" rid="fig2-0265532213480338">d</xref>) attempts to illustrate what differential assessment literacy might look like for these three groups and for the community of professional language testing experts. It should be noted that the labelled dimensions on the eight axes (i.e. knowledge of theory, technical skills, etc.) are hypothesized from the discussion of possible AL/LAL components across various papers in this special issue, while the values (i.e. 0–4) are hypothesized according to the different stages of literacy suggested by Pill and Harding. The diagrams are for illustrative purposes only, to show how it might be possible to conceptualize and represent differential AL/LAL; the actual characterization for each group is naturally open to debate.</p>
<fig id="fig2-0265532213480338" position="float">
<label>Figure 2.</label>
<caption>
<p>Differential AL/LAL profiles for four constituencies.</p>
<p>(a) Profile for test writers.</p>
<p>(b) Profile for classroom teachers.</p>
<p>(c) Profile for university administrators.</p>
<p>(d) Profile for professional language testers.</p>
</caption>
<graphic xlink:href="10.1177_0265532213480338-fig2.tif"/>
</fig>
<p>In relation to understanding how AL/LAL develops and matures over time, the Pill and Harding paper is particularly insightful in making a useful interdisciplinary link with understandings of literacy drawn from other educational fields, for example scientific or mathematical literacy. As the authors suggest, this approach helps move us away from a notion of AL/LAL as a polarized dichotomy (i.e. one is either literate or illiterate in this area) towards a continuum which identifies and describes progressive stages of literacy along the way, for example nominal, functional, procedural/conceptual and multidimensional. This characterization merits further thought and exploration to see if it might provide us with a ‘literacy ladder’ for assessment; that is, a way of conceptualizing AL/LAL progression which could assist in designing modular courses in language testing and even in certificating levels of achievement as part of a broader professional development programme for certain stakeholder groups if appropriate to do so.</p>
<p>While <xref ref-type="bibr" rid="bibr5-0265532213480338">Fulcher’s (2012)</xref> expanded working definition of language assessment literacy helpfully draws together a broad range of elements that could be key components of the construct, it does not address the issue of the degree or depth that might be necessary for any of the elements depending upon the nature and extent of the stakeholder’s involvement in assessment. <xref ref-type="fig" rid="fig2-0265532213480338">Figure 2(a</xref>–<xref ref-type="fig" rid="fig2-0265532213480338">d</xref>) attempts to operationalize the range of key components making up the AL/LAL construct and to grade them on some sort of a continuum of depth/intensity.</p>
<p>Finally, it is worth noting that several of papers in this special issue make it clear that language assessment literacy is not necessarily a value-free concept. It is important to consider, therefore, what or whose values shape any programme to develop language assessment literacy. Two of the papers commented on a certain tension experienced between the perspectives of ‘professional language testers’ and of ‘non-language testers.’ This observation resonates with the tension commented on by <xref ref-type="bibr" rid="bibr2-0265532213480338">Brumfit (2010)</xref> in his discussion of language education in British society. He described a tension between ‘expertise and popular understanding’:
<disp-quote>
<p>there has been an increase of linguistically expert academics at the same time as experts of all kinds have been increasingly marginalised by governments (see Brumfit 2001, Chapter 6). Several of the tensions mentioned above reflect differences between specialist and popular views of language and its role. (<xref ref-type="bibr" rid="bibr2-0265532213480338">Brumfit, 2010</xref>, p. 21)</p>
</disp-quote>
</p>
<p>Brumfit’s comments referred primarily to the tension that arose in the late 1990s between specialist and popular views concerning primary and secondary language education issues in the UK. A similar tension can be discerned in the world of language testing; there is always the danger that testing experts risk adopting a somewhat superior attitude as far as the value and significance of their professional expertise is concerned but which leads to them being marginalized in a way that is counterproductive for all concerned. The development of AL/LAL across a range of stakeholder groups is likely to be most successful when it reflects a dynamic and iterative process informed by a collaborative ongoing dialogue taking place at the interface between language testing experts and non-specialist test stakeholders.</p>
<p>In her Guest Editorial to this valuable special issue, Inbar-Lourie highlights four strands to emerge from the authors’ contributions: first, the breadth of scope in terms of how the AL/LAL construct can be interpreted and defined; second, the nature and extent of the language component within an AL/LAL construct; third, the site of authority for determining the precise nature of AL/LAL; and fourth, what constitutes the essential core components of AL/LAL. This final paper has offered some additional reflections on these strands and has attempted to suggest some practical methods for conceptualizing range and depth of content and for differentiating profiles of stakeholder need and levels of proficiency. Hopefully, some of these ideas can be refined more fully to reveal new insights and approaches in this important area for our field.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0265532213480338">
<citation citation-type="book">
<collab>Blue</collab> (<year>2010</year>). (Ed.) <source>Developing academic literacy</source>. <publisher-loc>Bern, Switzerland</publisher-loc>: <publisher-name>Peter Lang</publisher-name>.</citation>
</ref>
<ref id="bibr2-0265532213480338">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brumfit</surname><given-names>C</given-names></name>
</person-group> (<year>2010</year>). <article-title>Literacy or literacies? Academic identities in a language-sharing world</article-title>. In <person-group person-group-type="editor">
<name><surname>Blue</surname><given-names>G.</given-names></name>
</person-group> (Ed.), <source>Developing academic literacy</source>. <publisher-loc>Bern, Switzerland</publisher-loc>: <publisher-name>Peter Lang</publisher-name>.</citation>
</ref>
<ref id="bibr3-0265532213480338">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Davies</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Textbook trends in teaching language testing</article-title>. <source>Language Testing</source>, <volume>25</volume>(<issue>3</issue>), <fpage>327</fpage>–<lpage>347</lpage>.</citation>
</ref>
<ref id="bibr4-0265532213480338">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dyas</surname><given-names>D.</given-names></name>
<name><surname>Hughes</surname><given-names>E.</given-names></name>
</person-group> (<year>2005</year>). <source>The Bible in Western culture: The student’s guide</source>. <publisher-loc>Abingdon, Oxon</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr5-0265532213480338">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fulcher</surname><given-names>G.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Assessment literacy for the language classroom</article-title>, <source>Language Assessment Quarterly</source>, <volume>9</volume>(<issue>2</issue>), <fpage>113</fpage>–<lpage>132</lpage>.</citation>
</ref>
<ref id="bibr6-0265532213480338">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hyland</surname><given-names>K.</given-names></name>
<name><surname>Hamp-Lyons</surname><given-names>L.</given-names></name>
</person-group> (<year>2002</year>). <article-title>EAP: Issues and directions</article-title>. <source>Journal of English for Academic Purposes</source>, <volume>1</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr7-0265532213480338">
<citation citation-type="book">
<collab>Lado</collab> (<year>1961</year>). <source>Language testing: The construction and use of foreign language tests</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Longman</publisher-name>.</citation>
</ref>
<ref id="bibr8-0265532213480338">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rea-Dickins</surname><given-names>P.</given-names></name>
</person-group> (<year>1997</year>). <article-title>So, why do we need relationships with stakeholders in language testing? A view from the UK</article-title>. <source>Language Testing</source>, <volume>14</volume>(<issue>3</issue>), <fpage>304</fpage>–<lpage>314</lpage>.</citation>
</ref>
<ref id="bibr9-0265532213480338">
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Saville</surname><given-names>N.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Developing a model for investigating the impact of language assessment within educational contexts by a public examination provider</article-title>. PhD dissertation, <publisher-name>University of Bedfordshire</publisher-name>.</citation>
</ref>
<ref id="bibr10-0265532213480338">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>L.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Stakeholders in language testing</article-title>. <source>Cambridge ESOL Research Notes</source> <volume>2</volume>, <fpage>2</fpage>-<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr11-0265532213480338">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Developing assessment literacy</article-title>. <source>Annual Review of Applied Linguistics</source>, <volume>29</volume>, <fpage>21</fpage>–<lpage>36</lpage>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>