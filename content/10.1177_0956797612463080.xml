<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612463080</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612463080</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Curse of Planning</article-title>
<subtitle>Dissecting Multiple Reinforcement-Learning Systems by Taxing the Central Executive</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Otto</surname><given-names>A. Ross</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612463080">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Gershman</surname><given-names>Samuel J.</given-names></name>
<xref ref-type="aff" rid="aff2-0956797612463080">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Markman</surname><given-names>Arthur B.</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612463080">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Daw</surname><given-names>Nathaniel D.</given-names></name>
<xref ref-type="aff" rid="aff3-0956797612463080">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-0956797612463080"><label>1</label>Department of Psychology, University of Texas at Austin</aff>
<aff id="aff2-0956797612463080"><label>2</label>Department of Psychology and Princeton Neuroscience Institute, Princeton University</aff>
<aff id="aff3-0956797612463080"><label>3</label>Department of Psychology and Center for Neural Science, New York University</aff>
<author-notes>
<corresp id="corresp1-0956797612463080">A. Ross Otto, Center for Neural Science, New York University, 4 Washington Place, New York, NY 10003 E-mail: <email>rotto@nyu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>24</volume>
<issue>5</issue>
<fpage>751</fpage>
<lpage>761</lpage>
<history>
<date date-type="received">
<day>30</day>
<month>3</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>24</day>
<month>8</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>A number of accounts of human and animal behavior posit the operation of parallel and competing valuation systems in the control of choice behavior. In these accounts, a flexible but computationally expensive model-based reinforcement-learning system has been contrasted with a less flexible but more efficient model-free reinforcement-learning system. The factors governing which system controls behavior—and under what circumstances—are still unclear. Following the hypothesis that model-based reinforcement learning requires cognitive resources, we demonstrated that having human decision makers perform a demanding secondary task engenders increased reliance on a model-free reinforcement-learning strategy. Further, we showed that, across trials, people negotiate the trade-off between the two systems dynamically as a function of concurrent executive-function demands, and people’s choice latencies reflect the computational expenses of the strategy they employ. These results demonstrate that competition between multiple learning systems can be controlled on a trial-by-trial basis by modulating the availability of cognitive resources.</p>
</abstract>
<kwd-group>
<kwd>cognitive neuroscience</kwd>
<kwd>decision making</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Accounts of decision making across cognitive science, neuroscience, and behavioral economics posit that decisions arise from two qualitatively distinct systems that differ broadly in their reliance on controlled versus automatic processing (<xref ref-type="bibr" rid="bibr3-0956797612463080">Daw, Niv, &amp; Dayan, 2005</xref>; <xref ref-type="bibr" rid="bibr5-0956797612463080">Dickinson, 1985</xref>; <xref ref-type="bibr" rid="bibr10-0956797612463080">Kahneman &amp; Frederick, 2002</xref>; <xref ref-type="bibr" rid="bibr12-0956797612463080">Loewenstein &amp; O’Donoghue, 2004</xref>). This distinction is thought to be of considerable practical importance, for instance, as a possible substrate for compulsion in drug abuse (<xref ref-type="bibr" rid="bibr7-0956797612463080">Everitt &amp; Robbins, 2005</xref>) and other disorders of self-control (<xref ref-type="bibr" rid="bibr12-0956797612463080">Loewenstein &amp; O’Donoghue, 2004</xref>).</p>
<p>However, one challenge for investigating such a division of labor experimentally is that, in typical formulations, it is often unclear which system produced a given behavior, and the contributions of each system can often be conclusively distinguished only by procedures that are both laborious and theory dependent (<xref ref-type="bibr" rid="bibr6-0956797612463080">Dickinson &amp; Balleine, 2004</xref>; <xref ref-type="bibr" rid="bibr9-0956797612463080">Gläscher, Daw, Dayan, &amp; O’Doherty, 2010</xref>). Moreover, although different theories share a common rhetorical theme, there is less consensus as to the fundamental, defining characteristics of the two systems, which makes it a challenge to relate data grounded in different models’ predictions. One particularly large gap in this regard is between research in human and animal cognitive psychology. Human research is typically grounded in a distinction between procedural versus explicit learning and elucidated by manipulating factors such as working memory (WM) load (<xref ref-type="bibr" rid="bibr8-0956797612463080">Foerde, Knowlton, &amp; Poldrack, 2006</xref>; <xref ref-type="bibr" rid="bibr27-0956797612463080">Zeithamova &amp; Maddox, 2006</xref>). More invasive animal research has traditionally been conducted on parallel brain structures for instrumental learning (<xref ref-type="bibr" rid="bibr6-0956797612463080">Dickinson &amp; Balleine, 2004</xref>; <xref ref-type="bibr" rid="bibr26-0956797612463080">Yin &amp; Knowlton, 2006</xref>) and has usually involved two-stage learning-and-transfer paradigms, such as latent learning or reward devaluation. This latter domain has been of recent interest to human cognitive neuroscientists because of the close relationship between traditional associative-learning models and the reinforcement-learning algorithms that have been used to characterize activity in dopaminergic systems in both humans and animals (temporal-difference learning; <xref ref-type="bibr" rid="bibr17-0956797612463080">O’Doherty, Dayan, Friston, Critchley, &amp; Dolan, 2003</xref>; <xref ref-type="bibr" rid="bibr21-0956797612463080">Schultz, Dayan, &amp; Montague, 1997</xref>).</p>
<p>For these reasons, reinforcement-learning theories may provide new leverage for reframing and formalizing the dual-system distinction in a manner that spans both animal and human traditions. One contemporary theoretical framework leverages the distinction between two families of reinforcement-learning algorithms: model-based and model-free reinforcement learning (<xref ref-type="bibr" rid="bibr3-0956797612463080">Daw et al., 2005</xref>). Temporal-difference-based theories posit that the dopamine system is model free in the sense that it directly learns preferences for actions using a principle of repeating reinforced actions (akin to Thorndike’s law of effect) without ever explicitly learning or reasoning about the structure of the environment. In model-based reinforcement learning, by contrast, the system learns an internal “model” of the proximal consequences of actions in the environment (such as the map of a maze) in order to prospectively evaluate candidate choices. This algorithmic distinction closely echoes theories of instrumental conditioning in animals (<xref ref-type="bibr" rid="bibr5-0956797612463080">Dickinson, 1985</xref>), but the computational detail of <xref ref-type="bibr" rid="bibr3-0956797612463080">Daw et al.’s (2005)</xref> framework leads to relatively specific predictions that afford clear identification of each system’s contribution to choice behavior.</p>
<p>Consistent with prior work suggesting the parallel operation of distinct valuation systems (<xref ref-type="bibr" rid="bibr6-0956797612463080">Dickinson &amp; Balleine, 2004</xref>), previous research found that people appear to exhibit a mixture of both strategies in their choice patterns (<xref ref-type="bibr" rid="bibr2-0956797612463080">Daw, Gershman, Seymour, Dayan, &amp; Dolan, 2011</xref>). However, it remains to be seen whether these two forms of choice behavior reflect any of the characteristics associated with controlled and automatic processing in human cognitive neuroscience and, even more fundamentally, whether they really capture distinct and separable processes. Underlining the question, recent functional MRI (fMRI) work unexpectedly revealed overlapping neural signatures of the two strategies (<xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al., 2011</xref>).</p>
<p>To investigate these questions, we paired the multistep choice paradigm of Daw and colleagues (<xref ref-type="bibr" rid="bibr2-0956797612463080">2011</xref>; <xref ref-type="fig" rid="fig1-0956797612463080">Fig. 1</xref>) with a demanding concurrent task manipulation designed to tax WM resources. It has been demonstrated that concurrent WM load drives people away from explicit or rule-based systems toward reliance on putatively implicit systems in perceptual categorization (<xref ref-type="bibr" rid="bibr27-0956797612463080">Zeithamova &amp; Maddox, 2006</xref>), probabilistic classification (<xref ref-type="bibr" rid="bibr8-0956797612463080">Foerde et al., 2006</xref>), and simple prediction (<xref ref-type="bibr" rid="bibr18-0956797612463080">Otto, Taylor, &amp; Markman, 2011</xref>). Contemporary theories differentiating model-based versus model-free reinforcement learning hypothesize that increased demands on central executive resources influence the trade-off between the two systems because model-based strategies involve planning processes that putatively draw on executive resources (<xref ref-type="bibr" rid="bibr16-0956797612463080">Norman &amp; Shallice, 1986</xref>), whereas model-free strategies simply apply the parsimonious principle of repeating previously rewarded actions (<xref ref-type="bibr" rid="bibr3-0956797612463080">Daw et al., 2005</xref>; <xref ref-type="bibr" rid="bibr4-0956797612463080">Dayan, 2009</xref>).</p>
<fig id="fig1-0956797612463080" position="float">
<label>Fig. 1.</label>
<caption>
<p>Paradigm and model predictions for the two-stage choice task. In the multistep choice paradigm (a), participants are presented with two options in the first stage and asked to choose one. Each option has a 70% probability of leading to one of two second stages and a 30% probability of leading to the other. In the second stage, participants again have to choose between two options, each of which is associated with different probabilities of reward. The graphs (b and c) depict the predicted probability of repeating a first-stage action in the second stage (“stay probability”) as a function of whether that choice was rewarded or unrewarded and whether the transition from the first-stage state to the second-stage state in the previous trial was common (70% probability) or rare (30% probability). Under a model-free choice strategy (b), a first-stage choice resulting in reward is more likely to be repeated on the subsequent trial regardless of whether that reward occurred after a common or a rare transition. Under a model-based choice strategy (c), rewards after rare transitions should affect the value of the unchosen first-stage option, thus leading to a predicted interaction between the factors of reward and transition probability. Panels (b) and (c) are adapted from “Model-Based Influences on Humans’ Choices and Striatal Prediction Errors,” by <xref ref-type="bibr" rid="bibr2-0956797612463080">Daw, Gershman, Seymour, Dayan, and Dolan, 2011</xref>, <italic>Neuron, 69</italic>, p. 1206. Copyright 2011 by Elsevier.</p>
</caption>
<graphic xlink:href="10.1177_0956797612463080-fig1.tif"/>
</fig>
<p>In Experiment 1, we utilized a within-subjects design in which some trials of the choice task were accompanied by a numerical Stroop task that has been demonstrated to displace explicit processing resources in perceptual category learning (<xref ref-type="bibr" rid="bibr25-0956797612463080">Waldron &amp; Ashby, 2001</xref>). We hypothesized that if learning, planning, or both in a model-based system is constrained by the availability of central executive resources, then choice behavior on these trials should, selectively, reflect reduced model-based contributions and increased model-free contributions. As a corollary, we predicted that response times (RTs)—a widely used index of cognitive cost (<xref ref-type="bibr" rid="bibr19-0956797612463080">Payne, Bettman, &amp; Johnson, 1993</xref>)—would be slower on trials in which model-based influence was prevalent in participants’ choices than on trials in which choice appeared relatively model free. To further highlight model-based choice’s dependence on central executive resources, we conceptually replicated this phenomenon in Experiment 2.</p>
<sec id="section1-0956797612463080">
<title>Experiment 1</title>
<sec id="section2-0956797612463080">
<title>Method</title>
<p>Our experimental procedure is described in detail in this section. Readers seeking an intuitive understanding of the task and our predictions are encouraged to advance to the Results section.</p>
<sec id="section3-0956797612463080">
<title>Participants</title>
<p>A total of 43 undergraduates at the University of Texas participated in Experiment 1 in exchange for course credit and were paid 2.5¢ per rewarded trial to incentivize choice. The data of 25 participants were used in analyses (participant inclusion criteria are detailed in the Supplemental Material available online).</p>
</sec>
<sec id="section4-0956797612463080">
<title>Materials and procedure</title>
<p>Participants performed 300 trials of the two-stage reinforcement-learning task (<xref ref-type="fig" rid="fig1-0956797612463080">Fig. 1a</xref>); on 150 of these trials (WM-load trials), the task was accompanied by a numerical Stroop task. These WM-load trials were positioned randomly, but with the constraint that the ordering would yield equal numbers of three trial types of interest (50 each for Lag 0, Lag 1, and Lag 2 trials, with lag defined by the number of trials since the most recent WM-load trial; see the Results section for more details). Participants were instructed to perform the WM task as well as possible and to make choices with whatever cognitive resources they had remaining (i.e., “with what was left over”). After being familiarized with the reinforcement-learning task’s structure and goals, they were given 15 practice WM-load trials to familiarize them with the response procedure.</p>
<p>The reinforcement-learning task followed the same general procedure in both no-WM-load and WM-load trials (see <xref ref-type="fig" rid="fig2-0956797612463080">Fig. 2</xref> for a timeline). In the first step of no-WM-load trials, two fractal images appeared side by side on a black background, and participants had 2 s to choose between the left- or right-hand image using the “Z” or “?” key, respectively. After a choice was made, the selected image was highlighted for the remainder of the response period, and the background color changed according to which second-stage state the participant had been transitioned to. The second-stage state could be either common (70% probability) or rare (30% probability). After the transition, the image selected in the first stage was minimized and moved to the top of the screen. Two different fractal images were then displayed, and participants again had 2 s to choose one. The selected action was highlighted for the remainder of the response period. Then, either a picture of a quarter (indicating that they had been rewarded on that trial) or the number zero (indicating that they had not been rewarded on that trial) was shown. The reward probabilities associated with second-stage actions were governed by independently drifting Gaussian random walks (<italic>SD</italic> = 0.025) with reflecting boundaries at 0.25 and 0.75. Mappings of actions to stimuli and transition probabilities were randomized across participants.</p>
<fig id="fig2-0956797612463080" position="float">
<label>Fig. 2.</label>
<caption>
<p>Timeline of events in WM-load trials (left) and no-WM-load trials (right) in Experiment 1. The first stage of no-WM-load trials began with a blank screen, and then two fractal images appeared side by side on a black background. Participants had 2 s to choose between the two images, after which their selection was highlighted for the remainder of the response period. The first stage then transitioned to the second-stage state, which was signaled by a change in background color. The image selected in the first stage shrank and moved to the top of the screen, and two new images appeared. Participants again had 2 s to choose an image, and their response was highlighted for the remainder of the response period. Then, either a picture of a quarter (shown in the timeline on the left) or the number zero (shown in the timeline on the right) appeared to indicate that they had either been rewarded or not been rewarded, respectively, on that trial. This was followed by a blank screen and a fixation cross. WM-load trials followed the same general procedure as no-WM-load trials, with the following differences. They began with a cue that these trials would include a numerical Stroop task. Two different numbers of different physical sizes then appeared above the choice stimuli in the first stage for 200 ms and were subsequently covered by white masks. After second-stage reward feedback was provided, either the word “VALUE” (shown here) or “SIZE” appeared alone on a black screen, and participants had to indicate whether the number with the larger value or the larger size, respectively, had appeared on the left or the right during the first stage by pressing one of two keys. Feedback was given for 1 s. WM-load trials were highlighted in red throughout. Critically, event timing was equated between the two trial types.</p>
</caption>
<graphic xlink:href="10.1177_0956797612463080-fig2.tif"/>
</fig>
<p>WM-load trials followed the same procedure, except that participants additionally had to perform a numerical Stroop task, which required them to remember which of two numbers was physically and numerically larger (<xref ref-type="bibr" rid="bibr25-0956797612463080">Waldron &amp; Ashby, 2001</xref>; <xref ref-type="fig" rid="fig2-0956797612463080">Fig. 2</xref>). These trials were signaled in two ways. First, during the 1-s intertrial interval preceding the first stage, participants were warned with the message “WATCH FOR NUMBERS.” Second, during both stages of the choice task, the screen was outlined in red. At the beginning of the first stage, two digits were presented for 200 ms above and to the left and right, respectively, of the choice stimuli; they were then covered by a white mask for another 200 ms. After second-stage reward feedback was provided, either the word “VALUE” or “SIZE” appeared alone on a black screen, and there was a 1-s response period in which participants used the “Z” or “?” key to indicate whether the number with the larger value or larger size appeared on the left or the right side of the screen, respectively, during the first stage. Their response was followed by 1 s of feedback (“CORRECT” or “INCORRECT”) and then an intertrial interval. If participants failed to choose one of the images in either response stage or in the numerical Stroop task, a red “X” appeared for 1 s to indicate that their response was too slow, and the trial was aborted. Crucially, the trial lengths were equated across WM-load and no-WM-load trials.</p>
</sec>
</sec>
<sec id="section5-0956797612463080">
<title>Results</title>
<p>Participants performed 300 trials of a two-stage reinforcement-learning task (<xref ref-type="fig" rid="fig1-0956797612463080">Fig. 1a</xref>). In each two-stage trial, people made an initial first-stage choice between two options (depicted as fractals), which probabilistically led to one of two second-stage “states” (colored green or blue). In each of these states, participants made another choice between two options, which were associated with different probabilities of monetary reward. Each of the first-stage responses usually led to a particular second-stage state (70% of the time) but sometimes led to the other second-stage state (30% of the time). Because the second-stage reward probabilities independently changed over time, decision makers needed to make trial-by-trial adjustments to their choice behavior in order to effectively maximize payoffs.</p>
<p>Model-based and model-free strategies make qualitatively different predictions about how second-stage rewards influence first-stage choices on subsequent trials. For example, consider a first-stage choice that results in a rare transition to a second stage wherein the second-stage choice was rewarded. Under a pure model-free strategy—by virtue of the reinforcement principle—one would repeat the same first-stage response in the following trial because it ultimately resulted in reward. In contrast, a model-based choice strategy, utilizing a model of the transition structure and immediate rewards to prospectively evaluate the first-stage actions, would predict a decreased tendency to repeat the same first-stage option because the other first-stage action would actually be more likely to lead to that second-stage state.</p>
<p>These patterns of dependency of choices on the previous trial’s events can be distinguished by a two-factor analysis of the effect of the previous trial’s reward (rewarded vs. unrewarded) and transition type (common vs. rare) on the first-stage choice in the current trial.<sup><xref ref-type="fn" rid="fn1-0956797612463080">1</xref></sup> The predicted choice pattern for a pure model-free strategy and a pure model-based strategy are depicted in <xref ref-type="fig" rid="fig1-0956797612463080">Figures 1b</xref> and <xref ref-type="fig" rid="fig1-0956797612463080">1c</xref>, respectively, derived from model simulations (<xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al., 2011</xref>; see the Reinforcement-Learning Model section in the Supplemental Material). A pure model-free strategy predicts only a main effect of reward, whereas a model-based strategy predicts a full crossover interaction between reward and transition type because transition probabilities are taken into account. Following <xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al. (2011)</xref>, we factorially examined the impact that both the transition type and reward on the previous trial had on participants’ tendency to repeat the same first-stage choice on the current trial. To examine the relationship between these signatures of choice strategies and the concurrent WM-load manipulation, we crossed these factors with a third factor defining the position of the most recent WM-load trial relative to the current trial. We sorted trials according to when the most recent WM-load trial had occurred relative to the current trial, which yielded three trial types of interest: Lag 0, Lag 1, and Lag 2, which refer to trials in which WM load occurred on the current trial, the previous trial, or the trial preceding the previous trial, respectively. Trials in which WM load occurred more than once across the current trial and its two predecessors did not fall into any of these categories and were excluded from analysis.</p>
<sec id="section6-0956797612463080">
<title>Strategy as a function of concurrent WM load</title>
<p>We hypothesized that if WM load interferes with model-based decision making, behavior on Lag 0 trials should be consistent with model-free decision making (<xref ref-type="fig" rid="fig1-0956797612463080">Fig. 1b</xref>) because participants do not have the cognitive resources to carry out a model-based strategy on those trials. Conversely, we hypothesized that behavior on Lag 2 trials would reflect a mixture of both model-based and model-free strategies—mirroring the results of <xref ref-type="bibr" rid="bibr2-0956797612463080">Daw and colleagues’ (2011)</xref> study—because these trials involved no WM load either on the current trial or on the preceding trial, and thus participants could bring their full cognitive resources to bear on these trials. We reasoned further that if WM load disrupts participants’ ability to integrate information crucial for model-based choice, then behavior on Lag 1 trials should appear model free (mirroring behavior on Lag 0 trials). In contrast, if participants are able to integrate this information while under load and apply it on the subsequent trial, then behavior on Lag 1 trials should resemble a mixture of both strategies, mirroring behavior on Lag 2 trials.</p>
<p>As <xref ref-type="fig" rid="fig3-0956797612463080">Figure 3a</xref> shows, the pattern of results on Lag 2 trials suggests that participants’ choices on these trials reflect both the main effect of reward (characteristic of model-free reinforcement learning) and its interaction with the rare or common transition (characteristic of model-based reinforcement learning); this pattern is consistent with the single-task results obtained by <xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al. (2011)</xref>. In contrast, choices on Lag 0 and Lag 1 trials (<xref ref-type="fig" rid="fig3-0956797612463080">Figs. 3b</xref> and <xref ref-type="fig" rid="fig3-0956797612463080">3c</xref>) appear sensitive only to reward on the previous trial and not to the transition type. Qualitatively, these choice patterns resemble a pure model-free strategy, which suggests that WM load interferes with model-based choice.</p>
<fig id="fig3-0956797612463080" position="float">
<label>Fig. 3.</label>
<caption>
<p>Results from Experiment 1: average proportion of trials on which participants chose to stay with the response they selected in the first stage of the previous trial as a function of whether they received a reward on the previous trial and whether the second-stage state transitioned to on the previous trial was common or rare. Results are shown separately for (a) Lag 2 trials, (b) Lag 1 trials, and (c) Lag 0 trials. Lag 0, Lag 1, and Lag 2 trials were those in which working memory (WM) load was taxed on the present trial, the previous trial, and the trial preceding the previous trial, respectively. Error bars depict standard errors of the mean.</p>
</caption>
<graphic xlink:href="10.1177_0956797612463080-fig3.tif"/>
</fig>
<p>To quantify these effects of WM load on choice behavior, we conducted a mixed-effects logistic regression (<xref ref-type="bibr" rid="bibr20-0956797612463080">Pinheiro &amp; Bates, 2000</xref>) to explain the first-stage choice on each trial <italic>t</italic> (coded as stay vs. switch) using binary predictors indicating whether reward was received on <italic>t</italic> – 1 and the transition type (common or rare) that had produced it. Further, we estimated these factors under each trial type—Lag 0, Lag 1, and Lag 2, represented by binary indicators—and, to capture any individual differences, specified all coefficients as random effects over participants. The full regression specification and coefficient estimates are reported in <xref ref-type="table" rid="table1-0956797612463080">Table 1</xref>.</p>
<table-wrap id="table1-0956797612463080" position="float">
<label>Table 1.</label>
<caption>
<p>Results of the Logistic Regression Investigating the Influence of Working-Memory-Load Lag, Previous Outcome, and Previous Transition Type on First-Stage Response Repetition in Experiment 1</p>
</caption>
<graphic alternate-form-of="table1-0956797612463080" xlink:href="10.1177_0956797612463080-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Predictor</th>
<th align="center">Estimate</th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Intercept</td>
<td>1.00 (0.18)</td>
<td>&lt; .0001</td>
</tr>
<tr>
<td>Lag 0</td>
<td>−0.23 (0.14)</td>
<td>.118</td>
</tr>
<tr>
<td>Lag 1</td>
<td>−0.43 (0.12)</td>
<td>&lt; .0001</td>
</tr>
<tr>
<td>Lag 0 × Reward</td>
<td>0.34 (0.13)</td>
<td>.010</td>
</tr>
<tr>
<td>Lag 1 × Reward</td>
<td>0.19 (0.09)</td>
<td>.031</td>
</tr>
<tr>
<td>Lag 2 × Reward</td>
<td>0.23 (0.12)</td>
<td>.044</td>
</tr>
<tr>
<td>Lag 0 × Transition Type</td>
<td>0.07 (0.09)</td>
<td>.434</td>
</tr>
<tr>
<td>Lag 1 × Transition Type</td>
<td>−0.07 (0.08)</td>
<td>.390</td>
</tr>
<tr>
<td>Lag 2 × Transition Type</td>
<td>0.02 (0.09)</td>
<td>.776</td>
</tr>
<tr>
<td>Lag 0 × Reward × Transition Type</td>
<td>0.06 (0.09)</td>
<td>.478</td>
</tr>
<tr>
<td>Lag 1 × Reward × Transition Type</td>
<td>−0.07 (0.08)</td>
<td>.383</td>
</tr>
<tr>
<td>Lag 2 × Reward × Transition Type</td>
<td>−0.23 (0.09)</td>
<td>.011</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0956797612463080">
<p>Note: Standard errors are given in parentheses. Lag 0, Lag 1, and Lag 2 refer to trials in which working memory load occurred on the current trial, the previous trial, or the trial preceding the previous trial, respectively. Previous outcome refers to whether the participant was rewarded on the previous trial. Transition type refers to whether the transition from the first stage to the second stage in the previous trial was common or rare.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>We found a significant main effect of reward for each trial type (<italic>p</italic>s &lt; .05), which indicates that participants had a general tendency to repeat rewarded first-stage responses, consistent with intact use of a model-free strategy. This finding also suggests that concurrent task demands did not produce trivially random or otherwise unstructured behavior. However, we found a significant three-way interaction between Lag 2, reward, and transition type (<italic>p</italic> &lt; .05), which suggests that the interaction characteristic of a model-based choice strategy was evident in Lag 2 trials, as hypothesized. Neither the interactions among Lag 0, reward, and transition type nor among Lag 1, reward, and transition type were significant, which indicates that this model-based interaction was not present in these trial types (<italic>p</italic>s &gt; .25).</p>
<p>To examine whether these differences between trial types were themselves significant, we conducted a planned contrast on the Lag 2 three-way interaction (Lag 2 × Reward × Transition Type, indicative of model-based learning). This interaction was significantly larger than the same interactions at both the Lag 1 and Lag 0 levels (<italic>p</italic> &lt; .05). Further, we found no differences in model-free behavior between any of the trial types (e.g., Lag 0 × Reward, Lag 1 × Reward, and Lag 2 × Reward) that we considered (<italic>p</italic>s &gt; .30). All of these results are consistent with the hypothesis that concurrent demands selectively interfere with model-based learning and planning while sparing model-free decision making. (For analyses of second-stage choice behavior and secondary task performance, refer to the Supplemental Materials.)</p>
</sec>
<sec id="section7-0956797612463080">
<title>Choice RTs</title>
<p>We also predicted that model-based choice, by virtue of its hypothesized cognitive costs, would incur larger RTs at the first-stage choice than model-free choices would (<xref ref-type="bibr" rid="bibr11-0956797612463080">Keramati, Dezfouli, &amp; Piray, 2011</xref>). We compared Lag 2 trials (in which behavior reflected the influence of a model-based strategy) with Lag 1 trials (in which behavior appeared to reflect only a model-free strategy). The comparison between the two single-task trial types that exhibited different degrees of model usage provided a clean test of the hypothesis: In Lag 0 trials, the RTs were confounded by the demands of the concurrent task itself. A mixed-effects linear model (see the Choice and RT Analyses section in the Supplemental Material) carried out on first-stage RTs revealed that participants exhibited significantly larger RTs on Lag 2 choices than on Lag 1 choices (<xref ref-type="fig" rid="fig4-0956797612463080">Fig. 4</xref>; β = 2.05, <italic>p</italic> &lt; .05), which suggests that model-based choice—evident on Lag 2 trials—indeed bore the signature of a cognitively costly process. Put another way, choice was faster on Lag 1 trials—where behavior appeared model free—which supports the notion that the process governing choice on those trials was cognitively less expensive.</p>
<fig id="fig4-0956797612463080" position="float">
<label>Fig. 4.</label>
<caption>
<p>Results from Experiment 1: difference in median response times (RTs) between Lag 2 and Lag 1 trials for individual participants.</p>
</caption>
<graphic xlink:href="10.1177_0956797612463080-fig4.tif"/>
</fig>
</sec>
<sec id="section8-0956797612463080">
<title>Reinforcement-learning model</title>
<p>One limitation of the foregoing regression analysis is that it only accounted for the influence of reinforcement occurring on the immediately preceding trial. Most reinforcement-learning models, in contrast, posit a decaying influence of all previous trials. We extended our regression analysis by fitting a dual-system reinforcement-learning model—a computational instantiation of the principles governing two hypothesized choice systems (<xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al., 2011</xref>; <xref ref-type="bibr" rid="bibr9-0956797612463080">Gläscher et al., 2010</xref>)—to behavior in this task. This model consists of a model-free system that updates estimates of choice values using temporal-difference learning and of a model-based system that learns a transition-and-reward model of the task and uses these to compute choice values on the fly (see Reinforcement-Learning Model in the Supplemental Material). The values are linearly mixed according to a weight parameter that determines the balance between model-free and model-based control—weights closer to 0 indicate model-free control, whereas weights closer to 1 indicate model-based control. The mixed value is then used to generate choices according to a softmax rule (<xref ref-type="bibr" rid="bibr23-0956797612463080">Sutton &amp; Barto, 1998</xref>). To accommodate the present paradigm, we fit two separate mixing weights: one for Lag 0 and Lag 1 trials (combined) and one for Lag 2 trials only. We found that Lag 2 weights were significantly larger than the Lag 0 and Lag 1 weights (<xref ref-type="fig" rid="fig5-0956797612463080">Fig. 5</xref>), <italic>t</italic>(24) = 2.94, <italic>p</italic> &lt; .01; this suggests that participants’ behavior was more model-based at longer lags and corroborates the results of the regression analysis.</p>
<fig id="fig5-0956797612463080" position="float">
<label>Fig. 5.</label>
<caption>
<p>Results from Experiment 1: best-fitting mixing weights across Lag 2 versus Lag 0 and Lag 1 trials (combined) resulting from fitting the reinforcement-learning algorithm to subjects’ choices. Weights closer to 0 indicate more model-free control, whereas weights closer to 1 indicate more model-based control. Error bars indicate standard errors.</p>
</caption>
<graphic xlink:href="10.1177_0956797612463080-fig5.tif"/>
</fig>
</sec>
</sec>
</sec>
<sec id="section9-0956797612463080">
<title>Experiment 2</title>
<p>Because the within-subjects WM-load manipulation we utilized in Experiment 1 was rather intricate and novel, we sought to provide a between-subjects replication of the study using a separate WM-load manipulation in which one group of participants counted auditory tones while performing the same choice task as in Experiment 1 (<xref ref-type="bibr" rid="bibr8-0956797612463080">Foerde et al., 2006</xref>). In brief, we found that the behavior exhibited by single-task participants in the current experiment resembled the mixture of strategies observed in Lag 2 trials in the previous experiment, whereas the behavior of dual-task participants in the current experiment resembled the model-free pattern of choice observed in Lag 0 and Lag 1 conditions in the previous experiment (<xref ref-type="fig" rid="fig6-0956797612463080">Fig. 6</xref>; <xref ref-type="table" rid="table2-0956797612463080">Table 2</xref>; see the Supplemental Material for details of Experiment 2).</p>
<fig id="fig6-0956797612463080" position="float">
<label>Fig. 6.</label>
<caption>
<p>Results from (a) the single-task condition and (b) the dual-task condition of Experiment 2: average proportion of trials on which participants chose to stay with the response they selected in the first stage of the previous trial as a function of whether they received a reward on the previous trial and whether the second-stage state transitioned to on the previous trial was common or rare.</p>
</caption>
<graphic xlink:href="10.1177_0956797612463080-fig6.tif"/>
</fig>
<table-wrap id="table2-0956797612463080" position="float">
<label>Table 2.</label>
<caption>
<p>Results of the Logistic Regression Investigating the Influence of Working-Memory-Load Condition, Previous Outcome, and Previous Transition Type on First-Stage Response Repetition in Experiment 2</p>
</caption>
<graphic alternate-form-of="table2-0956797612463080" xlink:href="10.1177_0956797612463080-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Predictor</th>
<th align="center">Estimate</th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Intercept</td>
<td>1.15 (0.13)</td>
<td>&lt; .0001</td>
</tr>
<tr>
<td>Load</td>
<td>−0.25 (0.13)</td>
<td>.058</td>
</tr>
<tr>
<td>Reward</td>
<td>0.42 (0.07)</td>
<td>.000</td>
</tr>
<tr>
<td>Transition</td>
<td>0.01 (0.03)</td>
<td>.823</td>
</tr>
<tr>
<td>Load × Reward</td>
<td>0.01 (0.07)</td>
<td>.824</td>
</tr>
<tr>
<td>Load × Transition</td>
<td>−0.02 (0.03)</td>
<td>.433</td>
</tr>
<tr>
<td>Reward × Transition Type</td>
<td>−0.11 (0.04)</td>
<td>.005</td>
</tr>
<tr>
<td>Load × Reward × Transition Type</td>
<td>0.08 (0.04)</td>
<td>.047</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0956797612463080">
<p>Note: Standard errors are given in parentheses. Working memory load was manipulated by having each participant perform either single-task or dual-task trials. Previous outcome refers to whether the participant was rewarded on the previous trial. Transition type refers to whether the transition from the first stage to the second stage in the previous trial was common or rare.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section10-0956797612463080" sec-type="discussion">
<title>General Discussion</title>
<p>A number of dual-system accounts of choice behavior posit a distinction between two systems distinguished by, among other things, the extent to which central executive or prefrontal resources are employed (<xref ref-type="bibr" rid="bibr6-0956797612463080">Dickinson &amp; Balleine, 2004</xref>; <xref ref-type="bibr" rid="bibr13-0956797612463080">McClure, Laibson, Loewenstein, &amp; Cohen, 2004</xref>). Still, the contributions of the two putative systems have proven laborious to isolate behaviorally (<xref ref-type="bibr" rid="bibr24-0956797612463080">Valentin, Dickinson, &amp; O’Doherty, 2007</xref>) or with neuroimaging (<xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al., 2011</xref>). Informed by a contemporary theoretical framework that makes quantitative predictions about the behavioral signatures of the two systems and the arbitration of behavioral control among the two (<xref ref-type="bibr" rid="bibr3-0956797612463080">Daw et al., 2005</xref>), we demonstrated how human decision makers trade off the concurrent cognitive demands of the environment with their usage of computationally expensive choice strategies. In particular, when burdened with concurrent WM load, decision makers relied on a pure reinforcement-based strategy—akin to model-free reinforcement learning—and eschewed the transition structure of the environment. When unencumbered by these demands, participants’ choices reflected a mixture of model-based and model-free strategies, mirroring previous results (<xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al., 2011</xref>).</p>
<p>The present results are evocative of past research revealing that concurrent cognitive demands shift the onus of learning from explicit, declarative systems to procedural-learning systems (<xref ref-type="bibr" rid="bibr8-0956797612463080">Foerde et al., 2006</xref>). It is important to note that although previous work has revealed that concurrent demands can shift people’s response strategies, these studies have relied on comparing results across multiple task methodologies chosen to favor either strategy (<xref ref-type="bibr" rid="bibr25-0956797612463080">Waldron &amp; Ashby, 2001</xref>; <xref ref-type="bibr" rid="bibr27-0956797612463080">Zeithamova &amp; Maddox, 2006</xref>) or post hoc assessments of declarative knowledge (<xref ref-type="bibr" rid="bibr8-0956797612463080">Foerde et al., 2006</xref>). The two-step reinforcement-learning task used in the experiments reported here, in contrast, afforded unambiguous identification of the simultaneous contributions of model-based and model-free choice strategies within the same task and permitted dynamic assessment of trial-by-trial arbitration of control between the two systems. Here, accordingly, we present evidence of a difference in strategy use between trial types that occurred fully interleaved, consistent with rapid strategic switching within participants and task.</p>
<p>These results complement previous fMRI investigations using the present task—a previous finding of convergent neural correlates for the two strategies (<xref ref-type="bibr" rid="bibr2-0956797612463080">Daw et al., 2011</xref>) left open the question of whether they were actually psychologically or functionally distinct. Here, our behavioral results provide a compelling demonstration that model-based and model-free valuation are dissociable, and these findings further underscore the utility of within-subjects manipulations for dissociating the behavioral contributions of putatively separate neural systems. Finally, the distinction as we operationalize it is arguably of more biological relevance than previous attempts, because the model-free strategy on which participants appeared to fall back under WM load was exactly that predicted by prominent neurocomputational accounts of the dopamine system (<xref ref-type="bibr" rid="bibr15-0956797612463080">Montague, Dayan, &amp; Sejnowski, 1996</xref>).</p>
<p>It is also worth noting that model-based choice relies on at least two constituent processes: (a) learning of second-stage reward probabilities and environment- transition probabilities from feedback and (b) planning by using these reward probabilities and environment-transition probabilities prospectively to inform first-stage choice on subsequent trials (<xref ref-type="bibr" rid="bibr22-0956797612463080">Sutton, 1990</xref>). Insofar as the learning relevant to the choice on Trial <italic>t</italic> occurs on earlier trials (and, specifically, for the effects quantified here on the preceding trial, <italic>t</italic> – 1), but the planning occurs on the trial itself, we might expect WM load occurring at Lag 1 (i.e., on trial <italic>t</italic> – 1) to primarily affect learning and WM load at Lag 0 (Trial t) to primarily affect planning. By this logic, our finding of a similar strategic deficit at both lags may suggest that WM load disrupted both putative subprocesses. That said, it is possible that these processes are not as temporally isolated as we ascribe (e.g., action planning on Trial <italic>t</italic> may begin as soon as the feedback is received on the preceding trial) or that results also reflect other executive demands not isolated to a single trial (e.g., switching between dual and single tasks from <italic>t</italic> – 1 to <italic>t</italic>), making this interpretation tentative. Future work should aim to disambiguate more precisely whether concurrent executive demands incapacitate planning, learning, or some combination thereof, perhaps by using more specifically directed distractor tasks.</p>
<p>Although the model-based strategy we observed in the Lag 2 trials was, by definition, not predicted by a model-free reinforcement-learning system of the sort associated with the dopamine system, it is clearly possible to produce model-free switching (win-stay-lose-shift) via a deliberative or explicit strategy. Indeed, this is the question that the present manipulation was designed to address, and the finding that the model-free, but not the model-based, behavior is robust to concurrent load is consistent with the prediction that it arises from a distinct, striatal procedural-learning system that itself is also model free. Still, it is possible in principle that load promotes a shift to increased reliance on a cheaper—but still declarative in nature—win-stay-lose-shift strategy. However, the best-fitting learning rates recovered in our computational modeling (see Table S1 in the Supplemental Material) were low,<sup><xref ref-type="fn" rid="fn2-0956797612463080">2</xref></sup> which supports the idea that these influences arose from an incremental-learning process characteristic of implicit learning rather than a rule-based win-stay-lose-shift strategy.</p>
<p>Whereas <xref ref-type="bibr" rid="bibr2-0956797612463080">Daw and colleagues (2011)</xref> relied in part on individual differences in model-based choice to examine the two systems’ neural substrates, we explicitly manipulated reliance on these strategies within subjects and within tasks. As it is well documented that there are considerable individual differences in WM capacity and executive function (<xref ref-type="bibr" rid="bibr1-0956797612463080">Conway, Kane, &amp; Engle, 2003</xref>; <xref ref-type="bibr" rid="bibr14-0956797612463080">Miyake et al., 2000</xref>), a significant portion of the individual variability reported by Daw and colleagues may be attributable to individual differences in WM capacity, and likewise, these differences could have potentially modulated the effects of WM load reported here. Exactly how individual limitations in cognitive capacity, executive control, or a combination of the two modulate model-based choice warrants additional examination. Further, characterizing more precisely how humans balance the contributions of model-based and model-free choice is of considerable practical importance because contemporary accounts of a number of serious disorders of compulsion ascribe this behavior to abnormal expression of habitual or stimulus-driven control systems (<xref ref-type="bibr" rid="bibr7-0956797612463080">Everitt &amp; Robbins, 2005</xref>; <xref ref-type="bibr" rid="bibr12-0956797612463080">Loewenstein &amp; O’Donoghue, 2004</xref>).</p>
</sec>
</body>
<back>
<ack>
<p>We gratefully acknowledge Jeanette Mumford and Bradley Doll for helpful conversations and Grant Loomis for assistance with data collection.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research was supported by National Institute of Mental Health Grant MH077708 to Arthur B. Markman. A. Ross Otto was supported by a Mike Hogg Endowment Fellowship from the University of Texas. Samuel J. Gershman was supported by a Graduate Research Fellowship from the National Science Foundation. Nathaniel D. Daw was supported in part by National Institute of Neurological Disorders and Stroke Grant R01 NS 078784, a Scholar Award from the McKnight Foundation, and an Award in Understanding Human Cognition from the McDonnell Foundation.</p>
</fn>
<fn fn-type="supplementary-material">
<label>Supplemental Material</label>
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0956797612463080">
<label>1.</label>
<p>In general, reinforcement-learning models predict that a given trial’s choice depends on learning also from even earlier trials (and in the present study, we used fits of these models to verify that our results held when these longer-term dependencies were accounted for). However, because the most recent trial exerted the largest effect on choice in these models (and this effect becomes exclusive as free-learning-rate parameters approach 1), this factorial analysis provided a clear picture of the critical qualitative features of behavior less dependent on the specific parametric and structural assumptions of the full models.</p>
</fn>
<fn fn-type="other" id="fn2-0956797612463080">
<label>2.</label>
<p>Further, we fitted a separate model that allowed for different learning rates across the three trial types of interest (Lag 0, Lag 1, and Lag 2) and found that learning rates did not vary significantly as a function of WM-load lag, <italic>F</italic> = 0.83, <italic>p</italic> = .44.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Conway</surname><given-names>A. R. A.</given-names></name>
<name><surname>Kane</surname><given-names>M. J.</given-names></name>
<name><surname>Engle</surname><given-names>R. W.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Working memory capacity and its relation to general intelligence</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>7</volume>, <fpage>547</fpage>–<lpage>552</lpage>.</citation>
</ref>
<ref id="bibr2-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Daw</surname><given-names>N. D.</given-names></name>
<name><surname>Gershman</surname><given-names>S. J.</given-names></name>
<name><surname>Seymour</surname><given-names>B.</given-names></name>
<name><surname>Dayan</surname><given-names>P.</given-names></name>
<name><surname>Dolan</surname><given-names>R. J.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title>. <source>Neuron</source>, <volume>69</volume>, <fpage>1204</fpage>–<lpage>1215</lpage>.</citation>
</ref>
<ref id="bibr3-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Daw</surname><given-names>N. D.</given-names></name>
<name><surname>Niv</surname><given-names>Y.</given-names></name>
<name><surname>Dayan</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>, <fpage>1704</fpage>–<lpage>1711</lpage>.</citation>
</ref>
<ref id="bibr4-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dayan</surname><given-names>P.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Goal-directed control and its antipodes</article-title>. <source>Neural Networks</source>, <volume>22</volume>, <fpage>213</fpage>–<lpage>219</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dickinson</surname><given-names>A.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Actions and habits: The development of behavioural autonomy</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>308</volume>, <fpage>67</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr6-0956797612463080">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dickinson</surname><given-names>A.</given-names></name>
<name><surname>Balleine</surname><given-names>B.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The role of learning in the operation of motivational systems</article-title>. In <person-group person-group-type="editor">
<name><surname>Gallistel</surname><given-names>R.</given-names></name>
</person-group> (Ed.), <source>Stevens’ handbook of experimental psychology: Vol. 3. Learning, motivation, and emotion</source> (<edition>3rd ed.</edition>). <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr7-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Everitt</surname><given-names>B. J.</given-names></name>
<name><surname>Robbins</surname><given-names>T. W.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Neural systems of reinforcement for drug addiction: From actions to habits to compulsion</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>, <fpage>1481</fpage>–<lpage>1489</lpage>.</citation>
</ref>
<ref id="bibr8-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Foerde</surname><given-names>K.</given-names></name>
<name><surname>Knowlton</surname><given-names>B. J.</given-names></name>
<name><surname>Poldrack</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Modulation of competing memory systems by distraction</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>103</volume>, <fpage>11778</fpage>–<lpage>11783</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gläscher</surname><given-names>J.</given-names></name>
<name><surname>Daw</surname><given-names>N.</given-names></name>
<name><surname>Dayan</surname><given-names>P.</given-names></name>
<name><surname>O’Doherty</surname><given-names>J. P.</given-names></name>
</person-group> (<year>2010</year>). <article-title>States versus rewards: Dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title>. <source>Neuron</source>, <volume>66</volume>, <fpage>585</fpage>–<lpage>595</lpage>.</citation>
</ref>
<ref id="bibr10-0956797612463080">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kahneman</surname><given-names>D.</given-names></name>
<name><surname>Frederick</surname><given-names>S.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Representativeness revisited: Attribute substitution in intuitive judgment</article-title>. In <person-group person-group-type="editor">
<name><surname>Gilovich</surname><given-names>T.</given-names></name>
<name><surname>Griffin</surname><given-names>D.</given-names></name>
<name><surname>Kahneman</surname><given-names>D.</given-names></name>
</person-group> (Eds.), <source>Heuristics and biases: The psychology of intuitive judgment</source> (pp. <fpage>49</fpage>–<lpage>81</lpage>). <publisher-loc>Cambridge, England</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0956797612463080">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Keramati</surname><given-names>M.</given-names></name>
<name><surname>Dezfouli</surname><given-names>A.</given-names></name>
<name><surname>Piray</surname><given-names>P.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Speed/accuracy trade-off between the habitual and the goal-directed processes</article-title>. <source>PLoS Computational Biology</source>, <volume>7</volume>(<issue>5</issue>), <fpage>e1002055</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002055">http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002055</ext-link></citation>
</ref>
<ref id="bibr12-0956797612463080">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Loewenstein</surname><given-names>G.</given-names></name>
<name><surname>O’Donoghue</surname><given-names>T.</given-names></name>
</person-group> (<year>2004</year>). <source>Animal spirits: Affective and deliberative processes in economic behavior</source> (Working Papers No. 04–14). <publisher-loc>Ithaca, NY</publisher-loc>: <publisher-name>Cornell University Center for Analytic Economics</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ideas.repec.org/p/ecl/corcae/04-14.html">http://ideas.repec.org/p/ecl/corcae/04-14.html</ext-link></citation>
</ref>
<ref id="bibr13-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McClure</surname><given-names>S. M.</given-names></name>
<name><surname>Laibson</surname><given-names>D. I.</given-names></name>
<name><surname>Loewenstein</surname><given-names>G.</given-names></name>
<name><surname>Cohen</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Separate neural systems value immediate and delayed monetary rewards</article-title>. <source>Science</source>, <volume>306</volume>, <fpage>503</fpage>–<lpage>507</lpage>.</citation>
</ref>
<ref id="bibr14-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Miyake</surname><given-names>A.</given-names></name>
<name><surname>Friedman</surname><given-names>N. P.</given-names></name>
<name><surname>Emerson</surname><given-names>M. J.</given-names></name>
<name><surname>Witzki</surname><given-names>A. H.</given-names></name>
<name><surname>Howerter</surname><given-names>A.</given-names></name>
<name><surname>Wager</surname><given-names>T. D.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The unity and diversity of executive functions and their contributions to complex “frontal lobe” tasks: A latent variable analysis</article-title>. <source>Cognitive Psychology</source>, <volume>41</volume>, <fpage>49</fpage>–<lpage>100</lpage>.</citation>
</ref>
<ref id="bibr15-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Montague</surname><given-names>P. R.</given-names></name>
<name><surname>Dayan</surname><given-names>P.</given-names></name>
<name><surname>Sejnowski</surname><given-names>T. J.</given-names></name>
</person-group> (<year>1996</year>). <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title>. <source>The Journal of Neuroscience</source>, <volume>16</volume>, <fpage>1936</fpage>–<lpage>1947</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612463080">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Norman</surname><given-names>D. A.</given-names></name>
<name><surname>Shallice</surname><given-names>T.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Attention to action: Willed and automatic control of behavior</article-title>. In <person-group person-group-type="editor">
<name><surname>Davidson</surname><given-names>R. J.</given-names></name>
<name><surname>Schwartz</surname><given-names>G. E.</given-names></name>
<name><surname>Shapiro</surname><given-names>D.</given-names></name>
</person-group> (Eds.), <source>Consciousness and self-regulation: Advances in research and theory</source> (<volume>Vol. 4</volume>, pp. <fpage>1</fpage>–<lpage>18</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Plenum</publisher-name>.</citation>
</ref>
<ref id="bibr17-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>O’Doherty</surname><given-names>J. P.</given-names></name>
<name><surname>Dayan</surname><given-names>P.</given-names></name>
<name><surname>Friston</surname><given-names>K.</given-names></name>
<name><surname>Critchley</surname><given-names>H.</given-names></name>
<name><surname>Dolan</surname><given-names>R. J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Temporal difference models and reward-related learning in the human brain</article-title>. <source>Neuron</source>, <volume>38</volume>, <fpage>329</fpage>–<lpage>337</lpage>.</citation>
</ref>
<ref id="bibr18-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Otto</surname><given-names>A. R.</given-names></name>
<name><surname>Taylor</surname><given-names>E. G.</given-names></name>
<name><surname>Markman</surname><given-names>A. B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>There are at least two kinds of probability matching: Evidence from a secondary task</article-title>. <source>Cognition</source>, <volume>118</volume>, <fpage>274</fpage>–<lpage>279</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612463080">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Payne</surname><given-names>J. W.</given-names></name>
<name><surname>Bettman</surname><given-names>J. R.</given-names></name>
<name><surname>Johnson</surname><given-names>E. J.</given-names></name>
</person-group> (<year>1993</year>). <source>The adaptive decision maker</source>. <publisher-loc>Cambridge, England</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-0956797612463080">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pinheiro</surname><given-names>J. C.</given-names></name>
<name><surname>Bates</surname><given-names>D. M.</given-names></name>
</person-group> (<year>2000</year>). <source>Mixed-effects models in S and S-PLUS</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr21-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schultz</surname><given-names>W.</given-names></name>
<name><surname>Dayan</surname><given-names>P.</given-names></name>
<name><surname>Montague</surname><given-names>P. R.</given-names></name>
</person-group> (<year>1997</year>). <article-title>A neural substrate of prediction and reward</article-title>. <source>Science</source>, <volume>275</volume>, <fpage>1593</fpage>–<lpage>1599</lpage>.</citation>
</ref>
<ref id="bibr22-0956797612463080">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sutton</surname><given-names>R. S.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Integrated architecture for learning, planning, and reacting based on approximating dynamic programming</article-title>. In <person-group person-group-type="editor">
<name><surname>Morgan</surname><given-names>M. B.</given-names></name>
</person-group> (Ed.), <source>Proceedings of the Seventh International Conference (1990) on Machine Learning</source> (pp. <fpage>216</fpage>–<lpage>224</lpage>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>.</citation>
</ref>
<ref id="bibr23-0956797612463080">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sutton</surname><given-names>R. S.</given-names></name>
<name><surname>Barto</surname><given-names>A. G.</given-names></name>
</person-group> (<year>1998</year>). <source>Reinforcement learning</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr24-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Valentin</surname><given-names>V. V.</given-names></name>
<name><surname>Dickinson</surname><given-names>A.</given-names></name>
<name><surname>O’Doherty</surname><given-names>J. P.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Determining the neural substrates of goal-directed learning in the human brain</article-title>. <source>Journal of Neuroscience</source>, <volume>27</volume>, <fpage>4019</fpage>–<lpage>4026</lpage>.</citation>
</ref>
<ref id="bibr25-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Waldron</surname><given-names>E. M.</given-names></name>
<name><surname>Ashby</surname><given-names>F. G.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The effects of concurrent task interference on category learning: Evidence for multiple category learning systems</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>8</volume>, <fpage>168</fpage>–<lpage>176</lpage>.</citation>
</ref>
<ref id="bibr26-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yin</surname><given-names>H. H.</given-names></name>
<name><surname>Knowlton</surname><given-names>B. J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The role of the basal ganglia in habit formation</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>7</volume>, <fpage>464</fpage>–<lpage>476</lpage>.</citation>
</ref>
<ref id="bibr27-0956797612463080">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zeithamova</surname><given-names>D.</given-names></name>
<name><surname>Maddox</surname><given-names>W. T.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Dual-task interference in perceptual category learning</article-title>. <source>Memory &amp; Cognition</source>, <volume>34</volume>, <fpage>387</fpage>–<lpage>398</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>