<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="article-commentary" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">CTJ</journal-id>
<journal-id journal-id-type="hwp">spctj</journal-id>
<journal-id journal-id-type="nlm-ta">Clin Trials</journal-id>
<journal-title>Clinical Trials</journal-title>
<issn pub-type="ppub">1740-7745</issn>
<issn pub-type="epub">1740-7753</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1740774512470221</article-id>
<article-id pub-id-type="publisher-id">10.1177_1740774512470221</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Commentaries</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Commentary on ‘Small-sample behavior of novel phase I cancer trial designs’</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Cheung</surname><given-names>Ying Kuen</given-names></name>
</contrib>
<aff id="aff1-1740774512470221">Department of Biostatistics, Columbia University, NY, USA</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-1740774512470221">Ying Kuen Cheung, Department of Biostatistics, Columbia University, 722 West 168th Street, Room 641, New York, NY 10032, USA. Email: <email>yc632@columbia.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2013</year>
</pub-date>
<volume>10</volume>
<issue>1</issue>
<fpage>86</fpage>
<lpage>87</lpage>
<permissions>
<copyright-statement>© The Author(s), 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">The Society for Clinical Trials</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p>Oron and Hoff [<xref ref-type="bibr" rid="bibr1-1740774512470221">1</xref>] introduce the term ‘long memory’ to describe phase I dose-finding designs that recursively assign patients to a current maximum tolerated dose (MTD) estimate. Long memory designs represent the principal design strategy since the 1990s and include model-based and nonparametric methods. The authors conduct a series of simulation studies comparing two long memory designs (continual reassessment method (CRM) and cumulative cohorts design (CCD)) and a ‘short memory’ up-and-down design. The simulation studies are augmented by case studies of published Bayesian phase I trials. My comments will revolve around three observations I derive from the reading of the article:
<list id="list1-1740774512470221" list-type="order">
<list-item><p>Long memory designs such as the CRM tend to converge to a dose quickly early in a trial.</p></list-item>
<list-item><p>Planning for a model-based phase I design can be a complex task.</p></list-item>
<list-item><p>No single method is <italic>uniformly</italic> superior to other methods in terms of MTD selection.</p></list-item>
</list></p>
<sec id="section1-1740774512470221">
<title>The yardstick of success</title>
<p>The authors demonstrate by simulation that when the CRM settles on a dose, there is no guarantee that the converged dose is the MTD. This is factually true and is in line with known theoretical results [<xref ref-type="bibr" rid="bibr2-1740774512470221">2</xref>,<xref ref-type="bibr" rid="bibr3-1740774512470221">3</xref>]. But it remains debatable whether ‘mixing it up’ (short memory) is a better dose allocation strategy than ‘settling on a dose’ (long memory). The authors consider the number n* of cohorts treated at the MTD as a criterion and find that short memory designs look superior in terms of variability of n*, whereas long memory designs win in terms of average. As statisticians, we can also imagine some other summary of the distribution of n* as a yardstick of success, or simply compare the stochastic ordering of distributions. My main concern, however, is that counting the number of patients at the MTD <italic>only</italic> does not adequately reflect the practical and ethical considerations in a phase I trial. First, for a trial targeting p = 0.3, we will consider it a success if we treat majority of patients at a dose with toxicity probability 0.27 even though it may not be the MTD. Therefore, the number of cohorts treated at an <italic>acceptable</italic> dose with toxicity probability falling between 0.25 and 0.35 (say) may be a more reasonable metric than n*. Since the CRM can be calibrated to converge to any prespecified indifference interval [<xref ref-type="bibr" rid="bibr4-1740774512470221">4</xref>], it will eventually settle on an acceptable dose. Second, the ethical impetus for sequential dose assignments in phase I trials is to minimize the risk of treating patients at high and toxic doses (cf. the 3 + 3 design). Being ‘memoryless’ and using only data in the most current cohort, a short memory design will re-escalate to a dose with a nonnegligible probability no matter how toxic the dose is. This is perhaps the most worrisome property of a short memory design. It would be reassuring if the authors could examine also the distribution of the number of cohorts treated at a dose with toxicity probability 0.5 (say) or more.</p>
</sec>
<sec id="section2-1740774512470221">
<title>The importance of theory-guided calibration</title>
<p>The fundamental difficulties in planning a CRM trial is that the statistician needs to specify quite a few design parameters, namely, an initial design (for a two-stage CRM), the skeleton, and the prior distribution of the model parameters (for a Bayesian CRM), <italic>and</italic> to evaluate a design under a large parameter space (set of dose–toxicity scenarios). Even though statisticians are generally equipped with the skills to perform simulation and ample computing powers, obtaining a foolproof specification can be elusive without a <italic>systematic</italic> look at the method’s theoretical properties. For example, the authors point out that the two-stage CRM in Neuenschwander <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr5-1740774512470221">5</xref>] is incoherent. This problem is probably not a result of lack of simulation but that pointwise properties [<xref ref-type="bibr" rid="bibr6-1740774512470221">6</xref>] such as coherence require examining <italic>every</italic> possible sample path generated by a dose-finding design and hence cannot be verified by simulation that gives aggregate results. For another example, the CRM used in the simulation in section 4 has inferior performance in the last scenario in Table 1. A quick examination of the CRM specification reveals that it has a wide indifference interval (I got 0.30 ± 0.13 using the function ‘crmsens’ in R package ‘dfcrm’) that may prevent the method from escalating to the highest dose level. This indicates the complexity of specifying a CRM design due to the large number of design parameters when there is no guidance from theory. It is important that we can translate theory into tools that can be used to calibrate a design in a <italic>timely and reproducible</italic> manner. For two-stage CRM, incoherence can be avoided by choosing a compatible [<xref ref-type="bibr" rid="bibr7-1740774512470221">7</xref>] initial design based on coherence theory (the function ‘cohere’ implements the theory). Also we can choose the skeleton based on the theory of indifference intervals: without delving into simulation myself, I think the CRM’s performance in the simulation in section 4 can be improved by choosing a skeleton with a narrower indifference interval (e.g., 0.30 ± 0.08) by using the function ‘getprior’.</p>
</sec>
<sec id="section3-1740774512470221">
<title>The complexity in practice</title>
<p>No design is best, some are useful. As Table 1 in the article shows that no method wins under all scenarios in terms of MTD selection, it seems reasonable to choose a method on ground of simplicity, assuming it is properly calibrated and evaluated. The authors suggest short memory designs are simpler than long memory designs. This may be true from a statistician’s standpoint, as we realize that there are pitfalls when planning a model-based trial. However, complexity <italic>in planning</italic> should not be the determining factor of choosing a design – and there are software for design calibration and implementation of ‘complex’ methods, such as the CRM. Rather, the main consideration should be whether a method can accommodate the complexity <italic>in practice</italic> that arises in many forms, including protocol amendments, delayed outcomes, efficacy–toxicity trade-off, and so on; Thall [<xref ref-type="bibr" rid="bibr8-1740774512470221">8</xref>] describes an interesting case. The 3 + 3 method is appealing to clinicians because it is simple in planning (and also because of tradition). It is therefore important for statisticians to be able to articulate the advantages of these ‘novel’ phase I designs over the 3 + 3 design. Recent statistical principles, whether they are long memory or short memory, will likely show their advantages and usefulness in the complex situations. While it is in principle straightforward to extend long memory (model-based) designs to these situations, research on short memory designs will be worthwhile.</p>
<p>Finally, I would like to congratulate the authors on opening up a debate on the two design approaches. I hope the debate will continue in the context of complex dose-finding settings, and I anticipate that it will become even more interesting there.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.</p>
</fn>
<fn fn-type="conflict">
<label>Conflict of interest</label>
<p>None declared.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1740774512470221">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Oron</surname><given-names>AP</given-names></name>
<name><surname>Hoff</surname><given-names>PD</given-names></name>
</person-group>. <article-title>Small-sample behavior of novel phase I cancer trial designs</article-title>. <source>Clin Trials</source> <year>2012</year>.</citation>
</ref>
<ref id="bibr2-1740774512470221">
<label>2.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cheung</surname><given-names>YK</given-names></name>
<name><surname>Chappell</surname><given-names>R</given-names></name>
</person-group>. <article-title>A simple technique to evaluate model sensitivity in the continual reassessment method</article-title>. <source>Biometrics</source> <year>2002</year>; <bold>58</bold>: <fpage>671</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr3-1740774512470221">
<label>3.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Azriel</surname><given-names>D</given-names></name>
<name><surname>Mandel</surname><given-names>M</given-names></name>
<name><surname>Rinott</surname><given-names>Y</given-names></name>
</person-group>. <article-title>The treatment versus experimentation dilemma in dose finding studies</article-title>. <source>J Stat Plan Infer</source> <year>2011</year>; <volume>141</volume>: <fpage>2759</fpage>–<lpage>68</lpage>.</citation>
</ref>
<ref id="bibr4-1740774512470221">
<label>4.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>SM</given-names></name>
<name><surname>Cheung</surname><given-names>YK</given-names></name>
</person-group>. <article-title>Model calibration in the continual reassessment method</article-title>. <source>Clin Trials</source> <year>2009</year>; <volume>6</volume>: <fpage>227</fpage>–<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr5-1740774512470221">
<label>5.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Neuenschwander</surname><given-names>B</given-names></name>
<name><surname>Branson</surname><given-names>M</given-names></name>
<name><surname>Gsponer</surname><given-names>T</given-names></name>
</person-group>. <article-title>Critical aspects of the Bayesian approach to phase I cancer trials</article-title>. <source>Stat Med</source> <year>2008</year>; <volume>27</volume>: <fpage>2420</fpage>–<lpage>39</lpage>.</citation>
</ref>
<ref id="bibr6-1740774512470221">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cheung</surname><given-names>YK</given-names></name>
</person-group>. <article-title>Stochastic approximation and modern model-based designs for dose finding clinical trials</article-title>. <source>Stat Sci</source> <year>2010</year>; <volume>25</volume>: <fpage>191</fpage>–<lpage>201</lpage>.</citation>
</ref>
<ref id="bibr7-1740774512470221">
<label>7.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cheung</surname><given-names>YK</given-names></name>
</person-group>. <article-title>Coherence principles in dose finding studies</article-title>. <source>Biometrika</source> <year>2005</year>; <volume>92</volume>: <fpage>863</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr8-1740774512470221">
<label>8.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thall</surname><given-names>PF</given-names></name>
</person-group>. <article-title>Bayesian models and decision algorithms for complex early phase clinical trials</article-title>. <source>Stat Sci</source> <year>2010</year>; <volume>25</volume>: <fpage>227</fpage>–<lpage>44</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>