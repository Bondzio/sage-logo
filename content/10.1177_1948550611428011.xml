<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SPP</journal-id>
<journal-id journal-id-type="hwp">spspp</journal-id>
<journal-title>Social Psychological and Personality Science</journal-title>
<issn pub-type="ppub">1948-5506</issn>
<issn pub-type="epub">1948-5514</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1948550611428011</article-id>
<article-id pub-id-type="publisher-id">10.1177_1948550611428011</article-id>
<title-group>
<article-title>Emotion Appraisal Dimensions can be Inferred From Vocal Expressions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Laukka</surname>
<given-names>Petri</given-names>
</name>
<xref ref-type="aff" rid="aff1-1948550611428011">1</xref>
<xref ref-type="corresp" rid="corresp1-1948550611428011"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Elfenbein</surname>
<given-names>Hillary Anger</given-names>
</name>
<xref ref-type="aff" rid="aff2-1948550611428011">2</xref>
</contrib>
<bio>
<title>Bios</title>
<p>
<bold>Petri Laukka</bold> is an associate professor of psychology at Stockholm University. His main research interests revolve around emotion science (especially nonverbal communication of affect) and music psychology; and draw on methods from social and cognitive psychology as well as social neuroscience.</p>
<p>
<bold>Hillary Anger Elfenbein</bold> is a professor of organizational behavior at Washington University in St. Louis. She holds a PhD in Organizational Behavior, MA in Statistics, and BAs in Physics and Sanskrit from Harvard. Her research focuses on emotion and interpersonal perception.</p>
</bio>
</contrib-group>
<aff id="aff1-1948550611428011"><label>1</label>Department of Psychology, Stockholm University, Stockholm, Sweden</aff>
<aff id="aff2-1948550611428011"><label>2</label>Olin Business School, Washington University in St. Louis, St Louis, MO, USA</aff>
<author-notes>
<corresp id="corresp1-1948550611428011">Petri Laukka, Stockholm University, Department of Psychology, 10691 Stockholm, Sweden Email: <email>petri.laukka@psychology.su.se</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>9</month>
<year>2012</year>
</pub-date>
<volume>3</volume>
<issue>5</issue>
<fpage>529</fpage>
<lpage>536</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">Social and Personality Psychology Consortium</copyright-holder>
</permissions>
<abstract>
<p>Vocal expressions are thought to convey information about speakers' emotional states but may also reflect the antecedent cognitive appraisal processes that produced the emotions. We investigated the perception of emotion-eliciting situations on the basis of vocal expressions. Professional actors vocally portrayed different emotions by enacting emotion-eliciting situations. Judges then rated these expressions with respect to the emotion-eliciting situation described in terms of appraisal dimensions (i.e., novelty, intrinsic pleasantness, goal conduciveness, urgency, power, self- and other responsibility, and norm compatibility), achieving good agreement. The perceived appraisal profiles for the different emotions were generally in accord with predictions based on appraisal theory. The appraisal ratings also correlated with a variety of acoustic measures related to pitch, intensity, voice quality, and temporal characteristics. Results suggest that several aspects of emotion-eliciting situations can be inferred reliably and validly from vocal expressions which, thus, may carry information about the cognitive representation of events.</p>
</abstract>
<kwd-group>
<kwd>acoustic correlates</kwd>
<kwd>appraisal</kwd>
<kwd>emotion</kwd>
<kwd>vocal expression</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1948550611428011">
<title>Introduction</title>
<p>Emotional expressions serve important functions in social interaction. Notably, they allow individuals to communicate vital information to others, thereby facilitating the coordination of adaptive interactions among individuals. The many functions served by vocal expressions—that is, what is communicated nonverbally through the tone of voice apart from the basic meaning conveyed by the words—can be illustrated using Bühler’s (<xref ref-type="bibr" rid="bibr4-1948550611428011">1934/1990</xref>) Organon model (<xref ref-type="bibr" rid="bibr23-1948550611428011">Scherer, 1988</xref>). According to this model, vocal expressions function as (a) a <italic>symptom</italic> of the state of the speaker, and thereby express emotions, intentions, and attitudes; (b) a <italic>signal</italic> to the perceiver or to the observer, thereby serving as an appeal to produce a reaction; and (c) a <italic>symbol</italic> that represents an object or event.</p>
<p>The function of vocal expressions as a symptom has received the majority of research attention. Large-scale reviews covering decades of this work show that vocal expressions intended to convey specific emotional categories such as anger, fear, happiness, and sadness are generally recognized with accuracy above chance within and across cultures (<xref ref-type="bibr" rid="bibr7-1948550611428011">Elfenbein &amp; Ambady, 2002</xref>), and that these stimuli are associated with relatively distinct acoustic characteristics (<xref ref-type="bibr" rid="bibr13-1948550611428011">Juslin &amp; Laukka, 2003</xref>). More recently, work that measures acoustical properties in order to classify the emotional states of speakers has been gathering interest from researchers in the speech science and affective computing communities (e.g., <xref ref-type="bibr" rid="bibr26-1948550611428011">Schuller, Batliner, Steidl, &amp; Seppi, 2011</xref>). In contrast with the symptom aspect of vocal expression, the signaling aspect has received considerably less research attention and has been investigated primarily through the ability of emotional expressions to regulate social behavior by inducing affective responses in the perceiver (<xref ref-type="bibr" rid="bibr21-1948550611428011">Russell, Bachorowski, &amp; Fernandez-Dols, 2003</xref>). In this article, we focus on the third and least investigated of the possible functions for vocal expressions—namely, the symbolic function—and present a study exploring the extent to which vocal emotion expressions may carry information about the cognitive representation of events.</p>
<p>Although the syntax of verbal language is designed to represent a symbolic function, how might one conceive of a symbolic function for nonverbal expressions? A proposed answer is that vocal expressions may reflect the antecedent cognitive appraisal processes that produced the affective state in the speaker (<xref ref-type="bibr" rid="bibr11-1948550611428011">Johnstone, van Reekum, &amp; Scherer, 2001</xref>; <xref ref-type="bibr" rid="bibr22-1948550611428011">Scherer, 1986</xref>). According to the cognitive appraisal approach, emotions are elicited and differentiated by a person’s evaluations of the significance of events. These appraisals—which often proceed effortlessly and generate emotions automatically—are conducted in relation to the person’s needs, goals, and concerns along a small number of theoretically postulated dimensions. Although different theories postulate slightly different appraisal dimensions, most theories contain dimensions related to novelty, intrinsic pleasantness, goal conduciveness, urgency, power, responsibility, and compatibility with norms (e.g., <xref ref-type="bibr" rid="bibr8-1948550611428011">Ellsworth &amp; Scherer, 2003</xref>). Theorizing that vocal expressions do contain information about these antecedent evaluation processes, <xref ref-type="bibr" rid="bibr23-1948550611428011">Scherer (1988)</xref> further speculated that this “should allow the listener to reconstruct the major features of the emotion-producing event in its effect on the speaker” (p. 94). If listeners can infer the nature of the emotion-eliciting situation from vocal expressions, then this would provide support for a symbolic function of vocal expressions.</p>
<p>Very few previous studies have explored the perception of appraisal dimensions from expressive behavior and with mixed results. <xref ref-type="bibr" rid="bibr6-1948550611428011">Devillers et al. (2006)</xref> used audio-visual clips of emotional interviews recorded from TV shows as their stimulus materials and asked judges to rate a large set of scales describing appraisal dimensions. They found that the interrater agreement between judges was fairly low for most appraisal dimensions but reasonably high agreement was found for some appraisal dimensions (e.g., intrinsic pleasantness and goal conduciveness). Using a different approach, <xref ref-type="bibr" rid="bibr24-1948550611428011">Scherer and Grandjean (2008)</xref> asked the observers to judge photos of facial expressions on the basis of discrete emotion categories, social message types, appraisal results, and action tendencies. They found that emotion categories and appraisals were judged significantly more accurately and confidently than messages or action tendencies. We extend this existing work by presenting the first investigation of the extent to which appraisal dimensions can be perceived from vocal emotion displays.</p>
<p>In the present study, we first instructed professional actors to portray a wide range of different emotions vocally by enacting various emotion-eliciting situations. Judges then rated the resulting expressions with respect to appraisal dimensions, which represent our cognitive understanding of the characteristics of situations that elicit emotion. If emotion appraisal dimensions can be inferred from vocal expressions, then we expect that (a) judges will be able to conduct their appraisal ratings in a consistent fashion, as indicated by meaningful interrater agreement and (b) perceived appraisal profiles for the different emotions will be consistent with predictions based on appraisal theory (as specified, e.g., by <xref ref-type="bibr" rid="bibr8-1948550611428011">Ellsworth &amp; Scherer, 2003</xref>).</p>
<p>We also investigated the acoustical correlates of the judges' appraisal ratings. <xref ref-type="bibr" rid="bibr22-1948550611428011">Scherer (1986)</xref> made detailed predictions about the acoustic patterns associated with different cognitive appraisals, based on the idea that the outcome of each appraisal of the emotion-eliciting stimulus is an effect on physiological responding in ways that influence voice production. However, no previous studies have directly investigated the extent to which listeners' ratings of appraisal dimensions are associated with various acoustic measures.</p>
</sec>
<sec id="section2-1948550611428011" sec-type="methods">
<title>Methods</title>
<sec id="section3-1948550611428011">
<title>Stimulus Material</title>
<p>The present study required relatively clear vocal expressions of a large number of well-defined emotions in order to test predictions based on appraisal theory. We therefore used a selection of 300 stimuli from the VENEC corpus, which is a large cross-cultural database of vocal affect expressions that were recorded by professional actors (<xref ref-type="bibr" rid="bibr17-1948550611428011">Laukka et al., 2010</xref>). Twenty actors born and raised in the United States (10 women) were instructed to express vocally 15 emotions (amusement, anger, contempt, disgust, distress, fear, guilt, happiness, negative surprise, pride, positive surprise, relief, sadness, serenity, and shame) with a moderate level of emotion intensity. The actors were provided with scenarios adapted from current emotion research (e.g., <xref ref-type="bibr" rid="bibr8-1948550611428011">Ellsworth &amp; Scherer, 2003</xref>; <xref ref-type="bibr" rid="bibr18-1948550611428011">Lazarus, 1991</xref>; <xref ref-type="bibr" rid="bibr19-1948550611428011">Ortony, Clore, &amp; Collins, 1988</xref>) describing typical situations in which each emotion may be elicited and were instructed to try to enact finding themselves in these situations. They were told to try to remember similar situations that they had experienced personally and that had evoked the specified emotions, and if possible try to put themselves into the same emotional state of mind. The actors were further instructed to try to express the emotions as convincingly as possible but without using overtly stereotypical expressions. In order to maintain consistency across portrayals, in each case the verbal material consisted of short phrases with emotionally neutral content (e.g., Let me tell you something, That is exactly what happened).</p>
<p>The recordings were made in a room with dampened acoustics, and the actors' speech was recorded directly onto a computer with 44 kHz sampling frequency using a high-quality microphone (sE Electronics USB2200A, Shanghai, China). To enable a wide dynamic range while avoiding clipping, each actor first produced sample stimuli, and recording levels were optimized and held constant based on the loudest sample. This process ensured that no stimuli used in this study were clipped.</p>
</sec>
<sec id="section4-1948550611428011">
<title>Acoustic Analyses</title>
<p>The VENEC corpus has been analyzed with regard to a variety of acoustic voice cues using the <italic>Praat</italic> acoustic analysis software (<xref ref-type="bibr" rid="bibr3-1948550611428011">Boersma &amp; Weenink, 2008</xref>), and we refer to <xref ref-type="bibr" rid="bibr16-1948550611428011">Laukka, Neiberg, Forsell, Karlsson, and Elenius (2011)</xref> for a detailed description of the procedure. The results of the acoustic analyses were used to investigate the relationships between perceived appraisal dimensions and voice cues. The raw values for each cue and speaker were <italic>Z</italic> transformed to control for differences in baseline values between speakers (<xref ref-type="bibr" rid="bibr1-1948550611428011">Banse &amp; Scherer, 1996</xref>). For this experiment, we selected 24 voice cues to represent aspects of the voice which have previously been associated with emotion according to a large-scale review (<xref ref-type="bibr" rid="bibr13-1948550611428011">Juslin &amp; Laukka, 2003</xref>), namely fundamental frequency (F0), voice intensity, formants and voice quality, and temporal characteristics, as described briefly in <xref ref-type="table" rid="table1-1948550611428011">Table 1</xref>
. Although there are literally thousands of features that can be extracted to describe any given vocal utterance, recent studies in this area emphasize the importance of choosing a range of theoretically and empirically motivated nonredundant vocal features—rather than focusing on their sheer number (<xref ref-type="bibr" rid="bibr2-1948550611428011">Batliner et al., 2011</xref>; Sundberg, Patel, Björkner, &amp; Scherer, <xref ref-type="bibr" rid="bibr28-1948550611428011">2011</xref>). Using a limited number of vocal features is consistent with our goal of testing the theoretical proposition that vocal expressions of emotion can convey a signal value—rather than specifying an acoustic formula to determine the likely human appraisal of any given vocal utterance.</p>
<table-wrap id="table1-1948550611428011" position="float">
<label>Table 1.</label>
<caption>
<p>Correlations (Pearson r) Between Selected Acoustic Measures and Listeners' Mean Ratings of Novelty, Pleasantness, Goal Conduciveness, Urgency, Power, and Norm Compatibility</p>
</caption>
<graphic alternate-form-of="table1-1948550611428011" xlink:href="10.1177_1948550611428011-table1.tif"/>
<table>
<thead>
<tr>
<th>Voice Cue (Explanation)</th>
<th>Novelty</th>
<th>Pleasantness</th>
<th>Goal Conduciveness</th>
<th>Urgency</th>
<th>Power</th>
<th>Norm Compatibility</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Pitch cues</td>
</tr>
<tr>
<td> F0M (mean pitch)</td>
<td>0.44<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.01 ns</td>
<td>0.01 ns</td>
<td>0.39<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.18<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.02 ns</td>
</tr>
<tr>
<td> F0Q1 (pitch quantile 1)</td>
<td>0.27<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.04 ns</td>
<td>−0.06 ns</td>
<td>0.27<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.28<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.07 ns</td>
</tr>
<tr>
<td> F0Q5 (pitch quantile 5)</td>
<td>0.43<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.11 ns</td>
<td>0.09 ns</td>
<td>0.32<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.05 ns</td>
<td>0.07 ns</td>
</tr>
<tr>
<td> F0SD (pitch SD)</td>
<td>0.31<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.16<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>0.17<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>0.16<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>0.21<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.13<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
</tr>
<tr>
<td> F0fracrise (percentage of frames with F0 rise)</td>
<td>0.29<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.11 ns</td>
<td>0.12<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.23<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.04 ns</td>
<td>0.07 ns</td>
</tr>
<tr>
<td> F0fracstat (percentage of frames with stationary F0)</td>
<td>−0.50<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.07 ns</td>
<td>0.00 ns</td>
<td>−0.43<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.00 ns</td>
<td>0.07 ns</td>
</tr>
<tr>
<td> Jitter (cycle-to-cycle variations of F0)</td>
<td>−0.15<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.10 ns</td>
<td>−0.06 ns</td>
<td>−0.11 ns</td>
<td>−0.06 ns</td>
<td>−0.09 ns</td>
</tr>
<tr>
<td colspan="7">Intensity cues</td>
</tr>
<tr>
<td> IntM (mean intensity)</td>
<td>0.69<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.11 ns</td>
<td>0.11 ns</td>
<td>0.58<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.24<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.08 ns</td>
</tr>
<tr>
<td> IntQ1 (intensity quantile 1)</td>
<td>0.58<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.50<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.26<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.11 ns</td>
</tr>
<tr>
<td> IntQ5 (intensity quantile 5)</td>
<td>0.72<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.07 ns</td>
<td>0.08 ns</td>
<td>0.59<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.26<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.05 ns</td>
</tr>
<tr>
<td> IntSD (intensity SD)</td>
<td>0.06 ns</td>
<td>−0.15<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.00 ns</td>
<td>−0.07 ns</td>
<td>−0.12<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
</tr>
<tr>
<td> Intfracfall (percentage of frames with intensity fall)</td>
<td>0.37<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.12<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.08 ns</td>
<td>0.38<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.00</td>
<td>−0.10 ns</td>
</tr>
<tr>
<td> Shimmer (cycle-to-cycle variations of intensity)</td>
<td>−0.12 ns</td>
<td>0.04 ns</td>
<td>0.06 ns</td>
<td>−0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.08 ns</td>
<td>0.00</td>
</tr>
<tr>
<td colspan="7">Formant and voice quality cues</td>
</tr>
<tr>
<td> F1M (formant 1 mean)</td>
<td>0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.13<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.11 ns</td>
<td>0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.07 ns</td>
<td>−0.12<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
</tr>
<tr>
<td> F2M (formant 2 mean)</td>
<td>−0.06 ns</td>
<td>0.01 ns</td>
<td>0.04 ns</td>
<td>−0.05 ns</td>
<td>0.09 ns</td>
<td>0.02 ns</td>
</tr>
<tr>
<td> F3M (formant 3 mean)</td>
<td>−0.04 ns</td>
<td>−0.03 ns</td>
<td>−0.01 ns</td>
<td>−0.01 ns</td>
<td>0.03 ns</td>
<td>0.01 ns</td>
</tr>
<tr>
<td> F1B (formant 1 bandwidth)</td>
<td>−0.10 ns</td>
<td>−0.18<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.21<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.03 ns</td>
<td>−0.20<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.23<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
</tr>
<tr>
<td> F2B (formant 2 bandwidth)</td>
<td>−0.11 ns</td>
<td>−0.11 ns</td>
<td>−0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.08 ns</td>
<td>−0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.13<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
</tr>
<tr>
<td> F3B (formant 3 bandwidth)</td>
<td>0.06 ns</td>
<td>−0.11 ns</td>
<td>−0.10 ns</td>
<td>0.08 ns</td>
<td>−0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.10 ns</td>
</tr>
<tr>
<td> H1MH2 (difference in amplitude between the first and second harmonics)</td>
<td>−0.02 ns</td>
<td>−0.05 ns</td>
<td>−0.06 ns</td>
<td>0.00 ns</td>
<td>−0.24<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.07 ns</td>
</tr>
<tr>
<td> H1MA3 (difference in amplitude between F0 and F3)</td>
<td>−0.26<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.17<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.17<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.13<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.21<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
</tr>
<tr>
<td> HF500 (proportion of high vs. low frequency spectral energy, cutoff: 500 Hz)</td>
<td>0.34<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.13<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>0.23<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.31<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>0.15<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
</tr>
<tr>
<td colspan="7">Temporal cues</td>
</tr>
<tr>
<td> PercSilence (% silence in the speech signal)</td>
<td>−0.26<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.37<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.33<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.16<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.33<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">**</xref></sup></td>
<td>−0.34<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
</tr>
<tr>
<td> Duration (duration of the utterance)</td>
<td>−0.37<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.10 ns</td>
<td>−0.09 ns</td>
<td>−0.37<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">***</xref></sup></td>
<td>−0.14<sup><xref ref-type="table-fn" rid="table-fn2-1948550611428011">*</xref></sup></td>
<td>−0.04 ns</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1948550611428011">
<p>
<italic>N</italic> = 300.</p>
</fn>
<fn id="table-fn2-1948550611428011">
<p>
<italic><sup>*</sup>p</italic> &lt; .05. <sup>**</sup> <italic>p</italic> &lt; .01. <sup>***</sup> <italic>p</italic> &lt; .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The F0 of the voice represents the rate with which the vocal folds open and close across the glottis and is strongly related to subjective impressions of pitch. Voice intensity reflects the effort required to produce the speech and is subjectively heard as loudness. Voice quality, or timbre, is largely determined by the settings of the supralaryngeal vocal tract, the phonatory mechanisms of the larynx, and the vocal tract resonances called formants. Temporal characteristics include the speech rate, and the amount of pauses (silent periods) in the speech signal.</p>
</sec>
<sec id="section5-1948550611428011">
<title>Listener Judgments</title>
<sec id="section6-1948550611428011">
<title>Participants and procedure</title>
<p>Twelve judges born and raised in the United States (eight women, mean age = 26 years) rated each stimulus on continuous interval scales describing the emotion eliciting situation in terms of eight appraisal dimensions. Sessions were conducted individually, one appraisal dimension at a time, each requiring approximately 1 hr. The order of sessions was randomized across judges, and stimulus order was randomized within sessions, to control for possible order effects. Eight judges rated all dimensions, and up to four additional judges rated the dimensions with lower average reliability. Sessions were spaced across days to limit fatigue. MediaLab software presented stimuli and recorded judgments. Participants listened to stimuli through headphones with constant sound levels.</p>
<p>Judges received the following instructions: “You will hear a number of phrases, spoken by persons whose goal was to enact finding themselves in various emotion-eliciting situations. We want you to answer the following question about the emotion-eliciting event: NOVELTY—How suddenly and abruptly did the event occur? (<italic>Not at all sudden/abrupt</italic> = 1, <italic>Very sudden/abrupt</italic> = 5); INTRINSIC PLEASANTNESS—How pleasant is the event? (<italic>Very unpleasant</italic> = 1, <italic>Very pleasant</italic> = 5); GOAL CONDUCIVENESS—Did the event help the person to reach a goal or satisfy a need? (<italic>Prevented a goal/need</italic> = 1, <italic>Helped a goal/need</italic> = 5); URGENCY—Did the person need to respond to the event urgently (<italic>Not at all urgent</italic> = 1, <italic>Very urgent</italic> = 5); POWER—Could the outcome of the situation be modified by appropriate human action? (<italic>Expresser powerless</italic> = 1, <italic>Expresser powerful</italic> = 5): SELF-RESPONSIBILITY—Was the speaker responsible for the event? (<italic>No personal responsibility</italic> = 1, <italic>Great deal of personal responsibility</italic> = 5); OTHER-RESPONSIBILITY—Was another person responsible for the event? (<italic>No responsibility by others</italic> = 1, <italic>Great deal of responsibility by others</italic> = 5); NORM COMPATIBILITY—Do you think that the event was compatible with the speakers norms? (<italic>Violated own norms</italic> = 1, <italic>Very consistent with own norms</italic> = 5).”</p>
<p>The raw rating values for each appraisal dimension and rater were <italic>Z</italic> transformed before being entered into the statistical analyses, to control for differences in baseline values between different raters and rating scales.</p>
</sec>
</sec>
<sec id="section8-1948550611428011">
<title>Appraisal predictions</title>
<p>The literature on emotion appraisal (e.g., <xref ref-type="bibr" rid="bibr8-1948550611428011">Ellsworth &amp; Scherer, 2003</xref>; <xref ref-type="bibr" rid="bibr25-1948550611428011">Scherer, Schorr, &amp; Johnstone, 2001</xref>) yielded the following predictions about the expected outcome of the listeners' appraisal ratings. It was not possible, based on the literature, to make predictions for all appraisal dimensions and all emotions.<list list-type="alpha-lower">
<list-item>
<p>Novelty: high = anger, fear, happiness, negative surprise, positive surprise; low = sadness, serenity.</p>
</list-item>
<list-item>
<p>Pleasantness: high = amusement, happiness, pride, positive surprise, relief, serenity; low = anger, contempt, disgust, distress, fear, guilt, negative surprise, sadness, shame.</p>
</list-item>
<list-item>
<p>Goal conduciveness: high = happiness, pride, relief; low = anger, distress, fear, guilt, negative surprise, sadness, shame.</p>
</list-item>
<list-item>
<p>Urgency: high = anger, distress, fear; low = amusement, happiness, relief, sadness, serenity.</p>
</list-item>
<list-item>
<p>Power: high = anger, contempt, happiness, pride; low = distress, fear, guilt, sadness, shame.</p>
</list-item>
<list-item>
<p>Self-responsibility: high = guilt, shame, pride; low = anger, contempt, distress, fear, sadness, negative surprise, positive surprise.</p>
</list-item>
<list-item>
<p>Other responsibility: high = anger, contempt; low = guilt, shame, pride.</p>
</list-item>
<list-item>
<p>Norm compatibility: high = amusement, happiness, pride; low = anger, contempt, guilt, shame.</p>
</list-item>
</list>
</p>
</sec>
</sec>
<sec id="section9-1948550611428011">
<title>Results</title>
<sec id="section10-1948550611428011">
<title>Rater Consistency</title>
<p>First, we examined the extent to which the listeners were able to rate the stimuli on appraisal scales in a consistent fashion. Following <xref ref-type="bibr" rid="bibr20-1948550611428011">Rosenthal and Rosnow (1991)</xref>, we calculated the average correlation reliability coefficient for each appraisal scale (<inline-formula id="inline-formula1-1948550611428011">
<mml:math id="mml-inline1-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>) and used the Spearman-Brown formula to calculate total reliability (<italic>R</italic>). Total reliability was good for all scales except for responsibility: novelty (<inline-formula id="inline-formula2-1948550611428011">
<mml:math id="mml-inline2-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>= .46, <italic>R </italic>= .87, <italic>N </italic>= 8), pleasantness (<inline-formula id="inline-formula3-1948550611428011">
<mml:math id="mml-inline3-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula> = .50, <italic>R </italic>= .87, <italic>N </italic>= 8), goal conduciveness (<inline-formula id="inline-formula4-1948550611428011">
<mml:math id="mml-inline4-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>= .43, <italic>R </italic>= .85, <italic>N </italic>= 8), urgency (<inline-formula id="inline-formula5-1948550611428011">
<mml:math id="mml-inline5-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>= .39, <italic>R </italic>= .83, <italic>N </italic>= 8), power (<inline-formula id="inline-formula6-1948550611428011">
<mml:math id="mml-inline6-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>= .29, <italic>R </italic>= .81, <italic>N </italic>= 11), self-responsibility (<inline-formula id="inline-formula7-1948550611428011">
<mml:math id="mml-inline7-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>= .12, <italic>R </italic>= .62, <italic>N </italic>= 12), other responsibility (<inline-formula id="inline-formula8-1948550611428011">
<mml:math id="mml-inline8-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>= .11, <italic>R </italic>= .52, <italic>N </italic>= 9), and norm compatibility (<inline-formula id="inline-formula9-1948550611428011">
<mml:math id="mml-inline9-1948550611428011">
<mml:mover accent="true">
<mml:mi>r</mml:mi>
<mml:mo>¯</mml:mo>
</mml:mover> 
</mml:math>
</inline-formula>= .39, <italic>R </italic>= .83, <italic>N </italic>= 8). Because reliability for self- and other responsibility was below the criteria for what is considered acceptable, these dimensions are not included in the analyses below.</p>
</sec>
<sec id="section11-1948550611428011">
<title>Listeners' Perceptions of Emotion Appraisal Dimensions</title>
<p>The listeners' appraisal ratings varied systematically as a function of intended emotion, as evidenced by significant effects of intended emotion in repeated measures analyses of variance (ANOVAs) using intended emotion as a within-subject factor with 15 levels: amusement, anger, contempt, disgust, distress, fear, guilt, happiness, negative surprise, pride, positive surprise, relief, sadness, serenity, and shame. The ANOVAs used the stimulus as the unit-of-analysis (averaging across the listeners' mean ratings), and compared values across the 15 emotions, with separate ANOVA models for each appraisal dimension: novelty, <italic>F</italic>(14, 266) = 19.45, MSE = .26, partial η<sup>2</sup> = .51; pleasantness, <italic>F</italic>(14, 266) = 24.52, MSE = .24, partial η<sup>2</sup> = .56; goal conduciveness, <italic>F</italic>(14, 266) = 20.08, MSE = .25, partial η<sup>2</sup> = .51; urgency, <italic>F</italic>(14, 266) = 18.11, MSE = .24, partial η<sup>2</sup> = .49; power, <italic>F</italic>(14, 266) = 20.45, MSE = .17, partial η<sup>2</sup> = .52; and norm compatibility, <italic>F</italic>(14, 266) = 20.72, MSE = .22, partial η<sup>2</sup> = .52 (all <italic>p</italic>s &lt; .001, Huynh-Feldt corrected). It was possible to collapse these data across the two different verbal sentences used, given that we did not find any significant Emotion × Sentence Type interactions when entering this factor into the above ANOVAs.</p>
<p>The listeners' mean appraisal ratings as a function of intended emotion are shown in <xref ref-type="fig" rid="fig1-1948550611428011">Figure 1</xref>
, together with the 95% confidence intervals for the mean appraisal rating for each scale and for each emotion. For example, anger expressions received high ratings (mean rating &gt; 0) on novelty, urgency, and power, and low ratings (mean rating &lt; 0) on pleasantness, goal conduciveness, and norm compatibility. Sadness received low ratings on novelty, pleasantness, goal conduciveness, power, and norm compatibility (mean rating &lt; 0), whereas the mean rating on the urgency scale was neither high nor low (i.e., the mean rating did not differ significantly from 0, as evidenced by the 95% confidence intervals). As a final example, amusement and happiness received high ratings on pleasantness, goal conduciveness, power, and norm compatibility (mean ratings &gt; 0), and amusement further received low ratings on the urgency scale (the mean ratings for the remaining scales were not significantly different from 0).</p>
<fig id="fig1-1948550611428011" position="float">
<label>Figure 1.</label>
<caption>
<p>Listeners' mean appraisal ratings (<italic>z</italic> scores) as a function of intended emotion. Bars indicate 95% confidence intervals. *Result in accordance with theoretical predictions; ‡Result in predicted direction but not statistically significant; ¬Result goes against predictions. Note that we did not have predictions for all emotions/appraisals. Am = amusement, An = anger, Co = contempt, Dg = disgust, Dt = distress, Fe = fear, Gu = guilt, Ha = happiness, Ns = negative surprise, Pr = pride, Ps = positive surprise, Re = relief, Sa = sadness, Se = serenity, and Sh = shame.</p>
</caption>
<graphic xlink:href="10.1177_1948550611428011-fig1.tif"/>
</fig>
<p>The predicted appraisal profiles for each emotion are also shown in <xref ref-type="fig" rid="fig1-1948550611428011">Figure 1</xref>. We compared the match between the results and predictions, and found that 44 out of 56 predictions were in accordance with the results (i.e., the observed mean rating was in the predicted direction and statistically different from 0, as evidenced by the 95% confidence intervals of the mean). When including also instances where the mean ratings were in the predicted direction, but not significantly different from 0, then 52 out of 56 predictions were consistent with the results. Only four instances were in the opposite direction of the predictions, none of which were statistically significant. Thus, we conclude that the listeners were able to infer aspects of emotion-eliciting situations described in terms of appraisals, and that the perceived appraisal profiles for the different emotions were generally in accord with appraisal theory-based predictions.</p>
</sec>
<sec id="section12-1948550611428011">
<title>Acoustic Correlates of Appraisal Dimensions</title>
<p>The listeners' mean appraisal ratings for each stimulus were significantly correlated (Pearson <italic>r</italic>) with several acoustic voice cues for each appraisal scale (see <xref ref-type="table" rid="table1-1948550611428011">Table 1</xref>). Most correlations were small to medium in terms of their magnitude but, even so, they give indications of which cues the listeners utilized in order to make their inferences about the emotion-eliciting situations.</p>
<p>For example, regarding pitch cues, high ratings of novelty and urgency were associated with high mean pitch (F0M), high minimum and maximum pitch (F0Q1 and F0Q5), large pitch variability (F0SD), a large proportion of frames with pitch rise (F0fracrise), and a small proportion of frames with stationary pitch (F0fracstat). Regarding intensity cues, both novelty and urgency were associated with high mean intensity (IntM), high minimum intensity (IntQ1), high maximum intensity (IntQ5), and a large proportion of frames with intensity fall (Intfracfall). Further, regarding voice quality cues, novelty and urgency ratings were associated with small values of H1MA3 (i.e., the difference in amplitude between F0 and F3), which is a measure of spectral tilt at the higher formant frequencies. Because H1MA3 is expected to be large for breathy voices and small for creaky voices (e.g., <xref ref-type="bibr" rid="bibr10-1948550611428011">Hanson, 1997</xref>), this correlation suggests an association between perceived novelty/urgency and a more creaky than breathy voice quality. Novelty and urgency were further associated with high values of HF500, which is a measure of the proportion of high versus low frequency spectral energy (<xref ref-type="bibr" rid="bibr12-1948550611428011">Juslin &amp; Laukka, 2001</xref>). As the proportion of high-frequency energy increases, the voice sounds sharper and less soft—thus, novelty and urgency ratings were associated with a more sharp than soft voice quality. Finally, regarding temporal cues, high ratings on both novelty and urgency were associated with a small percentage of silence (PercSilence) and a fast speech rate (duration; note that the correlation is negative because a short duration corresponds to a fast speech rate).</p>
<p>Listener ratings of novelty and urgency showed the largest correlations with the acoustic measures and were mainly associated with pitch and intensity cues. In contrast, pleasantness, goal conduciveness, and norm compatibility ratings generally showed smaller correlations with the voice cues, and the temporal and voice quality cues seemed to play a relatively larger role in predicting these appraisal dimensions. For example, the strongest correlations between pleasantness ratings and voice cues were observed for PercSilence (<italic>r</italic> = −.37), followed by the bandwidth of the first formant (F1B; <italic>r</italic> = −.18) and H1MA3 (<italic>r</italic> = −.17). Power ratings, finally, were approximately equally correlated with all types of voice cues (i.e., pitch, intensity, formant and voice quality, and temporal cues).</p>
</sec>
</sec>
<sec id="section13-1948550611428011">
<title>Discussion</title>
<p>Although Bühler’s (<xref ref-type="bibr" rid="bibr4-1948550611428011">1934/1990</xref>) Organon model (<xref ref-type="bibr" rid="bibr23-1948550611428011">Scherer, 1988</xref>) postulated three different functions of vocal expressions—a <italic>symptom</italic>, a <italic>signal</italic>, and a <italic>symbol</italic>—research to date has essentially ignored the symbolic function of vocal expressions. Our study was novel for exploring the extent to which vocal emotion expressions may carry information about the cognitive representation of events.</p>
<p>Using a wide range of emotional states conveyed by professional actors, we assessed the perceived emotional content of vocal expressions in terms of the dimensions postulated within theories of emotional appraisal (e.g., <xref ref-type="bibr" rid="bibr8-1948550611428011">Ellsworth &amp; Scherer, 2003</xref>). The results showed for the first time that listeners could reliably infer several aspects of emotion-eliciting situations from vocal expressions of affect, as described in terms of appraisal dimensions—that is, novelty, intrinsic pleasantness, goal conduciveness, urgency, power, and norm compatibility. Moreover, these perceived appraisal profiles for the various vocal expressions were generally in accord with predictions based on appraisal theory. Finally, we showed for the first time that judges' ratings along each appraisal dimension were significantly correlated with several acoustic cues. The implications of these findings are discussed below.</p>
<p>Regarding the first finding, we argue that these results are theoretically worthwhile by suggesting that vocal expressions may carry many different kinds of information—including rudimentary cognitive representational information about the nature of the situation that elicited the emotional response in the speaker (see <xref ref-type="bibr" rid="bibr23-1948550611428011">Scherer, 1988</xref>). Such a symbolic function of vocal expressions (Bühler, <xref ref-type="bibr" rid="bibr4-1948550611428011">1934/1990</xref>) has hitherto received little research attention but may be important for mapping out the multifarious ways that vocal expression is involved in the regulation of social interaction.</p>
<p>Further, the finding that listeners could reliably rate vocal expressions along several appraisal dimensions has bearing on the current debate about “what kinds of representation that a perceptual system concerned with emotional life might use to specify what it sees and hears” (<xref ref-type="bibr" rid="bibr5-1948550611428011">Cowie, 2009</xref>, p. 3520), by suggesting that people may infer the emotions of another person at least partly in terms of appraisal dimensions. According to appraisal theory, a relatively small set of appraisal dimensions lie at the root of all conceivable emotional responses (<xref ref-type="bibr" rid="bibr8-1948550611428011">Ellsworth &amp; Scherer, 2003</xref>) and, correspondingly, the perception of a relatively small number of appraisal dimensions from vocal expressions would render the recognition and differentiation of a wide range of subtle affective states possible. Along these lines, several studies have shown that listeners are able to perceive more fine-grained nuances from vocal expressions than would be expected if they were only able to perceive either prototypical basic emotions or arousal and valence (<xref ref-type="bibr" rid="bibr1-1948550611428011">Banse &amp; Scherer, 1996</xref>; <xref ref-type="bibr" rid="bibr12-1948550611428011">Juslin &amp; Laukka, 2001</xref>; <xref ref-type="bibr" rid="bibr27-1948550611428011">Simon-Thomas, Keltner, Sauter, Sinicropi-Yao, &amp; Abramson, 2009</xref>). Also, seen from a methodological perspective, letting judges rate emotional expressions on scales describing appraisal dimensions presents a novel way of studying social cognition that is not tied to particular emotion categories, and that allows for the assessment of nuanced and multifaceted emotional concepts from nonverbal signals. Such a method could prove particularly valuable in assessing emotion recognition from naturalistic stimuli, which often convey rather complex affective states (e.g., <xref ref-type="bibr" rid="bibr5-1948550611428011">Cowie, 2009</xref>).</p>
<p>The finding that the perceived appraisal profiles for the different vocal expressions matched well our predictions based on appraisal theory implies that the listeners understood the concept of appraisal and were able to use the rating scales as intended. It should be noted that the test was not intended to investigate how well different emotions can be differentiated based on their respective appraisal patterns but, rather, our focus was in examining how consistently the listeners were able to rate appraisals from vocal expressions. Indeed both the predicted and the perceived appraisal patterns for several emotions were very similar (e.g., amusement, happiness, and pride; distress and fear; guilt and shame), but the results also give additional support to the efforts of researchers working within appraisal theory to link emotions with specific appraisal patterns (<xref ref-type="bibr" rid="bibr8-1948550611428011">Ellsworth &amp; Scherer, 2003</xref>; <xref ref-type="bibr" rid="bibr25-1948550611428011">Scherer, et al., 2001</xref>).</p>
<p>Finally, our findings on the vocal correlates of appraisal dimensions give the first indications of what voice cues people use when judging appraisal dimensions from vocal expressions. We found that ratings on appraisal dimensions that are related to the level of arousal (i.e., novelty and urgency) received the largest correlations with voice cues, whereas smaller correlations were observed for appraisal dimensions that are more related to the valence dimension (i.e., intrinsic pleasantness, goal conduciveness, and norm compatibility). Dimensions related to potency (i.e., power) fell in the middle in terms of effect sizes. This is in accord with previous research, showing that arousal is more strongly reflected in the voice than valence (<xref ref-type="bibr" rid="bibr15-1948550611428011">Laukka, Juslin, &amp; Bresin, 2005</xref>).</p>
<sec id="section14-1948550611428011">
<title>Limitations and Future Research</title>
<p>Several caveats should be noted when interpreting these findings. First, we utilized posed expressions—due to the need for stimuli that represented well-defined emotions in a test of appraisal-theory-based predictions, as well as the need to include 15 different emotional states from each expresser. Thus, naturalistic expressions were not desirable or feasible for this work.<sup><xref ref-type="fn" rid="fn1-1948550611428011">
1
</xref></sup> Research suggests that acted expressions may be more prototypical—and, thus, easier to recognize—than spontaneous expressions (e.g., <xref ref-type="bibr" rid="bibr14-1948550611428011">Laukka, Audibert, &amp; Aubergé, in press</xref>), in part because everyday cues may not be sufficiently intense and also because people strategically pose and suppress their expressions in everyday interactions. Replication of the present results using more naturalistic and less prototypical stimuli would be worthwhile. Second, more research is needed to determine whether judges infer the dimensions of emotional appraisal directly—versus whether they first attribute an emotion to the stimulus, and then judge appraisals on the basis of the chosen emotion. Currently, both interpretations of the present results are feasible.<sup><xref ref-type="fn" rid="fn2-1948550611428011">
2
</xref></sup>
</p>
<p>More research is also needed to expand the approach of examining appraisal dimensions, their expression through the voice, and the ability of listeners to infer these dimensions. We note, in particular, that we examined each dimension in isolation, but combinations of appraisal dimensions may be best suited for assessing the perceived emotional content of vocal expressions. Further, although we tried to be inclusive of the major dimensions of emotional appraisal presented in theoretical models, we had to leave out several likely candidate dimensions from our study—for example, predictability; see <xref ref-type="bibr" rid="bibr9-1948550611428011">Fontaine, Scherer, Roesch, and Ellsworth (2007)</xref>—due to time constraints, and these could be included in subsequent attempts. Future research should also examine the similarities and differences between the acoustical correlates of emotion appraisal dimensions and other dimensional representations of emotion (e.g., activation, valence, and potency). Ideally, future work such as this will make use of a more complete set of acoustical cues that represent features of the voice than the set of 24 features used in the present study. Finally, we encourage research that takes completely new perspectives to examining the long-theorized symbolic function of vocal expressions.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors wish to thank Wanda Chui and Daniel Neiberg for their assistance with the data collection and the acoustic analyses, respectively.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn3-1948550611428011">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn4-1948550611428011">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by grants from the Swedish Research Council (grant no. 2006-1360) to Petri Laukka and the National Science Foundation (NSF-BCS-0617624) to Hillary Anger Elfenbein.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-1948550611428011">
<label>1.</label>
<p>We thank an anonymous reviewer for emphasizing the value of studying vocal expression using naturalistic stimuli in future work.</p>
</fn>
<fn fn-type="other" id="fn2-1948550611428011">
<label>2.</label>
<p>We thank an anonymous reviewer for suggesting this possible interpretation of our findings.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Banse</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Acoustic profiles in vocal emotion expression</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>70</volume>, <fpage>614</fpage>–<lpage>636</lpage>.</citation>
</ref>
<ref id="bibr2-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Batliner</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Steidl</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Schuller</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Seppi</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Vogt</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Wagner</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>… Amir</surname>
<given-names>N.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Whodunnit—Searching for the most important feature types signaling emotion-related user states in speech</article-title>. <source>Computer Speech and Language</source>, <volume>25</volume>, <fpage>4</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr3-1948550611428011">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Boersma</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Weenink</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Praat: Doing phonetics by computer [Computer program]</article-title>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.praat.org/">http://www.praat.org/</ext-link>
</citation>
</ref>
<ref id="bibr4-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bühler</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>1990</year>). <source>Theory of language. The representational function of language</source>. (<person-group person-group-type="editor">
<name>
<surname>Goodwin</surname>
<given-names>D. F.</given-names>
</name>
</person-group> Trans.). <publisher-loc>Amsterdam, The Netherlands</publisher-loc>: <publisher-name>John Benjamins</publisher-name>. <comment>(Original work published 1934)</comment>.
</citation>
</ref>
<ref id="bibr5-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cowie</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Perceiving emotion: Towards a realistic understanding of the task</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>364</volume>, <fpage>3515</fpage>–<lpage>3525</lpage>.</citation>
</ref>
<ref id="bibr6-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Devillers</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Cowie</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Martin</surname>
<given-names>J-C.</given-names>
</name>
<name>
<surname>Douglas-Cowie</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Abrilian</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>McRorie</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Real life emotions in French and English TV video clips: An integrated annotation protocol combining continuous and discrete approaches</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Calzolari</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Choukri</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Gangemi</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Maegaard</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Mariani</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Odijk</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Tapias</surname>
<given-names>D.</given-names>
</name>
</person-group> (Eds.), <source>Proceedings of the Fifth International Conference on Language Resources and Evaluation</source> (pp. <fpage>1105</fpage>–<lpage>1110</lpage>). <publisher-loc>Paris, France</publisher-loc>: <publisher-name>European Language Resources Association</publisher-name>
</citation>
</ref>
<ref id="bibr7-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Elfenbein</surname>
<given-names>H. A.</given-names>
</name>
<name>
<surname>Ambady</surname>
<given-names>N.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>On the universality and cultural specificity of emotion recognition: A meta-analysis</article-title>. <source>Psychological Bulletin</source>, <volume>128</volume>, <fpage>203</fpage>–<lpage>235</lpage>.</citation>
</ref>
<ref id="bibr8-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ellsworth</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Appraisal processes in emotion</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Davidson</surname>
<given-names>R. J.</given-names>
</name>
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
<name>
<surname>Goldsmith</surname>
<given-names>H. H.</given-names>
</name>
</person-group> (Eds.), <source>Handbook of affective sciences</source> (pp. <fpage>572</fpage>–<lpage>595</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr9-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fontaine</surname>
<given-names>J. R. J.</given-names>
</name>
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
<name>
<surname>Roesch</surname>
<given-names>E. B.</given-names>
</name>
<name>
<surname>Ellsworth</surname>
<given-names>P. C.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>The world of emotion is not two-dimensional</article-title>. <source>Psychological Science</source>, <volume>18</volume>, <fpage>1050</fpage>–<lpage>1057</lpage>.</citation>
</ref>
<ref id="bibr10-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hanson</surname>
<given-names>H. M.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Glottal characteristics of female speakers: Acoustic correlates</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>101</volume>, <fpage>466</fpage>–<lpage>481</lpage>.</citation>
</ref>
<ref id="bibr11-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Johnstone</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>van Reekum</surname>
<given-names>C. M.</given-names>
</name>
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Vocal correlates of appraisal processes</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
<name>
<surname>Schorr</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Johnstone</surname>
<given-names>T.</given-names>
</name>
</person-group> (Eds.), <source>Appraisal processes in emotion: Theory, methods, research</source> (pp. <fpage>271</fpage>–<lpage>284</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr12-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Juslin</surname>
<given-names>P. N.</given-names>
</name>
<name>
<surname>Laukka</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion</article-title>. <source>Emotion</source>, <volume>1</volume>, <fpage>381</fpage>–<lpage>412</lpage>.</citation>
</ref>
<ref id="bibr13-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Juslin</surname>
<given-names>P. N.</given-names>
</name>
<name>
<surname>Laukka</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Communication of emotions in vocal expression and music performance: Different channels, same code?</article-title>. <source>Psychological Bulletin</source>, <volume>129</volume>, <fpage>770</fpage>–<lpage>814</lpage>.</citation>
</ref>
<ref id="bibr14-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Laukka</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Audibert</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Aubergé</surname>
<given-names>V</given-names>
</name>
</person-group> (<year>in press</year>). <article-title>Exploring the determinants of the graded structure of vocal emotion expressions</article-title>. <source>Cognition and Emotion</source>, <comment>doi:10.1080/02699931.2011.602047</comment>
</citation>
</ref>
<ref id="bibr15-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Laukka</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Juslin</surname>
<given-names>P.N.</given-names>
</name>
<name>
<surname>Bresin</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>A dimensional approach to vocal expression of emotion</article-title>. <source>Cognition and Emotion</source>, <volume>19</volume>, <fpage>633</fpage>–<lpage>653</lpage>.</citation>
</ref>
<ref id="bibr16-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Laukka</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Neiberg</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Forsell</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Karlsson</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Elenius</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Expression of affect in spontaneous speech: Acoustic correlates, perception, and automatic detection of irritation and resignation</article-title>. <source>Computer Speech and Language</source>, <volume>25</volume>, <fpage>84</fpage>–<lpage>104</lpage>.</citation>
</ref>
<ref id="bibr17-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Laukka</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Elfenbein</surname>
<given-names>H. A.</given-names>
</name>
<name>
<surname>Chui</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Thingujam</surname>
<given-names>N. S.</given-names>
</name>
<name>
<surname>Iraki</surname>
<given-names>F. K.</given-names>
</name>
<name>
<surname>Rockstuhl</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Althoff</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Presenting the VENEC corpus: Development of a cross-cultural corpus of vocal emotion expressions and a novel method of annotating emotion appraisals</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Devillers</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Schuller</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Cowie</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Douglas-Cowie</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Batliner</surname>
<given-names>A.</given-names>
</name>
</person-group> (Eds.), <source>Proceedings of the LREC 2010 Workshop on Corpora for Research on Emotion and Affect</source> (pp. <fpage>53</fpage>–<lpage>57</lpage>). <publisher-loc>Paris, France</publisher-loc>: <publisher-name>European Language Resources Association</publisher-name>.</citation>
</ref>
<ref id="bibr18-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Lazarus</surname>
<given-names>R. S.</given-names>
</name>
</person-group> (<year>1991</year>). <source>Emotion and adaptation</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr19-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ortony</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Clore</surname>
<given-names>G. L.</given-names>
</name>
<name>
<surname>Collins</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>1988</year>). <source>The cognitive structure of emotions</source>. <publisher-loc>Cambridge, England</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Rosenthal</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Rosnow</surname>
<given-names>R. L.</given-names>
</name>
</person-group> (<year>1991</year>). <source>Essentials of behavioral research: Methods and data analysis</source>. <edition>(2nd Ed.). </edition>
<publisher-loc>New York, NY</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr21-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Russell</surname>
<given-names>J. A.</given-names>
</name>
<name>
<surname>Bachorowski</surname>
<given-names>J-A.</given-names>
</name>
<name>
<surname>Fernandez-Dols</surname>
<given-names>J-M.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Facial and vocal expressions of emotion</article-title>. <source>Annual Review of Psychology</source>, <volume>54</volume>, <fpage>329</fpage>–<lpage>349</lpage>.</citation>
</ref>
<ref id="bibr22-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
</person-group> (<year>1986</year>). <article-title>Vocal affect expression: A review and a model for future research</article-title>. <source>Psychological Bulletin</source>, <volume>99</volume>, <fpage>143</fpage>–<lpage>165</lpage>.</citation>
</ref>
<ref id="bibr23-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
</person-group> (<year>1988</year>). <article-title>On the symbolic functions of vocal affect expression</article-title>. <source>Journal of Language and Social Psychology</source>, <volume>7</volume>, <fpage>79</fpage>–<lpage>100</lpage>.</citation>
</ref>
<ref id="bibr24-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
<name>
<surname>Grandjean</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Facial expressions allow inference of both emotions and their components</article-title>. <source>Cognition and Emotion</source>, <volume>22</volume>, <fpage>789</fpage>–<lpage>801</lpage>.</citation>
</ref>
<ref id="bibr25-1948550611428011">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
<name>
<surname>Schorr</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Johnstone</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>2001</year>). <source>Appraisal processes in emotion: Theory, methods, research</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr26-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schuller</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Batliner</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Steidl</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Seppi</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Recognizing realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge</article-title>. <source>Speech Communication</source>, <volume>53</volume>, <fpage>1062</fpage>–<lpage>1087</lpage>.</citation>
</ref>
<ref id="bibr27-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Simon-Thomas</surname>
<given-names>E. R.</given-names>
</name>
<name>
<surname>Keltner</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Sauter</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Sinicropi-Yao</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Abramson</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>The voice conveys specific emotions: Evidence from vocal burst displays</article-title>. <source>Emotion</source>, <volume>9</volume>, <fpage>838</fpage>–<lpage>846</lpage>.</citation>
</ref>
<ref id="bibr28-1948550611428011">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sundberg</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Patel</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Björkner</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Scherer</surname>
<given-names>K. R.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Interdependencies among voice source parameters in emotional speech</article-title>. <source>IEEE Transactions on Affective Computing</source>, <volume>2</volume>, <fpage>162</fpage>–<lpage>174</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>