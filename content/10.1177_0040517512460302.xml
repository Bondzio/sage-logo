<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="EN">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TRJ</journal-id>
<journal-id journal-id-type="hwp">sptrj</journal-id>
<journal-title>Textile Research Journal</journal-title>
<issn pub-type="ppub">0040-5175</issn>
<issn pub-type="epub">1746-7748</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0040517512460302</article-id>
<article-id pub-id-type="publisher-id">10.1177_0040517512460302</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Original articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Research on computer-aided analysis and reverse reconstruction for the weave pattern of fabric</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Zhong</surname><given-names>Ping</given-names></name>
<xref ref-type="corresp" rid="corresp1-0040517512460302"/>
</contrib>
<contrib contrib-type="author">
<name><surname>Ye</surname><given-names>Tao</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Shi</surname><given-names>Yunlong</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Tu</surname><given-names>Xinxing</given-names></name>
</contrib>
<aff id="aff1-0040517512460302">College of Science, Donghua University, P.R. China</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0040517512460302">Ping Zhong, Donghua University, No.2999 Road Renming North, Songjiang District, Shanghai, China. Email: <email>pzhong937@dhu.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2013</year>
</pub-date>
<volume>83</volume>
<issue>3</issue>
<fpage>298</fpage>
<lpage>310</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013 Reprints and permissions: sagepub.co.uk/journalsPermissions.nav</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This paper proposes a method to realize computer-aided analysis and reverse reconstruction for the weave pattern of fabric. Firstly, a double-face imaging system is designed to get the redundant image information used to correct node classification, and then the image gridding processing is realized by using the projection algorithm in the warp and weft directions. The grid node color was determined by the color clustering method. Then the attributes and classification of the grid nodes are determined by the analysis of the edge strength information. The correction of node classification is realized with the information of adjacent nodes and the color of the double surface image of the corresponding fabric region. After that, the acquired nodes were encoded with 0 and 1, and then converted into a digital matrix; the basic weave structure matrix of the fabric was derived by matrix transformations. In the meantime, two one-dimensional row and column matrices and a color mapping table are adopted to represent the warp and weft yarn color array. On the basis of the above, the digital files of the woven pattern of sample fabric are established. Finally, the model of the fabric weave pattern can be obtained by the reverse reconstruction method and the automatic recognition of the fabric weave pattern is realized. Through comparison of the original fabric sample image with the reconstructed weave pattern model, it is proved that the proposed method is effective.</p>
</abstract>
<kwd-group>
<kwd>weave pattern</kwd>
<kwd>dual-side imaging system</kwd>
<kwd>interlacing nodes</kwd>
<kwd>matrix transformations</kwd>
<kwd>reverse reconstruction</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The weave pattern of woven fabric mainly consists of warp and weft yarn density, fabric structure, color, the array parameter of dyed yarn and so on. In modern textile enterprises, except for the warp and weft count, which is determined by the method of sampling, weighing and calculation, other parameters still need to rely on analysis of the special testers with the help of auxiliary equipments. Such an artificial analyzing method has certain disadvantages, such as time consumption, low efficiency and influence caused by subjective factors of testers on the testing results; that is to say, different testers may come to diverse testing results. Nowadays, textile manufacturing has a tendency to produce various kinds of textiles with small batches. The development in computers and image processing technology has offered new ways to design fabric structure and to automatically recognize the weave patterns. Such ways can not only automatically and efficiently recognize the structure parameters of the sample fabric, but also achieve the products’ reverse model building and reconstruction. It has become a new researching hotspot in textile engineering by machine vision to directly obtain surface parameters of the fabric surface.<sup><xref ref-type="bibr" rid="bibr1-0040517512460302">1</xref>–<xref ref-type="bibr" rid="bibr5-0040517512460302">5</xref></sup> Many workable algorithms have been brought forward by studies from the researchers all over the world.<sup><xref ref-type="bibr" rid="bibr6-0040517512460302">6</xref>–<xref ref-type="bibr" rid="bibr10-0040517512460302">10</xref></sup> From the perspective of image processing methods, one way is to judge interlacing types through the means of gray analysis, edge enhancement, thresholding, histogram equalization, wave filtering, etc. For example, Kang et al.<sup><xref ref-type="bibr" rid="bibr11-0040517512460302">11</xref></sup> take advantage of the fabric light transmittance image to locate the yarns through gray analysis of the yarn clearance, and obtain the degree of fineness and density; then they can determine the node types by using the ratio of the long axis to the short axis of the elliptical protuberances presented by the interlacing points in reflecting light image. Another way is to calculate and analyze the image after converting the spatial domain into the frequency domain through FFT (Fast Fourier Transformation), two-dimensional (2D) spectral analysis, wavelet transformation, etc.<sup><xref ref-type="bibr" rid="bibr12-0040517512460302">12</xref>–<xref ref-type="bibr" rid="bibr14-0040517512460302">14</xref></sup></p>
<p>To analyze and recognize the weave pattern of fabric, first of all we need to segment the cross-yarns. There are mainly two methods of segmentation. The one proposed by Kang et al.<sup><xref ref-type="bibr" rid="bibr11-0040517512460302">11</xref></sup> is to distinguish the cross-yarn’s type through the geometrical shape of interlacing nodes; the other is the Fuzzy C-Means (FCM) clustering method introduced by Kou et al.,<sup><xref ref-type="bibr" rid="bibr15-0040517512460302">15</xref></sup> which classifies the interlacing nodes by analyzing the texture characteristic. However, these two methods both can only recognize the interlacing type of the fabric, and are unable to distinguish the weave patterns. In addition, the noise in the image that is likely to be caused by fabric winkling, curling and fading has brought comparative difficulty in fabric recognition and even led to incorrect recognition.<sup><xref ref-type="bibr" rid="bibr16-0040517512460302">16</xref>–<xref ref-type="bibr" rid="bibr20-0040517512460302">20</xref></sup> What’s more, due to the fact that single-face image information merely is adopted in the traditional fabric structure analyzing method, which usually results in an inadequacy in the obtained image information, it is hard to conduct accurate analysis to the weave pattern, particularly when the fabric structure is relatively complex. The double-face image information of the fabric can contribute to an increase in the information content. The single-face image only involves half of the interweaving information between warp and weft yarns; the complete geometrical shape of the yarns cannot be observed from a single face; instead, the integrated interweave information can only be acquired from the double-face field of view.</p>
<p>On this basis, this paper designs a double-face imaging system that can collect images of both the front-side and back-side of the woven fabric in the detection process. This kind of collection, together with brightness characteristics and color feature of the fabric in the warp and weft directions, can help to separate cross-yarns. The classification of the interlacing type is the key to recognizing the weave pattern. Accordingly, this paper proposes a classification method of interlacing node type based on the quadrilateral boundary. At the same time, in order to guarantee the accuracy of the classification, this paper raises a correction method based on neighboring information and double-face image information. The coding mode is introduced to encode the warp and weft nodes with 0 and 1; with that, the digital matrix is build up to represent node classification. The largest independent group theory is used to decide the basic construct unit of the fabric weave pattern, and two single dimension arrays, together with a color mapping table, are established to represent the arrangement of the color of the warp and weft yarns; through the above processing, a digital file can be established that is the foundation of fabric weave pattern recognition and the reverse reconstruction model. The method proposed in this paper was proved to be efficient by comparing the original sample image to the reconstructed image. The structure of this paper is arranged as follows: the first section analyzes generally the researching content of this paper and the second section introduces the double-face imaging system; in the third section the gridding processing method of the woven fabric image is put forward; in the forth section a digital node classification method and correction method are introduced, and in the fifth section we bring forward a digital file to represent the weave pattern of the fabric surface, as well as a method of reconstruction of the weave pattern. Finally we present the final discussion and conclusion.</p>
<sec id="sec1-0040517512460302"><title>Image acquisition device of fabric surface</title>
<p>In this paper, the double-face imaging device of fabric is as shown in <xref ref-type="fig" rid="fig1-0040517512460302">Figure 1</xref>. In the work process, the electric motor controlled by a computer drives cramping apparatus to whirl and locates the front and back of the fabric, and then the image of both sides of the fabric surface is captured to be preserved in the computer for processing.
<fig id="fig1-0040517512460302" position="float"><label>Figure 1.</label><caption><p>The schema of the double-face imaging system: (1) charge-coupled device; (2) clamp mechanism for the testing sample; (3) sample fabric; (4) electric motor and executive mechanism.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig1.tif"/></fig></p>
<p>In the double-face imaging device, the distance between the camera and the fabric sample is 300 mm. The whole imaging device is made up of four parts. (1) The first is the optical imaging system, which is controlled by the computer, is in charge of acquiring the double-face image of the target region of the fabric. The optical imaging system mainly includes the digital charge-coupled device (CCD) and optical lens. In this paper, the digital CCD is a DH-SV200FC, of which the resolution is 1628 × 1236 pixels and the optical lens is TEC-M55, whose focal length is 55 mm; the aperture is F2.8-F32C and the working distance is from 140 mm to infinity. (2) The second part is the sample fabric clamping mechanism. It contains two splints of the same structure, and in each splint there are four predefined reference points (used for locating the corresponding region between double-face images) with a geometric relationship in four angular points of the sampling window; the specific sample-cramping apparatus consists of a transparent double-face splint and a fixing skeleton. A side of splints is combined together with a hinge and the other side can open and close. The tested sample can smoothly be fitted into a proper site inside. By moving the two pairs of the locating slice in the upper side of the cramping apparatus, it can select the size of the tested region. (3) The third part is the electric motor and executive mechanism. It is used to control the cramping apparatus to swivel 180 degrees precisely and realize double-face imaging of fabric. (4) The fourth part is the system software, which is used to carry out a procedure of collecting, controlling, processing and analyzing the images; this system software is developed on the Microsoft Visual C++ 6.0 platform and can work on the operating system of Windows XP.</p>
<p>Using the imaging device designed in the paper, we can obtain an image of the tested sample with the size of 1628 × 1236 pixels. <xref ref-type="fig" rid="fig2-0040517512460302">Figure 2</xref> shows the partial area of the double-face image of fabric captured; the image area within the blue box is selected area for processing.
<fig id="fig2-0040517512460302" position="float"><label>Figure 2.</label><caption><p>The partial double-face images of fabric: (a) the front image of the fabric; (b) the back image of the fabric. (Color online only.).</p></caption><graphic xlink:href="10.1177_0040517512460302-fig2.tif"/></fig></p>
</sec>
<sec id="sec2-0040517512460302"><title>The grid processing of the woven fabric image</title>
<p>The key steps to automatically recognize the weave pattern of woven fabric is to realize the interlacing node segmentation in the fabric image, which is based on gridding processing in the fabric image. Aiming at a precise grid to the image, the image should be preprocessed firstly so as to suppress image noise and enhance significant image features. Through these methods, we can precisely separate the interlacing nodes and lay the foundation for the follow-up image analysis.</p>
</sec>
<sec id="sec3-0040517512460302"><title>Image region selection and filtering</title>
<p>Effective image region selection is to select the specified part of image as a follow-up image for processing objects from the original image. In general, the original image we obtained was too large and contained a large amount of redundant information. It is not conducive to real-time processing of images, where we can select an effective image region from the original image to realize fabric woven pattern analysis quickly. Of course, if the image structure is relatively complicated, a larger image area selection is allowed so that it can include more interlacing nodes and increase redundant information. For example, the image region within the blue box, shown in <xref ref-type="fig" rid="fig2-0040517512460302">Figure 2</xref>, is selected for processing. The image preprocessing operation is required to improve image quality. In this paper, several steps on the sample fabric image are carried out, which include (a) grayscale transformation, (b) image filtering, (c) contrast enhancement, etc. The image noises are unavoidable due to the impact of the imaging environment and conditions. Hence, it is necessary for the image to be filtered after grayscale transformation. Traditional average filtering and median filtering will make the images become blurred and lose the image detail when they have got rid of noise. In this paper, the spatial filter has been used to remove image noise. We combine average filtering with the median filtering for the image of woven fabric. Firstly, an improved algorithm of average filtering has been used to wipe image noise off. The average filter can be represented by the following equation:
<disp-formula id="disp-formula1-0040517512460302"><label>(1)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math1-0040517512460302"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula1-0040517512460302" xlink:href="10.1177_0040517512460302-eq1.tif"/></disp-formula>
</p>
<p>Considering the precision of the projection algorithm, the kernel of average filter used in this paper is
<disp-formula id="disp-formula2-0040517512460302"><label>(2)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math2-0040517512460302"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo/><mml:mo>[</mml:mo></mml:mrow><mml:mtable align="left"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" columnspan="1"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:msup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" columnspan="1"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mrow><mml:mo/><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula2-0040517512460302" xlink:href="10.1177_0040517512460302-eq2.tif"/></disp-formula>
</p>
<p>Since the common neighbor average filtering will make the image illegible, the improved average filtering algorithm has changed the pixel disposal method, in which the pixel will be replaced by the average value only if the difference between the value of the pixel and the average value of the neighbor pixel is more than a threshold that we have set up before. Otherwise, the value of the pixel will not be changed. The advantage of the algorithm is that the image will not be illegible but the noises of high intensity can be wiped off. Through the experiment, the threshold value of 30 can be used to better retain the image details.</p>
<p>Similarly, median filtering has selected the middle pixel values after sorting in the filter window:
<disp-formula id="disp-formula3-0040517512460302"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math3-0040517512460302"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Med</mml:mi><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">}</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula3-0040517512460302" xlink:href="10.1177_0040517512460302-eq3.tif"/></disp-formula>
</p>
<p>In <xref ref-type="disp-formula" rid="disp-formula3-0040517512460302">Equation (3)</xref>, the size of the window is 5 × 5, and <italic>N</italic> is 25. In fact, the selected pixel is not necessarily linked with the operated pixels. Therefore, if an object in the image is too small, after sorting, it may be that there are no pixels in the middle of the filter window and the object will be filtered out or even disappear after filtering. So we set a threshold value before filter operation. Only when the difference between the value of operated pixel and the median value of neighborhood pixels is greater than the threshold value dose the operated pixel have been replaced by the pixel with the median value, or the original pixel value is retained. With this method, if there is a noise pixel point, its pixel values will have a larger difference, and it can be got rid of. If the pixel is not a noise point, the difference between the median value of the neighborhood and the operated pixel value will not be great and the original pixel value can be retained as long as the appropriate threshold is set before. Through experiments, it is found that the threshold value between 20 and 30 is preferable to retain the image details while removing noise. <xref ref-type="fig" rid="fig3-0040517512460302">Figure 3</xref> shows the image processing results.
<fig id="fig3-0040517512460302" position="float"><label>Figure 3.</label><caption><p>The results of image preprocessing: (a) the original image; (b) the converted gray image; (c) the image after filter operation; (d) the enhanced image.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig3.tif"/></fig></p>
</sec>
<sec id="sec4-0040517512460302"><title>Automatic fabric image gridding</title>
<p>The automatic gridding of fabric images can be achieved by using the brightness characteristic information of the image. When constructing the gridding model, all the yarns are regarded as a straight stripe in the vertical and horizontal directions. However, the shape and location assigned to the grids are merely approximations.</p>
<p>The brightness distribution of fabric image has an obvious gradient change. The brightness gradient arranged in descending order is: the axial line of yarns, the marginal zone of yarns and the transition region between yarns; what is more, there is a brightness mutation in the transition region, so the brightness valley value can be found by using the projection algorithm in the vertical and horizontal directions. With this method, the yarns can be detected and the interlacing node can be finally segmented.</p>
<p>Assuming the brightness of any pixel point is <italic>G</italic>(<italic>i</italic>,<italic>j</italic>). The gray value of the fabric image can be mapped into two independent one-dimensional waves that can be described by following equations:
<disp-formula id="disp-formula4-0040517512460302"><label>(4)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math4-0040517512460302"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula4-0040517512460302" xlink:href="10.1177_0040517512460302-eq4.tif"/></disp-formula>
<disp-formula id="disp-formula5-0040517512460302"><label>(5)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math5-0040517512460302"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic alternate-form-of="disp-formula5-0040517512460302" xlink:href="10.1177_0040517512460302-eq5.tif"/></disp-formula>
where <italic>G(j)</italic> is the accumulated gray value of the <italic>i</italic>th column pixels of the fabric image and <italic>G(i)</italic> is the accumulated gray value of the <italic>j</italic>th row pixels of the fabric image.</p>
<p>Taking <xref ref-type="fig" rid="fig3-0040517512460302">Figure 3(d)</xref> for example, the gray projection result of the converted image is shown in <xref ref-type="fig" rid="fig4-0040517512460302">Figure 4</xref>. The valley points of the curve in <xref ref-type="fig" rid="fig4-0040517512460302">Figure 4</xref> correspond to the weft gap position in the fabric image, and the peak points represent the axes of the yarn. As long as the valley points of the curve in <xref ref-type="fig" rid="fig4-0040517512460302">Figure 4</xref> are figured out, it will be possible to locate the weft gaps and realize preliminary segmentation of the weft yarn.
<fig id="fig4-0040517512460302" position="float"><label>Figure 4.</label><caption><p>The gray fabric image and projection curve.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig4.tif"/></fig></p>
<p>The same as for the weft yarn, the warp yarn of an ideal fabric should be sited in the vertical direction. In the gray projection method, an accumulation of the pixel points arraying in a row can allow one to locate the weft. In an ideal smooth curve, to locate the valley point one only needs to detect two adjacent points of the projection curve. That is to say, if a point <italic>G</italic>[<italic>i</italic>] in the projection curve can meet the condition <italic>G</italic>[<italic>i</italic>–1] &lt;<italic>G</italic>[<italic>i</italic>] &lt;<italic>G</italic>[<italic>i</italic> + 1], then <italic>G</italic>[<italic>i</italic>] is the valley point. However, from <xref ref-type="fig" rid="fig4-0040517512460302">Figure 4</xref> we note that the valley points in the curve can be divided into two categories: one can be called the truth value, namely the minimum value in actual representation of yarn clearance, while the other is called the partial minimum value, which comes from the fact that the brightness of the pixel points in a fabric image does not completely follow a descending distribution. In order to filter these points with local minimum value, a filter, namely Filter=[0,0,1,0,0], is involved in the paper, which can realize only when the pixel point is the minimum value among the five adjacent points can be considered as the position point of yarn gaps.</p>
<p>After filtering the partial minimum value point, the curve valley points can represent the exact position of yarn gaps. If we accurately locate these gaps points and then associate them with the fabric image, the fabric image gridding can be achieved. <xref ref-type="fig" rid="fig5-0040517512460302">Figure 5</xref> shows the grid results.
<fig id="fig5-0040517512460302" position="float"><label>Figure 5.</label><caption><p>Fabric gridding processing result.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig5.tif"/></fig></p>
</sec>
<sec id="sec5-0040517512460302"><title>The method of digital classification</title>
<sec id="sec6-0040517512460302"><title>Preliminary classification of the nodes</title>
<p>The yarn’s interlacing type depends on the relative position formed when the weft and warp interweave. If the warp is at the top, then the interlacing type is the warp interlacing type; conversely, if the weft is at the top, it is a weft interlacing type. In this paper the interlacing type is described through a quadrilateral boundary of interlacing nodes. The boundary intensity information of the four sides of the node determines the type of interlacing nodes. Ideally, two states may appear in boundary intensity, which can be digitized into 1 and 0, namely, there exists an edge or does not exist an edge between the nodes. According to different combinations of four edge intensities of the nodes, the nodes can be classified into eight types. Either the warp or weft interlacing nodes can further be divided into four subtypes. In fact, the edge intensity is not absolutely digitized 0 or 1 for there is an existence of some pixel in the edge which value is higher or lower than the threshold value. So the classification for the nodes is a typical K-type division problem with four input data (the boundary intensity of four sides) and eight output states (the specific type of nodes). <xref ref-type="table" rid="table1-0040517512460302">Table 1</xref> lists the all node types and their numbers.
<table-wrap id="table1-0040517512460302" position="float"><label>Table 1.</label><caption><p>The node type and number</p></caption>
<graphic alternate-form-of="table1-0040517512460302" xlink:href="10.1177_0040517512460302-table1.tif"/>
</table-wrap></p>
<p>In <xref ref-type="table" rid="table1-0040517512460302">Table 1</xref>, node 0 and the node 4 have the same boundaries. Using the boundary information it is difficult to distinguish them, but during the next correction steps we can determine which node type they are by using their color information.</p>
<p>After the initial nodes model is obtained, we can adjust the node image through the analysis of the adjacent gray information of the four sides of each node. A threshold value can be set for the edge pixels. If the edge gray value is lower than the threshold value, then the edge can retained and recorded as a closed state 1; if the node edge grayscale value is higher than the threshold value, then the edge will be removed and recorded as open state 0.</p>
<p>It is already adequate to divide a node into two categories for extraction of the fabric structure. The reason that the two main types are further divide into eight subtypes is that the four edge intensity information is relevant to the boundary intensity of neighboring node, and the boundary intensity information of the adjacent node can be used to correct the errors in node classification. For instance, if a left neighboring node is an open left weft (type 1), then there are only two options: it is an open right weft (type 2) or both the left and the right wefts are open (type 3). There are two principles for error correction based on edge information of neighboring nodes: firstly, if the surface interlacing is a weft interlacing, its back-side interlacing is definitely a warp interlacing, and vice versa; secondly, the types of two neighboring nodes should meet the restriction condition listed in <xref ref-type="table" rid="table2-0040517512460302">Table 2</xref>.
<table-wrap id="table2-0040517512460302" position="float"><label>Table 2.</label><caption><p>Impossible-existing types of neighboring nodes</p></caption>
<graphic alternate-form-of="table2-0040517512460302" xlink:href="10.1177_0040517512460302-table2.tif"/>
<table frame="hsides"><thead align="left">
<tr><th>Node Type</th>
<th>left adjacent nodes</th>
<th>upper adjacent nodes</th>
<th>right adjacent nodes</th>
<th>lower adjacent nodes</th></tr></thead>
<tbody align="left">
<tr>
<td>0</td>
<td>0, 1, 2, 3</td>
<td>6, 7</td>
<td>0, 1, 2, 3</td>
<td>5, 7</td></tr>
<tr>
<td>1</td>
<td>0, 1, 4, 5, 6, 7</td>
<td>6, 7</td>
<td>0, 1, 2, 3</td>
<td>5, 7</td></tr>
<tr>
<td>2</td>
<td>0, 1, 2, 3</td>
<td>6, 7</td>
<td>0, 2, 4, 5, 6, 7</td>
<td>5, 7</td></tr>
<tr>
<td>3</td>
<td>0, 1, 4, 5, 6, 7</td>
<td>6, 7</td>
<td>0, 2, 4, 5, 6, 7</td>
<td>5, 7</td></tr>
<tr>
<td>4</td>
<td>2, 3</td>
<td>4, 5, 6, 7</td>
<td>1, 3</td>
<td>4, 5, 6, 7</td></tr>
<tr>
<td>5</td>
<td>2, 3</td>
<td>1, 2, 3, 4, 5</td>
<td>1, 3</td>
<td>4, 5, 6, 7</td></tr>
<tr>
<td>6</td>
<td>2, 3</td>
<td>4, 5, 6, 7</td>
<td>1, 3</td>
<td>0, 1, 2, 3, 4, 6</td></tr>
<tr>
<td>7</td>
<td>2, 3</td>
<td>0, 1, 2, 3, 45</td>
<td>1, 3</td>
<td>0, 1, 2, 3, 4, 6</td></tr>
</tbody>
</table>
</table-wrap></p>
<p>The eight pre-classification subtypes are reconverted to warp and weft node types by <xref ref-type="disp-formula" rid="disp-formula3-0040517512460302">Equation (3)</xref> and the results can be saved by a matrix:
<disp-formula id="disp-formula6-0040517512460302"><label>(6)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math6-0040517512460302"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo/><mml:mo>(</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo/><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo/><mml:mo>{</mml:mo></mml:mrow><mml:mtable align="left"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" columnspan="1"><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mrow><mml:mo/><mml:mo/></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula6-0040517512460302" xlink:href="10.1177_0040517512460302-eq6.tif"/></disp-formula>
where <italic>i</italic> is the code value of eight interlacing subtypes extracted from the boundary intensity; <italic>C</italic>(<italic>i</italic>) represents warp and weft interlacing type: when <italic>C</italic>(<italic>i</italic>) = 0, it represents the weft node and when <italic>C</italic>(<italic>i</italic>) = 1 it represents the warp node. So we can use a matrix to represent the node type, and <xref ref-type="fig" rid="fig6-0040517512460302">Figure 6</xref> shows the results of the node type represented by the matrix.
<fig id="fig6-0040517512460302" position="float"><label>Figure 6.</label><caption><p>Thematrix representation of the node type.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig6.tif"/></fig></p>
</sec>
</sec>
<sec id="sec7-0040517512460302"><title>Correction of node types</title>
<p>In the process of recognition, owing to the rough surface of the sample fabric or the complexity of the fabric image texture, mistakes are inevitable in yarn segmentation, which influence accurate classification of the node types in the fabric. Meanwhile, since the original image conversion to a gray image may lose information and lead to insufficient information of the gap region between nodes, these will also affect the classification result. However, for artificial recognition, the testers can carry out correct judgment according to their experience. In this paper, in order to assure the accuracy of the node classification, three correction methods are proposed to correct and regulate the matrix, which represents the node types.</p>
<sec id="sec8-0040517512460302"><title>The adjacent node information correction</title>
<p>In node classification, the node type is defined by boundary intensity, and the four edge intensity information types of the node are relevant to the boundary intensity of the neighboring nodes. Whether the node type exists or not can be judged according to the already-known neighboring information of the node type. <xref ref-type="table" rid="table2-0040517512460302">Table 2</xref> lists all impossible types of adjacent nodes. For instance, if a node type is a close weft interlacing of type 0, according to the boundary intensity, its left and right neighboring nodes in the weft direction (left and right directions) cannot be any one of the four kinds of weft node types, and its upper neighboring node and lower neighboring node (warp direction) would not exist as types 6, 7 and 5, 7, respectively. Undoubtedly this conclusion can be applied to verify the interlacing type that has already been determined.</p>
</sec>
<sec id="sec9-0040517512460302"><title>The dual-side image information correction</title>
<p>In the paper, the color clustering method is used to determine the color of the pre-classification nodes. In the process, how to get the feature color set of node is key. To get feature colors includes two principles, namely the maximal frequency principle and the compatibility principle. The former guarantees that the <italic>N</italic> feature colors of the color sets can represent the main color component in the target image area, and the latter requires that the feature colors have a relatively uniform distribution in color space. We adopt the compatible sphere space color model, in which each feature color occupies one sphere space of radius <italic>R</italic>; the image color within this space can all be merged together into the feature color. The interval between each feature color should be greater than <italic>R</italic>. Obviously, the larger the compatible range is, that is, the greater <italic>R</italic> is, the fewer the node feature colors there are. For the different textiles, we can adopt an experimental method to select the <italic>R</italic> value and determine the set of characteristic colors according to the algorithm. Through the experiment, we use <italic>k</italic> (<italic>k = </italic>16) colors as the color feature sets of each node. To determine the node color feature set we can use following steps:
<list id="list1-0040517512460302" list-type="alpha-lower">
<list-item><p>draw statistics of histogram <italic>P</italic>(<italic>x<sub>n</sub></italic>) for all the color <italic>x<sub>n</sub></italic> of the node image (<italic>n</italic> ∈ [1, 2, … , <italic>N</italic>], where <italic>N</italic> is the number of colors of the node image);</p></list-item>
<list-item><p>array <italic>P</italic>(<italic>x<sub>n</sub></italic>) with descending order to get the color sequence <inline-formula id="ilm1-0040517512460302"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math1-0040517512460302"><mml:mrow><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mrow><mml:mo>'</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>;</p></list-item>
<list-item><p>the processing of colors starts with the color which has the maximum frequency. Take as the center and merge compatible body colors; for the residual colors, still began with the color with the maximum frequency to merge its compatibility of color body; Repeated iteration of the above merging process, until completion of <italic>N</italic> colors.</p></list-item>
<list-item><p>after the above process, the color set that contains <italic>k</italic> feature colors can be obtained.</p></list-item>
</list></p>
<p>The color of the node can be determined by repeating the previous steps for node <italic>k</italic> feature colors, until the <italic>k</italic> feature colors of the node are merged into a color.</p>
<p>For some nodes that have the same color, their colors obtained with the above method sometimes have subtle differences, so through the adjacent node similarity calculation, we can determine whether their colors can be merged into one color.</p>
<p>If the color of the node image represents the feature colors, the color of the single node region can be described with the single feature vector:
<disp-formula id="disp-formula7-0040517512460302"><label>(7)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math7-0040517512460302"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo/><mml:mo>[</mml:mo></mml:mrow><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo/><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math><graphic alternate-form-of="disp-formula7-0040517512460302" xlink:href="10.1177_0040517512460302-eq7.tif"/></disp-formula>
</p>
<p>In the formula, <italic>S</italic>(<italic>x<sub>i</sub></italic>) is the pixel numbers of the <italic>i</italic>th feature color of a feature set in this image region, and <italic>C<sub>f</sub></italic> is the vector reflecting differences between them. In the paper, we use the two vector angles in the vector space to reflect the relationship of the similarity degree. So here we can define a texture similarity:
<disp-formula id="disp-formula8-0040517512460302"><label>(8)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math8-0040517512460302"><mml:mrow><mml:msub><mml:mrow><mml:mi>Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>mn</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mi>arccos</mml:mi><mml:mrow><mml:mo/><mml:mo>[</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>fm</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>fn</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mrow><mml:mo/></mml:mrow><mml:mo>‖</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>fm</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo/></mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mo/></mml:mrow><mml:mo>‖</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>fn</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo/></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo/><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula8-0040517512460302" xlink:href="10.1177_0040517512460302-eq8.tif"/></disp-formula>
</p>
<p>The larger the <inline-formula id="ilm2-0040517512460302"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math2-0040517512460302"><mml:mrow><mml:msub><mml:mrow><mml:mi>Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>mn</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is, the more similar the color characteristics of the two node regions are. Through the experiment, when <inline-formula id="ilm3-0040517512460302"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math3-0040517512460302"><mml:mrow><mml:msub><mml:mrow><mml:mi>Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>mn</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is greater than 0.9, we can infer that the two adjacent nodes have the same color.</p>
<p>According to the fabric color characteristics, all nodes of the image are processed with the above method. Through obtaining the color feature set of the node, the color of the node can be decided upon. And whether the colors of adjacent nodes can be merged is determined by calculating the similarity <inline-formula id="ilm4-0040517512460302"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math4-0040517512460302"><mml:mrow><mml:msub><mml:mrow><mml:mi>Ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>mn</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Finally, a new picture based on similarity measurement can be obtained. According to the image similarity measurement, we can get a more precise node color.</p>
<p>The fabric color pattern is made up of warp and weft yarns with different colors. The node color of fabric can be used to correct the errors in node classification. Of course, this method only applies to the nodes interlaced by the yarns of different colors. If the node’s upper-side color can match that of the weft yarn, this node is a weft node, and vice versa. If the node color gained by pre-classification is contradictory with that directly extracted from the image, the node classification results need to be corrected.</p>
<p>The color information of the dyed yarn can be used to regulate the properties of warp and weft nodes. The rules are as follows:
<list id="list2-0040517512460302" list-type="roman-lower">
<list-item><p>if the node’s color is identical to the corresponding warp yarn, yet different from the weft, this node is defined as a warp interlacing node;</p></list-item>
<list-item><p>if the node’s color is identical to the corresponding weft yarn, yet different with the weft, this node is defined as a weft interlacing node;</p></list-item>
<list-item><p>if the node’s color is identical to the color of both corresponding warp and weft yarns, its properties cannot be identified and then the node will be looked at as an undetermined interlacing node, which means the color information correction method is invalid and cannot be used for judging and correcting the node interlaced by the same color yarn.</p></list-item>
</list></p>
</sec>
<sec id="sec10-0040517512460302"><title>The basic structure image of fabric</title>
<p>Through multilevel correction, the accurate interlacing type image made by different type nodes can be obtained. In fact, the fabric is made up by a basic structure unit repeatedly arrayed one by one, which enables the fabric image to display certain texture features. Each of the basic structure units in the fabric image has the same structure. The key of recognition of a fabric weave pattern is to acquire the basic structure unit.</p>
<p>In this paper, the basic structure unit of fabric is extracted by using the matrix transform method. Firstly, the interlacing type image has been encoded with 0 and 1, in which the “0” represents the warp interlacing node and the “1”represents the weft interlacing node. By this method, the interlacing type image is converted to a digital matrix, the elements of which are composed of the “0” and “1”. Secondly, we calculate the largest independent group of row and column vectors of the matrix. The number of the maximal independent set of the row vectors in the matrix is the number of weft cycles, and then the number of the maximal independent set of the column vectors in the matrix is the number of warp cycles. <xref ref-type="fig" rid="fig7-0040517512460302">Figure 7</xref> shows the algorithm to acquire the fabric basic structure unit.
<fig id="fig7-0040517512460302" position="float"><label>Figure 7.</label><caption><p>The method to obtain the basic structure unit.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig7.tif"/></fig></p>
</sec>
</sec>
<sec id="sec11-0040517512460302"><title>Digitization and classification experiment</title>
<sec id="sec12-0040517512460302"><title>Digital file representation of the fabric weave pattern</title>
<p>In the study, after the basic structure unit and yarn color information of the fabric are extracted, the parameter information of the fabric can be formed into a digital file used to represent the weave pattern of the fabric and realize reverse reconstruction.</p>
<p>This digital file involves two parts. (1) The first is the woven pattern structure, which is made up of the basic structure unit repeatedly arrayed one by one in the interlacing image. Matrix A can be use to represent the basic structure unit, which consists of 0 and 1, where 0 represents the weft node and 1 represents the warp node. (2) The second part is two single-dimension matrices and a color mapping table that lists all colors and their corresponding coding of the fabric sample. The color mapping table is similar to the bitmap's palette.</p>
<p>If the total number of colors of the fabric sample is <italic>N</italic>, the different colors can be encoded as 0, 1, 2 , … , N–1 and stored in a coding table. Through the color coded value in the color mapping table, the actual color can be found.</p>
<p>Two one-dimension matrices can be used to represent the yarn color of the tested sample in the warp and weft directions in which the element is the code value of the colors. For example, the weave pattern shown in <xref ref-type="fig" rid="fig5-0040517512460302">Figure 5</xref> has a total of three colors and can construct the mapping table shown in <xref ref-type="fig" rid="fig8-0040517512460302">Figure 8</xref>.
<fig id="fig8-0040517512460302" position="float"><label>Figure 8.</label><caption><p>The color and color mapping table.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig8.tif"/></fig></p>
<p>The color array of the yarn can be easily and clearly shown by two one-dimensional matrices:
<disp-formula id="disp-formula9-0040517512460302"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math9-0040517512460302"><mml:mrow><mml:mtext>Warp color</mml:mtext><mml:mo> </mml:mo><mml:mtext>matrix</mml:mtext><mml:mo>:</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo/><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo/><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><graphic alternate-form-of="disp-formula9-0040517512460302" xlink:href="10.1177_0040517512460302-eq9.tif"/></disp-formula>
<disp-formula id="disp-formula10-0040517512460302"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="math10-0040517512460302"><mml:mrow><mml:mtext>Weft color matrix</mml:mtext><mml:mo>:</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo/><mml:mo>[</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo/><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><graphic alternate-form-of="disp-formula10-0040517512460302" xlink:href="10.1177_0040517512460302-eq10.tif"/></disp-formula>
where <italic>x<sub>i</sub></italic> is the coded value of the warp yarn of the <italic>i</italic>th row, and <italic>y<sub>j</sub></italic> is the coded value of the weft yarn of the <italic>j</italic>th column. There are a variety of woven patterns in actual production and we can easily get the digital file of the fabric weave pattern from matrix A, B, C and the color mapping table.</p>
</sec>
<sec id="sec13-0040517512460302"><title>Reverse reconstruction of the fabric weave pattern</title>
<p>Using the digital files composed of matrices A, B, C and the color mapping table, we can achieve the reconstruction of the fabric weave patterns. The reconstruction methods are as follows.</p>
<p>Firstly, matrix A, which represents the basic structure cycle unit, is expanded along the warp and weft directions, and makes the expanded matrix meet the pre-set size requirement. Then the matrix elements 0 and 1 are converted to a black and white interlacing image, and every node should be set to the same length and width to form a standard interlacing image.</p>
<p>Secondly, in the warp (vertical) direction, the directly neighboring warp nodes are merged into one interlacing node; this is the same in the warp (horizontal) direction.</p>
<p>Finally, according to matrices B and C, the color is filled at each node. In the process of color filling there are two necessary conditions that should be met simultaneously: when the handling node is a warp tissue node, its color value should be in correspondence with the warp color, and when the handling node is a weft node is should also be in correspondence with the weft color.</p>
<p>So far, we have completed the mathematical modeling of fabric samples; the model contains the complete original fabric information, such as the accurate interlacing structure and color information of the original fabric. At the same time, based on the actual need, we can also carry out an expansion on the model to achieve a simulated diagram of the fabric weave style at a larger scale so as to reach a deeper understanding of the fabric pattern. <xref ref-type="fig" rid="fig9-0040517512460302">Figure 9</xref> shows the reconstruction process.
<fig id="fig9-0040517512460302" position="float"><label>Figure 9.</label><caption><p>Structure and fabric yarn color information.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig9.tif"/></fig>
</p>
</sec>
</sec>
<sec id="sec14-0040517512460302"><title>Experimental details</title>
<p>According to the digital processing method mentioned above, we have developed a set of fabric image processing software; <xref ref-type="fig" rid="fig10-0040517512460302">Figure 10</xref> shows the software interface. In the software, the main functions include image collecting control, image preprocessing, parameter extraction, mathematical modeling and so on. Particularly through the reverse mathematics modeling function, we can realize digital simulation of the weave pattern of the fabric based on the original acquisition of the fabric image. The simulated fabric weave pattern also can prove the effectiveness of the method proposed in the paper by the comparison between the original fabric image and the simulated fabric weave pattern. In <xref ref-type="fig" rid="fig11-0040517512460302">Figure 11</xref>, many kinds of fabric samples were used in the experiment to prove the validity of the method proposed.
<fig id="fig10-0040517512460302" position="float"><label>Figure 10.</label><caption><p>The software interface.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig10.tif"/></fig>
<fig id="fig11-0040517512460302" position="float"><label>Figure 11.</label><caption><p>The real fabric images and the reverse mathematics modeling images based on the digital file.</p></caption><graphic xlink:href="10.1177_0040517512460302-fig11.tif"/></fig></p>
<p><xref ref-type="fig" rid="fig11-0040517512460302">Figure 11(a) and (c)</xref> show the images of the woven fabric from different magnifications; the basic structures of knitting and the color matrices can be correctly extracted with the proposed method in this paper. <xref ref-type="fig" rid="fig11-0040517512460302">Figure 11(b) and (d)</xref> show the corresponding reconstruction images, respectively. In <xref ref-type="fig" rid="fig11-0040517512460302">Figure 11(e)</xref>, its feature is that the left half of the image is made by the warp and weft yarns with the same color and the right half of the image is made by the warp and weft yarns with a different color. From the reconstructed image shown in <xref ref-type="fig" rid="fig11-0040517512460302">Figure 11(f)</xref> it can be seen that for the fabric made from the warp and weft yarns with the same color, it is difficult to distinguish the warp and weft node types by the method proposed in this paper. To distinguish this kind of node we need the shape information of nodes and the global image information; this means the algorithm proposed in this paper need to be further improved.</p>
</sec>
<sec id="sec15-0040517512460302" sec-type="conclusions"><title>Conclusion</title>
<p>The method proposed in this paper improves the traditional recognition method. It enables us to obtain a more accurate fabric weave pattern. In this paper we realize automatic recognition and digital representation of the fabric weave pattern by using the double-sided image information of the fabric sample, the interlacing node separation with the gray projection algorithm, color information extraction and the derivation of the basic structure unit. The multilevel correction method is used in this paper to ensure the correctness of the digital representation of node classification. The fabric weave pattern reverse reconstruction modeling is realized based on the digital file. Many models of the weaving pattern are established that prove the effectiveness of the proposed method. However, this tolerant ability is not unlimited to recognize the fabric patterns correctly. There are two main factors leading to recognition errors. The first reason is the quality of the image. In the imaging process, if light intensity is uneven or the fabric positioning is not accurate, it easily leads to image quality degradation, which results in fabric node segmentation errors and the reduction of the recognition rate. Another factor is the complexity of the structure of the fabric itself, which will also affect the recognition accuracy. The algorithm proposed in this paper can accurately handle some organizational structure with high regularity, such as plain, twill and satin weave, as well as fabric with strong texture changing regularity. However, for those fabrics with seriously declining yarns or a complex combination weave, it is necessary to improve the algorithm’s robustness in order to achieve the identification of structural parameters of complex structural or non-obvious regularity fabric. Particularly for the fabrics that contain a large curve or overlap tissue, it is very difficult to realize the accurate positioning of the yarn by using the image processing method and this easily leads to classification errors. These problems will require further study.</p>
</sec>
</body>
<back>
<sec id="sec16-0040517512460302"><title>Funding</title>
<p>This work was supported by the National Natural Science Foundation of China (Grant Nos. 11174048).</p>
</sec>
<ref-list>
<title>References</title>
<ref id="bibr1-0040517512460302"><label>1</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>C-FJ</given-names></name><name><surname>Lee</surname><given-names>C-J</given-names></name></person-group>. <article-title>A back-propagation neural network for recognizing fabric defects</article-title>. <source>Textil Res J</source> <year>2003</year>; <volume>73</volume>: <fpage>147</fpage>–<lpage>151</lpage>.</citation></ref>
<ref id="bibr2-0040517512460302"><label>2</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>C-FJ</given-names></name><name><surname>Lee</surname><given-names>C-J</given-names></name><name><surname>Tsai</surname><given-names>C-C</given-names></name></person-group>. <article-title>Using a neural network to identify fabric defects in dynamic cloth inspection</article-title>. <source>Textil Res J</source> <year>2003</year>; <volume>73</volume>: <fpage>238</fpage>–<lpage>244</lpage>.</citation></ref>
<ref id="bibr3-0040517512460302"><label>3</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>E</given-names></name><name><surname>Gowayed</surname><given-names>Y</given-names></name><name><surname>Abouiiana</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>Detection and classification of defect in knitted fabric structures</article-title>. <source>Textil Res J</source> <year>2006</year>; <volume>76</volume>: <fpage>295</fpage>–<lpage>300</lpage>.</citation></ref>
<ref id="bibr4-0040517512460302"><label>4</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>SR</given-names></name><name><surname>Latifi</surname><given-names>M</given-names></name><name><surname>Shaikhzadeh</surname><given-names>NS</given-names></name></person-group>. <article-title>Computer vision-aided fabric inspection system for on-circular knitting machine</article-title>. <source>Textil Res J</source> <year>2005</year>; <volume>75</volume>: <fpage>492</fpage>–<lpage>497</lpage>.</citation></ref>
<ref id="bibr5-0040517512460302"><label>5</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Zhu</surname><given-names>L</given-names></name><etal/></person-group>. <article-title>A new approach for image processing in foreign fiber detection</article-title>. <source>Comput Electron Agric</source> <year>2009</year>; <volume>68</volume>: <fpage>68</fpage>–<lpage>77</lpage>.</citation></ref>
<ref id="bibr6-0040517512460302"><label>6</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>M</given-names></name><name><surname>Escofet</surname><given-names>J</given-names></name><name><surname>Millán</surname><given-names>MS</given-names></name></person-group>. <article-title>Weave-repeat identification by structural analysis of fabric images</article-title>. <source>J Appl Opt</source> <year>2003</year>; <volume>42</volume>: <fpage>3361</fpage>–<lpage>3372</lpage>.</citation></ref>
<ref id="bibr7-0040517512460302"><label>7</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>KuoEscofet</surname><given-names>J</given-names></name><name><surname>Millán</surname><given-names>MS</given-names></name><name><surname>Rallo</surname><given-names>M</given-names></name></person-group>. <article-title>Modeling of woven fabric structures based on Fourier image analysis</article-title>. <source>J Appl Opt</source> <year>2001</year>; <volume>40</volume>: <fpage>6171</fpage>–<lpage>6176</lpage>.</citation></ref>
<ref id="bibr8-0040517512460302"><label>8</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>CC</given-names></name><name><surname>Liu</surname><given-names>SC</given-names></name><name><surname>Yu</surname><given-names>WH</given-names></name></person-group>. <article-title>Woven fabric analysis by image processing. Part I: identification of weave patterns</article-title>. <source>Textil Res J</source> <year>2000</year>; <volume>70</volume>: <fpage>481</fpage>–<lpage>485</lpage>.</citation></ref>
<ref id="bibr9-0040517512460302"><label>9</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>KuoGao</surname><given-names>WD</given-names></name><name><surname>Liu</surname><given-names>HJ</given-names></name><name><surname>Xu</surname><given-names>BJ</given-names></name><etal/></person-group>. <article-title>The automatic recognition for weft array parameter</article-title>. <source>Cotton Manuf Technol</source> <year>2002</year>; <volume>30</volume>: <fpage>28</fpage>–<lpage>29</lpage>.</citation></ref>
<ref id="bibr10-0040517512460302"><label>10</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>KuoLi</surname><given-names>XK</given-names></name></person-group>. <article-title>The quality testing of yarn based on the texture analysis on image</article-title>. <source>Comput Eng</source> <year>1999</year>; <volume>25</volume>: <fpage>55</fpage>–<lpage>62</lpage>.</citation></ref>
<ref id="bibr11-0040517512460302"><label>11</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kang</surname><given-names>TJ</given-names></name><name><surname>Kim</surname><given-names>CH</given-names></name><name><surname>Oh</surname><given-names>KW</given-names></name></person-group>. <article-title>Automatic recognition of fabric weave patterns by digital image analysis</article-title>. <source>Textil Res J</source> <year>1999</year>; <volume>69</volume>: <fpage>77</fpage>–<lpage>83</lpage>.</citation></ref>
<ref id="bibr12-0040517512460302"><label>12</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>MS</given-names></name><name><surname>Escofet</surname><given-names>J</given-names></name></person-group>. <article-title>Fourier-domain-based angular correlation for quasiperiodic pattern recognition. Applications to web inspection</article-title>. <source>J Appl Opt</source> <year>1996</year>; <volume>35</volume>: <fpage>6253</fpage>–<lpage>6260</lpage>.</citation></ref>
<ref id="bibr13-0040517512460302"><label>13</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>C-H</given-names></name><name><surname>Pang</surname><given-names>GKH</given-names></name></person-group>. <article-title>Fabric defect detection by Fourier analysis</article-title>. <source>IEEE Trans Ind Appl</source> <year>2000</year>; <volume>36</volume>: <fpage>1267</fpage>–<lpage>1276</lpage>.</citation></ref>
<ref id="bibr14-0040517512460302"><label>14</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>KuoXu</surname><given-names>B</given-names></name></person-group>. <article-title>Identifying fabric structures with fast Fourier transform techniques</article-title>. <source>Textil Res J</source> <year>1996</year>; <volume>66</volume>: <fpage>496</fpage>–<lpage>506</lpage>.</citation></ref>
<ref id="bibr15-0040517512460302"><label>15</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>C-FJ</given-names></name><name><surname>Shih</surname><given-names>C-Y</given-names></name><name><surname>Lee</surname><given-names>J-Y</given-names></name></person-group>. <article-title>Automatic recognition of fabric weave patterns by a fuzzy C-means clustering method</article-title>. <source>Textil Res J</source> <year>2004</year>; <volume>74</volume>: <fpage>107</fpage>–<lpage>111</lpage>.</citation></ref>
<ref id="bibr16-0040517512460302"><label>16</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>YJ</given-names></name><name><surname>Jang</surname><given-names>J</given-names></name></person-group>. <article-title>Applying image analysis to automatic inspection of fabric density for woven fabrics</article-title>. <source>Fibers Polym</source> <year>2005</year>; <volume>26</volume>: <fpage>156</fpage>–<lpage>161</lpage>.</citation></ref>
<ref id="bibr17-0040517512460302"><label>17</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>M</given-names></name><name><surname>Linka</surname><given-names>A</given-names></name><name><surname>Volf</surname><given-names>P</given-names></name></person-group>. <article-title>Automatic assessing and monitoring of weaving density</article-title>. <source>Fibers Polym</source> <year>2009</year>; <volume>10</volume>: <fpage>830</fpage>–<lpage>836</lpage>.</citation></ref>
<ref id="bibr18-0040517512460302"><label>18</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>A</given-names></name><name><surname>Benslimane</surname><given-names>R</given-names></name><name><surname>Orazio</surname><given-names>LD</given-names></name><etal/></person-group>. <article-title>A system for textile design patterns retrieval. Part I: design patterns extraction by adaptive and efficient color image segmentation method</article-title>. <source>J Textil Inst</source> <year>2006</year>; <volume>97</volume>: <fpage>301</fpage>–<lpage>312</lpage>.</citation></ref>
<ref id="bibr19-0040517512460302"><label>19</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>C-FJ</given-names></name><name><surname>Shih</surname><given-names>C-Y</given-names></name><name><surname>Kao</surname><given-names>C-Y</given-names></name><etal/></person-group>. <article-title>Color and pattern analysis of printed fabric by an unsupervised clustering method</article-title>. <source>Textil Res J</source> <year>2005</year>; <volume>75</volume>: <fpage>9</fpage>–<lpage>12</lpage>.</citation></ref>
<ref id="bibr20-0040517512460302"><label>20</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kuo</surname><given-names>H</given-names></name><name><surname>Palhang</surname><given-names>M</given-names></name><name><surname>Hosseini</surname><given-names>SA</given-names></name><etal/></person-group>. <article-title>Identification of printed pattern repeat and its dimensions by image analysis</article-title>. <source>J Textil Inst</source> <year>2006</year>; <volume>97</volume>: <fpage>71</fpage>–<lpage>78</lpage>.</citation></ref>
</ref-list>
</back>
</article>