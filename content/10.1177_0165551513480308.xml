<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JIS</journal-id>
<journal-id journal-id-type="hwp">spjis</journal-id>
<journal-title>Journal of Information Science</journal-title>
<issn pub-type="ppub">0165-5515</issn>
<issn pub-type="epub">1741-6485</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0165551513480308</article-id>
<article-id pub-id-type="publisher-id">10.1177_0165551513480308</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group></article-categories>
<title-group>
<article-title>A method of feature selection and sentiment similarity for Chinese micro-blogs</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Yang</surname><given-names>Dong-Hui</given-names></name>
<aff id="aff1-0165551513480308">School of Management, Harbin Institute of Technology, People’s Republic of China</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Yu</surname><given-names>Guang</given-names></name>
<aff id="aff2-0165551513480308">School of Management, Harbin Institute of Technology, People’s Republic of China</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0165551513480308">Dong-Hui Yang, School of management, Harbin Institute of Technology, Harbin 150001, People’s Republic of China. Email: <email>ydh95130@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2013</year>
</pub-date>
<volume>39</volume>
<issue>4</issue>
<fpage>429</fpage>
<lpage>441</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Chartered Institute of Library and Information Professionals</copyright-holder>
</permissions>
<abstract>
<p>Sentiment analysis of messages posted on micro-blogs is helpful in determining the current usefulness and acceptability of target products or services. It is the basis for finding users with similar attitudes. In this paper, we propose a new sentiment similarity technique to analyse Chinese micro-blog accounts. However, the Chinese text features have not been well studied. Therefore, we first chose four types of feature sets and selected principle features by combining information gain and support vector machine techniques. Next, we compared the four types of features to determine which type of feature contributed more than others. Here we used three classification techniques: decision tree, support vector machines and naive Bayes. Finally, we used Karhunen–Loéve transform technique and average precision between positive and negative features to measure sentiment similarity. Experiment evaluations demonstrated that this new method is efficient and performs better than original average distance for Chinese micro-blogs.</p>
</abstract>
<kwd-group>
<kwd>Chinese micro-blogs</kwd>
<kwd>feature selection</kwd>
<kwd>Karhunen–Loéve transform</kwd>
<kwd>sentiment similarity</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0165551513480308" sec-type="intro">
<title>1. Introduction</title>
<p>More and more users are sharing real-time news and information sources in micro-blog communities. People post their thoughts and opinions on news and information in micro-blogs every day. ‘What other people think’ has always been an important piece of information for most of us during the decision-making process [<xref ref-type="bibr" rid="bibr1-0165551513480308">1</xref>]. Social influences play a key role when people are making a purchase decision. Social networks are new forms of self-representation and communication, and are subject to social behaviour that is different from that in the real world [<xref ref-type="bibr" rid="bibr2-0165551513480308">2</xref>]. The recent developments in Web 2.0 have provided more opportunities to investigate web-based social networks. Some examples of such social networks are Facebook, MySpace, LinkedIn, Twitter, MyBlogLog, Flickr, Youtube, Join me, Delicious, RenRen and Sina Weibo. As the new social media, micro-blogs are becoming a popular communication platform. They allow users to post short messages to describe, update and share their current status and opinion [<xref ref-type="bibr" rid="bibr3-0165551513480308">3</xref>]. Therefore, the micro-blog platform is becoming an ever-larger social network.</p>
<p>Seeking and extracting useful information from Chinese micro-blog websites also poses significant challenges. Chinese micro-blogs have special posting content and characteristics. People can not only post words, but also music and videos. While some networks like Twitter and Facebook have been well documented, the popular Chinese micro-blog social network, Sina Weibo, has not been well studied [<xref ref-type="bibr" rid="bibr4-0165551513480308">4</xref>]. Sina Weibo is the biggest Chinese micro-blog platform. It is a new and promising platform that has almost 300 million users. According to the statistics of Hitwise, the utilization rate and user loyalty of Sina Weibo had surpassed those of Twitter by April 2011. Moreover, there is a vast difference in the content shared in Chinese micro-blogs when compared with a global social network such as Twitter. Most previous works have focused on Twitter, and little emphasis has been given to Chinese social media. That is why we focus on Chinese micro-blogs in this work.</p>
<p>Detecting the current attitude of users can provide information on online services and products. Furthermore, it has other potential applications. A good number of companies have considered opinion mining and sentiment analysis as part of their mission. Sentiment analysis has been used as a sub-component technology of recommendation systems to recommend items that receive much positive text feedback [<xref ref-type="bibr" rid="bibr5-0165551513480308">5</xref>, <xref ref-type="bibr" rid="bibr6-0165551513480308">6</xref>]. Sentiment analysis attempts to identify and analyse opinions and emotions. Using sentiment analysis on social networka, users’ words can be classified into positive or negative attitudes on certain topics. A fundamental technology in many current opinion mining and sentiment analysis applications is classification. Diverse methods have been studied for improving sentiment classification performance [<xref ref-type="bibr" rid="bibr7-0165551513480308">7</xref>, <xref ref-type="bibr" rid="bibr8-0165551513480308">8</xref>]. The approaches that have been adopted in previous sentiment classification studies can be classified into two categories: machine learning technique and semantic orientation technique [<xref ref-type="bibr" rid="bibr9-0165551513480308">9</xref>]. The machine learning approach tends to be more accurate, but the semantic orientation approach has better generality [<xref ref-type="bibr" rid="bibr10-0165551513480308">10</xref>, <xref ref-type="bibr" rid="bibr11-0165551513480308">11</xref>].</p>
<p>One important issue of sentiment analysis is to define the list of feature sets for opinion classification. Different feature sets are used in different social media research. Recently, several researchers summarized text features into four types: lexical features, syntactic features, structural features and content-specific features [<xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>, <xref ref-type="bibr" rid="bibr13-0165551513480308">13</xref>]. Since different languages have their special characteristics, the feature set can be remotely changed. According to our experiment platform, we adjusted those features and formed new feature sets that also contain Chinese part-of-speech tag features and micro-blog features.</p>
<p>In our research, we collected data from the biggest Chinese micro-blog, Sina Weibo. In China, there are currently 40 million people suffering from diabetes. Patients, doctors and hospitals open accounts in Sina Weibo to express their ideas and share diabetes information. They make up a large social network through micro-blogging. Therefore, we collected network data on the topic of diabetes. A corpus of 884 text posts that contained positive or negative opinions were extracted from 50 diabetes accounts. We wanted to obtain the attitudes of people and their friends to produce a new network. In this new network, people will have the same attitudes on the same topics. Finding people with the same attitude in the network requires calculation of the degree of sentiment similarity. In this paper, to calculate sentiment similarity, we used average precision based on Karhunen–Loéve transform, which is widely performed to analyse data in many fields [<xref ref-type="bibr" rid="bibr14-0165551513480308">14</xref>].</p>
<p>The remainder of this paper is organized as follows. A literature review is presented in Section 2. Techniques and experiment processes of feature selection are described in Section 3. Section 4 is the result of sentiment similarity of Chinese micro-blog texts. We talk about conclusions and future works in the last section.</p>
</sec>
<sec id="section2-0165551513480308">
<title>2. Literature review</title>
<sec id="section3-0165551513480308">
<title>2.1. Sentiment classification</title>
<p>Text mining tries to solve the crisis of information overload by combining techniques from data mining, machine learning, natural language processing, information retrieval and knowledge management. An important part of text information mining is to find out what other people think and what are their opinions and emotions. Therefore, there has been an explostion of interest in opinion mining or sentiment analysis in the literature. David and Pinch published a paper in which the term ‘opinion mining’ appeared, to explain the popularity of the term within web communities in 2003 [<xref ref-type="bibr" rid="bibr15-0165551513480308">15</xref>]. For a recent survey of opinion mining, see the paper of Pang and Lee, who summarize the techniques and approaches of this field [<xref ref-type="bibr" rid="bibr1-0165551513480308">1</xref>].</p>
<p>One important issue of sentiment analysis is to classify different sentiments. Sentiment classification studies attempt to determine whether a text is objective or subjective, and whether a subjective text contains positive or negative sentiments [<xref ref-type="bibr" rid="bibr7-0165551513480308">7</xref>]. The common methods try to classify sentiment into positive and negative [<xref ref-type="bibr" rid="bibr8-0165551513480308">8</xref>, <xref ref-type="bibr" rid="bibr11-0165551513480308">11</xref>, <xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>, <xref ref-type="bibr" rid="bibr16-0165551513480308">16</xref><xref ref-type="bibr" rid="bibr17-0165551513480308"/><xref ref-type="bibr" rid="bibr18-0165551513480308"/><xref ref-type="bibr" rid="bibr19-0165551513480308"/>–<xref ref-type="bibr" rid="bibr20-0165551513480308">20</xref>]. This is called binary sentiment classification, that is, classifying each document with two labels. Moreover, Koppel and Schler find that neutral examples are crucial in learning polarity and permit accurate classification [<xref ref-type="bibr" rid="bibr21-0165551513480308">21</xref>]. When researchers want to gain attitudes from social media, they consider two polarity sentiments and refine data by removing noise [<xref ref-type="bibr" rid="bibr22-0165551513480308">22</xref>, <xref ref-type="bibr" rid="bibr23-0165551513480308">23</xref>]. Some studies have attempted to classify emotions into multiple types, including love, happiness, sadness, anger and horror [<xref ref-type="bibr" rid="bibr24-0165551513480308">24</xref>]. Therefore, polarity of sentiment is changed into multiclass problems. Generally, the category set consists of strong positive, positive, neutral, negative and strong negative [<xref ref-type="bibr" rid="bibr25-0165551513480308">25</xref>].</p>
</sec>
<sec id="section4-0165551513480308">
<title>2.2. Sentiment similarity</title>
<p>Part of the common approach is using a sentiment dictionary to find the sentiment words and sentences in sentiment searching. In sentiment analysis, several sentiment dictionaries are generally used to post words values. <italic>SentiWordNet</italic> gives each synset of <italic>WordNet</italic> three sentiment values. <italic>SenticNet</italic> was developed by Cambria et al. and its concepts are labelled with sentiment values between −1 and +1. <italic>WordNet-Affect</italic> marks sentiment labels at some synsets in <italic>WordNet</italic> [<xref ref-type="bibr" rid="bibr26-0165551513480308">26</xref>]. Ku et al. create a Chinese sentiment dictionary, <italic>The National Taiwan University Sentiment Dictionary</italic> (<italic>NTUSD</italic>), with 11,088 Chinese words. <italic>HowNet-VSA</italic> is a sentiment dictionary that can be used with both Chinese and English contents [<xref ref-type="bibr" rid="bibr1-0165551513480308">1</xref>]. Also, there are two English dictionaries with no labels called <italic>WeFeelFine</italic> dictionary and <italic>NELL</italic> dictionary [<xref ref-type="bibr" rid="bibr26-0165551513480308">26</xref>].</p>
<p>Sentiment similarity has not received enough attention to date. Turney and Littman proposed sentiment similarity of word pairs to calculate semantic similarity [<xref ref-type="bibr" rid="bibr27-0165551513480308">27</xref>]. Hassan and Radev created a graph-based method using <italic>WordNet</italic> similarity measures [<xref ref-type="bibr" rid="bibr28-0165551513480308">28</xref>]. Marneffe et al. used sentiment orientation of the adjectives to calculate the probability of rating [<xref ref-type="bibr" rid="bibr29-0165551513480308">29</xref>]. Those works focused on semantic similarity measures that used latent semantic analysis, point-wise mutual information and <italic>WordNet</italic>-based similarity.</p>
<p>In several papers, the researchers used sentiment similarity instead of classes to analyse users’ opinions [<xref ref-type="bibr" rid="bibr7-0165551513480308">7</xref>, <xref ref-type="bibr" rid="bibr14-0165551513480308">14</xref>, <xref ref-type="bibr" rid="bibr30-0165551513480308">30</xref>]. Sentiment similarity was used to identify the authors based on their writing styles [<xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>]. According to Abbasi et al., the new stylometric similarity detection techniques can be applied to assess the degree of similarity between individuals based on writing style. The results were better than those of other techniques such as principal component analysis, <italic>N</italic>-gram models, Markov models and cross entropy [<xref ref-type="bibr" rid="bibr14-0165551513480308">14</xref>]. They used Karhunen–Loéve transforms to assess the degree of similarity and a pattern disruption mechanism to determine dissimilarity. Owing to its better performance, we use this technique to calculate sentiment similarity in our experiment.</p>
</sec>
<sec id="section5-0165551513480308">
<title>2.3. Sentiment analysis features</title>
<p>The sentiment analysis features for social media proposed in previous works can be divided into four types as follows.</p>
<list id="list1-0165551513480308" list-type="order">
<list-item>
<p>Lexical features, including character-based and word-based features, such as total number of digit characters, space characters, total different words, word-length frequency and frequency of once and twice occurring words [<xref ref-type="bibr" rid="bibr7-0165551513480308">7</xref>,<xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>]. When it comes to Chinese text, total number of non-Chinese characters and average sentence length in terms of words are added in this paper. In total, we chose seven more important lexical features for Chinese micro-blogs.</p>
</list-item>
<list-item>
<p>Syntactic features, including punctuation, <italic>N</italic>-gram and part-of-speech. Punctuation frequency is combined into feature sets and can improve the performance of user identification [<xref ref-type="bibr" rid="bibr31-0165551513480308">31</xref>]. Some kinds of punctuation are very important in sentiment analysis, such as the exclamation and question marks. Unigrams, bigrams and trigrams are commonly used as features in previous works. Some <italic>N</italic>-grams worked better than others for the produce-review and movie review polarity classification [<xref ref-type="bibr" rid="bibr8-0165551513480308">8</xref>, <xref ref-type="bibr" rid="bibr32-0165551513480308">32</xref>]. Part-of-speech (POS) is the features set for counts of the numbers of nouns, verbs, adjectives, and any other parts of speech. We count 16 types of Chinese POS tags in this paper.</p>
</list-item>
<list-item>
<p>Content-specific features, including content-specific words, function words and structural features. Content-specific words are used to discriminate posted topics as users may use different words on specific topics [<xref ref-type="bibr" rid="bibr13-0165551513480308">13</xref>]. For this reason, special words can provide a clue about the user. For example, a diabetes doctor may use ‘type I’, ‘type II’ and ‘blood sugar’ as his keywords. Different function words were shown to have good discrimination in many studies [<xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>, <xref ref-type="bibr" rid="bibr33-0165551513480308">33</xref>]. However, there is no generally accepted set of function words because they have different discriminating power in different fields. Structural features express the writing layout. DeVel introduced several structural features for email [<xref ref-type="bibr" rid="bibr34-0165551513480308">34</xref>]. Zheng revised some features and formed 14 structural features for online messages [<xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>]. According to the traits of micro-blogs, we adopted eight structural features, such as number of other users, number of images used and number of URLs used.</p>
</list-item>
<list-item>
<p>Micro-blog features, including the emoticons and abbreviations. Emoticons are one of the most powerful signals to differentiate polar positive and negative messages [<xref ref-type="bibr" rid="bibr35-0165551513480308">35</xref>]. The English emoticon data set was created by Go et al. for a project at Stanford University by collecting positive and negative emoticons of tweets [<xref ref-type="bibr" rid="bibr23-0165551513480308">23</xref>]. Pak et al. queried Twitter for two types of emotions: happy emoticons such as ‘:-)’, ‘:)’, ‘=)’ and ‘:D’ and sad emoticons such as ‘:-(’, ‘:(’, ‘=(’ and ‘;(’ [<xref ref-type="bibr" rid="bibr36-0165551513480308">36</xref>]. An abbreviation is a shortened form of a word or phrase for quick writing. For example, the meaning of ‘OMG’ is ‘Oh My God’. It can be treated as individual token. English abbreviations are easily found in Internet Lingo Dictionary [<xref ref-type="bibr" rid="bibr37-0165551513480308">37</xref>]. As Chinese text has no such abbreviations, we do not use this feature in Chinese micro-blogs.</p>
</list-item>
</list>
</sec>
</sec>
<sec id="section6-0165551513480308">
<title>3. Techniques and experiments</title>
<sec id="section7-0165551513480308">
<title>3.1. Data</title>
<p>We collected data from one Chinese micro-blog, Sina Weibo, which has almost 300 million users. It is the first and the biggest micro-blog website in China. We obtained the data Sina API. People share information and opinions on this website constantly. Healthcare is a promising and useful area to provide convenient service via social media. Increasing numbers of doctors and hospitals are opening their micro-blog accounts to help patients. Among those patients, a relative majority are suffering from diabetes. Therefore, we searched diabetes as our topic and chose 50 users as our research seeds, including diabetes hospital accounts, prominant diabetes doctors’ validated accounts and diabetes magazines’ validated accounts. They all have many followers who want to obtain diabetes information and treatments. Account information for the 50 users is listed in <xref ref-type="table" rid="table1-0165551513480308">Table 1</xref>. We collected 884 texts about diabetes among their messages from the date that Sina Weibo was released until 30 April 2012. For conveniently tackling sentiment analysis, the number of positive opinions was equal to −1 in our dataset.</p>
<table-wrap id="table1-0165551513480308" position="float">
<label>Table 1.</label>
<caption>
<p>Collected diabetes micro-blogging information.</p>
</caption>
<graphic alternate-form-of="table1-0165551513480308" xlink:href="10.1177_0165551513480308-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Number of followers</th>
<th align="left">Number of followed</th>
<th align="left">Number of micro-blogs</th>
<th align="left">Average number followed</th>
</tr>
</thead>
<tbody>
<tr>
<td>27,872</td>
<td>444,358</td>
<td>48,594</td>
<td>8887</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section8-0165551513480308">
<title>3.2. Feature selection</title>
<sec id="section9-0165551513480308">
<title>3.2.1. Feature set</title>
<p>Syntactic, semantic, link-based, and content-specific features are four feature categories that have been used in previous sentiment analysis. For example, syntactic, lexical and content-specific features have been successfully applied to English and Chinese [<xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>, <xref ref-type="bibr" rid="bibr13-0165551513480308">13</xref>, <xref ref-type="bibr" rid="bibr30-0165551513480308">30</xref>]. Zheng et al. [<xref ref-type="bibr" rid="bibr12-0165551513480308">12</xref>] created 270 write–print features for online messages for English and 117 for Chinese. However, they do not use POS tags as features. More researches have use POS tag in English sentiment analysis. In this paper, we wanted to find out whether POS tags make contributions in Chinese text. Two useful tags post software and word segmentation softwares, AntConc and MyTxtSegTag (<ext-link ext-link-type="uri" xlink:href="http://www.corpus4u.org/">http://www.corpus4u.org/</ext-link>), were used to tackle Chinese texts in our experiments. We also identified a set of <italic>N</italic>-grams, including unigrams, bigrams and trigrams, using AntConc. Emoticons are commonly used in micro-blogs to express emotions. Related works show that emoticons are one of the most powerful signals to differentiate positive from negative messages [<xref ref-type="bibr" rid="bibr35-0165551513480308">35</xref>, <xref ref-type="bibr" rid="bibr37-0165551513480308">37</xref>]. We selected three positive and three negative emoticons as features in our dataset. Finally, we used a 126 features as the original feature set for classification experiments. All of the features are shown in <xref ref-type="table" rid="table2-0165551513480308">Table 2</xref>.</p>
<table-wrap id="table2-0165551513480308" position="float">
<label>Table 2.</label>
<caption>
<p>Adopted feature set in the classification experiments.</p>
</caption>
<graphic alternate-form-of="table2-0165551513480308" xlink:href="10.1177_0165551513480308-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Category</th>
<th align="left">Features</th>
<th align="left">Description</th>
<th align="left">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lexical features</td>
<td>1. Total number of words (<italic>M</italic>)<break/>2. Total number of characters/<italic>C</italic><break/>3. Total number of non-Chinese characters/<italic>C</italic><break/>4. Average sentence length in terms of word<break/>5. Total number of different words/<italic>M</italic></td>
<td/>
<td>F1</td>
</tr>
<tr>
<td/>
<td>6. Hapax legomena</td>
<td>Frequency of once-occurring words</td>
<td/>
</tr>
<tr>
<td/>
<td>7. Hapax dislegomena</td>
<td>Frequency of twice-occurring words</td>
<td/>
</tr>
<tr>
<td/>
<td>8–10. Frequency of <italic>N</italic>-grams</td>
<td>Number of unigrams, bigrams and trigrams</td>
<td>F2</td>
</tr>
<tr>
<td>Syntactic features</td>
<td>11–19. Frequency of punctuation (nine features)</td>
<td>‘,’, ‘ˆ’, ‘.’, ‘?’, ‘!’, ‘:’, ‘ˆˆˆ’, ‘;’, ‘′’</td>
<td/>
</tr>
<tr>
<td/>
<td>20–35. Frequency of POS tags (16 features)</td>
<td>Numbers of nouns, verbs, adjectives, adverbs exclamations, etc.</td>
<td/>
</tr>
<tr>
<td>Content-specific features</td>
<td>36–97. Frequency of function words (62 features)<break/>98–112. Frequency of content specific keywords (15 features)<break/>113. Total number of sentences<break/>114. Total number of posts<break/>115. Number of sentences per post<break/>116. Number of characters per post<break/>117. Number of words per post<break/>118. Number of other users<break/>119. Number of images used<break/>120. Number of URLs used</td>
<td>The lists of function words and content-specific keywords are supplied in the <xref ref-type="app" rid="app1-0165551513480308">Appendix</xref></td>
<td>F3</td>
</tr>
<tr>
<td>Emoticon features</td>
<td>121–126. Positive and negative emoticons (six features)</td>
<td>The list of emoticons is supplied in the <xref ref-type="app" rid="app1-0165551513480308">Appendix</xref></td>
<td>F4</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>After we counted the number of features, several of them were equal to zero. We deleted those features that made no contribution to classifications and 100 features were left. Among these, we picked the features that were relevant to Sina Weibo text. Some features may be redundant, which will reduce the prediction accuracy. Therefore, feature selection should be undertaken to gain a subset of features that are relevant to our target. In this work, we used a new method to operate feature selection. As we know that both information gain and support vector machine have better performance than other methods in different cases, we first used two methods to select features and second blended two selected feature sets together to determine an optimum feature set. We explain this method in the following sections.</p>
</sec>
<sec id="section10-0165551513480308">
<title>3.2.2. Information gain</title>
<p>Information gain performs better on feature selection in text categorization than other methods, such as document frequency and mutual information [<xref ref-type="bibr" rid="bibr30-0165551513480308">30</xref>]. Information gain is a common approach to selecting features [<xref ref-type="bibr" rid="bibr14-0165551513480308">14</xref>]. In previous research, features were selected with information gain &gt;0.0025. In our experiment, we assumed the class to be <italic>C<sub>i</sub></italic> (<italic>C</italic><sub>1</sub> = positive and <italic>C</italic><sub>2</sub> = negative), and the probability of the positive class to be equal to −1. The input feature is <italic>F<sub>j</sub></italic> (<italic>F</italic><sub>1</sub>, <italic>F</italic><sub>2</sub>, <italic>…, F</italic><sub>m</sub>). Then a simplified version of information gain for our issue is given below:</p>
<p>
<disp-formula id="disp-formula1-0165551513480308">
<mml:math display="block" id="math1-0165551513480308">
<mml:mrow>
<mml:mi>I</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub>
<mml:mi>F</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow>
<mml:msub>
<mml:mi>F</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:munderover>
<mml:mrow>
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub>
<mml:mi>C</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mstyle><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub>
<mml:mi>g</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub>
<mml:mi>C</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:munderover>
<mml:mrow>
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msub>
<mml:mi>C</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi><mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mstyle><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub>
<mml:mi>g</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:msub>
<mml:mi>C</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi><mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0165551513480308" xlink:href="10.1177_0165551513480308-eq1.tif"/></disp-formula>
</p>
<p>where <inline-formula id="inline-formula1-0165551513480308"><mml:math display="inline" id="math2-0165551513480308"><mml:mrow><mml:mi>IG</mml:mi></mml:mrow></mml:math></inline-formula> denotes information gain for feature <inline-formula id="inline-formula2-0165551513480308"><mml:math display="inline" id="math3-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>; <inline-formula id="inline-formula3-0165551513480308"><mml:math display="inline" id="math4-0165551513480308"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes entropy across sentiment classes <italic>C</italic>; <inline-formula id="inline-formula4-0165551513480308"><mml:math display="inline" id="math5-0165551513480308"><mml:mrow><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the <italic>j</italic>th feature conditional entropy; and <inline-formula id="inline-formula5-0165551513480308"><mml:math display="inline" id="math6-0165551513480308"><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>ij</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the instance for which the value of a feature is <italic><inline-formula id="inline-formula6-0165551513480308"><mml:math display="inline" id="math7-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></italic> and the value of the class is <italic>i</italic>.</p>
<p>We calculated feature information gains and sorted them using WEKA data mining tool. Then, 11 features are selected whose information gain was &gt;0.0025, as shown in <xref ref-type="table" rid="table3-0165551513480308">Table 3</xref>.</p>
<table-wrap id="table3-0165551513480308" position="float">
<label>Table 3.</label>
<caption>
<p>Information gain ranked attributes.</p>
</caption>
<graphic alternate-form-of="table3-0165551513480308" xlink:href="10.1177_0165551513480308-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Rank</th>
<th align="left">Value of <italic>IG</italic>
</th>
<th align="left">Attributes</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>0.1896</td>
<td>69</td>
</tr>
<tr>
<td>2</td>
<td>0.1365</td>
<td>6</td>
</tr>
<tr>
<td>3</td>
<td>0.1234</td>
<td>90</td>
</tr>
<tr>
<td>4</td>
<td>0.1198</td>
<td>85</td>
</tr>
<tr>
<td>5</td>
<td>0.1093</td>
<td>38</td>
</tr>
<tr>
<td>6</td>
<td>0.1081</td>
<td>14</td>
</tr>
<tr>
<td>7</td>
<td>0.1001</td>
<td>37</td>
</tr>
<tr>
<td>8</td>
<td>0.085</td>
<td>100</td>
</tr>
<tr>
<td>9</td>
<td>0.0738</td>
<td>79</td>
</tr>
<tr>
<td>10</td>
<td>0.0519</td>
<td>50</td>
</tr>
<tr>
<td>11</td>
<td>0.0519</td>
<td>80</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section11-0165551513480308">
<title>3.2.3. Support vector machine</title>
<p>Because the support vector machine (SVM) is a very strong classifier and outperforms other classifiers in many fields, we used this method in our experiment to achieve a better feature set. Support vectors are the important vectors to make a distinction between two classifications. For two-class classification problems, the basic SVM concepts are briefly described below [<xref ref-type="bibr" rid="bibr38-0165551513480308">38</xref>].</p>
<p>Given a training set of instance–label pairs (<italic>x<sub>i</sub>, y<sub>i</sub></italic>), <italic>i</italic> = 1, 2, …, <italic>m</italic>, where <inline-formula id="inline-formula7-0165551513480308"><mml:math display="inline" id="math8-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula id="inline-formula8-0165551513480308"><mml:math display="inline" id="math9-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and with the non-negative slack variables <inline-formula id="inline-formula9-0165551513480308"><mml:math display="inline" id="math10-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the data points can be correctly classified by</p>
<p>
<disp-formula id="disp-formula2-0165551513480308">
<mml:math display="block" id="math11-0165551513480308">
<mml:mrow>
<mml:mo>&lt;</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>·</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>&gt;</mml:mo>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>≥</mml:mo>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>for</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0165551513480308" xlink:href="10.1177_0165551513480308-eq2.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula3-0165551513480308">
<mml:math display="block" id="math12-0165551513480308">
<mml:mrow>
<mml:mo>&lt;</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>·</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>&gt;</mml:mo>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>≥</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>for</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-0165551513480308" xlink:href="10.1177_0165551513480308-eq3.tif"/>
</disp-formula>
</p>
<p>The support vector technique solves an optimization problem to find the minimum number of training errors.</p>
<p>
<disp-formula id="disp-formula4-0165551513480308">
<mml:math display="block" id="math13-0165551513480308">
<mml:mrow>
<mml:munder>
<mml:mrow>
<mml:mtext>Min</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>ξ</mml:mi>
</mml:mrow>
</mml:munder>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:msup>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:mi>w</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi>C</mml:mi>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-0165551513480308" xlink:href="10.1177_0165551513480308-eq4.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula5-0165551513480308">
<mml:math display="block" id="math14-0165551513480308">
<mml:mrow>
<mml:mtext>Subject</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>to</mml:mtext>
<mml:mo>:</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>&lt;</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>·</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>&gt;</mml:mo>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>≥</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>≥</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-0165551513480308" xlink:href="10.1177_0165551513480308-eq5.tif"/>
</disp-formula>
</p>
<p>SVM also can be applied to select features. It gives a couple of support vectors that discriminate different classes with minimum training errors, as shown in <xref ref-type="fig" rid="fig1-0165551513480308">Figure 1</xref> [<xref ref-type="bibr" rid="bibr39-0165551513480308">39</xref>]. We rank the support vectors by their importance in SVM. Therefore, important features are gained in a new way. According to this method, in our experiment, we obtain support vectors lists which are SVM selected attributes: <bold>69</bold>, <bold>6</bold>, <bold>37</bold>, 74, 5, 2, <bold>80</bold>, 28, 10, <bold>100</bold>, 15, 83, <bold>38</bold>, 17, 54, <bold>79</bold>, 32, <bold>90</bold>, 97, 81, 18, 66, 72, 30, 99, 47, 43, 49, 36, 9, 7, 88, 92, 21, 24, 8, 82, 20, 3, 73, 91, 59, 68, 25, 62, 46, 13, <bold>50</bold>, 4, 34, 23, 63, 55, 26, 27, 67, 60, 35, 87, 71, 40, 84, 41, 42, 70, 22, <bold>85</bold>, 39, 57, 51, 53, 16, 96, 86, 58, 76, 44, 48, 11, 93, 61, 94, 56, 12, 19, <bold>14</bold>, 89, 75, 1, 95, 33, 77, 45, 31, 98, 64, 78, 52, 29, 65. Attributes in bold appeared in both the <italic>IG</italic> selected set and the SVM selected set.</p>
<fig id="fig1-0165551513480308" position="float">
<label>Figure 1.</label>
<caption><p>Example for selecting better support vectors by applying SVM on testing data.</p></caption>
<graphic xlink:href="10.1177_0165551513480308-fig1.tif"/></fig>
<p>Both information gain and SVM have better performances in many cases. They have different strong points that can influence classification accuracy. Therefore, we blended these two methods to select Chinese text features. We combined those features with that obtained from information gain. Different methods may have diverse feature selection sets. We chose more features from SVM method based on features obtained from information gain.</p>
<p>When using SVM, choosing the optimal input feature subset and setting the best kernel parameters are crucial. These two problems influence the SVM classification accuracy. The kernel parameters, <italic>C</italic> and <italic>γ</italic>, can be obtained using LIBSVM tools (<ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">http://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>). In our experiments, we choose 10-fold cross-validation. We used IG-11, SVM-18, IGSVM-21, IGSVM-48, IGSVM-67, IGSVM-86 and Original-100 to express the methods and numbers of features, respectively. To compare with different classification methods, we used the naive Bayes classifier as a control group in our experiments. Precision, recall and <italic>F</italic>-mean are common metrics to measure the classification accuracy.</p>
<p>
<disp-formula id="disp-formula6-0165551513480308">
<mml:math display="block" id="math15-0165551513480308">
<mml:mrow>
<mml:mi>F</mml:mi>
<mml:mo>-</mml:mo>
<mml:mtext>mean</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:mtext>precision</mml:mtext>
<mml:mo>×</mml:mo>
<mml:mtext>recall</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>precision</mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext>recall</mml:mtext>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula6-0165551513480308" xlink:href="10.1177_0165551513480308-eq6.tif"/>
</disp-formula>
</p>
<p>We used those three metrics to discover whether features selected by our new method outperform other features. <xref ref-type="table" rid="table4-0165551513480308">Tables 4</xref><xref ref-type="table" rid="table5-0165551513480308"/>–<xref ref-type="table" rid="table6-0165551513480308">6</xref> show the classification accuracies when using J48, SVM and naive Bayes respectively. When using LIBSVM tools, the optimum parameters are: −G 1.220703125E −C 2048.0, −G 3.05517578125E − 5 −C 8192.0, −G 3.05517578125E −C 8192.0, −G 3.0517578125E −5 −C 32768.0, −G 1.220703125E − 4 −C 2048, −G 3.05E −5 −C 8192.0 and −G 1.22E − 4 −C 1.0 for data IG-11, SVM-18, IGSVM-21, IGSVM-48, IGSVM-67, IGSVM-86 and Original-100, respectively.</p>
<table-wrap id="table4-0165551513480308" position="float">
<label>Table 4.</label>
<caption>
<p>Classification accuracy when using J48 as classifier.</p>
</caption>
<graphic alternate-form-of="table4-0165551513480308" xlink:href="10.1177_0165551513480308-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">J48</th>
<th align="left">TP rate</th>
<th align="left">FP rate</th>
<th align="left">Precision</th>
<th align="left">Recall</th>
<th align="left">
<italic>F</italic>-Measure</th>
</tr>
</thead>
<tbody>
<tr>
<td>IG-11</td>
<td>0.8</td>
<td>0.2</td>
<td>0.802</td>
<td>0.8</td>
<td>0.8</td>
</tr>
<tr>
<td>SVM-18</td>
<td>0.71</td>
<td>0.29</td>
<td>0.711</td>
<td>0.71</td>
<td>0.71</td>
</tr>
<tr>
<td>IGSVM-21</td>
<td>0.82</td>
<td>0.18</td>
<td>
<bold>0.821</bold>
</td>
<td>
<bold>0.82</bold>
</td>
<td>
<bold>0.82</bold>
</td>
</tr>
<tr>
<td>IGSVM-48</td>
<td>0.7</td>
<td>0.3</td>
<td>0.701</td>
<td>0.7</td>
<td>0.7</td>
</tr>
<tr>
<td>IGSVM-67</td>
<td>0.7</td>
<td>0.3</td>
<td>0.7</td>
<td>0.7</td>
<td>0.7</td>
</tr>
<tr>
<td>IGSVM-86</td>
<td>0.71</td>
<td>0.29</td>
<td>0.71</td>
<td>0.71</td>
<td>0.71</td>
</tr>
<tr>
<td>Original-100</td>
<td>0.76</td>
<td>0.24</td>
<td>0.76</td>
<td>0.76</td>
<td>0.76</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table5-0165551513480308" position="float">
<label>Table 5.</label>
<caption>
<p>Classification accuracy when using SVM as classifier.</p>
</caption>
<graphic alternate-form-of="table5-0165551513480308" xlink:href="10.1177_0165551513480308-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">SVM</th>
<th align="left">TP rate</th>
<th align="left">FP rate</th>
<th align="left">Precision</th>
<th align="left">Recall</th>
<th align="left">
<italic>F</italic>-Measure</th>
</tr>
</thead>
<tbody>
<tr>
<td>IG-11</td>
<td>0.89</td>
<td>0.11</td>
<td>
<bold>0.891</bold>
</td>
<td>
<bold>0.89</bold>
</td>
<td>
<bold>0.89</bold>
</td>
</tr>
<tr>
<td>SVM-18</td>
<td>0.89</td>
<td>0.11</td>
<td>0.89</td>
<td>0.89</td>
<td>0.89</td>
</tr>
<tr>
<td>IGSVM-21</td>
<td>0.89</td>
<td>0.11</td>
<td>
<bold>0.891</bold>
</td>
<td>
<bold>0.89</bold>
</td>
<td>
<bold>0.89</bold>
</td>
</tr>
<tr>
<td>IGSVM-48</td>
<td>0.89</td>
<td>0.11</td>
<td>
<bold>0.891</bold>
</td>
<td>
<bold>0.89</bold>
</td>
<td>
<bold>0.89</bold>
</td>
</tr>
<tr>
<td>IGSVM-67</td>
<td>0.85</td>
<td>0.15</td>
<td>0.85</td>
<td>0.85</td>
<td>0.85</td>
</tr>
<tr>
<td>IGSVM-86</td>
<td>0.87</td>
<td>0.13</td>
<td>0.87</td>
<td>0.87</td>
<td>0.87</td>
</tr>
<tr>
<td>Original-100</td>
<td>0.83</td>
<td>0.17</td>
<td>0.83</td>
<td>0.83</td>
<td>0.83</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table6-0165551513480308" position="float">
<label>Table 6.</label>
<caption>
<p>Classification accuracy when using naive Bayes as classifier.</p>
</caption>
<graphic alternate-form-of="table6-0165551513480308" xlink:href="10.1177_0165551513480308-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Naive Bayes</th>
<th align="left">TP rate</th>
<th align="left">FP rate</th>
<th align="left">Precision</th>
<th align="left">Recall</th>
<th align="left">
<italic>F</italic>-Measure</th>
</tr>
</thead>
<tbody>
<tr>
<td>IG-11</td>
<td>0.83</td>
<td>0.17</td>
<td>0.863</td>
<td>0.83</td>
<td>0.826</td>
</tr>
<tr>
<td>SVM-18</td>
<td>0.87</td>
<td>0.13</td>
<td>
<bold>0.882</bold>
</td>
<td>
<bold>0.87</bold>
</td>
<td>
<bold>0.869</bold>
</td>
</tr>
<tr>
<td>IGSVM-21</td>
<td>0.87</td>
<td>0.13</td>
<td>
<bold>0.882</bold>
</td>
<td>
<bold>0.87</bold>
</td>
<td>
<bold>0.869</bold>
</td>
</tr>
<tr>
<td>IGSVM-48</td>
<td>0.85</td>
<td>0.15</td>
<td>0.868</td>
<td>0.85</td>
<td>0.848</td>
</tr>
<tr>
<td>IGSVM-67</td>
<td>0.83</td>
<td>0.17</td>
<td>0.841</td>
<td>0.83</td>
<td>0.829</td>
</tr>
<tr>
<td>IGSVM-86</td>
<td>0.76</td>
<td>0.24</td>
<td>0.79</td>
<td>0.76</td>
<td>0.754</td>
</tr>
<tr>
<td>Original-100</td>
<td>0.71</td>
<td>0.29</td>
<td>0.745</td>
<td>0.71</td>
<td>0.699</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>When using J48 to classify seven data sets, the best result was achieved on the IGSVM-21 feature set. Because we used the LIBSVM tool with optimum parameters to classify those data sets, their <italic>F</italic>-means were difficult to distinguish in the first four datasets and the last three results were a little low. The SVM classifier outperformed the other two classifiers. The <italic>F</italic>-mean of IGSVM-21 was one of the top four results. Considering the naive Bayes classifier, SVM-18 and IGSVM-21 performed much better than the other datasets. Although the classification accuracy of IG-11 and IGSVM-48 was better when using SVM classifier, their results were worse than IGSVM-21 when using J48 and SVM classifiers. Therefore, we can say that IGSVM-21 outperformed the other feature sets when using three different classifiers. We chose IGSVM-21 as the final feature set.</p>
</sec>
</sec>
<sec id="section12-0165551513480308">
<title>3.3. Feature comparison</title>
<p>Following feature selection, we determined which types of feature made a greater contribution to classification. As can be seen in <xref ref-type="table" rid="table2-0165551513480308">Table 2</xref>, we created four feature types, F1–F4, which denote features. The first feature set (F1) contains lexical features only. Syntactic features are added to the first feature set to form the second feature set (F1 + F2). The third feature set (F1 + F2 + F3) contains both content-specific features and the second feature set. The last feature set contains all features (F1 + F2 + F3 + F4). We examined the effect of adding new features to existing ones and determined which type of feature made a greater contribution to classification. We adopted J48, SVM and naive Bayes as the classifiers. Precision, recall, <italic>F</italic>-mean and receiver operating characteristic (ROC) area are commonly used as metrics. Meanwhile, growth of the <italic>F</italic>-mean is calculated to embody <italic>F</italic>-mean changes when adding new features to existing ones.</p>
<p>From <xref ref-type="table" rid="table7-0165551513480308">Table 7</xref>, we determined that SVM outperformed J48 and naive Bayes classifiers. This means that SVM is the best classifier in our experiment to classify Chinese social media sentiments. When using J48 as classifier, the results of the <italic>F</italic>-mean kept increasing as more types of features were added. The accuracy achieved was the best as all features were added. That is to say that four types of features were very useful. The values of <italic>F</italic>-mean growth were 0.025, 0.12 and 0.05, respectively, when adding F2, F3 and F4 to existing features. We obtained a high growth with putting F3 into the feature set. Therefore, content-specific features such as function words and content-specific keywords are much more important to classify different sentiments than syntactic feature and emoticons.</p>
<table-wrap id="table7-0165551513480308" position="float">
<label>Table 7.</label>
<caption>
<p>Accuracy for different feature sets and three different classifiers.</p>
</caption>
<graphic alternate-form-of="table7-0165551513480308" xlink:href="10.1177_0165551513480308-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Classifiers</th>
<th align="left">Feature sets</th>
<th align="left">Precision</th>
<th align="left">Recall</th>
<th align="left">
<italic>F</italic>-Measure</th>
<th align="left">Growth</th>
<th align="left">ROC area</th>
</tr>
</thead>
<tbody>
<tr>
<td>J48</td>
<td>F1</td>
<td>0.637</td>
<td>0.63</td>
<td>0.625</td>
<td>—</td>
<td>0.685</td>
</tr>
<tr>
<td/>
<td>F1 + F2</td>
<td>0.651</td>
<td>0.65</td>
<td>0.65</td>
<td>0.025</td>
<td>0.643</td>
</tr>
<tr>
<td/>
<td>F1 + F2 + F3</td>
<td>0.77</td>
<td>0.77</td>
<td>0.77</td>
<td>
<bold>0.12</bold>
</td>
<td>0.753</td>
</tr>
<tr>
<td/>
<td>F1 + F2 + F3 + F4</td>
<td>0.821</td>
<td>0.82</td>
<td>0.82</td>
<td>0.05</td>
<td>0.808</td>
</tr>
<tr>
<td>SVM</td>
<td>F1</td>
<td>0.81</td>
<td>0.81</td>
<td>0.81</td>
<td>—</td>
<td>0.81</td>
</tr>
<tr>
<td/>
<td>F1 + F2</td>
<td>0.89</td>
<td>0.89</td>
<td>0.89</td>
<td>
<bold>0.08</bold>
</td>
<td>0.89</td>
</tr>
<tr>
<td/>
<td>F1 + F2 + F3</td>
<td>0.891</td>
<td>0.89</td>
<td>0.89</td>
<td>0</td>
<td>0.89</td>
</tr>
<tr>
<td/>
<td>F1 + F2 + F3 + F4</td>
<td>0.891</td>
<td>0.89</td>
<td>0.89</td>
<td>0</td>
<td>0.89</td>
</tr>
<tr>
<td>Naive Bayes</td>
<td>F1</td>
<td>0.833</td>
<td>0.82</td>
<td>0.818</td>
<td>—</td>
<td>0.883</td>
</tr>
<tr>
<td/>
<td>F1 + F2</td>
<td>0.854</td>
<td>0.83</td>
<td>0.827</td>
<td>0.009</td>
<td>0.936</td>
</tr>
<tr>
<td/>
<td>F1 + F2 + F3</td>
<td>0.882</td>
<td>0.87</td>
<td>0.869</td>
<td>
<bold>0.042</bold>
</td>
<td>0.942</td>
</tr>
<tr>
<td/>
<td>F1 + F2 + F3 + F4</td>
<td>0.882</td>
<td>0.87</td>
<td>0.869</td>
<td>0</td>
<td>0.941</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Considering the SVM classifier, the accuracy was best when adding syntactic features to lexical features. There was an 8% improvement in accuracy. The results plateaud even with the addition of content-specific features and emoticon features. Therefore, syntactic features provide an accuracy increase compared with lexical features. The remaining features are all the same under a certain classifier.</p>
<p>The results of the naive Bayes classifier are similar to those of the J48 classifier. The <italic>F</italic>-mean results increase with the addition of new features into old feature sets until the last feature set. Content-specific features have higher accuracy growth (4.2%) than syntactic features (0.9%). The result of all features is equal to the result of the third feature set, which means that emoticon features have no effect in classifying the sentiment when using naive Bayes classifier.</p>
</sec>
</sec>
<sec id="section13-0165551513480308" sec-type="results">
<title>4. Results of KLT</title>
<p>We used the Karhunen–Loéve transform (KLT) and average distances of positive and negative texts to detect sentiment similarity. The Karhunen–Loéve transform, also known as the Hotelling transform, is widely used in data analysis in many fields. It is a minimum distortion transform under the measurement of mean square error. Because of this character, the KLT is known as the best transform. Abbasi et al. have used this technique to detect stylometric similarity in electronic markets [<xref ref-type="bibr" rid="bibr14-0165551513480308">14</xref>]. The KLT has advantages over other techniques such as principal component analysis and Markov models. Therefore, we used the KLT to assess the degree of similarity between each pair of diabetes accounts from micro-blogs. The procedure contins three steps:</p>
<list id="list2-0165551513480308" list-type="order">
<list-item>
<p>Let <inline-formula id="inline-formula10-0165551513480308"><mml:math display="inline" id="math16-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> be the eigenvector corresponding to the <italic>k</italic>th eigenvalue <inline-formula id="inline-formula11-0165551513480308"><mml:math display="inline" id="math17-0165551513480308"><mml:mrow><mml:msub><mml:mo>λ</mml:mo><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the covariance matrix <inline-formula id="inline-formula12-0165551513480308"><mml:math display="inline" id="math18-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, i.e.</p>
<p>
<disp-formula id="disp-formula7-0165551513480308">
<mml:math display="block" id="math19-0165551513480308">
<mml:mrow>
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:munder>
<mml:msub>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula7-0165551513480308" xlink:href="10.1177_0165551513480308-eq7.tif"/>
</disp-formula>
</p>
<p>It is expressed in matrix form:</p>
<p>
<disp-formula id="disp-formula8-0165551513480308">
<mml:math display="block" id="math20-0165551513480308">
<mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mo>⋯</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>σ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>⋯</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula8-0165551513480308" xlink:href="10.1177_0165551513480308-eq8.tif"/>
</disp-formula>
</p>
</list-item>
<list-item>
<p>As the covariance matrix <inline-formula id="inline-formula13-0165551513480308"><mml:math display="inline" id="math21-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mtext>T</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is Hermitian (symmetric if <bold><italic>x</italic></bold> is real), its eigenvectors are orthogonal. We can construct an <inline-formula id="inline-formula14-0165551513480308"><mml:math display="inline" id="math22-0165551513480308"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> unitary matrix <inline-formula id="inline-formula15-0165551513480308"><mml:math display="inline" id="math23-0165551513480308"><mml:mrow><mml:mi>Φ</mml:mi></mml:mrow></mml:math></inline-formula> satisfying:</p>
<p>
<disp-formula id="disp-formula9-0165551513480308">
<mml:math display="block" id="math24-0165551513480308">
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi>Φ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:mi>Φ</mml:mi>
<mml:mo>=</mml:mo>
<mml:mtext>I</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>i</mml:mtext>
<mml:mo>.</mml:mo>
<mml:mtext>e</mml:mtext>
<mml:mo>.</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msup>
<mml:mrow>
<mml:mi>Φ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>Φ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula9-0165551513480308" xlink:href="10.1177_0165551513480308-eq9.tif"/>
</disp-formula>
</p>
<p>The <italic>N</italic> eigen-equations <inline-formula id="inline-formula16-0165551513480308"><mml:math display="inline" id="math25-0165551513480308"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> above can be combined to be expressed as:</p>
<p>
<disp-formula id="disp-formula10-0165551513480308">
<mml:math display="block" id="math26-0165551513480308">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>Σ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>Φ</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>Φ</mml:mi>
<mml:mi>Λ</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula10-0165551513480308" xlink:href="10.1177_0165551513480308-eq10.tif"/>
</disp-formula>
</p>
<p>which can also be expressed in matrix form,</p>
<p>
<disp-formula id="disp-formula11-0165551513480308">
<mml:math display="block" id="math27-0165551513480308">
<mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mo>⋯</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>σ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>⋯</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
<mml:mo>⋯</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mo>⋯</mml:mo>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mo>⋯</mml:mo>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mn>0</mml:mn><mml:mo>⋯</mml:mo><mml:mn>0</mml:mn></mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mn>0</mml:mn><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mn>0</mml:mn><mml:mo>⋮</mml:mo></mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mo>⋮</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>⋱</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mo>⋯</mml:mo>
<mml:mn>0</mml:mn><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula11-0165551513480308" xlink:href="10.1177_0165551513480308-eq11.tif"/>
</disp-formula>
</p>
<p>where <inline-formula id="inline-formula17-0165551513480308"><mml:math display="inline" id="math28-0165551513480308"><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow></mml:math></inline-formula> is a diagonal matrix (Λ = <italic>diag</italic>(<inline-formula id="inline-formula18-0165551513480308"><mml:math display="inline" id="math29-0165551513480308"><mml:mrow><mml:msup><mml:mi>Φ</mml:mi><mml:mtext>T</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>Φ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>)). Multiplying by <inline-formula id="inline-formula19-0165551513480308"><mml:math display="inline" id="math30-0165551513480308"><mml:mrow><mml:msup><mml:mrow><mml:mi>Φ</mml:mi></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> on both sides, the covariance matrix <inline-formula id="inline-formula20-0165551513480308"><mml:math display="inline" id="math31-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> can be diagonalized as:</p>
<p>
<disp-formula id="disp-formula12-0165551513480308">
<mml:math display="block" id="math32-0165551513480308">
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi>Φ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:msub>
<mml:mrow>
<mml:mi>Σ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>Φ</mml:mi>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>Φ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:msub>
<mml:mrow>
<mml:mi>Σ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>Φ</mml:mi>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>Φ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mi>Φ</mml:mi>
<mml:mi>Λ</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>Λ</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula12-0165551513480308" xlink:href="10.1177_0165551513480308-eq12.tif"/>
</disp-formula>
</p>
</list-item>
<list-item>
<p>Now, given a vector <bold><italic>x</italic></bold>, we can define a unitary (orthogonal if <bold><italic>x</italic></bold> is real) Karhunen–Loéve transform of <bold><italic>x</italic></bold> as:</p>
<p>
<disp-formula id="disp-formula13-0165551513480308">
<mml:math display="block" id="math33-0165551513480308">
<mml:mrow>
<mml:mi>y</mml:mi>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mo>⋮</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>Φ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msup>
<mml:mi>x</mml:mi>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mo>⋮</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula13-0165551513480308" xlink:href="10.1177_0165551513480308-eq13.tif"/>
</disp-formula>
</p>
<p>where the <italic>i</italic>th <inline-formula id="inline-formula21-0165551513480308"><mml:math display="inline" id="math34-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> component of the transform vector is the projection of <italic>X</italic> onto <inline-formula id="inline-formula22-0165551513480308"><mml:math display="inline" id="math35-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>,</p>
<p>
<disp-formula id="disp-formula14-0165551513480308">
<mml:math display="block" id="math36-0165551513480308">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mo>&lt;</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>x</mml:mi>
<mml:mo>&gt;</mml:mo>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mtext>T</mml:mtext>
</mml:mrow>
</mml:msubsup>
<mml:msup>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula14-0165551513480308" xlink:href="10.1177_0165551513480308-eq14.tif"/>
</disp-formula>
</p>
<p>We see that by this transform, the vector <bold><italic>x</italic></bold> is now expressed in an <italic>N</italic>-dimensional space spanned by the <italic>N</italic> eigenvectors <inline-formula id="inline-formula23-0165551513480308"><mml:math display="inline" id="math37-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> = (<italic>i</italic> = 0, …, <italic>N</italic>– 1) as the basis vectors of the space.</p>
</list-item>
</list>
<p>The overall similarity between two users is the sum of the average distance between two instances of <inline-formula id="inline-formula24-0165551513480308"><mml:math display="inline" id="math38-0165551513480308"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Average distance of KLT data is intended to assess the degree of similarity based on principal features. We compared the non-KLT matrix with the KLT one. The average distances of positive text, negative text and combined text are shown in <xref ref-type="fig" rid="fig2-0165551513480308">Figure 2</xref>. It illustrates that the fluctuating curves of KLT and non-KLT are almost the same. The average distance curves of KLT data are always lower than original data. This means that the similarities between users are improved by using the KLT technique.</p>
<fig id="fig2-0165551513480308" position="float">
<label>Figure 2.</label>
<caption><p>Average distances of KLT and non-KLT.</p></caption>
<graphic xlink:href="10.1177_0165551513480308-fig2.tif"/></fig>
<p>In our experiment, we sorted the distances of all users with positive and negative texts. We obtained two sort lists of positive and negative similarities. As we collected 50 user accounts, there were 100 instances in similarity matrices. The gaps of sort lists between positive similarity and negative similarity were used as average precision to evaluate the sentiment similarity. Different window lengths were chosen to compare the average precision between non-KLT data and KLT data, as shown in <xref ref-type="table" rid="table8-0165551513480308">Table 8</xref>. The values of the sum and average precision of KLT data were better than the non-KLT ones. The values of precision growth were 7.5, 6.9 and 0.8% before and after KLT. The difference was bigger when the window length was shorter. The gaps of different window lengths were 8.3% (from KLT 20 to KLT 30) and 14.2% (from KLT 30 to KLT 40). Therefore, this technique was much better with window length becoming longer.</p>
<table-wrap id="table8-0165551513480308" position="float">
<label>Table 8.</label>
<caption>
<p>Sentiment similarity comparison results.</p>
</caption>
<graphic alternate-form-of="table8-0165551513480308" xlink:href="10.1177_0165551513480308-table8.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Sort number</th>
<th align="left">Sum</th>
<th align="left">Average precision</th>
<th align="left">Growth</th>
</tr>
</thead>
<tbody>
<tr>
<td>Non-KLT 20</td>
<td>585</td>
<td>58.50%</td>
<td>—</td>
</tr>
<tr>
<td>KLT 20</td>
<td>656</td>
<td>65.60%</td>
<td>7.50%</td>
</tr>
<tr>
<td>Non-KLT 30</td>
<td>1088</td>
<td>72.50%</td>
<td>—</td>
</tr>
<tr>
<td>KLT 30</td>
<td>1109</td>
<td>73.90%</td>
<td>6.90%</td>
</tr>
<tr>
<td>Non-KLT 40</td>
<td>1745</td>
<td>87.30%</td>
<td>—</td>
</tr>
<tr>
<td>KLT 40</td>
<td>1760</td>
<td>88.10%</td>
<td>0.80%</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section14-0165551513480308" sec-type="conclusions">
<title>5. Conclusions and future work</title>
<p>In this study, we proposed a sentiment similarity technique to analyse similar Chinese micro-blogging accounts. We chose 126 features from Sina Weibo on the topic of diabetes and we obtained several important conclusions.</p>
<p>First of all, the feature set was refined by blending information gain and SVM technique to select the optimum set of features. We used J48, SVM and naive Bayes as three classifiers to evaluate the results of different feature sets. The optimum feature set selected by using this new technique performed better than others. We also demonstrated that SVM (using LIBSVM tool) outperformed J48 and naive Bayes technique in our experiments.</p>
<p>Second, to the end of finding out which type of feature set is more important, we performed features comparison based on the optimum feature set. We found that content-specific features were the most important type, while syntactic features were more important than other two types (lexical features and emoticon features) for Sina Weibo. Emoticon features made almost no contribution.</p>
<p>Third, we used KLT and average distances of positive and negative texts to detect sentiment similarity. Experiments illustrated that KLT distances were shorter than non-KLT ones. Average precisions of KLT similarities were all better than the non-KLT data. Meanwhile, this technique is much better when the window length is longer.</p>
<p>In terms of future work, the next step is to attempt to use our technique to recommend patients with similar sentiment to accounts in micro-blogs. Those with similar attitudes to the same topic, such as diabetes medicine and techniques, will want to know each other. We will study how to recommend accounts in the diabetes network. In a sentiment similarity network, finding out who shares similar attitudes on the same topic will be meaningful and attractive for patients and doctors.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-0165551513480308">
<title>Appendix</title>
<p><graphic id="img1-0165551513480308" position="anchor" xlink:href="10.1177_0165551513480308-img1.tif"/></p>
</app>
</app-group>
<ack>
<p>The authors acknowledge the support by the China Scholarships Council (file no. 2011612202). This work is also supported by the National Natural Science Foundation of China (grant no. 71171068).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="bibr1-0165551513480308">
<label>[1]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pang</surname><given-names>B</given-names></name>
<name><surname>Lee</surname><given-names>L</given-names></name>
</person-group>. <article-title>Opinion mining and sentiment analysis</article-title>. <source>Foundations and Trends in Information Retrieval</source> <year>2008</year>; <volume>2</volume>(<issue>1–2</issue>): <fpage>1</fpage>–<lpage>135</lpage>.</citation>
</ref>
<ref id="bibr2-0165551513480308">
<label>[2]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bonhard</surname><given-names>P</given-names></name>
<name><surname>Sasse</surname><given-names>MA</given-names></name>
</person-group>. <article-title>‘Knowing me, knowing you’– using profiles and social networking to improve recommender systems</article-title>. <source>BT Technology Journal</source> <year>2006</year>; <volume>24</volume>(<issue>3</issue>): <fpage>84</fpage>–<lpage>98</lpage>.</citation>
</ref>
<ref id="bibr3-0165551513480308">
<label>[3]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chang</surname><given-names>PS</given-names></name>
<name><surname>Ting</surname><given-names>IH</given-names></name>
<name><surname>Wang</surname><given-names>SL</given-names></name>
</person-group>. <article-title>Towards social recommendation system based on the data from microblogs</article-title>. In: <source>International conference on advances in social networks analysis and mining</source> <year>2011</year>, pp. <fpage>672</fpage>–<lpage>677</lpage>.</citation>
</ref>
<ref id="bibr4-0165551513480308">
<label>[4]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yu</surname><given-names>L</given-names></name>
<name><surname>Asur</surname><given-names>S</given-names></name>
<name><surname>Huberman</surname><given-names>BA</given-names></name>
</person-group>. <article-title>What trends in Chinese social media</article-title>. In: <source>The 5th SNA-KDD workshop’11 (SNA-KDD’11)</source> <year>2011</year>, p. <volume>8</volume>(<issue>21</issue>).</citation>
</ref>
<ref id="bibr5-0165551513480308">
<label>[5]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rafeh</surname><given-names>R</given-names></name>
<name><surname>Bahrehmand</surname><given-names>A</given-names></name>
</person-group>. <article-title>An adaptive approach to dealing with unstable behaviour of users in collaborative filtering systems</article-title>. <source>Journal of Information Science</source> <year>2012</year>; <volume>38</volume>(<issue>3</issue>): <fpage>205</fpage>–<lpage>221</lpage>.</citation>
</ref>
<ref id="bibr6-0165551513480308">
<label>[6]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Terveen</surname><given-names>L</given-names></name>
<name><surname>Hill</surname><given-names>W</given-names></name>
<name><surname>Amento</surname><given-names>B</given-names></name>
<name><surname>McDonald</surname><given-names>D</given-names></name>
<name><surname>Creter</surname><given-names>J</given-names></name>
</person-group>. <article-title>PHOAKS: A system for sharing recommendations</article-title>. <source>Communications of the Association for Computing Machinery</source> <year>1997</year>; <volume>40</volume>: <fpage>59</fpage>–<lpage>62</lpage>.</citation>
</ref>
<ref id="bibr7-0165551513480308">
<label>[7]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dang</surname><given-names>Y</given-names></name>
<name><surname>Zhang</surname><given-names>Y</given-names></name>
<name><surname>Chen</surname><given-names>H</given-names></name>
</person-group>. <article-title>A lexicon enhanced method for sentiment classification: An experiment on online product reviews</article-title>. <source>IEEE Intelligent Systems</source> <year>2010</year>; <volume>25</volume>(<issue>4</issue>): <fpage>46</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr8-0165551513480308">
<label>[8]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Pang</surname><given-names>B</given-names></name>
<name><surname>Lee</surname><given-names>L</given-names></name>
<name><surname>Vaithyanathan</surname><given-names>V</given-names></name>
</person-group>. <article-title>Thumbs up? Sentiment classification using machine learning techniques</article-title>. In: <conf-name>Proceedings of the conference on empirical methods in natural language processing</conf-name>, <conf-loc>Morristown</conf-loc>, <year>2002</year>, pp. <fpage>79</fpage>–<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr9-0165551513480308">
<label>[9]</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>B</given-names></name>
</person-group>. <source>Web data mining</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2007</year>.</citation>
</ref>
<ref id="bibr10-0165551513480308">
<label>[10]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Na</surname><given-names>JC</given-names></name>
<name><surname>Thet</surname><given-names>T</given-names></name>
</person-group>. <article-title>Effectiveness of web search results for genre and sentiment classification</article-title>. <source>Journal of Information Science</source> <year>2009</year>; <volume>35</volume>(<issue>6</issue>):<fpage>709</fpage>–<lpage>726</lpage>.</citation>
</ref>
<ref id="bibr11-0165551513480308">
<label>[11]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>L</given-names></name>
<name><surname>Chaovalit</surname><given-names>P</given-names></name>
</person-group>. <article-title>Ontology-supported polarity mining</article-title>. <source>Journal of the American Society for Information Science and Technology</source> <year>2008</year>; <volume>59</volume>(<issue>1</issue>): <fpage>98</fpage>–<lpage>110</lpage>.</citation>
</ref>
<ref id="bibr12-0165551513480308">
<label>[12]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>R</given-names></name>
<name><surname>Li</surname><given-names>JX</given-names></name>
<name><surname>Chen</surname><given-names>H</given-names></name>
<name><surname>Huang</surname><given-names>Z</given-names></name>
</person-group>. <article-title>A framework for authorship identification of online messages: Writing-style features and classification techniques</article-title>. <source>Journal of the American Society for Information Science and Technology</source> <year>2006</year>; <volume>57</volume>(<issue>3</issue>): <fpage>378</fpage>–<lpage>393</lpage>.</citation>
</ref>
<ref id="bibr13-0165551513480308">
<label>[13]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>JX</given-names></name>
<name><surname>Zheng</surname><given-names>R</given-names></name>
<name><surname>Chen</surname><given-names>H</given-names></name>
</person-group>. <article-title>From fingerprint to writeprint</article-title>. <source>Communications of the ACM</source> <year>2006</year>; <volume>49</volume>(<issue>4</issue>): <fpage>76</fpage>–<lpage>82</lpage>.</citation>
</ref>
<ref id="bibr14-0165551513480308">
<label>[14]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abbasi</surname><given-names>A</given-names></name>
<name><surname>Chen</surname><given-names>H</given-names></name>
<name><surname>Nunamaker</surname><given-names>JF</given-names></name>
</person-group>. <article-title>Stylometric identification in electronic markets: Scalability and robustness</article-title>. <source>Journal of Management Information Systems</source> <year>2008</year>; <volume>25</volume>(<issue>1</issue>): <fpage>49</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr15-0165551513480308">
<label>[15]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>David</surname><given-names>S</given-names></name>
<name><surname>Pinch</surname><given-names>TJ</given-names></name>
</person-group>. <article-title>Six degrees of reputation: The use and abuse of online review and recommendation systems</article-title>. <source>Special Issue on Commercial Applications of the Internet</source> <year>2006</year>; <volume>7</volume>(<issue>1</issue>): <fpage>341</fpage>–<lpage>374</lpage>.</citation>
</ref>
<ref id="bibr16-0165551513480308">
<label>[16]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hua</surname><given-names>GY</given-names></name>
<name><surname>Sun</surname><given-names>Y</given-names></name>
<name><surname>Haughton</surname><given-names>D</given-names></name>
</person-group>. <article-title>Network analysis of US air transportation network</article-title>. <source>Data Mining for Social Network Data</source> <year>2010</year>; <volume>12</volume>: <fpage>75</fpage>–<lpage>89</lpage>.</citation>
</ref>
<ref id="bibr17-0165551513480308">
<label>[17]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Khan</surname><given-names>K</given-names></name>
<name><surname>Baharum</surname><given-names>B</given-names></name>
<name><surname>Khan</surname><given-names>A</given-names></name>
<name><surname>Fazal</surname><given-names>M</given-names></name>
</person-group>. <article-title>Mining opinion from text documents: A survey</article-title>. In: <source>3rd IEEE international conference on digital ecosystems and technologies</source> <year>2009</year>, pp. <fpage>217</fpage>–<lpage>222</lpage>.</citation>
</ref>
<ref id="bibr18-0165551513480308">
<label>[18]</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tanev</surname><given-names>H</given-names></name>
<name><surname>Pouliquen</surname><given-names>B</given-names></name>
<name><surname>Zavarella</surname><given-names>V</given-names></name>
<name><surname>Steinberger</surname><given-names>R</given-names></name>
</person-group>. <source>Automatic expansion of a social network using sentiment analysis, data mining for social network data</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2010</year>.</citation>
</ref>
<ref id="bibr19-0165551513480308">
<label>[19]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ma</surname><given-names>H</given-names></name>
<name><surname>Zhou</surname><given-names>TC</given-names></name>
<name><surname>Lyu</surname><given-names>M</given-names></name>
<name><surname>King</surname><given-names>I</given-names></name>
</person-group>. <article-title>Improving recommender systems by incorporating social contextual information</article-title>. <source>ACM Transactions on Information Systems</source> <year>2011</year>; <volume>29</volume>(<issue>2</issue>): <fpage>9.1</fpage>–<lpage>9.23</lpage>.</citation>
</ref>
<ref id="bibr20-0165551513480308">
<label>[20]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yang</surname><given-names>J</given-names></name>
<name><surname>Hou</surname><given-names>M</given-names></name>
<name><surname>Wang</surname><given-names>N</given-names></name>
</person-group>. <article-title>Recognizing sentiment polarity in Chinese reviews based on topic sentiment sentences</article-title>. In: <source>International conference on natural language processing and knowledge engineering (NLP-KE)</source>, <year>2010</year>; <volume>Vol. 8</volume>, pp. <fpage>21</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr21-0165551513480308">
<label>[21]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Koppel</surname><given-names>M</given-names></name>
<name><surname>Schler</surname><given-names>J</given-names></name>
</person-group>. <article-title>The importance of neutral examples for learning sentiment</article-title>. In: <source>Workshop on the analysis of informal and formal information exchange during negotiations, FINEXIN</source>, <year>2005</year>.</citation>
</ref>
<ref id="bibr22-0165551513480308">
<label>[22]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Adam</surname><given-names>B</given-names></name>
<name><surname>Smeaton</surname><given-names>AF</given-names></name>
</person-group>. <article-title>Classifying sentiment in microblogs: Is brevity an advantage?</article-title> In: <source>19th international conference on information and knowledge management</source>, <year>2010</year>; <volume>Vol. 10</volume>, pp. <fpage>26</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr23-0165551513480308">
<label>[23]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Go</surname><given-names>A</given-names></name>
<name><surname>Huang</surname><given-names>L</given-names></name>
<name><surname>Bhayani</surname><given-names>R</given-names></name>
</person-group>. <article-title>Twitter sentiment analysis</article-title>. <source>Entropy</source> <year>2009</year>; <volume>7</volume>: <fpage>30</fpage>–<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr24-0165551513480308">
<label>[24]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yang</surname><given-names>M</given-names></name>
<name><surname>Kiang</surname><given-names>M</given-names></name>
<name><surname>Chen</surname><given-names>H</given-names></name>
<name><surname>Yijun</surname><given-names>Li</given-names></name>
</person-group>. <article-title>Artificial immune system for illicit content identification in social media</article-title>. <source>Journal of the American Society for Information Science and Technology</source> <year>2012</year>; <volume>63</volume>(<issue>2</issue>): <fpage>256</fpage>–<lpage>269</lpage>.</citation>
</ref>
<ref id="bibr25-0165551513480308">
<label>[25]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tang</surname><given-names>H</given-names></name>
<name><surname>Tan</surname><given-names>S</given-names></name>
<name><surname>Cheng</surname><given-names>X</given-names></name>
</person-group>. <article-title>A survey on sentiment detection of reviews</article-title>. <source>Expert Systems with Applications</source> <year>2009</year>; <volume>36</volume>: <fpage>10760</fpage>–<lpage>10773</lpage>.</citation>
</ref>
<ref id="bibr26-0165551513480308">
<label>[26]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wu</surname><given-names>HH</given-names></name>
<name><surname>Charng</surname><given-names>A</given-names></name>
<name><surname>Tsai</surname><given-names>R</given-names></name>
<etal/>
</person-group>. <article-title>Sentiment value propagation for an integral sentiment dictionary based on commonsense knowledge</article-title>. In: <source>International conference on technologies and applications of artificial intelligence</source>, <year>2011</year>.</citation>
</ref>
<ref id="bibr27-0165551513480308">
<label>[27]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turney</surname><given-names>P</given-names></name>
<name><surname>Littman</surname><given-names>M</given-names></name>
</person-group>. <article-title>Measuring praise and criticism: Inference of semantic orientation from association</article-title>. <source>ACM Transactions on Information Systems</source> <year>2003</year>; <volume>21</volume>(<issue>4</issue>): <fpage>315</fpage>–<lpage>346</lpage>.</citation>
</ref>
<ref id="bibr28-0165551513480308">
<label>[28]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hassan</surname><given-names>A</given-names></name>
<name><surname>Radev</surname><given-names>D</given-names></name>
</person-group>. <article-title>Identifying text polarity using random walks</article-title>. <source>Annual Meeing of the Association for Computational Lingusitics</source>, <year>2010</year>; <fpage>395</fpage>–<lpage>403</lpage>.</citation>
</ref>
<ref id="bibr29-0165551513480308">
<label>[29]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marneffe</surname><given-names>M</given-names></name>
<name><surname>Manning</surname><given-names>C</given-names></name>
<name><surname>Potts</surname><given-names>C</given-names></name>
</person-group>. <article-title>Was it good? It was provocative. Learning the meaning of scalar adjectives</article-title>. <source>Annual Meeing of the Association for Computational Lingusitics</source>, <year>2010</year>; <fpage>167</fpage>–<lpage>176</lpage>.</citation>
</ref>
<ref id="bibr30-0165551513480308">
<label>[30]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abbasi</surname><given-names>A</given-names></name>
<name><surname>Chen</surname><given-names>H</given-names></name>
<name><surname>Salem</surname><given-names>A</given-names></name>
</person-group>. <article-title>Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums</article-title>. <source>ACM Transactions on Information Systems</source> <year>2008</year>; <volume>26</volume>(<issue>3</issue>): <fpage>1</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr31-0165551513480308">
<label>[31]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Baayen</surname><given-names>RH</given-names></name>
<name><surname>Van</surname><given-names>HH</given-names></name>
<name><surname>Neijt</surname><given-names>A</given-names></name>
<name><surname>Tweedie</surname><given-names>F</given-names></name>
</person-group>. <article-title>An experiment in authorship attribution</article-title>. In: <source>Proceedings of the 6th international conference on the statistical analysis of textual data</source>, <year>2002</year>.</citation>
</ref>
<ref id="bibr32-0165551513480308">
<label>[32]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thet</surname><given-names>TT</given-names></name>
<name><surname>Na</surname><given-names>JC</given-names></name>
<name><surname>Christopher</surname><given-names>S</given-names></name>
<name><surname>Khoo</surname><given-names>G</given-names></name>
</person-group>. <article-title>Aspect-based sentiment analysis of movie reviews on discussion boards</article-title>. <source>Journal of Information Science</source> <year>2010</year>; <volume>36</volume>(<issue>6</issue>): <fpage>823</fpage>–<lpage>848</lpage>.</citation>
</ref>
<ref id="bibr33-0165551513480308">
<label>[33]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lezcano</surname><given-names>L</given-names></name>
<name><surname>Elena</surname><given-names>GB</given-names></name>
</person-group>. <article-title>Bridging informal tagging and formal semantics via hybrid navigation</article-title>. <source>Journal of Information Science</source> <year>2012</year>; <volume>38</volume>(<issue>2</issue>): <fpage>140</fpage>–<lpage>155</lpage>.</citation>
</ref>
<ref id="bibr34-0165551513480308">
<label>[34]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>DeVel</surname><given-names>O</given-names></name>
<name><surname>Anderson</surname><given-names>A</given-names></name>
<name><surname>Corney</surname><given-names>M</given-names></name>
<name><surname>Mohay</surname><given-names>G</given-names></name>
</person-group>. <article-title>Mining e-mail content for author identification forensics</article-title>. <source>SIGMOD Record</source> <year>2001</year>; <volume>30</volume>(<issue>4</issue>): <fpage>55</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr35-0165551513480308">
<label>[35]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pandey</surname><given-names>V</given-names></name>
<name><surname>Iyer</surname><given-names>K</given-names></name>
</person-group>. <article-title>Sentiment analysis of microblogs</article-title>. <source>CS 229: Machine learning final projects</source>, <year>2009</year>.</citation>
</ref>
<ref id="bibr36-0165551513480308">
<label>[36]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Pak</surname><given-names>A</given-names></name>
<name><surname>Paroubek</surname><given-names>P</given-names></name>
</person-group>. <article-title>Twitter as a corpus for sentiment analysis and opinion mining</article-title>. In: <conf-name>Proceedings of the seventh conference on international language resources and evaluation, LREC′10</conf-name>, <conf-loc>Valletta</conf-loc>, <year>2010</year>, p. <fpage>5</fpage>.</citation>
</ref>
<ref id="bibr37-0165551513480308">
<label>[37]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kouloumpis</surname><given-names>E</given-names></name>
<name><surname>Wilson</surname><given-names>T</given-names></name>
<name><surname>Moore</surname><given-names>J</given-names></name>
</person-group>. <article-title>Twitter sentiment analysis: The good the bad and the OMG!</article-title> In: <source>Fifth international AAAI conference on weblogs and social media, ICWSM</source>, <year>2011</year>, <volume>Vol. 7</volume>, pp. <fpage>17</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr38-0165551513480308">
<label>[38]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Huang</surname><given-names>CL</given-names></name>
<name><surname>Wang</surname><given-names>CJ</given-names></name>
</person-group>. <article-title>A GA-based feature selection and parameters optimization</article-title>. <source>Expert Systems with Applications</source> <year>2006</year>; <volume>31</volume>: <fpage>231</fpage>–<lpage>240</lpage>.</citation>
</ref>
<ref id="bibr39-0165551513480308">
<label>[39]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chang</surname><given-names>CC</given-names></name>
<name><surname>Lin</surname><given-names>CJ</given-names></name>
</person-group>. <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Transactions on Intelligent Systems and Technology</source> <year>2011</year>; <volume>2</volume>(<issue>27</issue>): <fpage>1</fpage>–<lpage>27</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>