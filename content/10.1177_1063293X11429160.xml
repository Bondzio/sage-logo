<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">CER</journal-id>
<journal-id journal-id-type="hwp">spcer</journal-id>
<journal-title>Concurrent Engineering</journal-title>
<issn pub-type="ppub">1063-293X</issn>
<issn pub-type="epub">1531-2003</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1063293X11429160</article-id>
<article-id pub-id-type="publisher-id">10.1177_1063293X11429160</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>E-quality: Using dimensional index values for improving classification accuracy</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Tzu-Liang</surname><given-names>Tseng</given-names></name>
<xref ref-type="aff" rid="aff1-1063293X11429160">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Kwon</surname><given-names>Yongjin</given-names></name>
<xref ref-type="aff" rid="aff2-1063293X11429160">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Devaram</surname><given-names>Prashanth</given-names></name>
<xref ref-type="aff" rid="aff1-1063293X11429160">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1063293X11429160"><label>1</label>Department of Industrial, Manufacturing and Systems Engineering, The University of Texas at El Paso, El Paso, USA</aff>
<aff id="aff2-1063293X11429160"><label>2</label>Industrial and Information Systems Engineering, Ajou University, Suwon, Republic of Korea</aff>
<author-notes>
<corresp id="corresp1-1063293X11429160">Yongjin Kwon, Industrial and Information Systems Engineering, Ajou University, Suwon 443-749, Republic of Korea Email: <email>yk73@ajou.ac.kr</email></corresp>
<fn fn-type="other" id="bio1-1063293X11429160">
<p>Tseng Tzu-Liang is an associate professor of Industrial, Manufacturing, and Systems Engineering at University of Texas at El Paso (UTEP). His research focuses on the computational intelligence, data mining, bioinformatics, and advanced manufacturing. He published his articles in many refereed journals such as <italic>IEEE Transactions</italic>, <italic>IIE Transaction</italic>, <italic>Journal of Manufacturing Systems</italic>, and others. He has been serving as a principal investigator of many research projects, funded by National Science Foundation (NSF), National Aeronautics and Space Administration (NASA), Department of Electricity Development (DoEd), and KSEF. He is currently serving as an editor of <italic>Journal of Computer Standards &amp; Interfaces</italic>.</p>
</fn>
<fn fn-type="other" id="bio2-1063293X11429160">
<p>Yongjin Kwon has many years of engineering experience in industrial and academic settings. He has extensive experience and practical knowledge in current design, manufacturing, and quality control. His work has been cited a number of times in high-profile journals. He is currently a professor in the Department of Industrial and Information Systems Engineering at Ajou University. Prior to joining Ajou, he was on the faculty of Drexel University, Philadelphia, USA.</p>
</fn>
<fn fn-type="other" id="bio3-1063293X11429160">
<p>Prashanth Devaram was born in Khammam, India, on August 6, 1986. He graduated from Jawaharlal Nehru Technological University, Anantapur, with Bachelor of Technology in Mechanical Engineering in May 2007. In 2008, he enrolled in the MS program in Industrial Engineering at UTEP and graduated in December 2010. His research interests include e-quality and Internet-based manufacturing.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2012</year>
</pub-date>
<volume>20</volume>
<issue>1</issue>
<fpage>43</fpage>
<lpage>53</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>E-quality is a holistic approach to gauge and ascertain product quality in real time with the use of advanced technologies. E-quality manifests a sensor-based, networked, fully automated quality control, through which a reduction of inspection time is attained. Even with the e-quality, a part classification still remains as one of the most challenging tasks because the classification is based on the minute differences among a multitude of dimensional attributes. Part classification entails many steps that complicate the accuracy of final outcome. Achieving 100% classification accuracy is not a trivial matter. In this context, this study focuses on a novel approach for improving part classification accuracy in tune with the notion of e-quality and concurrent engineering. Two approaches are proposed and compared with the traditional multiclass support vector machine classification method. One of the approaches is to modify the data before applying the support vector machine. The other is a completely new Sine methodology using dimensional index values for classifying parts into different categories. Support vector machine is employed due to its higher generalization ability, especially when the data set is small and the class overlap is nonexistent. The data extracted from a machine vision system in a networked robotic inspection cell is used to test the proposed approaches. Experimental results show that the new Sine methodology performs better than the others, displaying near 100% classification accuracy.</p>
</abstract>
<kwd-group>
<kwd>e-quality</kwd>
<kwd>production efficiency</kwd>
<kwd>support vector machine classification</kwd>
<kwd>concurrent engineering</kwd>
<kwd>networked inspection</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1063293X11429160" sec-type="intro">
<title>Introduction</title>
<p>Relentless competition in recent years compelled many companies to shift the focus of quality control from a conventional, sample-based probability estimation to a new paradigm of sensor-based, automated, real-time, networked 100% part inspection (i.e. e-quality). By embedding various sensors, communications, and fast computing platforms for accurate and timely decision making onto the production lines, the chance of defective items propagating into the downstream is minimized. Part inspection data can be used for the instant analysis of process capability and distributed to the relevant entities for the initiation of corrective actions. E-quality, integrated with the information network, allows a remote quality control with a minimal human intervention. Such an idea is well aligned with the notion of concurrent engineering (CE), which intends to streamline the overall product development processes by creating communication channels and information sharing among various functional teams. By considering disparate factors in the entire life cycle of the product, CE becomes an effective strategy for the companies to maintain their competitiveness (<xref ref-type="bibr" rid="bibr15-1063293X11429160">Prasad, 1995</xref>, <xref ref-type="bibr" rid="bibr16-1063293X11429160">1996</xref>, <xref ref-type="bibr" rid="bibr17-1063293X11429160">1997</xref>, <xref ref-type="bibr" rid="bibr18-1063293X11429160">1998</xref>, <xref ref-type="bibr" rid="bibr19-1063293X11429160">1999</xref>). E-quality manifests the ubiquitous access to the quality-related information, allowing the instant sharing and reiteration of critical information for the continuous product improvement. This is the key factor for the successful implementation of CE. The conjunction of CE and e-quality introduces new dimensions to the conventional practice of quality control.</p>
<p>Part inspection entails a decision making based on the minute differences among a multitude of dimensional attributes. The classification of part category based on the dimensional gauging delineates one of the most challenging and complex tasks. With today’s advanced technologies, part inspection can be performed at a low cost using a host of affordable sensor devices. In this context, this study focuses on the improvement of classification accuracy in tune with the e-quality and CE. The concept presented in this study is novel (i.e. the combination of e-quality, CE, and the accuracy enhancement of part classification in a networked production environment), based on the prediction that the industry will follow a similar path to continue its effort to enhance product quality. Two approaches are proposed and compared with the traditional multiclass support vector machines (SVMs) classification method. SVM is employed due to its higher generalization ability, especially when the data set is small and the class overlap is nonexistent. The data extracted from a machine vision system in a networked robotic inspection cell are used to test the approaches. Experimental results show that the new Sine methodology performs better than the others with near 100% classification accuracy. The overall structure of the article assumes the following format. The configuration of networked inspection cell is provided in section “Networked robotic inspection cell,” while the steps of methodology development are detailed in section “Development of methodology.” Section “Comparative analysis of the classification results” illustrates the analytical outcome of the experimental data. Finally, the conclusion is drawn in section “Conclusion.”</p>
</sec>
<sec id="section2-1063293X11429160">
<title>Networked robotic inspection cell</title>
<p>Testing of e-quality involving the CE is performed on the state-of-the-art robotic inspection cell, situated in the Industrial Systems Engineering Laboratory at the University of Texas at El Paso. The cell emulates an e-quality-integrated production environment and serves as a test bed for the validation of proposed approaches. The test parts are designed in-house and manufactured using the fused deposition modeling machine, FDM 3000. The robotic inspection cell consists of a conveyer, optical sensors, a Cognex machine vision system, and a robot. The vision system performs the inspection by capturing and analyzing part images for dimensional variations. The Yamaha YK 350X SCARA robot picks up the bad parts on the conveyer and places them into the sorting bin. The robot can be controlled remotely through an onboard Ethernet card over the Internet. The inspection algorithm is a part of the vision control algorithms that make a decision on part classifications. <xref ref-type="fig" rid="fig1-1063293X11429160">Figure 1</xref> shows the overall setting of the system and the application programming interface (API)developed for the remote robotic control and vision inspection. The Cognex vision system (IS1403-10) is Ethernet based, and the operating software is embedded in the camera flash memory; hence, any image-processing algorithms and system parameters can be adjusted over the Internet. To further improve the real-time monitoring, a high-resolution Linksys camera is connected with a network video server. The real-time visual feedback and control allow the facilitation of the CE among any relevant functional teams.</p>
<fig id="fig1-1063293X11429160" position="float">
<label>Figure 1.</label>
<caption>
<p>Overall setting of the networked robotic inspection cell at the University of Texas at El Paso.</p>
</caption>
<graphic xlink:href="10.1177_1063293X11429160-fig1.tif"/>
</fig>
<p>Part classification involves analyzing the dimensional values where a slight difference in measurement makes the part fall into different categories. Three most popular methodologies include neural networks, SVMs, and fuzzy logics. SVMs are preferred due to its higher generalization ability, especially when the data set is small and the class overlap is scarce or nonexistent (<xref ref-type="bibr" rid="bibr21-1063293X11429160">Shigeo, 2005</xref>; <xref ref-type="bibr" rid="bibr23-1063293X11429160">Suykens and Vandewalle, 1999</xref>). However, SVMs do not provide 100% accuracy. Many SVM-derived approaches are developed and examined. The relevant studies are summarized in <xref ref-type="table" rid="table1-1063293X11429160">Table 1</xref>. SVMs can be combined with other statistical methods to enhance the classification accuracy. Depending on the data types, the combined SVMs outperform the traditional statistical analysis (<xref ref-type="bibr" rid="bibr1-1063293X11429160">Cai et al., 2003</xref>; <xref ref-type="bibr" rid="bibr2-1063293X11429160">Calster et al., 2009</xref>; <xref ref-type="bibr" rid="bibr3-1063293X11429160">Chao and Tong, 2009</xref>; <xref ref-type="bibr" rid="bibr4-1063293X11429160">Chen et al., 2006</xref>; <xref ref-type="bibr" rid="bibr5-1063293X11429160">Comack and Arslan, 2008</xref>; <xref ref-type="bibr" rid="bibr6-1063293X11429160">Du and Sun, 2008</xref>; <xref ref-type="bibr" rid="bibr7-1063293X11429160">Hong and Cho, 2008</xref>; <xref ref-type="bibr" rid="bibr8-1063293X11429160">Hong et al., 2008</xref>; <xref ref-type="bibr" rid="bibr9-1063293X11429160">Horng, 2009</xref>; <xref ref-type="bibr" rid="bibr11-1063293X11429160">Jayadeva and Chandra, 2005</xref>; <xref ref-type="bibr" rid="bibr12-1063293X11429160">Li and Liu, 2010</xref>; <xref ref-type="bibr" rid="bibr13-1063293X11429160">Meyer et al., 2003</xref>; <xref ref-type="bibr" rid="bibr14-1063293X11429160">Mondal et al., 2006</xref>; <xref ref-type="bibr" rid="bibr20-1063293X11429160">Shen et al., 2008</xref>; <xref ref-type="bibr" rid="bibr22-1063293X11429160">Sugumaran et al., 2008</xref>). In this regard, instead of directly employing the traditional multiclass SVM method, two new approaches are proposed.</p>
<table-wrap id="table1-1063293X11429160" position="float">
<label>Table 1.</label>
<caption>
<p>Review of various works on SVM</p>
</caption>
<graphic alternate-form-of="table1-1063293X11429160" xlink:href="10.1177_1063293X11429160-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Author</th>
<th align="left">Method</th>
<th align="left">Application</th>
<th align="left">Remarks</th>
</tr>
</thead>
<tbody>
<tr>
<td><xref ref-type="bibr" rid="bibr9-1063293X11429160">Horng (2009)</xref></td>
<td>Various multiclass SVMs</td>
<td>Classify the supraspinatus image into different disease groups</td>
<td>One against all fuzzy SVM was found to yield the best classification accuracy of 90%</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr7-1063293X11429160">Hong and Cho (2008)</xref></td>
<td>One versus rest SVMs and Naïve Bayes</td>
<td>Classify cancer data</td>
<td>Better accuracy of 81.5% compared to simple SVMs</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr14-1063293X11429160">Mondal et al. (2006)</xref></td>
<td>Various methods including MSVMs</td>
<td>Classify conotoxins</td>
<td>SVMs outperformed with 88.1% accuracy</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr20-1063293X11429160">Shen et al. (2008)</xref></td>
<td>Probabilistic-based multiclass SVM</td>
<td>Electroencephalography-based mental fatigue measurement</td>
<td>PWC-SVM performed better compared to one versus one SVM</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr12-1063293X11429160">Li and Liu (2010)</xref></td>
<td>Possibility-based kernel o be used with SVM</td>
<td>Echocardiogram data, Wisconsin diagnostic breast cancer data, BUPA (the British United Provident Association) liver disorders data, and Pima Indians diabetes data</td>
<td>Proposed kernel performed better compared to Gaussian and polynomial kernels</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr6-1063293X11429160">Du and Sun (2008)</xref></td>
<td>SVMs (OVA, one vs. one) and DAG</td>
<td>Pizza quality</td>
<td>One versus one found to be the best followed by DAG</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr1-1063293X11429160">Cai et al. (2003)</xref></td>
<td>Gaussian kernel–based SVM</td>
<td>Classification of functionally distinct proteins</td>
<td>Accuracy levels of 86.5%–99.4% were achieved</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr22-1063293X11429160">Sugumaran et al. (2008)</xref></td>
<td>Information gain and multiclass SVM</td>
<td>Identify faulty bearing conditions</td>
<td>Accuracy of 94% was achieved</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr8-1063293X11429160">Hong et al. (2008)</xref></td>
<td>Naïve Bayes classifiers and OVA SVMs with Gaussian kernel</td>
<td>Finger print classification</td>
<td>Accuracy of 90.8% for five class problem and 94.9% for four class problem with 1.8% rejection</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr4-1063293X11429160">Chen et al. (2006)</xref></td>
<td>ICA (independent component analysis) filter bank, a new RFE method and LS-SVM</td>
<td>Texture analysis</td>
<td>Proposed RFE-max method performed better</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr3-1063293X11429160">Chao and Tong (2009)</xref></td>
<td>Multiclass SVM (Gaussian kernel and one-against-one approach) with new defect cluster index</td>
<td>Wafer defect recognition</td>
<td>Proposed cluster index yielded better results.</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr11-1063293X11429160">Jayadeva and Chandra (2005)</xref></td>
<td>Multiclass proximal support vector machines</td>
<td>Iris data and Wane data</td>
<td>Proper membership value assignment improved results</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr5-1063293X11429160">Comack and Arslan (2008)</xref></td>
<td>Training algorithm based on Euclidean distance measure and K nearest neighbor with pairwise SVM</td>
<td>Iris, Wane, and Thyroid data</td>
<td>Proposed method performed better</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr2-1063293X11429160">Calster et al. (2009)</xref></td>
<td>Probabilistic algorithms</td>
<td>Classification of pregnancies of unknown location</td>
<td>Methods affiliated to SVM are found to perform good but not superior</td>
</tr>
<tr>
<td><xref ref-type="bibr" rid="bibr13-1063293X11429160">Meyer et al. (2003)</xref></td>
<td>Benchmarked SVM to 16 classification and 9 regression methods</td>
<td>21 data sets for classification and 12 for regression</td>
<td>SVMs have high potential but are not superior to all methods in all cases</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1063293X11429160">
<p>SVM: support vector machine; PWC: pairwise coupling; OVA: one versus all; DSG: directed acyclic graph; RFE: recursive feature elimination; LS: least squares; ICA: independent component analysis; BUPA: the British United Provident Association.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section3-1063293X11429160" sec-type="methods">
<title>Development of methodology</title>
<p>The SVM method is divided into a binary SVM and a multiclass SVM.</p>
<sec id="section4-1063293X11429160">
<title>Binary SVMs</title>
<p>The decision function for the binary support vector classification is of the form</p>
<p><disp-formula id="disp-formula1-1063293X11429160">
<label>(1)</label>
<mml:math display="block" id="math1-1063293X11429160">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mtext>sgn</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>〈</mml:mo>
<mml:mi>Φ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
<mml:mi>Φ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>〉</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq1.tif"/>
</disp-formula></p>
<p>where <italic>m</italic> = a total number of data points for training, <inline-formula id="inline-formula1-1063293X11429160">
<mml:math display="inline" id="math2-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> = ±1 depending on the class to which <italic>i</italic>th data point belongs to the output space, <inline-formula id="inline-formula2-1063293X11429160">
<mml:math display="inline" id="math3-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> = the Lagrange multiplier obtained by solving the quadratic equation, <inline-formula id="inline-formula3-1063293X11429160">
<mml:math display="inline" id="math4-1063293X11429160">
<mml:mrow>
<mml:mrow>
<mml:mo>〈</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>〉</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> = the dot product of <italic>x</italic> and <italic>y</italic>, <inline-formula id="inline-formula4-1063293X11429160">
<mml:math display="inline" id="math5-1063293X11429160">
<mml:mrow>
<mml:mi>Φ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> = a mapping function that maps input value <italic>x</italic> with a higher dimensional space, and <italic>b</italic> = a constant. <xref ref-type="disp-formula" rid="disp-formula1-1063293X11429160">Equation (1)</xref> serves as a basis for the second equation</p>
<p><disp-formula id="disp-formula2-1063293X11429160">
<label>(2)</label>
<mml:math display="block" id="math6-1063293X11429160">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mtext>sgn</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>k</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq2.tif"/>
</disp-formula></p>
<p>where <inline-formula id="inline-formula5-1063293X11429160">
<mml:math display="inline" id="math7-1063293X11429160">
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>〈</mml:mo>
<mml:mi>Φ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
<mml:mi>Φ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>〉</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. <xref ref-type="disp-formula" rid="disp-formula2-1063293X11429160">Equation (2)</xref> leads to the following quadratic equation. Maximize</p>
<p><disp-formula id="disp-formula3-1063293X11429160">
<label>(3)</label>
<mml:math display="block" id="math8-1063293X11429160">
<mml:mrow>
<mml:mi>α</mml:mi>
<mml:mo>∈</mml:mo>
<mml:msup>
<mml:mi>R</mml:mi>
<mml:mi>m</mml:mi>
</mml:msup>
<mml:mi>W</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>α</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>m</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mn>2</mml:mn>
</mml:mfrac>
</mml:mrow>
</mml:mstyle>
<mml:mstyle displaystyle="true">
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>m</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mstyle>
<mml:mi>k</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mi>x</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq3.tif"/>
</disp-formula></p>
<p>Subject to <inline-formula id="inline-formula6-1063293X11429160">
<mml:math display="inline" id="math9-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>≥</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> for all <italic>i</italic> = 1, …, <italic>m</italic> and</p>
<p><disp-formula id="disp-formula4-1063293X11429160">
<label>(4)</label>
<mml:math display="block" id="math10-1063293X11429160">
<mml:mrow>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq4.tif"/>
</disp-formula></p>
<p>where α = the Lagrange multiplier, <italic>y</italic> = the class index value in the output space, <italic>x</italic> = the input data point vector, <italic>m</italic> = the total number of data points, and <inline-formula id="inline-formula7-1063293X11429160">
<mml:math display="inline" id="math11-1063293X11429160">
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> = the kernel function. To allow for the possibility of examples violating the conditions, slack variables are introduced</p>
<p><disp-formula id="disp-formula5-1063293X11429160">
<label>(5)</label>
<mml:math display="block" id="math12-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>≥</mml:mo>
<mml:mn>0</mml:mn>
<mml:mspace width="0.25em"/>
<mml:mtext>for all</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq5.tif"/>
</disp-formula></p>
<p>The constraints get relaxed to the following:</p>
<p><disp-formula id="disp-formula6-1063293X11429160">
<label>(6)</label>
<mml:math display="block" id="math13-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mo>〈</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>〉</mml:mo>
</mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>b</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>≥</mml:mo>
<mml:mn>1</mml:mn>
<mml:mtext>-</mml:mtext>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mtext>for all</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula6-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq6.tif"/>
</disp-formula></p>
<p>where <inline-formula id="inline-formula8-1063293X11429160">
<mml:math display="inline" id="math14-1063293X11429160">
<mml:mrow>
<mml:mrow>
<mml:mo>〈</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>〉</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mo>∑</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>〈</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>〉</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula9-1063293X11429160">
<mml:math display="inline" id="math15-1063293X11429160">
<mml:mrow>
<mml:mi>w</mml:mi>
<mml:mo>=</mml:mo>
<mml:mo>∑</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>. A good generalizing classifier is found by controlling both the classifier capacity and the sum of the slacks. The latter can be used to set the upper bound on training errors. The soft margin classifier can be obtained by minimizing the objective function</p>
<p><disp-formula id="disp-formula7-1063293X11429160">
<label>(7)</label>
<mml:math display="block" id="math16-1063293X11429160">
<mml:mrow>
<mml:mi>τ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>ξ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">‖</mml:mo>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mo stretchy="false">‖</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>+</mml:mo>
<mml:mi>C</mml:mi>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula7-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq7.tif"/>
</disp-formula></p>
<p>Subject to constraints of <xref ref-type="disp-formula" rid="disp-formula5-1063293X11429160">equations (5)</xref> and <xref ref-type="disp-formula" rid="disp-formula6-1063293X11429160">(6)</xref>, where <italic>C</italic> &gt; 0 determines the trade-off between margin maximization and training error minimization. Incorporation of kernel and rewriting in terms of Lagrange multipliers leads to a problem of maximizing <xref ref-type="disp-formula" rid="disp-formula3-1063293X11429160">equation (3)</xref>, such as</p>
<p><disp-formula id="disp-formula8-1063293X11429160">
<label>(8)</label>
<mml:math display="block" id="math17-1063293X11429160">
<mml:mrow>
<mml:mtext>Subject to</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mn>0</mml:mn>
<mml:mo>≤</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>≤</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>C</mml:mi>
<mml:mspace width="0.25em"/>
<mml:mtext>for all</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>m</mml:mi>
<mml:mspace width="0.25em"/>
<mml:mtext>and</mml:mtext>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula8-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq8.tif"/>
</disp-formula></p>
<p>The only difference from the separable case is the upper bound <italic>C</italic> on the Lagrange multipliers <inline-formula id="inline-formula10-1063293X11429160">
<mml:math display="inline" id="math18-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>. By this, the influence of individuals that could be outliers is limited. The solution takes the form of <xref ref-type="disp-formula" rid="disp-formula1-1063293X11429160">equation (1)</xref>. Threshold <italic>b</italic> can be computed by exploiting the fact that for all support vectors, <italic>x<sub>i</sub>
</italic> with <inline-formula id="inline-formula11-1063293X11429160">
<mml:math display="inline" id="math19-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> &lt; <italic>C</italic>, the slack variable <inline-formula id="inline-formula12-1063293X11429160">
<mml:math display="inline" id="math20-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> is zero, hence</p>
<p><disp-formula id="disp-formula9-1063293X11429160">
<label>(9)</label>
<mml:math display="block" id="math21-1063293X11429160">
<mml:mrow>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>k</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula9-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq9.tif"/>
</disp-formula></p>
<p>Choosing <italic>b</italic> amounts to shift the hyperplane. Meyer et al. suggest shifting the hyperplane, such that support vectors with zero slack variables lie on the ±l lines (<xref ref-type="bibr" rid="bibr24-1063293X11429160">Vapnik and Chervonenkis, 1964</xref>).</p>
</sec>
<sec id="section5-1063293X11429160">
<title>Multiclass SVMs</title>
<p>As the SVMs employ direct decision functions, an extension to multiclass is not straightforward. There are roughly four types of SVMs that handle multiclass problems. In this study, a pairwise SVM is used in view of the superior results obtained by <xref ref-type="bibr" rid="bibr10-1063293X11429160">Hsu and Lin (2002)</xref>. In a pairwise SVM, a total of <italic>n</italic> (<italic>n</italic> − 1) / 2 decision functions are determined for all combinations of class pairs (using a binary SVM). <inline-formula id="inline-formula13-1063293X11429160">
<mml:math display="inline" id="math22-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula14-1063293X11429160">
<mml:math display="inline" id="math23-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> is the decision function for class <italic>i</italic> against class <italic>j</italic> and <italic>i</italic> ? <italic>j</italic>. After arriving at decision functions, a voting strategy is employed for classification of data into classes. <italic>X</italic> is classified into the following class</p>
<p><disp-formula id="disp-formula10-1063293X11429160">
<label>(10)</label>
<mml:math display="block" id="math24-1063293X11429160">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>arg</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>n</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>max</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:msub>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula10-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq10.tif"/>
</disp-formula></p>
<p>for</p>
<p><disp-formula id="disp-formula11-1063293X11429160">
<label>(11)</label>
<mml:math display="block" id="math25-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>≠</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mtext>sign</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula11-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq11.tif"/>
</disp-formula></p>
<p>where <inline-formula id="inline-formula15-1063293X11429160">
<mml:math display="inline" id="math26-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>D</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> = a decision function for class <italic>i</italic> against class <italic>j</italic>, and <inline-formula id="inline-formula16-1063293X11429160">
<mml:math display="inline" id="math27-1063293X11429160">
<mml:mrow>
<mml:mtext>sign</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> for <italic>x</italic> ≥ 0, while <inline-formula id="inline-formula17-1063293X11429160">
<mml:math display="inline" id="math28-1063293X11429160">
<mml:mrow>
<mml:mtext>sign</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
</inline-formula> for <italic>x</italic> ≤ 0. In case of two classes having the same identical number of votes, the class with the smallest index is selected to avoid any unclassifiable regions.</p>
</sec>
<sec id="section6-1063293X11429160">
<title>SVMs with modified data</title>
<p>It is assumed that the deficiency in the accuracy of SVMs is due to the improper development of separation boundary lines when the data points are closely located and the difference between the values of nonidentical categories is very small. A new procedure of modifying the data points is developed; hence, the values of all features for a particular category fall into a definite range and can be separated easily. Prior information about the limits is necessary. The procedure is an iterative process and has to be repeated twice. In the first iteration, the good parts are separated from the rest (bad). In the second iteration, the parts to be reworked are separated from the bad. After the second iteration, only the bad parts remained. <xref ref-type="fig" rid="fig2-1063293X11429160">Figure 2</xref> shows the flow diagram of the proposed procedure.</p>
<fig id="fig2-1063293X11429160" position="float">
<label>Figure 2.</label>
<caption>
<p>Proposed procedure that uses modified data with the SVM. SVM: support vector machine.</p>
</caption>
<graphic xlink:href="10.1177_1063293X11429160-fig2.tif"/>
</fig>
<p>In the first iteration, <xref ref-type="disp-formula" rid="disp-formula12-1063293X11429160">equation (12)</xref> is applied to the original data to obtain the index values (<inline-formula id="inline-formula18-1063293X11429160">
<mml:math display="inline" id="math29-1063293X11429160">
<mml:mrow>
<mml:mi>I</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>) for each value of a feature in the sample point. The values obtained are such that if the measured dimension of a particular feature is within the limits, the values will be less than or equal to 1 or else larger than 1. The idea is to bring the uniformity in the values of all features, so that the SVM can classify the parts into proper categories</p>
<p><disp-formula id="disp-formula12-1063293X11429160">
<label>(12)</label>
<mml:math display="block" id="math30-1063293X11429160">
<mml:mrow>
<mml:mi>I</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>4</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mrow>
<mml:mi>X</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo>·</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula12-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq12.tif"/>
</disp-formula></p>
<p>where <italic>X</italic><sub>1</sub>, <italic>X</italic><sub>2</sub>, and <italic>X</italic><sub>3</sub> are the lower tolerance limit (LTL), upper tolerance limit (UTL), and rework limits (RL), respectively, for a feature. <italic>X</italic> = the average of LTL and UTL and <italic>X</italic><sub>4</sub> = the measured dimension of a particular feature in the sample point. Rework has no role to play in the first iteration. SVM is applied to the modified data containing index values for the first iteration. The parts that are classified as not good are exported to the second iteration. In the second iteration, the RL replaces the UTL, if greater than, or replaces the LTL, if less than. The new average of limits is called <italic>X</italic><sup>1</sup>, replacing <italic>X</italic> in <xref ref-type="disp-formula" rid="disp-formula12-1063293X11429160">equation (12)</xref> to form <xref ref-type="disp-formula" rid="disp-formula13-1063293X11429160">equation (13)</xref>. The new LTL is called <inline-formula id="inline-formula19-1063293X11429160">
<mml:math display="inline" id="math31-1063293X11429160">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>, replacing <italic>X</italic>
<sub>1</sub> in <xref ref-type="disp-formula" rid="disp-formula12-1063293X11429160">equation (12)</xref>. A new set of index values is calculated for the parts categorized as not good by applying <xref ref-type="disp-formula" rid="disp-formula13-1063293X11429160">equation (13)</xref> to the feature dimensions</p>
<p><disp-formula id="disp-formula13-1063293X11429160">
<label>(13)</label>
<mml:math display="block" id="math32-1063293X11429160">
<mml:mrow>
<mml:mi>I</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>4</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo>·</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>−</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula13-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq13.tif"/>
</disp-formula></p>
<p>SVM is applied on the new set of obtained index values to separate the rework parts from the bad ones.</p>
</sec>
<sec id="section7-1063293X11429160">
<title>The sine methodology</title>
<p>The Sine methodology consists of two iterations and completely eliminates the need for training. First iteration isolates the good parts from the rest. Second iteration separates the reworkable parts from the bad parts. Suppose <italic>X</italic><sub>1</sub>, <italic>X</italic><sub>2</sub>, and <italic>X</italic><sub>3</sub> are the dimension limits for a particular feature to be known, prior to getting into the process of prediction. <italic>X</italic><sub>1</sub> and <italic>X</italic><sub>2</sub> are the LTL and UTL, respectively. <italic>X</italic><sub>3</sub> is the RL for the reworkable parts. <italic>X</italic> is the midpoint of LTL and UTL. <italic>X</italic><sub>4</sub> is the actual dimension measured for a feature. For the feature with no RL, a LTL is considered as the RL. In the first step, <xref ref-type="disp-formula" rid="disp-formula14-1063293X11429160">equation (14)</xref> is applied to each individual feature of the sample point. The values obtained are rounded off to two decimal points. If any of the features is not within the limits, the corresponding values are greater than zero or else equal to zero. The sum of values for all features is calculated. If the sum of the values is equal to zero, it indicates a good part or else is processed through the Step 2 for further analysis. The new index (<inline-formula id="inline-formula20-1063293X11429160">
<mml:math display="inline" id="math33-1063293X11429160">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>Ix</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>new</mml:mi>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula>) is expressed as follows</p>
<p><disp-formula id="disp-formula14-1063293X11429160">
<label>(14)</label>
<mml:math display="block" id="math34-1063293X11429160">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>Ix</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>new</mml:mi>
</mml:mrow>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mtext>Sin</mml:mtext>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>4</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mrow>
<mml:mi>X</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo>·</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mi>X</mml:mi>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>·</mml:mo>
<mml:mi>c</mml:mi>
<mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula14-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq14.tif"/>
</disp-formula></p>
<p>where <italic>c</italic> = a constant equal to 0.57296° or 0.0100001 radians, <inline-formula id="inline-formula21-1063293X11429160">
<mml:math display="inline" id="math35-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>4</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> <sub>=</sub> the measured dimension of a feature (on a part) in a data point, <inline-formula id="inline-formula22-1063293X11429160">
<mml:math display="inline" id="math36-1063293X11429160">
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> = the average of LTL and UTL, and <inline-formula id="inline-formula23-1063293X11429160">
<mml:math display="inline" id="math37-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> = the LTL. In the second step, for each individual feature, if the RL is less than the LTL, then the LTL value is replaced with the RL value. Similarly, if the RL is higher than the UTL, then the UTL value is replaced with the RL value. A new midpoint <italic>X</italic>
<sup>1</sup> is calculated. The obtained values are substituted into <xref ref-type="disp-formula" rid="disp-formula15-1063293X11429160">equation (15)</xref>, obtained by changing the variables in <xref ref-type="disp-formula" rid="disp-formula14-1063293X11429160">equation (14)</xref></p>
<p><disp-formula id="disp-formula15-1063293X11429160">
<label>(15)</label>
<mml:math display="block" id="math38-1063293X11429160">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>Ix</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>new</mml:mi>
</mml:mrow>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mtext>Sin</mml:mtext>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>4</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo>·</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>−</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>·</mml:mo>
<mml:mi>c</mml:mi>
<mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula15-1063293X11429160" xlink:href="10.1177_1063293X11429160-eq15.tif"/>
</disp-formula></p>
<p>where <inline-formula id="inline-formula24-1063293X11429160">
<mml:math display="inline" id="math39-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>4</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> = the measured dimension of a feature on a part (in a data point), <inline-formula id="inline-formula25-1063293X11429160">
<mml:math display="inline" id="math40-1063293X11429160">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> = the new LTL, and <inline-formula id="inline-formula26-1063293X11429160">
<mml:math display="inline" id="math41-1063293X11429160">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula> = the average of new LTL and UTL. <xref ref-type="fig" rid="fig3-1063293X11429160">Figure 3</xref> shows the geometric representation of the proposed methodology. <xref ref-type="fig" rid="fig4-1063293X11429160">Figure 4</xref> shows the flow diagram of the Sine methodology using the part from the case study as an example.</p>
<fig id="fig3-1063293X11429160" position="float">
<label>Figure 3.</label>
<caption>
<p>Geometric representation of the proposed Sine methodology.</p>
</caption>
<graphic xlink:href="10.1177_1063293X11429160-fig3.tif"/>
</fig>
<fig id="fig4-1063293X11429160" position="float">
<label>Figure 4.</label>
<caption>
<p>Flow diagram of the proposed Sine methodology.</p>
</caption>
<graphic xlink:href="10.1177_1063293X11429160-fig4.tif"/>
</fig>
<p>Sum of the corresponding values of the features is calculated as in the Iteration 1. If the sum obtained equals to zero, the sample is classified as rework else a bad part. In the current methodology, <xref ref-type="disp-formula" rid="disp-formula13-1063293X11429160">equations (13)</xref> and <xref ref-type="disp-formula" rid="disp-formula14-1063293X11429160">(14)</xref> are obtained by multiplying <xref ref-type="disp-formula" rid="disp-formula11-1063293X11429160">equations (11)</xref> and <xref ref-type="disp-formula" rid="disp-formula12-1063293X11429160">(12)</xref> with the constant ‘<italic>c</italic>’ and applying a sine function. By applying <xref ref-type="disp-formula" rid="disp-formula11-1063293X11429160">equation (11)</xref> or <xref ref-type="disp-formula" rid="disp-formula12-1063293X11429160">(12)</xref> to features in a data point, the index values are obtained. For the features that lie in the range of a particular class, the index values are obtained between 0 and 1. Features that do not lie within the range are given index values greater than 1. After arriving at the index values, it is required to classify the parts (data points). The main novelty of the new methodology is that it eliminates the need for the SVM in performing classifications, a process that requires analyzing the combined effect of index values of all features in a data point. Only if index values for all features reside between 0 and 1, a part can be classified into a particular class. To verify at a data-point level, a combined value of index values of all features is required. By summing up the index values of all features in a data point, a random value is obtained, which renders decision making impossible. It is required to reduce the index values for the features that are between 0 and 1 to 0. Then, summing up all the resultant values for the features of a data point is equal to 0, which can be verified easily. To modify the index values, a function that minimizes the index values that are less than or equal to 1 by a decimal point or two is required. The obtained values are rounded off and summed up to make a final decision. The importance of using a function is to turn the value ‘1’ into a value with the same number of decimals. The sine, tangent, and secant functions can serve the purpose. The ‘<italic>c</italic>’ value to convert the index value ‘1’ into a value with one decimal is obtained by the inverse function, <inline-formula id="inline-formula27-1063293X11429160">
<mml:math display="inline" id="math42-1063293X11429160">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, while converting into a value with two decimals is obtained by the inverse function, <inline-formula id="inline-formula28-1063293X11429160">
<mml:math display="inline" id="math43-1063293X11429160">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>01</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula29-1063293X11429160">
<mml:math display="inline" id="math44-1063293X11429160">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mo>∈</mml:mo>
<mml:mrow>
<mml:mo>{</mml:mo>
<mml:mi>Sine</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>Tangent</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>Secant</mml:mi>
<mml:mo>}</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>. ‘<italic>c</italic>’ values to convert ‘1’ into values with more than two decimals can be obtained.</p>
</sec>
</sec>
<sec id="section8-1063293X11429160">
<title>Comparative analysis of the classification results</title>
<p>To compare the classification accuracies, a part containing all three types of features is designed (i.e. positive, negative, and standard features) and fabricated. Positive features are those that can be reduced in size to meet the specifications. Negative features are those that can be increased in size to meet the specifications. Positive features have a RL on lower side, and negative features have RL on the higher side of the dimension limits. A term “standard feature” is coined and is defined as a feature that cannot be reworked. Length and width of the parts are considered as positive features. Diameters of the two circles are considered as negative features and the width of the extruded rectangles are considered as standard features. Though the width of the extruded rectangles can be considered as positive features, they are considered as standard features in view of the design requirement and the fact that change in the dimensions of the feature may affect the surface roughness. The part design is edited to produce additional parts with varied dimensions. Part dimensions are varied by 0.03–0.06 inches to produce the reworkable and the bad parts. A fused deposition modeling rapid prototyping machine (FDM 3000) is used to create the parts. Overall, a total of 103 parts are produced, of which include 44 good, 28 rework, and 31 bad parts. Among 103 parts, two-thirds of the parts are randomly marked as train and one-third as test. <xref ref-type="fig" rid="fig5-1063293X11429160">Figure 5</xref> shows the part fabrication process inside the rapid prototyping machine and a part design with the dimensions of six features considered for the study.</p>
<fig id="fig5-1063293X11429160" position="float">
<label>Figure 5.</label>
<caption>
<p>(a) The inside view of the Rapid Prototyping (RP) machine in production and (b) part dimensions of various features on a good part.</p>
</caption>
<graphic xlink:href="10.1177_1063293X11429160-fig5.tif"/>
</fig>
<sec id="section9-1063293X11429160">
<title>Approach I: classification with the multiclass SVMs</title>
<p>The part data are analyzed with various kernels without making any modifications to the data. First, the data are analyzed with a linear kernel. Fivefold cross validation is employed. The software gives a range and an increment for the training parameter that automatically selects the value, which generates the best results. A range of 0–500 with an increment of 0.01 is given. Overall accuracy of 84.47% is achieved. Second, a polynomial kernel is used, which requires the provision of the values for degree, gamma, and coefficient parameters. Several iterations are run by varying degrees from 1 to 25. The best classification accuracy of 92.23% is attained. Third, a radial basis function kernel is used, which requires the provision of the gamma values and a range for the training parameters. Several iterations are run with gamma values between 0.01 and 25. The best classification accuracy of 92.23% is achieved. Fourth, a sigmoid kernel is used, which requires the input of the values for gamma, coefficient, and the range for training parameters. Several iterations are made by varying the gamma and coefficient values. The best classification accuracy of 84.47% is achieved. Among the four kernels, the polynomial kernel yields the best classification accuracy of 92.23% with a least number of support vectors. Out of 45 good parts, 43 are predicted as good, 1 being predicted as rework, and 1 as bad. Out of 27 rework parts, 4 are predicted as good. Out of 31 bad parts, 2 are predicted as good. In total, 95 parts are classified correctly and 8 are predicted wrong.</p>
</sec>
<sec id="section10-1063293X11429160">
<title>Approach II: classification with the SVMs with modified data</title>
<p>For the first iteration, index values are calculated for all 103 parts and analyzed with the four kernels to find the best classification accuracy. The parts that are classified as not good in the first iteration are sent to the second iteration. Index values are calculated again with the new limits and analyzed with the SVM to differentiate the reworkable and the bad parts. The kernel that gives the best result in the first iteration is chosen for the second iteration.</p>
<sec id="section11-1063293X11429160">
<title>Iteration 1</title>
<p>With a linear kernel, a fivefold cross validation with a training parameter range of 0–500 with an increment of 0.01 is given. Classification accuracy of 83.95% is achieved. For a polynomial kernel, several iterations are performed with a different set of values for degree, gamma, and coefficient. A range of 0–100 with an increment of 0.1 is given for the training parameter with a fivefold cross validation. Using a radial basis kernel, the best result is 95%. The sigmoid kernel provides an accuracy of 84%. Polynomial and radial basis kernels result in the highest accuracy. Polynomial kernel results in a least number of support vectors and hence selected. The predictions for the test data are analyzed to find out the wrong predictions. It is found that a total of five cases are wrongly predicted. Since the training accuracy is 100%, it is concluded that a total of five wrong predictions out of 103 cases is made. Cases 88 and 91, which are originally rework, are incorrectly predicted as good, while the Cases 49, 53, and 103, which are originally good, are wrongly predicted as not good. All cases were predicted as not good including those of wrong predictions. Training cases are exported to the second iteration. A total of 59 cases are obtained, of which 40 are training and 19 being test cases. The good ones that are predicted as not good are marked as rework for the analysis in the second iteration.</p>
</sec>
<sec id="section12-1063293X11429160">
<title>Iteration 2</title>
<p>The second iteration employs the polynomial kernel. All 59 cases imported from the first iteration are evaluated with new limits to obtain the new set of data and then analyzed with the SVM using a polynomial kernel. Several trials are made with different set of values starting with the values from the previous iteration. The predictions made are analyzed to find out the wrong predictions. Case 53, which is Case 95 in the original data, is predicted wrong as rework. By analyzing the results, it is concluded that two good ones, which are predicted as not good in the first iteration and marked as rework, have been classified into the rework category in the second iteration. Overall, one bad part is wrongly predicted as rework, two rework parts are wrongly predicted as good, and three good parts are incorrectly predicted as rework. A total of six cases are predicted wrong out of 103 cases, resulting in the classification accuracy of 94%.</p>
</sec>
</sec>
<sec id="section13-1063293X11429160">
<title>Approach III: classification with the new sine methodology</title>
<p>The outcome of the new Sine methodology is shown in <xref ref-type="fig" rid="fig6-1063293X11429160">Figure 6</xref>. Each column stands for a feature measured on parts. Depending on the feature types, it is programmed to automatically take the RL for the second iteration. The user needs to enter the TL and RL for all features in the first three rows. The measured values are entered in the fourth row. The remaining process is done automatically, and the final result is displayed. With the new Sine methodology, all 103 cases are predicted correctly, resulting in 100% classification accuracy. <xref ref-type="fig" rid="fig6-1063293X11429160">Figure 6</xref> shows the prediction result of Case 103, distinguishing the values with fourth decimal point difference.</p>
<fig id="fig6-1063293X11429160" position="float">
<label>Figure 6.</label>
<caption>
<p>Prediction outcome of Case 103 with the new SVM methodology. SVM: support vector machine.</p>
</caption>
<graphic xlink:href="10.1177_1063293X11429160-fig6.tif"/>
</fig>
</sec>
<sec id="section14-1063293X11429160">
<title>Validation of the proposed methodologies</title>
<p>In this section, an additional example (i.e. a complex example) is provided to validate the proposed approaches. <xref ref-type="fig" rid="fig7-1063293X11429160">Figure 7</xref> illustrates a basic part with the dimensions of 20 features, which include the intruded and extruded circles, rectangles, and extruded squares. Basically, there should be no limitations on the number of features for analysis. Overall, a total of 263 parts are produced, of which include 110 good, 71 rework, and 82 bad parts. Among 263 parts, similar to the previous case, two-thirds of the parts are randomly marked as training and one-third as testing. The final computational results (i.e. the integrated confusion matrix for each approach) are presented in <xref ref-type="table" rid="table2-1063293X11429160">Table 2</xref>, showing the accuracy for each approach being 88.6%, 90.49%, and 98.86%, respectively.</p>
<fig id="fig7-1063293X11429160" position="float">
<label>Figure 7.</label>
<caption>
<p>Part design and dimensions, showing (a) the top view, (b) the additional dimensions, (c) the side view, and (d) the fabricated part.</p>
</caption>
<graphic xlink:href="10.1177_1063293X11429160-fig7.tif"/>
</fig>
<table-wrap id="table2-1063293X11429160" position="float">
<label>Table 2.</label>
<caption>
<p>Results based on the integrated confusion matrix for each approach.</p>
</caption>
<graphic alternate-form-of="table2-1063293X11429160" xlink:href="10.1177_1063293X11429160-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="5">Approach I</th>
<th align="left" colspan="5">Approach II</th>
<th align="left" colspan="5">Approach III</th>
</tr>
<tr>
<th/>
<th align="left" colspan="3">Predicted</th>
<th/>
<th/>
<th align="left" colspan="3">Predicted</th>
<th/>
<th/>
<th align="left" colspan="3">Predicted</th>
<th/>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>G</td>
<td>R</td>
<td>B</td>
<td>ST</td>
<td>A</td>
<td>G</td>
<td>R</td>
<td>B</td>
<td>ST</td>
<td>A</td>
<td>G</td>
<td>R</td>
<td>B</td>
<td>ST</td>
</tr>
<tr>
<td>G</td>
<td>91</td>
<td>13</td>
<td>6</td>
<td>110</td>
<td>G</td>
<td>96</td>
<td>6</td>
<td>8</td>
<td>110</td>
<td>G</td>
<td>110</td>
<td>0</td>
<td>0</td>
<td>110</td>
</tr>
<tr>
<td>R</td>
<td>1</td>
<td>69</td>
<td>1</td>
<td>71</td>
<td>R</td>
<td>2</td>
<td>66</td>
<td>3</td>
<td>71</td>
<td>R</td>
<td>0</td>
<td>70</td>
<td>1</td>
<td>71</td>
</tr>
<tr>
<td>B</td>
<td>4</td>
<td>73</td>
<td>5</td>
<td>82</td>
<td>B</td>
<td>3</td>
<td>3</td>
<td>76</td>
<td>82</td>
<td>B</td>
<td>0</td>
<td>2</td>
<td>80</td>
<td>82</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1063293X11429160">
<p>A: actual; G: good; R: rework; B: bad; ST: subtotal.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section15-1063293X11429160" sec-type="conclusions">
<title>Conclusion</title>
<p>The main focus of this study is to improve the classification accuracy of multidimensional attributes within the context of e-quality and CE. The new approaches result in the following: (1) with the traditional multiclass SVM, the accuracy is 92% (88.6% in a complex case); (2) with the modified data, the accuracy is 94% (90.49% in a complex case); and (3) with the new Sine methodology, the accuracy is 100% (98.86% in a complex case). The new Sine methodology performs better than the alternatives. The prediction accuracy of the first case is better than the complex example. The complex example contains an increased number of features that are more difficult to manufacture and inspect. Therefore, the complex example requires a higher level of computational efforts for prediction, along with the tighter UTL, LTL, and RL. Such compounding effects have contributed to a decreased level of prediction accuracy. In the industry applications, the proposed methodology can be utilized and implemented on the shop floor without a special setup. The new Sine methodology can accommodate as many features as needed, which can classify the parts into good, rework, and bad classes with a very high level of accuracy.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<p>This work was partially supported by the U.S. National Science Foundation (CCLI Phase I DUE-0737539), the US Department of Education (Award #P116B080100A). This article was completed with Ajou University Research Fellowship of 2011. This work was also partially supported by the Basic Science Research Program through the National Research Foundation (NRF) of Korea funded by the Ministry of Education, Science and Technology (grant no. 2010-0012517). The authors wish to express sincere gratitude for the financial support.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cai</surname><given-names>CZ</given-names></name>
<name><surname>Wang</surname><given-names>WL</given-names></name>
<name><surname>Sun</surname><given-names>LZ</given-names></name>
<etal/>
</person-group>. (<year>2003</year>) <article-title>Protein function classification via support vector machine approach</article-title>. <source>Mathematical Biosciences</source> <volume>185</volume>(<issue>2</issue>): <fpage>111</fpage>–<lpage>122</lpage>.</citation>
</ref>
<ref id="bibr2-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Calster</surname><given-names>B</given-names></name>
<name><surname>Condous</surname><given-names>G</given-names></name>
<name><surname>Kirk</surname><given-names>E</given-names></name>
<etal/>
</person-group>. (<year>2009</year>) <article-title>An application of methods for the probabilistic three-class classification of pregnancies of unknown location</article-title>. <source>Artificial Intelligence in Medicine</source> <volume>46</volume>(<issue>2</issue>): <fpage>139</fpage>–<lpage>154</lpage>.</citation>
</ref>
<ref id="bibr3-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chao</surname><given-names>L</given-names></name>
<name><surname>Tong</surname><given-names>L</given-names></name>
</person-group> (<year>2009</year>) <article-title>Water defect pattern recognition by multi-class support vector machines by using a novel defect cluster index</article-title>. <source>Sensors and Actuators</source> <volume>122</volume>(<issue>1</issue>): <fpage>227</fpage>–<lpage>235</lpage>.</citation>
</ref>
<ref id="bibr4-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>X</given-names></name>
<name><surname>Zeng</surname><given-names>X</given-names></name>
<name><surname>Alphen</surname><given-names>D</given-names></name>
</person-group> (<year>2006</year>) <article-title>Multi-class feature selection for texture classification</article-title>. <source>Pattern Recognition Letters</source> <volume>27</volume>(<issue>14</issue>): <fpage>1685</fpage>–<lpage>1691</lpage>.</citation>
</ref>
<ref id="bibr5-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Comack</surname><given-names>E</given-names></name>
<name><surname>Arslan</surname><given-names>A</given-names></name>
</person-group> (<year>2008</year>) <article-title>A new training method for support vector machines: clustering k-NN support vector machines</article-title>. <source>Expert Systems with Applications</source> <volume>35</volume>(<issue>3</issue>): <fpage>564</fpage>–<lpage>568</lpage>.</citation>
</ref>
<ref id="bibr6-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Du</surname><given-names>C</given-names></name>
<name><surname>Sun</surname><given-names>D</given-names></name>
</person-group> (<year>2008</year>) <article-title>Multi-classification of pizza using computer vision and support vector machine</article-title>. <source>Journal of Food Engineering</source> <volume>86</volume>(<issue>2</issue>): <fpage>234</fpage>–<lpage>242</lpage>.</citation>
</ref>
<ref id="bibr7-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hong</surname><given-names>J</given-names></name>
<name><surname>Cho</surname><given-names>S</given-names></name>
</person-group> (<year>2008</year>) <article-title>A probabilistic multiclass strategy of one-vs-rest support vector machines for cancer classification</article-title>. <source>Neurocomputing</source> <volume>71</volume>(<issue>16–18</issue>): <fpage>3275</fpage>–<lpage>3281</lpage>.</citation>
</ref>
<ref id="bibr8-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hong</surname><given-names>J</given-names></name>
<name><surname>Min</surname><given-names>J</given-names></name>
<name><surname>Cho</surname><given-names>U</given-names></name>
<etal/>
</person-group>. (<year>2008</year>) <article-title>Fingerprint classification using one-versus-all support vector machines dynamically ordered with Naïve Bayes classifiers</article-title>. <source>Pattern Recognition</source> <volume>41</volume>(<issue>2</issue>): <fpage>662</fpage>–<lpage>671</lpage>.</citation>
</ref>
<ref id="bibr9-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Horng</surname><given-names>M</given-names></name>
</person-group> (<year>2009</year>) <article-title>Multi-class support vector machine for classification of the ultrasonic images of supraspinatus</article-title>. <source>Expert Systems with Applications</source> <volume>36</volume>(<issue>4</issue>): <fpage>8124</fpage>–<lpage>8133</lpage>.</citation>
</ref>
<ref id="bibr10-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hsu</surname><given-names>CW</given-names></name>
<name><surname>Lin</surname><given-names>CJ</given-names></name>
</person-group> (<year>2002</year>) <article-title>A comparison of methods for multi-class support vector machines</article-title>. <source>IEEE Transactions on Neural Networks</source> <volume>13</volume>(<issue>2</issue>): <fpage>415</fpage>–<lpage>425</lpage>.</citation>
</ref>
<ref id="bibr11-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jayadeva</surname><given-names>KR</given-names></name>
<name><surname>Chandra</surname><given-names>S</given-names></name>
</person-group> (<year>2005</year>) <article-title>Fuzzy linear proximal support vector machines for multi-category data classification</article-title>. <source>Neurocomputing</source> <volume>67</volume>: <fpage>426</fpage>–<lpage>435</lpage>.</citation>
</ref>
<ref id="bibr12-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>D</given-names></name>
<name><surname>Liu</surname><given-names>C</given-names></name>
</person-group> (<year>2010</year>) <article-title>A class possibility based kernel to increase classification accuracy for small data sets using support vector machines</article-title>. <source>Expert Systems with Applications</source> <volume>37</volume>(<issue>4</issue>): <fpage>3104</fpage>–<lpage>3110</lpage>.</citation>
</ref>
<ref id="bibr13-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Meyer</surname><given-names>D</given-names></name>
<name><surname>Leisch</surname><given-names>F</given-names></name>
<name><surname>Hornik</surname><given-names>K</given-names></name>
</person-group> (<year>2003</year>) <article-title>The support vector machine under test</article-title>. <source>Neurocomputing</source> <volume>55</volume>(<issue>1–2</issue>): <fpage>169</fpage>–<lpage>186</lpage>.</citation>
</ref>
<ref id="bibr14-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mondal</surname><given-names>S</given-names></name>
<name><surname>Bhavna</surname><given-names>R</given-names></name>
<name><surname>Babu</surname><given-names>RM</given-names></name>
<etal/>
</person-group>. (<year>2006</year>) <article-title>Pseudo amino acid composition and multi-class support vector machines approach for conotoxins superfamily classification</article-title>. <source>Journal of Theoretical Biology</source> <volume>243</volume>(<issue>2</issue>): <fpage>252</fpage>–<lpage>260</lpage>.</citation>
</ref>
<ref id="bibr15-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Prasad</surname><given-names>B</given-names></name>
</person-group> (<year>1995</year>) <article-title>On influencing agents of CE</article-title>. <source>Journal of Concurrent Engineering: Research and Applications</source> <volume>3</volume>(<issue>2</issue>): <fpage>78</fpage>–<lpage>80</lpage>.</citation>
</ref>
<ref id="bibr16-1063293X11429160">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Prasad</surname><given-names>B</given-names></name>
</person-group> (<year>1996</year>) <source>Concurrent Engineering Fundamentals</source>, <volume>Vol. 1</volume>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr17-1063293X11429160">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Prasad</surname><given-names>B</given-names></name>
</person-group> (<year>1997</year>) <source>Concurrent Engineering Fundamentals</source>, <volume>Vol. 2</volume>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr18-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Prasad</surname><given-names>B</given-names></name>
</person-group> (<year>1998</year>) <article-title>How tools techniques in concurrent engineering contribute towards easing cooperation, creativity and uncertainty</article-title>. <source>Journal of Concurrent Engineering: Research and Applications</source> <volume>6</volume>(<issue>1</issue>): <fpage>2</fpage>–<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr19-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Prasad</surname><given-names>B</given-names></name>
</person-group> (<year>1999</year>) <article-title>Enabling principles of concurrency and simultaneity in concurrent engineering</article-title>. <source>Artificial Intelligence for Engineering Design Analysis and Manufacturing</source> <volume>13</volume>(<issue>3</issue>): <fpage>185</fpage>–<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr20-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shen</surname><given-names>K</given-names></name>
<name><surname>Li</surname><given-names>X</given-names></name>
<name><surname>Ong</surname><given-names>C</given-names></name>
<etal/>
</person-group>. (<year>2008</year>) <article-title>EEG-based mental fatigue measurement using multi-class support vector machines</article-title>. <source>Clinical Neurophysiology</source> <volume>119</volume>(<issue>7</issue>): <fpage>1524</fpage>–<lpage>1533</lpage>.</citation>
</ref>
<ref id="bibr21-1063293X11429160">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shigeo</surname><given-names>A</given-names></name>
</person-group> (<year>2005</year>) <source>Support Vector Machines for Pattern Classification</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr22-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sugumaran</surname><given-names>V</given-names></name>
<name><surname>Sabareesh</surname><given-names>GR</given-names></name>
<name><surname>Ramachandran</surname><given-names>KI</given-names></name>
</person-group> (<year>2008</year>) <article-title>Fault diagnostics of roller bearing using Kernel based neighboring hood score multi-class support vector machine</article-title>. <source>Expert Systems with Applications</source> <volume>34</volume>(<issue>2</issue>): <fpage>3090</fpage>–<lpage>3098</lpage>.</citation>
</ref>
<ref id="bibr23-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Suykens</surname><given-names>JAK</given-names></name>
<name><surname>Vandewalle</surname><given-names>J</given-names></name>
</person-group> (<year>1999</year>) <article-title>Least squares support vector machine classifiers</article-title>. <source>Neural Processing Letters</source> <volume>9</volume>: <fpage>293</fpage>–<lpage>300</lpage>.</citation>
</ref>
<ref id="bibr24-1063293X11429160">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vapnik</surname><given-names>V</given-names></name>
<name><surname>Chervonenkis</surname><given-names>A</given-names></name>
</person-group> (<year>1964</year>) <source>A Note on One Class of Perceptrons. Automation and Remote Control</source>, <volume>vol. 25</volume>.</citation>
</ref>
</ref-list>
</back>
</article>