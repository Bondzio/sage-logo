<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AEI</journal-id>
<journal-id journal-id-type="hwp">spaei</journal-id>
<journal-title>Assessment for Effective Intervention</journal-title>
<issn pub-type="ppub">1534-5084</issn>
<issn pub-type="epub">1938-7458</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1534508412447010</article-id>
<article-id pub-id-type="publisher-id">10.1177_1534508412447010</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Form Effects on DIBELS Next Oral Reading Fluency Progress- Monitoring Passages</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Petscher</surname><given-names>Yaacov</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Cummings</surname><given-names>Kelli D.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Biancarosa</surname><given-names>Gina</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Fien</surname><given-names>Hank</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Cummings</surname><given-names>Kelli D.</given-names></name>
<degrees>PhD, NCSP</degrees>
<xref ref-type="aff" rid="aff1-1534508412447010">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Park</surname><given-names>Yonghan</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff2-1534508412447010">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bauer Schaper</surname><given-names>Holle A.</given-names></name>
<degrees>MS</degrees>
<xref ref-type="aff" rid="aff1-1534508412447010">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1534508412447010"><label>1</label>University of Oregon, Eugene, OR, USA</aff>
<aff id="aff2-1534508412447010"><label>2</label>Gangneung-Wonju National University, Republic of Korea</aff>
<author-notes>
<corresp id="corresp1-1534508412447010">Kelli D. Cummings, Center on Teaching and Learning, 5292 University of Oregon, Eugene, OR 97403-5292, USA Email: <email>kellic@uoregon.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>38</volume>
<issue>2</issue>
<issue-title>Special Series: Measurement Issues in the Assessment of Reading Fluency</issue-title>
<fpage>91</fpage>
<lpage>104</lpage>
<permissions>
<copyright-statement>© 2013 Hammill Institute on Disabilities</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">Hammill Institute on Disabilities</copyright-holder>
</permissions>
<abstract>
<p>The purpose of this article is to describe passage effects on <italic>Dynamic Indicators of Basic Early Literacy Skills–Next Edition Oral Reading Fluency</italic> (<italic>DIBELS Next ORF</italic>) progress-monitoring measures for Grades 1 through 6. Approximately 572 students per grade (total <italic>N</italic> with at least one data point = 3,092) read all three <italic>DIBELS Next</italic> winter benchmark passages in the prescribed order, and within 2 weeks read four additional progress-monitoring passages in a randomly assigned and counterbalanced order. All 20 progress-monitoring passages were read by students in Grades 1 through 4; 16 passages were read in Grade 5 and 12 passages were read in Grade 6. Results focus on the persistence of form effects in spite of a priori criteria used in passage development. The authors describe the utility of three types of equating methods (i.e., mean, linear, and equipercentile equating) in ameliorating these effects. Their conclusions focus on preferred equating methods with small samples, the impact of form effects on progress-monitoring decision making, and recommendations for future use of ORF passages for progress monitoring.</p>
</abstract>
<kwd-group>
<kwd>achievement assessment</kwd>
<kwd>curriculum-based measurement</kwd>
<kwd>measurement theory</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>As the most widely used curriculum-based measurement (CBM) tool, oral reading fluency (ORF) boasts a 30-year history of research, implementation, and scale-up utility in education. The purpose of this manuscript is to describe a rationale for ORF passage equating as part of an efficient measurement system in early literacy. We propose that the rationale for equating holds true even if passages are authored with consistent readability levels, as assessed by standard readability formulae, and are empirically leveled. We provide preliminary evidence of the value of equated scores on the <italic>Dynamic Indicators of Basic Early Literacy Skills–Next Edition</italic> (<italic>DIBELS Next</italic>) progress-monitoring passages using three distinct equating procedures. Last, we provide suggestions for the way that equating data can be used, without sacrificing the efficiency and interpretability of CBM.</p>
<sec id="section1-1534508412447010">
<title>History of CBM</title>
<p>CBM was developed as a system for <italic>formative assessment</italic>, a methodology for adapting teaching to meet student needs (<xref ref-type="bibr" rid="bibr17-1534508412447010">Deno, 1985</xref>; <xref ref-type="bibr" rid="bibr31-1534508412447010">Kaminski &amp; Cummings, 2007</xref>). In addition to establishing individual student goals, formative assessment also provides a database on which effective instructional programs may be developed and evaluated (<xref ref-type="bibr" rid="bibr19-1534508412447010">Fuchs, 1986</xref>; <xref ref-type="bibr" rid="bibr47-1534508412447010">Taylor, Roehrig, Hensler, Connor, &amp; Schatschneider, 2010</xref>). Although the primary purpose of CBM remains the support of individual student learning, recent use of CBM technology has broadened considerably. CBM is now inextricably linked to assessment practices for purposes of response to intervention and other federally supported initiatives for educational decision making (<xref ref-type="bibr" rid="bibr12-1534508412447010">Cummings, Atkins, Allison, &amp; Cole, 2008</xref>).</p>
<p>Reading CBM (R-CBM) has become one of, if not the most, widely used assessment in education today (<xref ref-type="bibr" rid="bibr26-1534508412447010">Graney &amp; Shinn, 2005</xref>). Used for a broad range of decisions, assessment with R-CBM has become common practice in many U.S. schools for all students, including students in general and special education. The practice of progress monitoring with R-CBM is recognized in the education research community (e.g., <xref ref-type="bibr" rid="bibr29-1534508412447010">Hixson, Christ, &amp; Bradley-Johnson, 2008</xref>), and a federally funded research center has been formed to provide technical assistance to states and review the technical adequacy of progress-monitoring tools (i.e., National Center on Response to Intervention ([NCRTI]; <ext-link ext-link-type="uri" xlink:href="http://www.rti4success.org">http://www.rti4success.org</ext-link>). According to the NCRTI, there are at least six unique passage reading fluency assessments available. Although test authors rarely report usage statistics, public information is available from the DIBELS Data System (DDS) at the University of Oregon. According to <xref ref-type="bibr" rid="bibr15-1534508412447010">Cummings, Otterstedt, Kennedy, Baker, and Kame’enui (2011)</xref>, at least one in every six U.S. public schools (Grades K–6) utilizes the <italic>DIBELS ORF</italic> (DORF) assessment for R-CBM. In the current educational milieu of universal screening, accountability, and early intervention, learning more about the psychometric properties of such a widely used tool is essential.</p>
<p>Converging evidence has demonstrated CBM’s validity in the following key areas: (a) CBM displays high degrees of <italic>content validity</italic> because the content for CBM is either based on or mirrors the daily curriculum taught in the classroom (<xref ref-type="bibr" rid="bibr7-1534508412447010">Capizzi &amp; Barton-Arwood, 2009</xref>; <xref ref-type="bibr" rid="bibr20-1534508412447010">Fuchs &amp; Deno, 1994</xref>); (b) CBM displays high levels of <italic>decision utility</italic> (<xref ref-type="bibr" rid="bibr35-1534508412447010">Messick, 1989</xref>) in that it can be used to make instructional modifications when needed and results in better, more responsive teaching (<xref ref-type="bibr" rid="bibr17-1534508412447010">Deno, 1985</xref>; <xref ref-type="bibr" rid="bibr21-1534508412447010">Fuchs &amp; Fuchs, 2003</xref>); and (c) CBM has evidence of <italic>discriminant validity</italic> in that students who are grouped based on CBM data are more likely to benefit from similar instruction than students who are grouped based on other assessments (<xref ref-type="bibr" rid="bibr33-1534508412447010">Kranzler, Brownell, &amp; Miller, 1998</xref>; <xref ref-type="bibr" rid="bibr48-1534508412447010">Wesson, Vierthaler, &amp; Haubrich, 1989</xref>).</p>
<p>This set of technical features uniquely suits CBM for use in making individualized decisions about student progress. However, recent research findings have called into question the validity of CBM procedures due to error variability caused by nonequivalent passage sets (<xref ref-type="bibr" rid="bibr6-1534508412447010">Betts, Pickart, &amp; Heistad, 2009</xref>; <xref ref-type="bibr" rid="bibr18-1534508412447010">Francis et al., 2008</xref>).</p>
</sec>
<sec id="section2-1534508412447010">
<title>Alternate Forms and the Premise of Equality</title>
<p>As the most common R-CBM tool (<xref ref-type="bibr" rid="bibr41-1534508412447010">Petscher, Kim, &amp; Foorman, 2011</xref>), DORF has provided an important platform for education research. Like other CBM measures, DORF consists of multiple forms that are calibrated to end-of-year instructional levels associated with specific grades. Although probe sets differ across grades, multiple probes <italic>within</italic> grade-level sets are developed to be interchangeable in terms of their difficulty level (<xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith, Good, &amp; Atkins, 2010</xref>). Early research on CBM (e.g., <xref ref-type="bibr" rid="bibr44-1534508412447010">Shinn, Gleason, &amp; Tindal, 1989</xref>) downplayed the impact of variations in passage difficulty when estimating student performance and growth trajectories, but recent research has demonstrated that the ORF scores for a single student can vary significantly depending on the passage administered (<xref ref-type="bibr" rid="bibr6-1534508412447010">Betts et al., 2009</xref>; <xref ref-type="bibr" rid="bibr18-1534508412447010">Francis et al., 2008</xref>) and that this score variation can be predicted by the passage difficulty level.</p>
<p>These <italic>form effects</italic> (<xref ref-type="bibr" rid="bibr3-1534508412447010">Ardoin &amp; Christ, 2009</xref>; <xref ref-type="bibr" rid="bibr11-1534508412447010">Christ &amp; Silberglitt, 2007</xref>; <xref ref-type="bibr" rid="bibr18-1534508412447010">Francis et al., 2008</xref>) refer to passage-level variability that persists in spite of the application of a priori readability formulae. <xref ref-type="bibr" rid="bibr40-1534508412447010">Petscher and Kim (2011)</xref> highlight the impact that these differences in difficulty levels have for the screening accuracy and efficiency of DORF. Variations in passage difficulty affect the relationship between ORF measures and other criterion tests of reading, and can create an additional burden for schools in terms of the assessment load. Current administration and scoring guidelines for DORF also redistribute the burden of dealing with passage variability to schools’ testing teams. We describe the administration and scoring procedures in this way because schools are required to administer multiple passages for screening (i.e., benchmark testing) and then to report the best representative summary score (i.e., median or mean). Not only does administering multiple forms increase the time, effort, and cost of assessment, but evidence also suggests that this <italic>median method</italic> procedure does not satisfactorily remove form effects (<xref ref-type="bibr" rid="bibr9-1534508412447010">Chaparro et al., 2012</xref>; <xref ref-type="bibr" rid="bibr46-1534508412447010">Stoolmiller, Biancarosa, &amp; Fien, 2012</xref>).</p>
<sec id="section3-1534508412447010">
<title>Current Available Methods to Address Form Effects</title>
<p>ORF passages typically display high levels of agreement between alternative forms (i.e., between .80 and .90), and the DORF passages are no exception (<xref ref-type="bibr" rid="bibr24-1534508412447010">Good &amp; Kaminski, 2011b</xref>; <xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith et al., 2010</xref>). In spite of strong correlations between forms and invariance across passages in terms of readability estimates, neither of these statistics takes into account absolute score differences across forms (<xref ref-type="bibr" rid="bibr2-1534508412447010">Albano &amp; Rodriguez, 2011</xref>). In current, common practice, there are three primary ways in which form effects are controlled. We refer to these methods as a priori, procedural, and statistical.</p>
<sec id="section4-1534508412447010">
<title>A priori equating methods</title>
<p>Initial CBM-R methodology used samples of reading material taken directly from the generalized curriculum (<xref ref-type="bibr" rid="bibr43-1534508412447010">Shinn, 1989</xref>), and curriculum level served as a proxy for readability. At present, most assessment authors use readability formulae to assign passages to grade levels. Consistent with original CBM tenets of using a long-term goal measurement strategy (e.g., <xref ref-type="bibr" rid="bibr44-1534508412447010">Shinn et al., 1989</xref>), test authors typically link all passages to end-of-year text difficulty levels. Numerous studies affirm that, although readability levels provide desirable initial targets during the passage development phase, these formulae are insufficient to entirely remove form effects (<xref ref-type="bibr" rid="bibr2-1534508412447010">Albano &amp; Rodriguez, 2011</xref>; <xref ref-type="bibr" rid="bibr10-1534508412447010">Christ &amp; Ardoin, 2009</xref>).</p>
<p>To address the limitations of readability formulae, some assessment authors have turned to field testing (<xref ref-type="bibr" rid="bibr30-1534508412447010">Howe &amp; Shinn, 2002</xref>). As <xref ref-type="bibr" rid="bibr4-1534508412447010">Ardoin, Suldo, Witt, Aldrich, and McDonald (2005)</xref> note, actual student reading performance may be the best and only valid measure of passage difficulty. A study by <xref ref-type="bibr" rid="bibr10-1534508412447010">Christ and Ardoin (2009)</xref> confirms that passage sets that had undergone field testing, in which the most variable passages were removed, had lower standard errors of measurement than passages leveled only using readability formulae. <xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith et al. (2010)</xref> also demonstrate that the variability of DIBELS Next edition passages was reduced after the 32 least variable passages were selected from an original group of 40.</p>
</sec>
<sec id="section5-1534508412447010">
<title>Procedural methods</title>
<p>A priori methods consistently disappoint when it comes to completely ameliorating passage-to-passage variability. Another common approach to address variability has a history in original R-CBM administration procedures (<xref ref-type="bibr" rid="bibr34-1534508412447010">Marston, 1989</xref>), in which three reading passages are administered during a screening assessment and the median score is reported. Two of the six available reading assessments that are reported to the <xref ref-type="bibr" rid="bibr38-1534508412447010">NCRTI (n. d.)</xref> require three passages to be administered at each benchmark and progress-monitoring session. Although lengthening tests through repeated measurement certainly reduces variability, it is unclear that passing this expense on to schools is either appropriate or the best available option.</p>
</sec>
<sec id="section6-1534508412447010">
<title>Statistical methods</title>
<p>Statistical methods for equating are relevant to all tests that involve alternate forms. Although most of the historical work on equating is based on classical tests in education, equating procedures have been recently applied to ORF passage sets with some success (e.g., <xref ref-type="bibr" rid="bibr6-1534508412447010">Betts et al., 2009</xref>; <xref ref-type="bibr" rid="bibr18-1534508412447010">Francis et al., 2008</xref>). Equating is a relatively straightforward procedure, in which all versions of a test are linked, or scaled, to a common test form. Using R-CBM as an example, an appropriate equating result would allow an individual teacher to convert a raw score from one passage to a scaled score that is expressed in “common passage units,” with a specific mean and standard deviation. Given the linked screening-to-progress-monitoring format that most R-CBM test developers use, it makes sense to consider various progress-monitoring passages as linked to the most representative screening passage (see <xref ref-type="bibr" rid="bibr2-1534508412447010">Albano &amp; Rodriguez, 2011</xref>). In this way, the equated scores from any progress-monitoring form could be used interchangeably with the criterion-referenced goals that are typically associated with the more widely studied screening measures.</p>
</sec>
</sec>
<sec id="section7-1534508412447010">
<title>Challenges to Statistical Equating Method</title>
<p>Equating is not without its flaws nor is it a panacea. There is no substitute for careful passage authoring, field-testing, and passage review by content area experts. In fact, equating is only a successful procedure when it is applied to test forms that are designed to be exactly the same (<xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen &amp; Brennan, 2004</xref>). Even when all a priori assumptions are met, equating may still lead to inaccurate results or may introduce more error than it controls (<xref ref-type="bibr" rid="bibr46-1534508412447010">Stoolmiller et al., 2012</xref>)—particularly, if standard errors are not considered. As a final, notable challenge, equating results may be more difficult to interpret and could potentially remove some of the authenticity of R-CBM. Although raw scores have been demonstrated to contain more error variability than many equated scores, they still carry power in that the score that is plotted is one that can be directly observed rather than one that must be converted.</p>
</sec>
<sec id="section8-1534508412447010">
<title>The Future of ORF Assessment</title>
<p>We present the idea that the accuracy of R-CBM testing results may be improved by reducing error variance through statistical equating. We propose that the reading passages will retain their authenticity, although estimates of “true scores” may be more precise. Our recommendation is a balanced approach that includes streamlined assessment procedures for progress monitoring. When passages are equated, fewer passages can lead to better decisions about student progress. Like <xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen and Brennan (2004)</xref>, we know of no other way to accomplish this goal outside of formal equating methods.</p>
</sec>
</sec>
<sec id="section9-1534508412447010">
<title>Purpose of the Study</title>
<p>In this study, we address three key research questions:</p>
<list id="list1-1534508412447010" list-type="simple">
<list-item><p><italic>Research Question 1</italic>: Is there significant passage variability between the progress-monitoring forms that comprise the DORF testing materials?</p></list-item>
<list-item><p><italic>Research Question 2</italic>: Does the application of classic statistical equating procedures (i.e., mean, linear, and equipercentile equating) reduce this passage variability?</p></list-item>
<list-item><p><italic>Research Question 3</italic>: Which equating method works best in our sample?</p></list-item></list>
</sec>
<sec id="section10-1534508412447010" sec-type="methods">
<title>Method</title>
<sec id="section11-1534508412447010">
<title>Sample and Participant Selection</title>
<p>Approximately 3,092 students from 28 schools in 19 districts across 15 states formed the basis of our participant group in this study. We used a multistage cluster sampling strategy (<xref ref-type="bibr" rid="bibr39-1534508412447010">O’Connell &amp; McCoach, 2008</xref>) to select schools from a group (<italic>n</italic> = 87) of participants who were involved in a study examining characteristics of schools that elect to use the DDS for management of their screening and progress-monitoring R-CBM data (i.e., <italic>Sentinel Schools Project</italic>; <xref ref-type="bibr" rid="bibr13-1534508412447010">Cummings, Bousselot, et al., 2011</xref>). As a result, we have detailed information on all schools in the original sampling frame including school-level demographic statistics, assessment fidelity, instructional context, and school resources (see <xref ref-type="bibr" rid="bibr14-1534508412447010">Cummings, Kennedy, Otterstedt, Baker, &amp; Kame’enui, 2011</xref>). The 87 schools represent a sample of convenience, from which we invoked our particular sampling plan so that the subsample of schools in the equating study would represent the U.S. population in terms of the demographic characteristics that are reported to the <xref ref-type="bibr" rid="bibr37-1534508412447010">National Center for Education Statistics (2011)</xref>.</p>
<p>We present demographic characteristics for the Grade 2 students in our sample, whom we highlight in many of our results. In Grade 2, 50.3% of students are female; 53.2% are identified as White, 19.7% Hispanic, 15.5% Black, 5.5% American Indian/Alaskan Native, 5% two or more races, and 1% Asian, Hawaiian Native, or Pacific Islander. Nine percent of Grade 2 students in our sample had an Individualized Education Program (IEP), and 11.9% are learning English as another language. The percentage of students identified as Hispanic varied significantly across grades (range = 11.0% in Grade 6 to 22.8% in Grade 4; median = 19.2%), as did the percentage of students identified as White (range = 53.2% in Grades 2 and 4 to 63.4% in Grade 6; median = 55.8%). IEP and English Learner status also displayed across-grade variability. The percentage of students in special education ranged from 9.0% (Grade 2) to 13.8% (Grade 5; median = 10.4%). The percentage of English learners ranged from 6.1% (Grade 6) to 11.9% (Grade 2; median = 9%).</p>
<p>In addition to demographic data, it is also important to characterize participants in terms of their achievement levels. In fact, initial (i.e., fall of a school year) skill may account for more student- and school-level variability than traditional demographic data (<xref ref-type="bibr" rid="bibr28-1534508412447010">Hedges &amp; Hedberg, 2007</xref>; <xref ref-type="bibr" rid="bibr45-1534508412447010">Stoolmiller, 2008</xref>). The beginning of year average DORF performance for students in Grades 2 through 6 corresponds to the average range (48th–51st percentiles; median = 49th) of the DDS systemwide norms for the same measures (<xref ref-type="bibr" rid="bibr14-1534508412447010">Cummings, Kennedy, et al., 2011</xref>). As part of a separate aim of the Sentinel Schools Project, students in our sample also were administered the 10th edition of the <italic>Stanford Achievement Test</italic> (SAT10; <xref ref-type="bibr" rid="bibr27-1534508412447010">Harcourt, 2007</xref>). Grade-level SAT10 performance also falls within the average range for the Total Reading score across all grades (35th–44th percentiles; median = 48th).</p>
</sec>
<sec id="section12-1534508412447010">
<title>DIBELS Next ORF</title>
<p>DORF is a standardized, individually administered test of reading accuracy and fluency. The student reads a passage aloud for 1 min, and the examiner records the number of words read correctly. The DORF passages that we used in our study were drawn from the 20 progress-monitoring passages (per grade) used in DIBELS Next (<xref ref-type="bibr" rid="bibr23-1534508412447010">Good &amp; Kaminski, 2011a</xref>; <xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith et al., 2010</xref>). DIBELS Next passages differ from the more widely studied sixth edition (e.g., <xref ref-type="bibr" rid="bibr5-1534508412447010">Baker et al., 2008</xref>) in terms of the equating processes. DIBELS Next passages were authored using a proprietary readability formula (i.e., Dynamic Measurement Group [DMG] Passage Difficulty Index; <xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith et al., 2010</xref>), whereas DIBELS sixth edition passages were written to correspond to an end-of-grade level <italic>Spache</italic> readability result (<xref ref-type="bibr" rid="bibr22-1534508412447010">Good &amp; Kaminski, 2002</xref>). The DIBELS Next passages were also field-tested, which did not occur for the sixth edition (<xref ref-type="bibr" rid="bibr10-1534508412447010">Christ &amp; Ardoin, 2009</xref>). <xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith et al. (2010)</xref> selected the DIBELS Next passages from a group of 40 initial passages that had been authored to correspond to a consistent range on their passage difficulty index. The final 32 reading passages (9 for benchmark assessment, 20 for progress monitoring, and 3 for survey level assessment) were selected as those with the (a) smallest range of fluency scores across students in a pilot study (<xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith et al., 2010</xref>; range of student <italic>n</italic> per grade = 22–25) and (b) highest alternate-form reliability and validity coefficients.</p>
<p>Median alternate-form reliability coefficients for a single DIBELS Next DORF progress-monitoring passage range from .84 to .95 (overall median = .92) for students in Grades 1 through 6. <xref ref-type="bibr" rid="bibr42-1534508412447010">Powell-Smith et al. (2010)</xref> also report validity coefficients based on concurrent administration of a Grade 4 reading passage from the National Assessment of Education Progress (NAEP) 2002 Special Study of Oral Reading (<xref ref-type="bibr" rid="bibr16-1534508412447010">Daane, Campbell, Grigg, Goodman, &amp; Oranje, 2005</xref>) and a selected passage from the sixth edition of the DORF (<xref ref-type="bibr" rid="bibr25-1534508412447010">Good, Kaminski, &amp; Dill, 2002</xref>). Median concurrent validity coefficients ranged from .80 to .94, with the NAEP (overall median = .93) and from .81 to .93 (overall median = .89) with the DORF sixth edition.</p>
</sec>
<sec id="section13-1534508412447010">
<title>Procedures</title>
<sec id="section14-1534508412447010">
<title>Study design</title>
<p>We utilized a common-population, equivalent group equating design (<xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen &amp; Brennan, 2004</xref>) with counterbalancing and random assignment to different sets (or blocks) of passages to equate. Students also read three DORF passages in a common order as part of their schools’ standard practice for winter benchmark testing. We used the first benchmark passage from the winter triad as the reference form for equating in each grade.</p>
<p>Within 10 school days of winter benchmarking, students read four DORF progress-monitoring passages in a specific, counterbalanced order. We separated the DORF passages into five blocks, and then arranged passages within each block so that the first block contained progress-monitoring Passages 1 through 4, the second included Passages 5 through 8, and so on through the last block that was comprised of Passages 16 through 20. We used a cluster sampling approach based on average fall school achievement to assign schools to one of these five blocks. The average fall DORF score did not differ significantly between these blocks. We list the total number of participating students across block and by grade in <xref ref-type="table" rid="table1-1534508412447010">Table 1</xref>.</p>
<table-wrap id="table1-1534508412447010" position="float">
<label>Table 1.</label>
<caption>
<p>Description of Sample and Design</p>
</caption>
<graphic alternate-form-of="table1-1534508412447010" xlink:href="10.1177_1534508412447010-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="6">Block for Progress-Monitoring Passages<hr/></th>
</tr>
<tr>
<th align="left">Grade</th>
<th align="center">Reference Passage (MOY1)</th>
<th align="center">I (PM01–PM04)</th>
<th align="center">II (PM05–PM08)</th>
<th align="center">III (PM09–PM012)</th>
<th align="center">IV (PM13–PM16)</th>
<th align="center">V (PM17–PM20)</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>586</td>
<td>109</td>
<td>125</td>
<td>116</td>
<td>115</td>
<td>104</td>
<td>569</td>
</tr>
<tr>
<td>2</td>
<td>591</td>
<td>90</td>
<td>144</td>
<td>114</td>
<td>138</td>
<td>93</td>
<td>579</td>
</tr>
<tr>
<td>3</td>
<td>586</td>
<td>88</td>
<td>135</td>
<td>114</td>
<td>89</td>
<td>136</td>
<td>562</td>
</tr>
<tr>
<td>4</td>
<td>560</td>
<td>90</td>
<td>106</td>
<td>121</td>
<td>130</td>
<td>92</td>
<td>539</td>
</tr>
<tr>
<td>5</td>
<td>445</td>
<td>114</td>
<td>117</td>
<td>114</td>
<td>87</td>
<td>—</td>
<td>432</td>
</tr>
<tr>
<td>6</td>
<td>311</td>
<td>122</td>
<td>118</td>
<td>68</td>
<td>—</td>
<td>—</td>
<td>308</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1534508412447010">
<p><italic>Note</italic>. MOY1 = the first benchmark passage in the winter triad, used as the reference passage in our study; PM = the progress-monitoring passage, 20 unique progress-monitoring forms per grade. Total number of students with at least one data point is 3,092. Cell values indicate the number of students assigned to each block of four passages; per passage sample sizes may vary slightly.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>We randomly selected 25 students from each school to participate in the equating study and randomly assigned each of these students to a passage order within their school’s block. All four passages in each block were read during a single session in one of four possible orders. Similar to <xref ref-type="bibr" rid="bibr18-1534508412447010">Francis et al. (2008)</xref>, we did not arrange each block in all possible orders but rather ensured that each passage appeared in all possible positions (i.e., first, second, third, and last).</p>
</sec>
<sec id="section15-1534508412447010">
<title>Data collection</title>
<p>School personnel collected all DORF data, during benchmark testing and for the equating study. Participating schools have been using the DIBELS measures for an average of 6 years, and all schools reported receiving specific training on the DIBELS Next administration and scoring procedures. In addition, in the fall, project staff conducted web-based refresher trainings on all DIBELS Next administration and scoring rules with school-based coordinators and their testing teams.</p>
<p>School-based coordinators collected and mailed all test packets back to the University of Oregon where project staff rescored 100% of DORF probes to calculate interscorer agreement. At least two project staff rescored each probe and recorded a final, corrected score in cases where their scores disagreed with the school-based rater. If the two project staff could not agree on how to rescore the item, the principal investigator (and first author) was the deciding scorer. In all grades, the median interscorer agreement was strong, at .990 or above. Still, all analyses reported the corrected scores as determined by project staff.</p>
</sec>
<sec id="section16-1534508412447010">
<title>Analytic approach</title>
<p>This study involved three primary analytic procedures: tests of passage effects, statistical equating, and comparison of equating methods. All analyses were conducted separately for each grade (1–6).</p>
<p>To examine DORF form effects, we used a linear mixed-model analysis of variance (ANOVA) with repeated measures nested within each student. In addition to passage effects, we included presentation order as a predictor. We examined the effects of passage and presentation order before and after applying statistical equating. The base model for our analysis is as follows:</p>
<p><disp-formula id="disp-formula1-1534508412447010">
<mml:math display="block" id="math1-1534508412447010">
<mml:mrow>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>μ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>y</mml:mi>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>ε</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-1534508412447010" xlink:href="10.1177_1534508412447010-eq1.tif"/>
</disp-formula></p>
<p>where γ<sub><italic>ijk</italic></sub> is the DORF score for student <italic>i</italic> on passage <italic>j</italic> in order <italic>k</italic>; µ<sub><italic>i</italic></sub>~ <italic>N</italic>(0, σ<sup>2</sup>) and is the adjusted student DORF score across passages and orders for student <italic>i</italic>; α<sub><italic>j</italic></sub> is the fixed effect of passage <italic>j</italic>; β<sub><italic>k</italic></sub> is the fixed effect of presentation order <italic>k</italic>; γ<sub><italic>jk</italic></sub> is the interaction effect of passage <italic>j</italic> and presentation order <italic>k</italic>; and ϵ<sub><italic>ijk</italic></sub> is the random effect across repeated measures.</p>
<p>To control possible passage effects, we applied three classical equating methods: mean equating, linear equating, and equipercentile equating (<xref ref-type="bibr" rid="bibr2-1534508412447010">Albano &amp; Rodriguez, 2011</xref>; <xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen &amp; Brennan, 2004</xref>; see also <xref ref-type="bibr" rid="bibr46-1534508412447010">Stoolmiller et al., 2012</xref>, for a detailed description of these equating methods). We selected the first passage from the winter benchmark assessment (MOY1) within each grade as the reference passage.</p>
<p>Last, we evaluated the relative adequacy of our equating procedures using two methods. In the first, we compared the standard errors of equating (SEE) among alternative equating methods. SEE represents the accuracy of equating results as an index of random equating error, and it is highly influenced by sample size. Because we had a relatively small sample, we interpreted the results from SEE with reservation. For our second evaluation method, we utilized likelihood ratio tests of log-linear models. This analysis provided the basis for selecting an equating function that best represents the “true” relationship between passages (<xref ref-type="bibr" rid="bibr36-1534508412447010">Moses, 2009</xref>.</p>
</sec>
</sec>
</sec>
<sec id="section17-1534508412447010" sec-type="results">
<title>Results</title>
<p>We organize our results by preliminary data analysis and primary research questions. Within each research question, we focus on analyses for a single grade (i.e., Grade 2) and then highlight any differences between findings for all other grades (i.e., 1 and 3–6).</p>
<sec id="section18-1534508412447010">
<title>Preliminary Data Analysis</title>
<sec id="section19-1534508412447010">
<title>Missing data</title>
<p>Because our study involved multiple assessments and test forms, some missing data were unavoidable primarily due to student absence or mobility. Overall, the missing data rate in our study is small—more than 95% of students completed all required assessments (i.e., one reference passage and four progress-monitoring passages). Only 4.9% of all students missed one or more of these assessments, and the majority of missing cases (67.3%) came from students who were present at benchmark testing but unavailable for follow-up assessment. We used the available benchmark data to see if students missing follow-up data were different from students with complete data. We found no statistically significant differences in benchmark DORF performance between the two groups. Based on this analysis; the small, overall missing data rate; and the recommendations of <xref ref-type="bibr" rid="bibr1-1534508412447010">Aday and Cornelius (2006</xref>; that is, that it is not necessary to impute data for missing values unless 10% or more values are missing), we assume that the effect of missing data on our analyses is minimal and we do not impute missing data.</p>
</sec>
<sec id="section20-1534508412447010">
<title>Preliminary analysis of DORF scores by passage</title>
<p>Even before conducting formal statistical analyses, we observed varying performance scores on different DORF passages in all grades. In the last column in <xref ref-type="table" rid="table2-1534508412447010">Table 2</xref>, we present the average number of words read correctly for the first DORF passage from the winter benchmark triad. Across all six grades, these passages served as the reference forms. Also in <xref ref-type="table" rid="table2-1534508412447010">Table 2</xref>, we display descriptive statistics for the progress-monitoring passages with the smallest and largest average number of words read correctly (i.e., the most and least difficult passages as read by students in our sample) as well as an index of common readability (i.e., Flesch-Kincaid). When examining <xref ref-type="table" rid="table2-1534508412447010">Table 2</xref>, three features are clear: (a) readability levels as assessed by text readability formula do not correspond well to the passages’ observed difficulty, (b) DORF benchmark passages are representative of overall readability level within a grade, and (c) variations in average DORF scores among alternate progress-monitoring forms are substantial.</p>
<table-wrap id="table2-1534508412447010" position="float">
<label>Table 2.</label>
<caption>
<p>Descriptive Statistics for Progress-Monitoring Passages and Reference Passage by Grade</p>
</caption>
<graphic alternate-form-of="table2-1534508412447010" xlink:href="10.1177_1534508412447010-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Progress-Monitoring Passage<hr/></th>
<th/>
</tr>
<tr>
<th align="left">Grade/Statistic</th>
<th align="center">Passage With Minimum Mean</th>
<th align="center">Passage With Maximum Mean</th>
<th align="center">Reference Passage (MOY1)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>PM05—Picking apples</td>
<td>PM04—A pancake breakfast</td>
<td/>
</tr>
<tr>
<td> <italic>M (SD)</italic></td>
<td>32.72 (28.31)</td>
<td>52.54 (37.45)</td>
<td>41.00 (30.91)</td>
</tr>
<tr>
<td> Skewness</td>
<td>1.58</td>
<td>0.65</td>
<td>1.12</td>
</tr>
<tr>
<td> Kurtosis</td>
<td>3.36</td>
<td>−0.65</td>
<td>0.64</td>
</tr>
<tr>
<td> Read</td>
<td>2.2</td>
<td>1.8</td>
<td>0.6</td>
</tr>
<tr>
<td>2</td>
<td>PM05—A day for trees</td>
<td>PM18—Canoe fun</td>
<td/>
</tr>
<tr>
<td> <italic>M (SD)</italic></td>
<td>68.37 (36.54)</td>
<td>89.37 (40.00)</td>
<td>82.35 (34.23)</td>
</tr>
<tr>
<td> Skewness</td>
<td>0.79</td>
<td>−0.04</td>
<td>−0.01</td>
</tr>
<tr>
<td> Kurtosis</td>
<td>1.27</td>
<td>−0.82</td>
<td>−0.43</td>
</tr>
<tr>
<td> Read</td>
<td>2.7</td>
<td>2.9</td>
<td>3.4</td>
</tr>
<tr>
<td>3</td>
<td>PM08–How Ryan made a difference</td>
<td>PM15—Amazing dolphins</td>
<td/>
</tr>
<tr>
<td> <italic>M</italic> (<italic>SD</italic>)</td>
<td>84.47 (39.53)</td>
<td>109.47 (40.66)</td>
<td>97.19 (40.24)</td>
</tr>
<tr>
<td> Skewness</td>
<td>0.84</td>
<td>0.21</td>
<td>0.18</td>
</tr>
<tr>
<td> Kurtosis</td>
<td>0.60</td>
<td>−0.17</td>
<td>−0.44</td>
</tr>
<tr>
<td> Read</td>
<td>4.5</td>
<td>4.5</td>
<td>3.9</td>
</tr>
<tr>
<td>4</td>
<td>PM14—Wonderful water</td>
<td>PM06—Empty lot to a garden spot</td>
<td/>
</tr>
<tr>
<td> <italic>M</italic> (<italic>SD</italic>)</td>
<td>98.47 (31.35)</td>
<td>119.89 (31.37)</td>
<td>110.60 (36.94)</td>
</tr>
<tr>
<td> Skewness</td>
<td>0.30</td>
<td>0.33</td>
<td>−0.11</td>
</tr>
<tr>
<td> Kurtosis</td>
<td>0.88</td>
<td>0.80</td>
<td>−0.12</td>
</tr>
<tr>
<td> Read</td>
<td>6.4</td>
<td>5.1</td>
<td>5.6</td>
</tr>
<tr>
<td>5</td>
<td>PM05–How water moves through plants</td>
<td>PM03—Exploring Australia</td>
<td/>
</tr>
<tr>
<td> <italic>M</italic> (<italic>SD</italic>)</td>
<td>112.84 (36.72)</td>
<td>139.64 (39.74)</td>
<td>128.07 (38.81)</td>
</tr>
<tr>
<td> Skewness</td>
<td>−0.17</td>
<td>0.13</td>
<td>0.20</td>
</tr>
<tr>
<td> Kurtosis</td>
<td>−0.24</td>
<td>0.51</td>
<td>0.36</td>
</tr>
<tr>
<td> Read</td>
<td>6.3</td>
<td>6.8</td>
<td>6.7</td>
</tr>
<tr>
<td>6</td>
<td>PM08—Ice country</td>
<td>PM03—Making a comic book</td>
<td/>
</tr>
<tr>
<td> <italic>M</italic> (<italic>SD</italic>)</td>
<td>119.94 (37.48)</td>
<td>139.41 (40.92)</td>
<td>130.62 (43.25)</td>
</tr>
<tr>
<td> Skewness</td>
<td>−0.08</td>
<td>0.17</td>
<td>0.17</td>
</tr>
<tr>
<td> Kurtosis</td>
<td>−0.04</td>
<td>1.13</td>
<td>0.41</td>
</tr>
<tr>
<td> Read.</td>
<td>7.6</td>
<td>7.1</td>
<td>7.0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1534508412447010">
<p><italic>Note</italic>. MOY1 = the first benchmark passage in the winter triad, used as the reference passage in our study; PMxx = the progress-monitoring (PM) passage number, 20 PM forms/grade. Readability estimates by Flesch-Kincaid (F-K) formula, comprised of average sentence length and average number of syllables/word. F-K values are interpreted as grade-level text difficulty.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>For example, the smallest difference in average scores between the “easiest” and “most difficult” passages is 19.47 (Grade 6); the largest difference is 26.80 (Grade 5). When expressed in terms of standardized effect sizes (i.e., Hedge’s <italic>g</italic>), these score differences correspond to a magnitude range of .50 to .70, which are considered moderate to large effects. Practically speaking, with this level of passage difficulty ranges, the percentile rank of a Grade 5 student can be expected to change by as much as 26 units depending on the passage that is administered.</p>
</sec>
</sec>
<sec id="section21-1534508412447010">
<title>Question 1: Are DIBELS Next Progress-Monitoring Passages Equivalent Within a Grade?</title>
<p>To examine DORF passage effects, we conducted a series of analyses using mixed-model ANOVAs with repeated measures. We included passages, presentation orders, and their interaction term as fixed effects and adjusted DORF scores were set to randomly vary across students. We first explored different measurement-level error structures to determine the best variance–covariance structure for our analytic model. By comparing the model fits with different error structures using the Bayesian Information Criterion, we selected a simple scaled identity structure that assumed constant variance across measurements with no correlation. In the following sections, we present the results of this analytic model with a focus on Grade 2 DORF scores.</p>
<p>We first fit the full model with all three fixed effects, but the interaction effect was not statistically significant (<italic>p</italic> = .683). In our final model, with the interaction term dropped, the effect of passage is statistically significant, <italic>F</italic>(19, 1228) = 27.50, <italic>p</italic> &lt; .001, suggesting that students’ DORF scores can significantly vary depending on the passage they are asked to read. The passage effect explained 23.1% of the variance among repeated measurements within students in Grade 2 and was significant across all six grades.</p>
<p>The effect of order is also statistically significant in Grade 2, <italic>F</italic>(3, 1720) = 3.03, <italic>p</italic> = .028. Post hoc pairwise comparisons using Šidák’s adjusted <italic>p</italic> values reveal differences in estimated average DORF scores between the first (78.53) and last (i.e., fourth; 79.93) passages read with a mean difference of 1.40 (<italic>p</italic> = .036). Although DORF scores were slightly higher for later orders, score variation by order was negligible compared with variation by passage. The order effect explained less than 1% of the variance among repeated measurements within students. Similar results were found in all grades except Grade 1, where the effect of presentation order was not statistically significant.</p>
</sec>
<sec id="section22-1534508412447010">
<title>Question 2: What Is the Impact of Various Statistical Equating Procedures?</title>
<p>We have demonstrated that significant form effects exist in the assessment of ORF using the DORF progress-monitoring passages. These findings are similar to those from past studies with an earlier edition of DIBELS (i.e., DIBELS sixth edition) and with other types of R-CBM measures (<xref ref-type="bibr" rid="bibr2-1534508412447010">Albano &amp; Rodriguez, 2011</xref>; <xref ref-type="bibr" rid="bibr6-1534508412447010">Betts et al., 2009</xref>; <xref ref-type="bibr" rid="bibr10-1534508412447010">Christ &amp; Ardoin, 2009</xref>; <xref ref-type="bibr" rid="bibr18-1534508412447010">Francis et al., 2008</xref>). We found form effects in all six grades that are assessed in DIBELS Next. For our second research question, we explore three classical equating methods to deal with form effects persistent in the DIBELS Next DORF assessment. We examine how these equating methods can help remove inconsistencies across passages.</p>
<sec id="section23-1534508412447010">
<title>Descriptive statistics of equated scores</title>
<p>We present descriptive statistics from the equating results with Grade 2 DORF passages in <xref ref-type="table" rid="table3-1534508412447010">Table 3</xref>. At the top of the table, we display descriptive statistics for the reference passage. Below the first row, we list the descriptive statistics for 20 progress-monitoring passages following the implementation of these equating methods: identity (i.e., no equating), mean equating, linear equating, and equipercentile equating, with no smoothing (<italic>S</italic> = 0), a small degree of smoothing (<italic>S</italic> = 0.3), and a large degree of smoothing (<italic>S</italic> = 1.0). Because we equated a large number of passages, we present ranges for all statistics in the table.</p>
<table-wrap id="table3-1534508412447010" position="float">
<label>Table 3.</label>
<caption>
<p>Descriptive Statistics for 20 Progress-Monitoring Passages With Different Equating Methods (Grade 2)</p>
</caption>
<graphic alternate-form-of="table3-1534508412447010" xlink:href="10.1177_1534508412447010-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center">Skewness</th>
<th align="center">Kurtosis</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reference form (MOY1)</td>
<td>82.35</td>
<td>34.23</td>
<td>−0.01</td>
<td>−0.43</td>
</tr>
<tr>
<td colspan="5">Progress monitoring (PM01~PM20)</td>
</tr>
<tr>
<td> No equated scores</td>
<td>68.37–89.37</td>
<td>29.70–40.00</td>
<td>−0.24–0.79</td>
<td>−1.00–1.27</td>
</tr>
<tr>
<td> Mean-equated scores</td>
<td>81.97–82.77</td>
<td>29.70–40.00</td>
<td>−0.24–0.79</td>
<td>−1.00–1.27</td>
</tr>
<tr>
<td> Linear-equated scores</td>
<td>82.04–82.39</td>
<td>34.31–34.45</td>
<td>−0.23–0.79</td>
<td>−1.00–1.27</td>
</tr>
<tr>
<td> Equipercentile equated (no smoothing)</td>
<td>82.15–82.40</td>
<td>34.04–34.18</td>
<td>−0.09–−0.05</td>
<td>−0.61–−0.55</td>
</tr>
<tr>
<td> Equipercentile equated (<italic>S</italic> = 0.3)</td>
<td>82.07–82.36</td>
<td>33.94–34.32</td>
<td>−0.07–−0.04</td>
<td>−0.61–−0.55</td>
</tr>
<tr>
<td> Equipercentile equated (<italic>S</italic> = 1.0)</td>
<td>81.76–82.53</td>
<td>33.60–35.04</td>
<td>−0.14–0.04</td>
<td>−0.76–−0.45</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-1534508412447010">
<p><italic>Note</italic>. MOY1 = the first benchmark passage in the winter triad, used as the reference passage in our study; PMxx = the progress–monitoring (PM) passage number, 20 PM forms/grade. Minor discrepancies between equated score means are because of rounding error, as dynamic indicators of basic early literacy skills oral reading fluency scores are expressed as nonnegative integers.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>For all Grade 2 progress-monitoring passages, we see wide ranges in the mean, standard deviation, skewness, and kurtosis of DORF raw scores. These values are also markedly different from the raw scores for the reference passage. The mean scores for progress-monitoring passages range from 68.37 to 89.37 (reference passage <italic>M</italic> = 82.35). The results of all three equating methods change these patterns and lead to near-identical means across all 20 passages (median difference between min/max-equated passage means = 0.35); these means also resemble that of the reference passage by design.</p>
<p>Examining the other statistics in <xref ref-type="table" rid="table3-1534508412447010">Table 3</xref> demonstrates the way in which different equating methods control for other types of passage variability. For example, mean equating only controls average differences in DORF scores; linear equating and equipercentile equating address differences in means and differences in variances. As a result, the range of standard deviations for mean-equated scores is the same as the range of standard deviations for DORF raw scores. Linear-equated and equipercentile-equated scores demonstrate a narrow range of standard deviations that are close to the value of the standard deviation for the reference passage.</p>
<p>Equipercentile equating is the only method that not only addresses variance in means and standard deviations but also controls for differences in the overall distribution of raw scores across passages. In <xref ref-type="fig" rid="fig1-1534508412447010">Figure 1</xref>, we display the impact of different equating methods for a single Grade 2 progress-monitoring passage (PM05—“A day for trees”). We selected this passage as an example because the descriptive statistics for this passage are most markedly different from the Grade 2 reference passage. The mean of PM05 is the lowest, and its skewness and kurtosis are the highest among the Grade 2 progress-monitoring passages. As noted in <xref ref-type="fig" rid="fig1-1534508412447010">Figure 1</xref>, the graphs for raw scores on the progress-monitoring passage (panel b) and the scores on the reference passage (panel a), display very different patterns. When we scaled scores through mean equating, the histogram moved to the right (panel c), to have the same mean as the reference passage, still other characteristics of the distribution did not change. Graphs e through g display the impact of equipercentile equating with different degrees of smoothing. The equipercentile equating method produced score distributions that are closest to the reference passage.</p>
<fig id="fig1-1534508412447010" position="float">
<label>Figure 1.</label>
<caption>
<p>Distribution of Dynamic Indicators of Basic Early Literacy Skills Oral Reading Fluency (DORF) scores on a reference form (the first passage from the winter benchmark assessment [MOY1]) and a progress-monitoring form (PM05); scaled using different equating methods.</p>
</caption>
<graphic xlink:href="10.1177_1534508412447010-fig1.tif"/>
</fig>
</sec>
<sec id="section24-1534508412447010">
<title>Examination of form effects after equating</title>
<p>We investigated whether and how different equating methods addressed DORF-form effects, again using a mixed-model repeated-measures ANOVA. The analytic model was similar to the one described in detail in the “Analytic Approach” section of the “Results” but with the addition of one factor: students’ risk level (i.e., benchmark status) in reading and its interaction with passage and order effects. We added these terms to the model to explore whether equated scores work differently depending on the location of those scores. We defined reading risk based on students’ middle-of-year benchmark DORF scores and the DIBELS Next benchmark goals (<xref ref-type="bibr" rid="bibr24-1534508412447010">Good &amp; Kaminski, 2011b</xref>).</p>
<p>In raw scores with no equating, we found significant interactions between risk status and passage, <italic>F</italic>(38, 1237) = 3.69, <italic>p</italic> &lt; .001, as well as between risk status and presentation order, <italic>F</italic>(6, 1699) = 2.16, <italic>p</italic> = .044. DORF scores are influenced by the passage administered and the order in which the passage was presented, and these effects are moderated by student risk status. For example, some passages were substantially easier or more difficult to read than other passages for at-risk readers, still those same passages were very similar in difficulty for students with low risk. We also found an interaction between order effect and risk status. Students at low risk were more likely to experience a practice effect (i.e., in which passages read later were read more quickly than passages that were read first) than students who were at moderate or high risk.</p>
<p>When we fit the same models with scaled scores from the different equating methods, all main effects for passage dropped out of the model with one exception. In the model with mean-equated scores, we still found a significant interaction between risk status and passage, <italic>F</italic>(38, 1237) = 3.69, <italic>p</italic> &lt; .001, implying that the mean equating method could not control for differential relations between passages and different levels of scores in our sample. This finding is consistent with a well-known drawback of mean equating (<xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen &amp; Brennan, 2004</xref>). Our results suggest that form effects can be successfully controlled through linear equating and equipercentile equating in the estimation of ORF scores with DORF passages.</p>
</sec>
<sec id="section25-1534508412447010">
<title>Summary of form and presentation effects across grades</title>
<p>In the previous sections, we have focused on Grade 2 data as an example of our exploration of different equating methods as a solution for DORF-form effects. In <xref ref-type="table" rid="table4-1534508412447010">Table 4</xref>, we summarize the results of our analyses in all other grades for which the DORF passages are available.</p>
<table-wrap id="table4-1534508412447010" position="float">
<label>Table 4.</label>
<caption>
<p>Significance of Fixed Effects Across Different Equating Methods (Grades 1–6)</p>
</caption>
<graphic alternate-form-of="table4-1534508412447010" xlink:href="10.1177_1534508412447010-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Fixed Effects</th>
<th align="center">Scale</th>
<th align="center">Grade 1</th>
<th align="center">Grade 2</th>
<th align="center">Grade 3</th>
<th align="center">Grade 4</th>
<th align="center">Grade 5</th>
<th align="center">Grade 6</th>
</tr>
</thead>
<tbody>
<tr>
<td>Passage effect (from main effect only models)</td>
<td>Raw score</td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
</tr>
<tr>
<td/>
<td>Mean E.</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>Linear E.</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0.3)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 1.0)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td>Passage by risk interaction</td>
<td>Raw score</td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">***</xref></td>
</tr>
<tr>
<td/>
<td>Mean E.</td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">***</xref></td>
</tr>
<tr>
<td/>
<td>Linear E.</td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0.3)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 1.0)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td>Order effect (from main effect only models)</td>
<td>Raw score</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
</tr>
<tr>
<td/>
<td>Mean E.</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
</tr>
<tr>
<td/>
<td>Linear E.</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0.3)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 1.0)</td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">****</xref></td>
</tr>
<tr>
<td>Order by risk interaction</td>
<td>Raw scores</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>Mean E.</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>Linear E.</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0)</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">***</xref></td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 0.3)</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td/>
<td>E. E. (<italic>S</italic> = 1.0)</td>
<td><italic>ns</italic></td>
<td><xref ref-type="table-fn" rid="table-fn5-1534508412447010">**</xref></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
<td><italic>ns</italic></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-1534508412447010">
<p><italic>Note</italic>. Mean E. = mean-equated scores; Linear E. = linear-equated scores; E. E. = equipercentile-equated scores.</p>
</fn>
<fn id="table-fn5-1534508412447010">
<label>**</label>
<p><italic>p</italic> &lt; .05. *** <italic>p</italic> &lt; .01. ****<italic>p</italic> &lt; .001. <italic>ns</italic> = nonsignificant (<italic>p</italic> &gt; .05).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>We display the impact of passage effects in the top half of <xref ref-type="table" rid="table4-1534508412447010">Table 4</xref>. Raw scores with no equating resulted in significant passage effects across all grades (<italic>p</italic> &lt; .001), whereas equated scores based on all types of equating methods demonstrated no significant passage effects (<italic>p</italic> &gt; .05), also across all grades. Mean equating was insufficient to remove the interaction effect between passage and reading risk status (<italic>p</italic> &lt; .001). This interaction effect persisted when we used linear-equated scores in the model (<italic>p</italic> &lt; .05), but only for Grade 1. When we scaled scores using the equipercentile methods, with different degrees of smoothing, there were no significant passage effects, main or interaction, in any grades (<italic>p</italic> &gt; .05).</p>
<p>We present fixed effects related to presentation order in the bottom half of <xref ref-type="table" rid="table4-1534508412447010">Table 4</xref>. As expected, the equating methods applied in our study did not control for the effect of order. In grades with significant order effects, DORF scores were typically lower for passages that were administered earlier in the sequence. However, the estimated score differences between different orders were not substantial, with less than a 3.5-point difference between scores across all grades.</p>
</sec>
</sec>
<sec id="section26-1534508412447010">
<title>Question 3: What Equating Method Works the Best in Our Sample?</title>
<p>The evaluation of different equating methods involves examination of accuracy and efficiency. In this section, we compare the equating methods that we applied in our study, using two different approaches: comparisons of SEEs for relative accuracy and likelihood ratio tests of log-linear models for relative efficiency.</p>
<sec id="section27-1534508412447010">
<title>Accuracy of alternative equating methods for DORF passages</title>
<p>Equating accuracy is usually evaluated using the SEE, a measure of the amount of random equating error due to sampling (<xref ref-type="bibr" rid="bibr46-1534508412447010">Stoolmiller et al., 2012</xref>). One can estimate SEE by calculating the standard deviation of equated scores over replications of an equating procedure (<xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen &amp; Brennan, 2004</xref>). In our study, we computed the SEE for all equating methods applied to each passage through bootstrap replications. We present a display of the estimated SEE across different equating methods in <xref ref-type="fig" rid="fig2-1534508412447010">Figure 2</xref> for a Grade 2 progress-monitoring passage (PM05).</p>
<fig id="fig2-1534508412447010" position="float">
<label>Figure 2.</label>
<caption>
<p>Standard errors of equating (SEEs) from different equating methods for a Grade 2 progress-monitoring passage (PM05).</p>
</caption>
<graphic xlink:href="10.1177_1534508412447010-fig2.tif"/></fig>
<p>SEE is expressed across score ranges on each probe form and varies significantly as a function of the sample size at each score point. For example, the SEE associated with mean equating is relatively small and consistent across all scores (0–210) along the horizontal axis. For all other equating methods, the SEE becomes rather large at the high and the low ends of the score distributions—where fewer cases are observed. Linear equating produced relatively small SEEs in the score range between 30 and 80. Equipercentile equating had acceptable levels of SEE only in a very narrow score range between 80 and 110 (where the highest frequency of cases was observed). When comparing equipercentile equating procedures with different smoothing values, the SEE was generally lower with more smoothing. However, these differences were not substantial—particularly, in the score ranges that are considered typical for Grade 2 progress monitoring with DORF. This pattern in the SEE distribution across equating methods was quite similar for different passages and for different grades.</p>
</sec>
<sec id="section28-1534508412447010">
<title>Efficiency of alternative equating methods for DORF passages</title>
<p>Another way to evaluate equating methods is to compare the <italic>efficiency</italic> of alternative equating methods with regard to their representation of the relation of scores between two different passages. According to <xref ref-type="bibr" rid="bibr36-1534508412447010">Moses (2009)</xref>, the likelihood ratio test for comparing log-linear models is preferred for selecting the most parsimonious equating function. In this approach, we construct log-linear models with polynomial functions of the test scores to differentiate moments of the observed distributions between two passages. These models are repeated for specific equating methods, and then we can compare model fits based on their likelihood ratio chi-square statistics. More detailed modeling and testing procedures can be found in Moses.</p>
<p>We focus on a single Grade 2 progress-monitoring passage (PM05) in the summary of our results evaluating equating efficiency. Passage PM05 represents a case in which passage effects are substantial (see <xref ref-type="table" rid="table2-1534508412447010">Table 2</xref> and <xref ref-type="fig" rid="fig1-1534508412447010">Figure 1</xref>). In <xref ref-type="table" rid="table5-1534508412447010">Table 5</xref>, we display multiple comparisons between alternative equating methods for PM05. We present likelihood ratio chi-square statistics for each method along the diagonal, statistics for model comparisons with exact <italic>p</italic> values below the diagonal, and signs of statistical significance for each comparison above the diagonal. For PM05, all equating methods demonstrated statistically better fits than no equating. Compared with mean equating, linear equating had a significantly better fit (<italic>p</italic> = .003) whereas equipercentile equating was only marginally better (<italic>p</italic> = .088). Because equipercentile equating, the most complex equating procedure in our study, did not have a better fit than the simpler linear equating (<italic>p</italic> = .966), we conclude that linear equating is the most efficient equating option in this sample. Our conclusion is the same when we correct <italic>p</italic> values for multiple comparisons.</p>
<table-wrap id="table5-1534508412447010" position="float">
<label>Table 5.</label>
<caption>
<p>Results From the Likelihood Ratio Tests to Evaluate Alternative Equating Methods for a Progress-Monitoring Passage (PM05; Grade 2)</p>
</caption>
<graphic alternate-form-of="table5-1534508412447010" xlink:href="10.1177_1534508412447010-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Identity Equating</th>
<th align="center">Mean Equating</th>
<th align="center">Linear Equating</th>
<th align="center">Equipercentile Equating (<italic>S</italic> = 0)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Identity</td>
<td>χ<sup>2</sup> = 193.14 (<italic>df</italic> = 219)</td>
<td><xref ref-type="table-fn" rid="table-fn7-1534508412447010">**</xref></td>
<td><xref ref-type="table-fn" rid="table-fn7-1534508412447010">****</xref></td>
<td><xref ref-type="table-fn" rid="table-fn7-1534508412447010">**</xref></td>
</tr>
<tr>
<td><italic>M</italic></td>
<td>Difference χ<sup>2</sup> = 5.87 (<italic>df</italic> = 1), <italic>p</italic> = .015</td>
<td>χ<sup>2</sup> = 187.27 (<italic>df</italic> = 218)</td>
<td><xref ref-type="table-fn" rid="table-fn7-1534508412447010">***</xref></td>
<td><xref ref-type="table-fn" rid="table-fn7-1534508412447010">*</xref></td>
</tr>
<tr>
<td>Linear</td>
<td>Difference χ<sup>2</sup> = 14.88 (<italic>df</italic> = 2), <italic>p</italic> &lt; .001</td>
<td>Difference χ<sup>2</sup> = 9.01 (<italic>df</italic> = 1), <italic>p</italic> = .003</td>
<td>χ<sup>2</sup> = 178.26 (<italic>df</italic> = 217)</td>
<td><italic>ns</italic></td>
</tr>
<tr>
<td>Equipercentile</td>
<td>Difference χ<sup>2</sup> = 15.45 (<italic>df</italic> = 6), <italic>p</italic> = .017</td>
<td>Difference χ<sup>2</sup> = 9.58 (<italic>df</italic> = 5), <italic>p</italic> = .088</td>
<td>Difference χ<sup>2</sup> = 0.57 (<italic>df</italic> = 4), <italic>p</italic> = .966</td>
<td>χ<sup>2</sup> = 177.69 (<italic>df</italic> = 217)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-1534508412447010">
<p><italic>Note</italic>. The model fit for each equating method is presented on the diagonal in a likelihood ratio chi-square statistics. Below the diagonal are comparisons of the fits between different equating models. Statistical differences between the models are presented below the diagonal.</p>
</fn>
<fn id="table-fn7-1534508412447010">
<label>*</label>
<p><italic>p</italic> &lt; .10. **<italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01. ****<italic>p</italic> &lt; .001. <italic>ns</italic> = nonsignificant (<italic>p</italic> &gt; .10).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The relative adequacy among alternative equating methods is form specific and, thus, may vary by DORF passage and across grades. In Grade 2, we found that linear equating was the most efficient equating method for 13 out of 20 progress-monitoring passages, mean equating was preferred for only one passage, and no equating was necessary for the remaining six passages. In Grade 1, mean equating offered the best fit for 13 passages, equipercentile for six passages, and linear for one passage. With the exception of Grade 1, equipercentile equating was not better than linear equating for any passages. In Grades 3 through 6, linear equating was the preferred method for approximately 25% of all passages. For all other passages in those grades, no statistical equating procedures demonstrated a better fit than the identity model.</p>
</sec>
</sec>
</sec>
<sec id="section29-1534508412447010" sec-type="discussion">
<title>Discussion</title>
<p>The purpose of our study was to determine the impact of DORF form effects on student reading outcomes in the DIBELS Next materials. We examined the effectiveness of three classical equating procedures in ameliorating these form effects, and then selected the most parsimonious scaling approach.</p>
<sec id="section30-1534508412447010">
<title>Key Findings</title>
<p>DORF passages display significant form effects, such that student reading performance is meaningfully impacted depending on the passage they are asked to read. We observed the smallest form effects in Grade 6, where the differences between passages amounted to an effect size of .50. This difference in DORF passage difficulty could result in a score that was approximately 19 percentile ranks higher for a Grade 6 student who is asked to read PM03 compared with the score that would result if that same student was assigned PM08.</p>
<p>We found that statistical equating procedures resulted in scaled scores that could be linked to a representative screening passage and displayed no significant variability across the progress-monitoring passage set. All three equating methods (i.e., mean, linear, and equipercentile) appeared to perform better than no equating in terms of eliminating the main effect of passage difficulty, but linear and equipercentile equating were the only approaches that could reduce the effect of passage across the entire range of scores. With the exception of Grade 1 (where passage effects persisted for students at certain skill levels), linear equating was successful in producing scaled passage scores that were truly interchangeable across a range of student skill levels (see <xref ref-type="table" rid="table4-1534508412447010">Table 4</xref>, “Passage by Risk Interaction”).</p>
<p>Equating methods cannot remove the impact of presentation order, and we did observe small-order effects in our models. Furthermore, investigation revealed that the order effect was significant only for students whose reading performance was designated as low risk and only for students in Grades 2 through 6. Similar to other researchers (e.g., <xref ref-type="bibr" rid="bibr40-1534508412447010">Petscher &amp; Kim, 2011</xref>), our findings are indicative of a practice effect. Unlike prior research results, however, we found the practice effect to be more pronounced for <italic>higher achieving students</italic>. This finding warrants further investigation in more tightly controlled settings to evaluate the mechanisms by which practice effects exert themselves during assessment contexts.</p>
<p>As <xref ref-type="bibr" rid="bibr46-1534508412447010">Stoolmiller et al. (2012)</xref> note, equating as a statistical procedure contributes random error variability to test scores—and a critical role for researchers is to quantify the extent to which equated scores reduce more variability than they create. We assessed the accuracy and efficiency of our equating result using the SEE and an evaluation of goodness of model fit. Results indicate that linear equating methods produced the best results in our sample. Linear equating demonstrated a satisfactory SEE for the score ranges that are typically associated with a need for DORF progress monitoring in Grade 2 (winter scores = 30–80). Linear equating reduced the impact of passage variability in all grades, and across all score levels with one exception (Grade 1). In addition, linear equating produced the most efficient result for 28% of the 108 passages evaluated in this study.</p>
</sec>
<sec id="section31-1534508412447010">
<title>Limitations</title>
<p>The magnitude of equating errors is heavily dependent on sample size. According to <xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen and Brennan’s (2004)</xref> criteria, the minimum required sample sizes to achieve standard errors that are less than 0.1 raw score standard deviation units along most of the score range with a random group design are approximately 400 students per passage for linear equating and 1,500 students per passage for equipercentile equating. For passage PM05 in Grade 2, our sample size was only 144 students per passage, which is much smaller than what is required for linear or equipercentile equating. Therefore, interpretations about relative accuracy are limited by the small sample in our study. With larger sample sizes, the patterns in the SEE across alternative equating methods might be different from the results of our present study. Given the known advantages of using equipercentile equating (e.g., the ability to control for differences in overall score distributions; <xref ref-type="bibr" rid="bibr32-1534508412447010">Kolen &amp; Brennan, 2004</xref>), future studies may find more accurate equating results with more students.</p>
<p>We found remarkable consistency in our results across grades, with some exceptions for Grade 1. Our hypothesis is that, because of persistent floor effects on R-CBM measures in the winter of Grade 1 (e.g., <xref ref-type="bibr" rid="bibr8-1534508412447010">Catts, Petscher, Schatschneider, Bridges, &amp; Mendoza, 2008</xref>), it may be challenging to obtain the range of student reading performance necessary for equating at that time. Future research should consider obtaining reading samples for equating with early readers later in the year, when performance is more likely to approximate a normal distribution.</p>
<p>As a final note, we acknowledge that these data were collected across a large number of schools (<italic>n</italic> = 28), with widely variable instructional practices. Early reading instruction is a prime determinant of student reading performance and certainly impacted our estimates of form difficulty in this project. Future equating research would do well to analyze cluster variability and its impact on observed passage difficulty to provide a more nuanced lens within which to evaluate form effects.</p>
</sec>
<sec id="section32-1534508412447010">
<title>Practical Implications</title>
<p>We argue that passage-to-passage variability makes it difficult for schools to make good decisions about their students’ progress. Based on the results of this study, in combination with the results from other researchers’ prior work (e.g., <xref ref-type="bibr" rid="bibr2-1534508412447010">Albano &amp; Rodriguez, 2011</xref>; <xref ref-type="bibr" rid="bibr6-1534508412447010">Betts et al., 2009</xref>; <xref ref-type="bibr" rid="bibr10-1534508412447010">Christ &amp; Ardoin, 2009</xref>; <xref ref-type="bibr" rid="bibr18-1534508412447010">Francis et al., 2008</xref>), statistical equating appears to be a viable option as a way to substantially reduce measurement error while maintaining ease of use in data interpretation.</p>
<p>Current DORF administration rules state that passage variability for progress-monitoring materials has been controlled in two ways: (a) passage placement (i.e., order) and (b) interpretive guidelines (i.e., educators are instructed to “meet and make a considered decision about maintaining or modifying the [student’s] instruction” if “<italic>three consecutive data points</italic> fall below the aimline”; <xref ref-type="bibr" rid="bibr23-1534508412447010">Good &amp; Kaminski, 2011a</xref>, p. 36). Given the results of our study, we have two concerns with the current recommendations for progress-monitoring practice.</p>
<p>Our results suggest that passage placement alone is not sufficient to rule out substantive differences in difficulty levels. In three out of six grades, the passages with the minimum and maximum mean performance levels were passages that would be administered consecutively using current DIBELS Next recommendations. For example, in Grade 5, the most difficult passage was PM05. The easiest passage was PM03. As noted in the “Preliminary Analysis” section of the “Results,” the magnitude of the average difference between these two passages is such that a range of 26 percentile points would be expressed on a graph of student progress. With this level of passage variability, school teams will struggle with identifying the extent to which student gains or losses in reading performance are due to true changes in reading skill, behavioral problems (e.g., lack of student motivation), or passage difficulty. Schools also may be faced with additional assessment requirements because they must administer multiple progress-monitoring passages to reach a stable estimate of student performance.</p>
<p>We therefore recommend an approach to test development in which passages are written to a targeted range, outlier passages are removed during field or pilot testing, and schools are provided with equating tables to use when interpreting scores.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>The authors would like to thank Dr. Gina Biancarosa, who provided initial consultation to their team on the design of this study.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The research reported here was supported exclusively by the Center on Teaching and Learning (CTL), a research and outreach unit in the College of Education at the University of Oregon, its Director, Edward J. Kame’enui, Dean-Knight Professor, and Associate Director, Scott Baker. The data, analyses, results, and opinions expressed are those of the authors and CTL, and do not necessarily represent views of the College of Education or the University of Oregon.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Aday</surname><given-names>L. A.</given-names></name>
<name><surname>Cornelius</surname><given-names>L. J.</given-names></name>
</person-group> (<year>2006</year>). <source>Designing and conducting health surveys: A comprehensive guide</source> (<edition>3rd ed.</edition>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr2-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Albano</surname><given-names>A. D.</given-names></name>
<name><surname>Rodriguez</surname><given-names>M. C.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Statistical equating with measures of oral reading fluency</article-title>. <source>Journal of School Psychology</source>. doi:<pub-id pub-id-type="doi">10.1016/j.jsp.2011.07.002</pub-id></citation>
</ref>
<ref id="bibr3-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ardoin</surname><given-names>S. P.</given-names></name>
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>CBM of oral reading: Standard errors associated with progress monitoring outcomes from DIBELS, AIMSweb, and an experimental passage set</article-title>. <source>School Psychology Review</source>, <volume>38</volume>, <fpage>266</fpage>–<lpage>283</lpage>.</citation>
</ref>
<ref id="bibr4-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ardoin</surname><given-names>S. P.</given-names></name>
<name><surname>Suldo</surname><given-names>S. M.</given-names></name>
<name><surname>Witt</surname><given-names>J.</given-names></name>
<name><surname>Aldrich</surname><given-names>S.</given-names></name>
<name><surname>McDonald</surname><given-names>E.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Accuracy of readability estimates’ predictions of CBM</article-title>. <source>School Psychology Quarterly</source>, <volume>20</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr5-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Baker</surname><given-names>S. K.</given-names></name>
<name><surname>Smolkowski</surname><given-names>K.</given-names></name>
<name><surname>Katz</surname><given-names>R.</given-names></name>
<name><surname>Fien</surname><given-names>H.</given-names></name>
<name><surname>Seeley</surname><given-names>J.</given-names></name>
<name><surname>Kame’enui</surname><given-names>E.</given-names></name>
<name><surname>. . . Carrie</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Reading fluency as a predictor of reading proficiency in low performing high poverty schools</article-title>. <source>School Psychology Review</source>, <volume>37</volume>, <fpage>18</fpage>–<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr6-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Betts</surname><given-names>J.</given-names></name>
<name><surname>Pickart</surname><given-names>M.</given-names></name>
<name><surname>Heistad</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>An investigation of the psychometric evidence of CBM-R passage equivalence: Utility of readability statistics and equating for alternate forms</article-title>. <source>Journal of School Psychology</source>, <volume>47</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>17</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jsp.2008.09.001</pub-id></citation>
</ref>
<ref id="bibr7-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Capizzi</surname><given-names>A. M.</given-names></name>
<name><surname>Barton-Arwood</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Using a CBM graphic organizer to facilitate collaboration in reading</article-title>. <source>Intervention in School and Clinic</source>, <volume>45</volume>, <fpage>14</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr8-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Catts</surname><given-names>H. W.</given-names></name>
<name><surname>Petscher</surname><given-names>Y.</given-names></name>
<name><surname>Schatschneider</surname><given-names>C.</given-names></name>
<name><surname>Bridges</surname><given-names>M.</given-names></name>
<name><surname>Mendoza</surname><given-names>K.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Floor effects in universal screening and their impact on the early identification of reading disabilities</article-title>. <source>Journal of Learning Disabilities</source>, <volume>42</volume>, <fpage>163</fpage>–<lpage>176</lpage>.</citation>
</ref>
<ref id="bibr9-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chaparro</surname><given-names>E. A.</given-names></name>
<name><surname>Park</surname><given-names>Y.</given-names></name>
<name><surname>Fien</surname><given-names>H.</given-names></name>
<name><surname>Baker</surname><given-names>S. K.</given-names></name>
<name><surname>Stoolmiller</surname><given-names>M.</given-names></name>
<name><surname>Smith</surname><given-names>J. M.</given-names></name>
<name><surname>Basaraba</surname><given-names>D.</given-names></name>
</person-group> (<year>2012</year>). <source>Evaluating equating methods for progress monitoring ORF passages in second grade</source>. Manuscript in preparation.</citation>
</ref>
<ref id="bibr10-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Ardoin</surname><given-names>S. P.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Curriculum-based measurement of oral reading: Passage equivalence and probe-set development</article-title>. <source>Journal of School Psychology</source>, <volume>47</volume>, <fpage>55</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr11-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Silberglitt</surname><given-names>B.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Curriculum-based measurement of oral reading fluency: The standard error of measurement</article-title>. <source>School Psychology Review</source>, <volume>36</volume>, <fpage>130</fpage>–<lpage>146</lpage>.</citation>
</ref>
<ref id="bibr12-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cummings</surname><given-names>K. D.</given-names></name>
<name><surname>Atkins</surname><given-names>T.</given-names></name>
<name><surname>Allison</surname><given-names>R.</given-names></name>
<name><surname>Cole</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Response to Intervention: Investigating the new role of special educators</article-title>. <source>Teaching Exceptional Children</source>, <volume>40</volume>(<issue>4</issue>), <fpage>24</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr13-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cummings</surname><given-names>K. D.</given-names></name>
<name><surname>Bousselot</surname><given-names>T.</given-names></name>
<name><surname>Smith</surname><given-names>J. M.</given-names></name>
<name><surname>Brown</surname><given-names>M.</given-names></name>
<name><surname>Kennedy</surname><given-names>P. C.</given-names></name>
<name><surname>Baker</surname><given-names>S. K.</given-names></name>
<name><surname>Kame’enui</surname><given-names>E. J.</given-names></name>
</person-group> (<year>2011</year>). <source>Sentinel schools in the DIBELS Data System (2010-2011): A description of methods</source> (Tech. Rep. No. 1103). <publisher-loc>Eugene</publisher-loc>: <publisher-name>University of Oregon</publisher-name>.</citation>
</ref>
<ref id="bibr14-1534508412447010">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Cummings</surname><given-names>K. D.</given-names></name>
<name><surname>Kennedy</surname><given-names>P. C.</given-names></name>
<name><surname>Otterstedt</surname><given-names>J.</given-names></name>
<name><surname>Baker</surname><given-names>S. K.</given-names></name>
<name><surname>Kame’enui</surname><given-names>E. J.</given-names></name>
</person-group> (<year>2011</year>). <source>DIBELS Data System: 2010-2011 percentile ranks</source> (Tech. Rep. No. 1101). <publisher-loc>Eugene</publisher-loc>: <publisher-name>University of Oregon</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://dibels.uoregon.edu/techreports/">https://dibels.uoregon.edu/techreports/</ext-link></citation>
</ref>
<ref id="bibr15-1534508412447010">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Cummings</surname><given-names>K. D.</given-names></name>
<name><surname>Otterstedt</surname><given-names>J.</given-names></name>
<name><surname>Kennedy</surname><given-names>P. C.</given-names></name>
<name><surname>Baker</surname><given-names>S. K.</given-names></name>
<name><surname>Kame’enui</surname><given-names>E. J.</given-names></name>
</person-group> (<year>2011</year>). <source>DIBELS Data System: 2009-2010 percentile ranks</source> (Tech. Rep. No. 1102). <publisher-loc>Eugene</publisher-loc>: <publisher-name>University of Oregon</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://dibels.uoregon.edu/techreports/">https://dibels.uoregon.edu/techreports/</ext-link></citation>
</ref>
<ref id="bibr16-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Daane</surname><given-names>M. C.</given-names></name>
<name><surname>Campbell</surname><given-names>J. R.</given-names></name>
<name><surname>Grigg</surname><given-names>W. S.</given-names></name>
<name><surname>Goodman</surname><given-names>M. J.</given-names></name>
<name><surname>Oranje</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <source>NAEP 2002 special study of oral reading</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr17-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Deno</surname><given-names>S. L.</given-names></name>
</person-group> (<year>1985</year>). <article-title>CBM: The emerging alternative</article-title>. <source>Exceptional Children</source>, <volume>52</volume>, <fpage>219</fpage>–<lpage>232</lpage>.</citation>
</ref>
<ref id="bibr18-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Francis</surname><given-names>D. J.</given-names></name>
<name><surname>Santi</surname><given-names>K. L.</given-names></name>
<name><surname>Barr</surname><given-names>C.</given-names></name>
<name><surname>Fletcher</surname><given-names>J. M.</given-names></name>
<name><surname>Varisco</surname><given-names>A.</given-names></name>
<name><surname>Foorman</surname><given-names>B. R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Form effects on the estimation of students’ ORF using DIBELS</article-title>. <source>Journal of School Psychology</source>, <volume>46</volume>, <fpage>315</fpage>–<lpage>342</lpage>.</citation>
</ref>
<ref id="bibr19-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>L.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Monitoring progress among mildly handicapped pupils: Review of current practice and research</article-title>. <source>Remedial and Special Education</source>, <volume>7</volume>, <fpage>5</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr20-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Deno</surname><given-names>S. L.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Must instructionally useful performance assessment be based in the curriculum?</article-title> <source>Exceptional Children</source>, <volume>61</volume>, <fpage>15</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr21-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Fuchs</surname><given-names>D.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Can diagnostic reading assessment enhance general educators instructional differentiation and student learning?</article-title> In <person-group person-group-type="editor">
<name><surname>Foorman</surname><given-names>B.</given-names></name>
</person-group> (Ed.), <source>Preventing and remediating reading difficulties</source> (pp. <fpage>325</fpage>–<lpage>351</lpage>). <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>York Press</publisher-name>.</citation>
</ref>
<ref id="bibr22-1534508412447010">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Good</surname><given-names>R. H.</given-names></name>
<name><surname>Kaminski</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2002</year>). <source>DIBELS oral reading fluency passages for first through third grades</source> (Tech. Rep. No. 10). <publisher-loc>Eugene</publisher-loc>: <publisher-name>University of Oregon</publisher-name>. Retrieved from: <ext-link ext-link-type="uri" xlink:href="https://dibels.uoregon.edu/techreports/">https://dibels.uoregon.edu/techreports/</ext-link></citation>
</ref>
<ref id="bibr23-1534508412447010">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Good</surname><given-names>R. H.</given-names></name>
<name><surname>Kaminski</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2011a</year>). <source>DIBELS Next assessment manual</source>. <publisher-loc>Eugene, OR</publisher-loc>: <publisher-name>Dynamic Measurement Group</publisher-name>. Available from <ext-link ext-link-type="uri" xlink:href="https://dibels.org/">https://dibels.org/</ext-link></citation>
</ref>
<ref id="bibr24-1534508412447010">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Good</surname><given-names>R. H.</given-names></name>
<name><surname>Kaminski</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2011b</year>). <source>DIBELS Next technical manual</source>. <publisher-loc>Eugene, OR</publisher-loc>: <publisher-name>Dynamic Measurement Group</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://dibels.org/next/index.php">https://dibels.org/next/index.php</ext-link></citation>
</ref>
<ref id="bibr25-1534508412447010">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Good</surname><given-names>R. H.</given-names></name>
<name><surname>Kaminski</surname><given-names>R. A.</given-names></name>
<name><surname>Dill</surname><given-names>S.</given-names></name>
</person-group> (<year>2002</year>). <article-title>DIBELS oral reading fluency</article-title>. In <person-group person-group-type="editor">
<name><surname>Good</surname><given-names>R. H.</given-names></name>
<name><surname>Kaminski</surname><given-names>R. A.</given-names></name>
</person-group> (Eds.), <source>DIBELS</source> (<edition>6th ed.</edition>). <publisher-loc>Eugene, OR</publisher-loc>: <publisher-name>Institute for the Development of Educational Achievement</publisher-name>. Available from <ext-link ext-link-type="uri" xlink:href="http://dibels.uoregon.edu/">http://dibels.uoregon.edu/</ext-link></citation>
</ref>
<ref id="bibr26-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Graney</surname><given-names>S. B.</given-names></name>
<name><surname>Shinn</surname><given-names>M. R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Effects of R-CBM teacher feedback in general education classrooms</article-title>. <source>School Psychology Review</source>, <volume>34</volume>, <fpage>184</fpage>–<lpage>202</lpage>.</citation>
</ref>
<ref id="bibr27-1534508412447010">
<citation citation-type="book">
<collab>Harcourt Brace Educational Measurement</collab>. (<year>2007</year>). <source>Stanford Achievement Test-10</source> (<edition>10th ed.</edition>). <publisher-loc>San Antonio, TX</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr28-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hedges</surname><given-names>L. V.</given-names></name>
<name><surname>Hedberg</surname><given-names>E. C.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Intraclass correlation values for planning group-randomized trials in education</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>29</volume>, <fpage>60</fpage>–<lpage>87</lpage>.</citation>
</ref>
<ref id="bibr29-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hixson</surname><given-names>M. D.</given-names></name>
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
<name><surname>Bradley-Johnson</surname><given-names>S.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Best practices in the analysis of progress monitoring data and decision making</article-title>. In <person-group person-group-type="editor">
<name><surname>Grimes</surname><given-names>J.</given-names></name>
<name><surname>Thomas</surname><given-names>A.</given-names></name>
</person-group> (Eds.), <source>Best practices in school psychology-V</source> (pp. <fpage>2133</fpage>–<lpage>2146</lpage>). <publisher-loc>Bethesda, MD</publisher-loc>: <publisher-name>NASP</publisher-name>.</citation>
</ref>
<ref id="bibr30-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Howe</surname><given-names>K. B.</given-names></name>
<name><surname>Shinn</surname><given-names>M. M.</given-names></name>
</person-group> (<year>2002</year>). <source>Standard reading assessment passages for use in general outcomes measurement</source>. <publisher-loc>Eden Prairie, MN</publisher-loc>: <publisher-name>Edformation</publisher-name>.</citation>
</ref>
<ref id="bibr31-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kaminski</surname><given-names>R. A.</given-names></name>
<name><surname>Cummings</surname><given-names>K. D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Assessment for learning: Using general outcomes measures</article-title>. <source>Threshold</source>, <volume>2007</volume>, <fpage>26</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr32-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kolen</surname><given-names>M. J.</given-names></name>
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2004</year>). <source>Test equating, scaling, and linking: Methods and practices</source> (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr33-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kranzler</surname><given-names>J.</given-names></name>
<name><surname>Brownell</surname><given-names>M.</given-names></name>
<name><surname>Miller</surname><given-names>M.</given-names></name>
</person-group> (<year>1998</year>). <article-title>The construct validity of R-CBM: An empirical test of a plausible rival hypothesis</article-title>. <source>Journal of School Psychology</source>, <volume>36</volume>, <fpage>399</fpage>–<lpage>415</lpage>.</citation>
</ref>
<ref id="bibr34-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Marston</surname><given-names>D. B.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Curriculum-based measurement: What it is and why do it?</article-title> In <person-group person-group-type="editor">
<name><surname>Shinn</surname><given-names>M. R.</given-names></name>
</person-group> (Ed.), <source>Curriculum-based measurement: Assessing special children</source> (pp. <fpage>18</fpage>–<lpage>78</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford Press</publisher-name>.</citation>
</ref>
<ref id="bibr35-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Messick</surname><given-names>S.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Validity</article-title>. In <person-group person-group-type="editor">
<name><surname>Linn</surname><given-names>R. L.</given-names></name>
</person-group> (Ed.), <source>Educational measurement</source> (<edition>3rd ed.</edition>, pp. <fpage>13</fpage>–<lpage>103</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Macmillan</publisher-name>.</citation>
</ref>
<ref id="bibr36-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moses</surname><given-names>T.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A comparison of statistical significance tests for selecting equating functions</article-title>. <source>Applied Psychological Measurement</source>, <volume>33</volume>, <fpage>285</fpage>–<lpage>306</lpage>.</citation>
</ref>
<ref id="bibr37-1534508412447010">
<citation citation-type="gov">
<collab>National Center for Education Statistics</collab>. (<year>2011</year>). <source>Common core of data: Public elementary/secondary school universe Survey 2009-10</source> (Version 1a) [Data file and documentation]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://nces.ed.gov/ccd/pubschuniv.asp">http://nces.ed.gov/ccd/pubschuniv.asp</ext-link></citation>
</ref>
<ref id="bibr38-1534508412447010">
<citation citation-type="web"><collab>National Center for Response to Intervention [NCRTI]</collab> (<year>n. d.</year>). Progress Monitoring Tools. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.rti4success.org/progressMonitoringTools/">http://www.rti4success.org/progressMonitoringTools/</ext-link></citation>
</ref>
<ref id="bibr39-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>O’Connell</surname><given-names>A. A.</given-names></name>
<name><surname>McCoach</surname><given-names>D. B.</given-names></name>
</person-group> (<year>2008</year>). <source>Multilevel modeling of educational data</source>. <publisher-loc>Charlotte, NC</publisher-loc>: <publisher-name>Information Age Publishing, Inc</publisher-name>.</citation>
</ref>
<ref id="bibr40-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Petscher</surname><given-names>Y.</given-names></name>
<name><surname>Kim</surname><given-names>Y.-S.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The utility and accuracy of oral reading fluency score types in predicting reading comprehension</article-title>. <source>Journal of School Psychology</source>, <volume>49</volume>, <fpage>107</fpage>-<lpage>129</lpage>.</citation>
</ref>
<ref id="bibr41-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Petscher</surname><given-names>Y.</given-names></name>
<name><surname>Kim</surname><given-names>Y.-S.</given-names></name>
<name><surname>Foorman</surname><given-names>B. R.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The importance of predictive power in early screening assessments: Implications for placement in the response to intervention framework</article-title>. <source>Assessment for Effective Intervention</source>, <volume>36</volume>, <fpage>158</fpage>–<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr42-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Powell-Smith</surname><given-names>K. A.</given-names></name>
<name><surname>Good</surname><given-names>R. H.</given-names></name>
<name><surname>Atkins</surname><given-names>T.</given-names></name>
</person-group> (<year>2010</year>). <source>DIBELS Next ORF Readability Study</source> (Tech. Rep. No. 7). <publisher-loc>Eugene, OR</publisher-loc>: <publisher-name>Dynamic Measurement Group</publisher-name>.</citation>
</ref>
<ref id="bibr43-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shinn</surname><given-names>M. R.</given-names></name>
</person-group> (<year>1989</year>). <source>CBM: Assessing special children</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr44-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shinn</surname><given-names>M. R.</given-names></name>
<name><surname>Gleason</surname><given-names>M. M.</given-names></name>
<name><surname>Tindal</surname><given-names>G.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Varying the difficulty of testing materials: Implications for CBM</article-title>. <source>Journal of Special Education</source>, <volume>23</volume>, <fpage>223</fpage>–<lpage>233</lpage>.</citation>
</ref>
<ref id="bibr45-1534508412447010">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stoolmiller</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <source>Multilevel models for ORF in the DDS: Variation by state, district, school, and student</source> (Tech. Rep. No. 0801). <publisher-loc>Eugene</publisher-loc>: <publisher-name>University of Oregon</publisher-name>.</citation>
</ref>
<ref id="bibr46-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stoolmiller</surname><given-names>M.</given-names></name>
<name><surname>Biancarosa</surname><given-names>G.</given-names></name>
<name><surname>Fien</surname><given-names>H.</given-names></name>
</person-group> (<year>2012</year>). <source>The measurement properties of DIBELS ORF in grade 2: Implications for equating studies</source>. Manuscript submitted for publication.</citation>
</ref>
<ref id="bibr47-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>J.</given-names></name>
<name><surname>Roehrig</surname><given-names>A. D.</given-names></name>
<name><surname>Hensler</surname><given-names>B. S.</given-names></name>
<name><surname>Connor</surname><given-names>C. M.</given-names></name>
<name><surname>Schatschneider</surname><given-names>C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Teacher quality moderates the genetic effects on early reading</article-title>. <source>Science</source>, <volume>328</volume>, <fpage>512</fpage>–<lpage>514</lpage>.</citation>
</ref>
<ref id="bibr48-1534508412447010">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wesson</surname><given-names>C. L.</given-names></name>
<name><surname>Vierthaler</surname><given-names>J. M.</given-names></name>
<name><surname>Haubrich</surname><given-names>P. A.</given-names></name>
</person-group> (<year>1989</year>). <article-title>An efficient technique for establishing reading groups</article-title>. <source>Reading Teacher</source>, <volume>42</volume>, <fpage>466</fpage>–<lpage>469</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>