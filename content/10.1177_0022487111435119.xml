<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JTE</journal-id>
<journal-id journal-id-type="hwp">spjte</journal-id>
<journal-title>Journal of Teacher Education</journal-title>
<issn pub-type="ppub">0022-4871</issn>
<issn pub-type="epub">1552-7816</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022487111435119</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022487111435119</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Target Practice</article-title>
<subtitle>Reader Response Theory and Teachers’ Interpretations of Students’ SAT 10 Scores in Data-Based Professional Development</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Atkinson</surname><given-names>Becky M.</given-names></name>
<xref ref-type="aff" rid="aff1-0022487111435119">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022487111435119"><label>1</label>University of Alabama, Tuscaloosa, USA</aff>
<author-notes>
<corresp id="corresp1-0022487111435119">Becky M. Atkinson, University of Alabama, Box 870302, Tuscaloosa, AL 35487, USA Email: <email>atkin014@bamaed.ua.edu</email></corresp>
<fn fn-type="other" id="bio1-0022487111435119">
<p>Becky M. Atkinson is an assistant professor in the College of Education at The University of Alabama. She teaches courses in social and cultural foundations of education and multicultural education. Her research looks at how teachers make meaning of their practice through the lenses of teacher knowledge scholarship and reader response theory, with a particular focus on developing and sustaining critical conversations about issues of social justice in teaching and learning. Her interests in pragmatic semiotics, feminist poststructuralism, and critical theory seed her current research. She has published articles in <italic>Educational Theory, Qualitative Inquiry, Educational Studies</italic>, and the <italic>Journal of Educational Research</italic>.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>63</volume>
<issue>3</issue>
<issue-title>Beyond the Teacher Certification Program Debate: From Models to Features</issue-title>
<fpage>201</fpage>
<lpage>213</lpage>
<permissions>
<copyright-statement>© 2012 American Association of Colleges for Teacher Education</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Association of Colleges for Teacher Education</copyright-holder>
</permissions>
<abstract>
<p>The study reported in this article examines how teachers read and respond to their students’ Stanford Achievement Test 10 (SAT 10) scores with the goal of investigating the assumption that data-based teaching practice is more “objective” and less susceptible to divergent teacher interpretation. The study uses reader response theory to frame teachers’ responses to their students’ SAT 10 test scores as interpretations of a text shaped through unexamined assumptions and political interests related to accountability, rather than strictly statistical “official” interpretations of “objective” data. The findings illustrate that teachers’ interpretations of SAT 10 data that inform their data-based practice are vulnerable to the pervasive influence of local school responses to accountability pressures. More specifically, the findings reveal how moral and discursive texts imbricated in accountability discourses mediate the ways in which teachers read and respond to their students’ SAT 10 scores.</p>
</abstract>
<kwd-group>
<kwd>educational policy</kwd>
<kwd>qualitative research</kwd>
<kwd>teacher knowledge</kwd>
<kwd>teacher research</kwd>
<kwd>teaching context</kwd>
</kwd-group>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>May/June 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>Over the past 30 years accountability in education has gained international currency, holding educators responsible for student achievement primarily as measured by regularly administered standardized tests (<xref ref-type="bibr" rid="bibr35-0022487111435119">Riffert, 2005</xref>). In the United States, as a focus of emphasis of the No Child Left Behind Act (<xref ref-type="bibr" rid="bibr29-0022487111435119">NCLB; 2002</xref>) and the more recent program of school reform, <xref ref-type="bibr" rid="bibr34-0022487111435119">Race to the Top (2009)</xref>, standardized testing for accountability purposes exerts weighty influence in schooling contexts from school district central offices to the smaller settings of thousands of classrooms. Standardized test scores as “the coin of the realm in public education in the United States” (<xref ref-type="bibr" rid="bibr13-0022487111435119">Haladyna, Nolen, &amp; Haas, 1991</xref>, p. 2), bring widespread ramifications not only for schools but also for communities and businesses:</p>
<p>
<disp-quote><p>Student achievement scores are now the barometer of student, teacher, principal, school, and district effectiveness. Student performance on standardized tests also affects the community, business and industry, real estate values and the overall vitality of a state and community. (<xref ref-type="bibr" rid="bibr38-0022487111435119">Shen &amp; Cooley, 2008</xref>, p. 319)</p></disp-quote>
</p>
<p>The emphasis on test scores as indicators of student achievement, school effectiveness, and community prosperity brings with it increased efforts and urgencies to improve those scores, especially in schools that struggle to meet state-determined goals. Based on the assumption that student test data and accountability requirements bring about changes in teaching practice that lead to higher test scores and better student achievement, policy makers, lawmakers, and educators have directed attention to looking at how to use data (primarily test scores) more effectively to inform data-based teaching practice. More specifically, school administrators and teachers have been encouraged to use test score data to implement instructional practices based on what <xref ref-type="bibr" rid="bibr16-0022487111435119">Ingram, Louis, and Schroeder (2004)</xref> identify as the “unexamined assumption that external data and accountability systems will lead to positive change in the daily interaction between teachers and students” (p. 1258), which results in higher test scores. Toward this end, districts and schools have instituted professional development programs in which teachers are asked to examine test score data and make decisions about students and about instruction.</p>
<p>But when measures of and decisions about progress in student learning, teacher quality, and school effectiveness rely primarily on improved test scores, it seems inevitable that attention will narrow to using test score data strategically and superficially to raise test scores rather than address more complex issues related to student learning, educational resources, and social inequities (<xref ref-type="bibr" rid="bibr12-0022487111435119">Haladyna, 2006</xref>; <xref ref-type="bibr" rid="bibr37-0022487111435119">Schildkamp &amp; Kuiper, 2009</xref>). Such narrow interpretations of test score data lead to the appropriation of educational strategies and practices that have been called “educational triage” (<xref ref-type="bibr" rid="bibr4-0022487111435119">Booher-Jennings, 2006</xref>; <xref ref-type="bibr" rid="bibr11-0022487111435119">Gillborn &amp; Youdell, 2000</xref>). In professional development, this can lead to a focus on teacher learning about and analyzing data that precludes other learning that might otherwise be appropriate. At the classroom level, this strategic use of test data in the name of data-based practice can constrain teaching and learning to a form of “target practice” that has troubling repercussions not only for students but also for teachers, as suggested in the study reported in this article. This project examined how teachers read and respond to their students’ Stanford Achievement Test 10 (SAT 10) scores with the goal of investigating the assumption that data-based teaching practice is more “objective” and less susceptible to divergent teacher interpretation. The study used reader response theory to analyze teachers’ responses to their students’ SAT 10 test scores as interpretations of a text shaped through unexamined assumptions and political interests related to accountability, rather than strictly statistical “official” interpretations of “objective” data. The findings illustrate that teachers’ interpretations of SAT 10 data that inform their “data-based practice” are vulnerable to the pervasive influence of local school responses to accountability pressures. More specifically, the findings reveal how moral and discursive texts imbricated in accountability discourses mediate the ways in which teachers read and respond to their students’ SAT 10 scores.</p>
<sec id="section1-0022487111435119">
<title>SAT 10</title>
<p>The SAT 10 is the current version of a norm-referenced standardized measure of student achievement first written in 1926. During the years since then, it has been used to compare individual student achievement with norms established by representative national samples of student achievement from the referent group, usually by grade level. Although the SAT series had long been used to measure student achievement in various places nationwide, its use grew with the NCLB legislation mandating achievement testing in reading and math during Grades 3 to 8 and once in Grades 10 to 12 (<xref ref-type="bibr" rid="bibr7-0022487111435119">Ding &amp; Navarro, 2004</xref>). The federal government does not require the administration of the SAT 10 specifically. It has become more commonly used for NCLB-related accountability purposes, especially in those states that did not already have a state achievement test. It is used in the state where this study took place.</p>
<p>The SAT 10 instrument is offered in two forms, traditional paper/pencil or electronic, and at 13 levels from kindergarten to Grade 12. It consists of an arrangement of multiple choice, short answer, and “extended response” items that may require students to show work, write several sentences, or draw a graph (<xref ref-type="bibr" rid="bibr32-0022487111435119">Pearson Education, 2011</xref>). Pearson, the publisher of the SAT series, offers a range of types of score reports that schools can purchase. These include four different versions of individual student reports, class reports, and reports by school, which were used by these schools. The student report displaying national percentile rankings, stanines, and subject area clusters of content and skills tested for that grade level was the version used in all of the schools in the study.</p>
</sec>
<sec id="section2-0022487111435119">
<title>The Interpretive Context of Test Score Data</title>
<p>This project grew from the recognition that test score data alone “provides no judgment or interpretation, and no sustainable basis of action” (<xref ref-type="bibr" rid="bibr37-0022487111435119">Schildkamp &amp; Kuiper, 2009</xref>, p. 482). In order for test score data to be informative, it must be read, interpreted, and transformed into information, processes that involve making meaning of the data within the context of the teaching and learning situation. This makes the interpretation of test data vulnerable to multiple interpretations and competing interests, especially in the context of high-stakes accountability. At the classroom level, the inevitable processes mediating how teachers interpret test score data into information used for instructional practice are often overlooked by policy makers, making commonly held assumptions that data-based decision making is more “objective” and somehow free of human interpretation seem naïve. Attempts to bypass or direct teachers’ interpretations of quantitative data because of faith in the objectivity of data discount the mediated nature of teachers’ assessments of knowledge.</p>
<p>Even before the heavy-handed mandates of NCLB’s accountability requirements, research into the effects of standardized testing for accountability warned about attempts to separate the context in which teachers enact their knowledge of practice from numerical representations of their students’ learning outcomes (<xref ref-type="bibr" rid="bibr13-0022487111435119">Haladyna et al., 1991</xref>; <xref ref-type="bibr" rid="bibr26-0022487111435119">McNeil 2000</xref>; <xref ref-type="bibr" rid="bibr40-0022487111435119">Smith, 1991</xref>). In her 1991 study of the effects of testing on teachers, Smith concluded, “numeric test scores mean little to the teachers we studied, particularly without the interpretive context that teachers alone possess” (p. 9). Her findings suggest that it is simplistic to assume that teachers separate their knowledge of students and the schooling context from their students’ test scores. In addition to any numerical indicators of statistical significance, the meanings teachers ascribe to students’ test scores reflect the teachers’ knowledge of the microcontext for testing—classroom conditions and students’ individual experiences on the days of testing, implications of poor performance for students’ futures as well as teachers’ sense of their own professional expertise and job tenure. Teachers’ knowledge of the macrocontext for testing—repercussions for poor performance at the community and state levels, and newspaper publication of test scores and school rankings—also mediates how teachers’ interpret their students’ test scores. The point here is that the processes of turning data into information inheres in teachers’ interpretive processes that are socially, culturally, historically, and professionally informed in irreducible ways that have real consequences for students as well as teachers. So, examining how teachers make meaning of and interpret student SAT 10 score data calls for a more robust theory of interpretation than that put forward by more simplistic assumptions undergirding testing policy.</p>
</sec>
<sec id="section3-0022487111435119">
<title>Reader Response Theory</title>
<p>Reader response theory (<xref ref-type="bibr" rid="bibr9-0022487111435119">Fish, 1980</xref>; <xref ref-type="bibr" rid="bibr17-0022487111435119">Iser, 1974</xref>, <xref ref-type="bibr" rid="bibr18-0022487111435119">1978</xref>; <xref ref-type="bibr" rid="bibr36-0022487111435119">Rosenblatt, 1978</xref>), also known as the transactional theory of the literary work suggested by <xref ref-type="bibr" rid="bibr36-0022487111435119">Rosenblatt (1978)</xref>, offers an approach to looking at texts and readers’ responses to texts that emphasizes the reader’s role in constructing the meaning of a text. Rosenblatt emphasized the give and take between the reader and a text as one of “Transaction . . . [that] permits emphasis on the to-and-fro, spiraling, nonlinear, continuously reciprocal influence of reader and text in the making of meaning” (p. xvi). Although reader response theory includes a variety of approaches to analyzing a reader’s relationship with a text, there is general agreement that neither the text nor the reader exerts sole authorship of a text’s meaning, but that meaning is transacted through their mutual engagement (<xref ref-type="bibr" rid="bibr9-0022487111435119">Fish, 1980</xref>; <xref ref-type="bibr" rid="bibr17-0022487111435119">Iser, 1974</xref>, <xref ref-type="bibr" rid="bibr18-0022487111435119">1978</xref>; <xref ref-type="bibr" rid="bibr36-0022487111435119">Rosenblatt, 1978</xref>).</p>
<p><xref ref-type="bibr" rid="bibr17-0022487111435119">Iser (1974)</xref> and <xref ref-type="bibr" rid="bibr9-0022487111435119">Fish (1980)</xref> suggest that the reader’s transaction with a text produces a “virtual text,” a reconstruction of the text as mediated through her interpretive resources. This is not restricted to literary texts, although reader response theory has been located primarily in a range of literary criticism and theory, including those influenced by feminist and poststructuralist theory (<xref ref-type="bibr" rid="bibr6-0022487111435119">Derrida, 1974</xref>; <xref ref-type="bibr" rid="bibr9-0022487111435119">Fish, 1980</xref>; <xref ref-type="bibr" rid="bibr17-0022487111435119">Iser, 1974</xref>, <xref ref-type="bibr" rid="bibr18-0022487111435119">1978</xref>; <xref ref-type="bibr" rid="bibr39-0022487111435119">Sheriff, 1989</xref>; <xref ref-type="bibr" rid="bibr42-0022487111435119">Tompkins, 1980</xref>). Fish asserted emphatically, “linguistic and textual (scientific) facts are not <italic>objects</italic> of interpretation, but its <italic>products</italic>” (p. 9). <xref ref-type="bibr" rid="bibr3-0022487111435119">Barone (2001)</xref> and others extend the use of reader response theory as an approach useful for educational research, especially that concerned with narrative (<xref ref-type="bibr" rid="bibr2-0022487111435119">Atkinson &amp; Rosiek, 2008</xref>, <xref ref-type="bibr" rid="bibr1-0022487111435119">2010</xref>; <xref ref-type="bibr" rid="bibr43-0022487111435119">Zeek, Foote, &amp; Walker, 2001</xref>).</p>
<p>Furthermore, the respective “interpretive communities” (<xref ref-type="bibr" rid="bibr9-0022487111435119">Fish, 1980</xref>) of which readers and writers are members mediate the meaning of texts. Fish described interpretive communities as groups of people who share common experiences such as traditions, habits, practices, and attitudes that provide semiotic resources for interpretation of human activity. Specifically, these interpretive resources mediate the work of writers in constructing texts as well as that of readers in receiving, reconstructing, and appropriating responses to texts. The concept of readers and writers as members of various communities draws attention to the production of meanings for texts as communal as well as individual. So when teachers read their students’ test scores, they appropriate interpretive discourses shaped by their experiences as educators in their schools and communities characterized by social class, race, ethnicity, local history, and politics.</p>
<p>More specifically, the concept of interpretive communities suggests that the politics of federal and state accountability policy, prevailing educational ideologies and discourses, and the particular urgencies and status of the local school and its district mediate meanings and responses teachers construct for their students’ test scores. Local influences in particular create constraining conditions in which teachers interpret test score data following the direction of their local school and district leadership. The meanings they transact with their students’ SAT 10 scores and put into place through a variety of instructional practices produce discursive and strategic texts by which students are read as particular kinds of learners and teachers as certain kinds of teachers. This study illustrates how teachers’ responses to test score data reveal the pervasive influence of local school interests and stereotypical discourses of cultural and moral deficits related to low-income students on how they interpret test scores and make decisions based on those interpretations.</p>
<sec id="section4-0022487111435119">
<title>Teachers’ Interpretations of SAT 10 Test Scores: Methods and Materials</title>
<p>This qualitative study consisted of 18 interviews, 2 interviews with each of the nine third- through fifth-grade teachers from two different school districts who agreed to participate. The nine participants were located through connections with the university’s College of Education partnership with the three schools represented in the study. I went through official channels first requesting permission to carry out the study from the two school boards involved and then obtaining permission from the school principals, who initiated the request for participation to the teachers. I then met with those who had expressed interest in participating to explain the study and their involvement which, in addition to responding to interview questions, required that they bring de-identified class and individual student SAT 10 reports to the interviews as reference points for the interview questions. Those who chose to participate were guaranteed confidentiality for themselves and their students, and signed consent forms. Participants received transcriptions of their interviews and preliminary summaries of findings accompanied by requests for revisions or corrections.</p>
</sec>
<sec id="section5-0022487111435119">
<title>School Demographics and Teacher Participants</title>
<p>The teachers worked in three different elementary schools within a county that encompasses a university-centered midsize urban area located in the southeastern region of the United States. All three schools are Title I schools in primarily working-class and economically disadvantaged communities. The schools have populations composed primarily of African American and European American students, with small percentages of Latino/a students. South, a city school, is the most homogeneous of the three schools with a majority population of 90% African American students. East and Southeast, both county schools, are located in what is commonly referred to as the urban fringe of a midsized city. A total of nine teachers participated in the study, four from South, two from East, and three from Southeast. Three African American teachers, all faculty at South, participated in the study. All of the other participants were European American (see <xref ref-type="table" rid="table1-0022487111435119">Table 1</xref>).</p>
<table-wrap id="table1-0022487111435119" position="float">
<label>Table 1.</label>
<caption><p>School Demographics and Teacher Participants</p></caption>
<graphic alternate-form-of="table1-0022487111435119" xlink:href="10.1177_0022487111435119-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">South City School</th>
<th align="center">East County School 1</th>
<th align="center">Southeast County School 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Total number of students</td>
<td>381</td>
<td>490</td>
<td>375</td>
</tr>
<tr>
<td>Percentage of African American students</td>
<td>90%</td>
<td>19.4%</td>
<td>67.14%</td>
</tr>
<tr>
<td>Percentage of Latino/a students</td>
<td>3.85%</td>
<td>1.39%</td>
<td>7.14%</td>
</tr>
<tr>
<td>Percentage of European American students</td>
<td>3.85%</td>
<td>77.78%</td>
<td>25.71%</td>
</tr>
<tr>
<td>Unreported ethnicity</td>
<td>0%</td>
<td>1.39%</td>
<td>0%</td>
</tr>
<tr>
<td>Teacher participants: Race; years of teaching experience; gender</td>
<td>3 African Americans; 5, 8, and 11 years experience; 1 male, 2 female</td>
<td>2 European Americans; 4 to 5 years experience; 1 male, 1 female</td>
<td>3 European Americans; 1 to 2 years experience; 3 female</td>
</tr>
<tr>
<td/>
<td>1 European American; 3 years experience; female</td>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0022487111435119">
<p>Note: Data source—<ext-link ext-link-type="uri" xlink:href="http://schooltree.org">http://schooltree.org</ext-link></p>
</fn>
</table-wrap-foot></table-wrap>
<p>Although all three schools have struggled in the past with making “Average Yearly Progress” (AYP), a state-determined measurement of school improvement mandated by NCLB, all three had achieved “All Clear” status for the past few years and were aiming to maintain that status. However, the possibility of failing to make AYP consistently surfaced as a subtext in teachers’ conversations during the study, as SAT 10 scores figured in the determination of AYP for the state.</p>
</sec>
<sec id="section6-0022487111435119">
<title>Data Collection</title>
<p>I interviewed each of the nine teachers twice, using a semistructured interview protocol with follow-up questions (see <xref ref-type="app" rid="app1-0022487111435119">appendix</xref>). After opening questions about the teachers’ educational background and length of teaching experience, succeeding questions fell into three broad categories: (a) using data, (b) interpreting data, and (c) significance of testing to their teaching. The interviews lasted from 45 min to an hour; they were audiotaped and transcribed. I also took field notes about the timing and conditions of each interview. The interviews all took place in each teacher’s respective classroom at the end of the school day. In each case, the teacher was currently working with the class whose scores we were discussing but had not been the students’ teacher for the previous year when they took the test. For the city school, South, and County School 1, East, the interviews took place in January, February, and March before that year’s SAT 10 testing. The interviews for County School 2, Southeast, took place in May after the SAT 10 testing had been completed and at the conclusion of the school year. (Please note that school names are pseudonyms.)</p>
</sec>
<sec id="section7-0022487111435119">
<title>Data Analysis</title>
<p>The 18 transcriptions of the interviews provided the major source of data, with field notes used minimally. The unit of analysis was the teachers’ responses to the interview questions. The reader response theoretical frame informing the study suggested that readers’ responses to texts can be understood in three ways according to how readers use texts to construct meaning, including conventional, visionary, and critical (<xref ref-type="bibr" rid="bibr3-0022487111435119">Barone, 2001</xref>; <xref ref-type="bibr" rid="bibr9-0022487111435119">Fish, 1980</xref>; <xref ref-type="bibr" rid="bibr17-0022487111435119">Iser, 1974</xref>). As reader response theory has only recently been extended to educational inquiry, I was interested in investigating the constraints and possibilities of these analytical categories and how they could initially map the data. Data analysis began with a deductive-like examination of the individual teachers’ interview transcripts to code elements in each that illustrated the response types.</p>
<sec id="section8-0022487111435119">
<title>Conventional</title>
<p>Readers respond to texts in conventional ways that conform to prevailing discourses, common practices, and dominant ideologies that reinforce their own beliefs and experiences. In this study, teachers’ responses that described common school practices and confirmed stereotypical discourses and deficit ideologies related to low income and student achievement were labeled as conventional. For example, comments attributing low test scores to students or their families not valuing education were labeled conventional in that they reflected deficit discourses associated with low-income students who quite frequently are also students of color (<xref ref-type="bibr" rid="bibr24-0022487111435119">Lynn, Bacon, Totten, Bridges, &amp; Jennings, 2010</xref>).</p>
</sec>
<sec id="section9-0022487111435119">
<title>Visionary</title>
<p>Visionary responses are transacted by readers who, in <xref ref-type="bibr" rid="bibr3-0022487111435119">Barone’s (2001)</xref> words, “pragmatize the imaginary” (p. 178); they are open to generating new ideas (the imaginary) for practical considerations (pragmatize), or think reflexively on taken for granted assumptions. Teachers’ expression of new ways to think about practice stimulated by SAT 10 results would be labeled visionary, as would new ways to think self-reflexively. For example, one teacher described how he looked at state reading achievement scores “as a reflective piece . . . on what I need to do to change my teaching.”</p>
</sec>
<sec id="section10-0022487111435119">
<title>Critical</title>
<p>Critical responses, like visionary responses, involve new thinking about unexamined assumptions. The difference is that critical responses emerge from an ideological, political, or professional stance that the reader brings to the text to reveal taken-for-granted assumptions in the <italic>text</italic>. However, visionary responses illustrate a reader’s awakening to or realization of a different perspective or practical strategy suggested by the text, which may address the <italic>reader’s</italic> taken for granted assumptions.</p>
<p>Critical responses can take three forms. Some offer an informed and political critique of the text, drawing informally or formally on critical theory to challenge sociopolitical implications in the text’s propositions. This type is illustrated by comments such as one teacher’s observation that the SAT 10 measured not what students had learned but the “have’s and the have not’s.”</p>
<p>A second critical type response points to the silences, voices, and experiences not represented in the text but nonetheless present as “surplus of meaning” (<xref ref-type="bibr" rid="bibr6-0022487111435119">Derrida, 1974</xref>) that brings to the surface unacknowledged meanings for texts by virtue of what is not said nor included. The teachers’ comments about their experiences of feeling silenced as well as their silences about the misuses of testing data exemplify this type of critical response.</p>
<p>A third form of critical response includes those that dispute the premise of the text; readers may offer a counter narrative of an explanation in the text or redefine terms. One teacher’s comment that the SAT 10 was basically “a test of reading,” rather than a test of multiple skills and achievement, illustrates this third type of critical response.</p>
</sec>
</sec>
<sec id="section11-0022487111435119">
<title>Inductive Analysis</title>
<p>I did not expect individual teachers’ transcripts to sort neatly into the suggested response types but for each one to manifest a messy and complicated mix of responses. Although the initial approach mapped out particular elements of response types within each transcript, I conducted a more inductive analysis of the coded data, grouping them by response types to support a more nuanced and robust analysis (<xref ref-type="bibr" rid="bibr22-0022487111435119">LeCompte &amp; Schensul, 1999</xref>).</p>
<p>I then completed multiple examinations of the grouped data fragments, coding again for recurring ideas, constructs, and processes—such as “targeted instruction” and “target student”—as well as repeated patterns—such as references to students by labels as “my 2,” “bump kids,” or “special ed student.” These emerging codes represented meanings associated with how the teachers understood and interpreted test scores within their school context. Working with the newly coded data within each response type, I generated categories and where needed, subcategories, based on shared meanings such as “targeted.” Then I completed axial coding by looking for relationships among categories to see which could be collapsed into larger categories (<xref ref-type="bibr" rid="bibr41-0022487111435119">Strauss &amp; Corbin, 1998</xref>). For example, I examined how “targeted” and “student labels” could be collapsed into “using data with students,” and then how “using data with students” expressed a conventional response to SAT 10 scores.</p>
<sec id="section12-0022487111435119">
<title>Identifying themes</title>
<p>The purpose of the study was to see if there were variations in how teachers interpreted SAT 10 data and what influences mediated those variations. Asking questions about how the larger categories and their respective response types could be connected enabled me to identify the political, professional, social, and cultural discourses the teachers appropriated in their explanations (<xref ref-type="bibr" rid="bibr8-0022487111435119">Ely, Anzul, Friedman, Garner, &amp; Steinmetz, 1991</xref>; <xref ref-type="bibr" rid="bibr15-0022487111435119">Hatch, 2002</xref>). What discourses did the teachers draw on to appropriate conventional responses to student SAT 10 data in explaining their actions and decisions? Similarly, how did professional, political, and social discourses mediate the teachers’ critical responses that acknowledged the inequities between student “have’s” and “have not’s” but did not question the validity of the SAT 10 and their schools’ misuse of data? Following this process of inquiry, I derived themes and subthemes for each response type that connected the meanings, practices, and discourses present in the categories of coded data associated with each of the response types.</p>
<p>The analysis of the conventional responses yielded two themes across all of the schools: “target practice” and “personal responsibility.” Critical responses suggested the presence of three subthemes and an overarching theme of “constraints and contradiction.” I did not find any visionary responses across all the schools, although two teachers at East offered visionary responses suggesting a theme of “utilitarian reflection.” The critical and conventional response themes play across and through each other to uncover discursive and moral texts energized by deficit discourses and classist ideologies operating through educational accountability policies to position students and teachers as subjects in need of control and regulation in the name of equity and accountability.</p>
</sec>
</sec>
</sec>
<sec id="section13-0022487111435119">
<title>Findings and Discussion</title>
<p>The teachers’ responses evidenced a complex divergence of interpretations of SAT 10 data not anticipated by policy makers who assume that reading “objective” data results in “objective” interpretation and information dissemination. If teachers’ interpretations of test score data were as straightforward and “objective” as is commonly assumed, the teachers would have all offered conventional responses similarly infused with statistical language related to content and/or performance standards, and <italic>unrelated</italic> to whether a child “tried” that day, or “didn’t care.” The occurrence of multiple interpretations indicates that the teachers read and constructed more complex meanings in the test scores than just numerical comparisons or indications of learning achievement; they read them within “the interpretive context” (<xref ref-type="bibr" rid="bibr40-0022487111435119">Smith, 1991</xref>) of their knowledge of prevailing educational ideologies, local context, and their experiences with students. The following sections present findings organized by themes and the three response types: conventional, critical, and visionary. Then I discuss differences by school and individuals.</p>
<sec id="section14-0022487111435119">
<title>Conventional Responses: Target Practice and Personal Responsibility</title>
<p>The majority of the teachers’ comments evidenced awareness of the implications for achievement test scores associated with students from poverty and low-income communities who attend Title I schools. Their responses appropriated mainstream discourses of cultural deficits related to low-income students and achievement to explain low test scores. That, along with accountability interests in assigning blame and leadership pressures to raise test scores, produced a form of schooling from which I derived the theme “target practice.”</p>
<sec id="section15-0022487111435119">
<title>Target practice</title>
<p>The recurrence of the word <italic>target</italic> in various forms distinguished its meanings as a highly significant concept to the teachers across almost all of the interviews. The theme of “target” or “targeting” was used to describe strategic processes directed by the schools’ priority of maintaining their “All Clear” AYP status. I repeatedly heard phrases like “target instruction,” “target students,” and “targeted for” particular intervention strategies or special placement. The SAT 10 and the many different standardized tests taken by the students enabled an increasingly individualized and focused accumulation of data that fostered a sort of typology of students. Students whose SAT 10 scores ranged in Stanines 1 to 3 and/or were quite a bit below grade level were labeled as the “low, lows.” The “really really high” students whose scores ranged in Stanines 7 or higher were seen as students, according to two teachers from Southeast, “who can teach themselves” or “the ones who will get it anyway.” “Bump kids,” in other literature referred to as “bubble kids” (<xref ref-type="bibr" rid="bibr4-0022487111435119">Booher-Jennings, 2006</xref>; <xref ref-type="bibr" rid="bibr25-0022487111435119">Madaus &amp; Russell, 2010</xref>), were those scoring in the low-average to average range who were most likely to raise or “bump up” their test scores with “targeted” instruction. The primary focus for teachers’ “data meetings” was to identify where individual students fell in these categories so as to locate the “bump kids.” A South teacher remarked, “We get those ones in the middle so we can bump those up.” As one teacher pointed out, “average kids are the ones we like because they will show growth.”</p>
<p>Teachers at each school reported distributing time according to these attributions to students’ potential to improve their test scores. One teacher commented that she was told “not to worry about the ‘low low’s’ or the ‘really really high’s’” but to “get the middle to bump up.” Several teachers at different schools observed that they met every day with their “bump kids” but only twice a week with the “low” and “high” kids because, as a South fourth-grade teacher put it,</p>
<p>
<disp-quote><p>[bump kids] are the ones you focus on because you want the kids in the middle to be up here at the top, . . . the middle kids who are almost there (grade-level average) . . . you focus on them more. You want them to move on up. And then ones that are at the bottom, you do the best you can because it [SAT 10] doesn’t measure progress.</p></disp-quote>
</p>
<p>A Southeast teacher commented similarly, “I pull the middle group every day . . . because those are those bump ones and that’s what we’ve been told—is to get them . . . and then to pull the lower ones and the high ones about twice a week or three times a week.”</p>
</sec>
<sec id="section16-0022487111435119">
<title>Personal responsibility</title>
<p>During the interviews, the teachers at all the schools insisted that student test scores reflected student effort as well as student achievement. In response to questions about individual students’ test scores, all spoke of the importance of students caring about doing well. A fifth-grade teacher at East said, “I feel like 80% will do well and 20% will fall short whether they can do it or not because but they are not willing to try and do any better than what they are doing.” This seemed to express the commonly held meaning of “caring”—“to try and do better than they are doing”—implying motivation and effort to score higher. Explanations for students’ low scores centered around the importance of a student’s “caring” or “trying.”</p>
<p>Teachers attributed lack of effort to individual student apathy or laziness, usually identified as</p>
<list id="list1-0022487111435119" list-type="bullet">
<list-item><p>“those students aren’t working as hard, you know, they aren’t trying their best . . . ”</p></list-item>
<list-item><p>“it’s because of their attitude towards this test and they didn’t really care about the test.”</p></list-item>
<list-item><p>“some of them don’t like school; they don’t care . . . ”</p></list-item>
<list-item><p>“It’s attitude for him . . . I think he can do better than what he does, if he would try.”</p></list-item>
</list>
<p>Some teachers saw lack of effort as the consequence of economic background or family values and circumstances:</p>
<list id="list2-0022487111435119" list-type="bullet">
<list-item><p>“But when you’re in the classroom and you know this child knows this information, they just told it to you and they are sitting here just half asleep or they didn’t eat breakfast, or they are tired taking the test, . . . they don’t care, and some of our kids don’t know if they are going to have a meal when they get home, so what are they worried about bubbling in a test?”</p></list-item>
<list-item><p>“Some of them are trained . . . you are going to work or you are going to live off the government, you are not going to college most likely, or if you do you are going to grow up and be a wrestler, . . . ‘I am going to be a rapper,’ ‘I am going to work at Mercedes (local production plant).’”</p></list-item>
<list-item><p>“This child is a behavior problem and I don’t think anybody really cares about him at home. He’s my child (who) will come in with snot across his face—dried snot—and I’m just thinking, ‘did nobody look at this baby before he left?’”</p></list-item>
<list-item><p>“They’ve been told by people in the past or they think they can’t do it, so they give up.”</p></list-item>
</list>
<p>Efforts to encourage students to “take ownership” of their test scores was the tonic prescribed by the school leadership to address students’ apparent lack of effort. The recurring phrase <italic>taking ownership</italic> of their test scores underscored the deficit discourses appropriated by the teachers and school leadership that placed responsibility for low test scores on the students and their families rather than on structural inequalities or schools’ indifference. A South teacher expressed this stereotypical thinking:</p>
<p>
<disp-quote><p>I think because it is [a] lower socioeconomic school and you don’t have the parental economics that plays a big role . . . at some of my students’ houses, there are not any books to read. They don’t go to the library . . . they don’t take their textbooks home like they are supposed to. You call them if they [students] get into trouble or sick and you can’t get anybody, so I mean that this is not important to them.</p></disp-quote>
</p>
<p>The fact that these schools are Title I schools in low-income communities coupled with the schools’ ascription of deficits to students and their families reinscribes commonly held stereotypes that low-income communities do not value education (<xref ref-type="bibr" rid="bibr14-0022487111435119">Hale, 2001</xref>; <xref ref-type="bibr" rid="bibr24-0022487111435119">Lynn et al., 2010</xref>; <xref ref-type="bibr" rid="bibr30-0022487111435119">Noguera, 2007</xref>). A 1st-year teacher at Southeast shared that her African American university supervisor commented that the parents of the students at the primarily African American low-income school where she did her student teaching “don’t care . . . because they don’t want their children to do better than they did.” Both comments speak to the racialized overtones of deficit discourses associated with low-income students, who are disproportionately students of color. This was no more dramatically symbolized and embodied than by the practice of “coloring in the bars.”</p>
<p><italic>“Coloring in the bars.”</italic> Teachers at all three schools described similar practices intended to guide students to “take ownership” of their SAT 10 scores. In meetings with each class and its teacher, the counselor/coordinator distributed blank grids resembling bar graphs similar to those displayed in SAT 10 student reports. Each student was also given his individual SAT 10 score report. Students were directed to use markers to color code their percentile ranks and stanines in each of the content and skills areas on the blank grid. Students colored below-grade-level scores red, grade level and just below yellow, and above grade level green. One teacher from Southeast explained,</p>
<p>
<disp-quote><p>The counselor came in to go over the SAT scores with the class so that they could color in their bars and see where they fail. So that they would know, “I made a one on these or I made a two on these and I have to be responsible for my own learning.”</p></disp-quote>
</p>
<p>Another teacher at South, where the same activity occurred, said that students with scores landing in the “red and yellow” bars learned they needed to “be” in the “green bar.” The act of coloring in the bars embodied the schools’ transfer of responsibility for achievement to the students. Students inscribed their own learner subjectivity into small, colored bar graphs that located their distance from the norm and learned how far off “target” they were, or as one teacher said, “where they fail.”</p>
<p>The “coloring in the bars” activity was followed with what seemed to be intended as a “pep talk” by each school’s counselor. In a private conference with individual students to guide them to “take ownership” of their SAT 10 score, the counselor and the student together looked at the charts with the colored-in bars and discussed what the student needed to do to raise her scores.</p>
</sec>
</sec>
<sec id="section17-0022487111435119">
<title>Critical Responses: Constraints and Contradiction</title>
<p>Critical response findings proved to be messy indeed. I analyzed the three types of critical responses separately for subthemes but also considered them collectively as dimensions of a theme. This collectivity provided an overarching theme of “constraints and contradictions.” The subtheme of “Testing as a Technology of Power” for Type 1 responses reflects the teachers’ awareness of the sorting and stratifying uses of the SAT 10 and the consequences for students and themselves. The subtheme for Type 2 responses, “Silenced Voices,” speaks to teachers’ marginalization as professionals held accountable but given no voice or respect for their professional knowledge and experience. It also reveals the teachers’ silence concerning the validity and misuses of the SAT 10 data. Type 3 responses suggest the subtheme of “Real Teachers” by which the teachers redefined “real teachers” as those with experiences “in the trenches.”</p>
<sec id="section18-0022487111435119">
<title>Critical Response Type 1: Testing as a technology of power</title>
<p>Each teacher’s comments revealed awareness of how the SAT 10 scores were used as a mechanism of power and control over students, which reinforced differences in income levels and foreclosed opportunities for more stimulating curriculum and pedagogy. They reported that students doing well at school invariably represented families with higher incomes. A fourth-grade teacher at Southeast commented, “These scores, I see . . . I look at it more as a have and a have not kind of thing.”</p>
<p>Test scores determined curriculum content, quality, and depth, and instructional approach for many students. All of the schools used homogeneous small groups based on test scores for within-classroom instruction. At South, the highest scoring students, Stanines 7 to 9, were often, but not consistently, placed in “advanced” type of classes that offered more challenging learning experiences not bound to scripted programs nor subject to curriculum pacing guides. Classes at the other schools included the range from above to below grade level and adhered to the various scripted curriculum programs required by the state and local district. As another teacher observed in reference to the curriculum resources offered with the required programs, “It’s supposed to be a level playing field because you get resources and all that, but it’s not.” A South teacher who worked with “average” or “struggling” students offered that highest scoring students and their teachers “get to go to another style of teaching and you know and do something fun and I hate that because my classroom can be the same as the gifted class . . . if I didn’t have to adhere . . . to follow every program.”</p>
<p>This points to how testing policy leverages educational opportunity and teacher autonomy in ways that maintain social inequities under the rhetoric of educational accountability. Engaging teachers in the regulatory work of “target practice” through testing’s technology compels teachers to act as enforcers that maintain the boundaries separating the “have’s from the have not’s.”</p>
</sec>
<sec id="section19-0022487111435119">
<title>Critical Response Type 2: Silenced voices</title>
<p>Type 2 critical responses offer experiences and voices not represented in a text but present in its margins as leftover or “surplus meaning” (<xref ref-type="bibr" rid="bibr6-0022487111435119">Derrida, 1974</xref>). Considering testing policy as the text, the teachers find themselves pushed to the margins. Regularly monitored, they are required to speak publisher’s scripted words because theirs are seen as inadequate. The fifth-grade teacher from East observed, “I can’t bring anything inside the program.” A teacher from South expressed frustration at being directed to change his teaching but “(they’re) not giving me room to change because I have to follow a program.” Scripted data meetings, intended to be collaborations among professionals sharing data, also silence teachers’ voices, as noted by a Southeast teacher, “Most of the time they [data meetings] were scripted—she [Reading Coach] knew what she wanted to say.”</p>
<p>Furthermore, the surplus of meaning in the margin of the test scores was the students’ progress made possible by the teachers’ work with them. Several teachers expressed frustration at witnessing how their students had progressed, realizing that if they did not perform well on the SAT 10 or the state achievement test, both the teachers’ and students’ work would not be valued. A male third-grade teacher at South expressed concern that the SAT 10 did not show the progress of students who</p>
<p>
<disp-quote><p>came in writing on the wrong side of the paper . . . This test doesn’t show where they are when they leave the classroom. I mean that’s what upsets me. This test doesn’t know my kids . . . and this test doesn’t show what that child has learned over a period of time being with me.</p></disp-quote>
</p>
<p>I did not hear any critiques of the validity of the SAT 10 or how the schools used the data, a silence that provokes troubling questions about teacher political disengagement, fatigue, disempowerment, or surrender.</p>
</sec>
<sec id="section20-0022487111435119">
<title>Critical Response Type 3: Real teachers</title>
<p>Critical Type 3 responses develop alternative definitions for terms or concepts proposed in a text or offer counter narratives to a text’s “plot.” The teachers asserted that “real teachers” are found “in the trenches,” not monitoring fidelity to curriculum or checking up on pacing guides. As a third-grade teacher at South observed, “We’re fed up with people coming in who couldn’t handle the trenches and telling us how to deal with the trenches. Only real teachers understand this struggle every day.” Policy makers and state department consultants are seen as outsiders who left or were never in the trenches, which makes their authority questionable in the eyes of many of these teachers. A fourth-grade teacher from Southeast shared, “I get ill because a lot of the times the people that are telling you how to do your job have never done your job . . . the people who make the policy have never taught in a classroom.” A third-grade teacher from East expressed aggravation at the irony as “the tests are pushed by people who have not been in a classroom in a very, very, very, long time . . . don’t tell me how to do my job if you are not in the trenches with me.”</p>
<p>Even differences in school contexts call into question a consultant’s authority to “tell us what to do,” according to a South fourth-grade teacher,</p>
<p>
<disp-quote><p>If you . . . taught all your . . . teaching career in a school where the kids had enough money, they had food at home, they have nice clothes, they don’t have to worry about where they are going to sleep—you have no idea what I go through every day, because my kids don’t have that and I’ve never taught at a school where kids had that.</p></disp-quote>
</p>
<p>The awareness that teaching in Title I schools presents challenges that teachers in middle class or affluent schools rarely encounter speaks to the teachers’ thinking that somehow their schools are “deficient,” or not “normal.” When asked how they thought the Educational Testing Services arrived at the norms established for the average percentile rankings, several teachers indicated a disjuncture between their own students and the students in the imaginary who “are the norm.” One observed, “I know there’s a norm out there . . . Some kids in a different school could probably handle that type of question or that type of test or whatever. But here, you know, some of ours just can’t.” Most of the teachers shared the opinion that they did not feel that their students were represented in the normed rankings. Of course, the irony of the situation is that even though these teachers felt that their schools and students were not represented in the norm, they “enforced” the norm on their students and urged them to “take ownership” of test scores calculated by that same norm.</p>
<p>Considered collectively, these findings reveal contradictions between the teachers’ recognition and criticism of the reductive processes and constraining conditions to which they are subject and their shortsightedness relative to similarly reductive processes to which students are subjected. As dimensions of the overarching theme “constraints and contradictions,” themes from the three response types illustrate how the constraints of accountability testing contribute to contradictions imbricated in teachers’ critical interpretations of their experiences with test data. Teachers’ demands for respect for their professional judgment and knowledge contradict their lack of professional and knowledgeable critique of the validity and use of SAT 10 data. They critique the applicability of SAT 10 norms to their students but compel their students to “take responsibility” for not performing to those norms. Finally, there is the contradiction in teachers assigning responsibility for low test scores to students and their families without reflection on their responsibility to reflect on how their teaching may be contributing to students’ low sores.</p>
</sec>
</sec>
<sec id="section21-0022487111435119">
<title>Visionary Responses: Utilitarian Reflection</title>
<p>Visionary responses indicate that a reader has generated new ideas to apply to practical considerations in response to a text. Two teachers at East described how they responded to state achievement scores that evoked a theme of utilitarian reflection on personal efforts to raise them. The third-grade teacher offered that the scores were “a reflective piece,” that caused him to reflect generally on how he needed to change his teaching. The fifth-grade teacher recounted how the high incidence of the below-grade-level score of “2” on state math achievement tests led her to analyze patterns in her students’ answers. Based on her analysis, she spent time “training” students to fill out the required answer grids and saw fewer errors, despite her frustration at having to devote time for that.</p>
<p>These examples illustrate how visionary responses could range from practical analysis of the testing data to a more general reconsideration of one’s teaching. Both involve a certain amount of reflection. Both show responses to test data that do not blame the students. Each teacher’s interpretation of the test data drew from imagining reasons for low test scores outside of the deficit thinking discourses that characterized the other responses.</p>
</sec>
</sec>
<sec id="section22-0022487111435119">
<title>Differences by Schools and Individuals</title>
<p>Presenting findings grouped by the three response types highlights the numerous commonalities shared by the individual teachers across schools, specifically the similar ways in which deficit discourses are appropriated to explain school practices and variations in student achievement. This section presents differences by school and individuals mostly distinguished by differences in years of teaching experience and racialized discourse.</p>
<sec id="section23-0022487111435119">
<title>South</title>
<p>South teachers, three African American and one European American in an almost homogeneous African American Title I school, offered stronger and more numerous critiques of the effects of accountability policy on their practice and their sense of themselves as professionals. This seemed mostly directed toward the state department monitors, as noted in the third-grade male teacher’s response, “you’re regulating from the state department because you couldn’t handle it” and may reflect South’s previous years of “Needs Improvement” status. Their discontent was directed to accountability pressures and state monitors, not to the school leadership.</p>
<p>The one European American teacher in the group appropriated racialized discourses to explain students’ low test scores in terms of “their culture.” In response to a question about students’ “caring” about education, she said, “I have had kids say ‘I don’t like White people and I don’t have to listen to what you say.’” However, she also spoke of her perception of students’ sense of entitlement as a reason many didn’t care about school, “you owe me something because one, I don’t have any money, two I used to be a slave.” She saw the students’ objection to a White teacher as an obstacle, but not her own attitude, a deeply disturbing finding.</p>
</sec>
<sec id="section24-0022487111435119">
<title>East</title>
<p>The visionary responses offered by two European American teachers from East, a predominately European American Title I school, distinguish them from the others. The fifth-grade teacher, who had to deal with the SAT 10, reading tests as well as three different kinds of state achievement tests, was more outspoken about the state’s disregard of teachers’ input on testing and curriculum. By contrast, the third-grade male teacher, who had fewer tests to prepare and account for, offered mild critiques of the district’s guidance.</p>
</sec>
<sec id="section25-0022487111435119">
<title>Southeast</title>
<p>The inexperience of the three European American female fourth-grade teachers at this majority African American Title I school distinguished their responses from those of the other schools. Whereas the more experienced teachers at South and East referred to previous years’ teaching when they exercised more autonomy, these novices made no such comparisons. Their concerns about staying on track with the pacing guide and weekly collaborations on teaching strategies were not tinged with the fatigued and/or angry overtones I heard in the more experienced teachers’ comments at South or the fifth-grade teacher at East.</p>
</sec>
</sec>
<sec id="section26-0022487111435119">
<title>Implications for Data-Based Practice</title>
<p>In asking what the findings of this study suggest, several implications and questions emerge for: (a) reader response analysis, (b) teachers and the text of the test, (c) teacher education and research, and (d) the accountability of school leaders to teachers. After offering reflections on limitations and possibilities of reader response analysis suggested by this study, I discuss implications for teachers and the text of the test using the reader response framing of the text as the regulator and co-constructor of its meaning. That is followed by questions for teacher education and research suggested by the study. Finally, implications for educational leaders conclude the article.</p>
<sec id="section27-0022487111435119">
<title>Reader Response Analysis</title>
<p>In doing this, or any analysis, the researcher is reading and responding to the data and co-constructing a virtual text. Keeping that in mind, I found that beginning data analysis by coding for the three response types may have narrowed my initial examination. It left open the possibility of excluding significant data that might have been included by initial inductive analysis. Reader response theory encompasses a broad range of theoretical frameworks from social theory to feminism, any of which could be used to inform an inductive analysis from which multiple responses and perspectives could be generated and put alongside each other for deeper understanding.</p>
</sec>
<sec id="section28-0022487111435119">
<title>Teachers Reading the Text of the Test</title>
<p><xref ref-type="bibr" rid="bibr36-0022487111435119">Rosenblatt (1978)</xref> suggests that the product of a reader’s transaction of meaning with a literary text is a poem. The poststructuralist semiotics claim in reader response theory used in this analysis generalizes that concept to propose that the products of the teachers’ transactions of meaning with their students’ SAT 10 scores are texts. Findings from this study suggest that teachers read much more in test scores than just decontextualized numbers and charts on the page. They read their students’ SAT 10 scores as a moral and a discursive text that together prescribe classroom practices and teachers’ expectations in an economy of merits and deficits.</p>
<p>However, the teachers did not expand the text on their own. <xref ref-type="bibr" rid="bibr36-0022487111435119">Rosenblatt (1978)</xref> reminds us, “the text regulates what shall be held in the forefront of the reader’s attention” (p. 11), suggesting that the reader does not construct meaning independent from the text he or she reads. Rather, she co-constructs meaning regulated by what the text puts in front of her. So if teachers interpreted moral and discursive meanings in SAT 10 scores, the resources for those meanings were already in place in the interpretive context shaped by accountability policy and deficit discourses associated with low-income students. SAT 10 scores act metonymically for the moral and discursive texts inhering in and imbricated with the percentile scores and stanines on students’ test reports.</p>
<sec id="section29-0022487111435119">
<title>The moral text</title>
<p>The term <italic>accountability</italic> implies moral intent, so it should come as no surprise that teachers read and produce a moral text in their transactions with SAT 10 scores. In a democracy that funds public education for all, so the thinking seems to go, it is the moral responsibility of students to do their best, teachers to teach well, and schools to effectively use resources so that public funds are not wasted and students are produced as contributing participants in society. SAT 10 scores interpreted as measures of achievement and effort are products of meritocratic ideologies that inscribe these moral and discursive meanings into testing and test scores.</p>
<p>Research since the Coleman report has supported the relationship between lower social economic status (SES), minority status. and lower educational achievement (<xref ref-type="bibr" rid="bibr5-0022487111435119">Borman &amp; Dowling, 2010</xref>; <xref ref-type="bibr" rid="bibr20-0022487111435119">Ladson-Billings, 2006</xref>; <xref ref-type="bibr" rid="bibr31-0022487111435119">Oakes, 2005</xref>). The complex relationships among SES, race, ethnicity, and immigrant status muddy understandings about how school achievement relates to these variables. However, many educators influenced by popular discourses of cultural deficits linking low income, race, low achievement, and disregard for the value of education blame students and their families for low test scores (<xref ref-type="bibr" rid="bibr14-0022487111435119">Hale, 2001</xref>; <xref ref-type="bibr" rid="bibr24-0022487111435119">Lynn et al., 2010</xref>; <xref ref-type="bibr" rid="bibr30-0022487111435119">Noguera, 2007</xref>). In this way, low test scores seen as a consequence of students’ disregard for the value of education or lack of effort are read as a moral weakness, even failure.</p>
<p>This perspective sustains the “culture of low expectations” (<xref ref-type="bibr" rid="bibr21-0022487111435119">Landsman, 2004</xref>) associated with deficit thinking that was clearly evidenced in the schools’ practices of “coloring in the bars,” and the face-to-face conferences with counselors. These activities enact the “public spectacle of failure” identified by <xref ref-type="bibr" rid="bibr23-0022487111435119">Lipman (2004)</xref>, which embarrasses and humiliates students publically. But even more insidiously, these practices symbolically transfer the failure of the structures of schooling and its agents in the form of the teachers, principals, and counselors, to the vulnerable shoulders of children.</p>
<p>The “target” language wove itself into teachers’ explanations normalizing the attributions of blame for low scores to students’ and their families’ behaviors and attitudes. Conversely, that same action points to the personal and professional responsibility avoided by the teachers, school leaders, and ultimately by policy makers as they assign ownership solely to students. This fosters a morally bankrupt climate of self-interest and self-protection at the expense of the most vulnerable. Whether intentional or not, the moral text is deployed as a politically expedient text by passing the blame for low scores to students and their families.</p>
</sec>
<sec id="section30-0022487111435119">
<title>The discursive text</title>
<p>These teachers also read their students’ SAT 10 scores as a discursive text that “told” them “what was wrong,” or “what we needed to work on.” Test scores “told” the teachers what kinds of students they had and located students in certain learner subject positions. This prescriptive emphasis on what the test scores “tell” constructs a passive teacher as the listener and reader who receives this information uncritically, as evidenced in the absence of teachers’ critique of the SAT 10 data and how it was used.</p>
<p>The SAT 10 scores alone were not the only testing outcomes shaping the discursive text. With almost every teacher, questions about their students’ SAT 10 scores invariably evoked mention of students’ other achievement scores. The repeated comments about the student performance on these other tests suggest that each test result informed the teachers’ thinking in the context of the other scores. When explaining a student’s SAT 10 score, I often heard a teacher refer to a student as “my 2,” or “she’s an intensive,” meaning she receives frequent reading intervention work. Students were ranked comparatively to each other and to a discursively regulated norm, their identities produced as “differentiated identities,” broken, and in need of fixing and/or intervention (<xref ref-type="bibr" rid="bibr23-0022487111435119">Lipman, 2004</xref>, p. 63).</p>
<p>Furthermore, one of the many consequences of these school practices is the discursive production of teachers’ practice as strategic “target practice” directing teachers to hunt for “bump kids” and take aim at higher scores. That leaves the “low low kids” as “collateral damage” (<xref ref-type="bibr" rid="bibr28-0022487111435119">Nichols &amp; Berliner, 2007</xref>), and “really really high kids” to fend for themselves when placed in mixed ability classes. In this way, teachers, leaders, and schools enact moral and discursive texts that mutually support and inform each other to uphold a kind of meritocratic imperialism that maintains the status quo of social and economic inequities.</p>
</sec>
</sec>
<sec id="section31-0022487111435119">
<title>Teacher Education and Teacher Research</title>
<p>The examples of teachers’ thinking about and interpreting SAT 10 scores presented in this article illustrate that teachers’ data-based practice is vulnerable to a variety of contingencies and interests. Data-based decision making did not prevent students from being targeted as merits or deficits, and actually seemed to be the mechanism by which students were “branded” into specific learner subjectivities. This study reminds us that schools are contested sites of control between federal, state, and local leaders. As “docile bodies” (<xref ref-type="bibr" rid="bibr10-0022487111435119">Foucault, 1977/1995</xref>) in the hierarchy of accountability, students and teachers often become the leverage point for these conflicts.</p>
<p>This presents a challenge to teacher education and research to support new teachers in “dealing with data” in productive ways that foster their own learning and growth as well as that of their students. When asked, all of these teachers said that their teacher education programs did not include training in how to read and interpret standardized test scores. Without that, teachers are left vulnerable to training and direction provided by the local district, which may not be correct, and may foreclose opportunities to think in more reflective or innovative ways. Data-based practice can be more generative and productive than that reported by this group of teachers, especially if allied with critical reflection (<xref ref-type="bibr" rid="bibr19-0022487111435119">Joyce, Calhoun, &amp; Hopkins, 1999</xref>; <xref ref-type="bibr" rid="bibr27-0022487111435119">Mid-continent Research for Education and Learning, 2010</xref>).</p>
<p>The other challenge is to provoke teachers’ realization that schools are sites of controversy and control. They should realize that their engagement and understanding of the political processes that produce scenarios such as these teachers spoke of require strategic dialogue and collaborative conversations that challenge prevailing political expediencies overshadowing good teaching practice.</p>
</sec>
<sec id="section32-0022487111435119">
<title>The Accountability of Educational Leaders to Teachers</title>
<p>Finally, it is important for principals and other school leaders to recognize their own complicity in evading responsibility for data-based practice that finds itself transformed into “target practice.” How do principals and leaders actually facilitate the generative and critical reflection on student data that appeared to be absent from the experiences reported by the teachers in this study?</p>
<p>It is clear that the achievement tests that currently supply the data on which data-based practice is based are highly questionable in their composition, validity, and the uses to which they are put (<xref ref-type="bibr" rid="bibr33-0022487111435119">Popham, 2010</xref>). However, standardized testing as a weighty presence in schools is here to stay. This is the time when educational leaders themselves need to develop their own data analysis skills and knowledge, what <xref ref-type="bibr" rid="bibr33-0022487111435119">Popham (2010)</xref> calls “assessment literacy,” so that they can speak out about the “instructional insensitivity” of most accountability tests. This needs to be accompanied with commitment to critical reflection to create conditions in which faculty can exercise their powers of critical thought in their deliberations on their students’ test scores. Leaders must be accountable for how they support and provide for their teachers’ engagements in an activity with so much potential to harm and that is so vulnerable to political exigencies. Teachers and leaders in the field need to develop critiques of accountability testing and policies that counter attitudes of resignation to the inevitability of testing. A waiting game sustained in the hope that accountability testing will go away is counterproductive.</p>
<p>In conclusion, we need further conversations about reconceptualizing teacher education and educational policy making in such a way as to recognize the irreducible and unguarantee-able nature of interpretation inherent to human practices such as teaching and learning, and more specifically, reading and interpreting data. Such a teacher education and practice need the data-informed information important to evaluating student learning as authentically as possible, as well as the resilience and reflexivity of teachers’ informed and critical reflection for evaluating its motivations and consequences.</p>
</sec>
</sec>
</body>
<back>
<app-group>
<app id="app1-0022487111435119">
<title>Appendix</title>
<sec id="section33-0022487111435119">
<title>Teachers’ Interpretations of Students’ SAT 10 Scores: Interview Protocol (Spread Over Two Interviews)</title>
<sec id="section34-0022487111435119">
<title>Using Data</title>
<list id="list3-0022487111435119" list-type="order">
<list-item><p>How and when do you receive your current students’ SAT 10 scores?</p></list-item>
<list-item><p>What is the first thing you do with them?</p></list-item>
<list-item><p>When first looking at the reports, where do you look first? Why?</p></list-item>
<list-item><p>In looking at the students’ SAT 10 scores, what information are you seeking and for what purpose?</p></list-item>
<list-item><p>What do you do with the information you locate?</p></list-item>
<list-item><p>What does your grade level do with the information? Your school?</p></list-item>
<list-item><p>What information on this report do you find to be the most meaningful? The least?</p></list-item>
<list-item><p>How often do you refer to these scores throughout the school year?</p></list-item>
</list>
</sec>
<sec id="section35-0022487111435119">
<title>Interpreting Data</title>
<list id="list4-0022487111435119" list-type="order">
<list-item><p>What does the percentile ranking mean?</p></list-item>
<list-item><p>What does the stanine level mean?</p></list-item>
<list-item><p>How do you interpret the content clusters?</p></list-item>
<list-item><p>What scores indicate that a student is struggling in an area or skill? Succeeding?</p></list-item>
<list-item><p>(I point to a particular math or reading score) What does this score mean for this student? For your instruction with this student?</p></list-item>
<list-item><p>Do you see any relationship between these scores and scores on other tests?</p></list-item>
<list-item><p>What is the easiest item on this report for you to understand and interpret? Most difficult?</p></list-item>
<list-item><p>To what extent do you think that these scores indicate the student’s learning?</p></list-item>
<list-item><p>What do you think these scores are a measure of?</p></list-item>
<list-item><p>How does your administration affect the way you interpret this data?</p></list-item>
<list-item><p>How do you explain students’ high scores? Low scores? Unexpected scores?</p></list-item>
</list>
</sec>
<sec id="section36-0022487111435119">
<title>Significance of the data</title>
<list id="list5-0022487111435119" list-type="order">
<list-item><p>What does this data mean for your teaching plans?</p></list-item>
<list-item><p>How would you describe this data’s impact on your practice? On your relationship with your students?</p></list-item>
<list-item><p>Can you recall a specific time when the test results made you change something about the way you teach?</p></list-item>
<list-item><p>What do you think the SAT 10 measures?</p></list-item>
<list-item><p>To whom is this data most important? Least important?</p></list-item>
<list-item><p>For whom does this data have the most impact?</p></list-item>
</list>
</sec>
</sec>
</app>
</app-group>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) received the following financial support for the research, authorship, and/or publication of this article: a research grant from the College of Education at The University of Alabama, and a Faculty Research Grant from The University of Alabama. The grants paid for the transcription of the interviews conducted for this study.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Atkinson</surname><given-names>B.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Teachers responding to narrative inquiry: An approach to narrative inquiry criticism</article-title>. <source>Journal of Educational Research</source>, <volume>103</volume>(<issue>2</issue>), <fpage>91</fpage>-<lpage>102</lpage>.</citation>
</ref>
<ref id="bibr2-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Atkinson</surname><given-names>B.</given-names></name>
<name><surname>Rosiek</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Researching and representing teacher voice(s): A reader response approach</article-title>. In <person-group person-group-type="editor">
<name><surname>Jackson</surname><given-names>A.</given-names></name>
<name><surname>Mazzei</surname><given-names>L.</given-names></name>
</person-group> (Eds.), <source>For voice in qualitative inquiry: Challenging conventional, interpretive, and critical conceptions</source> (pp. <fpage>175</fpage>-<lpage>196</lpage>). <publisher-loc>London, England</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr3-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Barone</surname><given-names>T.</given-names></name>
</person-group> (<year>2001</year>). <source>Touching eternity: The enduring outcomes of teaching</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Teachers College Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Booher-Jennings</surname><given-names>D.</given-names></name>
</person-group> (<year>2006</year>, <month>June</month>). <article-title>Rationing education in an era of accountability</article-title>. <source>Phi Delta Kappan</source>, <volume>87</volume>, <fpage>756</fpage>-<lpage>761</lpage>.</citation>
</ref>
<ref id="bibr5-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Borman</surname><given-names>G.</given-names></name>
<name><surname>Dowling</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Schools and inequality: A multilevel analysis of Coleman’s equality of educational opportunity data</article-title>. <source>Teachers College Record</source>, <volume>112</volume>(<issue>5</issue>), <fpage>1201</fpage>-<lpage>1246</lpage>.</citation>
</ref>
<ref id="bibr6-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Derrida</surname><given-names>J.</given-names></name>
</person-group> (<year>1974</year>). <source>Of grammatology</source>. <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Johns Hopkins University Press</publisher-name>.</citation>
</ref>
<ref id="bibr7-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ding</surname><given-names>C.</given-names></name>
<name><surname>Navarro</surname><given-names>V.</given-names></name>
</person-group> (<year>2004</year>). <article-title>An examination of student mathematics learning in elementary and middle schools: A longitudinal look from the US</article-title>. <source>Studies in Educational Evaluation</source>, <volume>30</volume>, <fpage>237</fpage>-<lpage>253</lpage>.</citation>
</ref>
<ref id="bibr8-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ely</surname><given-names>M.</given-names></name>
<name><surname>Anzul</surname><given-names>M.</given-names></name>
<name><surname>Friedman</surname><given-names>T.</given-names></name>
<name><surname>Garner</surname><given-names>D.</given-names></name>
<name><surname>Steinmetz</surname><given-names>A.</given-names></name>
</person-group> (<year>1991</year>). <source>Doing qualitative research: Circle within circles</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge Falmer</publisher-name>.</citation>
</ref>
<ref id="bibr9-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fish</surname><given-names>S.</given-names></name>
</person-group> (<year>1980</year>). <source>Is there a text in this class? The authority of interpretive communities</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Foucault</surname><given-names>M.</given-names></name>
</person-group> (<year>1995</year>). <source>Discipline and punish: The birth of the prison</source> (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Vintage Books</publisher-name>. (<comment>Original work published 1977</comment>)</citation>
</ref>
<ref id="bibr11-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gillborn</surname><given-names>G.</given-names></name>
<name><surname>Youdell</surname><given-names>D.</given-names></name>
</person-group> (<year>2000</year>). <source>Rationing education: Policy, practice, reform, and equity</source>. <publisher-loc>Buckingham, UK</publisher-loc>: <publisher-name>Open University Press</publisher-name>.</citation>
</ref>
<ref id="bibr12-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The perils of standardized achievement testing</article-title>. <source>Educational Horizons</source>, <volume>85</volume>(<issue>1</issue>), <fpage>30</fpage>-<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr13-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T.</given-names></name>
<name><surname>Nolen</surname><given-names>S.</given-names></name>
<name><surname>Haas</surname><given-names>N.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Raising standardized achievement test scores and the origins of test score pollution</article-title>. <source>Educational Researcher</source>, <volume>20</volume>(<issue>5</issue>), <fpage>2</fpage>-<lpage>7</lpage>.</citation>
</ref>
<ref id="bibr14-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hale</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <source>Learning while Black: Creating educational excellence for African American children</source>. <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Johns Hopkins University Press</publisher-name>.</citation>
</ref>
<ref id="bibr15-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hatch</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <source>Doing qualitative research in educational settings</source>. <publisher-loc>Albany</publisher-loc>: <publisher-name>State University of New York Press</publisher-name>.</citation>
</ref>
<ref id="bibr16-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ingram</surname><given-names>D.</given-names></name>
<name><surname>Louis</surname><given-names>K.</given-names></name>
<name><surname>Schroeder</surname><given-names>R.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Accountability policies and teacher decision making: Barriers to the use of data to improve practice</article-title>. <source>Teachers College Record</source>, <volume>106</volume>(<issue>6</issue>), <fpage>1258</fpage>-<lpage>1287</lpage>.</citation>
</ref>
<ref id="bibr17-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Iser</surname><given-names>W.</given-names></name>
</person-group> (<year>1974</year>). <source>The implied reader: Patterns of communication in prose fiction from Bunyan to Beckett</source>. <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Johns Hopkins University Press</publisher-name>.</citation>
</ref>
<ref id="bibr18-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Iser</surname><given-names>W.</given-names></name>
</person-group> (<year>1978</year>). <source>The act of reading: A theory of aesthetic response</source>. <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Johns Hopkins University Press</publisher-name>.</citation>
</ref>
<ref id="bibr19-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Joyce</surname><given-names>B.</given-names></name>
<name><surname>Calhoun</surname><given-names>E.</given-names></name>
<name><surname>Hopkins</surname><given-names>D.</given-names></name>
</person-group> (<year>1999</year>). <source>The new structure of school improvement: Inquiring schools and achieving students</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>Open University Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ladson-Billings</surname><given-names>G.</given-names></name>
</person-group> (<year>2006</year>). <article-title>From the achievement gap to the education debt: Understanding achievement in U.S. schools</article-title>. <source>Educational Researcher</source>, <volume>35</volume>(<issue>7</issue>), <fpage>3</fpage>-<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr21-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Landsman</surname><given-names>J.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Confronting the racism of low expectations</article-title>. <source>Educational Leadership</source>, <volume>62</volume>(<issue>4</issue>), <fpage>28</fpage>-<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr22-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>LeCompte</surname><given-names>M. D.</given-names></name>
<name><surname>Schensul</surname><given-names>J. J.</given-names></name>
</person-group> (<year>1999</year>). <source>Designing and conducting ethnographic research</source>. <publisher-loc>Lanham, MD</publisher-loc>: <publisher-name>Rowman &amp; Littlefield</publisher-name>.</citation>
</ref>
<ref id="bibr23-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lipman</surname><given-names>P.</given-names></name>
</person-group> (<year>2004</year>). <source>High stakes education, inequality, globalization, and urban school reform</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr24-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lynn</surname><given-names>M.</given-names></name>
<name><surname>Bacon</surname><given-names>J.</given-names></name>
<name><surname>Totten</surname><given-names>T.</given-names></name>
<name><surname>Bridges</surname><given-names>T.</given-names></name>
<name><surname>Jennings</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Examining teachers’ beliefs about African American male students in a low-performing high school in an African American school district</article-title>. <source>Teachers College Record</source>, <volume>112</volume>(<issue>1</issue>), <fpage>289</fpage>-<lpage>330</lpage>.</citation>
</ref>
<ref id="bibr25-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Madaus</surname><given-names>G.</given-names></name>
<name><surname>Russell</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Paradoxes of high-stakes testing</article-title>. <source>Journal of Education</source>, <volume>190</volume>(<issue>1/2</issue>), <fpage>21</fpage>-<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr26-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McNeil</surname><given-names>L.</given-names></name>
</person-group> (<year>2000</year>). <source>Contradictions of school reform</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr27-0022487111435119">
<citation citation-type="book">
<collab>Mid-continent Research for Education and Learning</collab>. (<year>2010</year>). <source>The power of data</source>. <publisher-loc>Denver, CO</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr28-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Nichols</surname><given-names>S. L.</given-names></name>
<name><surname>Berliner</surname><given-names>D. C.</given-names></name>
</person-group> (<year>2007</year>). <source>Collateral damage: How high stakes testing corrupts America’s schools</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Harvard Education Press</publisher-name>.</citation>
</ref>
<ref id="bibr29-0022487111435119">
<citation citation-type="gov">
<collab>No Child Left Behind Act</collab>. (<year>2002</year>). <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/nclb/accountability/ayp/testing-faq.html">http://www2.ed.gov/nclb/accountability/ayp/testing-faq.html</ext-link></comment></citation>
</ref>
<ref id="bibr30-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Noguera</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Extended view: Race, student achievement and the power and limitations of teaching</article-title>. <source>Sage Race Relations Abstracts</source>, <volume>32</volume>, <fpage>44</fpage>-<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr31-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Oakes</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <source>Keeping track: How schools structure inequality</source> (<edition>2nd ed.</edition>). <publisher-loc>New Haven, CT</publisher-loc>: <publisher-name>Yale University Press</publisher-name>.</citation>
</ref>
<ref id="bibr32-0022487111435119">
<citation citation-type="web">
<collab>Pearson Education</collab>. (<year>2011</year>). <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.pearsonassessments.com/HAIWEB/Cultures/en-us/Productdetail.htm?Pid=SAT10C">http://www.pearsonassessments.com/HAIWEB/Cultures/en-us/Productdetail.htm?Pid=SAT10C</ext-link></comment></citation>
</ref>
<ref id="bibr33-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Popham</surname><given-names>J.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Assessment illiteracy: Professional suicide</article-title>. <source>UCEA Review</source>, <volume>51</volume>(<issue>2</issue>), <fpage>1</fpage>-<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr34-0022487111435119">
<citation citation-type="gov">
<collab>Race to the Top</collab>. (<year>2009</year>). <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/programs/racetothetop/executive-summary.pdf">www2.ed.gov/programs/racetothetop/executive-summary.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr35-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Riffert</surname><given-names>F.</given-names></name>
</person-group> (<year>2005</year>).<article-title>The use and misuse of standardized testing: A Whiteheadian point of view</article-title>. <source>Interchange</source>, <volume>36</volume>(<issue>1-2</issue>), <fpage>231</fpage>-<lpage>252</lpage>.</citation>
</ref>
<ref id="bibr36-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rosenblatt</surname><given-names>L.</given-names></name>
</person-group> (<year>1978</year>). <source>The reader, the text and the poem</source>. <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Johns Hopkins University Press</publisher-name>.</citation>
</ref>
<ref id="bibr37-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schildkamp</surname><given-names>K.</given-names></name>
<name><surname>Kuiper</surname><given-names>W.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Data-informed curriculum reform: Which data, what purposes, and promoting and hindering factors</article-title>. <source>Teaching and Teacher Education</source>, <volume>26</volume>, <fpage>482</fpage>-<lpage>496</lpage>.</citation>
</ref>
<ref id="bibr38-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shen</surname><given-names>J.</given-names></name>
<name><surname>Cooley</surname><given-names>V.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Critical issues in using data for decision-making</article-title>. <source>International Journal of Leadership in Education</source>, <volume>11</volume>(<issue>3</issue>), <fpage>319</fpage>-<lpage>329</lpage>.</citation>
</ref>
<ref id="bibr39-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sheriff</surname><given-names>J.</given-names></name>
</person-group> (<year>1989</year>). <source>The fate of meaning: Charles Peirce, structuralism, and literature</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr40-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Smith</surname><given-names>M. L.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Put to the test: The effects of external testing on teachers</article-title>. <source>Educational Researcher</source>, <volume>20</volume>(<issue>5</issue>), <fpage>8</fpage>-<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr41-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Strauss</surname><given-names>A.</given-names></name>
<name><surname>Corbin</surname><given-names>J.</given-names></name>
</person-group> (<year>1998</year>). <source>Basics of qualitative research</source> (<edition>2nd ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr42-0022487111435119">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tompkins</surname><given-names>J.</given-names></name>
</person-group> (<year>1980</year>). <source>Reader response criticism: From formalism to structuralism</source>. <publisher-loc>Baltimore, MD</publisher-loc>: <publisher-name>Johns Hopkins University Press</publisher-name>.</citation>
</ref>
<ref id="bibr43-0022487111435119">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zeek</surname><given-names>C.</given-names></name>
<name><surname>Foote</surname><given-names>M.</given-names></name>
<name><surname>Walker</surname><given-names>C.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Teacher stories and transactional inquiry: Hearing the voices of mentor teachers</article-title>. <source>Journal of Teacher Education</source>, <volume>52</volume>(<issue>5</issue>), <fpage>377</fpage>-<lpage>385</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>