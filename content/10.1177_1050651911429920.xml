<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JBT</journal-id>
<journal-id journal-id-type="hwp">spjbt</journal-id>
<journal-title>Journal of Business and Technical Communication</journal-title>
<issn pub-type="ppub">1050-6519</issn>
<issn pub-type="epub">1552-4574</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1050651911429920</article-id>
<article-id pub-id-type="publisher-id">10.1177_1050651911429920</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Users’ Abilities to Review Web Site Pages</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Elling</surname>
<given-names>Sanne</given-names>
</name>
<xref ref-type="aff" rid="aff1-1050651911429920">1</xref>
<xref ref-type="corresp" rid="corresp1-1050651911429920"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lentz</surname>
<given-names>Leo</given-names>
</name>
<xref ref-type="aff" rid="aff1-1050651911429920">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jong</surname>
<given-names>Menno de</given-names>
</name>
<xref ref-type="aff" rid="aff2-1050651911429920">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-1050651911429920">
<label>1</label>Utrecht University, Utrecht, Netherlands</aff>
<aff id="aff2-1050651911429920">
<label>2</label>Department of Technical and Professional Communication, Faculty of Behavioral Sciences, University of Twente, Enschede, Netherlands</aff>
<author-notes>
<corresp id="corresp1-1050651911429920">Sanne Elling, Utrecht University, Trans 10, 3512 JK Utrecht, the Netherlands. E-mail: <email>S.Elling@uu.nl</email>
</corresp>
<fn fn-type="other" id="fn1-1050651911429920">
<p>
<bold>Sanne Elling</bold> is a PhD student at the Utrecht Institute for Linguistics (UIL-OTS) at Utrecht University in the Netherlands. Her research project is on user-focused methods of Web site usability evaluation.</p>
</fn>
<fn fn-type="other" id="fn2-1050651911429920">
<p>
<bold>Leo Lentz</bold> is a professor of Document Design and Communication at Utrecht University. His research focuses on usability and comprehension of information in different genres, including health care information, financial documents and forms, presented in digital and hard copy formats.</p>
</fn>
<fn fn-type="other" id="fn3-1050651911429920">
<p>
<bold>Menno de Jong</bold> is a professor of communication studies at the University of Twente in the Netherlands. His main research interests include the methodology of applied research techniques. He has published research articles on various methods of usability evaluation.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2012</year>
</pub-date>
<volume>26</volume>
<issue>2</issue>
<fpage>171</fpage>
<lpage>201</lpage>
<permissions>
<copyright-statement>© SAGE Publications 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Web sites increasingly encourage users to provide comments on the quality of the content by clicking on a feedback button and filling out a feedback form. Little is known about users’ abilities to provide such feedback. To guide the development of evaluation tools, this study examines to what extent users with various background characteristics are able to provide useful comments on informational Web sites. Results show that it is important to keep the feedback tools both simple and attractive so that users will be able and willing to provide useful feedback on Web site pages.</p>
</abstract>
<kwd-group>
<kwd>Web site evaluation</kwd>
<kwd>user review</kwd>
<kwd>feedback</kwd>
<kwd>usability</kwd>
<kwd>informational Web sites</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Organizations increasingly recognize the importance of giving the user a voice, and many Web sites contain a feedback option that invites users to comment on various aspects of the Web site. In this article, we focus on evaluation methods that enable users to give feedback on specific pages of a Web site. Such methods have received little attention in the literature about Web site evaluation methods, and there is no generally accepted term for this type of method yet. These methods, however, can be categorized as self-reported metrics (<xref ref-type="bibr" rid="bibr39-1050651911429920">Tullis &amp; Albert, 2008</xref>) because, as in surveys, users are asked about their experiences on the Web site. But unlike surveys, the methods we focus on ask for page-level feedback and allow for open comments, sometimes combined with an overall rating or some scale questions. We propose to call these methods <italic>user page reviews</italic> because they invite users to review a Web site by clicking on a button that appears on selected pages. In such reviews, users evaluate a Web site in much the same way as experts evaluate a Web site (<xref ref-type="bibr" rid="bibr45-1050651911429920">Welle Donker-Kuijer, De Jong, &amp; Lentz, 2008</xref>). Although users cannot be expected to have professional expertise about Web design, they can provide feedback from their own perspective about their own attitudes and experiences with the Web site.</p>
<p>Tools that can be used for gathering user feedback on Web site pages include Opinionlab, Kampyle, Usabilla, and Infocus. These instruments enable Web site visitors to share their opinions on everything they consider important. Selected Web site pages (or sometimes all pages) contain a button that users can click on if they want to react to something. These buttons can be small icons (e.g., a thumb or a plus–minus icon with the word <italic>feedback</italic>) or longer text links that invite reactions to the page. Users click on the link to open a screen on which they can provide their comments. Users can give open-ended comments, but the instruments often also ask users to choose a feedback category, provide ratings, or answer questions about page-specific topics. <xref ref-type="fig" rid="fig1-1050651911429920">Figures 1</xref>-<xref ref-type="fig" rid="fig4-1050651911429920">4</xref> show screen shots of feedback forms from Opinionlab, Kampyle, Infocus, and Usabilla, respectively.</p>
<fig id="fig1-1050651911429920" position="float">
<label>Figure 1.</label>
<caption>
<p>Opinionlab feedback form.</p>
</caption>
<graphic alternate-form-of="fig1-1050651911429920" xlink:href="10.1177_1050651911429920-fig1.tif"/>
</fig>
<fig id="fig2-1050651911429920" position="float">
<label>Figure 2.</label>
<caption>
<p>Kampyle feedback form.</p>
</caption>
<graphic alternate-form-of="fig2-1050651911429920" xlink:href="10.1177_1050651911429920-fig2.tif"/>
</fig>
<fig id="fig3-1050651911429920" position="float">
<label>Figure 3.</label>
<caption>
<p>Infocus feedback form.</p>
</caption>
<graphic alternate-form-of="fig3-1050651911429920" xlink:href="10.1177_1050651911429920-fig3.tif"/>
</fig>
<fig id="fig4-1050651911429920" position="float">
<label>Figure 4.</label>
<caption>
<p>Usabilla add-note option and invitation to users to click on elements they like.</p>
</caption>
<graphic alternate-form-of="fig4-1050651911429920" xlink:href="10.1177_1050651911429920-fig4.tif"/>
</fig>
<p>The Opinionlab form in <xref ref-type="fig" rid="fig1-1050651911429920">Figure 1</xref> looks rather dense and asks users to complete several tasks: to choose a topic from predefined categories, enter an open comment, rate the page on three aspects as well as overall, enter an e-mail address (which is optional), and indicate whether or not their comment is about the Web site. The Kampyle form in <xref ref-type="fig" rid="fig2-1050651911429920">Figure 2</xref> uses icons that users select to express their feelings and categorize their feedback. It asks users to rate the site by choosing an emoticon, to select a feedback topic (under the topics that are visible in the figure are rows with subtopics), and to fill in an open comment. The Infocus form (see <xref ref-type="fig" rid="fig3-1050651911429920">Figure 3</xref>) includes a list of predefined categories, a place to formulate comments, and three options for marking specific elements or segments: Users can underline, point an arrow at, or draw a frame around a relevant section that they want to comment on. This marking function is different from most other tools, in which users have to describe the exact location of the object of their feedback. Only Usabilla (see <xref ref-type="fig" rid="fig4-1050651911429920">Figure 4</xref>) offers a form of marking that enables users to add a note on the Web page. But it is not possible to mark the exact size of the selection or to use other more precise markings. Besides these four feedback tools, many examples of feedback buttons can be found on Web sites.</p>
<p>Even though such methods for obtaining user feedback on Web site pages have become more and more popular in practice, little is known about the merits and limitations of these methods. The literature on methods of user-focused Web site evaluation has focused strongly on the use of think-aloud usability testing and surveys and neglected the possibilities of asking users to review a Web site (e.g., <xref ref-type="bibr" rid="bibr7-1050651911429920">Cunliffe, 2000</xref>; <xref ref-type="bibr" rid="bibr39-1050651911429920">Tullis &amp; Albert, 2008</xref>). The user page reviews strongly depend on users’ skills in providing feedback and on their willingness to do so. Users may consider giving feedback as an extra task in addition to what they are doing on the Web site. In this article, we focus on the skills that are needed to provide feedback, examining the extent to which users are able to provide comments on a Web site. More knowledge about the users’ skills may provide useful information about the value of the output these review methods yield. Furthermore, this research may contribute to our knowledge about how to design effective user feedback tools. Before discussing our research questions, we address related work on the skills that users need to adequately provide comments on Web site pages.</p>
<sec id="section1-1050651911429920">
<title>Related Work</title>
<p>Although it may be important to give users the opportunity to provide feedback, they must be able to express that feedback. Users need some critical capacity to signal the problems they are experiencing with a Web site before they can report these problems. They need to be able to monitor their thinking processes and behavior. This critical skill is closely related to the concept of <italic>metacognition</italic>, which has been described as “one’s knowledge and beliefs about one’s own cognitive processes and one’s resulting attempts to regulate those cognitive processes to maximize learning and memory” (<xref ref-type="bibr" rid="bibr32-1050651911429920">Ormrod, 2006</xref>). Literature on metacognition and comprehension monitoring (e.g., <xref ref-type="bibr" rid="bibr1-1050651911429920">Baker, 1989</xref>) shows that readers with higher verbal abilities have greater awareness and control of their own cognitive activities while reading than do readers with lower verbal abilities. Readers with higher verbal abilities also appear to use a greater diversity of evaluation standards in their assessment of text quality.</p>
<p>Once users are able to signal their own problems with a Web page, the next step is that they attribute these problems to the Web site. The self-serving bias predicts that users tend to blame the Web site when they fail to reach their goals (<xref ref-type="bibr" rid="bibr25-1050651911429920">Moon, 2003</xref>; <xref ref-type="bibr" rid="bibr35-1050651911429920">Serenko, 2007</xref>). But several studies have shown that users sometimes blame themselves for problems they encounter. <xref ref-type="bibr" rid="bibr34-1050651911429920">Schriver (1997)</xref> concluded that users of manuals for complicated home electronics predominantly blamed themselves for their problems with the described products. In a follow-up study, <xref ref-type="bibr" rid="bibr19-1050651911429920">Jansen and Balijon (2002)</xref> found that a high percentage of the respondents (more than 50%) reported blaming themselves and not the manual when something went wrong. And <xref ref-type="bibr" rid="bibr35-1050651911429920">Serenko (2007)</xref>, who studied “interface agents” (software systems that support users on the computer), found that users may attribute their success to an interface agent and hold themselves responsible for task failure. Thus, users who experience problems and signal these problems may not attribute them to the Web site and therefore may not report them with the feedback option.</p>
<p>The final step in the feedback process is to submit the feedback by formulating the problem on a feedback form. Besides describing their feedback, users are often asked for additional information, such as the category of the comment or answers to scaled questions. Asking users to formulate feedback is not common. Some usability specialists, such as <xref ref-type="bibr" rid="bibr31-1050651911429920">Nielsen (2001)</xref> and <xref ref-type="bibr" rid="bibr38-1050651911429920">Spyridakis, Wei, Barrick, Cuddihy, and Maust (2005)</xref>, have argued that users’ self-reports may be unreliable. Others, such as <xref ref-type="bibr" rid="bibr33-1050651911429920">Sauro (2010)</xref>, considered users’ self-reports to be an effective evaluation method in addition to traditional user testing or heuristic evaluations. In certain contexts, user feedback might be very useful. <xref ref-type="bibr" rid="bibr27-1050651911429920">Nichols, McKay, and Twidale (2003)</xref> advocated empowering end users to proactively contribute to usability activities with end-user reporting tools. Karahasanovíc, Nyhamar Hinkel, Sjø´berg, and Thomas (2009) described a feedback-collection method that asked for written feedback at different times during an experiment. They compared this method with concurrent and retrospective think-aloud protocols and concluded that the feedback method revealed more difficulties than did the concurrent and retrospective think-aloud protocols and that the feedback method seemed better at identifying difficulties that prevented progress or caused significant delay. <xref ref-type="bibr" rid="bibr6-1050651911429920">Castillo, Hartson, and Hix (1998)</xref> found that users are able to identify and report their own critical incidents, but these studies involved highly educated participants. Earlier research on the evaluation of documents has shown that highly educated people provide more comments and more diverse feedback than do people who are less educated (<xref ref-type="bibr" rid="bibr10-1050651911429920">De Jong &amp; Schellens, 2001</xref>). If less educated or inexperienced users are indeed less able to formulate their feedback, user page reviews will probably fail to uncover some relevant problems.</p>
<p>All these steps in the feedback process require that users perform two tasks at the same time: finding and using the information while reviewing a Web page. Switching between these tasks is difficult for users because it requires a configuration of mental resources. <xref ref-type="bibr" rid="bibr24-1050651911429920">Monsell (2003)</xref> provided an overview of studies on task switching, which, for example, show that task switching leads to longer response times and a higher error rate. Users are inclined to concentrate on one task and to neglect other tasks, a phenomenon that is called “cognitive lockup” (<xref ref-type="bibr" rid="bibr26-1050651911429920">Neerincx, Lindenberg, &amp; Pemberton, 2001</xref>). Older people especially seem to have trouble switching between tasks (<xref ref-type="bibr" rid="bibr21-1050651911429920">Kramer, Hahn, &amp; Gopher, 1999</xref>). A study by <xref ref-type="bibr" rid="bibr2-1050651911429920">Barkas-Avila, Oberholzer, Schmutz, De Vito, and Opwis (2007)</xref> of error messages for users who fill out a form online shows practical consequences of problems with task switching. Their results show that error feedback should be provided only after users have completed the whole form because providing error feedback immediately negatively affects users’ performance. This study also underlines that trying to perform additional tasks while completing the primary task may cause users to expend too many additional cognitive resources, resulting in cognitive overload. This difficulty of having to switch between two different tasks is likely present in all user page reviews. The same difficulty has been reported in think-aloud usability testing in which users may stop verbalizing thoughts when they encounter difficulties during their task completion (<xref ref-type="bibr" rid="bibr3-1050651911429920">Boren &amp; Ramey, 2000</xref>). Also, verbalizing thoughts during task performance can lead to reactivity: Participants perform worse in concurrent think-aloud conditions than in conditions in which they think aloud afterward (<xref ref-type="bibr" rid="bibr12-1050651911429920">Elling, Lentz, &amp; De Jong, 2011</xref>; <xref ref-type="bibr" rid="bibr41-1050651911429920">Van den Haak, De Jong, &amp; Schellens, 2003</xref>, <xref ref-type="bibr" rid="bibr42-1050651911429920">2004</xref>).</p>
</sec>
<sec id="section2-1050651911429920">
<title>Research Questions</title>
<p>In this study, we focus on the extent to which users are able to productively formulate review comments on Web sites. We conducted the study in a controlled context in which users with various background characteristics provided feedback. Our research questions focused on (1) the number and characteristics of the users’ comments, (2) the consistency of the users’ comments in relation to the opinions they provided in a questionnaire, and (3) the consistency between the users’ comments.</p>
<sec id="section3-1050651911429920">
<title>What Are the Numbers and Characteristics of Users’ Comments?</title>
<p>Our first research question comprises several subquestions. We first looked at the number and types of user problems that our study generated:</p>
<p>1a. To what extent are participants able to provide comments on the relevant issues for informational Web sites (navigation, textual and visual content, and design)?</p>
<p>Then, we investigated whether differences in the participants’ backgrounds (age, education level) correlated with differences in the characteristics of their feedback:</p>
<p>1b. Are there any age- or education-related differences between participants’ user page reviews?</p>
<p>To get more detailed information about the characteristics of the comments, we looked at the participants’ descriptions of their comments, their use of the marking options included in the tool, and their categorization of the comments. Thus, we addressed the following three subquestions:</p>
<p>1c. How do participants formulate their comments on a Web site?</p>
<p>1d. To what extent do participants use the problem-marking function?</p>
<p>1e. How well are participants able to categorize their feedback?</p>
<p>The main purpose of Web site evaluation methods is to provide insight into the way users interact with Web sites and to detect and diagnose problems that they encounter. We examined, then, the extent to which participants’ comments pointed to usability problems on the Web site and the severity of these problems. In other words, we addressed the following subquestion:</p>
<p>1f. How useful is the feedback that the participants provide?</p>
</sec>
<sec id="section4-1050651911429920">
<title>Are Users’ Comments Consistent With the Opinions They Provided on the Questionnaire?</title>
<p>Our second research question focuses on users’ overall consistency in their judgments about a Web site. To address this question, we analyzed the correspondence between the participants’ user page review comments and the results of a questionnaire that we asked them to complete. We expected that users who reported many problems on the Web site would express stronger negative opinions in the questionnaire. Thus, we investigated the following question:</p>
<p>1. To what extent do participants’ comments on specific Web pages relate to their overall opinions about the Web site?</p>
</sec>
<sec id="section5-1050651911429920">
<title>Do Users Report the Same Problems?</title>
<p>User page reviews ask for local and open-ended feedback and can therefore generate very diverse user comments because all users have their own opinions about different aspects of the Web site. Moreover, an evaluation on a local level generates more specific user problems than does a global evaluation. These characteristics of user page reviews may make it difficult to realize a high overlap in comments between participants. Nevertheless, when all the users review the same pages and use the same scenarios, we can expect some consistency in the results. Our last research question, then, is this:</p>
<p>2. To what extent do participants report the same problems on a Web site?</p>
</sec>
</sec>
<sec id="section6-1050651911429920">
<title>Method</title>
<p>In this study, 93 participants provided feedback on three Web sites. Two Web sites were evaluated by 30 participants each, and one Web site was evaluated by 33 participants. We obtained our institutions’ approval to conduct human-subjects research. The participants, who received financial compensation for taking part in the study, were recruited by a specialized agency that has an extensive database of potential research participants. All the participants indicated that they use the Internet at least once a week. Men and women were almost equally distributed in the groups, with 43 men and 50 women participating in the study. Participants’ ages ranged from 18 to 71 (with an average age of 44). The participants were divided into four different age categories (18- 29, 30- 39, 40- 54, and 55 and older) and three education levels (low, medium, and high), based on the highest level of education that they had completed. Participants in the low-level educational group ranged from having an elementary school education to a junior general secondary education. Those in the medium-level educational group had an intermediate vocational education, a senior general secondary education, or a preuniversity education. Those in the high-level educational group had a higher vocational or university education. All groups were almost equally represented in the three Web site evaluations. Characteristics were mixed in such a way that, for example, all age groups consisted of nearly equal numbers of men and women of different educational levels. <xref ref-type="table" rid="table1-1050651911429920">Table 1</xref>
 provides an overview of the participants’ background characteristics.</p>
<table-wrap id="table1-1050651911429920" position="float">
<label>Table 1.</label>
<caption>
<p>Background Characteristics of the Participants in Each of the Three User Page Review Studies</p>
</caption>
<graphic alternate-form-of="table1-1050651911429920" xlink:href="10.1177_1050651911429920-table1.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Web site 1 (<italic>N</italic> = 30)</th>
<th>Web site 2 (<italic>N</italic> = 30)</th>
<th>Web site 3 (<italic>N</italic> = 33)</th>
<th>Total (<italic>N</italic> = 93)</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">Age</td>
</tr>
<tr>
<td> 18–29</td>
<td>6</td>
<td>5</td>
<td>6</td>
<td>17</td>
</tr>
<tr>
<td> 30-39</td>
<td>6</td>
<td>8</td>
<td>7</td>
<td>21</td>
</tr>
<tr>
<td> 40-54</td>
<td>9</td>
<td>10</td>
<td>11</td>
<td>30</td>
</tr>
<tr>
<td> 55 and older</td>
<td>9</td>
<td>7</td>
<td>9</td>
<td>25</td>
</tr>
<tr>
<td colspan="5">Gender</td>
</tr>
<tr>
<td> Male</td>
<td>15</td>
<td>14</td>
<td>14</td>
<td>43</td>
</tr>
<tr>
<td> Female</td>
<td>15</td>
<td>16</td>
<td>19</td>
<td>50</td>
</tr>
<tr>
<td colspan="5">Education</td>
</tr>
<tr>
<td> Low</td>
<td>7</td>
<td>11</td>
<td>8</td>
<td>26</td>
</tr>
<tr>
<td> Middle</td>
<td>12</td>
<td>7</td>
<td>13</td>
<td>32</td>
</tr>
<tr>
<td> High</td>
<td>11</td>
<td>12</td>
<td>12</td>
<td>35</td>
</tr>
</tbody>
</table>
</table-wrap>
<sec id="section7-1050651911429920">
<title>Web sites</title>
<p>The participants evaluated three Web sites of medium to large Dutch municipalities: <ext-link ext-link-type="uri" xlink:href="www.apeldoorn.nl">www.apeldoorn.nl</ext-link> (Web site 1), <ext-link ext-link-type="uri" xlink:href="www.dordrecht.nl">www.dordrecht.nl</ext-link> (Web site 2), and <ext-link ext-link-type="uri" xlink:href="www.nijmegen.nl">www.nijmegen.nl</ext-link> (Web site 3). Municipal Web sites provide information and services to citizens and to other interested users, such as tourists and businesses. These Web sites invariably contain a variety of information because they are designed to satisfy the informational needs of a broad target group. The three Web sites cover similar types of information, but this information is structured and presented in different ways.</p>
</sec>
<sec id="section8-1050651911429920">
<title>Scenario Tasks</title>
<p>The participants evaluated the Web sites based on two scenario tasks that they were given. The content of the tasks differed per Web site, but they resembled each other in difficulty and length of navigation path. Moreover, all the tasks (a) covered realistic activities that correspond to those that users usually perform on municipal Web sites; (b) included searching for information, as well as reading, understanding, and applying the relevant information to the described scenario; and (c) applied to different domains of the Web site. For example, a task pertaining to information on a government subsidy supporting people who are buying a house for the first time asked participants to search for answers to three questions: (1) What is the name of this subsidy? (2) Do you meet the requirements for this subsidy? and (3) What should you do to make a request for this subsidy? The shortest navigation path to this information consisted of six links, the last of which was a link to a pdf file containing information about all kinds of subsidies available for buying or renovating a house.</p>
</sec>
<sec id="section9-1050651911429920">
<title>Tool for User Page Reviews</title>
<p>In our study, we used the Infocus tool<sup>
<xref ref-type="bibr" rid="bibr1-1050651911429920">1</xref>
</sup> for gathering user feedback. This feedback tool is an extension of the plus–minus method (<xref ref-type="bibr" rid="bibr8-1050651911429920">De Jong, 1998</xref>; <xref ref-type="bibr" rid="bibr36-1050651911429920">Sienot, 1997</xref>) and Focus (<xref ref-type="bibr" rid="bibr9-1050651911429920">De Jong &amp; Lentz, 2001</xref>), both of which were developed to evaluate paper documents. With the Infocus tool, users can surf through a Web site and click on a comments button whenever they want to give positive or negative feedback. The tool enables participants to select and comment on any element on a Web screen, ranging from specific words, sentences, paragraphs, chapters, illustrations, and navigational aids to the entire Web page. After clicking the comments button, participants see a screen shot of the Web page. They have several options for marking the specific element that they want to comment on: They can point to it by drawing an arrow, underline it, or add a frame around it. On the left side of the screen, participants can choose one of the predefined comment categories and type in their feedback. Users’ feedback is saved in a database together with images and URLs of the marked Web screen and some log data such as the navigational path and a time outline.</p>
<p>Like many other tools for user page reviews, Infocus is able to ask users to categorize their feedback. In this study, we used four categories: (a) <italic>navigation</italic> (easy or difficult to find), (b) <italic>content</italic> (clear or unclear information), (c) <italic>design</italic> (does or does not look good), and (d) <italic>other</italic>. This categorization has three potential benefits. First, the availability of comment categories may remind participants of their reviewing task. The comment categories emphasize that we are asking participants to provide feedback on the Web site and not, for example, answers on tasks. Second, the specific categories chosen may guide the kinds of feedback participants provide. By explaining the categories and including them on the feedback screen, we are underlining which aspects of Web site quality we consider important to evaluate. Third, the comment categories may serve as clues to help us to interpret unclear comments when we analyze the data.</p>
</sec>
<sec id="section10-1050651911429920">
<title>Posttest Questionnaire</title>
<p>After the review process, participants filled out a questionnaire. The first part asked users for demographic information and about their experiences reviewing the Web site. The second part asked users for their opinions about the Web site. This Web Site Evaluation Questionnaire (WEQ), which we developed specifically to evaluate informational Web sites such as municipal Web sites, focuses on three dimensions: navigation, content, and design (<xref ref-type="bibr" rid="bibr11-1050651911429920">Elling, Lentz &amp; De Jong, 2007</xref>, <xref ref-type="bibr" rid="bibr13-1050651911429920">2012</xref>). The WEQ consists of 25 questions to which participants provide their responses on a 5-point Likert-type scale.</p>
</sec>
<sec id="section11-1050651911429920">
<title>Procedure</title>
<p>We conducted our study in a laboratory setting using Infocus. We chose this controlled environment in order to get more insight into the users’ abilities to provide feedback and to compare the results of different groups of users who reviewed the same pages of a Web site. The four evaluation sessions took place in a computer lab with workstations for small groups (seven to nine participants). Each session lasted 90 minutes. To reduce potential work tempo differences, we invited participants to work in rather homogeneous group settings. One group consisted of participants who were older than 55 years, and the other three groups were distinguished by participants’ level of education (low, medium, and high). We made this division because we expected that older and less educated participants would need more time to complete the evaluation.</p>
<p>To reduce the participants’ cognitive load, we split the tasks of navigating and reviewing. We had two solid reasons for doing so: First, research has shown that users are not good at combining different tasks. Second, in a pilot study, we observed that participants appeared to struggle with trying to carry out scenario tasks and review the Web site at the same time. They concentrated on finding the right answers to the scenario tasks and often forgot to provide feedback on the problems they encountered. Our observations are in line with “cognitive lockup,” the idea that users concentrate on one task and neglect other tasks (<xref ref-type="bibr" rid="bibr26-1050651911429920">Neerincx et al., 2001</xref>). In our pilot study, participants who had trouble finding or understanding the information, in particular, seemed to have no cognitive energy left to comment on their experiences. To ensure that participants could use all their cognitive energy in reviewing the Web site, we prioritized the reviewer role and reduced the requirements for participants to perform tasks on the Web site. Of course, this somewhat artificial sequential procedure deviates from the online way of reviewing, but it enables all the users to provide feedback and answer our main research question about users’ abilities to provide feedback on a Web site.</p>
<p>The review session started with a brief introduction in which participants were told that we investigated municipal Web sites to improve them and that we needed their help to find the problems that users might experience. Then, we illustrated the functions of Infocus and explained the four feedback categories (navigation, content, design, and other). The facilitator described how to formulate a comment, mark something on a screen shot, choose a feedback category, and save the comment while the participants followed each step on their screens. Participants were then given the opportunity to practice on a tourist Web site and to write and save some comments. After this practice session, the facilitator asked participants to open the Web site to be evaluated and to surf freely on the Web site for 10 minutes while providing comments with Infocus. After this, the guided review procedure started in which participants received two task scenarios that reflected the kinds of tasks users perform on municipal Web sites. After giving them the task scenarios, the facilitator asked them to open the home page of the Web site and decide for themselves which link they would choose on the home page given the first task description. The facilitator then showed the group the link that led to the required information and asked the participants to give feedback about the home page. Participants formulated their comments individually on their computers.</p>
<p>When everyone had completed their feedback, the participants were asked to click on the link and to look at the new page to decide what the next step should be. Then, the facilitator again showed the link that led to the information and asked each of them to provide feedback on this page. If there were alternative options to reach the information, the facilitator explained these options before continuing on one of the possible routes. When the participants reached the final information section, they used that section to think about the answer to the main question in the task description though they were not required to write down the answer. After a while, the facilitator gave them the answer to the scenario question and asked them to provide feedback on that page. The participants were allowed 5 minutes to complete each step of the task, including searching, thinking, and formulating a comment. When they had finished the task, the participants were asked to provide feedback on the whole process of task completion. After finishing both tasks, the participants filled out the questionnaire.</p>
</sec>
<sec id="section12-1050651911429920">
<title>Analysis</title>
<p>To answer our research subquestion 1a, we determined the number of comments made per participant, the proportion of positive versus negative comments, and the number of unclear comments. We also categorized the comments using the four feedback categories (navigation, content, design, and other). The first author and an independent rater coded all the comments, achieving a satisfactory Cohen’s κ (.81).</p>
<p>Next, we analyzed the negative feedback. As <xref ref-type="bibr" rid="bibr18-1050651911429920">Hornbæk (2010)</xref> argued, the matching of comments into a list of user problems is not straightforward and can be done in different ways. In our matching procedure, we used a four-step analysis. First, we gathered the comments that were made on the same page; second, we divided the feedback into comments on different parts of the page (menu, illustration, text block, etc.); third, we analyzed these groups of comments using the categories; and fourth, we analyzed these comments using the participants’ exact description. The description of the cause of the problem was decisive for merging two comments into one user problem. If, for example, two comments were about the bad legibility of the text, we would consider them to be two different problems if one comment criticized the small font and the other the color of the text. This rather strict and fine-grained method of matching resulted in a large list of different user problems with relatively little overlap.</p>
<p>To answer subquestion 1b about the influence of user characteristics such as age and education level, we analyzed the comments for differences between the four age categories and the three education levels using analysis of variance. We used four dependent variables: overall number of comments, number of positive comments, number of negative comments, and number of unclear comments. To answer subquestions 1c (about formulation), 1d (about marking), and 1e (about feedback categorization), we used a detailed analysis of the comments as they were collected in the database. We qualitatively analyzed the ways in which participants formulated their comments. Then we analyzed the degree to which participants used the various marking options and determined the quality of their categorizations by comparing the participants’ categorizations with those of the two expert coders.</p>
<p>To answer subquestions 1f, concerning the severity of the problems, two independent experts rated the severity of the comments on the two Web pages with most comments of each of the three Web sites. In total, they rated 88 unique comments (24% of the whole set of 373), 18 comments from Web site 1, 26 from Web site 2, and 44 from Web site 3. Many studies have shown that experts are not good at rating the seriousness of problems and that their severity judgments are highly personal with little agreement between different experts who rate the same set of problems (e.g., <xref ref-type="bibr" rid="bibr15-1050651911429920">Hassenzahl, 2000</xref>; <xref ref-type="bibr" rid="bibr16-1050651911429920">Hertzum &amp; Jacobsen, 2003</xref>; <xref ref-type="bibr" rid="bibr17-1050651911429920">Hertzum, Jacobsen, &amp; Molich, 2002</xref>; <xref ref-type="bibr" rid="bibr23-1050651911429920">Molich &amp; Dumas, 2008</xref>; <xref ref-type="bibr" rid="bibr30-1050651911429920">Nielsen, 1995</xref>; <xref ref-type="bibr" rid="bibr40-1050651911429920">Uldall-Espersen, Frøkjær, &amp; Hornbæk, 2008</xref>). We should therefore interpret the experts’ severity ratings with caution. We asked the experts to determine the seriousness of the problems described in the comments, looking at their impact on the users’ task performance. Following the user-focused part of the rating that <xref ref-type="bibr" rid="bibr40-1050651911429920">Uldall-Espersen, Frøkjær, and Hornbæk (2008)</xref> used, we asked the experts to rate the problems according to three severity levels: level 1, <italic>cosmetic</italic> comments about problems that do not interfere with users’ task performance; level 2, <italic>critical</italic> comments that point to problems that annoy users and can in any way disturb their task performance; and level 3, <italic>catastrophic</italic> comments that may cause users to give up their task. The experts received information about the tasks the participants performed, and they had screen shots of the pages to which the comments referred. After the experts rated the comments on the first Web site, we discussed the comments that were rated differently by both experts and strengthened the agreement about the interpretation of the three severity levels for rating the remaining comments. Then the experts independently rated the comments on the other two Web sites. The Cohen’s κ score of the two independent experts was .66, indicating that they had substantial agreement (76% of the comments were rated identically by both experts). The experts discussed which severity score to choose for those comments that they rated differently.</p>
<p>To answer question 2, concerning the relationship between the participants’ page-review comments and their overall opinions about the Web site, we analyzed the correlation between the number of negative comments per dimension (i.e., navigation, content, and design) and the participants’ score on the corresponding dimension in the WEQ. For this analysis, we used the experts’ categorization because this categorization is most adequately related to the questionnaire. The reliability of the participants’ WEQ scores was measured using a Cronbach’s α. The navigation dimension included 13 items with an α of .93, the content dimension had 9 items with an α of .82, and the design dimension included 3 items with an α of .88.</p>
<p>To answer research question 3, concerning the consistency between users’ comments, we first analyzed the relationship between the sample size and the number of problems detected. This relationship has been investigated for several other evaluation methods, in particular, heuristic evaluation and think-aloud usability testing. Some of these previous studies reported the encouraging outcome that five or six participants would suffice to detect the majority of the problems (<xref ref-type="bibr" rid="bibr28-1050651911429920">Nielsen, 1994a</xref>, <xref ref-type="bibr" rid="bibr29-1050651911429920">1994b</xref>; <xref ref-type="bibr" rid="bibr44-1050651911429920">Virzi, 1992</xref>) whereas other studies showed that larger samples were needed to obtain more or less exhaustive results (<xref ref-type="bibr" rid="bibr14-1050651911429920">Faulkner, 2003</xref>; <xref ref-type="bibr" rid="bibr22-1050651911429920">Lewis, 1994</xref>; <xref ref-type="bibr" rid="bibr37-1050651911429920">Spool &amp; Schroeder, 2001</xref>). Of course, this relationship varies depending on the type of evaluation method. User page reviews are likely to generate many different comments, making it difficult, if not impossible, to find a complete set of problems. For this analysis, we used a Monte Carlo procedure (<xref ref-type="bibr" rid="bibr22-1050651911429920">Lewis, 1994</xref>; <xref ref-type="bibr" rid="bibr28-1050651911429920">Nielsen, 1994a</xref>; <xref ref-type="bibr" rid="bibr44-1050651911429920">Virzi, 1992</xref>). For each sample size, ranging from 1 to <italic>N</italic>-1, we took 1,000 random subsamples (with replacements) and computed the mean number of different problems detected. To facilitate our comparison between the three Web sites, we expressed the results in terms of percentages of the total number of problems detected by a sample of 30 participants. To further explore the stability of the results, we used an adaptation of the Monte Carlo procedure. For all possible sample sizes (ranging from 1 to <italic>N</italic>-2), we randomly selected 1,000 sets of two independent subsamples (with replacements). We computed the percentage of agreement between two subsamples by dividing the number of shared problems in both subsamples by the total list of problems in either subsample.</p>
</sec>
</sec>
<sec id="section13-1050651911429920">
<title>Results</title>
<p>The review sessions took place in a positive atmosphere. Participants seemed to feel motivated to evaluate the Web site. As we expected, the sessions with participants who were 55 years of age and older took more time because these older participants needed more help and instructions about how to make comments than did younger participants. Contrary to our expectations, less educated participants sometimes indicated that the sessions took too much time: They did not need all the time given to make comments because “everything was clear” to them. Overall, the participants judged their experiences and their review abilities positively. They reported that the review task was easy to do (mean score on a 5-point scale: 4.03, <italic>SD</italic> = 0.80) and that they enjoyed the process of giving comments on Web pages (mean score: 4.15, <italic>SD</italic> = 0.82).</p>
<sec id="section14-1050651911429920">
<title>Number and Characteristics of Users’ Comments</title>
<p>To answer our first research question we addressed subquestions on the extent to which the participants were able to provide comments on relevant issues, whether there were age- or education-related differences between participants’ page reviews, how participants formulated their comments on a web page, the extent to which they used the marking function, how well they categorized their feedback, and how useful their feedback was.</p>
</sec>
<sec id="section15-1050651911429920">
<title>To What Extent Are Participants Able to Provide Comments on the Relevant Issues for Informational Web sites?</title>
<p>On average, each of the participants provided 16.1 comments during a session (<italic>SD</italic> = 6.1). Of these comments, an average of 0.8 (<italic>SD</italic> = 1.3) were judged to be unclear (5%). Of the clear comments, an average of 9.5 (<italic>SD</italic> = 5.6) were negative (62%). If we consider the negative comments to be the core of the evaluation, the net result of the user page-review evaluation is 9.5 potential user problems per participant. Because many of the negative comments may point to the same potential user problem, we also looked at the number of different potential problems per Web site. This analysis resulted in, respectively, 107, 110, and 156 potential user problems for the three Web sites.</p>
<p>
<xref ref-type="table" rid="table2-1050651911429920">Table 2</xref>
 gives an overview of the categories of the total set of negative comments on the three municipal Web sites (based on the two coders’ categorization). Navigation appears to be an important aspect of the participants’ feedback, but many comments were also made about content and design. Whereas the proportion of navigation-related comments was nearly stable across the three Web sites, the proportion of content and design comments varied between the three sites, with relatively more comments about content in Web sites 1 and 3 and design in Web site 2. Web site 2, for example, had a lot of comments on the inconvenient arrangement of the home page and on poor legibility due to the size and color of the fonts. The other Web sites had more content comments, such as long texts with complicated and vaguely formulated information. The category <italic>other</italic> included feedback on technical problems (e.g., a picture that did not appear correctly), redundant information (e.g., an item on the home page with detailed information about clearing away leaves in autumn), or missing functionalities (e.g., a form that was not available online).</p>
<table-wrap id="table2-1050651911429920" position="float">
<label>Table 2.</label>
<caption>
<p>Number of Negative Comments per Category for Each of the Three Web sites</p>
</caption>
<graphic alternate-form-of="table2-1050651911429920" xlink:href="10.1177_1050651911429920-table2.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Web site 1</th>
<th>Web site 2</th>
<th>Web site 3</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Navigation</td>
<td>132 (43%)</td>
<td>132 (45%)</td>
<td>127 (46%)</td>
<td>391 (44%)</td>
</tr>
<tr>
<td>Content</td>
<td>116 (38%)</td>
<td>57 (19%)</td>
<td>91 (33%)</td>
<td>264 (30%)</td>
</tr>
<tr>
<td>Design</td>
<td>30 (10%)</td>
<td>92 (31%)</td>
<td>37 (13%)</td>
<td>159 (18%)</td>
</tr>
<tr>
<td>Other</td>
<td>27 (9%)</td>
<td>15 (5%)</td>
<td>24 (9%)</td>
<td>66 (8%)</td>
</tr>
<tr>
<td>Total</td>
<td>305</td>
<td>296</td>
<td>279</td>
<td>880</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In all, we can conclude that users were able to report many comments and that they succeeded in addressing navigation, content, and design issues.</p>
</sec>
<sec id="section16-1050651911429920">
<title>Are There Any Age- or Education-Related Differences Between Participants’ User Page Reviews?</title>
<p>Differences between the four age categories and the three education levels were analyzed in regard to four dependent variables: total number of comments, number of positive comments, number of negative comments, and number of unclear comments. The results are shown in <xref ref-type="table" rid="table3-1050651911429920">Tables 3</xref> and <xref ref-type="table" rid="table4-1050651911429920">4</xref>

. Regarding the total number of comments made, we found no significant differences between the four age groups and between the three education levels, for age: <italic>F</italic>(3, 89) = .443, <italic>p</italic> = .723; for educational level: <italic>F</italic>(2, 90) = 2.839, <italic>p</italic> = .064. Also, no significant interaction effect was found. The same applied to the number of positive comments, for age: <italic>F</italic>(3, 89) = .484, <italic>p</italic> = .694; for educational level: <italic>F</italic>(2, 90) = .372, <italic>p</italic> = .690. For the other two dependent variables, however, significant education-related differences were found, for the number of negative comments: <italic>F</italic>(2, 90) = 6.818, <italic>p</italic> &lt; .01; for the number of unclear comments: <italic>F</italic>(2, 90) = 5.725, <italic>p</italic> &lt; .01. Post hoc analyses showed that participants with a high education level produced more negative comments than did participants with a low or medium education level and that the participants with a higher level of education produced fewer unclear comments than did participants with a lower level of education.</p>
<table-wrap id="table3-1050651911429920" position="float">
<label>Table 3.</label>
<caption>
<p>Mean Number of All Comments, Positive Comments, Negative Comments, and Unclear Comments for the Three Educational Levels (SD)</p>
</caption>
<graphic alternate-form-of="table3-1050651911429920" xlink:href="10.1177_1050651911429920-table3.tif"/>
<table>
<thead>
<tr>
<th>Education Level</th>
<th>Low</th>
<th>Middle</th>
<th>High</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>All comments</td>
<td>15.0 (3.9)</td>
<td>15.0 (4.9)</td>
<td>18.0 (7.9)</td>
<td>16.1 (6.1)</td>
</tr>
<tr>
<td>Positive comments</td>
<td>6.5 (4.1)</td>
<td>5.5 (4.2)</td>
<td>5.8 (4.1)</td>
<td>5.9 (4.1)</td>
</tr>
<tr>
<td>Negative comments*</td>
<td>7.1 (3.2)</td>
<td>8.7 (4.8)</td>
<td>11.9 (6.7)</td>
<td>9.5 (5.6)</td>
</tr>
<tr>
<td>Unclear comments*</td>
<td>1.4 (1.9)</td>
<td>0.8 (1.1)</td>
<td>0.3 (0.5)</td>
<td>0.8 (1.3)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1050651911429920">
<p>* <italic>p</italic> &lt; .05.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table4-1050651911429920" position="float">
<label>Table 4.</label>
<caption>
<p>Mean Number of All Comments, Positive Comments, Negative Comments, and Unclear Comments for the Four Age Groups (SD)</p>
</caption>
<graphic alternate-form-of="table4-1050651911429920" xlink:href="10.1177_1050651911429920-table4.tif"/>
<table>
<thead>
<tr>
<th>Age</th>
<th>18-29</th>
<th>30-39</th>
<th>40-54</th>
<th>&gt;55</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>All comments</td>
<td>15.9 (5.5)</td>
<td>17.0 (5.9)</td>
<td>16.6 (6.7)</td>
<td>15.0 (6.0)</td>
<td>16.1 (6.1)</td>
</tr>
<tr>
<td>Positive comments</td>
<td>5.2 (3.9)</td>
<td>6.5 (4.8)</td>
<td>6.2 (3.9)</td>
<td>5.4 (4.2)</td>
<td>5.9 (4.1)</td>
</tr>
<tr>
<td>Negative comments</td>
<td>10.2 (5.6)</td>
<td>10.1 (5.2)</td>
<td>9.1 (6.3)</td>
<td>8.8 (5.4)</td>
<td>9.5 (5.6)</td>
</tr>
<tr>
<td>Unclear comments</td>
<td>0.5 (1.2)</td>
<td>0.3 (0.6)</td>
<td>1.2 (1.4)</td>
<td>0.8 (1.6)</td>
<td>0.8 (1.3)</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In all, the user page review appeared to be usable for all age groups and all education levels included in our study. When we focus on quality of feedback, however, higher educated participants were better able to provide feedback, producing more negative comments (which could serve as clues to improve the Web site) and fewer unclear comments.</p>
</sec>
<sec id="section17-1050651911429920">
<title>How Do Participants Formulate Their Comments on a Web site?</title>
<p>Looking at the participants’ comments in more detail, we made some interesting observations about the content of these comments. First, participants differed in the ways they formulated their comments, which could be written in the form of reporting a problem’s cause, effect, or solution. For example, on one of the Web pages, participants had difficulty navigating back to the home page due to the small size of the home icon. Several participants reported this problem by describing its effect (“I can’t find my way back to the home page”). Others reported its cause (“The icon of the homepage is too small and inconspicuous”) or a possible solution (“This icon should get more attention”). Individual participants sometimes reported both the cause and the effect and occasionally a solution. These differences in style of reporting sometimes made it difficult to categorize the comments. In the preceding example, the cause of the reported problem was a design issue, but the effect concerns navigation: not finding the way back to the home page. The experts systematically categorized on the cause of the problem unless the cause was not mentioned by the participant.</p>
<p>Second, the participants sometimes used the comment button to provide an answer on the scenario task. Participants who used the feedback option to comment on the scenario or to formulate answers to scenario questions did not understand, or forgot, the reviewing task at hand. We did not find differences in the number of scenario-related comments between the education levels, <italic>F</italic>(2, 90) = 2.215, <italic>p</italic> = .115). On average, participants in all the groups entered between 0.4 and 1.0 scenario-related comment. There was, however, a significant difference between the age groups, <italic>F</italic>(3, 89) = 2.794, <italic>p</italic> &lt; .05: Older participants made significantly more scenario-related comments (1.0, <italic>SD</italic> = 1.4) than did younger participants (0.29, <italic>SD</italic> = 0.7). Older participants perhaps experience more trouble understanding the task of reviewing a Web site and separating this task from searching for information and answering scenario questions.</p>
<p>Third, some participants repeated the same comment several times. For example, one participant reported that the Web site did not fill the whole screen. He repeated this comment in nearly the same wording on several Web pages. Perhaps, the structure of the guided review procedure pressures participants to comment even when they are uncertain of what to say. Repeating the same comment could be a reaction to this pressure. Other participants wrote down a positive remark (e.g., “This is clear to me”) when they did not see any problem.</p>
<p>Fourth, participants sometimes made more than one comment in one feedback screen. In the instructions, we asked participants to make a new annotation for every comment they wanted to report. Nevertheless, they sometimes combined two comments, often a positive and a negative statement (e.g., “The home page looks up to date, but it is difficult to find what you look for on this page”). In doing so, participants may have been trying to soften their negative feedback by saying something nice about the Web page as well. Giving negative feedback can be seen as a “face threatening act” (<xref ref-type="bibr" rid="bibr4-1050651911429920">Brown &amp; Levinson, 1987</xref>) that people want to compensate for by showing appreciation and respect.</p>
<p>Finally, the older participants often referred to using another medium in their feedback (e.g., “I would have used the telephone by now, or It is a lot easier to go and ask for the information”). They seemed to feel less familiar with the Internet than the younger participants did and, as a result, seemed to prefer other media, such as the telephone or face-to-face contact. This observation corresponds with the research finding that people need time to get accustomed to new media (<xref ref-type="bibr" rid="bibr43-1050651911429920">Van Dijk, Pieterson, Van Deursen, &amp; Ebbers, 2007</xref>).</p>
</sec>
<sec id="section18-1050651911429920">
<title>To What Extent Do Participants Use the Problem-Marking Function?</title>
<p>The marking function was introduced to help users adequately indicate the object of their feedback. A verbal description of the location of an object on the screen is far less specific and effective than a picture of the specific page with, for example, an arrow pointing at that object. Of all the participants’ negative comments, 51% were accompanied by some form of marking—a reasonably good result, considering that most of the participants were introduced to this functionality for the first time. So participants seem to be able to mark the object of their feedback, which makes this marking functionality a useful innovation for the user page reviews.</p>
<p>Of course, for some comments, the absence of a marking could be explained by the nature of the comment. In three situations, marking an object was impossible. First, some comments were about the whole Web page (e.g., It’s strange that the design of this page deviates from the design of the <italic>other</italic> parts of the Web site). Second, some comments were about items that were missing on the page or that could not be found (e.g., I don’t know what link I should choose here). And third, some comments did not even pertain to any object on the Web page (e.g., I had to click too much to reach this information).</p>
</sec>
<sec id="section19-1050651911429920">
<title>How Well Are Participants Able to Categorize Their Feedback?</title>
<p>Participants categorized all their feedback into the four categories (i.e., content, navigation, design, and other). Also, one of the authors and an independent coder categorized all the participants’ feedback. This categorization resulted in a Cohen’s κ of .81, indicating a high agreement between the two expert coders. But the agreement between the participants and the expert coders was considerably lower, with a κ of .26. The expert coders categorized 51% of the comments differently than the participants had.</p>
<p>Content was the most problematic category. Participants tended to put not only comments about content in this category but also comments about unclear labels for links and other navigational problems. The expert coders transferred as many as 191 of these comments into the navigation category. These differences in coding choices between the participants and the experts do not mean that the participants did not thoughtfully categorize their problems. The categories had rather open and broad formulations, so many different comments fit into these categories. This result raises questions about users’ ability to categorize their feedback. In our study, we explained the categories to the participants, providing examples, and they could ask questions if they did not understand the categories. In an online remote setting, users would not have these opportunities, which would probably result in users’ having even more difficulty categorizing comments than what they would have in a laboratory setting.</p>
</sec>
<sec id="section20-1050651911429920">
<title>How Useful Is the Feedback That the Participants Provide?</title>
<p>A sample of 88 negative comments were rated on severity by two independent experts. Of these comments, 21 (24%) were rated as cosmetic problems, 48 comments (55%) were rated as critical problems, and 19 comments (22%) were rated as catastrophic. In other words, 77% of the negative comments pointed to problems that disturbed participants or even prevented them from reaching their goals on the Web site. Examples of cosmetic comments are “This home page is boring,” and “The design of this page is not inviting.” Examples of critical comments are “It is not easy to find the relevant link on this page,” and “The navigation path to the information is too laborious.” Examples of catastrophic comments are “The information about school holidays cannot be found because of the misleading link label,” and “Crucial information about the procedure is missing in this text.” Thus, participants were able to provide useful feedback, with the majority of their comments referring to critical or catastrophic usability problems on the Web site that were related to the scenario tasks.</p>
</sec>
<sec id="section21-1050651911429920">
<title>To What Extent Do Participants’ Comments on Specific Web Pages Relate to Their Overall Opinions About the Web site?</title>
<p>If participants report many problems in a certain category, we would expect that the overall score for the corresponding dimension in the WEQ would be low. <xref ref-type="table" rid="table5-1050651911429920">Table 5</xref>
 displays the Pearson correlations between the number of problems detected in three Infocus categories (we did not include the category <italic>other</italic>) and the scores for the corresponding WEQ dimensions. On all three aspects, we found significant but weak negative correlations between the number of negative comments and the scores in the questionnaire: The more negative comments that participants made in a category, the lower the scores for the corresponding WEQ dimension. Although the numbers of content and design comments correlated significantly with their corresponding WEQ dimensions, the number of navigation comments correlated significantly with each of the three WEQ dimensions. This finding may be attributed to the fact that navigation problems may be caused by unclear link names or visual cues. In all, these correlations provide some initial support for the consistency of the judgments underlying the user page review comments. The two evaluation methods show similar tendencies in the participants’ opinions about the quality of the Web site. The participants’ detailed comments in the user page review corresponded to their more global evaluations about using the Web site. This result means that participants were able to provide stable feedback.</p>
<table-wrap id="table5-1050651911429920" position="float">
<label>Table 5.</label>
<caption>
<p>Correlations Between the Number of Problems Detected in Three Infocus Categories and the Overall Scores for the WEQ Dimensions</p>
</caption>
<graphic alternate-form-of="table5-1050651911429920" xlink:href="10.1177_1050651911429920-table5.tif"/>
<table>
<thead>
<tr>
<th rowspan="2">Infocus Category</th>
<th>WEQ Score</th>
<th>WEQ Score</th>
<th>WEQ Score</th>
</tr>
<tr>
<th>Navigation</th>
<th>Content</th>
<th>Design</th>
</tr>
</thead>
<tbody>
<tr>
<td>Navigation</td>
<td>−.34**</td>
<td>−.27**</td>
<td>−.23*</td>
</tr>
<tr>
<td>Content</td>
<td>−.15</td>
<td>−.25*</td>
<td>−.18</td>
</tr>
<tr>
<td>Design</td>
<td>−.03</td>
<td>−.14</td>
<td>−.35**</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1050651911429920">
<p>
<italic>Note</italic>. WEQ = Web site Evaluation Questionnaire.</p>
</fn>
<fn id="table-fn3-1050651911429920">
<p>*<italic>p</italic> &lt; .05, **<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section22-1050651911429920">
<title>To What Extent Do Participants Report the Same Problems on a Web site?</title>
<p>Using a Monte Carlo analysis, we explored the relationship between the sample size and the exhaustiveness of the list of user problems. <xref ref-type="fig" rid="fig5-1050651911429920">Figure 5</xref>
 displays a graph of this relationship. The graph represents the mean percentage of problems detected (of the total number of problems detected for that site by 30 participants) for sample sizes ranging from 1 to 30. All three Web sites show a remarkable similarity. The patterns show little dependence on the particular characteristics of the Web site evaluated. None of the three patterns shows a clear curve toward a finite set of comments. Based on the graph, then, we must conclude that additional participants would likely detect new problems. Our analysis of the agreement between two independent samples shows that two samples of 15 participants have an agreement between 35% and 48%. To obtain results with at least 60% agreement between two samples, a sample size of between 20 and 25 participants would be needed. In sum, to a certain extent, the participants were able to point to the same problems, but they also showed a clear diversity in the comments they reported.</p>
<fig id="fig5-1050651911429920" position="float">
<caption>
<p>Relationship between the number of participants and the percentage of problems detected in the three municipal Web sites (<italic>N</italic> = 30).</p>
</caption>
<graphic xlink:href="10.1177_1050651911429920-fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="section23-1050651911429920">
<title>Discussion</title>
<p>In this article, we have argued that it is important to enable users to give feedback on specific pages of Web sites. We introduced user page reviews (e.g., Opinionlab, Usabilla, Kampyle, and Infocus) as a category of remote evaluation methods. In our study, we examined the extent to which users are capable of giving adequate written feedback in open-ended comments, focusing on the numbers and characteristics of users’ comments, the consistency of users’ feedback and their questionnaire opinions, and the correspondence between users’ comments.</p>
<sec id="section24-1050651911429920">
<title>Numbers and Characteristics of Users’ Comments</title>
<p>The results of our study are encouraging. Participants reported that they liked to produce the user page reviews, and they did not have any substantial problems with providing feedback. Overall, the number of unclear comments was small (5%). Highly educated participants produced more adequate feedback (in terms of both potential clues for revision of the Web site and clarity) than did less educated participants. This finding is in line with research that shows that higher educated people are better able to monitor their own cognitive activities (<xref ref-type="bibr" rid="bibr1-1050651911429920">Baker, 1989</xref>) and provide more comments and more diverse feedback (<xref ref-type="bibr" rid="bibr10-1050651911429920">De Jong &amp; Schellens, 2001</xref>). This result also corresponds with studies by <xref ref-type="bibr" rid="bibr20-1050651911429920">Karahasanovíc et al. (2009)</xref> and Castillo et al. (1998) that reported positive findings on providing feedback by higher educated users. In our study, we also found that older participants and participants with lower education levels were capable of providing feedback on relevant dimensions.</p>
<p>The participants provided substantial feedback on the content and design of these informational Web sites. Additionally, they detected many problems that involved navigational issues. The large number of navigation comments may have been a result of the step-by-step procedure we used, which could have triggered attention to the quality of specific links. But navigation may just generally be an important aspect of users’ perception of Web site quality. Future research combining user page reviews with free surfing tasks might clarify this issue.</p>
<p>The marking function was used in 51% of the comments, so many participants were able to mark the object of their comments and thus easily identify the cause of their problem. But not all the comments could be related to a specific object on the page. And some participants perhaps did not mark the objects of their comments because they needed to get used to a new functionality before they could optimally use it. That may especially have been the case with our sample that included many older and less educated people. In an unpublished follow-up evaluation study that we conducted with younger, more highly educated users, the marking function was used substantially more often.</p>
<p>In both our pilot study and our main study, participants had difficulty categorizing their own feedback. This corresponds with <xref ref-type="bibr" rid="bibr5-1050651911429920">Bruun, Gull, Hofmeister, and Stage’s finding(2009)</xref> that users had trouble categorizing the severity of their comments. These findings raise the question of whether it is feasible to ask lay people to categorize their comments. They may not be able to reflect on categories when they formulate feedback because the combined task of formulating and categorizing feedback—in addition to performing tasks on the Web site—may demand too much cognitive energy. Despite this difficulty, there are potential benefits to categorization, as we described earlier. First, the categories prompted the participants to perform the role of reviewer although the effectiveness of this prompt is difficult to determine. Second, the categories guided participants to the kind of feedback we desired. Most of the comments did indeed fit into the three categories of content, navigation, and design. But the third function, guiding the evaluator in the interpretation of the comments, was not achieved. The categories chosen by participants did not help us to interpret unclear comments. Moreover, in more than 50% of the comments, we did not accept the category that the participant had chosen because it did not match our definition of the category. This finding should be a warning for all the user page-review tools that ask users to categorize their comments. These user categorizations should be considered carefully. More research is needed on users’ ability to categorize their comments and on the types of categories that are best to use.</p>
<p>The severity rating of a selection of the reported problems shows that participants were able to provide useful feedback. Most (77%) of the negative comments pointed to problems that might disturb or prevent users’ adequate task performance. The expert rating provides a first indication of the usefulness of the comments that are reported in a user page review. Future studies could compare other evaluation approaches, such as think-aloud usability testing, to get more insight into the extent to which the comments correspond with the problems users experience when performing tasks.</p>
</sec>
<sec id="section25-1050651911429920">
<title>Consistency of Users’ Feedback and Opinions</title>
<p>The correlations we found between the participants’ feedback and their overall scores on the questionnaire indicate that the detailed comments participants provided reflected their general attitude toward the Web site. Thus, participants were able to provide feedback that reflected their opinions.</p>
</sec>
<sec id="section26-1050651911429920">
<title>Correspondence Between Users’ Comments</title>
<p>The Monte Carlo analysis showed that participants were somewhat able to point to the same problems. Due to the enormous diversity of problems that participants may mention, relatively large sample sizes may be necessary to collect a stable and more or less exhaustive list of problems, in line with studies by <xref ref-type="bibr" rid="bibr22-1050651911429920">Lewis (1994)</xref>, <xref ref-type="bibr" rid="bibr37-1050651911429920">Spool and Schroeder (2001)</xref>, and Faulkner (2003). Because the user page review asks for open comments on a local level, we could expect that this feature would result in a broad range of different comments. And our rather fine-grained way of matching increased this effect because we were cautious in considering two comments as referring to the same problem.</p>
</sec>
</sec>
<sec id="section27-1050651911429920">
<title>Conclusions and Future Research</title>
<p>This study shows that users are able to provide useful feedback in situations in which they can devote themselves entirely to the review task. However, a possible limitation of this study is that it shows only one way of evaluating a Web site with the user page review method: a step-by-step laboratory procedure with the <italic>Infocus</italic> tool. We chose this procedure because research shows that it is difficult for users to conduct tasks and provide feedback at the same time (<xref ref-type="bibr" rid="bibr24-1050651911429920">Monsell, 2003</xref>; <xref ref-type="bibr" rid="bibr26-1050651911429920">Neerincx et al., 2001</xref>). Older and less educated users, in particular, may have trouble switching between task processing and reviewing (<xref ref-type="bibr" rid="bibr21-1050651911429920">Kramer et al., 1999</xref>). In our procedure, then, we chose to reduce the participants’ cognitive load by allowing them to focus on one task: reporting problems with the feedback option. In this way, we could optimally study the abilities of users to provide feedback. But the results of our study cannot be generalized to contexts in which users conduct more than one task. Further research is needed about producing feedback under different extents of cognitive load.</p>
<p>Also, the evaluation in this study does not resemble evaluations that are done in practice. Our goal was to study users’ abilities to provide feedback and not the evaluation method as a whole. Future research should test the user page reviews in more natural circumstances and compare the outcomes of the evaluation to those of observational methods, such as the think-aloud method, in order to obtain more information about the merits and restrictions of user page reviews.</p>
<p>The literature about problems with task switching raises questions about the online feedback tools. To what extent are online users able to provide adequate feedback? What are the characteristics of the users who provide feedback with online tools? Perhaps especially highly educated and experienced users are triggered to provide their feedback while other users need all their cognitive energy just to realize their primary goals on the Web site. It would be useful, then, to have more knowledge about the personal characteristics of the online respondents. Also, comparisons should be made between the feedback that users give with the online tools and in the laboratory: To what extent do these comments correspond with each other? We plan to address these questions in future research.</p>
<p>In this study, we have gained more knowledge about users’ abilities to provide feedback on Web site pages in a controlled context. Although our study showed that most users have the ability to provide feedback, evaluators in an online context should carefully interpret the results of the user page-review tools. Less educated users have more trouble reporting their problems and are possibly less inclined to share their feedback. Tools should have a clear and uncomplicated design. Also, to stimulate users to share their feedback, these tools need to be eye-catching, attractive, fast, and easy to use. Extra options, such as a categorization, scales, or ratings, should be used with reserve.</p>
</sec>
</body>
<back>
<notes>
<fn-group>
<title>Note</title>
<fn fn-type="other" id="fn4-1050651911429920">
<p>The Infocus tool is available free of charge. Interested people can contact S. Elling for more information.</p>
</fn>
</fn-group>
</notes>
<ack>
<title>Acknowledgments</title>
<p>This article is based on a research project financed by the Dutch Organization for Scientific Research (NWO). It is part of the research program Evaluation of Municipal Web sites. The authors would like to thank master’s students Floris Baan and Philip Henssen, who contributed to the studies in this article. The authors would also like to thank the anonymous reviewers for their valuable comments on earlier versions of this article.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn5-1050651911429920">
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn6-1050651911429920">
<p>The authors disclosed receipt of the financial support for the research, authorship, and/or publication of this article from the Dutch Organization for Scientific Research (NWO).</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Baker</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>1989</year>). <article-title>Metacognition, comprehension monitoring, and the adult reader</article-title>. <source>Educational Psychology Review</source>, <volume>1</volume>, <fpage>3</fpage>–<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr2-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Barkas-Avila</surname>
<given-names>J. A.</given-names>
</name>
<name>
<surname>Oberholzer</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Schmutz</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>DeVito</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Opwis</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Usable error message presentation in the World Wide Web: Do not show errors right away</article-title>. <source>Interacting With Computers</source>, <volume>19</volume>, <fpage>330</fpage>–<lpage>341</lpage>.</citation>
</ref>
<ref id="bibr3-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Boren</surname>
<given-names>M. T.</given-names>
</name>
<name>
<surname>Ramey</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Thinking aloud: Reconciling theory and practice</article-title>. <source>IEEE Transactions on Professional Communication</source>, <volume>43</volume>, <fpage>261</fpage>–<lpage>278</lpage>.</citation>
</ref>
<ref id="bibr4-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Brown</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Levinson</surname>
<given-names>S. C.</given-names>
</name>
</person-group> (<year>1987</year>). <source>Politeness: Some universals in language usage</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr5-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bruun</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Gull</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Hofmeister</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Stage</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Let your users do the testing: A comparison of three remote asynchronous usability testing methods</article-title>. <source>Proceedings of the 27th International Conference on Human Factors in Computing Systems</source> (<comment>CHI 2009</comment>; pp. <fpage>1619</fpage>–<lpage>1628</lpage>). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Castillo</surname>
<given-names>J. C.</given-names>
</name>
<name>
<surname>Hartson</surname>
<given-names>H. R.</given-names>
</name>
<name>
<surname>Hix</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Remote usability evaluation: Can users report their own critical incidents?</article-title>. <source>In Proceedings of the Conference on Human Factors on Computing Systems</source> (<comment>CHI ’98;</comment> pp. <fpage>253</fpage>–<lpage>254</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr7-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cunliffe</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Developing usable Websites: A review and model</article-title>. <source>Internet Research</source>, <volume>10</volume>, <fpage>295</fpage>–<lpage>307</lpage>.</citation>
</ref>
<ref id="bibr8-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>De Jong</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>1998</year>). <source>Reader feedback in text design: Validity of the plus–minus method for the pretesting of public information brochures</source>. <publisher-loc>Atlanta, GA</publisher-loc>: <publisher-name>Rodopi</publisher-name>.</citation>
</ref>
<ref id="bibr9-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>De Jong</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Lentz</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Focus: Design and evaluation of a software tool for collecting reader feedback</article-title>. <source>Technical Communication Quarterly</source>, <volume>10</volume>, <fpage>387</fpage>–<lpage>401</lpage>.</citation>
</ref>
<ref id="bibr10-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>De Jong</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Schellens</surname>
<given-names>P. J.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Readers’ background characteristics and their feedback on documents: The influence of gender and educational level on evaluation results</article-title>. <source>Journal of Technical Writing and Communication</source>, <volume>31</volume>, <fpage>267</fpage>–<lpage>281</lpage>.</citation>
</ref>
<ref id="bibr11-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Elling</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Lentz</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>De Jong</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Website evaluation questionnaire: Development of a research-based tool for evaluating informational Web sites</article-title>. <source>Lecture Notes in Computer Science</source>, <volume>4656</volume>, <fpage>293</fpage>–<lpage>304</lpage>.</citation>
</ref>
<ref id="bibr12-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Elling</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Lentz</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>De Jong</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Retrospective think-aloud method: Using eye movements as an extra cue for participants’ verbalizations</article-title>. <source>Proceedings of the 2011 Annual Conference on Human Factors in Computing Systems</source> (pp. <fpage>1161</fpage>–<lpage>1170</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr13-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Elling</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Lentz</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>De Jong</surname>
</name>
</person-group>. (<year>2012</year>). <article-title>Measuring the quality of governmental websites in a controlled versus an online setting with the ‘Website Evaluation Questionnaire’</article-title>. <source>Government Information Quarterly</source>, <comment>(in press)</comment>.</citation>
</ref>
<ref id="bibr14-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Faulkner</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Beyond the five-user assumption: Benefits of increased sample sizes in usability testing</article-title>. <source>Behavior Research Methods Instruments &amp; Computers</source>, <volume>35</volume>, <fpage>379</fpage>–<lpage>383</lpage>.</citation>
</ref>
<ref id="bibr15-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hassenzahl</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Prioritizing usability problems: Data-driven and judgement-driven severity estimates</article-title>. <source>Behaviour &amp; Information Technology</source>, <volume>19</volume>, <fpage>29</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr16-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hertzum</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Jacobsen</surname>
<given-names>N. E.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>The evaluator effect: A chilling fact about usability evaluation methods</article-title>. <source>International Journal of Human-Computer Interaction</source>, <volume>15</volume>, <fpage>183</fpage>–<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr17-1050651911429920">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Hertzum</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Jacobsen</surname>
<given-names>N. E.</given-names>
</name>
<name>
<surname>Molich</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2002</year>). <source>Usability inspections by groups of specialists: Perceived agreement in spite of disparate observations</source>. <conf-name>Paper presented at the Interactive Poster CHI 2002</conf-name>, <conf-loc>Minneapolis, MN</conf-loc>.</citation>
</ref>
<ref id="bibr18-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hornbæk</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Dogmas in the assessment of usability evaluation methods</article-title>. <source>Behavior &amp; Information Technology</source>, <volume>29</volume>, <fpage>97</fpage>–<lpage>111</lpage>.</citation>
</ref>
<ref id="bibr19-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jansen</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Balijon</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>How do people use instruction guides? Confirming and disconfirming patterns of use</article-title>. <source>Document Design</source>, <volume>3</volume>, <fpage>195</fpage>–<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr20-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Karahasanovíc</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Nyhamar Hinkel</surname>
<given-names>U.</given-names>
</name>
<name>
<surname>Sjø´berg</surname>
<given-names>D. I. K.</given-names>
</name>
<name>
<surname>Thomas</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Comparing of feedback-collection and think-aloud methods in program comprehension studies</article-title>. <source>Behavior Information Technology</source>, <volume>28</volume>, <fpage>139</fpage>–<lpage>164</lpage>.</citation>
</ref>
<ref id="bibr21-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kramer</surname>
<given-names>A. F.</given-names>
</name>
<name>
<surname>Hahn</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Gopher</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Task coordination and aging: Explorations of executive control processes in the task switching paradigm</article-title>. <source>Acta Psychologica</source>, <volume>101</volume>, <fpage>339</fpage>–<lpage>378</lpage>.</citation>
</ref>
<ref id="bibr22-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lewis</surname>
<given-names>J. R.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Sample sizes for usability studies: Additional considerations</article-title>. <source>Human Factors</source>, <volume>36</volume>, <fpage>368</fpage>–<lpage>378</lpage>.</citation>
</ref>
<ref id="bibr23-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Molich</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Dumas</surname>
<given-names>J. S.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Comparative usability evaluation (CUE-4)</article-title>. <source>Behaviour &amp; Information Technology</source>, <volume>27</volume>, <fpage>263</fpage>–<lpage>281</lpage>.</citation>
</ref>
<ref id="bibr24-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Monsell</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Task switching</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>7</volume>, <fpage>134</fpage>–<lpage>140</lpage>.</citation>
</ref>
<ref id="bibr25-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Moon</surname>
<given-names>Y.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Don’t blame the computer: When self-disclosure moderates the self-serving bias</article-title>. <source>Journal of Consumer Psychology</source>, <volume>13</volume>, <fpage>125</fpage>–<lpage>137</lpage>.</citation>
</ref>
<ref id="bibr26-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Neerincx</surname>
<given-names>M. A.</given-names>
</name>
<name>
<surname>Lindenberg</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Pemberton</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Support concepts for Web navigation: A cognitive engineering approach</article-title>. <source>In Proceedings of the 10th International Conference on World Wide Web</source> (pp. <fpage>119</fpage>–<lpage>128</lpage>). <publisher-loc>Hong Kong</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr27-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Nichols</surname>
<given-names>D. M.</given-names>
</name>
<name>
<surname>McKay</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Twidale</surname>
<given-names>M. B.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Participatory usability: Supporting proactive users</article-title>. <source>Proceedings of 4th ACM SIGCHI NZ, Symposium on Computer-Human Interaction</source> (CHINZ ’03; pp. <fpage>63</fpage>–<lpage>68</lpage>). <publisher-loc>Dunedin, New Zealand</publisher-loc>: <publisher-name>SIGCHI</publisher-name>.</citation>
</ref>
<ref id="bibr28-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nielsen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1994a</year>). <article-title>Estimating the number of subjects needed for a thinking aloud test</article-title>. <source>International Journal of Human–Computer Studies</source>, <volume>41</volume>, <fpage>385</fpage>–<lpage>397</lpage>.</citation>
</ref>
<ref id="bibr29-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Nielsen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1994b</year>). <article-title>Heuristic evaluation</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Nielsen</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Mack</surname>
<given-names>R. L.</given-names>
</name>
</person-group> (Eds.), <source>Usability inspection methods</source> (pp. <fpage>25</fpage>–<lpage>62</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr30-1050651911429920">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Nielsen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1995</year>). <source>Severity ratings for usability problems</source>. <comment>Retrieved from</comment>
<ext-link ext-link-type="uri" xlink:href="http://www.useit.com/papers/heuristic/severityrating.html">http://www.useit.com/papers/heuristic/severityrating.html</ext-link>
</citation>
</ref>
<ref id="bibr31-1050651911429920">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Nielsen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2001</year>). <source>Alertbox: First rule of usability? Don’t listen to users</source>. <comment>Retrieved from</comment>
<ext-link ext-link-type="uri" xlink:href="http://www.useit.com/alertbox/20010805.html">http://www.useit.com/alertbox/20010805.html</ext-link>
</citation>
</ref>
<ref id="bibr32-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Ormrod</surname>
<given-names>J. E.</given-names>
</name>
</person-group> (<year>2006</year>). <source>Educational psychology: Developing learners</source>. (<edition>5th ed</edition>.). <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Pearson Education</publisher-name>.</citation>
</ref>
<ref id="bibr33-1050651911429920">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Sauro</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2010</year>). <source>Can users self-report usability problems?</source>
<comment>Retrieved from</comment>
<ext-link ext-link-type="uri" xlink:href="http://www.measuringusability.com/blog/self-reporting.php">http://www.measuringusability.com/blog/self-reporting.php</ext-link>
</citation>
</ref>
<ref id="bibr34-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Schriver</surname>
<given-names>K. A.</given-names>
</name>
</person-group> (<year>1997</year>). <source>Dynamics in document design: Creating text for readers</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr35-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Serenko</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Are interface agents scapegoats? Attributions of responsibility in human-agent interaction</article-title>. <source>Interacting With Computers</source>, <volume>19</volume>, <fpage>293</fpage>–<lpage>303</lpage>.</citation>
</ref>
<ref id="bibr36-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sienot</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Pretesting Web sites: A comparison between the plus–minus method and the think-aloud method for the World Wide Web</article-title>. <source>Journal of Business and Technical Communication</source>, <volume>11</volume>, <fpage>469</fpage>–<lpage>482</lpage>.</citation>
</ref>
<ref id="bibr37-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Spool</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Schroeder</surname>
<given-names>W.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Testing Web sites: Five users is nowhere near enough</article-title>. <source>CHI Extended Abstracts</source> (pp. <fpage>85</fpage>–<lpage>286</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr38-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Spyridakis</surname>
<given-names>J. H.</given-names>
</name>
<name>
<surname>Wei</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Barrick</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Cuddihy</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Maust</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Internet-based research: Providing a foundation for Web design guidelines</article-title>. <source>IEEE Transactions on Professional Communication</source>, <volume>48</volume>, <fpage>242</fpage>–<lpage>260</lpage>.</citation>
</ref>
<ref id="bibr39-1050651911429920">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Tullis</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Albert</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Measuring the user experience: Collecting, analyzing, and presenting usability metrics</source>. <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Kaufmann</publisher-name>.</citation>
</ref>
<ref id="bibr40-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Uldall-Espersen</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Frøkjær</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Hornbæk</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Tracing impact in a usability improvement process</article-title>. <source>Interacting With Computers</source>, <volume>20</volume>, <fpage>48</fpage>–<lpage>63</lpage>.</citation>
</ref>
<ref id="bibr41-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Van den Haak</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>De Jong</surname>
<given-names>M. D. T.</given-names>
</name>
<name>
<surname>Schellens</surname>
<given-names>P. J.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Retrospective vs. concurrent think-aloud protocols: Testing the usability of an online library catalogue</article-title>. <source>Behaviour &amp; Information Technology</source>, <volume>22</volume>, <fpage>339</fpage>–<lpage>251</lpage>.</citation>
</ref>
<ref id="bibr42-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Van den Haak</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>De Jong</surname>
<given-names>M. D. T.</given-names>
</name>
<name>
<surname>Schellens</surname>
<given-names>P. J.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Employing think-aloud protocols and constructive interaction to test the usability of online library catalogues: A methodological comparison</article-title>. <source>Interacting With Computers, 16</source>, <fpage>1153</fpage>–<lpage>1170</lpage>.</citation>
</ref>
<ref id="bibr43-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Van Dijk</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Pieterson</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Van Deursen</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Ebbers</surname>
<given-names>W.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>E-services for citizens: The Dutch usage case</article-title>. <source>Lecture Notes in Computer Science</source>, <volume>4656</volume>, <fpage>155</fpage>–<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr44-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Virzi</surname>
<given-names>R. A.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>Refining the test phase of usability evaluation: How many subjects is enough?</article-title>. <source>Human Factors</source>, <volume>34</volume>, <fpage>457</fpage>–<lpage>468</lpage>.</citation>
</ref>
<ref id="bibr45-1050651911429920">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Welle Donker-Kuijer</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>de Jong</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Lentz</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Heuristic Web site evaluation: Exploring the effects of guidelines on experts’ detection of usability problems</article-title>. <source>Technical Communication</source>, <volume>55</volume>, <fpage>392</fpage>–<lpage>404</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>