<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TIM</journal-id>
<journal-id journal-id-type="hwp">sptim</journal-id>
<journal-title>Transactions of the Institute of Measurement and Control</journal-title>
<issn pub-type="ppub">0142-3312</issn>
<issn pub-type="epub">1477-0369</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0142331211425404</article-id>
<article-id pub-id-type="publisher-id">10.1177_0142331211425404</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Development of CO<sub>2</sub> emission model of an acid gas incinerator using Nelder–Mead least squares support vector regression</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Pathmanathan</surname><given-names>Elangeshwaran</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Ibrahim</surname><given-names>Rosdiazli</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Asirvadam</surname><given-names>Vijanth Sagayan</given-names></name>
</contrib>
<aff id="aff1-0142331211425404">Electrical and Electronic Engineering Department, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, Tronoh, Perak, Malaysia</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0142331211425404">Elangeshwaran Pathmanathan, Electrical and Electronic Engineering Department, Universiti Teknologi PETRONAS, Bandar Seri Iskandar, 31720 Tronoh, Perak, Malaysia Email: <email>elanz117@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>34</volume>
<issue>8</issue>
<fpage>974</fpage>
<lpage>982</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This paper aims to develop a CO<sub>2</sub> emission model of an acid gas incinerator using Nelder–Mead least squares support vector regression (LS-SVR). The Malaysia Department of Environment is actively imposing the Clean Air Regulation to mandate heavy industries to comply with emission limits. One of the latest measures is to mandate the installation of analytical instrumentation known as a continuous emission monitoring system (CEMS) to report the emission level online to the Department of Environment office. As a hardware-based analyser, CEMS is expensive, maintenance intensive and often unreliable. Therefore, software predictive techniques are often preferred and considered a feasible alternative to replace the CEMS for regulatory compliance. The LS-SVR model is built based on the emissions from an acid gas incinerator that operates in a liquefied natural gas complex. Simulated annealing is first used to determine the initial hyper-parameters, which are further optimized based on the performance of the model using a Nelder–Mead simplex algorithm. The LS-SVR model is shown to outperform a benchmark model based on back-propagation neural networks in both training and testing data.</p>
</abstract>
<kwd-group>
<kwd>Artificial neural networks</kwd>
<kwd>industrial pollution</kwd>
<kwd>predictive algorithms</kwd>
<kwd>support vector machines</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0142331211425404" sec-type="intro">
<title>Introduction</title>
<p>Power plants, chemical plants, government utilities and petroleum refineries typically produce more than the plant’s capacity in order to meet demands. Hence these industries must continuously monitor their exhaust stacks for primary pollutants such as nitrogen oxides, carbon monoxide, sulphur dioxide and carbon dioxide under regulations promulgated under the New Source Performance Standards [US Environmental Protection Agency (EPA) PA 40 CFR Part 60) or the Clean Air Act Amendments Title IV (40 CFR Part 75), 2008] and the Malaysian <xref ref-type="bibr" rid="bibr6-0142331211425404">Environmental Quality Act, 1978</xref> (2000). A continuous emission monitoring system (CEMS) utilizes a sample probe, umbilical and sample conditioning system to extract a representative sample of the stack gas exhaust stream to provide a continuous flow for direct measurement of the pollutant concentration on individual analysers (<xref ref-type="bibr" rid="bibr7-0142331211425404">Fan and Zheng, 2009</xref>). A data acquisition system (DAS) is typically used to collect, calculate emission rates, alarm and store historical data from these CEMS (<xref ref-type="bibr" rid="bibr24-0142331211425404">Zheng and Tang, 2008</xref>). The promulgated regulations in US EPA 40 CFR covering the monitoring of combustion units for primary and other pollutants allow for the use of predictive emissions monitoring systems (PEMS) in lieu of CEMS. A PEMS is a sophisticated software-based sensor/prediction system that is directly interfaced with the process control system and inputs from the combustion or pollution control process. The <xref ref-type="bibr" rid="bibr20-0142331211425404">US EPA (2011)</xref> have revealed the devastating effects of the continuous release of pollutants such as carbon dioxide, methane, nitrous dioxide and other greenhouse (heat-trapping) gases into the atmosphere. Climate change induced by these gases can cause damage to human health, agriculture, natural ecosystems, coastal areas and other climate sensitive systems. Currently, emission monitoring is done via analytical instruments, which are very expensive to install and maintain; the cost for an online NO<sub>x</sub> analytical and monitoring system is around $100,000–200,000 and the cost for maintenance is approximately $15,000 per year (<xref ref-type="bibr" rid="bibr5-0142331211425404">Dong and McAvoy, 1995</xref>). Several studies have been performed to develop predictive systems for industrial emissions. One of the earlier ideas was presented by <xref ref-type="bibr" rid="bibr1-0142331211425404">Baines (1999)</xref>, a consultant from Fisher-Rosemount Solutions. The author proposed the idea of predicting the stack contaminants in real time by correlating emissions with key unit parameters like fuel type, air and fuel flows, and combustion temperature. <xref ref-type="bibr" rid="bibr2-0142331211425404">Chakravarthy et al. (2000)</xref> have developed a PEMS for industrial process heaters. The authors have used heuristic an optimizer genetic algorithm (GA) to tune the NO<sub>x</sub> kinetic parameters. <xref ref-type="bibr" rid="bibr4-0142331211425404">Deng and Stobart (2007)</xref> examined the idea of using a hybrid clustering technique that involves the modifications of the Fuzzy C-means method and using neural networks to achieve the best performance of the fuzzy system for a diesel engine emission model. <xref ref-type="bibr" rid="bibr26-0142331211425404">Zheng et al. (2008)</xref> used a generalized regression neural network (GRNN) to establish a non-linear model between the parameters of the boiler (of 300 MW steam capacity) and the NO<sub>x</sub> emissions. Later, the same authors have proposed the idea of replacing the existing GRNN PEMS with a least squares support vector regression (LS-SVR) model. The authors claim that this new algorithm yields better accuracy of a coal combustion unit (<xref ref-type="bibr" rid="bibr25-0142331211425404">Zheng et al., 2010</xref>). The main objective of this paper is to develop a CO<sub>2</sub> emission model of an acid gas incinerator based on an LS-SVR algorithm with Nelder–Mead (NM) optimization of its hyper-parameters. This paper is organized as follows: Section 2 presents a brief literature on artificial neural networks (ANNs) and LS-SVR. Section 3 describes the modelling background in terms of the parameters used, a case study of an acid gas incinerator and the dataset used. Section 4 presents the results and analysis, and Section 5 gives a brief conclusion.</p>
</sec>
<sec id="section2-0142331211425404">
<title>Literature review</title>
<sec id="section3-0142331211425404">
<title>Artificial neural network (ANN)</title>
<p>An ANN is represented as non-linear interconnected layers of processing nodes, which are normally referred to as neurons, a term borrowed from neurobiology (<xref ref-type="bibr" rid="bibr27-0142331211425404">Zhou et al., 2004</xref>). The most common class of ANN is the multilayered feedforward network, which primarily consists of an input layer, one or more hidden layers and an output layer. The input layer holds the data and distributes it to the network via interconnections (neurons) in the hidden layer(s), where they are processed by the activation function to produce an output signal. This type of network is also known as a multilayer perceptron (MLP) (<xref ref-type="bibr" rid="bibr9-0142331211425404">Haykins, 1999</xref>); a typical MLP network is shown in <xref ref-type="fig" rid="fig1-0142331211425404">Figure 1</xref>. Common transfer functions used are linear, log-sigmoid, and tan-sigmoid as shown in <xref ref-type="disp-formula" rid="disp-formula1-0142331211425404">Equations (1)</xref>, <xref ref-type="disp-formula" rid="disp-formula2-0142331211425404">(2)</xref> and <xref ref-type="disp-formula" rid="disp-formula3-0142331211425404">(3)</xref>.</p>
<fig id="fig1-0142331211425404" position="float">
<label>Figure 1</label>
<caption>
<p>Typical feedforward back-propagation network.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig1.tif"/>
</fig>
<p>
<disp-formula id="disp-formula1-0142331211425404">
<label>(1)</label>
<mml:math display="block" id="math1-0142331211425404">
<mml:mrow>
<mml:mi>y</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>A</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0142331211425404" xlink:href="10.1177_0142331211425404-eq1.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula2-0142331211425404">
<label>(2)</label>
<mml:math display="block" id="math2-0142331211425404">
<mml:mrow>
<mml:mi>y</mml:mi>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>A</mml:mi>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0142331211425404" xlink:href="10.1177_0142331211425404-eq2.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula3-0142331211425404">
<label>(3)</label>
<mml:math display="block" id="math3-0142331211425404">
<mml:mrow>
<mml:mi>y</mml:mi>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi>A</mml:mi>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:mfrac>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-0142331211425404" xlink:href="10.1177_0142331211425404-eq3.tif"/>
</disp-formula>
</p>
<p>where <italic>A</italic> is the element activation function and is represented by <xref ref-type="disp-formula" rid="disp-formula4-0142331211425404">Equation (4)</xref>:</p>
<p>
<disp-formula id="disp-formula4-0142331211425404">
<label>(4)</label>
<mml:math display="block" id="math4-0142331211425404">
<mml:mrow>
<mml:mi>A</mml:mi>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-0142331211425404" xlink:href="10.1177_0142331211425404-eq4.tif"/>
</disp-formula>
</p>
<p>The variable <inline-formula id="inline-formula1-0142331211425404"><mml:math display="inline" id="math5-0142331211425404"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the weight term and <inline-formula id="inline-formula2-0142331211425404"><mml:math display="inline" id="math6-0142331211425404"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the network input. The network uses these weights to identify the strength of the interconnections between neurons. These weights are adjusted throughout the learning process. For an MLP network, learning algorithms based on the gradient or Jacobian of the network error with respect to the weights are preferred because of their superior performance. A common Jacobian-based algorithm is the back-propagation algorithm. The back-propagation neural network (BPNN) has been widely used to develop soft sensors for prediction of NO<sub>x</sub> (<xref ref-type="bibr" rid="bibr18-0142331211425404">Tronci et al., 2002</xref>). Some of the weaknesses of BPNN are the need for numerous controlling parameters, difficulty in obtaining a stable solution and the danger of overfitting. The solution shown by <xref ref-type="bibr" rid="bibr26-0142331211425404">Zheng et al. (2008)</xref> points to the fact that the BPNN is unreliable even if all of network objects are pre-determined.</p>
</sec>
<sec id="section4-0142331211425404">
<title>Support vector regression (SVR)</title>
<p>The foundations of support vector machines (SVM) were laid by <xref ref-type="bibr" rid="bibr22-0142331211425404">Vapnik (1995)</xref>. Early SVMs were developed for classification problems; later they were extended to the domain of regression problems (<xref ref-type="bibr" rid="bibr23-0142331211425404">Vapnik et al., 1997</xref>) and collectively known as support vector regression (SVR). The formulation for SVR begins with a set of data (<xref ref-type="bibr" rid="bibr8-0142331211425404">Gunn, 1998</xref>):</p>
<p>
<disp-formula id="disp-formula5-0142331211425404">
<label>(5)</label>
<mml:math display="block" id="math7-0142331211425404">
<mml:mrow>
<mml:mi>D</mml:mi>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>{</mml:mo>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>,</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mo>,</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>}</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>∈</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>∈</mml:mo>
<mml:mi>R</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-0142331211425404" xlink:href="10.1177_0142331211425404-eq5.tif"/>
</disp-formula>
</p>
<p>which is to be approximated to a non-linear function:</p>
<p>
<disp-formula id="disp-formula6-0142331211425404">
<label>(6)</label>
<mml:math display="block" id="math8-0142331211425404">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mi>ϕ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula6-0142331211425404" xlink:href="10.1177_0142331211425404-eq6.tif"/>
</disp-formula>
</p>
<p>Where <inline-formula id="inline-formula3-0142331211425404"><mml:math display="inline" id="math9-0142331211425404"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a mapping function to higher or infinite dimensional feature space. The mapping function is implicitly defined. The optimal regression function is given by the minimum of the functional:</p>
<p>
<disp-formula id="disp-formula7-0142331211425404">
<label>(7)</label>
<mml:math display="block" id="math10-0142331211425404">
<mml:mrow>
<mml:mtext>Φ</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>ξ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mo stretchy="false">|</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">|</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>+</mml:mo>
<mml:mi>C</mml:mi>
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:munder>
<mml:mo stretchy="false">(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>+</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>ξ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>+</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula7-0142331211425404" xlink:href="10.1177_0142331211425404-eq7.tif"/>
</disp-formula>
</p>
<p>Where <inline-formula id="inline-formula4-0142331211425404"><mml:math display="inline" id="math11-0142331211425404"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> is a pre-specified value, and <inline-formula id="inline-formula5-0142331211425404"><mml:math display="inline" id="math12-0142331211425404"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula6-0142331211425404"><mml:math display="inline" id="math13-0142331211425404"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are slack variables that represent the upper and lower constraints on the system outputs. <xref ref-type="disp-formula" rid="disp-formula7-0142331211425404">Equation (7)</xref> is a quadratic programming (QP) problem that is solved first by introducing a Hessian matrix (<italic>H</italic>). If <italic>H</italic> is proven to be a positive semi-definite matrix, then <inline-formula id="inline-formula7-0142331211425404"><mml:math display="inline" id="math14-0142331211425404"><mml:mrow><mml:mi>Φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a convex function and a conditional global minimizer exists. On the other hand, if <italic>H</italic> is found to be positive definite, then the solution to the convex minimization problem is global and unique (<xref ref-type="bibr" rid="bibr21-0142331211425404">Vanderbei, 1998</xref>).</p>
<p>Using an <inline-formula id="inline-formula8-0142331211425404"><mml:math display="inline" id="math15-0142331211425404"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow></mml:math></inline-formula> -sensitive loss function, <xref ref-type="fig" rid="fig2-0142331211425404">Figure 2</xref>,</p>
<fig id="fig2-0142331211425404" position="float">
<label>Figure 2</label>
<caption>
<p>
<inline-formula id="inline-formula9-0142331211425404">
<mml:math display="inline" id="math16-0142331211425404">
<mml:mrow>
<mml:mi>ε</mml:mi>
</mml:mrow>
</mml:math>
</inline-formula> -sensitive loss function.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig2.tif"/>
</fig>
<p>
<disp-formula id="disp-formula8-0142331211425404">
<label>(8)</label>
<mml:math display="block" id="math17-0142331211425404">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>L</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ε</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>{</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:mi>for</mml:mi>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo>&lt;</mml:mo>
<mml:mi>ε</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="left" columnspan="1">
<mml:mrow>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>ε</mml:mi>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="left">
<mml:mrow>
<mml:mi>otherwise</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula8-0142331211425404" xlink:href="10.1177_0142331211425404-eq8.tif"/>
</disp-formula>
</p>
<p>Employing the Lagrangian duality theorem, the minimization problem becomes:</p>
<p>
<disp-formula id="disp-formula9-0142331211425404">
<label>(9)</label>
<mml:math display="block" id="math18-0142331211425404">
<mml:mrow>
<mml:mo>max</mml:mo>
<mml:mi>W</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>α</mml:mi>
<mml:mo>,</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mi>max</mml:mi>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msubsup>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mi>ε</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>α</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi>ε</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mo>−</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mo>.</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>K</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula9-0142331211425404" xlink:href="10.1177_0142331211425404-eq9.tif"/>
</disp-formula>
</p>
<p>With constraints</p>
<p>
<disp-formula id="disp-formula10-0142331211425404">
<label>(10)</label>
<mml:math display="block" id="math19-0142331211425404">
<mml:mrow>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mo>≤</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>≤</mml:mo>
<mml:mi>C</mml:mi>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula10-0142331211425404" xlink:href="10.1177_0142331211425404-eq10.tif"/>
</disp-formula>
</p>
<p>Solving <xref ref-type="disp-formula" rid="disp-formula9-0142331211425404">Equations (9)</xref> and <xref ref-type="disp-formula" rid="disp-formula10-0142331211425404">(10)</xref> determines the Lagrange multipliers, <inline-formula id="inline-formula10-0142331211425404"><mml:math display="inline" id="math20-0142331211425404"><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the regression function is given by</p>
<p>
<disp-formula id="disp-formula11-0142331211425404">
<label>(11)</label>
<mml:math display="block" id="math21-0142331211425404">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:munder>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>SVs</mml:mi>
</mml:mrow>
</mml:munder>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>K</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula11-0142331211425404" xlink:href="10.1177_0142331211425404-eq11.tif"/>
</disp-formula>
</p>
<p>where</p>
<p>
<disp-formula id="disp-formula12-0142331211425404">
<label>(12)</label>
<mml:math display="block" id="math22-0142331211425404">
<mml:mrow>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mo stretchy="false">〈</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mo>,</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">〉</mml:mo>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>K</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mo>=</mml:mo>
<mml:mo>−</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>*</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>K</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>r</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>K</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>s</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula12-0142331211425404" xlink:href="10.1177_0142331211425404-eq12.tif"/>
</disp-formula>
</p>
</sec>
<sec id="section5-0142331211425404">
<title>Least squares support vector regression (LS-SVR)</title>
<p>The SVM formulation above leads to a QP problem with linear constraints. The size of the matrix involved in the QP problem is directly proportional to the size of the training data (<xref ref-type="bibr" rid="bibr25-0142331211425404">Zheng et al., 2010</xref>). To reduce the complexity of the optimization problem, <xref ref-type="bibr" rid="bibr17-0142331211425404">Suykens et al. (2002)</xref> introduced a modified version of the SVM called the least squares SVM (LS-SVM). This formulation results in a set of linear equations instead of a QP problem. The LS-SVM is used for both classification and regression problems. The formulation for LS-SVR is begun by taking a training set as in <xref ref-type="disp-formula" rid="disp-formula5-0142331211425404">Equation (5)</xref> and estimate using a non-linear function as in <xref ref-type="disp-formula" rid="disp-formula6-0142331211425404">Equation (6)</xref> where <inline-formula id="inline-formula11-0142331211425404"><mml:math display="inline" id="math23-0142331211425404"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is a mapping function to a high dimensional and potentially infinite dimensional feature space; in this paper, there are 22 dimensions in the low dimensional space and 484 dimensions in high dimension space. Next, the optimization is formulated in the primal weight space:</p>
<p>
<disp-formula id="disp-formula13-0142331211425404">
<label>(13)</label>
<mml:math display="block" id="math24-0142331211425404">
<mml:mrow>
<mml:mo>min</mml:mo>
<mml:mtext>Φ</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>e</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:msup>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mi>w</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi>γ</mml:mi>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:mfrac>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msubsup>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula13-0142331211425404" xlink:href="10.1177_0142331211425404-eq13.tif"/>
</disp-formula>
</p>
<p>Subject to:</p>
<p>
<disp-formula id="disp-formula14-0142331211425404">
<label>(14)</label>
<mml:math display="block" id="math25-0142331211425404">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mi>ϕ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula14-0142331211425404" xlink:href="10.1177_0142331211425404-eq14.tif"/>
</disp-formula>
</p>
<p>The optimization formulation in <xref ref-type="disp-formula" rid="disp-formula13-0142331211425404">Equation (13)</xref> is a ridge regression cost function formulated in feature space. Constructing the Lagrangian of the problem, the dual problem is derived:</p>
<p>
<disp-formula id="disp-formula15-0142331211425404">
<label>(14)</label>
<mml:math display="block" id="math26-0142331211425404">
<mml:mrow>
<mml:mi>L</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>e</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>α</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mtext>Φ</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>e</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">{</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mi>ϕ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">}</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula15-0142331211425404" xlink:href="10.1177_0142331211425404-eq15.tif"/>
</disp-formula>
</p>
<p>The conditions for optimality are given by (<xref ref-type="bibr" rid="bibr17-0142331211425404">Suykens et al., 2002</xref>):</p>
<p>
<disp-formula id="disp-formula16-0142331211425404">
<label>(15)</label>
<mml:math display="block" id="math27-0142331211425404">
<mml:mrow>
<mml:mtable align="right" width="80%">
<mml:mtr>
<mml:mtd columnalign="left" columnspan="1">
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:mi>L</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:mi>w</mml:mi>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>→</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mi>w</mml:mi>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>ϕ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="left" columnspan="1">
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:mi>L</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:mi>b</mml:mi>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>→</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="left" columnspan="1">
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:mi>L</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>→</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>γ</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>.</mml:mo>
<mml:mo>.</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="left" columnspan="1">
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:mi>L</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>∂</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mfrac>
<mml:mo>=</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>→</mml:mo>
<mml:mspace width="0.25em"/>
<mml:msup>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mi>ϕ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>e</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mspace width="0.25em"/>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>.</mml:mo>
<mml:mo>.</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula16-0142331211425404" xlink:href="10.1177_0142331211425404-eq16.tif"/>
</disp-formula>
</p>
<p>Upon elimination of the variables <inline-formula id="inline-formula12-0142331211425404"><mml:math display="inline" id="math28-0142331211425404"><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula13-0142331211425404"><mml:math display="inline" id="math29-0142331211425404"><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> and solving in <inline-formula id="inline-formula14-0142331211425404"><mml:math display="inline" id="math30-0142331211425404"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula15-0142331211425404"><mml:math display="inline" id="math31-0142331211425404"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> one gets the following solution in dual space:</p>
<p>
<disp-formula id="disp-formula17-0142331211425404">
<label>(16)</label>
<mml:math display="block" id="math32-0142331211425404">
<mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>v</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:mtd>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mtext>Ω</mml:mtext>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">/</mml:mo>
<mml:mi>γ</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mi>b</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
<mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula17-0142331211425404" xlink:href="10.1177_0142331211425404-eq17.tif"/>
</disp-formula>
</p>
<p>Where <inline-formula id="inline-formula16-0142331211425404"><mml:math display="inline" id="math33-0142331211425404"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mo>⋯</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula><inline-formula id="inline-formula17-0142331211425404"><mml:math display="inline" id="math34-0142331211425404"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>;</mml:mo><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:mo>;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula id="inline-formula18-0142331211425404"><mml:math display="inline" id="math35-0142331211425404"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mo>⋯</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. The ‘kernel trick’ (<xref ref-type="bibr" rid="bibr3-0142331211425404">Cristianini and Shawe-Taylor, 2000</xref>) is applied here as shown:</p>
<p>
<disp-formula id="disp-formula18-0142331211425404">
<label>(17)</label>
<mml:math display="block" id="math36-0142331211425404">
<mml:mrow>
<mml:mtable align="left">
<mml:mtr>
<mml:mtd columnalign="center">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mtext>Ω</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mi>ij</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>ϕ</mml:mi>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mi>ϕ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd columnalign="center" columnspan="1">
<mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mspace width="0.25em"/>
<mml:mo>=</mml:mo>
<mml:mi>K</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi>i</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula18-0142331211425404" xlink:href="10.1177_0142331211425404-eq18.tif"/>
</disp-formula>
</p>
<p>Hence the resulting LS-SVR model becomes:</p>
<p>
<disp-formula id="disp-formula19-0142331211425404">
<label>(18)</label>
<mml:math display="block" id="math37-0142331211425404">
<mml:mrow>
<mml:mi>f</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>α</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>K</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mi>b</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula19-0142331211425404" xlink:href="10.1177_0142331211425404-eq19.tif"/>
</disp-formula>
</p>
<p>In this paper, a radial basis function (RBF) kernel function is used:</p>
<p>
<disp-formula id="disp-formula20-0142331211425404">
<mml:math display="block" id="math38-0142331211425404">
<mml:mrow>
<mml:mi>K</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mtext>exp</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>−</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mo stretchy="false">|</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">|</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
<mml:mrow>
<mml:mi>σ</mml:mi>
</mml:mrow>
</mml:mfrac>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula20-0142331211425404" xlink:href="10.1177_0142331211425404-eq20.tif"/>
</disp-formula>
</p>
<p>Where <inline-formula id="inline-formula19-0142331211425404"><mml:math display="inline" id="math39-0142331211425404"><mml:mrow><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula20-0142331211425404"><mml:math display="inline" id="math40-0142331211425404"><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula> are solutions to the linear system represented by <xref ref-type="disp-formula" rid="disp-formula17-0142331211425404">Equation (16)</xref>. The LS-SVR formulation can be used to handle large datasets with no dimensionality problem. In <xref ref-type="disp-formula" rid="disp-formula18-0142331211425404">Equation (17)</xref>, a kernel function is used to replace the high order mapping function. In this paper, the RBF kernel will be used exclusively for all computations involving kernel operations. In the case of the RBF kernel, only two hyper-parameters need to be tuned (<inline-formula id="inline-formula21-0142331211425404"><mml:math display="inline" id="math41-0142331211425404"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula id="inline-formula22-0142331211425404"><mml:math display="inline" id="math42-0142331211425404"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>), which is fewer than the standard SVM.</p>
</sec>
</sec>
<sec id="section6-0142331211425404">
<title>Modelling background</title>
<sec id="section7-0142331211425404">
<title>Back-propagation neural network (BPNN) model</title>
<p>Two models will be developed in this paper using two distinctive algorithms. The first model is developed as a benchmark using a BPNN. The optimal network parameters are chosen by varying the number of layers and the number of hidden neurons per layer. The parameter that gives the best performance is chosen and shown below:</p>
<list id="list1-0142331211425404" list-type="bullet">
<list-item><p>Number of layers=2;</p></list-item>
<list-item><p>Number of neurons (hidden layer)=20;</p></list-item>
<list-item><p>Transfer functions (input layer)=tan-sigmoid;</p></list-item>
<list-item><p>Transfer function (hidden layer)=linear;</p></list-item>
<list-item><p>Training algorithm=Levenberg–Marquardt.</p></list-item>
</list>
</sec>
<sec id="section8-0142331211425404">
<title>Nelder–Mead (NM) least squares support vector regression (LS-SVR)</title>
<p>It is understood that the hyper-parameters involved for the standard SVM with RBF kernel are <inline-formula id="inline-formula23-0142331211425404"><mml:math display="inline" id="math43-0142331211425404"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula id="inline-formula24-0142331211425404"><mml:math display="inline" id="math44-0142331211425404"><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula25-0142331211425404"><mml:math display="inline" id="math45-0142331211425404"><mml:mrow><mml:mi>ε</mml:mi></mml:mrow></mml:math></inline-formula>. On the other hand, the LS-SVR algorithm with RBF kernel involves only two hyper-parameters; <inline-formula id="inline-formula26-0142331211425404"><mml:math display="inline" id="math46-0142331211425404"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula27-0142331211425404"><mml:math display="inline" id="math47-0142331211425404"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bibr17-0142331211425404">Suykens et al., 2002</xref>). The common method for optimizing these hyper-parameters is to use the ‘grid-search’ method, which is a global exhaustive search technique. Based on trial-and-error formulation, the hyper-parameters are changed exponentially to find those that give the best performance for the model (<xref ref-type="bibr" rid="bibr10-0142331211425404">Hsu et al., 2010</xref>). Grid-search is straightforward but naïve. There are other advanced methods, which give better performance results and save computational cost. One such method is the NM simplex algorithm, which will be used in this paper. This algorithm is nicknamed the ‘amoeba’ because of its biological-like search patterns. In two dimensions, it consists of a search triangle, ‘crawler’ or ‘simplex’, with three points that represent the highest (worst) point, next highest point and the lowest (best) point. The intuition is to move away from high point towards the low point. The simplex moves in several transformations that are known as ‘reflection’, ‘contraction’, ‘reflection and expansion’, and ‘multiple contractions’ to find the optimal value for the minimization problem (<xref ref-type="bibr" rid="bibr16-0142331211425404">Press et al., 1998</xref>). The parameter settings for the simplex algorithm are shown below:</p>
<list id="list2-0142331211425404" list-type="bullet">
<list-item><p>Expansion steps=2;</p></list-item>
<list-item><p>Size of initial simplex=1.2;</p></list-item>
<list-item><p>Contraction steps=0.5;</p></list-item>
<list-item><p>Reflection steps=1;</p></list-item>
<list-item><p>Shrinkage steps=0.5;</p></list-item>
<list-item><p>Number of optimization steps=15;</p></list-item>
<list-item><p>Number of function evaluations=50;</p></list-item>
<list-item><p>Stopping criterion based on value of function=1e-6;</p></list-item>
<list-item><p>Stopping criterion based on change in minimizer=1e-6.</p></list-item>
</list>
<p>The initial values of the hyper-parameters (<inline-formula id="inline-formula28-0142331211425404"><mml:math display="inline" id="math48-0142331211425404"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="inline-formula29-0142331211425404"><mml:math display="inline" id="math49-0142331211425404"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>) are found using simulated annealing (SA). This technique is borrowed from metallurgy where it is a global optimizer for a large search space. SA is designed to find an optimized solution in a given time, rather than finding the best possible solution (<xref ref-type="bibr" rid="bibr14-0142331211425404">Kirkpatrick, 1983</xref>); this saves computation time and is suitable for finding initial values for further optimizations. SA parameter settings are as follows:</p>
<list id="list3-0142331211425404" list-type="bullet">
<list-item><p>Initial temperature=1;</p></list-item>
<list-item><p>Max number of function evaluations=40;</p></list-item>
<list-item><p>Number of steps at fixed temperature=20;</p></list-item>
<list-item><p>Energy tolerance=1e-45.</p></list-item>
</list>
<p>Using SA alone is not very strong because of the stopping criterion (i.e. number of iterations). The combination of SA and NM is much stronger. SA is only used as a ‘cost effective’ method for locating the initial hyper-parameters. The performances of the models are gauged using standard performance functions in the form of correlation factor (<italic>R</italic>), root mean square error (RMSE) and accuracy.</p>
<p>
<disp-formula id="disp-formula21-0142331211425404">
<label>(19)</label>
<mml:math display="block" id="math50-0142331211425404">
<mml:mrow>
<mml:mi>RMSE</mml:mi>
<mml:mo>=</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:mfrac>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msqrt>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula21-0142331211425404" xlink:href="10.1177_0142331211425404-eq21.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula22-0142331211425404">
<label>(20)</label>
<mml:math display="block" id="math51-0142331211425404">
<mml:mrow>
<mml:mi>R</mml:mi>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:msqrt>
<mml:mrow>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:msqrt>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mover>
<mml:mrow>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
</mml:mrow>
<mml:mo>¯</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:msqrt>
</mml:mrow>
</mml:msqrt>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula22-0142331211425404" xlink:href="10.1177_0142331211425404-eq22.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula23-0142331211425404">
<label>(21)</label>
<mml:math display="block" id="math52-0142331211425404">
<mml:mrow>
<mml:mi>Accuracy</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>100</mml:mn>
<mml:mo>−</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:mfrac>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mo stretchy="false">(</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mover>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfrac>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>×</mml:mo>
<mml:mn>100</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula23-0142331211425404" xlink:href="10.1177_0142331211425404-eq23.tif"/>
</disp-formula>
</p>
<p>Where <inline-formula id="inline-formula30-0142331211425404"><mml:math display="inline" id="math53-0142331211425404"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the predicted value, <inline-formula id="inline-formula31-0142331211425404"><mml:math display="inline" id="math54-0142331211425404"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the true value and <inline-formula id="inline-formula32-0142331211425404"><mml:math display="inline" id="math55-0142331211425404"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> is the number of testing samples. An overall flow of the NM LS-SVR algorithm is presented in <xref ref-type="fig" rid="fig3-0142331211425404">Figure 3</xref>.</p>
<fig id="fig3-0142331211425404" position="float">
<label>Figure 3</label>
<caption>
<p>Nelder–Mead least squares support vector regression (LS-SVR) flowchart. RMSE, root mean square error.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig3.tif"/>
</fig>
</sec>
<sec id="section9-0142331211425404">
<title>Case study</title>
<p>In this paper, the CO<sub>2</sub> emission from a tangentially fired acid gas incinerator is modelled. This incinerator train is part of a liquefied natural gas (LNG) complex. The plant is divided into four sections consisting of upstream facilities, gas treating, liquefaction and storage/terminal. The first part of the gas treating section deals with the acid gas in the feed. CO<sub>2</sub> and H<sub>2</sub>S are removed to meet product specification. An amine-based solvent is used in the acid gas removal process. Solvents with absorbed acid gases are regenerated in regenerator column at high temperature and low pressure. The acid gases are then incinerated before being discharged safely into the environment. In this LNG complex, acid gases are incinerated using two methods: one uses steam boilers and the other uses acid gas thermal incinerators (<xref ref-type="bibr" rid="bibr11-0142331211425404">Jamaludin et al., 2005</xref>). The process flow scheme of the incinerator train is shown in <xref ref-type="fig" rid="fig4-0142331211425404">Figure 4</xref>. The main constituents of the inputs to the incinerator are acid gas, flash gas, fuel gas and combustion air. The flue gases that are released into the stack consist of NO<sub>x</sub>, SO<sub>2</sub>, SO<sub>3</sub>, CO, CO<sub>2</sub> and H<sub>2</sub>S. In this paper, only CO<sub>2</sub> emission will be modelled based on the plant input and output process variable data. The data is obtained from the LNG complex incinerator train distributed control system historical dataset taken from April 2010 to November 2010, and contains the data from 29 input process variables and seven output process variables. The most relevant process variables are selected by reviewing the design and operational specifications in the incinerator material balance sheet. An initial correlation analysis was also performed to the dataset to determine the variables that have the strongest variance with each other.</p>
<fig id="fig4-0142331211425404" position="float">
<label>Figure 4</label>
<caption>
<p>Process flow scheme of incinerator train.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig4.tif"/>
</fig>
<p>As a result, 22 input variables and one output variable (i.e. flue gas CO<sub>2</sub> content) are chosen with 1044 samples, taken at 30 minutes apart for the whole month of September 2010. The number of data points for LS-SVR is limited by the computational ability of the machine that is used to run the model, mainly because the M-file implementation is memory intensive compared with C-Mex or C++ languages. <xref ref-type="table" rid="table1-0142331211425404">Tables 1</xref> and <xref ref-type="table" rid="table2-0142331211425404">2</xref> outline the characteristics of the chosen input and output variables. The relationship between CO<sub>2</sub> and the input variables are not easily modelled mathematically (and it is beyond the scope of this paper) and any attempt on graphical plotting shows erratic distributions (which on some level demonstrates a highly non-linear relationship).</p>
<table-wrap id="table1-0142331211425404" position="float">
<label>Table 1</label>
<caption>
<p>Selected input process variables</p>
</caption>
<graphic alternate-form-of="table1-0142331211425404" xlink:href="10.1177_0142331211425404-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Process variables</th>
<th align="left">Engineering units</th>
<th align="left">Operation units</th>
</tr>
</thead>
<tbody>
<tr>
<td>A-91921 sour feed gas flow</td>
<td>T/D</td>
<td>Incinerator unit 2</td>
</tr>
<tr>
<td>Fuel gas to A-91921</td>
<td>%</td>
<td>Incinerator unit 2</td>
</tr>
<tr>
<td>Fuel gas to incinerator A-91921</td>
<td>Density</td>
<td>Incinerator unit 2</td>
</tr>
<tr>
<td>K-91921 suction air flow</td>
<td>T/D</td>
<td>Incinerator unit 2</td>
</tr>
<tr>
<td>A-91921 stack exh. temp.</td>
<td>°C</td>
<td>Incinerator unit 2</td>
</tr>
<tr>
<td>A-91921 stack exh. Temp.</td>
<td>°C</td>
<td>Incinerator unit 2</td>
</tr>
<tr>
<td>H<sub>2</sub>O analyser A</td>
<td>ppm</td>
<td>N/A</td>
</tr>
<tr>
<td>H<sub>2</sub>O analyser B</td>
<td>ppm</td>
<td>N/A</td>
</tr>
<tr>
<td>CO<sub>2</sub> analyser</td>
<td>%</td>
<td>N/A</td>
</tr>
<tr>
<td>H<sub>2</sub>S analyser</td>
<td>ppm</td>
<td>N/A</td>
</tr>
<tr>
<td>C-91101 feed gas flow</td>
<td>T/D</td>
<td>Flash gas unit</td>
</tr>
<tr>
<td>CO<sub>2</sub> in wet feed gas fr. C-91101</td>
<td>ppm</td>
<td>Flash gas unit</td>
</tr>
<tr>
<td>H2S in wet feed gas fr. C-91101</td>
<td>ppm</td>
<td>Flash gas unit</td>
</tr>
<tr>
<td>C-91101 treated NG temp.</td>
<td>°C</td>
<td>Flash gas unit</td>
</tr>
<tr>
<td>C-91101 treated NG temp.</td>
<td>T/D</td>
<td>Flash gas unit</td>
</tr>
<tr>
<td>Lean slvt to C-91101 flow</td>
<td>T/D</td>
<td>Flash gas unit</td>
</tr>
<tr>
<td>C-91103 solvent reflux flow</td>
<td>T/D</td>
<td>Sour water unit</td>
</tr>
<tr>
<td>HP F/G to KG-91320 flow</td>
<td>T/D</td>
<td>Gas turbine unit</td>
</tr>
<tr>
<td>MR IGV position feedback</td>
<td>N/A</td>
<td>Gas turbine unit</td>
</tr>
<tr>
<td>MR turbine speed</td>
<td>N/A</td>
<td>Gas turbine unit</td>
</tr>
<tr>
<td>MR comb. reference temp.</td>
<td>°C</td>
<td>Gas turbine unit</td>
</tr>
<tr>
<td>MR MW actual power</td>
<td>MW</td>
<td>Gas turbine unit</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table2-0142331211425404" position="float">
<label>Table 2</label>
<caption>
<p>Output process variables (only CO<sub>2</sub> is selected for this paper)</p>
</caption>
<graphic alternate-form-of="table2-0142331211425404" xlink:href="10.1177_0142331211425404-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Process variables</th>
<th align="left">Engineering units</th>
<th align="left">Operation units</th>
</tr>
</thead>
<tbody>
<tr>
<td>A-91921 flue gas O<sub>2</sub> content</td>
<td>mol%</td>
<td>Incinerator common stacks</td>
</tr>
<tr>
<td>A-91921 flue gas NO<sub>x</sub> content</td>
<td>mg/nm<sup>3</sup></td>
<td>Incinerator common stacks</td>
</tr>
<tr>
<td>A-91921 flue gas SO<sub>2</sub> content</td>
<td>ppmv</td>
<td>Incinerator common stacks</td>
</tr>
<tr>
<td>A-91921 CO content</td>
<td>mg/m<sup>3</sup></td>
<td>Incinerator common stacks</td>
</tr>
<tr>
<td><bold>A-91921 CO<sub>2</sub> content</bold></td>
<td><bold>%</bold></td>
<td><bold>Incinerator common stacks</bold></td>
</tr>
<tr>
<td>A-91921 flue gas H<sub>2</sub>S content</td>
<td>ppm</td>
<td>Incinerator common stacks</td>
</tr>
<tr>
<td>SO<sub>3</sub> content</td>
<td>ppm</td>
<td>Incinerator common stacks</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>However, the relationship can be proven by correlation analysis and by studying the process flow and material balance of the feed and product gases. Before modelling, data is pre-processed to remove bad inputs and non-values. For ANN modelling, 70% of samples are used for training, 15% for validation and 15% for testing; the SVR model uses 80% for training and cross-validation, and 20% for testing.</p>
</sec>
</sec>
<sec id="section10-0142331211425404">
<title>Results and analysis</title>
<p>The individual model performance of BPNN and LS-SVR will be presented in this section. <xref ref-type="table" rid="table3-0142331211425404">Tables 3</xref> and <xref ref-type="table" rid="table4-0142331211425404">4</xref> presents a comparison of the performance of the models in terms of correlation, root means squared and accuracy of the predicted values. The performance of both models in terms of computational time is also shown. <xref ref-type="fig" rid="fig5-0142331211425404">Figures 5</xref>, <xref ref-type="fig" rid="fig6-0142331211425404">6</xref>, <xref ref-type="fig" rid="fig7-0142331211425404">7</xref> and <xref ref-type="fig" rid="fig8-0142331211425404">8</xref> show distribution of real and predicted results. This will be followed by a brief analysis of the results obtained. The performance of the LS-SVR model depends greatly upon the values of the hyper-parameters selected. The initial values for the hyper-parameters are: <inline-formula id="inline-formula33-0142331211425404"><mml:math display="inline" id="math56-0142331211425404"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> =34.84 and <inline-formula id="inline-formula34-0142331211425404"><mml:math display="inline" id="math57-0142331211425404"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> =11.89, and was computed using SA. These values were then used for further optimization using the NM simplex algorithm. The objective function is to minimize model error (MSE) based on several constraints such as the maximum number of optimization steps, the maximum number of function evaluations, the stopping criterion based on the relative change in value of the function in each step and the stopping criterion based on the change in the minimizer in each step. <xref ref-type="table" rid="table5-0142331211425404">Table 5</xref> shows the simplex computation for this model. The optimized values for the hyper-parameters are:</p>
<p>
<disp-formula id="disp-formula24-0142331211425404">
<mml:math display="block" id="math58-0142331211425404">
<mml:mrow>
<mml:mi>γ</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>32</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>96</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula24-0142331211425404" xlink:href="10.1177_0142331211425404-eq24.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula25-0142331211425404">
<mml:math display="block" id="math59-0142331211425404">
<mml:mrow>
<mml:mi>σ</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>11</mml:mn>
<mml:mo>.</mml:mo>
<mml:mn>43</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula25-0142331211425404" xlink:href="10.1177_0142331211425404-eq25.tif"/>
</disp-formula>
</p>
<table-wrap id="table3-0142331211425404" position="float">
<label>Table 3</label>
<caption><p>Performance of the back-propagation neural network model</p></caption>
<graphic alternate-form-of="table3-0142331211425404" xlink:href="10.1177_0142331211425404-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">RMSE</th>
<th align="left"><italic>R</italic></th>
<th align="left">Accuracy</th>
<th align="left">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training data</td>
<td>0.4216</td>
<td>0.8549</td>
<td>92.71%</td>
<td>21.0 s</td>
</tr>
<tr>
<td>Testing data</td>
<td>0.3787</td>
<td>0.8919</td>
<td>93.20%</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0142331211425404">
<p>RMSE, root mean square error.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table4-0142331211425404" position="float">
<label>Table 4</label>
<caption>
<p>Performance of least squares support vector regression model</p>
</caption>
<graphic alternate-form-of="table4-0142331211425404" xlink:href="10.1177_0142331211425404-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">RMSE</th>
<th align="left"><italic>R</italic></th>
<th align="left">Accuracy</th>
<th align="left">Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training data</td>
<td>0.0806</td>
<td>0.9955</td>
<td>98.76%</td>
<td>65.1 s</td>
</tr>
<tr>
<td>Testing data</td>
<td>0.3090</td>
<td>0.9330</td>
<td>94.64%</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0142331211425404">
<p>RMSE, root mean square error.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<fig id="fig5-0142331211425404" position="float">
<label>Figure 5</label>
<caption>
<p>CO<sub>2</sub> training data for back-propagation neural network.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig5.tif"/>
</fig>
<fig id="fig6-0142331211425404" position="float">
<label>Figure 6</label>
<caption>
<p>CO<sub>2</sub> testing data for back-propagation neural network.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig6.tif"/>
</fig>
<fig id="fig7-0142331211425404" position="float">
<label>Figure 7</label>
<caption>
<p>CO<sub>2</sub> training data for least squares support vector regression.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig7.tif"/>
</fig>
<fig id="fig8-0142331211425404" position="float">
<label>Figure 8</label>
<caption>
<p>CO<sub>2</sub> testing data for least squares support vector regression.</p>
</caption>
<graphic xlink:href="10.1177_0142331211425404-fig8.tif"/>
</fig>
<table-wrap id="table5-0142331211425404" position="float">
<label>Table 5</label>
<caption>
<p>Hyper-parameters optimization steps</p>
</caption>
<graphic alternate-form-of="table5-0142331211425404" xlink:href="10.1177_0142331211425404-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Iteration</th>
<th align="left">Function evaluation</th>
<th align="left">Min <italic>f(x)</italic></th>
<th align="left">Log(γ)</th>
<th align="left">Log(σ)</th>
<th align="left">Procedure</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>3</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Initial</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract inside</td>
</tr>
<tr>
<td>3</td>
<td>7</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract outside</td>
</tr>
<tr>
<td>4</td>
<td>9</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract inside</td>
</tr>
<tr>
<td>5</td>
<td>10</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Reflect</td>
</tr>
<tr>
<td>6</td>
<td>12</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract inside</td>
</tr>
<tr>
<td>7</td>
<td>14</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract outside</td>
</tr>
<tr>
<td>8</td>
<td>16</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract inside</td>
</tr>
<tr>
<td>9</td>
<td>17</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Reflect</td>
</tr>
<tr>
<td>10</td>
<td>18</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Reflect</td>
</tr>
<tr>
<td>11</td>
<td>20</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract inside</td>
</tr>
<tr>
<td>12</td>
<td>22</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Contract inside</td>
</tr>
<tr>
<td>13</td>
<td>23</td>
<td>1.413835e-001</td>
<td>3.5508</td>
<td>2.4753</td>
<td>Reflect</td>
</tr>
<tr>
<td>14</td>
<td>25</td>
<td>1.413743e-001</td>
<td>3.4954</td>
<td>2.4364</td>
<td>Reflect</td>
</tr>
<tr>
<td>15</td>
<td>27</td>
<td>1.413743e-001</td>
<td>3.4954</td>
<td>2.4364</td>
<td>Contract inside</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The CO<sub>2</sub> emission as shown in the above graphs presents a highly non-linear distribution; hence it takes a model with the capabilities to map these non-linearities with precision and accuracy. As shown in <xref ref-type="table" rid="table3-0142331211425404">Tables 3</xref> and <xref ref-type="table" rid="table4-0142331211425404">4</xref>, the performance of the LS-SVR model is clearly superior in both training and testing data compared with the BPNN model. The graphs also show that the LS-SVR model is able to track and predict the CO<sub>2</sub> emission with much better accuracy than BPNN model. The accuracy achieved by both models is above 90%, which satisfies the conditions under the US EPA Performance Specification 16 for Predictive Emission Monitoring Systems (<xref ref-type="bibr" rid="bibr19-0142331211425404">2008</xref>). The models developed can be used in real world applications. LS-SVR is a much more ‘transparent’ algorithm, where the resultant model has weights and biases that were optimized to suit the particular dataset. Another plausible explanation is that, with BPNN, the initial weights are chosen randomly and hence the performance varies significantly with the number of simulations as shown by <xref ref-type="bibr" rid="bibr26-0142331211425404">Zheng et al. (2008)</xref>. In contrast, LS-SVR initial parameters are optimized via SA. The solutions provided by SVMs are unique and global in a dual space problem; this is achieved by solving the Karush–Kuhn Tucker (KKT) system at its core. The generalization ability of LS-SVM could be further improved by increasing the number of samples used for validation and reducing the stopping criteria when training using cross-validation. SVM algorithms are a kernel-based method; hence the complexity is in the order of <inline-formula id="inline-formula35-0142331211425404"><mml:math display="inline" id="math60-0142331211425404"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which means that the computational power and required running time is higher than the neural network-based method.</p>
<p>The LS-SVR solution comes from solving the linear system in <xref ref-type="disp-formula" rid="disp-formula17-0142331211425404">Equation (16)</xref> by matrix inversion; this is a fairly tedious process to be solved numerically and results in a longer overall computation time, as shown in <xref ref-type="table" rid="table4-0142331211425404">Table 4</xref>. Hence, the usability of LS-SVR depends heavily upon the number of samples used and the processing power of the machine that it currently runs on. To overcome this, several fixes have been proposed such as using quadratic Renyi entropy to select the most significant support vectors (<xref ref-type="bibr" rid="bibr12-0142331211425404">Jiang et al., 2008</xref>), sequential learning LS-SVM (<xref ref-type="bibr" rid="bibr13-0142331211425404">Jung and Polani, 2008</xref>), and using analytic QP and sparseness (<xref ref-type="bibr" rid="bibr15-0142331211425404">Platt, 1999</xref>). One of the weaknesses of the NM optimizer as shown in <xref ref-type="table" rid="table5-0142331211425404">Table 5</xref> is that it only searches one better point (i.e. min f(x)=1.413743 e-001) in 15 iterations. This could be improved by increasing the expansion sizes that includes the contraction, reflection and shrinkage. In the future, a more complex optimizer will be considered to perform the hyper-parameters optimization. In <xref ref-type="table" rid="table3-0142331211425404">Table 3</xref>, it can be seen that the testing performance of BPNN is better than training, which shows a good generalization ability, but to reduce overgeneralization, the number of training samples could be improved, while reducing validation and testing samples.</p>
</sec>
<sec id="section11-0142331211425404">
<title>Conclusion and recommendations</title>
<p>In a nutshell, this paper has brought into focus the ability to model the CO<sub>2</sub> emission from an acid gas incinerator using an NM LS-SVR algorithm. The modelling was done using a series of steps that includes optimizing initial hyper-parameters and further optimization using a NM simplex algorithm. As a result, the LS-SVR model performs significantly better for both training and testing data compared with the benchmark BPNN model.</p>
<p>Additionally, the ability to track and predict future values of CO<sub>2</sub> emission with remarkable accuracy has also been demonstrated by the LS-SVR model. The superiority of the LS-SVR model can be accounted for by the ability to reach a global and unique solution for the minimization of a convex function. The proposed algorithm is robust in terms of the class of the problem that it can handle, be it classification or complex regression with multiple dimensions. However, one of the drawbacks of LS-SVR includes heavy computational dependencies when it comes to large datasets. Nevertheless, this could be improved using newer techniques. Overall, the primary objective of the paper has been achieved, and in the future, a hybrid model that involves SVMs with complex optimization will be developed in order to improve further the predictive accuracy and computational efficiencies.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Baines</surname><given-names>G</given-names></name>
</person-group> (<year>1999</year>) <source>Neural Networks for Boiler Emission Prediction</source>. <publisher-loc>Austin, TX</publisher-loc>: <publisher-name>Performance Consultant Fisher-Rosemount Solutions</publisher-name>, <fpage>435</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr2-0142331211425404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chakravarthy</surname><given-names>SSS</given-names></name>
<name><surname>Vohra</surname><given-names>AK</given-names></name>
<name><surname>Gill</surname><given-names>BS</given-names></name>
</person-group> (<year>2000</year>) <article-title>Predictive emission monitors (PEMS) for NOx generation in process heaters</article-title>. <source>Computers and Chemical Engineering</source> <volume>23</volume>: <fpage>1649</fpage>–<lpage>59</lpage>.</citation>
</ref>
<ref id="bibr3-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cristianini</surname><given-names>N</given-names></name>
<name><surname>Shawe-Taylor</surname><given-names>J</given-names></name>
</person-group> (<year>2000</year>) <source>An Introduction to Support Vector Machine and Other Kernel-based Learning Method</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Deng</surname><given-names>Jiamei</given-names></name>
<name><surname>Stobart</surname><given-names>R</given-names></name>
</person-group> (<year>2007</year>) <source>Combined Hybrid Clustering Techniques and Neural Fuzzy Networks to Predict Diesel Engine Emissions</source>. <publisher-loc>Brighton</publisher-loc>: <publisher-name>IEEE</publisher-name>.</citation>
</ref>
<ref id="bibr5-0142331211425404">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Dong</surname><given-names>Dong</given-names></name>
<name><surname>McAvoy</surname><given-names>TJ</given-names></name>
</person-group> (<year>1995</year>) <source>Emission Monitoring Using Multivariate Soft Sensors</source>. <conf-name>American Control Conference</conf-name>, <conf-loc>Seattle, WA</conf-loc>.</citation>
</ref>
<ref id="bibr6-0142331211425404">
<citation citation-type="book">
<collab>Environmental Quality (Clean Air) Regulations</collab> <year>1978</year> (<year>2000</year>) <article-title>Federal Subsidiary Legislation Environmental Quality Act 1974 [ACT 127]</article-title>. <publisher-loc>Malaysia</publisher-loc>: <publisher-name>Department of Environment</publisher-name>.</citation>
</ref>
<ref id="bibr7-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fan</surname><given-names>Xiaoliang</given-names></name>
<name><surname>Zheng</surname><given-names>Haiming</given-names></name>
</person-group> (<year>2009</year>) <source>Design CEMS for Flue Gas from Thermal Power Plant</source>. <publisher-loc>Baoding</publisher-loc>: <publisher-name>IEEE</publisher-name>.</citation>
</ref>
<ref id="bibr8-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gunn</surname><given-names>SR</given-names></name>
</person-group> (<year>1998</year>) <source>Support Vector Machines for Classification and Regression</source>. Technical Report, <publisher-name>Faculty of Engineering, Science and Mathematics, School of Electronics and Computer Science, University of Southampton</publisher-name>, <fpage>1</fpage>–<lpage>66</lpage>.</citation>
</ref>
<ref id="bibr9-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Haykins</surname><given-names>S</given-names></name>
</person-group> (<year>1999</year>) <source>Neural Networks: A Comprehensive Foundation</source>. <edition>2nd ed.</edition> <publisher-loc>Englewood Cliﬀs, NJ</publisher-loc>: <publisher-name>Prentice-Hall</publisher-name>.</citation>
</ref>
<ref id="bibr10-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hsu</surname><given-names>Chih-Wei</given-names></name>
<name><surname>Chang</surname><given-names>Chih-Chung</given-names></name>
<name><surname>Lin</surname><given-names>Chih-Jen</given-names></name>
</person-group> (<year>2010</year>) <source>A Practical Guide to Support Vector Classification</source>. <publisher-loc>Taiwan</publisher-loc>: <publisher-name>Department of Computer Science, National Taiwan University, Taipei</publisher-name>.</citation>
</ref>
<ref id="bibr11-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jamaludin</surname><given-names>S</given-names></name>
<name><surname>Manaf</surname><given-names>S</given-names></name>
<name><surname>Oortwin</surname><given-names>P</given-names></name>
</person-group> (<year>2005</year>) <source>World Largest Tangentially Fired Acid Gas Incinerator in LNG Plant: Startup and Operational Experiences</source>. Technical Report, <publisher-loc>Malaysia</publisher-loc>: <publisher-name>LNG Sdn. Bhd &amp; Shell Global Solutions International</publisher-name>.</citation>
</ref>
<ref id="bibr12-0142331211425404">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Jiang</surname><given-names>Jingqing</given-names></name>
<name><surname>Song</surname><given-names>Chuyi</given-names></name>
<name><surname>Haiyan</surname><given-names>Zhao</given-names></name>
<name><surname>Wu</surname><given-names>Chunguo</given-names></name>
<name><surname>Yanchun</surname><given-names>Liang</given-names></name>
</person-group> (<year>2008</year>) <article-title>Adaptive and iterative least squares support vector regression based on quadratic Renyi entropy</article-title>. <conf-name>IEEE International Conference on Granular Computing</conf-name>, <conf-date>26–28 August</conf-date>, <conf-loc>Beijing, IEEE</conf-loc>.</citation>
</ref>
<ref id="bibr13-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jung</surname><given-names>T</given-names></name>
<name><surname>Polani</surname><given-names>D</given-names></name>
</person-group> (<year>2008</year>) <source>Sequential Learning with LS-SVM for Large-Scale Data Sets</source>. Technical Report, <publisher-name>Departmentt of Computer Science, University of Mainz</publisher-name>, <publisher-loc>Mainz</publisher-loc>.</citation>
</ref>
<ref id="bibr14-0142331211425404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kirkpatrick</surname><given-names>S</given-names></name>
<name><surname>Gelatt</surname><given-names>CD</given-names></name>
<name><surname>Vecchi</surname><given-names>MP</given-names></name>
</person-group> (<year>1983</year>) <article-title>Optimization by simulated annealing</article-title>. <source>Science, New Series</source> <volume>220</volume>: <fpage>671</fpage>–<lpage>80</lpage>.</citation>
</ref>
<ref id="bibr15-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Platt</surname><given-names>JC</given-names></name>
</person-group> (<year>1999</year>) <article-title>Using analytic QP and sparseness to speed training of support vectormachines</article-title>. In: <person-group person-group-type="editor">
<name><surname>Kearns</surname><given-names>MS</given-names></name>
<name><surname>Solla</surname><given-names>SA</given-names></name>
<name><surname>Cohn</surname><given-names>DA</given-names></name>
</person-group> (eds) <source>Advances in Neural Information Processing Systems</source> <volume>11</volume>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr16-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Press</surname><given-names>WH</given-names></name>
<name><surname>Teukolsky</surname><given-names>SA</given-names></name>
<name><surname>Vetterling</surname><given-names>WT</given-names></name>
<name><surname>Flannery</surname><given-names>BP</given-names></name>
</person-group> (<year>1998</year>) <source>Numerical Recipes in C</source>. <edition>2nd ed.</edition> <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Suykens</surname><given-names>JAK</given-names></name>
<name><surname>Gestel</surname><given-names>TV</given-names></name>
<name><surname>Brabenter</surname><given-names>JD</given-names></name>
<name><surname>Moor</surname><given-names>BD</given-names></name>
<name><surname>Vandewelle</surname><given-names>J</given-names></name>
</person-group> (<year>2002</year>) <source>Least Squares Support Vector Machines</source>. <publisher-loc>Hackensack, NJ</publisher-loc>: <publisher-name>World Scientific Publishing</publisher-name>.</citation>
</ref>
<ref id="bibr18-0142331211425404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tronci</surname><given-names>S</given-names></name>
<name><surname>Baratti</surname><given-names>R</given-names></name>
<name><surname>Servida</surname><given-names>A</given-names></name>
</person-group> (<year>2002</year>) <article-title>Monitoring pollutant emissions in a 4.8 MW power plant through neural network</article-title>. <source>Neurocomputing</source> <volume>43</volume>: <fpage>3</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr19-0142331211425404">
<citation citation-type="book">
<collab>US Environmental Protection Agency</collab> (<year>2008</year>) <article-title>Clean Air Act as of 2008</article-title>. <source>Chapter 85: Air Pollution Prevention and Control</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>US Government Printing Office</publisher-name>.</citation>
</ref>
<ref id="bibr20-0142331211425404">
<citation citation-type="gov">
<collab>US Environmental Protection Agency</collab> (<year>2011</year>) <article-title>Climate Change - Health and Environmental Effects</article-title>. US EPA Home Page. [Online] [Cited: <day>8</day> <month>April</month> <year>2011</year>] <ext-link ext-link-type="uri" xlink:href="http://www.epa.gov/climatechange/effects/index.html">http://www.epa.gov/climatechange/effects/index.html</ext-link>.</citation>
</ref>
<ref id="bibr21-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vanderbei</surname><given-names>RJ</given-names></name>
</person-group> (<year>1998</year>) <source>LOQO: An Interior Point Code for Quadratic Programming</source>. Technical Report, <publisher-name>Statistics and Operations Research, Princeton University</publisher-name>.</citation>
</ref>
<ref id="bibr22-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vapnik</surname><given-names>V</given-names></name>
</person-group> (<year>1995</year>) <source>The Nature of Statistical Learning Theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr23-0142331211425404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vapnik</surname><given-names>V</given-names></name>
<name><surname>Golowich</surname><given-names>S</given-names></name>
<name><surname>Smola</surname><given-names>A</given-names></name>
</person-group> (<year>1997</year>) <article-title>Support vector method for function approximation, regression estimation, and signal processing</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>9</volume>: <fpage>281</fpage>–<lpage>7</lpage>.</citation>
</ref>
<ref id="bibr24-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>Haiming</given-names></name>
<name><surname>Tang</surname><given-names>Guiji</given-names></name>
</person-group> (<year>2008</year>) <source>Developing Data Acquisition and Handling System for Continous Emission Monitoring System from Coal-Fired Power Plant</source>. <publisher-loc>Baoding, China</publisher-loc>: <publisher-name>IEEE</publisher-name>.</citation>
</ref>
<ref id="bibr25-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>Ligang</given-names></name>
<name><surname>Jia</surname><given-names>Hailin</given-names></name>
<name><surname>Yu</surname><given-names>Shuijun</given-names></name>
<name><surname>Yu</surname><given-names>Minggao</given-names></name>
</person-group> (<year>2010</year>) <source>Prediction of NOx Concentration from Coal Combustion Using LS-SVR</source>. <publisher-loc>Jiaozuo Henan</publisher-loc>: <publisher-name>IEEE</publisher-name>, <fpage>1</fpage>–<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr26-0142331211425404">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>Ligang</given-names></name>
<name><surname>Yu</surname><given-names>Shuijun</given-names></name>
<name><surname>Yu</surname><given-names>Minggao</given-names></name>
</person-group> (<year>2008</year>) <source>Monitoring NOx Emissions from Coal-Fired Boilers using Generalized Regression Neural Network</source>. <publisher-loc>Jiaozuo Henan</publisher-loc>: <publisher-name>IEEE</publisher-name>, <fpage>1916</fpage>–<lpage>19</lpage>.</citation>
</ref>
<ref id="bibr27-0142331211425404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>Hao</given-names></name>
<name><surname>Cen</surname><given-names>Kefa</given-names></name>
<name><surname>Fan</surname><given-names>Jianren</given-names></name>
</person-group> (<year>2004</year>) <article-title>Modeling and optimization of the NOx emission characteristics of a tangentially ﬁredboiler with artiﬁcial neural networks</article-title>. <source>Energy</source> <volume>29</volume>: <fpage>167</fpage>–<lpage>83</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>