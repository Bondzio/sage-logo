<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ARP</journal-id>
<journal-id journal-id-type="hwp">sparp</journal-id>
<journal-title>The American Review of Public Administration</journal-title>
<issn pub-type="ppub">0275-0740</issn>
<issn pub-type="epub">1552-3357</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0275074011399120</article-id>
<article-id pub-id-type="publisher-id">10.1177_0275074011399120</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Can a Single Performance Metric Do It All? A Case Study in Education Accountability</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Kukla-Acevedo</surname><given-names>Sharon</given-names></name>
<xref ref-type="aff" rid="aff1-0275074011399120">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Streams</surname><given-names>Megan E.</given-names>
</name>
<xref ref-type="aff" rid="aff2-0275074011399120">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Toma</surname><given-names>Eugenia</given-names></name>
<xref ref-type="aff" rid="aff3-0275074011399120">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-0275074011399120"><label>1</label>Central Michigan University, Mt. Pleasant</aff>
<aff id="aff2-0275074011399120"><label>2</label>Tennessee State University, Nashville</aff>
<aff id="aff3-0275074011399120"><label>3</label>University of Kentucky, Lexington</aff>
<author-notes>
<corresp id="corresp1-0275074011399120">Sharon Kukla-Acevedo, Central Michigan University, Department of Political Science, 247 Anspach Hall, Mt. Pleasant, MI Email: <email>kukla1sa@cmich.edu</email></corresp>
<fn fn-type="other" id="bio1-0275074011399120">
<p>Sharon Kukla-Acevedo is an Assistant Professor in the Political Science Department at Central Michigan University. Her research combines the fields of public management and education policy, with current research endeavors focusing on superintendents as public managers. Ongoing interests also include the evaluation of teacher effectiveness, evaluation of teacher preparation programs, teacher turnover and mobility.</p></fn>
<fn fn-type="other" id="bio2-0275074011399120">
<p>Megan E. Streams is an assistant professor of public administration in the College of Public Service and Urban Affairs at Tennessee State University. Her research interests are public finance and budgeting, with a policy interest in education; topics include the interaction of K-12 and higher education policies, impact of demographic trends on education finance, and teacher compensation and career trajectories. She has professional experience in higher education administration and budgeting as well as performance measurement in public agencies and survey methods.</p></fn>
<fn fn-type="other" id="bio3-0275074011399120">
<p>Eugenia F. Toma is the Wendell H. Ford Professor of Public Policy in the Martin School of Public Policy &amp; Administration at the University of Kentucky. She specializes in the economics of schooling. As a Fulbright Fellow to New Zealand in 1992, Toma worked with The Treasury and the Ministry of Education to analyze school reform. In 1998-99, she served as economics program director for the National Science Foundation (NSF) and participated in a multi-agency initiative to enhance scientific inquiry in education research. She currently is serving as PI of an NSF grant to evaluate the effectiveness of teacher professional development programs.</p></fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>42</volume>
<issue>3</issue>
<fpage>303</fpage>
<lpage>319</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Public administrators are committed to improving public service delivery, as evidenced by decades of accountability efforts at all levels of government. This movement is especially salient in the public education system, where student standardized test scores are increasingly used as the key performance metric to evaluate schools, teachers - and most recently - teacher preparation program (TPP) effectiveness. Evaluating TPPs using a single quantitative performance metric at the student level is a complicated endeavor. This paper illustrates a key challenge in this type of accountability system, not yet examined in the literature: graduates of individual TPPs tend to cluster in a very small number of districts. We present a case study to show how geographic stratification inhibits the ability of statistical models to disentangle the effect of district and school from TPP on student achievement, particularly in rural states.</p>
</abstract>
<kwd-group>
<kwd>teacher preparation program</kwd>
<kwd>accountability</kwd>
<kwd>student achievement</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The public management literature has long suggested that one means of improving the delivery of public services is through enhanced accountability, particularly through performance measurement. There is widespread agreement with this idea both within the academy and, increasingly, among public leaders. But there exists a potentially deep divide between the idea of accountability and its effective implementation. The choice of metrics used to hold agencies accountable is partly responsible for this divide. Research suggests that selecting or designing appropriate measures is an extremely difficult task for public agencies at all levels (<xref ref-type="bibr" rid="bibr36-0275074011399120">U.S. General Accounting Office, 1997</xref>; <xref ref-type="bibr" rid="bibr37-0275074011399120">Wang, 2000</xref>). Performance metrics are expected to be most successful when they are closely aligned to the stated goals of the organization (<xref ref-type="bibr" rid="bibr14-0275074011399120">Hatry, 1999</xref>), indicative of actual performance (<xref ref-type="bibr" rid="bibr16-0275074011399120">Heckman, Heinrich, &amp; Smith, 1999</xref>), and difficult to game (<xref ref-type="bibr" rid="bibr1-0275074011399120">Baker, 2002</xref>). In addition, a single piece of information cannot meet the complex needs of multiple decision makers with sometimes conflicting goals (<xref ref-type="bibr" rid="bibr26-0275074011399120">Moore &amp; Braga, 2003</xref>; <xref ref-type="bibr" rid="bibr33-0275074011399120">Radin, 2000</xref>). Finally, governments use performance metrics to improve organizational performance. Yet, if the metric measures the efforts of individuals, it becomes very difficult to establish a relationship between individual and organizational behavior (<xref ref-type="bibr" rid="bibr8-0275074011399120">DeNisi, 2000</xref>; <xref ref-type="bibr" rid="bibr35-0275074011399120">Taylor, 2009</xref>).</p>
<p>This article focuses on the challenges of using a single performance metric, K-12 student test scores, to evaluate teacher preparation programs (TPPs) of institutions of higher education. The evaluation of these programs with test scores as the metric has become increasingly popular following the passage of <italic>No Child Left Behind.</italic> At least four states, including Tennessee, Ohio, Massachusetts, and Louisiana, have undertaken comprehensive efforts to create datasets and projects that will evaluate the effects of TPPs using student achievement data.</p>
<p>Using a case study from the state of Kentucky, we argue that there are inherent problems in using test scores to evaluate TPPs that go beyond the issues previously raised in the public administration literature. These problems are related to characteristics of teacher labor markets that create more than the usual challenges for designing a meaningful performance metric. Teachers tend to have well-defined geographic preferences as to where they want to work after graduation, related strongly to their hometown (<xref ref-type="bibr" rid="bibr5-0275074011399120">Boyd, Lankford, &amp; Loeb, 2005</xref>).<sup><xref ref-type="fn" rid="fn1-0275074011399120">1</xref></sup> These geographic preferences create analytical issues, and especially intractable issues, in rural states, where the tendency of graduates of individual TPPs to cluster in a very small number of districts will be more pronounced. We refer to this tendency as stratification. The evaluation problems for rural states are often overlooked in the literature but the majority—nearly 56%—of public school districts in the U.S. are located in rural areas and over 10.3 million students attend rural public schools (<xref ref-type="bibr" rid="bibr32-0275074011399120">Provasnik et al., 2007</xref>). We will demonstrate that the stratification phenomenon limits the extent to which states can use student outcome data to identify the role of the TPPs in contributing to the effectiveness of a teacher. The implications of our case study extend not only to other states but also to other arenas of the public sector, including the evaluation of higher education disciplines of all types.</p>
<sec id="section1-0275074011399120">
<title>Previous Work</title>
<p>Public education provides an excellent context to explore the challenges in performance metric implementation. Education is the single largest expenditure item of state and local governments,<sup><xref ref-type="fn" rid="fn2-0275074011399120">2</xref></sup> and incorrect measures could drive substantial misuse of state funds and misdirection in identifying highly performing public institutions. To the extent student test scores are made public and used, external stakeholders like parents and state policymakers seek to use them to evaluate each level (teachers, schools, districts; <xref ref-type="fig" rid="fig1-0275074011399120">Figure 1</xref>). The desire to evaluate yet another element in the educational process—the TPP—is motivating state policymakers to extract yet another layer of information out of student test scores: Which TPPs in the state produce teachers who consistently add the most value to their students’ achievements?</p>
<fig id="fig1-0275074011399120" position="float">
<label>Figure 1.</label>
<caption><p>Educational accountability of multiple agents based on the single performance measure of student test scores</p>
<p>Note: Value added to student achievement in a single year, measured by standardized testing (controlling for past student achievement, and thus for time-invariant household and student ability effects), serves as the input to evaluation of a series of nested public agents, some of which serve as principals in their turn. A new wave of accountability efforts is also directed at evaluation of teacher preparation programs (TPPs) by the state based on this same outcome measure (dashed arrow on left side of figure).</p></caption>
<graphic xlink:href="10.1177_0275074011399120-fig1.tif"/>
</fig>
<p>Meaningful program accountability requires that the performance metrics are relevant to the program <italic>level</italic> (<xref ref-type="bibr" rid="bibr8-0275074011399120">DeNisi, 2000</xref>; <xref ref-type="bibr" rid="bibr11-0275074011399120">Gormley &amp; Weimer, 1999</xref>; <xref ref-type="bibr" rid="bibr20-0275074011399120">Jennings &amp; Haist, 2004</xref>). In other words, to hold TPPs accountable for the performance of their graduates, the TPP needs to have a meaningful influence on what the teachers are doing in the classroom. <xref ref-type="bibr" rid="bibr23-0275074011399120">Lipsky’s (1980)</xref> classic service-delivery level research uses teachers to exemplify “street-level bureaucrats” who play a critical role in implementation. The multiple sources of discretion that teachers use in the classroom, the multitude of contextual influences (including peer teachers, continuing education/professional development experiences, and curricular and instructional guidance received at the school and district level), and postgraduate training all affect the teachers’ performance. These factors may become more influential for the teachers’ classroom behaviors than the undergraduate training as time elapses following graduation. In particular, a principal who is responsible for the ongoing personnel decisions that affect the teacher may have a higher degree of influence over behaviors that the teacher exhibits in the classroom than the TPP. Performance information in the form of student test scores has a most direct and meaningful link when used by principals. A similar argument can be made that parents and superintendents have a more meaningful ongoing influence on service delivery by teachers than their TPPs.</p>
<p>To accurately evaluate teachers/schools/TPPs, the performance management research suggests that that the metrics must reflect the <italic>true objectives</italic> of the organization as closely as possible (<xref ref-type="bibr" rid="bibr1-0275074011399120">Baker, 2002</xref>; <xref ref-type="bibr" rid="bibr16-0275074011399120">Heckman et al., 1999</xref>). If the true objective of the organization is to create meaningful learning or to educate productive citizens, then the performance measure or measures used should capture that. However, as in many program evaluations, K-12 education accountability models must rely on proxies for the true desired outcome of actual student learning (<xref ref-type="bibr" rid="bibr11-0275074011399120">Gormley &amp; Weimer, 1999</xref>). Standardized student test scores generally serve as the proxy in the models used to measure teacher and school effectiveness. Disjuncture between the proxy and the desired performance objective can become quite problematic. <xref ref-type="bibr" rid="bibr15-0275074011399120">Heckman, Heinrich, and Smith (1997)</xref> show that accountability systems that rely on proxies may encourage short-term gains in the proxies at the expense of the long-term gains in the desired outcome. In the education arena, this concern is frequently described as “teaching to the test”: teachers may be engaging in behaviors that result in immediate improvement of student test scores, but students’ long-term learning may suffer if, for example, teachers displace teaching in-depth content with test-taking skills. Studies indicate that teachers do change their classroom behaviors in attempts to maximize their students’ performance on the test (<xref ref-type="bibr" rid="bibr10-0275074011399120">Figlio &amp; Rouse, 2006</xref>; <xref ref-type="bibr" rid="bibr18-0275074011399120">Jacob, 2005</xref>). This type of behavior is problematic both in individual teacher accountability systems and when trying to assess TPP performance. Rather than identifying true effectiveness in promoting student achievement, the estimates of teacher effect on score gains may simply be indications of the teachers’ success in teaching test-taking skills or narrowing the classroom focus to test content—or the TPP’s ability to prepare teachers to do this. The proxy measurement issue is accentuated in this case by the multiple layers of agent and agency responsibility that lie between the student in the classroom and the TPP.</p>
<p>Accountability systems, along with their beneficial effects, are likely to create situations where stakeholders have the incentive to “game” the system (Courty &amp; Marschke, 1997). In other words, agencies act in ways that make it appear as though their agency is improving its performance, when in reality it is not (<xref ref-type="bibr" rid="bibr17-0275074011399120">Heinrich, 2002</xref>). Schools may take strategic actions to demonstrate undeserved student gains by excluding certain low-performing students from taking the tests through reclassification or suspension (<xref ref-type="bibr" rid="bibr7-0275074011399120">Cullen &amp; Reback, 2006</xref>; <xref ref-type="bibr" rid="bibr9-0275074011399120">Figlio &amp; Getzler, 2006</xref>; <xref ref-type="bibr" rid="bibr18-0275074011399120">Jacob, 2005</xref>). When schools select the higher achieving test takers during the accountability cycle, they attribute artificial gains to teachers, schools, or—if student test data are used to evaluate them—TPPs.</p>
</sec>
<sec id="section2-0275074011399120">
<title>Conceptual Model and Description of Case Data</title>
<p>The teacher accountability literature provides considerable methodological guidance to researchers who are attempting TPP accountability models using student test scores. Research on TPP accountability uses value-added models (VAMs) to estimate the contribution that a TPP makes to student achievement (<xref ref-type="bibr" rid="bibr4-0275074011399120">Boyd, Grossman, Lankford, Loeb, &amp; Wyckoff, 2009</xref>; <xref ref-type="bibr" rid="bibr29-0275074011399120">Noell, 2006</xref>). The value-added approach is attractive to researchers and decision makers because it nets out unchanging parental and student characteristic contributions to score gains. Because of this property, it is generally the first candidate to be considered when an input to internal evaluation processes is needed. <xref ref-type="bibr" rid="bibr29-0275074011399120">Noell (2006)</xref>, <xref ref-type="bibr" rid="bibr30-0275074011399120">Noell, Porter, and Patt (2007)</xref>, <xref ref-type="bibr" rid="bibr31-0275074011399120">Noell, Porter, Patt, and Dahir (2008)</xref>, and <xref ref-type="bibr" rid="bibr4-0275074011399120">Boyd et al. (2009)</xref> used these models to address many of the issues with accountability metrics that are described above. Yet neither has fully addressed the challenge of geographic stratification of TPP graduates, which is the focus of the remainder of this article.</p>
<p>For example, <xref ref-type="bibr" rid="bibr29-0275074011399120">Noell (2006)</xref> looks at the New Orleans school district and notes that it employed new teachers from 13 TPPs, the most of any district, but this only represented about 50% of the TPPs in the sample. The stratification of teachers is problematic because the unmeasured characteristics of the district may confound the TPP effect. For example, New Orleans public schools may have unique unmeasured characteristics that are correlated with its measured characteristics, such as being historically low performing (<xref ref-type="bibr" rid="bibr24-0275074011399120">Louisiana Recovery School Districts, n.d.</xref>). If new graduates of a particular TPP do not teach in New Orleans schools, then it may be inappropriate to indicate that one has measured an average teacher effect of that institution’s graduates. In fact, their measured effect with students in this unique school district is unknown, as is whether they are equally as effective with New Orleans public school students as with other districts students.</p>
<p>The <xref ref-type="bibr" rid="bibr4-0275074011399120">Boyd et al. (2009)</xref> study focuses on the New York City public school system. Like the Louisiana research, this study also does not address whether the distribution of recent graduates is geographically stratified across districts. However, it is less likely that stratification would pose as serious a threat to this type of analysis in that geographic region, given its density and scale. The New York City public school system educates over a million students a year and contains over 1,600 schools (<xref ref-type="bibr" rid="bibr27-0275074011399120">New York City Department of Education, 2010</xref>). This type of environment provides so many potential avenues for teachers that New York state TPPs are likely to be represented, to a certain extent, among schools in all five New York City boroughs.</p>
<p>The remainder of this article focuses on the use of VAMs to evaluate TPPs conducted in Kentucky. Kentucky is a particularly valuable state to study because it is relatively rural and relatively poor. With almost 17% of its population in poverty, Kentucky currently is ranked among the four states with the highest poverty rates. It has experienced long-standing achievement gaps between the poorer, rural areas and the higher income urban areas of the state—gaps that many argue are at the root of the huge variation in college attendance, economic prosperity, and health outcomes observed across the state. Kentucky, like other high poverty states, also contains a large number of small, rural school districts: 53% of its school districts and 39% of its students are in rural areas. Although we focus on a single state, the implications for the study apply to many others, particularly to those with large numbers of rural school districts and to those with large gaps in socioeconomic status and student performance.</p>
<p>We contend that the value-added approach does not address the fact that teachers from specific TPPs are not independently distributed across the state: indeed, they show persistent geographic concentration, so that only a small number of TPPs are typically represented in a given district and school (typically, with a single TPP dominating). Although this may seem a narrow technical issue in a single policy area, we suggest that it is a case of a more general complication of performance measurement in a complex real-world setting: where a single performance measure is depended on to provide meaningful evaluation for a series of entities, which may be nested (teacher/school/district) and in series (teacher/TPP; <xref ref-type="fig" rid="fig1-0275074011399120">Figure 1</xref>).</p>
<p>In a typical evaluation to estimate the effects of any intervention, preintervention outcome data are compared to postintervention data, controlling for other factors that are expected to affect the outcome. The coefficient on the intervention variable then estimates the magnitude of the program effect. Conceptually, the model applied to preservice college training is represented by the following<sup><xref ref-type="fn" rid="fn3-0275074011399120">3</xref></sup>:</p>
<p><disp-formula id="disp-formula1-0275074011399120">
<mml:math display="block" id="math1-0275074011399120">
<mml:mrow>
<mml:msub>
<mml:mtext>A</mml:mtext>
<mml:mrow>
<mml:mtext>it</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:msub>
<mml:mtext>A</mml:mtext>
<mml:mrow>
<mml:mtext>it</mml:mtext>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mtext>TPP</mml:mtext>
</mml:mrow>
<mml:mtext>j</mml:mtext>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mtext>Stu</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>it</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>3</mml:mn>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mtext>Tch</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>it</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mtext>Sch</mml:mtext>
</mml:mrow>
<mml:mrow>
<mml:mtext>kt</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mtext>u</mml:mtext>
<mml:mrow>
<mml:mtext>it</mml:mtext>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0275074011399120" xlink:href="10.1177_0275074011399120-eq1.tif"/>
</disp-formula></p>
<p>where <italic>i</italic> indexes students, <italic>t</italic> indexes timepoints, <italic>j</italic> indexes teachers, and <italic>k</italic> indexes schools; A<sub>i</sub> and A<sub>it-1</sub> are standardized student test scores;<sup><xref ref-type="fn" rid="fn4-0275074011399120">4</xref></sup> TPP<sub>j</sub> is an indicator variable designating the teacher’s preparation program; Stu<sub>it</sub> represents student-specific characteristics, such as race, gender, and subsidized lunch eligibility; Tch<sub>ij</sub> includes teacher-specific characteristics, including gender, race, experience, college entrance scores; and Sch<sub>kt</sub> refers to dichotomous variables that control for the unmeasured, time-invariant characteristics of schools within a district (this model is estimated separately by district, a constraint imposed by the geographic stratification problem discussed in full below). Finally, u<sub>it</sub> is a randomly distributed error term. Of primary interest is the estimation of β<italic>
<sub>1</sub>
</italic>, the coefficient on TPP, which can be interpreted as the impact of a particular teacher preservice training institution on student scores, controlling for all other included factors that are expected to influence scores, including student prior scores.</p>
<p>Note that this model requires observations for individual students, a match of those students to their individual teachers, observations of these teacher matches with multiple students, observations of these matches over multiple teachers, knowledge of a teacher’s TPP, and a sufficient number of teachers from multiple TPP programs. The same students must be observed over at least two consecutive time periods so that a pre- and postscore can be calculated. If all factors that influence a student’s score between periods in time are included in the empirical model, then any single estimated coefficient measures the contribution of a specific variable to the child’s level of achievement in the same time period. From this perspective, it initially appears that measuring the value added by a given TPP should be no different than measuring the value added by any other policy intervention, with the exception of the additional data requirements necessary for estimation of the effects of the TPP.</p>
<p>In Kentucky, as in most states at this time, the student–teacher matches are not available in a centralized, state location. States typically retain individual student information and individual teacher information but not in a way that enables the researcher to match the two. Rather, these matches can be made only by obtaining classroom rolls from individual schools or districts. With the approval of the state’s teacher licensing agency, the Education Professional Standards Board (EPSB), three anonymous, randomly selected school districts agreed to provide their 11th-grade classroom rolls from the 2005-2006 school year so that the teacher–student matches could be made for this case study.</p>
<p>Before discussing the implementation challenges, it is useful to examine the data in more detail. <xref ref-type="table" rid="table1-0275074011399120">Table 1</xref> provides summary statistics for the study sample. The smallest district contained three high schools that enrolled 771 11th-grade math students and the largest contained five high schools enrolling 1,699 11th-grade math students. After all missing information was accounted for, District 1 provided 477 complete student observations for use in the regression models, District 2 had 564 student observations, and District 3 contributed 1,137 complete observations. The aggregate number of math teachers in these districts ranged from 23 to 67. Our study sample contained between 21 and 63 teachers. At the school level, 11th-grade math student enrollments ranged from 188 to 440. The number of math teachers whose students took the test ranged from 7 to 18 per high school. <xref ref-type="table" rid="table1-0275074011399120">Table 1</xref> illustrates that these three districts had roughly similar math performance on the KCCT exam in the 2005-2006 school year.</p>
<table-wrap id="table1-0275074011399120" position="float">
<label>Table 1.</label>
<caption><p>Variable Means and Standard Deviations for Final Sample</p></caption>
<graphic alternate-form-of="table1-0275074011399120" xlink:href="10.1177_0275074011399120-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">District 1</th>
<th align="center">District 2</th>
<th align="center">District 3</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="4">Student variables</td>
</tr>
<tr>
<td> KCCT Grade 11 math score (scaled)</td>
<td>565 (43)</td>
<td>547 (44)</td>
<td>562 (48)</td>
</tr>
<tr>
<td> % Female</td>
<td>46.3</td>
<td>47.9</td>
<td>49.7</td>
</tr>
<tr>
<td> % Black<sup><xref ref-type="table-fn" rid="table-fn2-0275074011399120">a</xref></sup></td>
<td>0.4</td>
<td>16.1</td>
<td>16.5</td>
</tr>
<tr>
<td> % Gifted</td>
<td>33.3</td>
<td>22.9</td>
<td>23.4</td>
</tr>
<tr>
<td> % Disability<sup><xref ref-type="table-fn" rid="table-fn3-0275074011399120">b</xref></sup></td>
<td>5.9</td>
<td>6.9</td>
<td>4.8</td>
</tr>
<tr>
<td> % Free/reduced lunch</td>
<td>14.9</td>
<td>25.9</td>
<td>18.5</td>
</tr>
<tr>
<td> CTBS Grade 9 math score (NCE)<sup><xref ref-type="table-fn" rid="table-fn4-0275074011399120">c</xref></sup></td>
<td>63 (19)</td>
<td>58 (19)</td>
<td>64 (21)</td>
</tr>
<tr>
<td> CTBS Grade 9 science score (scaled)</td>
<td>702 (31)</td>
<td>698 (30)</td>
<td>707 (34)</td>
</tr>
<tr>
<td> KCCT Grade 10 reading score (scaled)</td>
<td>530 (43)</td>
<td>531 (45)</td>
<td>544 (51)</td>
</tr>
<tr>
<td colspan="4">Teacher variables</td>
</tr>
<tr>
<td> Years experience</td>
<td>14.5 (11.5)</td>
<td>12.3 (8.5)</td>
<td>13.0 (10.0)</td>
</tr>
<tr>
<td> % Female</td>
<td>61.0</td>
<td>67.2</td>
<td>73.4</td>
</tr>
<tr>
<td> <italic>N</italic> (final regression sample)</td>
<td>21</td>
<td>28</td>
<td>63</td>
</tr>
<tr>
<td colspan="4">% 11th-grade math teachers educated by</td>
</tr>
<tr>
<td>  TPP A</td>
<td>52</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>  TPP B</td>
<td>5</td>
<td>7</td>
<td>52</td>
</tr>
<tr>
<td>  TPP C</td>
<td>0</td>
<td>36</td>
<td>2</td>
</tr>
<tr>
<td>  Other KY TPPs</td>
<td>19</td>
<td>39</td>
<td>27</td>
</tr>
<tr>
<td>  TPPs in other states</td>
<td>24</td>
<td>18</td>
<td>19</td>
</tr>
<tr>
<td><italic>N</italic> (number of student observations)</td>
<td>477</td>
<td>564</td>
<td>1,137</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0275074011399120">
<p>Note: TPP = teacher preparation program; KCCT = Kentucky Core Content Test; CTBS = Comprehensive Tests of Basic Skills (national normed standardized test); NCE = normal curve equivalent.</p></fn>
<fn id="table-fn2-0275074011399120">
<label>a.</label>
<p>Black student percentage was too low to justify use the use of this variable as a predictor in the regression.</p></fn>
<fn id="table-fn3-0275074011399120">
<label>b.</label>
<p>Although a simple indicator variable for presence of a disability designation in the administrative data was used, we recognize that a wide range of disabilities may be represented by such a designation. If administrative data permit, greater resolution in this measure would be desirable.</p></fn>
<fn id="table-fn4-0275074011399120">
<label>c.</label>
<p>Scaled scores were unavailable for this test, so the normal curve equivalent values were used as the control variable instead. For this reason magnitude of the coefficients should not be compared between the 9th-grade math score and the other two scores used as independent variables (9th-grade science and 10th-grade reading).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The students in Districts 2 and 3 have fairly similar demographic characteristics. About half of the sample students in those districts are female; African American students make up just over 16% of the sample; and about 23% of sample students are in a gifted program. The only major difference between Districts 2 and 3 in terms of student demographics is that District 2 has a higher percentage (25%) of students who receive federally subsidized lunch than District 3 (18.5%). Students in District 1, the smallest of the three districts, look different than those in the other two districts. The district itself has less than 1% each of African American, Asian, and Latino/a students. Our study sample contained only 0.4% African American students, and there were no Latino/a or Asian students with complete information from that district to include in our regression models. Finally, a larger share of District 1 students are classified as gifted (33% vs. 23% in each of the other districts) and a smaller share participates in the subsidized lunch program.</p>
</sec>
<sec id="section3-0275074011399120">
<title>Demonstration of Geographic Stratification</title>
<p>EPSB recognizes 30 institutions of higher education with teacher training programs in the state, which are responsible for producing most of the teachers of roughly 670,000 Kentucky K-12 students. Among these programs, the majority are small, private institutions—each producing a few education graduates per year—while eight are publicly funded institutions with substantial numbers of graduates annually. <xref ref-type="table" rid="table2-0275074011399120">Table 2</xref> illustrates the clustered distribution of TPP graduates across the study districts. The authors created five TPP categories. TPPs A, B, and C are TPPs of three different Kentucky institutions of higher education and are singled out because they produced the most math teachers in one of each of the three districts in 2005-2006 examined for this case. “TPP, Other KY” indicates the number of graduates of all other Kentucky colleges and universities teaching 11th-grade math in the district. “TPP, Other State” indicates the number of graduates from programs in other states. Teachers who taught five or fewer students with test scores in the sample are not shown. <xref ref-type="table" rid="table2-0275074011399120">Table 2</xref> shows that there are at least 21 teachers who teach one or more high school math classes and 16 of these teachers received teacher training from a Kentucky institution in District 1. Of these 16 Kentucky graduates, however, 11 were trained in the same TPP (TPP A). No high school math teachers on staff in this district in the study year were trained in TPP C. In District 2, on the other hand, 10 of 23 state-trained teachers received credentials from TPP C but no teachers were trained in the dominant training institution for District 1, TPP A. District 3, which predominantly hires its teachers from yet another program, TPP B, did not have any high school math teachers who trained in TPP A in the study year, and had only one who trained in TPP C.</p>
<table-wrap id="table2-0275074011399120" position="float">
<label>Table 2.</label>
<caption><p>Stratification of High School Math Teachers by Teacher Preparation Program (TPP) in Three Kentucky districts, in final Regression Samples</p></caption>
<graphic alternate-form-of="table2-0275074011399120" xlink:href="10.1177_0275074011399120-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">District 1</th>
<th align="center" colspan="2">District 2</th>
<th align="center" colspan="2">District 3</th>
</tr>
<tr>
<th/>
<th align="center">No. of teachers<sup><xref ref-type="table-fn" rid="table-fn6-0275074011399120">a</xref></sup></th>
<th align="center">No. of students<sup><xref ref-type="table-fn" rid="table-fn7-0275074011399120">b</xref></sup></th>
<th align="center">No. of teachers</th>
<th align="center">No. of students</th>
<th align="center">No. of teachers</th>
<th align="center">No. of students</th>
</tr>
</thead>
<tbody>
<tr>
<td>TPP A</td>
<td>11</td>
<td>222</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>TPP B</td>
<td>1</td>
<td>41</td>
<td>2</td>
<td>92</td>
<td>33</td>
<td>621</td>
</tr>
<tr>
<td>TPP C</td>
<td>0</td>
<td>0</td>
<td>10</td>
<td>86</td>
<td>1</td>
<td>22</td>
</tr>
<tr>
<td>TPP, other KY</td>
<td>4</td>
<td>77</td>
<td>11</td>
<td>260</td>
<td>17</td>
<td>262</td>
</tr>
<tr>
<td>TPP, other State</td>
<td>5</td>
<td>137</td>
<td>5</td>
<td>126</td>
<td>12</td>
<td>232</td>
</tr>
<tr>
<td>N</td>
<td>21</td>
<td>477</td>
<td>28</td>
<td>564</td>
<td>63</td>
<td>1,137</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0275074011399120">
<p>Note: TPP = Teacher preparation program.</p></fn>
<fn id="table-fn6-0275074011399120">
<label>a.</label>
<p>Refers to the number of teachers that graduated from the relevant TPP.</p></fn>
<fn id="table-fn7-0275074011399120">
<label>b.</label>
<p>Refers to the number of 11th-grade math students taught in sample.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The data from these three districts starkly illustrate an issue that has been raised in other contexts in the teacher training literature. Teachers tend to enroll in training programs near their homes and to take jobs in districts near their institutions of training (<xref ref-type="bibr" rid="bibr5-0275074011399120">Boyd et al., 2005</xref>). For the small districts in this sample, the labor market segmentation is sufficiently severe that an evaluation of TPPs effectively implies a comparison of only the two most common programs, or a single dominant program to all others present. But because these dominant programs with sufficient numbers of graduates are not the same across the three districts, each district must be analyzed separately. Enlarging the sample to include data across all Kentucky school districts would expand the set of TPPs that could be evaluated but would not mitigate the stratification problem. The teacher stratification issue appears sufficiently severe to make evaluation of TPPs in rural areas of Kentucky using this type of approach infeasible from a practical perspective, as isolating the TPP effect from the district effect is difficult. It is especially problematic for any high-stakes budgeting or other policy decisions.</p>
<p>Note that the problem of confounding of geographic dimensions with TPP labor markets does not diminish with time—nor is it an artifact arising from our focus on 11th-grade math scores in this case.<sup><xref ref-type="fn" rid="fn5-0275074011399120">5</xref></sup> Consider the pattern shown in <xref ref-type="fig" rid="fig2-0275074011399120">Figure 2</xref>, which includes information about all teacher graduates (K-12) from the seven largest producers of Kentucky public school teacher bachelor’s degrees from 2001 to 2008. The horizontal dimension displays the TPPs from west to east (based on geographic center) and the vertical axis lists the Kentucky counties from west to east. Bars range from 0 to 8 years, indicating the total number of years between 2001 and 2008 in which the given TPP produced 10% or more of that county’s teachers.<sup><xref ref-type="fn" rid="fn6-0275074011399120">6</xref></sup> Thus the figure illustrates that TPP graduates tend to concentrate in geographic proximity to their institution and that this concentration persists through time. Another perspective on the extent of the concentration is shown in <xref ref-type="table" rid="table3-0275074011399120">Table 3</xref>. For each of the seven TPPs, the majority of counties had less than 10% of teaching staff from that program in each of the 8 years examined. Graduates from Northern Kentucky University (NKU) and the University of Louisville (U of L) are the most highly concentrated; only 7% of Kentucky counties have more than 10% or more of their teaching staffs from these TPPs in at least one of the 8 years studied. Because of this kind of stratification, it will be difficult to disentangle the effect of district and school from TPP on student achievement since geography and TPP tend to be linked so strongly over time. While geographic stratification by this metric is a function of both numbers of graduates a TPP generates and job location held by graduates, if a program produces fewer graduates it is likely to exacerbate the geographic preference effect.</p>
<fig id="fig2-0275074011399120" position="float">
<label>Figure 2.</label>
<caption><p>Persistence of teacher preparation program graduates’ geographic stratification in Kentucky counties over time, for period 2001-2008</p>
<p>Note: The seven teacher preparation programs (TPPs), which produce the most Kentucky public school teachers, are those of Murray State University, Western Kentucky University (WKU), University of Louisville (UL), Northern Kentucky University (NKU), Eastern Kentucky University (EKU), and Morehead State University. For ease in geographic presentation, we use Kentucky’s 120 counties here rather than 176 districts (most counties have only one district, while some have an internal “independent” district). Counties are ordered along the west–east dimension by their geographic center. TPPs are ordered from west to east based on their main campus location. The name of every other county on the <italic>y</italic>-axis is replaced by a dash because of space constraints. Source data for this figure were provided by Terry Hibpshman (EPSB).</p></caption>
<graphic xlink:href="10.1177_0275074011399120-fig2.tif"/>
</fig>
<table-wrap id="table3-0275074011399120" position="float">
<label>Table 3.</label>
<caption><p>Extent of Concentration of Teacher Preparation Program (TPP) Graduates in Kentucky, 2001-2008</p></caption>
<graphic alternate-form-of="table3-0275074011399120" xlink:href="10.1177_0275074011399120-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">TPP</th>
<th align="center">Percentage of Kentucky counties with less than 10% of teachers from TPP in each one of 8 years</th>
</tr>
</thead>
<tbody>
<tr>
<td>Eastern Kentucky University</td>
<td>51</td>
</tr>
<tr>
<td>Western Kentucky University</td>
<td>63</td>
</tr>
<tr>
<td>University of Kentucky</td>
<td>71</td>
</tr>
<tr>
<td>Morehead State University</td>
<td>73</td>
</tr>
<tr>
<td>Murray State University</td>
<td>82</td>
</tr>
<tr>
<td>University of Louisville</td>
<td>93</td>
</tr>
<tr>
<td>Northern Kentucky University</td>
<td>93</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn8-0275074011399120">
<p>Note: TPP = Teacher preparation program. The higher the percentage of counties with less than 10% of its teachers from that TPP in every one of 8 years examined, the <italic>greater</italic> the persistent concentration of the TPP’s graduates.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>This geographic stratification issue is our main focus, but this case also illustrates another challenge which will be severe for predominantly rural states. <xref ref-type="bibr" rid="bibr29-0275074011399120">Noell (2006)</xref> and others find that TPP effects are greatest during the first 3 years of the teacher’s classroom experience. After the 3rd year, the TPP effects are diluted by the cohort or peer effects of the school in which the teacher is hired. These peer effects are likely to be larger in schools where the teaching staff has been in place for longer periods of time. In our study sample, the average years of experience in the three districts ranges from 12.3 to 14.5 years (<xref ref-type="table" rid="table1-0275074011399120">Table 1</xref>). Restricting the data to those teachers with experience of 3 years or less or, at most, to those with 5 years or less would cause a loss of more student observations for those schools with greater numbers of more experienced teachers. The average years of experience itself may be associated with the quality of the school, as teacher turnover is expected to be lower in schools with more amenities, such as a higher performing student body. Thus one data choice that would make identifying TPP effects more likely—that is, a focus on recent graduates—would entail (a) a trade-off with sample size and (b) the risk of a school-level selection bias due to the nonrandom distribution of new teachers in schools from which those with more seniority tend to transfer.</p>
<p>The variation in time of graduation from a teacher preparation institution presents another complication to the interpretation of estimates of TPP effects. <xref ref-type="table" rid="table4-0275074011399120">Table 4</xref> lists individually the five TPP categories and the years in which the training occurred. The data indicate that there are teachers in these districts who received degrees from TPP B as early as 1969 and as recently as 2005. If the policymakers’ goal in identifying a TPP effect is to reward institutions that prepare better teachers, or to identify best curricular practices for diffusion to other institutions, one must make the assumption that the TPP effect represents some consistent educational or (less optimistically) selection practice on the part of the institution.</p>
<table-wrap id="table4-0275074011399120" position="float">
<label>Table 4.</label>
<caption><p>Range of Graduation Years of High School Math Teachers From Various TPPs Across Three Kentucky Districts, 2005-2006</p></caption>
<graphic alternate-form-of="table4-0275074011399120" xlink:href="10.1177_0275074011399120-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="center"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">TPP</th>
<th align="center">Year bachelor’s degree received</th>
<th align="center"><italic>N</italic> (year of degree known)</th>
</tr>
</thead>
<tbody>
<tr>
<td>A</td>
<td>1981-2004</td>
<td>7</td>
</tr>
<tr>
<td>B</td>
<td>1969-2005</td>
<td>16</td>
</tr>
<tr>
<td>C</td>
<td>1994-1999</td>
<td>3</td>
</tr>
<tr>
<td>Other KY</td>
<td>1974-2004</td>
<td>16</td>
</tr>
<tr>
<td>Other states</td>
<td>1967-2005</td>
<td>17</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn9-0275074011399120">
<p>Note: TPP = Teacher preparation program. The wide range of graduation years makes estimating the effect of a given teacher preparation program on student achievement challenging, if the assumption that preparation program quality and content remained constant over the period is not realistic.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section4-0275074011399120">
<title>District Level Results</title>
<p>Recognizing the deficiencies of the data and the more fundamental implementation challenges described above, the value-added model with school fixed effects (Equation 1 above) was estimated separately for each school district. We do not claim that this estimation solves the problems we identified above: rather, we provide an illustration of an attempt to estimate TPP value added using typical available administrative data and a value-added model. Indeed, our point is that this illustration and similar approaches do <italic>not</italic> address those fundamental issues—and hence policymakers relying on such approaches with typical administrative data risk making unwarranted judgments about TPP efficacy.</p>
<p>Student test scores for end of period are their 11th-grade math scores on the KCCT exam. All prior high school scores available for the students are included as controls, including science and reading scores in addition to math. The student characteristics include gender, race, special abilities status, and free and reduced lunch status. The teacher’s gender and experience levels are included. All time-invariant school characteristics are captured in the school fixed effects.<sup><xref ref-type="fn" rid="fn7-0275074011399120">7</xref></sup></p>
<p>The results of the regressions are listed in <xref ref-type="table" rid="table5-0275074011399120">Table 5</xref> and indicate reassuringly that a student’s past scores in both reading and math (and science in the largest district) significantly influence 11th-grade math scores. The 10th-grade reading and 9th-grade math scores are significant and positively related to 11th-grade math performance across all districts. In fact, prior student test scores are the only variables that are consistently statistically significant in explaining student outcomes across all three districts. The significance of the school fixed effects observed in two of the districts indicates strong school-level effects, even controlling for included student and teacher characteristics.</p>
<table-wrap id="table5-0275074011399120" position="float">
<label>Table 5.</label>
<caption><p>Individual Student Achievement Outcomes in Three Districts as a Function of Student Characteristics, 11th-Grade Math Teacher Characteristics, TPP, and Test Score History.</p></caption>
<graphic alternate-form-of="table5-0275074011399120" xlink:href="10.1177_0275074011399120-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" rowspan="2">District 1</th>
<th align="center" rowspan="2">District 2</th>
<th align="center" colspan="2">District 3</th>
</tr>
<tr>
<th/>
<th align="center">Model 1</th>
<th align="center">Model 2</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">Student variables</td>
</tr>
<tr>
<td> Female (= 1)</td>
<td>−2.99 (−1.23)</td>
<td>−4.21 (−1.82)</td>
<td>−4.78<xref ref-type="table-fn" rid="table-fn11-0275074011399120">**</xref> (−2.97)</td>
<td>−4.95 (−1.81)</td>
</tr>
<tr>
<td> Black (= 1)</td>
<td>—</td>
<td>−5.62 (−1.77)</td>
<td>−2.13 (−0.94)</td>
<td>0.84 (0.82)</td>
</tr>
<tr>
<td> Gifted (= 1)</td>
<td>9.83<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (3.52)</td>
<td>1.99 (0.63)</td>
<td>7.50<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (3.72)</td>
<td>2.58 (0.73)</td>
</tr>
<tr>
<td> Disability (= 1)</td>
<td>0.23 (0.04)</td>
<td>−12.76<xref ref-type="table-fn" rid="table-fn11-0275074011399120">**</xref> (−2.73)</td>
<td>−8.36<xref ref-type="table-fn" rid="table-fn11-0275074011399120">*</xref> (−2.20)</td>
<td>−9.98 (6.97)</td>
</tr>
<tr>
<td> Free/reduced lunch (= 1)</td>
<td>1.46 (0.45)</td>
<td>0.39 (0.15)</td>
<td>−0.58 (−0.26)</td>
<td>−1.40 (−0.37)</td>
</tr>
<tr>
<td> CTBS Grade 9 math score (NCE)</td>
<td>1.29<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (14.86)</td>
<td>1.18<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (14.70)</td>
<td>1.32<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (24.33)</td>
<td>1.35<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (14.34)</td>
</tr>
<tr>
<td> CTBS Grade 9 science score (scaled)</td>
<td>0.02 (0.35)</td>
<td>0.08 (1.52)</td>
<td>0.13<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (4.10)</td>
<td>0.17<xref ref-type="table-fn" rid="table-fn11-0275074011399120">**</xref> (3.10)</td>
</tr>
<tr>
<td> KCCT Grade 10 reading score (scaled)</td>
<td>0.23<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (5.67)</td>
<td>0.27<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (7.44)</td>
<td>0.18<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (8.07)</td>
<td>0.15<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (4.48)</td>
</tr>
<tr>
<td colspan="5">Teacher variables</td>
</tr>
<tr>
<td> Years experience</td>
<td>−0.05 (−0.43)</td>
<td>0.07 (0.46)</td>
<td>0.04 (0.46)</td>
<td>−0.19 (−0.02)</td>
</tr>
<tr>
<td> Female (= 1)</td>
<td>3.58 (1.28)</td>
<td>5.83<xref ref-type="table-fn" rid="table-fn11-0275074011399120">*</xref> (2.22)</td>
<td>3.84<xref ref-type="table-fn" rid="table-fn11-0275074011399120">*</xref> (2.22)</td>
<td>2.23 (0.67)</td>
</tr>
<tr>
<td> Most common TPP in district (= 1)</td>
<td>−4.54 (−1.63)</td>
<td>4.06 (1.15)</td>
<td>1.67 (1.07)</td>
<td>−1.14 (−0.38)</td>
</tr>
<tr>
<td> School 1</td>
<td>−10.79<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (−3.82)</td>
<td>0.23 (0.08)</td>
<td>−5.56<xref ref-type="table-fn" rid="table-fn11-0275074011399120">*</xref> (−2.55)</td>
<td>−19.14<xref ref-type="table-fn" rid="table-fn11-0275074011399120">***</xref> (−3.77)</td>
</tr>
<tr>
<td> School 2</td>
<td>−1.58 (−0.49)</td>
<td>2.54 (0.84)</td>
<td>−8.61<xref ref-type="table-fn" rid="table-fn11-0275074011399120">**</xref> (−2.75)</td>
<td>−15.43 (−1.26)</td>
</tr>
<tr>
<td> School 3</td>
<td>—</td>
<td>—</td>
<td>−7.48<xref ref-type="table-fn" rid="table-fn11-0275074011399120">**</xref> (−3.27)</td>
<td>−10.46<xref ref-type="table-fn" rid="table-fn11-0275074011399120">**</xref> (−2.92)</td>
</tr>
<tr>
<td> School 4</td>
<td>—</td>
<td>—</td>
<td>−0.97 (−0.40)</td>
<td>−7.09 (−1.46)</td>
</tr>
<tr>
<td> ACT composite score</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>1.31<xref ref-type="table-fn" rid="table-fn11-0275074011399120">**</xref> (3.11)</td>
</tr>
<tr>
<td><italic>N</italic></td>
<td>477</td>
<td>564</td>
<td>1,137</td>
<td>370</td>
</tr>
<tr>
<td>Adjusted <italic>R</italic><sup>2</sup></td>
<td>.66</td>
<td>.66</td>
<td>.72</td>
<td>.74</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn10-0275074011399120">
<p>Note: TPP = Teacher preparation program; CTBS = Comprehensive Tests of Basic Skills (national normed standardized test); NCE = normal curve equivalent; KCCT = Kentucky Core Content Test. <italic>T</italic> scores are shown in parentheses coefficient estimates. Ordinary least squares regression is used with school fixed effects (shown; one school is the omitted category in each district). The TPP indicator variable takes a 1 for students of teachers that graduated from the most common TPP of teachers in the district, and 0 otherwise. The coefficient indicates the effect of that TPP on student achievement relative to the effect of graduates of any other TPP present in the district, holding all else constant.</p></fn>
<fn id="table-fn11-0275074011399120">
<label>*</label>
<p><italic>p</italic> &lt; .05. **<italic>p</italic> &lt; .01. ***<italic>p</italic> &lt; .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Finally, and most important for our purposes, we consider the coefficient on the TPP variable. Recognizing that each district’s equation compares only its most common TPP to all others represented by that district’s high school math teachers, we see that there are no significant differences in student performance that can be attributed to the training institution. We ran an additional model that attempts to further separate the innate characteristics of the teacher from the effects of the TPP. The model includes teachers’ ACT score to correct for some of the observable and unobservable differences in teachers that are accepted into the training programs. One complication with this model is that relatively recent hires are the only teachers in the sample for whom we have reliable ACT scores. To ensure a sufficient sample size, the model containing ACT scores could therefore only be run for the largest district. The District 3 student sample size was reduced from 1,137 to 370 by focusing on students of these recently graduated teachers alone. As in the previous regression, the effect of dominant TPP relative to all others remained nonsignificant. On the other hand, the estimated effect of teacher ACT is positive and statistically significant, even controlling for all the variables listed in <xref ref-type="table" rid="table1-0275074011399120">Table 1</xref>.</p>
<p>There is more than one reason that a TPP effect may not have been identified: at the most basic level, perhaps TPP does not matter once the other included student and teacher characteristics are accounted for. We cannot make such a determination with confidence, however, due to unresolved issues in the structure of these data: if a TPP effect truly exists, the inability to identify a significant effect could be due to the fact that all teachers are included, regardless of year in which they graduated. TPPs may have varied in their selection or curricular practices over time, as discussed above. This would make our TPP signal a very noisy one. Furthermore, and most important for our focus in this case study, the approach of comparing a district’s most common TPP to all others is a very weak one. If the TPPs in the combined reference group have strongly divergent effects on student achievement relative to the most common TPP, it would of course be difficult to show an effect of that TPP in this design. Nonetheless, this admittedly problematic type of comparison is necessary due to the dramatic geographic stratification of TPPs across districts and the relatively small numbers of high school math teachers from the nondominant TPP(s) in each district.</p>
</sec>
<sec id="section5-0275074011399120">
<title>Alternatives</title>
<p>Although most agree that all public agencies should be held accountable for their outputs—and ideally, outcomes—this case study illustrates the substantial hurdles to implementation of accountability schemes for TPPs that rely on K-12 student measures. The cost of assembling appropriate data, the problems of separating experienced teachers from novice teachers, and (especially for rural districts) the segmentation of teacher markets pose serious problems for a high-stakes quantitative evaluation of teacher preparation. But these problems do not mean that accountability of the programs that prepare teachers must be ignored. This type of research endeavor still provides useful data to decision makers when trying to improve agency performance (<xref ref-type="bibr" rid="bibr2-0275074011399120">Bloom, Hill, &amp; Riccio, 2001</xref>; <xref ref-type="bibr" rid="bibr17-0275074011399120">Heinrich, 2002</xref>). The answer is not to give up on the idea of using data to evaluate TPPs—but to develop richer approaches rather than expecting only one metric, student value-added achievement, to bear the entire weight of the performance evaluation enterprise for a state’s educational system. Such an effort will require some creativity in developing alternative modes of TPP evaluation and may entail complementing strictly quantitative measures of performance with more qualitative approaches. We next explore some possible approaches.</p>
<p>The recognition that reliance on a single metric or even class of metrics can distort organizational incentives inspired the influential “Balanced Scorecard” approach to managerial performance measurement (<xref ref-type="bibr" rid="bibr21-0275074011399120">Kaplan &amp; Norton, 1996</xref>); though originated for the private sector, this model has had some influence in the public sector as part of the New Public Management movement toward improved use of performance measurement (see, for example, <xref ref-type="bibr" rid="bibr28-0275074011399120">Niven, 2008</xref>). Focusing on performance measurement for external stakeholders as opposed to internal management, <xref ref-type="bibr" rid="bibr11-0275074011399120">Gormley and Weimer (1999)</xref> also caution against using a single outcome variable in an accountability system when there are multiple important outcomes that an agency wishes to measure. Rather, they argue in favor of using multiple measures that can be combined into a single index (although this practice introduces a new set of concerns related to the building of the index and weighting of individual items). Challenges face even multiple-measure approaches in the public context, with costs more certain than potential benefits (<xref ref-type="bibr" rid="bibr12-0275074011399120">Halachmi, 2005</xref>). However, with thoughtful consideration an approach with multiple metrics still seems more likely to provide useful information to decision makers than overreliance on a single measure that is several layers removed from the original agency, and must bear the weight of evaluating each of those layers (<xref ref-type="fig" rid="fig1-0275074011399120">Figure 1</xref>).<sup><xref ref-type="fn" rid="fn8-0275074011399120">8</xref></sup> Standards-based evaluation is an example of such a multiple-measure endeavor in education. This type of evaluation moves away from the newer focus on outcomes, returning to the older focus on bureaucratic inputs. Specifically, the evaluation uses a number of rating scales based on the behaviors that teachers exhibit in the classroom as well as a review of lesson plans and samples of student work (Heneman &amp; Milanowski, 2004). Using standards-based evaluation data from four U.S. sites, Milanowski, Kimball, and Odden (2005) provide some early indications that the standards-based evaluation is correlated with student achievement. Additional studies show that principal assessments of teachers, such as those incorporated in standards-based evaluations, are correlated with teacher value-added estimates (<xref ref-type="bibr" rid="bibr13-0275074011399120">Harris &amp; Sass, 2007</xref>; <xref ref-type="bibr" rid="bibr19-0275074011399120">Jacob &amp; Lefgren, 2008</xref>). A strong candidate for a more balanced accountability system for TPPs might be a combination of standards-based evaluation and the VAMs that have been suggested.</p>
<p>Teacher warranty agreements, in theory, provide another potential mechanism for placing the judge of accountability at the school or district level. If implemented at a state systemwide level (rather than single-institution), teacher warranties could provide central government stakeholders with an approach to quality control that does not depend on data analysis prone to the stratification issue. For example, as part of its P-16 initiatives the Board of Regents of the University System of Georgia guarantees the quality of teachers trained in its TPPs (<xref ref-type="bibr" rid="bibr22-0275074011399120">Kettlewell, Kaste, &amp; Jones, 2000</xref>).<sup><xref ref-type="fn" rid="fn9-0275074011399120">9</xref></sup> Any school that hires a teacher trained in a state program can “return” that teacher for additional training, without cost to either the TPP graduate or the school, if not fully satisfied with the quality of the teacher within the first 2 years. Such an approach places the definition of quality at the school and district level, at least in theory. One would expect that schools will look at the test scores of the students of new teachers, but we can assume that schools may also utilize alternative means of assessing quality, such as review of lesson plans, classroom observations, or evaluation of students’ sample work. Guaranteeing graduate quality in a meaningful way would devolve the responsibility for judging quality to the lowest level decision makers closest to the actual performance being judged, and allows the development of competing measures of quality. These competing measures subsequently can provide information to other schools, districts, and states. However, it is not clear whether the Georgia policy functions more as a symbolic device valued for its salutary effect on TPPs, or as an actual post hoc mechanism for TPP graduate improvement. In his letter opening the Board of Regents’ 2007 report on teacher preparation (<xref ref-type="bibr" rid="bibr3-0275074011399120">2007</xref>), Chancellor Erroll Davis notes “Since instituting the guarantee, we have had no reports of school districts being dissatisfied with the teachers we prepare.” There are obvious, and serious principal-agent issues in the warranty approach to TPP accountability as a mechanism of <italic>state</italic> evaluation of TPP quality, with the institutions/TPPs having an incentive to conceal warranty claims from state-level policymakers. However, if the system were structured with the claims process operating through the state office of educational accountability rather than directly to the TPP or its institutional system, the signal of a developing quality issue might not be lost.</p>
</sec>
<sec id="section6-0275074011399120" sec-type="conclusions">
<title>Concluding Comments</title>
<p>This case study uses data from Kentucky school districts to illustrate some of the particular, underrecognized challenges of using the student test performance metric to evaluate TPPs. While many policymakers and scholars agree on the positive value of introducing accountability for process and outcomes in all areas of public service, schools provide an excellent example of some of the recent attempts to make data-driven accountability a reality. Many stakeholders are hoping to use student test scores as the single outcome to measure performance not only of K-12 teachers, schools, and districts but also as a measure of the performance of higher education institutions’ TPPs. But as demonstrated in this case study, linking test scores in a given year to the quality of a specific TPP is problematic at best and may not be feasible in rural regions. In Kentucky, as in many states, rural schools tend to hire disproportionately from a single TPP. Policymakers will have to take a close look at the geographic stratification of teacher training and hiring in their own state if they are to engage in evaluations of teacher training programs relying only on student test scores.</p>
<p>As noted earlier, 56% of the school districts in the United States are in rural areas (<xref ref-type="bibr" rid="bibr32-0275074011399120">Provasnik et al., 2007</xref>). This reality means that implementation of teacher preparation evaluations in the majority of school districts will face challenges similar to those we identify related to geographic stratification if such evaluations rely only on value-added models of student achievement. Although it is beyond the scope of this article to recommend a specific alternative form of evaluation, we do suggest that overreliance on the single measure of student value-added achievement gains as a device for measuring the effectiveness of teacher training programs is premature. As research on school effectiveness continues, perhaps we will learn more about the attributes of a good teacher. Some argue good teaching is tied to the aptitude of the teacher. Others argue it is the ability of the teacher to manage a classroom. If it is aptitude, we need not evaluate TPPs at all but merely evaluate their selection of teachers into training programs through such devices as SAT or ACT scores. On the other hand, classroom management can be taught by TPPs and we should be able to evaluate whether programs are effectively training teachers to manage their classrooms without relying exclusively on the K-12 students’ test scores. Peers and professionals can observe classroom management styles and assess whether the teacher is a good manager. In a wider public management context, this work suggests that any single performance indicator that is expected to signal performance of multiple actors and multiple institutions that themselves interact in complex ways may fall short. Our case study here is analogous to judging the performance of public administration academic programs by using a single outcome metric drawn from all budget offices or all city management agencies in which our graduating students are placed. Few in the public administration scholarly world or in those policy arenas would accept such a single-dimensional indicator of performance. Accountability is desirable; yet the way in which it is implemented is critical to its success.</p>
</sec>
</body>
<back>
<ack>
<p>The authors are indebted to the Kentucky Education Professional Standards Board for the opportunity to use administrative data that are not publicly available and to Terry Hibpshman for his role in extracting the necessary data. Jacob Fowles provided valuable assistance in manuscript preparation.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the authorship and/or publication of this article.</p></fn>
<fn fn-type="financial-disclosure">
<p>The author(s) received no financial support for the research and/or authorship of this article.</p></fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0275074011399120">
<label>1.</label>
<p>The implications of this preference pattern on the distribution of high- and low-quality teachers across schools and districts is an interesting empirical question. A priori, a process that tends to match teachers with students who are growing up and were educated in the same region or town of teacher origin could improve outcomes if common origin benefits student learning. Potential rationales for such a benefit could include theories of representative bureaucracy and culturally relevant pedagogy. On the other hand, concentration of poorly prepared teachers could theoretically worsen the outcomes of some districts if it so happens that the source of teachers in that region is not effective in training, or if nepotism results in weighing geographic factors above merit in hiring (see <xref ref-type="bibr" rid="bibr34-0275074011399120">Strauss, Bowes, Marks, &amp; Plesko, 2000</xref>, for an examination of this issue in Pennsylvania). However, the goal of our study is <italic>not</italic> to test whether or not the geographic stratification of teachers across a state has positive or negative implications for student achievement. Instead, we focus on the impact of this observed concentration in the teacher labor market on proposed efforts to indirectly evaluate TPPs through the measurement of their graduates’ effect on student achievement—in those schools and districts <italic>in which they choose to locate and are hired</italic>.</p></fn>
<fn fn-type="other" id="fn2-0275074011399120">
<label>2.</label>
<p>State and local governments combined spend approximately 30% of their budgets on the provision of this government service for their residents.</p></fn>
<fn fn-type="other" id="fn3-0275074011399120">
<label>3.</label>
<p>The evaluation focuses on the bachelor degree programs and does not attempt to include alternative teacher pathways or master’s degree programs.</p></fn>
<fn fn-type="other" id="fn4-0275074011399120">
<label>4.</label>
<p>An alternative specification of this equation considers the dependent variable as the change in achievement between time periods <italic>t</italic> and <italic>t</italic> − 1. See <xref ref-type="bibr" rid="bibr25-0275074011399120">McCaffrey, Lockwood, Koretz, and Hamilton (2003)</xref> for discussion.</p></fn>
<fn fn-type="other" id="fn5-0275074011399120">
<label>5.</label>
<p>Evaluating TPPs based on outcomes data for a single grade and subject at a time is not as unrealistic as it may seem. That is, a TPP that prepares excellent elementary school teachers may or may not also prepare high school teachers to the same quality level; for example, <xref ref-type="bibr" rid="bibr4-0275074011399120">Boyd et al. (2009)</xref> found that TPPs may not prepare English and Math teachers equally well. So, although our case study is limited in focus, full-fledged attempts to evaluate TPPs via student outcome data would still, to be meaningful, at least have to investigate the possibility that effects differ by grade levels and subjects.</p></fn>
<fn fn-type="other" id="fn6-0275074011399120">
<label>6.</label>
<p>There are 176 school districts and 120 counties in Kentucky—here teachers are analyzed by county to best illustrate the geographic stratification patterns, but most Kentucky districts are contiguous with counties (with some “independent” districts inside of a county associated with a city). In other words, most counties have only one district.</p></fn>
<fn fn-type="other" id="fn7-0275074011399120">
<label>7.</label>
<p>If a panel dataset were used with multiple years of 11th-grade math scores as the dependent variable, time-varying characteristics of schools could be broken out as independent variables. However, as we obtained 1 year of teacher–student matched data for this illustrative analysis, we use only the school fixed effects—which will encompass the effects for the study year of both those characteristics of schools that may change over time (e.g., demographic composition) as well as any nonchanging aspects (e.g., physical configuration). We are not interested in differentiating the effects of particular school characteristics in this illustration.</p></fn>
<fn fn-type="other" id="fn8-0275074011399120">
<label>8.</label>
<p>For a discussion of the single versus multiple measure issue in the policing context, see <xref ref-type="bibr" rid="bibr26-0275074011399120">Moore and Braga (2003)</xref>. With the advent of Compstat-style systems relying on fine-grained data collection and mapping, efforts to use performance measurement to satisfy internal accountability needs has grown in policing agencies. <xref ref-type="bibr" rid="bibr26-0275074011399120">Moore and Braga (2003)</xref> note the importance of using multiple measures though, rather than relying solely on a single outcome measure—and the need to ensure that internal accountability measures are aligned with external stakeholder expectations as well (all also relevant in the education agency context as well).</p></fn>
<fn fn-type="other" id="fn9-0275074011399120">
<label>9.</label>
<p>This approach is even used to market the TPPs themselves to potential students, as a teacher education recruitment web site describes to prospectives, “What does the guarantee mean to me?” (“USG Colleges of Education Deliver Quality—Guaranteed,” <ext-link ext-link-type="uri" xlink:href="http://www.destinationteaching.org/career/guaranteed.phtml">http://www.destinationteaching.org/career/guaranteed.phtml</ext-link>).</p></fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Baker</surname><given-names>G.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Distortion and risk in optimal incentive contracts</article-title>. <source>Journal of Human Resources</source>, <volume>37</volume>, <fpage>728</fpage>-<lpage>751</lpage>.</citation>
</ref>
<ref id="bibr2-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bloom</surname><given-names>H.</given-names></name>
<name><surname>Hill</surname><given-names>C.</given-names></name>
<name><surname>Riccio</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <source>Modeling the performance of welfare-to-work programs: The effects of program management and services, economic environment, and client characteristics</source> (<comment>Working Papers on Research Methodology</comment>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Manpower Demonstration Research Corporation</publisher-name>.</citation>
</ref>
<ref id="bibr3-0275074011399120">
<citation citation-type="web">
<collab>Board of Regents of the University System of Georgia Department of P-16 Initiatives</collab>. (<year>2007</year>). <source>2007 report on the preparation of new teachers by University System of Georgia Institutions</source>. <publisher-loc>Atlanta, GA</publisher-loc>: <publisher-name>Board of Regents of the University System of Georgia</publisher-name>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.usg.edu/educator_prep/publications/">http://www.usg.edu/educator_prep/publications/</ext-link></comment></citation>
</ref>
<ref id="bibr4-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boyd</surname><given-names>D.</given-names></name>
<name><surname>Grossman</surname><given-names>P.</given-names></name>
<name><surname>Lankford</surname><given-names>H.</given-names></name>
<name><surname>Loeb</surname><given-names>S.</given-names></name>
<name><surname>Wyckoff</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Teacher preparation and student achievement</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>31</volume>, <fpage>416</fpage>-<lpage>440</lpage>.</citation>
</ref>
<ref id="bibr5-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boyd</surname><given-names>D.</given-names></name>
<name><surname>Lankford</surname><given-names>H.</given-names></name>
<name><surname>Loeb</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The draw of home: How teachers’ preferences for proximity disadvantage urban schools</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>24</volume>, <fpage>113</fpage>-<lpage>132</lpage>.</citation>
</ref>
<ref id="bibr6-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Courty</surname><given-names>P.</given-names></name>
<name><surname>Marschke</surname><given-names>G.</given-names></name>
</person-group> (<year>2004</year>). <article-title>An empirical investigation of gaming responses to explicit performance incentives</article-title>. <source>Journal of Labor Economics</source>, <volume>22</volume>, <fpage>23</fpage>-<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr7-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cullen</surname><given-names>J. B.</given-names></name>
<name><surname>Reback</surname><given-names>R.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Tinkering toward accolades: School gaming under a performance accountability system</article-title>. In <person-group person-group-type="editor">
<name><surname>Gronberg</surname><given-names>T.</given-names></name>
<name><surname>Jansen</surname><given-names>D.</given-names></name>
</person-group> (Eds.), <source>Improving school accountability: Check-ups or choice, advances in applied microeconomics</source> (<volume>Vol. 14</volume>, pp. <fpage>1</fpage>-<lpage>34</lpage>). <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Elsevier Science</publisher-name>.</citation>
</ref>
<ref id="bibr8-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>DeNisi</surname><given-names>A.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Performance appraisal and performance management</article-title>: In <person-group person-group-type="editor">
<name><surname>Klein</surname><given-names>K.</given-names></name>
<name><surname>Kozlowski</surname><given-names>S.</given-names></name>
</person-group> (Eds.), <source>Multilevel theory, research, and methods in organizations: Foundations, extensions and new directions</source> (pp. <fpage>121</fpage>-<lpage>156</lpage>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr9-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Figlio</surname><given-names>D.</given-names></name>
<name><surname>Getzler</surname><given-names>L.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Accountability, ability and disability: Gaming the system?</article-title> <source>Advances in Applied Microeconomics: A Research Annual</source>, <volume>14</volume>, <fpage>35</fpage>-<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr10-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Figlio</surname><given-names>D</given-names></name>
<name><surname>Rouse</surname><given-names>C.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Do accountability and voucher threats improve low-performing schools?</article-title> <source>Journal of Public Economics</source>, <volume>90</volume>, <fpage>239</fpage>-<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr11-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gormley</surname><given-names>W.</given-names></name>
<name><surname>Weimer</surname><given-names>D.</given-names></name>
</person-group> (<year>1999</year>). <source>Organizational report cards</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr12-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Halachmi</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Performance measurement: Test the water before you dive in</article-title>. <source>International Review of Administrative Sciences</source>, <volume>71</volume>, <fpage>255</fpage>-<lpage>266</lpage>.</citation>
</ref>
<ref id="bibr13-0275074011399120">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Harris</surname><given-names>D.</given-names></name>
<name><surname>Sass</surname><given-names>T.</given-names></name>
</person-group> (<year>2007</year>, <month>June</month>). <article-title>What makes for a good teacher and who can tell?</article-title> <conf-name>Paper presented at the summer workshop of the National Bureau of Economic Research</conf-name>, <conf-loc>Cambridge, MA</conf-loc>.</citation>
</ref>
<ref id="bibr14-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hatry</surname><given-names>H.</given-names></name>
</person-group> <year>1999</year>. <source>Performance measurement: Getting results</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Urban Institute Press</publisher-name>.</citation>
</ref>
<ref id="bibr15-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Heckman</surname><given-names>J.</given-names></name>
<name><surname>Heinrich</surname><given-names>C.</given-names></name>
<name><surname>Smith</surname><given-names>J.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Assessing the performance of performance standards in public bureaucracies</article-title>. <source>American Economic Review</source>, <volume>87</volume>, <fpage>389</fpage>-<lpage>396</lpage>.</citation>
</ref>
<ref id="bibr16-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Heckman</surname><given-names>J.</given-names></name>
<name><surname>Heinrich</surname><given-names>C.</given-names></name>
<name><surname>Smith</surname><given-names>J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>The performance of performance standards</article-title>. <source>Journal of Human Resources</source>, <volume>38</volume>, <fpage>778</fpage>-<lpage>811</lpage>.</citation>
</ref>
<ref id="bibr17-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Heinrich</surname><given-names>C.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Outcomes-based performance management in the public sector: Implications for government accountability and effectiveness</article-title>. <source>Public Administration Review</source>, <volume>62</volume>, <fpage>712</fpage>-<lpage>725</lpage>.</citation>
</ref>
<ref id="bibr18-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>B.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Accountability, incentives and behavior: The impact of high-stakes testing in the Chicago public schools</article-title>. <source>Journal of Public Economics</source>, <volume>89</volume>, <fpage>761</fpage>-<lpage>796</lpage>.</citation>
</ref>
<ref id="bibr19-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>B.</given-names></name>
<name><surname>Lefgren</surname><given-names>L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Can principals identify effective teachers? Evidence on subjective performance evaluation in education</article-title>. <source>Journal of Labor Economics</source>, <volume>26</volume>, <fpage>101</fpage>-<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr20-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jennings</surname><given-names>E.</given-names></name>
<name><surname>Haist</surname><given-names>M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Putting performance measurement in context</article-title>. In <person-group person-group-type="editor">
<name><surname>Ingraham</surname><given-names>P.</given-names></name>
<name><surname>Lynn</surname><given-names>L.</given-names></name>
</person-group> (Eds.), <source>The art of governance: Analyzing management and administration</source> (pp. <fpage>173</fpage>-<lpage>194</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Georgetown University Press</publisher-name>.</citation>
</ref>
<ref id="bibr21-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kaplan</surname><given-names>R. S.</given-names></name>
<name><surname>Norton</surname><given-names>D. P.</given-names></name>
</person-group> (<year>1996</year>). <source>The balanced scorecard: Translating strategy into action</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Harvard Business School Press</publisher-name>.</citation>
</ref>
<ref id="bibr22-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kettlewell</surname><given-names>J. S.</given-names></name>
<name><surname>Kaste</surname><given-names>J. A.</given-names></name>
<name><surname>Jones</surname><given-names>S. A.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The Georgia story of P-16 partnerships</article-title>. <source>Education Policy</source>, <volume>14</volume>, <fpage>77</fpage>-<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr23-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lipsky</surname><given-names>M.</given-names></name>
</person-group> (<year>1980</year>). <source>Street level bureaucracy: Dilemmas of the individual in public services</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Basic Books</publisher-name>.</citation>
</ref>
<ref id="bibr24-0275074011399120">
<citation citation-type="web">
<collab>Louisiana Recovery School Districts</collab>. (<comment>n.d.</comment>). <source>About the recovery school district (RSD)</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.rsdla.net/About_the_RSD.aspx">http://www.rsdla.net/About_the_RSD.aspx</ext-link></comment></citation>
</ref>
<ref id="bibr25-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McCaffrey</surname><given-names>D.</given-names></name>
<name><surname>Lockwood</surname><given-names>J. R.</given-names></name>
<name><surname>Koretz</surname><given-names>D.</given-names></name>
<name><surname>Hamilton</surname><given-names>L.</given-names></name>
</person-group> (<year>2003</year>). <source>Evaluating value-added models for teacher accountability</source>. <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>RAND</publisher-name>.</citation>
</ref>
<ref id="bibr26-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moore</surname><given-names>M. H.</given-names></name>
<name><surname>Braga</surname><given-names>A. A.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Measuring and improving police performance: The lessons of Compstat and its progeny</article-title>. <source>Policing-an International Journal of Police Strategies &amp; Management</source>, <volume>26</volume>, <fpage>439</fpage>-<lpage>453</lpage>.</citation>
</ref>
<ref id="bibr27-0275074011399120">
<citation citation-type="gov">
<collab>New York City Department of Education</collab>. (<year>2010</year>). <source>About us</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://schools.nyc.gov/AboutUs/default.htm">http://schools.nyc.gov/AboutUs/default.htm</ext-link></comment></citation>
</ref>
<ref id="bibr28-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Niven</surname><given-names>P. R.</given-names></name>
</person-group> (<year>2008</year>). <source>Balanced scorecard step-by-step for government and nonprofit agencies</source> (<edition>2nd ed.</edition>). <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr29-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G.</given-names></name>
</person-group> (<year>2006</year>). <source>Assessing teacher preparation program effectiveness: A Pilot examination of value added approaches, interim technical report</source>. <publisher-loc>Baton Rouge, LA</publisher-loc>: <publisher-name>Louisiana State University</publisher-name>.</citation>
</ref>
<ref id="bibr30-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G.</given-names></name>
<name><surname>Porter</surname><given-names>B.</given-names></name>
<name><surname>Patt</surname><given-names>R. M.</given-names></name>
</person-group> (<year>2007</year>). <source>Value-added assessment of teacher preparation in Louisiana: 2004-2006, interim technical report</source>. <publisher-loc>Baton Rouge, LA</publisher-loc>: <publisher-name>Louisiana State University</publisher-name>.</citation>
</ref>
<ref id="bibr31-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G.</given-names></name>
<name><surname>Porter</surname><given-names>B.</given-names></name>
<name><surname>Patt</surname><given-names>R. M.</given-names></name>
<name><surname>Dahir</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <source>Value-added assessment of teacher preparation in Louisiana: 2004-2005 to 2006-2007, interim technical report</source>. <publisher-loc>Baton Rouge, LA</publisher-loc>: <publisher-name>Louisiana State University</publisher-name>.</citation>
</ref>
<ref id="bibr32-0275074011399120">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Provasnik</surname><given-names>S.</given-names></name>
<name><surname>Kewal Ramani</surname><given-names>A.</given-names></name>
<name><surname>Coleman</surname><given-names>M. M.</given-names></name>
<name><surname>Gilbertson</surname><given-names>L.</given-names></name>
<name><surname>Herring</surname><given-names>W.</given-names></name>
<name><surname>Xie</surname><given-names>Q.</given-names></name>
</person-group> (<year>2007</year>). <source>Status of education in rural America</source> (<comment>NCES 2007-040</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Center for Education Statistics, Institute of Education Sciences, U.S. Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr33-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Radin</surname><given-names>B.</given-names></name>
</person-group> (<year>2000</year>). <article-title>The Government Performance and Results Act and the tradition of federal management reform: Square pegs in round holes?</article-title> <source>Journal of Public Administration Research &amp; Theory</source>, <volume>10</volume>(<issue>1</issue>), <fpage>111</fpage>.</citation>
</ref>
<ref id="bibr34-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Strauss</surname><given-names>R. P.</given-names></name>
<name><surname>Bowes</surname><given-names>L. R.</given-names></name>
<name><surname>Marks</surname><given-names>M. S.</given-names></name>
<name><surname>Plesko</surname><given-names>M. R.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Improving teacher preparation and selection: Lessons from the Pennsylvania experience</article-title>. <source>Economics of Education Review</source>, <volume>19</volume>, <fpage>387</fpage>-<lpage>415</lpage>.</citation>
</ref>
<ref id="bibr35-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taylor</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Strengthening the link between performance measurement and decision making</article-title>. <source>Public Administration</source>, <volume>87</volume>, <fpage>853</fpage>-<lpage>871</lpage>.</citation>
</ref>
<ref id="bibr36-0275074011399120">
<citation citation-type="book">
<collab>U.S. General Accounting Office</collab>. (<year>1997</year>). <source>Measuring for results: Analytical challenges in measuring performance</source> (<comment>GAO/HEHS/GGD-97-138</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr37-0275074011399120">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wang</surname><given-names>X.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Performance measurement in budgeting: A Study of county government</article-title>. <source>Public Budgeting and Finance</source>, <volume>20</volume>(<issue>3</issue>), <fpage>102</fpage>-<lpage>118</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>