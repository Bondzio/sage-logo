<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LTJ</journal-id>
<journal-id journal-id-type="hwp">spltj</journal-id>
<journal-title>Language Testing</journal-title>
<issn pub-type="ppub">0265-5322</issn>
<issn pub-type="epub">1477-0946</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0265532212440950</article-id>
<article-id pub-id-type="publisher-id">10.1177_0265532212440950</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Assessing reading comprehension in adolescent low achievers: Subskills identification and task specificity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>van Steensel</surname><given-names>Roel</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Oostdam</surname><given-names>Ron</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>van Gelderen</surname><given-names>Amos</given-names></name>
</contrib>
<aff id="aff1-0265532212440950">University of Amsterdam, the Netherlands</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0265532212440950">Roel van Steensel, University of Amsterdam, Research Institute of Child Development and Education/Kohnstamm Institute, P.O. Box 94208, 1090 GE Amsterdam, the Netherlands. Email: <email>roelvansteensel@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>1</issue>
<fpage>3</fpage>
<lpage>21</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>On the basis of a validation study of a new test for assessing low-achieving adolescents’ reading comprehension skills – the SALT-reading – we analyzed two issues relevant to the field of reading test development. Using the test results of 200 seventh graders, we examined the possibility of identifying reading comprehension subskills and the effects of task specificity on test reliability. Regarding the former, we distinguished three subskills indicating different levels of understanding (‘retrieving’, ‘interpreting’, ‘reflecting’). However, confirmatory factor analyses did not support the presence of these subskills. Task specificity refers to the situation that different tasks within a test are not uniformly difficult for individual test takers, which constitutes a form of error negatively influencing test reliability. However, Generalizability Theory analysis showed that such task-specific effects did not occur: the reliability of the SALT-reading was primarily affected by error associated with the score variance within tasks.</p>
</abstract>
<kwd-group>
<kwd>Generalizability Theory</kwd>
<kwd>low achievers</kwd>
<kwd>reading assessment</kwd>
<kwd>reading comprehension</kwd>
<kwd>reading subskills</kwd>
<kwd>Structural Equation Modeling</kwd>
<kwd>task specificity</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>This paper deals with two issues relevant to the field of reading test development: the possibility of identifying reading comprehension subskills and the effects of task specificity. These issues were examined in a validation study of a new test for assessing low-achieving adolescents’ reading comprehension skills, the SALT-reading.<sup><xref ref-type="fn" rid="fn1-0265532212440950">1</xref></sup> Part of this study was an analysis of the test’s ‘content representativeness’ (<xref ref-type="bibr" rid="bibr24-0265532212440950">Messick, 1995</xref>), the extent to which a test is an adequate representation of the construct domain to be measured. <xref ref-type="bibr" rid="bibr2-0265532212440950">Bachman (2002)</xref> argues that to ensure content representativeness a language test should be construct-based and task-based. The former implies that the test covers important aspects of the ability to be measured. The latter implies that the test constitutes a representative sample of the tasks relevant to the construct domain, across which the tester can generalize to provide an overall assessment of the test taker’s proficiency in this domain. These demands raise two empirical questions: can subskills of reading comprehension be measured by means of reading comprehension tests? And, to what extent can generalizations be made across different reading tasks?</p>
<sec id="section1-0265532212440950">
<title>Reading comprehension subskills</title>
<p>Questions in reading comprehension tests are often classified on the basis of the location of the information to be obtained (local/global text level) and the explicitness of the match between the question and the information in the text, resulting in item categories such as identifying main idea, locating details, or making inferences (<xref ref-type="bibr" rid="bibr6-0265532212440950">Cerdán, Vidal-Abarca, Martínez, Gilabert, &amp; Gil, 2009</xref>; <xref ref-type="bibr" rid="bibr8-0265532212440950">Davis, 1968</xref>; <xref ref-type="bibr" rid="bibr12-0265532212440950">Goldman &amp; Durán, 1988</xref>; <xref ref-type="bibr" rid="bibr28-0265532212440950">OECD, 2003</xref>; <xref ref-type="bibr" rid="bibr30-0265532212440950">Rosenshine, 1980</xref>; <xref ref-type="bibr" rid="bibr32-0265532212440950">Rouet, Vidal-Abarca, Erboul, &amp; Millogo, 2001</xref>; <xref ref-type="bibr" rid="bibr38-0265532212440950">Song, 2008</xref>; <xref ref-type="bibr" rid="bibr42-0265532212440950">Vidal-Abarca, Gilabert, &amp; Rouet, 1998</xref>).</p>
<p>Theoretically, question answering is seen as a categorization mechanism that identifies the type of question and the relevant information sources (<xref ref-type="bibr" rid="bibr6-0265532212440950">Cerdán et al., 2009</xref>; <xref ref-type="bibr" rid="bibr12-0265532212440950">Goldman &amp; Durán, 1988</xref>; <xref ref-type="bibr" rid="bibr13-0265532212440950">Graesser, Gordon, &amp; Brainerd, 1992</xref>; <xref ref-type="bibr" rid="bibr25-0265532212440950">Mosenthal, 1996</xref>; <xref ref-type="bibr" rid="bibr32-0265532212440950">Rouet et al., 2001</xref>; <xref ref-type="bibr" rid="bibr42-0265532212440950">Vidal-Abarca et al., 1998</xref>). Identifying relevant information sources involves activating nodes in a knowledge network consisting of both textual information and background knowledge. <xref ref-type="bibr" rid="bibr32-0265532212440950">Rouet et al. (2001)</xref> argue that the nature of a test item determines how this activation process evolves, regarding both the number of nodes involved and their accessibility. Test items can differ first of all because they focus either on a single concept or on a (hierarchical) set of concepts. In the former case, the number of knowledge nodes to be activated is limited, whereas the latter case requires the activation of a larger number of nodes. Test items can also differ in the match between the wording of the item and the relevant information in the text. If there is an explicit, literal match between question and text, the knowledge node necessary for answering the question can be accessed directly. If there is no explicit match, the node must be accessed indirectly, through the contextual activation of relevant knowledge structures.</p>
<p>Research shows that such differences between test items result in different reading behavior. In two experimental studies involving university and high school students, <xref ref-type="bibr" rid="bibr32-0265532212440950">Rouet et al. (2001)</xref> and <xref ref-type="bibr" rid="bibr42-0265532212440950">Vidal-Abarca et al. (1998)</xref> found that ‘low-level questions’ – focusing on a single concept, requiring only superficial text processing – often resulted in students using a locate-and-memorize strategy, browsing several paragraphs rather quickly and searching a small number of paragraphs per question. On the other hand, ‘high-level questions’ – focusing on a broader set of concepts or concepts higher in a hierarchy, requiring readers to generate more connections between knowledge and text information – often resulted in students using a review-and-integrate strategy, pausing on more paragraphs during their search for the right answer, searching a larger number of paragraphs per question, and showing more search iterations (i.e. sequences of question reading, text reading, and answer writing).</p>
<p>Many tests include the different item types described above, sometimes broadly categorized as lower, higher, and intermediate. Lower level items involve retrieving detailed information, located in specific text segments (<xref ref-type="bibr" rid="bibr28-0265532212440950">OECD, 2003</xref>; <xref ref-type="bibr" rid="bibr30-0265532212440950">Rosenshine, 1980</xref>; <xref ref-type="bibr" rid="bibr32-0265532212440950">Rouet et al., 2001</xref>; <xref ref-type="bibr" rid="bibr38-0265532212440950">Song, 2008</xref>; <xref ref-type="bibr" rid="bibr42-0265532212440950">Vidal-Abarca et al., 1998</xref>). This information is either verbatim or requires a minor conversion, for instance, when the answer is a synonym of a word in the question (<xref ref-type="bibr" rid="bibr6-0265532212440950">Cerdán et al., 2009</xref>; <xref ref-type="bibr" rid="bibr8-0265532212440950">Davis, 1968</xref>; <xref ref-type="bibr" rid="bibr12-0265532212440950">Goldman &amp; Durán, 1988</xref>; <xref ref-type="bibr" rid="bibr28-0265532212440950">OECD, 2003</xref>). In the terminology introduced earlier, these items involve single, directly accessible nodes in the knowledge network. Higher level items involve comprehending a text’s theme or main idea (<xref ref-type="bibr" rid="bibr28-0265532212440950">OECD, 2003</xref>; <xref ref-type="bibr" rid="bibr30-0265532212440950">Rosenshine, 1980</xref>; <xref ref-type="bibr" rid="bibr38-0265532212440950">Song, 2008</xref>), the author’s general purpose, tone, or attitude (<xref ref-type="bibr" rid="bibr8-0265532212440950">Davis, 1968</xref>; <xref ref-type="bibr" rid="bibr28-0265532212440950">OECD, 2003</xref>), and the ability to draw conclusions from or make predictions on the basis of the text (<xref ref-type="bibr" rid="bibr30-0265532212440950">Rosenshine, 1980</xref>). These items involve multiple nodes related on a global text level, which need to be integrated into central, superordinate nodes. Finally, an intermediate category can be distinguished. These items tap the comprehension of the underlying relationships between local pieces of information, such as cause-effect relationships between sentences (<xref ref-type="bibr" rid="bibr28-0265532212440950">OECD, 2003</xref>; <xref ref-type="bibr" rid="bibr30-0265532212440950">Rosenshine, 1980</xref>). They involve the integration of nodes that are not related on the global, but on the local level.</p>
</sec>
<sec id="section2-0265532212440950">
<title>Divisibility of subskills</title>
<p>There is controversy about whether subskills such as those mentioned above are empirically distinguishable by tests (<xref ref-type="bibr" rid="bibr1-0265532212440950">Alderson, 2000</xref>; <xref ref-type="bibr" rid="bibr16-0265532212440950">Kalifa &amp; Weir, 2009</xref>; <xref ref-type="bibr" rid="bibr43-0265532212440950">Weir &amp; Porter, 1994</xref>): while some researchers provide evidence for dimensionality, others question whether separate skills can actually be measured. In an early study, <xref ref-type="bibr" rid="bibr8-0265532212440950">Davis (1968)</xref> established the presence of five skills on the basis of reading test data collected among 12th graders in academic high schools: memory for word meanings; inferencing; following passage structure; recognizing a writer’s purpose, attitude, tone, and mood; finding answers to questions asked explicitly or in paraphrase. Applying factor-analysis to the same data set, <xref ref-type="bibr" rid="bibr39-0265532212440950">Spearitt (1972)</xref> found support for the presence of the first four skills, although the factor intercorrelations were high (.75–.93). In another reanalysis <xref ref-type="bibr" rid="bibr40-0265532212440950">Thorndike (1973–1974)</xref> found two factors: general reading comprehension and word knowledge. <xref ref-type="bibr" rid="bibr35-0265532212440950">Schedl, Gordon, Carey, and Tang (1996)</xref> analyzed the factor structure of the reading comprehension items of the Test of English as a Foreign Language (TOEFL), but did not find evidence for the presence of the two postulated dimensions, that is, ‘reasoning’ (analogy, extrapolation, organization and logic, and author’s purpose/attitude) and ‘other’ (vocabulary, syntax, and explicitly stated information). <xref ref-type="bibr" rid="bibr23-0265532212440950">Meijer and Van Gelderen (2002)</xref> analyzed the Dutch PISA data – collected among students in all levels of secondary education – and found that the three subskills assumedly underlying the PISA reading test were indiscriminable because of very high intercorrelations (around .95). <xref ref-type="bibr" rid="bibr38-0265532212440950">Song (2008)</xref> conducted Structural Equation Modeling to analyze the results of a reading comprehension test for international (under)graduates and found evidence for the presence of two skills (although three were postulated): the ability to understand explicitly versus implicitly stated meaning.</p>
<p><xref ref-type="bibr" rid="bibr43-0265532212440950">Weir and Porter (1994)</xref> argue that divisibility is a function of the level of the readers being tested, claiming that for proficient readers, reading is unidimensional, while for less proficient readers it is not. Similarly, Alderson states: ‘once basic word recognition skills have been acquired, and children are able to understand connected text, the whole reading process becomes so very integrated that, although a variety of skills may be needed, they cannot be separately identified empirically’ (2000, p. 97). However, for beginning readers – including those who learn to read in a second language – it can be assumed that different abilities develop at a different rate, as a result of which proficiency in one skill does not necessarily mean proficiency in another. For low achievers – our target group – it could be assumed that they fall behind in specific kinds of skills but do better in others, making it similarly possible to empirically distinguish between these skills.</p>
</sec>
<sec id="section3-0265532212440950">
<title>Task specificity</title>
<p>The demand that a language test is task-based implies that the test tasks are a representative sample of all tasks relevant to the language use domain of interest (<xref ref-type="bibr" rid="bibr2-0265532212440950">Bachman, 2002</xref>). In reading comprehension tests, tasks usually consist of texts and questions. Together, test takers’ responses to these tasks are taken to provide a valid indication of their ability to understand texts from a particular domain. Texts naturally differ in topic/content, genre, format, and medium of presentation (<xref ref-type="bibr" rid="bibr1-0265532212440950">Alderson, 2000</xref>). This variability has two likely consequences. First, it will lead to score differences between test takers: as a function of their familiarity with a topic, genre, format, and/or medium, some test takers will do better on a particular task than others. Second, it is expected that different configurations of these characteristics cause an individual test taker’s scores to vary from one task to the other; this is also referred to as ‘task specificity’ or ‘case specificity’ (<xref ref-type="bibr" rid="bibr11-0265532212440950">Gagnon, Charlin, Lambert, Carrière, &amp; Van der Vleuten, 2008</xref>; <xref ref-type="bibr" rid="bibr20-0265532212440950">Linn, 1993</xref>; <xref ref-type="bibr" rid="bibr21-0265532212440950">Linn, Baker, &amp; Dunbar, 1991</xref>; <xref ref-type="bibr" rid="bibr26-0265532212440950">Norman, Bordage, Page, &amp; Keane, 2006</xref>).</p>
<p>The effects of task specificity were previously established by means of G(eneralizability) theory analysis in studies on performance-based writing and speaking assessments (<xref ref-type="bibr" rid="bibr18-0265532212440950">Lee, 2006</xref>; <xref ref-type="bibr" rid="bibr34-0265532212440950">Sawaki, 2007</xref>; <xref ref-type="bibr" rid="bibr36-0265532212440950">Schoonen, 2005</xref>; <xref ref-type="bibr" rid="bibr44-0265532212440950">Xi, 2007</xref>). Using ANOVA techniques, G theory analysis aims to identify the different sources of error affecting reliability. The general observation from these studies is that the different writing/speaking tasks within a test are not uniformly difficult for individual test takers: some test takers may be rank-ordered low on task A but high on task B, while for others the reverse is true, even though both tasks assumedly measure the same skill. This phenomenon – which in G theory analysis is understood as an interaction effect of test takers and tasks – constitutes a form of error affecting a test’s reliability (<xref ref-type="bibr" rid="bibr5-0265532212440950">Cardinet, Johnson, &amp; Pini, 2010</xref>; <xref ref-type="bibr" rid="bibr37-0265532212440950">Shavelson &amp; Webb, 1991</xref>), which is best countered by increasing the number of tasks.</p>
<p>Apart from the targeted skill, reading comprehension tests are different from writing and speaking assessments because they generally have a nested structure: comprehension questions are nested within tasks. Hence, it is possible that error variance is not only constituted by the score variance between tasks (between-group variance in ANOVA terms), but also by the score variance within tasks (within-group variance). Evidence for the importance of within-task variance comes from G theory analyses of medical knowledge assessments, which have nested structures comparable to those in reading tests (<xref ref-type="bibr" rid="bibr11-0265532212440950">Gagnon et al., 2008</xref>; <xref ref-type="bibr" rid="bibr26-0265532212440950">Norman et al., 2006</xref>): in both studies within-task variance was found to be the most important error source, while the contribution of between-task variance was very small or even absent. <xref ref-type="bibr" rid="bibr11-0265532212440950">Gagnon et al. (2008)</xref> and <xref ref-type="bibr" rid="bibr26-0265532212440950">Norman et al. (2006)</xref> also showed that, contrary to writing/speaking assessments, improvements in the reliability of these nested tests did not result from increasing the number of tasks, but from increasing the number of items per task (whereas the number of tasks could be decreased).</p>
<p>It is worthwhile examining whether the same mechanism is true for reading comprehension tests, not in the least because this would have an important practical advantage: since each extra task requires a test taker to read additional text, increasing the number of items per task while decreasing the number of tasks will likely result in shorter administration times.</p>
</sec>
<sec id="section4-0265532212440950">
<title>Research questions</title>
<p>The aim of this study was twofold. First, we wanted to examine the assumption that reading comprehension subskills can be distinguished in low achievers. Second, we wanted to examine to what extent the reliability of the SALT-reading was affected by task specificity and, consequently, what the optimal task/item ratio is for obtaining a sufficient reliability level. These aims lead to the following research questions:</p>
<list id="list1-0265532212440950" list-type="simple">
<list-item><p>1. To what extent does the SALT-reading measure different subskills?</p></list-item>
<list-item><p>2A. To what extent is the reliability of the SALT-reading affected by task specificity?</p></list-item>
<list-item><p>2B. What is the optimal number of tasks and items within tasks to maximize test reliability within a fixed administration time?</p></list-item></list>
</sec>
<sec id="section5-0265532212440950" sec-type="methods">
<title>Method</title>
<sec id="section6-0265532212440950">
<title>Participants</title>
<p>We administered the SALT-reading to 200 seventh graders from the lowest tracks of Dutch prevocational secondary education, the basic and middle-management programs (BP and MMP, respectively). This population is characterized by poor reading skills: PISA results revealed that, on average, ninth-grade students in these tracks only reach level 1 (BP) or 2 (MMP) of the PISA reading levels (<xref ref-type="bibr" rid="bibr9-0265532212440950">De Knecht-Van Eekelen, Gille, &amp; Van Rijn, 2007</xref>). The students were spread across 10 classes from nine schools. Of these, 38% were girls and 62% were boys; 48% were of native Dutch families, 38% were of nonnative families, and 14% were of mixed families. On 1 January 2008, the year of administration, the students had a mean age of 12.79 years (<italic>SD</italic> = 0.66).</p>
</sec>
<sec id="section7-0265532212440950">
<title>The SALT-reading</title>
<p>The SALT-reading was developed in an international study by the universities of Amsterdam and Utrecht (the Netherlands), the Ontario Institute for Studies in Education at the University of Toronto (Canada), and the Service de la Recherche en Education of the Canton of Geneva (Switzerland). This paper concerns the Dutch part of the project.</p>
<p>The test consists of a series of tasks, each comprising one or more texts followed by comprehension questions. Starting from the PISA definition of reading literacy, which acknowledges that reading involves understanding, using, and reflecting on written information for a variety of purposes and in a variety of situations (<xref ref-type="bibr" rid="bibr28-0265532212440950">OECD, 2003</xref>), we included relatively many tasks. To ensure the test was sufficiently task-based, we strived for maximum coverage of different configurations of topic/content, genre, format, and medium. We additionally assumed that, given the possibility of the effects of task specificity, maximizing the number of tasks would increase the chance of reaching a sufficient reliability. Given practical limitations (we could only plan three 45-minute sessions), we included nine tasks, the maximum students were able to complete in the allotted time in a pilot study conducted beforehand.</p>
<p>We started by identifying media types students likely come across regularly: (school) books, newspapers and magazines, official documents, and the internet. Within these media we searched for texts about topics relevant to our target group, representing different combinations of genres and formats. To find texts with relevant topics we inspected sources we assumed to be familiar to adolescents: children’s books, educational materials for students in secondary education, newspapers and magazines that were readily available (neighborhood newsletters, TV guides) or written especially for our target group (youth magazines), official documents students are likely acquainted with (house rules), and websites students might visit for recreational or information seeking purposes. Regarding genre we made a distinction between narrative, argumentative, expository, and instructive texts. Regarding format we made a distinction between continuous texts and discontinuous texts (i.e. texts containing lists, tables, graphs). This led to an initial pool of 14 texts.</p>
<p>For each text we constructed items based on the distinction between lower, intermediate, and higher levels of understanding (‘Divisibility of subskills’ section), which we labeled ‘retrieving’, ‘interpreting’, and ‘reflecting’, respectively. Retrieving is the ability to locate relevant details in the text, interpreting is the ability to make inferences about shorter passages (e.g. identifying causal relationships between sentences), and reflecting is the ability to make inferences about larger passages or the text as a whole (e.g. articulating the main idea or the writer’s intention). We ensured every task included items from all three categories.</p>
<p>On the basis of the results of a pilot study among 550 students (grades 7–9 from the target population), to whom we administered subsets of tasks, we selected nine tasks. The selection was based on psychometric analyses and the outcomes of interviews with 40 of the students. We based our decision to include tasks on (i) the average difficulty levels of the tasks: tasks that were too easy or difficult were excluded to prevent ceiling or floor effects; (ii) the contribution of the items in a task to overall test reliability: if many items in a task had low item-total correlations (&lt;.20), the task was omitted; and (iii) students’ judgments about the relevance and difficulty level of the texts and items. The latter was additionally investigated in a qualitative study among Dutch language teachers who were asked to judge the test (<xref ref-type="bibr" rid="bibr7-0265532212440950">Daas, Havermans, &amp; Van Noortwijk, 2009</xref>). The findings corroborated our assumption that the test tasks resemble those students have to be able to carry out in daily (academic) life. <xref ref-type="table" rid="table1-0265532212440950">Table 1</xref> provides an overview of the tasks.</p>
<table-wrap id="table1-0265532212440950" position="float">
<label>Table 1.</label>
<caption>
<p>Overview of reading tasks</p>
</caption>
<graphic alternate-form-of="table1-0265532212440950" xlink:href="10.1177_0265532212440950-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Medium</th>
<th align="left">Task</th>
<th align="left">Topic</th>
<th align="left">Genre</th>
<th align="left">Format</th>
<th align="left">Length (words)</th>
<th align="left" colspan="4"><italic>N</italic> items<hr/></th>
</tr>
<tr>
<th/>
<th/>
<th/>
<th/>
<th/>
<th/>
<th align="left">Total</th>
<th align="left">Retrieving</th>
<th align="left">Interpreting</th>
<th align="left">Reflecting</th>
</tr>
</thead>
<tbody>
<tr>
<td>(School) books</td>
<td>T1</td>
<td>Negative stereotyping</td>
<td>Narrative</td>
<td>Continuous</td>
<td>736</td>
<td>7</td>
<td>1</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td/>
<td>T2</td>
<td rowspan="2">Colonial history of Vietnam</td>
<td>Text 1: Narrative</td>
<td>Continuous</td>
<td>201</td>
<td>8</td>
<td>2</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td/>
<td/>
<td>Text 2: Expository</td>
<td>Continuous</td>
<td>202</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td rowspan="3">Newspapers/magazines</td>
<td>T3</td>
<td>Self-confidence</td>
<td>Narrative</td>
<td>Continuous</td>
<td>642</td>
<td>9</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>T4</td>
<td>An extraordinary profession</td>
<td>Expository</td>
<td>Continuous</td>
<td>415</td>
<td>7</td>
<td>3</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td>T5</td>
<td>Big city crime rates</td>
<td>Expository</td>
<td>Discontinuous</td>
<td>287</td>
<td>5</td>
<td>3</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td/>
<td>T6</td>
<td>Cell phones in school</td>
<td>Text 1: Argumentative</td>
<td>Continuous</td>
<td>306</td>
<td>8</td>
<td>3</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>Text 2: Expository</td>
<td>Continuous</td>
<td>501</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td>T7</td>
<td>Advantages biofuel</td>
<td>Argumentative</td>
<td>Continuous</td>
<td>572</td>
<td>8</td>
<td>2</td>
<td>5</td>
<td>1</td>
</tr>
<tr>
<td>Official documents</td>
<td>T8</td>
<td>Youth hostel house rules</td>
<td>Instructive</td>
<td>Discontinuous</td>
<td>411</td>
<td>7</td>
<td>2</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>Internet</td>
<td>T9</td>
<td>Information camping site</td>
<td>Web page 1–4, 6–8: Expository</td>
<td>Discontinuous</td>
<td>736</td>
<td>6</td>
<td>2</td>
<td>3</td>
<td>1</td>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>Web page 5: Instructive</td>
<td>Discontinuous</td>
<td>67</td>
<td/>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0265532212440950"><p><italic>Note</italic>: T = task</p></fn>
</table-wrap-foot>
</table-wrap>
<p>We constructed 65 items: 21 retrieving, 26 interpreting, and 18 reflecting items. To establish interrater reliability, all items were also categorized by an independent rater: the agreement was satisfactory (<italic>r</italic> = .83). All nine tasks included multiple-choice items (57 items), which mostly provided four choices. Six tasks also included open-ended items (8 items). Both types covered retrieving, interpreting, and reflecting items. For each of the open-ended items, we wrote a model answer. Mostly, test takers could be awarded 0 (wrong) or 1 (right). In some cases, there was an in-between score: for one item, for example, test takers had to describe how the protagonist of a story felt and why. Test takers were awarded 0.5 if they provided an appropriate feeling and another 0.5 if they provided a valid argument. All answers were coded twice. The interrater agreement was substantial (mean Cohen’s kappa = 0.73).</p>
<p>Not all media were equally well represented: the test includes only one official document and only one internet text. The former is a result of the fact that official documents are restrictive in the genres they allow. The latter is an outcome of the pilot study: we tested two internet tasks, but for one most items made very limited contributions to overall test reliability and we excluded it.</p>
</sec>
<sec id="section8-0265532212440950">
<title>Procedure</title>
<p>The test was administered to whole classes in three 45-minute sessions. We scheduled no more than two sessions per day to minimize test weariness. Eight tasks were paper-and-pencil assignments; the internet task was administered in a computer room. All sessions were introduced by a researcher or trained test leader. A familiar teacher was present to maintain order. Students’ questions were answered by the test leaders according to a standardized protocol.</p>
</sec>
<sec id="section9-0265532212440950">
<title>Analyses</title>
<sec id="section10-0265532212440950">
<title>Identifying subskills</title>
<p>To examine whether subskills of reading comprehension could be identified, confirmatory factor analyses were performed using Structural Equation Modeling software (EQS 6.1; <xref ref-type="bibr" rid="bibr3-0265532212440950">Bentler &amp; Wu, 2002</xref>). We compared a first-order single-factor model assuming the test measures one underlying ability with a hierarchical second-order model assuming the test measures three dimensions that load on one general reading comprehension factor. Item parcels were entered as observed variables. Parcels are aggregated indicators comprised of, in this case, the sum of multiple items (<xref ref-type="bibr" rid="bibr22-0265532212440950">Little, Cunningham, &amp; Shahar, 2002</xref>). The parcels were compiled by means of (stratified) random sampling. First, the 65 items were classified on the basis of the three hypothesized subskills, resulting in three sets of items: 21 retrieving, 26 interpreting, and 18 reflecting items. Subsequently, three samples were randomly drawn from each of these three sets, resulting in a total of nine samples. The items from these samples were added to form nine parcels (RTR1-2-3 with seven items in each parcel; INT1-2-3 with nine items in the first two parcels and eight in the third; RFL1-2-3 with six items in each parcel). In order to use all available information, the EM procedure in EQS was used to deal with missing values.</p>
<p>Several indicators were used to judge model fit (<xref ref-type="bibr" rid="bibr10-0265532212440950">Dunn, Everitt, &amp; Pickles, 1993</xref>; <xref ref-type="bibr" rid="bibr14-0265532212440950">Hu &amp; Bentler, 1999</xref>; <xref ref-type="bibr" rid="bibr41-0265532212440950">Ullman, 2001</xref>): the χ<sup>2</sup>-test, which should be nonsignificant; the ratio χ<sup>2</sup>/<italic>df</italic>, which should be less than 2; the comparative fit index (CFI), which should be above .90; the standardized root mean square residual (SRMR), which should be below .08; and the root mean square error of approximation (RMSEA), which should be below .06. The χ<sup>2</sup> difference test was used to compare both models (<xref ref-type="bibr" rid="bibr41-0265532212440950">Ullman, 2001</xref>).</p>
</sec>
<sec id="section11-0265532212440950">
<title>Task specificity</title>
<p>We examined the possible effects of task specificity using G theory analysis, which allows researchers to disentangle the multiple error sources affecting test reliability and helps to explore ways of improving measurement precision (<xref ref-type="bibr" rid="bibr4-0265532212440950">Brennan, 2001</xref>; <xref ref-type="bibr" rid="bibr5-0265532212440950">Cardinet, Johnson, &amp; Pini, 2010</xref>; <xref ref-type="bibr" rid="bibr37-0265532212440950">Shavelson &amp; Webb, 1991</xref>). G theory analysis involves two steps. In the first, the generalizability (G) study, the observed measurement is decomposed into different variance components (‘facets’). A distinction is made between differentiation, instrumentation, and generalization variance (<xref ref-type="bibr" rid="bibr5-0265532212440950">Cardinet, Johnson, &amp; Pini, 2010</xref>). Differentiation variance is similar to true score variance in classical test theory (CTT). Instrumentation variance is the variance caused by the measurement conditions, here the tasks in the test (T), the test items nested in the tasks (I:T), and the interactions between students and tasks (S×T), and between students and items in tasks (S×I:T). Generalization variance is similar to error variance in CTT, the part of the variance attributable to fluctuations arising from the random selection of the components of the measurement procedure as well as to purely random, unidentified error. Because in our case G theory analysis assumes relative measurement (the aim is to position students relative to one another in a score distribution), generalization variance is only constituted by the interaction terms – S×T and S×I:T – where the former constitutes the between-task error variance and the latter the within-task error variance. The interaction term S×I:T also includes the residual error (e). The results of the G study are summarized in the standard error of measurement and the generalizability coefficient, which, like the reliability coefficient in CTT, reflects the proportion of systematic variability in students’ scores (<xref ref-type="bibr" rid="bibr37-0265532212440950">Shavelson &amp; Webb, 1991</xref>).</p>
<p>The second step is the decision (D) study, which examines the effects of applying alternative measurement designs on minimizing error and maximizing reliability. In the case of multiple instrumentation facets, this could involve increasing the number of values (‘levels’) for those facets that contribute most to measurement error while decreasing the number of levels for facets with low impact on measurement error (<xref ref-type="bibr" rid="bibr5-0265532212440950">Cardinet, Johnson, &amp; Pini, 2010</xref>).</p>
<p>We used the software package EduG 6.0 (<xref ref-type="bibr" rid="bibr15-0265532212440950">IRDP, 2010</xref>). The fact that the test constituted an unbalanced nested design – the tasks comprised unequal numbers of items (see <xref ref-type="table" rid="table1-0265532212440950">Table 1</xref>) – required that we perform an additional operation before conducting the actual G theory analysis. Following <xref ref-type="bibr" rid="bibr5-0265532212440950">Cardinet, Johnson, and Pini (2010)</xref>, we computed sums of squares in which the tasks were weighted according to the numbers of items they comprised. These sums of squares were entered into EduG.</p>
</sec>
</sec>
</sec>
<sec id="section12-0265532212440950" sec-type="results">
<title>Results</title>
<sec id="section13-0265532212440950">
<title>Descriptive statistics</title>
<p><xref ref-type="table" rid="table2-0265532212440950">Table 2</xref> presents the descriptive statistics for the test as a whole, the subskill parcels used in the SEM analyses, and the task scores used in the G theory analyses. Of the 200 students 163 made all parts of the test. The EM procedure was applied to use the scores of the other 37 students (see ‘Identifying subskills’ subsection) as well. Therefore, we also present the numbers of students that completed the items in each of the subskill parcels and each of the nine tasks.</p>
<table-wrap id="table2-0265532212440950" position="float">
<label>Table 2.</label>
<caption>
<p>Descriptive statistics</p>
</caption>
<graphic alternate-form-of="table2-0265532212440950" xlink:href="10.1177_0265532212440950-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Variable</th>
<th align="left"><italic>N</italic></th>
<th align="left">Range</th>
<th align="left"><italic>M</italic></th>
<th align="left"><italic>SD</italic></th>
<th align="left">Kurtosis (SE)</th>
<th align="left">% correct</th>
</tr>
</thead>
<tbody>
<tr>
<td><italic>Total</italic></td>
<td>163</td>
<td>16.83–56</td>
<td>40.78</td>
<td>7.87</td>
<td>−0.15 (0.38)</td>
<td>63</td>
</tr>
<tr>
<td colspan="7"><italic>Subskills</italic></td>
</tr>
<tr>
<td>RTR1</td>
<td>171</td>
<td>1–7</td>
<td>4.97</td>
<td>1.26</td>
<td>0.33 (0.37)</td>
<td>71</td>
</tr>
<tr>
<td>RTR2</td>
<td>173</td>
<td>2–7</td>
<td>4.87</td>
<td>1.28</td>
<td>−0.59 (0.37)</td>
<td>70</td>
</tr>
<tr>
<td>RTR3</td>
<td>165</td>
<td>0–7</td>
<td>4.26</td>
<td>1.41</td>
<td>0.05 (0.38)</td>
<td>61</td>
</tr>
<tr>
<td>INT1</td>
<td>168</td>
<td>1–9</td>
<td>5.93</td>
<td>1.61</td>
<td>0.47 (0.37)</td>
<td>66</td>
</tr>
<tr>
<td>INT2</td>
<td>164</td>
<td>1–9</td>
<td>5.02</td>
<td>1.64</td>
<td>−0.50 (0.38)</td>
<td>56</td>
</tr>
<tr>
<td>INT3</td>
<td>170</td>
<td>1–8</td>
<td>5.61</td>
<td>1.50</td>
<td>0.19 (0.37)</td>
<td>70</td>
</tr>
<tr>
<td>RFL1</td>
<td>171</td>
<td>0–6</td>
<td>2.82</td>
<td>1.23</td>
<td>−0.44 (0.37)</td>
<td>47</td>
</tr>
<tr>
<td>RFL2</td>
<td>167</td>
<td>0–6</td>
<td>3.50</td>
<td>1.39</td>
<td>−0.54 (0.37)</td>
<td>58</td>
</tr>
<tr>
<td>RFL3</td>
<td>171</td>
<td>0–6</td>
<td>3.68</td>
<td>1.40</td>
<td>−0.27 (0.37)</td>
<td>61</td>
</tr>
<tr>
<td colspan="7"><italic>Tasks</italic></td>
</tr>
<tr>
<td>T1</td>
<td>191</td>
<td>0–7</td>
<td>4.59</td>
<td>1.55</td>
<td>0.49 (0.35)</td>
<td>66</td>
</tr>
<tr>
<td>T2</td>
<td>191</td>
<td>0–8</td>
<td>4.76</td>
<td>1.53</td>
<td>−0.05 (0.35)</td>
<td>60</td>
</tr>
<tr>
<td>T3</td>
<td>192</td>
<td>1.50–9</td>
<td>7.13</td>
<td>1.56</td>
<td>1.19 (0.35)***</td>
<td>79</td>
</tr>
<tr>
<td>T4</td>
<td>184</td>
<td>0–7</td>
<td>4.18</td>
<td>1.54</td>
<td>−0.49 (0.36)</td>
<td>60</td>
</tr>
<tr>
<td>T5</td>
<td>186</td>
<td>0–5</td>
<td>2.58</td>
<td>1.26</td>
<td>−0.84 (0.36)*</td>
<td>52</td>
</tr>
<tr>
<td>T6</td>
<td>182</td>
<td>1–8</td>
<td>5.08</td>
<td>1.58</td>
<td>−0.19 (0.36)</td>
<td>64</td>
</tr>
<tr>
<td>T7</td>
<td>182</td>
<td>0–8</td>
<td>4.57</td>
<td>1.69</td>
<td>0.01 (0.36)</td>
<td>57</td>
</tr>
<tr>
<td>T8</td>
<td>187</td>
<td>0–7</td>
<td>4.76</td>
<td>1.52</td>
<td>−0.06 (0.35)</td>
<td>68</td>
</tr>
<tr>
<td>T9</td>
<td>187</td>
<td>0–6</td>
<td>3.07</td>
<td>1.34</td>
<td>−0.41 (0.35)</td>
<td>51</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0265532212440950"><p><italic>Note</italic>: RTR = retrieving; INT = interpreting; RFL = reflecting; T = task</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The overall mean shows that the test is not too difficult for the students in our sample, neither is it too easy: on average, about two thirds of their answers were correct. There is some variation between the tasks in mean percentage of correct scores (51%–79%). <xref ref-type="bibr" rid="bibr45-0265532212440950">Yuan, Lambert, and Fouladi’s (2004)</xref> normalized multivariate kurtosis estimate proved to be nonsignificant for both the subskill parcels (0.40) and the task scores (−1.82), which implies that the multivariate normality assumption was not violated. For the subskill parcels this meant that Maximum Likelihood estimation could be used for the SEM analyses. In only two cases were the univariate kurtosis estimates statistically significant (T3 and T5). The Cronbach’s alpha reliability coefficient was .80.</p>
</sec>
<sec id="section14-0265532212440950">
<title>Identifying subskills</title>
<p>The two confirmatory factor models are presented in <xref ref-type="fig" rid="fig1-0265532212440950">Figures 1</xref> and <xref ref-type="fig" rid="fig2-0265532212440950">2</xref>. The model fit results and the outcome of the χ<sup>2</sup> difference test are given in <xref ref-type="table" rid="table3-0265532212440950">Table 3</xref>. Note that in the second-order model the error variances (indicated by D for ‘Disturbance’) associated with the first-order factors were constrained to be equal for the purpose of solving parameter conditions encountered during optimization.</p>
<fig id="fig1-0265532212440950" position="float">
<label>Figure 1.</label>
<caption>
<p>Subskills: Single-factor model</p>
</caption>
<graphic xlink:href="10.1177_0265532212440950-fig1.tif"/></fig>
<fig id="fig2-0265532212440950" position="float">
<label>Figure 2.</label>
<caption>
<p>Subskills: second-order factor model with three dimensions</p>
</caption>
<graphic xlink:href="10.1177_0265532212440950-fig2.tif"/></fig>
<table-wrap id="table3-0265532212440950" position="float">
<label>Table 3.</label>
<caption>
<p>Model fit results<sup><xref ref-type="fn" rid="fn2-0265532212440950">2</xref></sup></p>
</caption>
<graphic alternate-form-of="table3-0265532212440950" xlink:href="10.1177_0265532212440950-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left">a) 1 factor (<italic>N</italic> = 176)</th>
<th align="left">b) 2nd order (<italic>N</italic> = 176)</th>
<th align="left">Difference test (a) − (b)</th>
</tr>
</thead>
<tbody>
<tr>
<td>χ<sup>2</sup></td>
<td>23.61, <italic>df</italic> = 27, <italic>p</italic> = .65</td>
<td>22.65, <italic>df</italic> = 26, <italic>p</italic> = .65</td>
<td>0.96, <italic>df</italic> = 1, <italic>p &gt;</italic> .05</td>
</tr>
<tr>
<td>χ<sup>2</sup>/<italic>df</italic></td>
<td>0.87</td>
<td>0.87</td>
<td/>
</tr>
<tr>
<td>CFI</td>
<td>1.00</td>
<td>1.00</td>
<td/>
</tr>
<tr>
<td>SRMR</td>
<td>.04</td>
<td>.04</td>
<td/>
</tr>
<tr>
<td>RMSEA</td>
<td>.00</td>
<td>.00</td>
<td/>
</tr>
<tr>
<td>Constraints</td>
<td/>
<td><inline-formula id="inline-formula1-0265532212440950">
<mml:math display="inline" id="math1-0265532212440950">
<mml:mrow>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mrow>
<mml:mtext>D</mml:mtext>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> = <inline-formula id="inline-formula2-0265532212440950">
<mml:math display="inline" id="math2-0265532212440950">
<mml:mrow>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mrow>
<mml:mtext>D</mml:mtext>
<mml:mn>2</mml:mn>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> = <inline-formula id="inline-formula3-0265532212440950">
<mml:math display="inline" id="math3-0265532212440950">
<mml:mrow>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mrow>
<mml:mtext>D</mml:mtext>
<mml:mn>3</mml:mn>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula></td>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
<p>Both models have a good fit (the χ<sup>2</sup>-values are not significant, the χ<sup>2</sup>/<italic>df</italic> ratios are below 2, the CFIs are above .90, and the SRMRs and RMSEAs are below .08 and .06, respectively). However, the χ<sup>2</sup> difference test shows that the second-order model is no significant improvement of the more parsimonious, single-factor model. Thus, the data do not provide evidence for the presence of subskills, but support the assumption that the test is measuring only one underlying ability. The validity of the single-factor solution is further corroborated by the fact that in the second-order model the paths between the second- and first-order factors have very high regression weights (.94−.99).</p>
</sec>
<sec id="section15-0265532212440950">
<title>Task specificity</title>
<p>The complete results of the G study are presented in <xref ref-type="table" rid="table4-0265532212440950">Table 4</xref>. The table includes both the relative and absolute error variances, but given our assumption of relative measurement, we will only discuss the former.</p>
<table-wrap id="table4-0265532212440950" position="float">
<label>Table 4.</label>
<caption>
<p>G study outcomes</p>
</caption>
<graphic alternate-form-of="table4-0265532212440950" xlink:href="10.1177_0265532212440950-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Source</th>
<th align="left">Differentiation variance</th>
<th align="left">Source</th>
<th align="left">Relative error variance</th>
<th align="left">% relative</th>
<th align="left">Absolute error variance</th>
<th align="left">% absolute</th>
</tr>
</thead>
<tbody>
<tr>
<td>S</td>
<td>0.01230</td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>T</td>
<td/>
<td/>
<td>0.00000</td>
<td>0.0</td>
</tr>
<tr>
<td/>
<td/>
<td>I:T</td>
<td/>
<td/>
<td>0.00080</td>
<td>15.3</td>
</tr>
<tr>
<td/>
<td/>
<td>SxT</td>
<td>0.00000</td>
<td>0.0</td>
<td>0.00000</td>
<td>0.0</td>
</tr>
<tr>
<td/>
<td/>
<td>SxI:T, e</td>
<td>0.00443</td>
<td>100.0</td>
<td>0.00443</td>
<td>84.7</td>
</tr>
<tr>
<td>Sum of variances</td>
<td>0.01230</td>
<td/>
<td>0.00443</td>
<td>100.0</td>
<td>0.00524</td>
<td>100.0</td>
</tr>
<tr>
<td>SD</td>
<td>0.11093</td>
<td/>
<td>Relative SE:</td>
<td>0.06659</td>
<td>Absolute SE:</td>
<td>0.07237</td>
</tr>
<tr>
<td colspan="2">G coefficient relative</td>
<td>0.74</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td colspan="2">G coefficient absolute</td>
<td>0.70</td>
<td/>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0265532212440950"><p><italic>Note</italic>: S = Students; T = Tasks; I:T = Items in Tasks; e = (residual) error</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Three variance estimates are of importance. The first is the variance estimate of the student facet S (0.01230), which concerns the differentiation or true score variance. The second and third are the relative error variances associated with the interaction between students and tasks (S×T) and the interaction between students and items in tasks plus residual error (S×I:T, e), concerning the between-task and within-task error variance, respectively. The variance estimates for these interactions (column 4) show that the generalization or error variance (0.00443) was completely caused by S×I:T, e. This first of all suggests that the task effects described in Section IV did not occur: score variability was not caused by individual test takers scoring differently from one task to the other. It further implies that test score variability was mainly determined by individual test takers scoring differently from one item to the other and by other, undifferentiated sources of error (these two error sources cannot be disentangled in G theory analysis). Finally, the fact that the differentiation variance (0.01230) was nearly three times larger than the generalization variance (indicated by a G coefficient of 0.74) implies that a substantial proportion of the total score variance can be attributed to true score variance, that is, to differences in students’ actual reading comprehension ability. The G coefficient is somewhat lower than the Cronbach’s alpha coefficient reported earlier (0.80). This is likely because the G coefficient takes the nested design into account (Lee, 1999).</p>
<p>Subsequently, a D study was performed. Since the G study showed that S×I:T, e fully explained generalization variance, the obvious way of maximizing the test’s reliability is by increasing the number of items per task (as a consequence of which the number of tasks can be decreased). Assuming that three tasks could be comfortably fitted within one test session of 45 minutes, we made predictions of how many items would be needed per task to reach a satisfactory G coefficient of .80, when the number of tasks was decreased from nine to three and six (implying one and two test sessions, respectively). To examine the other possible solution to improving reliability (increasing the number of tasks, while decreasing the number of items per task), we also included the predicted G coefficients when the number of tasks was increased to 12 (implying four sessions). Finally, we computed the number of items needed per task when the current number of tasks (nine) remained the same. The results are in <xref ref-type="fig" rid="fig3-0265532212440950">Figure 3</xref>.</p>
<fig id="fig3-0265532212440950" position="float">
<label>Figure 3.</label>
<caption>
<p>D study results for 3, 6, 9, and 12 tasks and 5–21 items per task</p>
</caption>
<graphic xlink:href="10.1177_0265532212440950-fig3.tif"/></fig>
<p><xref ref-type="fig" rid="fig3-0265532212440950">Figure 3</xref> shows that in the case of three tasks 21 items per task would be needed to obtain a G coefficient of .80, which more than surpasses the number of questions one can reasonably pose about texts such as those included in the test. Increasing the number of tasks to six would imply 11 items per task, which is two items more than the maximum number of items per task in our current test (nine) and six items more than the minimum number (five). If we were to keep the same number of tasks as in the current test (nine), a minimum of seven items per task would be needed to reach a satisfactory G coefficient. Increasing the number of tasks to 12 would imply a minimum of six items per task.</p>
</sec>
</sec>
<sec id="section16-0265532212440950" sec-type="discussion">
<title>Discussion</title>
<p>We examined two issues relevant to the field of reading test development by analyzing the outcomes of a new test designed to assess the reading comprehension ability of low achievers in secondary education. The first issue concerned the identification of reading comprehension subskills. Although our assumption was that such subskills are more readily identified in low achievers, SEM analyses provided no evidence for dimensionality: a second-order model in which reading comprehension was assumed to consist of three dimensions (retrieving, interpreting, reflecting) proved no significant improvement on a single-factor model. This outcome is in line with the results of studies that failed to distinguish reading comprehension subskills (e.g. <xref ref-type="bibr" rid="bibr23-0265532212440950">Meijer &amp; Van Gelderen, 2002</xref>; <xref ref-type="bibr" rid="bibr35-0265532212440950">Schedl et al., 1996</xref>) and could be seen as supporting the claim that reading comprehension is a single, unitary ability, even for low-achieving readers (<xref ref-type="bibr" rid="bibr31-0265532212440950">Rost, 1993</xref>). Further explanation of this observation can be sought in cognitive models of reading comprehension. <xref ref-type="bibr" rid="bibr17-0265532212440950">Kintsch’s (1998)</xref> construction-integration model, for instance, makes clear that, during reading, several different processes are at work. Readers process local, micro-level information in the form of propositions, they connect these propositions to background knowledge, and to preceding and subsequent micro-level information in order to establish local cohesion, and they use this micro-level information in a constant process of macrostructure formation. The evolving macrostructure, in turn, guides micro-level processing, particularly by focusing the reader’s attention on elements relevant to the gist of the text. The fact that these processes are mutually dependent may have caused the unidimensionality observed in our analyses.</p>
<p>The question remains why other researchers, using similar analytical techniques, were able to identify subskills. In reviews of studies on this topic, <xref ref-type="bibr" rid="bibr16-0265532212440950">Kalifa and Weir (2009)</xref> and <xref ref-type="bibr" rid="bibr43-0265532212440950">Weir and Porter (1994)</xref> suggest that the ability to separate subskills is affected by both test and test taker characteristics. One possibility is that identifying dimensionality is hampered by a test being too easy: <xref ref-type="bibr" rid="bibr38-0265532212440950">Song (2008)</xref>, for instance, suggests that the fact that she found evidence for only two out of three assumed subskills was a result of the students in his study performing relatively well. Although the students in our sample were low achievers, the test was not overly difficult for them (nor was it disproportionately easy). Following Song’s suggestion a more challenging test could have increased the chance of exposing the postulated dimensions. The question remains, however, where in this respect the boundary between a sufficiently and an overly challenging test lies: after all, if a test is too difficult, limited variability in item scores due to floor effects will make it hard to find relationships among items.</p>
<p>Additionally, our sample consisted mostly of native speakers of Dutch and L2 students who were born in the Netherlands and had learned to read and write in Dutch from an early age. Had the sample included more beginning L2 readers, a different pattern might have emerged, as <xref ref-type="bibr" rid="bibr43-0265532212440950">Weir and Porter (1994)</xref> suggest.</p>
<p>Finally, item format might have played a part. Our test comprised mainly multiple-choice items. It has been shown that using such items affects the reading process. In a small-scale study, <xref ref-type="bibr" rid="bibr33-0265532212440950">Rupp, Ferne, and Choi (2006)</xref> made clear that multiple-choice questions (even those aiming to assess higher-level skills) can trigger test takers to divide a text in chunks aligned with individual questions. Consequently, the test taker focuses on the microstructure rather than the macrostructure, thereby automatically limiting his/her use of higher-order inferences. Other answer formats might have yielded other results.</p>
<p>The second issue analyzed in this study is the extent to which test reliability is influenced by the situation that the tasks within a test are not uniformly difficult for individual test takers (task specificity). A G study revealed that this interaction of test takers and tasks did not contribute to generalization (or error) variance: in other words, the hypothesized task-specific effects did not occur. Instead, generalization variance was fully explained by the interaction between test takers and items in tasks plus residual error, which parallels the outcomes of G theory analyses in other types of task-based assessments (<xref ref-type="bibr" rid="bibr11-0265532212440950">Gagnon et al., 2008</xref>; <xref ref-type="bibr" rid="bibr26-0265532212440950">Norman et al., 2006</xref>). This outcome resulted in the observation that increasing the number of items per task (instead of increasing the number of tasks) was the most efficacious way of maximizing the test’s reliability. This procedure has important limitations, however. In theory, reliability could be guaranteed by a test consisting of a single task including many items, but this would result in both practical and validity problems. First of all, there are only so many questions one can reasonably pose about texts such as the ones included in our test: a single-task test would require a long text for a test developer to devise a sufficient number of items. Moreover, for a test to be content valid it should have sufficient coverage of relevant characteristics of the domain of interest (<xref ref-type="bibr" rid="bibr1-0265532212440950">Alderson, 2000</xref>): a single-task test will normally not be enough to provide such coverage. Given these limitations as well as time considerations (three 45-minute sessions seemed to be the maximum we could ask from students and schools), keeping the current number of tasks (nine), while ensuring a minimum number of seven items per task seemed the optimal solution. In other situations (and countries) administration times might even be more limited. We believe our study gives clear indications for coping with such limitations.</p>
<p>What do these conclusions imply for the validity of the SALT-reading? We started this paper by referring to <xref ref-type="bibr" rid="bibr2-0265532212440950">Bachman’s (2002)</xref> claim that a test should be both construct-based and task-based. If we assume that subskills of reading comprehension exist, we must conclude that our test was not able to identify them. However, if we consider reading comprehension to be a unitary construct, the unidimensionality of the test scores – as indicated by the good fit of the single-factor model – can be seen as being strongly supportive of the validity of the test. To ensure our test was task-based we decided to include a relatively large and diverse set of reading tasks relevant to our target population and we assumed that this would lead to a sufficient reliability. While the Cronbach’s alpha reliability coefficient had a satisfactory value of .80, the G coefficient – which is theoretically more appropriate for nested test designs (<xref ref-type="bibr" rid="bibr19-0265532212440950">Lee &amp; Frisbie, 1999</xref>) – was .74. Although this suffices for basic research purposes (<xref ref-type="bibr" rid="bibr27-0265532212440950">Nunnally &amp; Bernstein, 1994</xref>), a reliability of at least .80 is desirable. As we have shown, this can be obtained within the practical limitations of test administration by increasing the number of items per task.</p>
<p>Finally, what do our conclusions imply for validity research into other, similar tests? First, we recommend that in the future test developers and researchers either be careful in their claims that reading tests can be used to diagnose specific reading problems or validate these claims by using qualitative studies to confirm that particular categories of test items trigger particular types of reading behavior (<xref ref-type="bibr" rid="bibr16-0265532212440950">Kalifa &amp; Weir, 2009</xref>). The latter authors suggest, for example, to incorporate think-aloud studies in the development process to examine whether test items elicit the assumed underlying processes (they call this the ‘cognitive validity’ of a test). We also recommend that, in addition to content validity considerations, outcomes of G theory analyses be used for deciding on the number of tasks and items to be included in reading tests. G theory analyses serve as a useful tool for reading test developers in determining the optimal balance between tasks and items, particularly because they allow taking the nested structure of such tests into account.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-0265532212440950">
<title>Appendix A</title>
<sec id="section17-0265532212440950">
<title>Subskill 1: Retrieving</title>
<sec id="section18-0265532212440950">
<title>Text fragment (T4)</title>
<p>Berghammer, whose stage name is Odo7, is an aroma jockey. He gives odor shows at dance parties, concerts and business presentations. Born in Austria, he used to be a visual artist: he painted, illustrated children’s books, made sculptures, and had his own advertising agency. (<italic>Source</italic>: Quest magazine (2006), G+J Uitgevers C.V.)</p>
</sec>
<sec id="section19-0265532212440950">
<title>Example item</title>
<list id="list2-0265532212440950" list-type="simple">
<list-item><p>Where does Erich Berghammer originally come from?</p></list-item>
<list-item><p>The Netherlands</p></list-item>
<list-item><p>Germany</p></list-item>
<list-item><p>Japan</p></list-item>
<list-item><p>Austria (correct)</p></list-item></list>
</sec>
</sec>
<sec id="section20-0265532212440950">
<title>Subskill 2: Interpreting</title>
<sec id="section21-0265532212440950">
<title>Text fragment (T3)</title>
<p>One more chance. That was all I got. Suddenly, I remembered my instructor sticking his tongue out once and waving his hands by his ears. ‘That’s what the board’s doing,’ he had said to me. I closed my eyes and pictured myself cracking the board in half. (<italic>Source</italic>: Stone Soup (2000), Children’s Art Foundation)</p>
</sec>
<sec id="section22-0265532212440950">
<title>Example item</title>
<list id="list3-0265532212440950" list-type="simple">
<list-item><p>Why does the instructor tell the narrator that the board is making a face at her?</p></list-item>
<list-item><p>To make her angry at the board (correct)</p></list-item>
<list-item><p>To make her laugh</p></list-item>
<list-item><p>To calm her down through humor</p></list-item>
<list-item><p>To embarrass her in front of her friends</p></list-item>
</list>
</sec>
</sec>
<sec id="section23-0265532212440950">
<title>Subskill 3: Reflecting</title>
<sec id="section24-0265532212440950">
<title>Example item (T4)</title>
<list id="list4-0265532212440950" list-type="simple">
<list-item><p>What is the writer’s objective for this article?</p></list-item>
<list-item><p>He wants to show how you can become an aroma jockey.</p></list-item>
<list-item><p>He wants to explain why dance parties are always so smelly.</p></list-item>
<list-item><p>He wants to tell how Erich has become an aroma jockey and what his work involves. (correct)</p></list-item>
<list-item><p>He wants to explain how you can influence people with fragrances.</p></list-item></list>
</sec>
</sec>
</app>
</app-group>
<ack><p>The project was funded by the Netherlands Organization for Scientific Research (NWO; project number 411-06-504).We thank the schools and students participating in the study as well as Suzanne Jak, Rudy Ligtvoet, and Frans Oort for their statistical advice and Jean Cardinet for his advice on EduG.</p></ack>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0265532212440950">
<label>1</label>
<p>The SALT (SAlsa Literacy Test) was named after the Salsa project, a study into predictors of at-risk adolescents’ literacy development.</p></fn>
<fn fn-type="other" id="fn2-0265532212440950">
<label>2</label>
<p>We also ran the models without applying the EM procedure. The outcomes were very much alike. For the single-factor model χ<sup>2</sup> = 22.38, <italic>df</italic> = 27, <italic>p</italic> = .72, χ<sup>2</sup>/df = 0.83, CFI = 1.00, SRMR = .04, RMSEA = .00. For the second-order model χ<sup>2</sup> = 21.63, <italic>df</italic> = 26, <italic>p</italic> = .71, χ<sup>2</sup>/<italic>df</italic> = 0.83, CFI = 1.00, SRMR = .04, RMSEA = .00. The χ<sup>2</sup>-difference test value is 0.75, <italic>df</italic> = 1, <italic>p</italic> &gt; .05.</p></fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alderson</surname><given-names>J. C.</given-names></name>
</person-group> (<year>2000</year>). <source>Assessing reading</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bachman</surname><given-names>L. F.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Some reflections on task-based language performance assessment</article-title>. <source>Language Testing</source>, <volume>19</volume>(<issue>4</issue>), <fpage>453</fpage>–<lpage>476</lpage>.</citation>
</ref>
<ref id="bibr3-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bentler</surname><given-names>P. M.</given-names></name>
<name><surname>Wu</surname><given-names>E.</given-names></name>
</person-group> (<year>2002</year>). <source>EQS for Windows: User’s guide</source>. <publisher-loc>Encino</publisher-loc>: <publisher-name>Multivariate Software</publisher-name>.</citation>
</ref>
<ref id="bibr4-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2001</year>). <source>Generalizability theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr5-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cardinet</surname><given-names>J.</given-names></name>
<name><surname>Johnson</surname><given-names>S.</given-names></name>
<name><surname>Pini</surname><given-names>G.</given-names></name>
</person-group> (<year>2010</year>). <source>Applying Generalizability Theory using EduG</source>. <publisher-loc>New York/London</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr6-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cerdán</surname><given-names>R.</given-names></name>
<name><surname>Vidal-Abarca</surname><given-names>E.</given-names></name>
<name><surname>Martínez</surname><given-names>T.</given-names></name>
<name><surname>Gilabert</surname><given-names>R.</given-names></name>
<name><surname>Gil</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Impact of question-answering tasks on search processes and reading comprehension</article-title>. <source>Learning and Instruction</source>, <volume>19</volume>(<issue>1</issue>), <fpage>13</fpage>–<lpage>27</lpage>.</citation>
</ref>
<ref id="bibr7-0265532212440950">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Daas</surname><given-names>R.</given-names></name>
<name><surname>Havermans</surname><given-names>R.</given-names></name>
<name><surname>Van Noortwijk</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Een nieuwe test voor het meten van leesbegripsvaardigheden bij leerlingen op het VMBO: Resultaten van een valideringsonderzoek gericht op de inhoudsvaliditeit</article-title>. [<trans-title xml:lang="en">A new test for measuring reading comprehension skills in students in prevocational secondary education: Results of a validition study focused on content validity</trans-title>]. Unpublished manuscript, <publisher-name>University of Amsterdam</publisher-name>.</citation>
</ref>
<ref id="bibr8-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Davis</surname><given-names>F. B.</given-names></name>
</person-group> (<year>1968</year>). <article-title>Research in comprehension in reading</article-title>. <source>Reading Research Quarterly</source>, <volume>3</volume>(<issue>4</issue>), <fpage>499</fpage>–<lpage>545</lpage>.</citation>
</ref>
<ref id="bibr9-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>De Knecht-Van Eekelen</surname><given-names>A.</given-names></name>
<name><surname>Gille</surname><given-names>E.</given-names></name>
<name><surname>Van Rijn</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <source>Resultaten PISA-2006: Praktische kennis en vaardigheden van 15-jarigen</source> [<trans-source xml:lang="en">Results PISA-2006: Practical knowledge and skills of 15-year-olds</trans-source>]. <publisher-loc>Arnhem</publisher-loc>: <publisher-name>Cito</publisher-name>.</citation>
</ref>
<ref id="bibr10-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dunn</surname><given-names>G.</given-names></name>
<name><surname>Everitt</surname><given-names>B.</given-names></name>
<name><surname>Pickles</surname><given-names>A.</given-names></name>
</person-group> (<year>1993</year>). <source>Modelling covariances and latent variables using EQS</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Chapman &amp; Hall</publisher-name>.</citation>
</ref>
<ref id="bibr11-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gagnon</surname><given-names>R.</given-names></name>
<name><surname>Charlin</surname><given-names>B.</given-names></name>
<name><surname>Lambert</surname><given-names>C.</given-names></name>
<name><surname>Carrière</surname><given-names>B.</given-names></name>
<name><surname>Van der Vleuten</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Script concordance testing: More cases or more questions?</article-title> <source>Advances in Health Sciences Education</source>, <volume>14</volume>(<issue>3</issue>), <fpage>367</fpage>–<lpage>375</lpage>.</citation>
</ref>
<ref id="bibr12-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldman</surname><given-names>S. R.</given-names></name>
<name><surname>Durán</surname><given-names>R. P.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Answering questions from oceanography texts: Learner, task, and text characteristics</article-title>. <source>Discourse Processes</source>, <volume>11</volume>(<issue>4</issue>), <fpage>373</fpage>–<lpage>412</lpage>.</citation>
</ref>
<ref id="bibr13-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Graesser</surname><given-names>A. C.</given-names></name>
<name><surname>Franklin</surname><given-names>S. P.</given-names></name>
</person-group> (<year>1992</year>). <article-title>QUEST: A model of question answering</article-title>. <source>Discourse Processes</source>, <volume>13</volume>, <fpage>279</fpage>–<lpage>303</lpage>.</citation>
</ref>
<ref id="bibr14-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hu</surname><given-names>L.</given-names></name>
<name><surname>Bentler</surname><given-names>P. M.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Cutoff criteria for fit indexes in covariance structure analysis: Conventional criteria versus new alternatives</article-title>. <source>Structural Equation Modeling</source>, <volume>6</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr15-0265532212440950">
<citation citation-type="book">
<collab>IRDP</collab> (<year>2010</year>). <source>EduG.User guide</source>. <publisher-loc>Neuchatel</publisher-loc>: <publisher-name>IRDP</publisher-name>.</citation>
</ref>
<ref id="bibr16-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kalifa</surname><given-names>H.</given-names></name>
<name><surname>Weir</surname><given-names>C. J.</given-names></name>
</person-group> (<year>2009</year>). <source>Examining reading: Research and practice in assessing second language reading</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kintsch</surname><given-names>W.</given-names></name>
</person-group> (<year>1998</year>). <source>Comprehension: A paradigm for cognition</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr18-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>Y. W.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Dependability of scores for a new ESL speaking assessment consisting of integrated and independent tasks</article-title>. <source>Language Testing</source>, <volume>23</volume>(<issue>2</issue>), <fpage>131</fpage>–<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr19-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>G.</given-names></name>
<name><surname>Frisbie</surname><given-names>D. A.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Estimating reliability under a generalizability theory model for test scores composed of testlets</article-title>. <source>Applied Measurement in Education</source>, <volume>12</volume>(<issue>3</issue>), <fpage>237</fpage>–<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr20-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Linn</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Assessment. Expanded expectations and challenges</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>15</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>16</lpage>.</citation>
</ref>
<ref id="bibr21-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Linn</surname><given-names>R. L.</given-names></name>
<name><surname>Baker</surname><given-names>E. L.</given-names></name>
<name><surname>Dunbar</surname><given-names>S. B.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Complex, performance-based assessment: Expectations and validity criteria</article-title>. <source>Educational Researcher</source>, <volume>20</volume>(<issue>8</issue>), <fpage>15</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr22-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Little</surname><given-names>T. D.</given-names></name>
<name><surname>Cunningham</surname><given-names>W. A.</given-names></name>
<name><surname>Shahar</surname><given-names>G.</given-names></name>
</person-group> (<year>2002</year>). <article-title>To parcel or not to parcel: Exploring the question, weighing the merits</article-title>. <source>Structural Equation Modeling</source>, <volume>9</volume>(<issue>2</issue>), <fpage>151</fpage>–<lpage>173</lpage>.</citation>
</ref>
<ref id="bibr23-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Meijer</surname><given-names>J.</given-names></name>
<name><surname>Van Gelderen</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <source>Lezen voor het leven: Een empirische vergelijking van een nationale en een internationale leesvaardigheidspeiling</source> [<trans-source xml:lang="en">Reading for life: An empirical comparison of a national and an international reading assessment</trans-source>]. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>SCO-Kohnstamminstituut</publisher-name>.</citation>
</ref>
<ref id="bibr24-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Messick</surname><given-names>S.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Validity of psychological assessment: Validation of inferences from persons’ responses and performances as scientific inquiry into score meaning</article-title>. <source>American Psychologist</source>, <volume>50</volume>(<issue>9</issue>), <fpage>741</fpage>–<lpage>749</lpage>.</citation>
</ref>
<ref id="bibr25-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mosenthal</surname><given-names>P. B.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Understanding the strategies of document literacy and their conditions of use</article-title>. <source>Journal of Educational Psychology</source>, <volume>88</volume>(<issue>2</issue>), <fpage>314</fpage>–<lpage>332</lpage>.</citation>
</ref>
<ref id="bibr26-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Norman</surname><given-names>G.</given-names></name>
<name><surname>Bordage</surname><given-names>G.</given-names></name>
<name><surname>Page</surname><given-names>G.</given-names></name>
<name><surname>Keane</surname><given-names>D.</given-names></name>
</person-group> (<year>2006</year>). <article-title>How specific is case specificity?</article-title> <source>Medical Education</source>, <volume>40</volume>(<issue>7</issue>), <fpage>618</fpage>–<lpage>623</lpage>.</citation>
</ref>
<ref id="bibr27-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Nunnally</surname><given-names>J. C.</given-names></name>
<name><surname>Bernstein</surname><given-names>I. H.</given-names></name>
</person-group> (<year>1994</year>). <source>Psychometric theory</source> (3rdedn). <publisher-loc>New York</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr28-0265532212440950">
<citation citation-type="book">
<collab>OECD</collab> (<year>2003</year>). <source>The PISA 2003 assessment framework: Mathematics, reading, science and problem solving knowledge and skills</source>. <publisher-loc>Paris</publisher-loc>: <publisher-name>OECD</publisher-name>.</citation>
</ref>
<ref id="bibr29-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pearson</surname><given-names>P. D.</given-names></name>
<name><surname>Hamm</surname><given-names>D. N.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The assessment of reading comprehension: A review of practices – past, present, and future</article-title>. In <person-group person-group-type="editor">
<name><surname>Paris</surname><given-names>S. G.</given-names></name>
<name><surname>Stahl</surname><given-names>S. A.</given-names></name>
</person-group> (Eds.),<source>Children’s reading comprehension and assessment</source> (pp. <fpage>13</fpage>–<lpage>70</lpage>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr30-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rosenshine</surname><given-names>B.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Skills hierarchies in reading comprehension</article-title>. In <person-group person-group-type="editor">
<name><surname>Spiro</surname><given-names>R. J.</given-names></name>
<name><surname>Bruce</surname><given-names>B. C.</given-names></name>
<name><surname>Brewer</surname><given-names>W. F.</given-names></name>
</person-group> (Eds.), <source>Theoretical issues in reading comprehension: Perspectives from cognitive psychology, linguistics, artificial intelligence, and education</source> (pp. <fpage>535</fpage>–<lpage>554</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr31-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rost</surname><given-names>D. H.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Assessing different components of reading comprehension: Fact or fiction?</article-title> <source>Language Testing</source>, <volume>10</volume>(<issue>1</issue>), <fpage>79</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr32-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rouet</surname><given-names>J.-F.</given-names></name>
<name><surname>Vidal-Abarca</surname><given-names>E.</given-names></name>
<name><surname>Erboul</surname><given-names>A. B.</given-names></name>
<name><surname>Millogo</surname><given-names>V.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Effects of information search tasks on the comprehension of instructional text</article-title>. <source>Discourse Processes</source>, <volume>31</volume>(<issue>2</issue>), <fpage>163</fpage>–<lpage>186</lpage>.</citation>
</ref>
<ref id="bibr33-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rupp</surname><given-names>A. A.</given-names></name>
<name><surname>Ferne</surname><given-names>T.</given-names></name>
<name><surname>Choi</surname><given-names>H.</given-names></name>
</person-group> (<year>2006</year>). <article-title>How assessing reading comprehension with multiple-choice questions shapes the construct: A cognitive processing perspective</article-title>. <source>Language Testing</source>, <volume>23</volume>(<issue>4</issue>), <fpage>441</fpage>–<lpage>474</lpage>.</citation>
</ref>
<ref id="bibr34-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sawaki</surname><given-names>Y.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Construct validation of analytic rating scales in a speaking assessment: Reporting a score profile and a composite</article-title>. <source>Language Testing</source>, <volume>24</volume>(<issue>3</issue>), <fpage>355</fpage>–<lpage>390</lpage>.</citation>
</ref>
<ref id="bibr35-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schedl</surname><given-names>M.</given-names></name>
<name><surname>Gordon</surname><given-names>A.</given-names></name>
<name><surname>Carey</surname><given-names>P. A.</given-names></name>
<name><surname>Tang</surname><given-names>K. L.</given-names></name>
</person-group> (<year>1996</year>). <source>An analysis of the dimensionality of TOEFL reading comprehension items</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr36-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schoonen</surname><given-names>R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Generalizability of writing scores: An application of structural equation modeling</article-title>. <source>Language Testing</source>, <volume>22</volume>(<issue>1</issue>), <fpage>1</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr37-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shavelson</surname><given-names>R. J.</given-names></name>
<name><surname>Webb</surname><given-names>N. M.</given-names></name>
</person-group> (<year>1991</year>). <source>Generalizability theory: A primer</source>. <publisher-loc>Newbury Park, CA</publisher-loc>: <publisher-name>SAGE Publications</publisher-name>.</citation>
</ref>
<ref id="bibr38-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Song</surname><given-names>M. Y.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Do divisible subskills exist in second language (L2) comprehension? A structural equation modeling approach</article-title>. <source>Language Testing</source>, <volume>25</volume>(<issue>4</issue>), <fpage>435</fpage>–<lpage>464</lpage>.</citation>
</ref>
<ref id="bibr39-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spearitt</surname><given-names>D.</given-names></name>
</person-group> (<year>1972</year>). <article-title>Identification of subskills of reading comprehension by maximum likelihood factor analysis</article-title>. <source>Reading Research Quarterly</source>, <volume>8</volume>(<issue>1</issue>), <fpage>92</fpage>–<lpage>111</lpage>.</citation>
</ref>
<ref id="bibr40-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thorndike</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1973–1974</year>). <article-title>Reading as reasoning</article-title>. <source>Reading Research Quarterly</source>, <volume>9</volume>(<issue>1</issue>), <fpage>135</fpage>–<lpage>147</lpage>.</citation>
</ref>
<ref id="bibr41-0265532212440950">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ullman</surname><given-names>J. B.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Structural Equation Modeling</article-title>. In <person-group person-group-type="editor">
<name><surname>Tabachnick</surname><given-names>B. G.</given-names></name>
<name><surname>Fidell</surname><given-names>L. S.</given-names></name>
</person-group> (Eds.), <source>Using multivariate statistics</source> (<edition>4th edn.</edition>, pp. <fpage>653</fpage>–<lpage>771</lpage>). <publisher-loc>Needham Heights, MA</publisher-loc>: <publisher-name>Allyn &amp; Bacon</publisher-name>.</citation>
</ref>
<ref id="bibr42-0265532212440950">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Vidal-Abarca</surname><given-names>E.</given-names></name>
<name><surname>Gilabert</surname><given-names>R.</given-names></name>
<name><surname>Rouet</surname><given-names>J.-F.</given-names></name>
</person-group> (<year>1998</year>). <article-title>The role of question type on learning from scientific text</article-title>. <conf-name>Paper presented at Seminario ‘Comprension y produccion de textos cientificos’</conf-name>, <conf-loc>Aveiro, Portugal</conf-loc>.</citation>
</ref>
<ref id="bibr43-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weir</surname><given-names>C. J.</given-names></name>
<name><surname>Porter</surname><given-names>D.</given-names></name>
</person-group> (<year>1994</year>). <article-title>The multi-divisible or unitary nature of reading: The language tester between Scylla and Charybdis</article-title>. <source>Reading in a Foreign Language</source>, <volume>10</volume>, <fpage>1</fpage>–<lpage>19</lpage>.</citation>
</ref>
<ref id="bibr44-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Xi</surname><given-names>X.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Evaluating analytic scoring for the TOEFL® Academic Speaking Test (TAST) for operational use</article-title>. <source>Language Testing</source>, <volume>24</volume>(<issue>2</issue>), <fpage>251</fpage>–<lpage>286</lpage>.</citation>
</ref>
<ref id="bibr45-0265532212440950">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yuan</surname><given-names>K. H.</given-names></name>
<name><surname>Lambert</surname><given-names>P. L.</given-names></name>
<name><surname>Fouladi</surname><given-names>R. T.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Mardia’s multivariate kurtosis with missing data</article-title>. <source>Multivariate Behavioral Research</source>, <volume>39</volume>(<issue>3</issue>), <fpage>413</fpage>–<lpage>437</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>