<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EVI</journal-id>
<journal-id journal-id-type="hwp">spevi</journal-id>
<journal-title>Evaluation</journal-title>
<issn pub-type="ppub">1356-3890</issn>
<issn pub-type="epub">1461-7153</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1356389012460961</article-id>
<article-id pub-id-type="publisher-id">10.1177_1356389012460961</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>S&amp;T indicators as a tool for formative evaluation of research programs</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lepori</surname><given-names>Benedetto</given-names></name>
<aff id="aff1-1356389012460961">University of Lugano, Switzerland</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Reale</surname><given-names>Emanuela</given-names></name>
<aff id="aff2-1356389012460961">CNR-CERIS, Italy</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-1356389012460961">Benedetto Lepori, Centre for Organizational Research, Faculty of Economics, University of Lugano, via Lambertenghi 10a, 6904 Lugano, Switzerland. Email: <email>blepori@unisi.ch</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>18</volume>
<issue>4</issue>
<fpage>451</fpage>
<lpage>465</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The aim of this article is to review the potential of Science and Technology (S&amp;T) Indicators for the evaluation of research programs. We suggest that indicators are a useful complement to other evaluation methodologies (surveys, case studies, panels) for summative evaluations, where the focus is on measuring program results and the degree of achievement of program objectives. However, we argue that indicators have a much broader potential to support formative evaluation, where the focus is on learning from past experience in order to design future research programs. Indicators, for example, are a valuable support for debates between social actors concerning strategic choices about research funding. We conclude by suggesting what needs to be done to realize this potential.</p>
</abstract>
<kwd-group>
<kwd>evaluation</kwd>
<kwd>positioning indicators</kwd>
<kwd>research programs</kwd>
<kwd>S&amp;T indicators</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1356389012460961" sec-type="intro">
<title>Introduction</title>
<p>Ten years ago, Hariolf Grupp remarked that the literature dealing with the use of Science and Technology (S&amp;T) indicators for program evaluation was limited (<xref ref-type="bibr" rid="bibr23-1356389012460961">Grupp, 2000</xref>) and mostly focused on bibliometric indicators. An assessment of the current situation would come to similar conclusions. Despite the longstanding debate on the increased use of quantitative indicators to complement qualitative approaches (<xref ref-type="bibr" rid="bibr18-1356389012460961">European Court of Auditors, 2007</xref>; <xref ref-type="bibr" rid="bibr58-1356389012460961">Roessner, 2000</xref>), and despite the rapid development of the S&amp;T indicators domain, as well as of their use in other domains of evaluation like funding agencies and research organizations (<xref ref-type="bibr" rid="bibr13-1356389012460961">Campbell, 2003</xref>; <xref ref-type="bibr" rid="bibr29-1356389012460961">HEFCE (Higher Education Funding Council for England), 2007</xref>; <xref ref-type="bibr" rid="bibr72-1356389012460961">Whitley and Glaser, 2007</xref>), there are few examples of the systematic use of indicators in S&amp;T program evaluation. Reviews by expert panels and impact assessment using large-scale surveys on projects and grant recipients have been until now the most widespread approaches (<xref ref-type="bibr" rid="bibr20-1356389012460961">Georghiou and Larédo, 2006</xref>), while in the most well-known evaluation manuals (<xref ref-type="bibr" rid="bibr31-1356389012460961">IPTS, 2002</xref>; <xref ref-type="bibr" rid="bibr60-1356389012460961">Ruegg and Feller, 2003</xref>) S&amp;T indicators have a minor role.</p>
<p>A recent study on European Framework Programs (EU FPs) also comes to disappointing conclusions (<xref ref-type="bibr" rid="bibr56-1356389012460961">Proneos, OST, NIFU-Step and CERIS-CNR, 2009</xref>): the existing indicators are hardly suitable to provide evidence for most of the envisaged evaluation questions, while the designing of ad hoc indicators would be a resource-consuming process, in many cases severely limited by the availability of data.</p>
<p>One could argue that S&amp;T indicators are not well suited to this purpose and that the debate should be discarded; it is the consequence of a fascination with numbers (<xref ref-type="bibr" rid="bibr58-1356389012460961">Roessner, 2000</xref>), and we should accept the inherently fuzzy and subjective nature of program evaluation (<xref ref-type="bibr" rid="bibr19-1356389012460961">Feller, 2007</xref>). This argument is supported by two methodological problems regarding the use of S&amp;T indicators for evaluation, namely <italic>demarcation</italic> – it is difficult to attribute research results to a specific funding program – and <italic>time-lags</italic> – most outputs become evident long after the end of the program (<xref ref-type="bibr" rid="bibr23-1356389012460961">Grupp, 2000</xref>).</p>
<p>However, our thesis is that these limitations are also related to underlying conceptions of S&amp;T indicators as exact measures, providing quantitative answers to questions, as well as to a largely summative conception of program evaluation, where the main goal is to objectively assess the degree of achievement of program goals. Instead, we argue that a broader and richer contribution of indicators can be envisaged within a conception of program evaluation oriented towards formative evaluation – learning lessons from the past in order to improve future programs (<xref ref-type="bibr" rid="bibr67-1356389012460961">Thoenig, 2000</xref>). In addition they can contribute to a socially-constructed discourse among the stakeholders involved in program operations, rather than as a top-down process ordered by the program promoter or funder.</p>
<p>We further argue that, while this conception is of broader significance for evaluation activities as a whole, since it is related to constructivist (<xref ref-type="bibr" rid="bibr25-1356389012460961">Guba and Lincoln, 1989</xref>) and theory-based approaches to evaluation (<xref ref-type="bibr" rid="bibr64-1356389012460961">Stame, 2004</xref>), it is particularly significant in the case of research programs, as they are typically multi-actor and occur in highly distributed settings, with multiple and in some cases conflicting goals. While the recognition of these characteristics has driven research program evaluation towards formative and constructivist approaches (e.g. see <xref ref-type="bibr" rid="bibr34-1356389012460961">Kuhlmann, 1998</xref>; <xref ref-type="bibr" rid="bibr49-1356389012460961">Molas-Gallart and Davies, 2005</xref>), the implications for the use of S&amp;T indicators have been hardly developed.</p>
<p>In the following, we take forward our argument in three steps. In section 2, we provide introductory notions on the epistemological status and development of S&amp;T indicators, while also highlighting their strength and weaknesses for the purposes of program evaluation. In section 3, we develop our core argument on research programs as multi-actor interaction spaces and on evaluation as a mediation process among the actors’ interests and needs. In section 4, we derive a framework for the use of S&amp;T indicators in evaluation, we highlight their potential, and we draw practical implications for how to handle their integration in evaluation processes. We conclude with a short discussion of more general implications.</p>
</sec>
<sec id="section2-1356389012460961">
<title>The epistemological and social foundations of S&amp;T indicators</title>
<p>The field of S&amp;T indicators was first developed through pioneering work during the 1960s at the OECD – focusing on measures of research input and especially R&amp;D expenditures and human resources (<xref ref-type="bibr" rid="bibr22-1356389012460961">Godin, 2005</xref>) – and by scholars interested in systematic ways of measuring scientific production (<xref ref-type="bibr" rid="bibr44-1356389012460961">Martin and Irvine, 1983</xref>). During the last three decades, the field has broadened and professionalized through the definition of methodological standards, the development of large-scale databases, as well as the creation of specialized centers (<xref ref-type="bibr" rid="bibr40-1356389012460961">Lepori et al., 2008</xref>).</p>
<p>While most of the works on indicators have adopted a positivistic view, assuming them to be objective measures of properties of S&amp;T systems, a small body of literature, rooted in insights from the sociology of measurement and statistics (<xref ref-type="bibr" rid="bibr16-1356389012460961">Desrosières, 2008</xref>; <xref ref-type="bibr" rid="bibr55-1356389012460961">Porter, 1995</xref>), has developed a conceptualization of them as socio-cognitive constructs, which reflect underlying assumptions on the world and thus depend on the values and interests of the actors developing them (<xref ref-type="bibr" rid="bibr22-1356389012460961">Godin, 2005</xref>). As shown in the case of scientific performance of countries, even the simplest indicators provide different results depending on the assumptions on how they should be constructed and normalized (<xref ref-type="bibr" rid="bibr5-1356389012460961">Barré, 2001</xref>).</p>
<p>Does this mean that indicators are nothing more than instruments to legitimize decisions already taken on different grounds and to impose the power of the State on society, as some sociological works suggest (<xref ref-type="bibr" rid="bibr22-1356389012460961">Godin, 2005</xref>)?</p>
<p>We consider this position too extreme. Very much like all kinds of measures, indicators depend on the underlying representations of reality and on the actors’ frames of reference; nevertheless, they are not arbitrary. On the contrary, a socio-constructivist approach highlights some central criteria to assess their quality and reliability, including the soundness of the theoretical framework and its applicability to the specific situation, the extent to which indicators have been validated through other kinds of measures and sources of errors have been detected and, finally, the transparency about the objectives and value choices of the actors involved.</p>
<p>Two central implications are that indicators are <italic>context-specific</italic> and debatable. They need to be interpreted taking into account their specific context of usage, limitations of applicability, and errors of measurement. Moreover, the actors might provide diverging interpretations of the same indicator, based on their frames of reference and goals. In other words, indicators do not provide answers to policy and evaluative questions; on the contrary, they are tools to structure and foster debate in multi-actor social spaces (<xref ref-type="bibr" rid="bibr6-1356389012460961">Barré, 2004</xref>). As we shall see, this makes them a potentially relevant tool in the framework of participative and moderating conceptions of S&amp;T evaluation (<xref ref-type="bibr" rid="bibr34-1356389012460961">Kuhlmann, 1998</xref>).</p>
<p>Since there is considerable fuzziness over where to draw the distinction between data and indicators, it is useful to distinguish between: <italic>descriptors</italic> – describing some aspects of reality without leading to further interpretation; <italic>markers</italic> – used in place of quantities that cannot be measured directly (e.g. patents as a marker of technological outputs); and <italic>indicators</italic> – explicitly building connections between quantities and non-observable properties. While descriptors are largely adopted in program evaluation (e.g. basic statistics on selection, participation, and results), our focus is especially on the use of more theory-based indicators, which allow to make statements on deeper issues like quality of research, societal impacts, or the fairness of selection processes.</p>
<sec id="section3-1356389012460961">
<title>A short overview of the field of S&amp;T indicators</title>
<p>A consultation of the most recent handbook of quantitative S&amp;T studies (<xref ref-type="bibr" rid="bibr46-1356389012460961">Moed et al., 2004</xref>) reveals an impressive range of available indicators, as well as a broadening scope of their usage. The recent Innovation Union Competitiveness Report published by the European Union also documents the large number of S&amp;T indicators available at the European level (<xref ref-type="bibr" rid="bibr17-1356389012460961">European Commission, 2011</xref>).</p>
<p>A simple categorization rests on the so-called linear model of innovation, which has driven the development of S&amp;T indicators from the 1960s onwards (<xref ref-type="bibr" rid="bibr22-1356389012460961">Godin, 2005</xref>), thus leading to a distinction among input, output, and impact indicators that is still largely adopted also in S&amp;T evaluation (<xref ref-type="bibr" rid="bibr49-1356389012460961">Molas-Gallart and Davies, 2005</xref>). Within the domain of input indicators, the OECD has taken on a leading role by developing standards for measuring research expenditure and funding (<italic>Frascati Manual</italic>; <xref ref-type="bibr" rid="bibr52-1356389012460961">OECD, 2002</xref>), innovation (<italic>Oslo Manual</italic>; <xref ref-type="bibr" rid="bibr53-1356389012460961">OECD, 2005</xref>), as well as human resources devoted to S&amp;T (<italic>Canberra Manual</italic>; <xref ref-type="bibr" rid="bibr51-1356389012460961">OECD, 1995</xref>).</p>
<p>Indicators on scientific outputs have focused on the analysis of scientific publications through bibliometrics, probably the most developed and fastest growing domain in S&amp;T indicators overall (<xref ref-type="bibr" rid="bibr70-1356389012460961">Van Raan, 2004</xref>), which is increasingly characterized by a plurality of methods and data sources (e.g. to better cover social sciences and humanities; <xref ref-type="bibr" rid="bibr50-1356389012460961">Nederhof, 2006</xref>). Concerning technological outputs, patents have become a standard indicator, thanks to the availability of large international databases (<xref ref-type="bibr" rid="bibr12-1356389012460961">Breschi and Lissoni, 2004</xref>). The broader field of transfer of research activities to economy and society (‘third mission’) remains, however, rather poorly covered (<xref ref-type="bibr" rid="bibr26-1356389012460961">Gulbrandsen and Slipersaeter, 2007</xref>). In the more general area of economic impact, considerable progress has been made regarding innovation indicators, through data collection from the Community Innovation Survey (<xref ref-type="bibr" rid="bibr7-1356389012460961">Bloch, 2007</xref>). A few additional indicators on economic impacts are routinely used in analyses like the European Innovation Scoreboard. These include: SMEs introducing product or process innovation, employment in high-tech firms and services, manufacturing and service exports, net-to-market and net-to-service sales. On the contrary, indicators measuring the social and environmental impact of research activities are probably the least developed domains (<xref ref-type="bibr" rid="bibr43-1356389012460961">Luukkonen, 1998</xref>).</p>
<p>A relevant tendency for evaluation purposes is the shift from national aggregates – comparing national innovation systems and their performance – to indicators at the level of individual actors – e.g. funding agencies (<xref ref-type="bibr" rid="bibr41-1356389012460961">Lepori et al., 2007</xref>) or higher education institutions (<xref ref-type="bibr" rid="bibr8-1356389012460961">Bonaccorsi et al., 2007</xref>) – as well as to indicators characterizing the position and linkages of individual actors – the so-called <italic>positioning indicators</italic> (<xref ref-type="bibr" rid="bibr40-1356389012460961">Lepori et al., 2008</xref>). Interesting applications for research evaluation are studies mapping the structure of scientific fields combining different types of information, like bibliometrics, data on funded projects and policy documents (<xref ref-type="bibr" rid="bibr45-1356389012460961">Merkx and Van den Besselaar, 2008</xref>). From a technical point of view, the development of software tools for the indexing of large bodies of heterogeneous documents allows this approach to be extended beyond the search strategies on databases (<xref ref-type="bibr" rid="bibr23-1356389012460961">Grupp, 2000</xref>), including also textual documents like project descriptions and policy documents, thus avoiding taxonomic problems (<xref ref-type="bibr" rid="bibr47-1356389012460961">Mogoutov and Kahane, 2007</xref>).</p>
</sec>
<sec id="section4-1356389012460961">
<title>Lessons from past experiences</title>
<p>To what extent have concepts and methods developed in the broader field of S&amp;T indicators been applied in the practice of program evaluation? Has the situation changed significantly in the last decade since <xref ref-type="bibr" rid="bibr23-1356389012460961">Grupp’s (2000)</xref> review?</p>
<p>At least from a European perspective, a few examples show that the situation has not changed significantly in the last 10 years – a fact that is confirmed by the low visibility of Grupp’s paper, which has received very few citations both in the Web of Science (1) and in Google Scholar (6).</p>
<p>A meta-analysis of the evaluations related to EU FPs (at both the European and national level) shows that quantitative approaches and S&amp;T indicators have hardly ever been used and most evaluations rely on other methods, such as experts, participant questionnaires, and reviews of official documents (<xref ref-type="bibr" rid="bibr3-1356389012460961">Arnold et al., 2005</xref>). Both the ex-post evaluation of FP6 and the interim evaluation of FP7 are essentially based on panel reviews, supported by extensive statistics and data on participation patterns, as a well as by a number of impact assessment studies. S&amp;T indicators are virtually absent in both reports. When looking at the larger set of EU impact studies most of them are also based on participants’ surveys or interviews.</p>
<p>However, two relevant exceptions stand out. First, a study on the bibliometric profiling of FP participants provides an analysis of the overall publication output and impact of lead scientists participating in FPs (identified through a survey of FP project coordinators). This study shows that lead scientists in FPs have a publication and citation performance that is higher than that of their counterparts within their scientific community (<xref ref-type="bibr" rid="bibr65-1356389012460961">Technopolis and OST, 2009</xref>). Second, a study using FP participation data provides relevant insights into the formation of the European research network, as well as into the emergence of a backbone of participants in FPs (<xref ref-type="bibr" rid="bibr4-1356389012460961">AVEDAS, CWTS, FAS. Research and Stockholm School of Economics, 2009</xref>). These two studies indicate some relevant trends in the use of S&amp;T indicators for evaluation; namely, a shift from projects to participants, the combination of participation data and bibliometric indicators, and a stronger focus on linkages and structuring effects.</p>
<p>A further attempt at using S&amp;T indicators more extensively, especially to map the European research landscape and to measure the outputs of previous FPs, was made in the ex-ante impact assessment of FP7 (<xref ref-type="bibr" rid="bibr15-1356389012460961">Delanghe and Muldur, 2007</xref>). The authors explain that the improvement of ex-ante impact assessment requires more rigorous and comprehensive collection of data on applicants and participants, as well as on outputs and impacts, which can be achieved through new reporting techniques and improved methodologies for the use of bibliometric or innovation data (i.e. from the Community Innovation Survey). Moreover, a new development in the field of S&amp;T indicators is suggested, with a shift from inputs to flows (collaboration patterns, co-publications, co-patenting, etc.).</p>
<p>Bibliometric indicators are reasonably well-established when measuring project results, especially for programs in domains that are thoroughly covered by international databases (<xref ref-type="bibr" rid="bibr66-1356389012460961">Technopolis and OST Paris, 2004</xref>; <xref ref-type="bibr" rid="bibr70-1356389012460961">Van Raan, 2004</xref>). However, technological indicators are much less developed. A mapping exercise of the evaluation practices and methods adopted by several Funding Agencies in Europe to assess strategic issues and impact, research fields and disciplines, and funding programs, shows that output indicators – such as papers, PhDs, and patents – are always considered in the analysis, although their role is not yet a prominent one, nor are they fully integrated in the evaluation design. Moreover, the analysis shows an ongoing shift in the main rationale for evaluation towards formative evaluations (<xref ref-type="bibr" rid="bibr57-1356389012460961">Reale et al., 2011</xref>).</p>
<p>We believe that there are some valid reasons to overcome the traditional scepticism expressed by evaluation specialists towards the use of quantitative indicators (at least concerning research programs; <xref ref-type="bibr" rid="bibr14-1356389012460961">Cozzens, 1997</xref>). This would lead to an enhanced contribution to program evaluation.</p>
<p>First, as explained above, the range of available S&amp;T indicators has greatly increased in the last few years, and many of them could be profitably applied to program evaluation too. Network and linkages indicators are a case in point. The recent Innovation Union Competitiveness Report (<xref ref-type="bibr" rid="bibr17-1356389012460961">European Commission, 2011</xref>) makes extensive use of these indicators to discuss the structural impact of the FP on the European Research Area (see also <xref ref-type="bibr" rid="bibr30-1356389012460961">Heller-Schuh et al., 2011</xref>; <xref ref-type="bibr" rid="bibr61-1356389012460961">Scherngell and Barber, 2011</xref>). Second, the ongoing debate within the evaluation community on how to trace the outcomes and impacts of research programs (<xref ref-type="bibr" rid="bibr59-1356389012460961">Rogers and Jordan, 2011</xref>) shows that expert judgements and participant self-assessment alone will not suffice – some kind of quantitative measurement will be required. The inability to address this issue might result in delegitimizing evaluation results and reducing their political impact.</p>
<p>However, we argue that, since indicators are not purely objective and technical instruments, their role cannot be properly investigated without addressing deeper issues about the nature of program evaluation itself. This will lead to a more realistic representation of how indicators can contribute to evaluation, while also broadening their focus from the measurement of results and impacts towards the assessment of program assumptions and operations – two questions that are largely neglected in summative evaluations.</p>
</sec>
</sec>
<sec id="section5-1356389012460961">
<title>Conceptualizing research programs and their evaluation</title>
<p>Traditionally, evaluation was dominated by a summative approach, whose main objective was to measure the extent to which program goals were achieved. It was assumed that program owners designed their methods rationally, based on some underlying intervention logic, which was not to be questioned by the evaluation itself (<xref ref-type="bibr" rid="bibr64-1356389012460961">Stame, 2004</xref>). Hence, evaluation approaches focused on methodologies to evaluate outputs and impacts against a non-intervention (counterfactual) benchmark, as with randomized experimental designs. The quest for quantitative indicators largely follows this paradigm.</p>
<p>New evaluation approaches have been developed in the last few decades in order to cope with the complex reality of most programs and with social demand for participation. These include responsive evaluation (<xref ref-type="bibr" rid="bibr1-1356389012460961">Abma, 2006</xref>; <xref ref-type="bibr" rid="bibr63-1356389012460961">Stake and Abma, 2005</xref>), constructivist evaluation (<xref ref-type="bibr" rid="bibr25-1356389012460961">Guba and Lincoln, 1989</xref>), as well as theory-based evaluation (<xref ref-type="bibr" rid="bibr64-1356389012460961">Stame, 2004</xref>; <xref ref-type="bibr" rid="bibr71-1356389012460961">Weiss, 2004</xref>). While it is not the purpose of this article to discuss these approaches in general, we wish to highlight some issues that apply specifically to research funding programs.</p>
<sec id="section6-1356389012460961">
<title>Programs as socially-constructed actor spaces</title>
<p>Research programs are one of the main tools adopted in research policy in order to achieve broader goals such as scientific excellence and social relevance. They provide financial incentives to research groups to direct their research agenda towards public goals and competition for resources is created through calls for proposals – hence, the groups wishing to receive funds have to align their research priorities to those of the funder (<xref ref-type="bibr" rid="bibr10-1356389012460961">Braun, 2003</xref>). Nevertheless, research programs are an extremely differentiated domain for what concerns their objectives, the rules for the selection of proposals, the type of output expected, and the contractual relationship between funding bodies and research groups. Responsive mode programs managed by research councils provide funding for basic research on topics proposed by the researchers, allowing for greater freedom in the research performed, whereas contract research explicitly requires the delivery of outputs defined at the contractual level. These differences also have an impact on the extent to which a research program should be considered a tool to implement public policies or a socially-constructed interaction space between policy and science (and, accordingly, between summative and responsive evaluation).</p>
<p>A first relevant characteristic of research programs is their <italic>high level of delegation</italic>, complemented by <italic>a distinctive principal-agent structure</italic> (<xref ref-type="bibr" rid="bibr11-1356389012460961">Braun and Guston, 2003</xref>), whereby the funding agency allocates money on the basis of the proposals it receives, but the decisions about how to carry out the projects are largely left to the research groups. Delegation is a well-known phenomenon in public programs – related to the diffusion of new public management practices – and also a familiar issue in evaluation (<xref ref-type="bibr" rid="bibr64-1356389012460961">Stame, 2004</xref>). It has distinctive features, both because research is an inherently risky undertaking whose success depends on the creativity of the performer and because controlling the results and measuring their quality is difficult. Delegation is more than a good management practice in research programs, especially if we understand research systems as distributed intelligence settings, where there is no single actor (e.g. the State) defining the goals and steering the system centrally (<xref ref-type="bibr" rid="bibr36-1356389012460961">Kuhlmann et al., 1999</xref>).</p>
<p>Second, today’s research funding systems are characterized by the <italic>coexistence of different funding schemes</italic> that tend to overlap in terms of the research they support; for example, thematically-oriented versus investigator-driven programs or programs managed at different institutional levels (national versus European). While in other domains this is not considered optimal and integrated programs have been promoted, in the research domain differentiation of funding schemes is seen as a sound policy to address partially conflicting goals and to encourage competition. Current research funding arrangements can be better understood as a market-space allowing coordination between <italic>funders</italic> thus providing resources related to specific policy goals on the one side, and the <italic>research organizations</italic>, providing specific research competences on the other side, rather than as a hierarchically organized implementation system (<xref ref-type="bibr" rid="bibr39-1356389012460961">Lepori, 2011</xref>). In STI policy evaluation, this has led to the emergence of approaches like systemic evaluation (<xref ref-type="bibr" rid="bibr2-1356389012460961">Arnold, 2004</xref>) or intelligent benchmarking (<xref ref-type="bibr" rid="bibr28-1356389012460961">Guy and Nauwelaars, 2003</xref>).</p>
<p>Third, almost <italic>all research programs are, to a certain extent, jointly designed and implemented together with their beneficiaries</italic>. Co-designing is relevant since program owners do not usually have sufficient competences to identify the most promising research areas. Joint management is needed since the competences for evaluating and selecting best-quality research are to be found among researchers themselves, hence their involvement in evaluation and selection committees. Accordingly, the designing and implementation of research programs can be best understood as a negotiation process between the State – which holds the resources and strives to achieve policy goals – and the research community – which possesses information and competences and whose members strive to pursue their own research agenda.</p>
<p>As the State provides almost all the resources, a certain level of accountability to the original policy goals is required (implying that summative evaluation will always be relevant); however, what matters in this context is a conception of research programs as complex interaction spaces (‘boundary objects’; <xref ref-type="bibr" rid="bibr27-1356389012460961">Guston, 1999</xref>; <xref ref-type="bibr" rid="bibr33-1356389012460961">Klerkx and Leeuwis, 2008</xref>) among largely autonomous and strategic actors belonging to different social spheres (‘policy’, ‘society’, ‘science’). Their function is to negotiate program designing and operations rather than to implement ex-ante defined policy objectives. As there are multiple spaces in which the interaction among the same actors – the State, funding agencies, performers – takes place concurrently, programs are inherently open systems and their working cannot be adequately understood without taking into account the interaction with other funding schemes.</p>
</sec>
<sec id="section7-1356389012460961">
<title>From program characteristics to evaluation approaches</title>
<p>Some scholars have developed the implications of this perspective for research policy and program evaluation (<xref ref-type="bibr" rid="bibr35-1356389012460961">Kuhlmann, 2003</xref>); yet, these are slow to penetrate evaluation practices, which still tend to follow a more linear understanding of how research works (<xref ref-type="bibr" rid="bibr49-1356389012460961">Molas-Gallart and Davies, 2005</xref>). A number of the main implications that follow from the earlier discussion are outlined below.</p>
<p>First, if programs are implemented by largely autonomous actors, evaluation has to start from their needs and from how they interpret the program goals, since these features are likely to be more relevant for program operation than the ex-ante goals stated in official documents. As in responsive evaluation, evaluation objectives and questions need to be constructed together with program participants, rather than decided by program owners alone. At the same time, how these actors respond to program measures and (financial) incentives determines program outcomes and impacts, a lesson endorsed by studies on behavioural additionality (<xref ref-type="bibr" rid="bibr54-1356389012460961">OECD, 2006</xref>). This means that the evaluation has to consider which action theory underpins actors’ behaviour and to what extent the programs influence it – not solely by measuring outputs and impacts, but also by explaining why and how these were generated.</p>
<p>Second, if programs have inherently open and fuzzy boundaries, problems of demarcation are fundamental and cannot be addressed only through sophisticated analytic methods. Trying to attribute outcomes and impacts to a specific program can become a contestable exercise and provides less and less meaningful results about the broader impact on science and society. Two promising approaches are focusing on participants rather than on projects and investigating interactions as sources of broader impact (<xref ref-type="bibr" rid="bibr48-1356389012460961">Molas-Gallart, 2011</xref>; <xref ref-type="bibr" rid="bibr62-1356389012460961">Spaapen and Van Drooge, 2011</xref>). Both approaches are examples of the shift towards theory-led evaluation. Assuming that research groups are able to develop long-term strategies, if a research program is able to shift the participants’ agenda, there is a chance that long-term results will be generated. Moreover, assuming that creating linkages and interactions among the stakeholders has a social impact, if a research program manages to strengthen these ties, this is likely to generate long-term impacts.</p>
<p>Third, if programs are co-designed by a complex web of actors, addressing the questions and needs of the program ‘owner’ alone is not sufficient. Rather, evaluation should become a source of strategic intelligence, enabling negotiation among the actors, making them more aware of the implications of different program designs, as well as of the interests and strategies of the involved actors. In this perspective, evaluation should not produce pre-defined recommendations and conclusions but rather an agenda for negotiation among actors, highlighting critical issues, open questions, and possible choices to be made (<xref ref-type="bibr" rid="bibr34-1356389012460961">Kuhlmann, 1998</xref>). As foreseen by socio-constructivist evaluation (<xref ref-type="bibr" rid="bibr25-1356389012460961">Guba and Lincoln, 1989</xref>), constant communication with the program actors throughout the whole process becomes at least as important as the final evaluation results.</p>
</sec>
</sec>
<sec id="section8-1356389012460961">
<title>A framework for the use of indicators in program evaluation</title>
<p>Our discussion of program evaluation draws on concepts introduced in section 2, such as the value-laden nature of indicators and their status as instruments for social debate. Hence the case for a broader conception of evaluation that opens up space for an expanded role for S&amp;T indicators, alongside their traditional function of measuring outputs and impacts in summative evaluation.</p>
<sec id="section9-1356389012460961">
<title>Indicators as contributions to the evaluation debate</title>
<p>Formative and constructivist conceptions of evaluation imply a shift in the timing of the process, from an approach in which evaluation is performed at the end of the program towards continuous evaluation throughout the whole program life. In such an approach, evidence from evaluation activities – indicators, surveys, experts’ assessment – is debated and evaluated by program actors when it becomes available. The current evaluation setting of the European Framework Programs including ex-ante, mid-term and final evaluation is an example of this approach (<xref ref-type="bibr" rid="bibr49-1356389012460961">Molas-Gallart and Davies, 2005</xref>).</p>
<p>This process approach potentially facilitates the use of S&amp;T indicators, as it provides room for interactive design and refinement during a program’s life. The main evaluation questions are constructed at the beginning of a program, will suggest indicators needed and how these can be designed, while also identifying the data to be collected while the program is underway. Following this, a first set of indicators can be debated with program actors to test in advance their feasibility, reliability and relevance, and to devise possible refinements. This interactive approach is ideal not only for technical reasons, but also because the nature of S&amp;T indicators requires them to be co-constructed together with the involved actors, in order for them to be accepted and have an impact on decision-making (<xref ref-type="bibr" rid="bibr6-1356389012460961">Barré, 2004</xref>).</p>
<p>Conceiving evaluation as a process also implies a different use of S&amp;T indicators, focusing on their ability to promote communication and the learning processes rather than on objectivity and precision.</p>
<p>As argued in section 2, this is their main strength. By adopting a strong stance on the characteristics of real programs and on value choices, they are able to simplify an overly complex reality to few numbers, which can be produced with a reasonable amount of effort. As such, they can raise questions, highlight the assumptions of participating actors, and provide counter-evidence to established positions. For instance, trying to use bibliometrics to measure the output of EU FPs was a resource-consuming exercise (<xref ref-type="bibr" rid="bibr66-1356389012460961">Technopolis and OST Paris, 2004</xref>), while using the simple impact factor is more feasible and likely to provoke a relevant debate about questions such as: Are the best scientists in Europe participating in FPs? Are some of the best publications in a field related to EU-FP funded research?</p>
<p>This information is not meant to demonstrate causality, but rather to act as input for broader evaluation debate, so that it can be evaluated according to different perspectives, and compared with what is normally expected at that stage of program development. Here, timeliness becomes a central criterion. Of course, the value of the adopted indicators will also depend on their <italic>reliability</italic> (i.e. confidence that they can actually measure what they intend, as well on their <italic>relevance</italic> to the evaluation questions posed. This needs to be based on sound theoretical assumptions and good quality data but also the robustness and transparency of the production processes (<xref ref-type="bibr" rid="bibr6-1356389012460961">Barré, 2004</xref>).</p>
</sec>
<sec id="section10-1356389012460961">
<title>Overcoming delimitation problems: From project to participants</title>
<p>Our discussion of research programs supports the idea that program participants, rather than funded projects, are the central analytical unit to be observed, as they are able to develop long-term strategies and research agendas and to combine available resources in order to pursue them (<xref ref-type="bibr" rid="bibr32-1356389012460961">Joly and Mangematin, 1996</xref>; <xref ref-type="bibr" rid="bibr38-1356389012460961">Latour and Woolgar, 1979</xref>). Analysing the strategies of research groups and contrasting them with those of other groups not participating in the program is relevant for different purposes. Ex-ante, such an analysis anticipates results and identifies critical limitations (e.g. the lack of interaction with societal stakeholders). During the program, it can clarify research groups’ logic of action and inform and implementation. Finally, the strategies of research groups allows for an evaluation of program results, since changes in participants’ profiles, strategies and linkages can be considered a proxy of long-term program impact.</p>
<p>Research groups are also the natural setting to produce S&amp;T indicators. These can include for example:</p>
<list id="list1-1356389012460961" list-type="bullet">
<list-item><p>Indicators on scientific production – both publication counts and impact factors (<xref ref-type="bibr" rid="bibr70-1356389012460961">Van Raan, 2004</xref>) – as well as on technological production, based especially on patents (<xref ref-type="bibr" rid="bibr24-1356389012460961">Grupp et al., 1991</xref>). Limitations of the former relate to the poor coverage of social sciences and humanities (<xref ref-type="bibr" rid="bibr50-1356389012460961">Nederhof, 2006</xref>), but they can be overcome by combining heterogeneous data sources, while limitations concerning patents relate to the lack of a clear measure of their value, but also to identification problems in the public sector related to regulations on intellectual property (<xref ref-type="bibr" rid="bibr42-1356389012460961">Lissoni et al., 2008</xref>).</p></list-item>
<list-item><p>Indicators that map the activity profiles of research groups (<xref ref-type="bibr" rid="bibr37-1356389012460961">Larédo and Mustar, 2000</xref>) and their trajectories over time (<xref ref-type="bibr" rid="bibr9-1356389012460961">Braam and Van den Besselaar, 2010</xref>) and consider, simultaneously, changes in the activity dimension of groups and their synergies and interactions, breaking with a tradition that measures separately the different types of outputs and impacts of programs.</p></list-item>
<list-item><p>Indicators of linkages and cooperation among research groups and with other relevant stakeholders. This includes measures of scientific cooperation using co-publications data (<xref ref-type="bibr" rid="bibr21-1356389012460961">Glänzel and Schubert, 2005</xref>), science-technology linkages through patent citations analysis (<xref ref-type="bibr" rid="bibr68-1356389012460961">Tijssen, 2001</xref>), and broader indicators, such as media presence, interlinking of websites, visibility in data sources, such as Google Scholar, which are not exclusively addressed to an academic audience. Measuring these characteristics might anticipate long-term impacts that are not yet measurable (<xref ref-type="bibr" rid="bibr49-1356389012460961">Molas-Gallart and Davies, 2005</xref>).</p></list-item>
</list>
<p>Gathering information on participants requires the availability of a database, allowing for the unambiguous identification of participants at the right organizational level. Unfortunately, in many cases participants’ databases are designed for management purposes and are hardly usable for evaluation. In FP7, for instance, the participants’ databases list legal entities (although they also contain information on participating research units), which poses evaluators with problems (<xref ref-type="bibr" rid="bibr15-1356389012460961">Delanghe and Muldur, 2007</xref>), since participation takes place at the unit level and legal entities have different meanings in each country.</p>
</sec>
<sec id="section11-1356389012460961">
<title>A broader set of evaluation questions</title>
<p>Beyond output and impact measurement, indicators can also help check program assumptions and operations (<xref ref-type="bibr" rid="bibr23-1356389012460961">Grupp, 2000</xref>).</p>
<p>A formative perspective assumes an open conception of research programs, embedded in a broader environment and largely meant to transform their features (e.g. expenditure patterns, participating actors and existing networks) rather than simply produce direct results. The correctness of the assumptions about what needs to change will therefore be as important for the success of the program as its operations.</p>
<p>Indicators can help answer assumption questions in two ways. Aggregate indicators at the national or sectorial level – research expenditure, impact factors by field, and patents – provide a broad overview of how a research and technological sector is evolving and to what extent gaps identified when launching a program have been addressed. Indicators at the group level can produce maps of actors in a field and their networks of relationships, thus identifying core actors and critical linkages and the weakness of a research field, such as excessive fragmentation, or oligopolistic power.</p>
<p>Program operations are a strategic as well as a technical issue. A program might fail its objectives if it is not able to involve the right participants and to build envisaged linkages, or if the stakeholders and research actors are not integrated at the design phase. The evaluation process has the aim to provide evidence on issues like the quality of communication, the functioning of the proposals’ selection process, and the level of commitment to program goals. Hence, a shift from evaluation of administrative processes towards indicators characterizing the <italic>participation of actors in program design and evaluation processes</italic> is suggested. A <italic>common system for the logging of administrative events</italic> to analyse program management and administration will be required for this purpose.</p>
</sec>
<sec id="section12-1356389012460961">
<title>Operational implications</title>
<p>The previous discussion has highlighted some organizational pre-requisites to fully exploit the potential of S&amp;T indicators for the purposes of program evaluation – most of which comply with good practice in organizing evaluations overall.</p>
<p>First advanced planning is extremely important, as collecting data, designing indicators, testing, and validation on the specific context of the evaluated program are processes that require time. The potential use of S&amp;T indicators should be discussed in the early phases of the program’s life, when the overall evaluation goal, its timeframe and available resources are also defined. Second, indicators are highly dependent on the availability of data sources. While for some indicators, existing data can be used, for others a well-organized system of program data collection becomes essential (<xref ref-type="bibr" rid="bibr60-1356389012460961">Ruegg and Feller, 2003</xref>). Key priorities in this respect are a well-structured participants’ database, a system for the logging of administrative events and, finally, a system of <italic>participants’ and projects’ surveys</italic> to track the scientific and technological output and to collect the participants’ opinions and feedback on management and impacts.</p>
<p>Third, indicator production is a highly specialized activity requiring specific conceptual and methodological competences, as well as resources in terms of manpower and infrastructures. This is not to suggest that all evaluations should include an extensive use of S&amp;T indicators and devote significant resources to this purpose. However there should be some reasonable relationships between goals and resources invested.</p>
<p>Finally, the use of indicators in evaluation heavily relies on the evaluators themselves having a clear idea of what S&amp;T indicators are, of their potential contribution, and of the requirements for their design and production. Accordingly, we argue that basic competences concerning S&amp;T indicators should be an integral part of the training of evaluators specialized in research and technological programs, just like other methods such as panel reviews or surveys. This should include the epistemological status of indicators, the principle of their co-construction with users, and the distinction between designing and production tasks. All these topics can be easily related to recent approaches to evaluation overall – i.e. theory-based, responsive, and constructivist evaluation.</p>
</sec>
</sec>
<sec id="section13-1356389012460961" sec-type="discussion|conclusions">
<title>Discussion and conclusions</title>
<p>In this article, we have argued that S&amp;T indicators are powerful tools for understanding and interpreting research and development. While their use has traditionally been limited by serious constraints (i.e. demarcation and time lag), which have impeded the emergence of systematic work on their design and production, we have suggested that there are good reasons for a broader use of S&amp;T indicators in future evaluation activities.</p>
<p>This is justified by parallel developments in both fields. The field of S&amp;T indicators has become significantly differentiated in recent years, shifting from the production of standardized (national) aggregates to <italic>positioning indicators</italic>, characterizing individual actors and their linkages within the innovation system. This conceptual development is supported by the increased availability of data sources, new data retrieval techniques, and advances in tools for data exploitation and analysis, which make it possible to move towards customized and contextualized indicators. At the same time, the diffusion of theory-led evaluation approaches, focusing more on participants and interactions than on projects, makes it possible to envisage a much broader role for S&amp;T indicators, which better fits their status than simply providing ‘objective’ answers to evaluation questions. We thus highlight the advantages of using S&amp;T indicators within a formative evaluation perspective and when programs are conceived as social constructions and actors’ spaces rather than as tools for implementing public policy objectives.</p>
<p>Indicators can contribute to the general debate about the validity and the advantages of public investment in R&amp;D. In this respect, developing indicators under a formative learning-oriented program evaluation approach must be seen as a complement to the summative efforts that contribute to feeding the ‘agenda for negotiation’ (<xref ref-type="bibr" rid="bibr35-1356389012460961">Kuhlman, 2003</xref>) among the different actors involved (policy actors, intermediaries, and performers). On the other hand, following a socio-constructivist approach, indicators can also improve the ability of external evaluators to understand the value and potential of the complex research initiatives they have to assess (<xref ref-type="bibr" rid="bibr69-1356389012460961">Trochim et al., 2008</xref>).</p>
<p>Finally, good evaluation must always be carefully related to the specificities of their field of application. R&amp;D indicators have to match the evolving character of contemporary S&amp;T programs.</p>
</sec>
</body>
<back>
<ack>
<p>The authors acknowledge support from the European Commission under the contract 30-CE-1232670/00-40, as well as contributions from their colleagues involved in this contract (Michael Braun, Philippe Larédo, Stig Slipersater, Aris Kaloudis, Ghislaine Filliatreau, OST). Preliminary versions were presented at the European Forum on Research and Development Impact Assessment (EUFORDIA), Prague, February 2009, at the Triple Helix Conference, Glasgow, June 2009, as well as at the ENID/STI Indicators Conference, Paris, March 2010. Finally, the authors would like to acknowledge useful advice from three anonymous referees.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research was partially funded from the European Commission under the contract 30-CE-1232670/00-40.</p>
</fn>
</fn-group>
<bio>
<p><bold>Benedetto Lepori</bold> is Head of Unit on Performance and Management of Research and Higher Education Institutions at the University of Lugano. He is an established scholar in the field of research and higher education policy and of S&amp;T indicators, with a specialization on methodological issues, funding and higher education indicators.</p>
<p><bold>Emanuela Reale</bold> is Senior Researcher at CERIS-CNR, where she is responsible for the Research Theme on Institutions and policies for the public sector of research, with a specific focus on higher education, research evaluation and indicators. She is expert in the Italian Committee for the Evaluation of Research (CIVR).</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abma</surname><given-names>T</given-names></name>
</person-group> (<year>2006</year>) <article-title>The practice and politics of responsive evaluation</article-title>. <source>American Journal of Evaluation</source> <volume>27</volume>(<issue>1</issue>): <fpage>31</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr2-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Arnold</surname><given-names>E</given-names></name>
</person-group> (<year>2004</year>) <article-title>Evaluating research and innovation policy: a systems world needs systems evaluation</article-title>. <source>Research Evaluation</source> <volume>13</volume>(<issue>1</issue>): <fpage>3</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr3-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Arnold</surname><given-names>E</given-names></name>
<name><surname>Clark</surname><given-names>J</given-names></name>
<name><surname>Muscio</surname><given-names>A</given-names></name>
</person-group> (<year>2005</year>) <article-title>What the evaluation record tells us about European Union Framework Programme performance</article-title>. <source>Science and Public Policy</source> <volume>32</volume>(<issue>5</issue>): <fpage>385</fpage>–<lpage>97</lpage>.</citation>
</ref>
<ref id="bibr4-1356389012460961">
<citation citation-type="book">
<collab>AVEDAS, CWTS, FAS. Research and Stockholm School of Economics</collab> (<year>2009</year>) <article-title><italic>Structuring Effects of Community Research – The Impact of the Framework Programme on Research and Technological Development</italic> (RTD) on Network Formation</article-title>. <publisher-loc>Brussels</publisher-loc>, Report for the <publisher-name>European Commission, Directorate-General for Research</publisher-name>.</citation>
</ref>
<ref id="bibr5-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Barré</surname><given-names>R</given-names></name>
</person-group> (<year>2001</year>) <article-title>Sense and nonsense of S&amp;T productivity indicators</article-title>. <source>Science and Public Policy</source> <volume>28</volume>(<issue>4</issue>): <fpage>259</fpage>–<lpage>66</lpage>.</citation>
</ref>
<ref id="bibr6-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Barré</surname><given-names>R</given-names></name>
</person-group> (<year>2004</year>) <article-title>S&amp;T indicators for policy making in a changing science-society relationship</article-title>. In: <person-group person-group-type="editor">
<name><surname>Moed</surname><given-names>HF</given-names></name>
<name><surname>Glänzel</surname><given-names>W</given-names></name>
<name><surname>Schmoch</surname><given-names>U</given-names></name>
</person-group> (eds) <source>Handbook of Quantitative Science and Technology Research</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>, <fpage>115</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr7-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bloch</surname><given-names>C</given-names></name>
</person-group> (<year>2007</year>) <article-title>Assessing recent developments in innovation measurement: the third edition of the <italic>Oslo Manual</italic></article-title>. <source>Science and Public Policy</source> <volume>34</volume>(<issue>1</issue>): <fpage>23</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr8-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bonaccorsi</surname><given-names>A</given-names></name>
<name><surname>Daraio</surname><given-names>C</given-names></name>
<name><surname>Lepori</surname><given-names>B</given-names></name>
<name><surname>Slipersaeter</surname><given-names>S</given-names></name>
</person-group> (<year>2007</year>) <article-title>Indicators on individual higher education institutions: addressing data problems and comparability issues</article-title>. <source>Research Evaluation</source> <volume>16</volume>(<issue>2</issue>): <fpage>66</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr9-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Braam</surname><given-names>R</given-names></name>
<name><surname>Van den Besselaar</surname><given-names>P</given-names></name>
</person-group> (<year>2010</year>) <article-title>Lyfe cycles of research groups: the case of CWTS</article-title>. <source>Research Evaluation</source> <volume>19</volume>(<issue>3</issue>): <fpage>173</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr10-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Braun</surname><given-names>D</given-names></name>
</person-group> (<year>2003</year>) <article-title>Lasting tensions in research policy-making – a delegation problem</article-title>. <source>Science and Public Policy</source> <volume>30</volume>(<issue>5</issue>): <fpage>309</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr11-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Braun</surname><given-names>D</given-names></name>
<name><surname>Guston</surname><given-names>D</given-names></name>
</person-group> (<year>2003</year>) <article-title>Principal-agent theory and research policy: an introduction</article-title>. <source>Science and Public Policy</source> <volume>30</volume>(<issue>5</issue>): <fpage>302</fpage>–<lpage>8</lpage>.</citation>
</ref>
<ref id="bibr12-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Breschi</surname><given-names>S</given-names></name>
<name><surname>Lissoni</surname><given-names>F</given-names></name>
</person-group> (<year>2004</year>) <article-title>Knowledge networks from patent data</article-title>. In: <person-group person-group-type="editor">
<name><surname>Moed</surname><given-names>HF</given-names></name>
<name><surname>Glänzel</surname><given-names>W</given-names></name>
<name><surname>Schmoch</surname><given-names>U</given-names></name>
</person-group> (eds) <source>Handbook of Quantitative Science and Technology Research</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer</publisher-name>, <fpage>613</fpage>–<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr13-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Campbell</surname><given-names>DFJ</given-names></name>
</person-group> (<year>2003</year>) <article-title>The evaluation of university research in the United Kingdom and The Netherlands, Germany and Austria</article-title>. In: <person-group person-group-type="editor">
<name><surname>Shapira</surname><given-names>P</given-names></name>
<name><surname>Kuhlmann</surname><given-names>S</given-names></name>
</person-group> (eds) <source>Learning from Science and Technology Policy Evaluation</source>. <publisher-loc>Cheltenham</publisher-loc>: <publisher-name>Edward Elgar</publisher-name>, <fpage>98</fpage>–<lpage>131</lpage>.</citation>
</ref>
<ref id="bibr14-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cozzens</surname><given-names>SE</given-names></name>
</person-group> (<year>1997</year>) <article-title>The knowledge pool: measurement challenges in evaluating fundamental research programs</article-title>. <source>Evaluation and Program Planning</source> <volume>20</volume>(<issue>1</issue>): <fpage>77</fpage>–<lpage>89</lpage>.</citation>
</ref>
<ref id="bibr15-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Delanghe</surname><given-names>H</given-names></name>
<name><surname>Muldur</surname><given-names>U</given-names></name>
</person-group> (<year>2007</year>) <article-title>Ex-ante impact assessment of research programmes: the experience of the European Union’s 7th Framework Programme</article-title>. <source>Science and Public Policy</source> <volume>34</volume>(<issue>3</issue>): <fpage>169</fpage>–<lpage>83</lpage>.</citation>
</ref>
<ref id="bibr16-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Desrosières</surname><given-names>A</given-names></name>
</person-group> (<year>2008</year>) <source>Gouverner par les nombres</source>. <publisher-loc>Paris</publisher-loc>: <publisher-name>Presses de l’école des mines</publisher-name>.</citation>
</ref>
<ref id="bibr17-1356389012460961">
<citation citation-type="book">
<collab>European Commission</collab> (<year>2011</year>) <source>Innovation Union Competitiveness Report</source>. <publisher-loc>Brussels</publisher-loc>: <publisher-name>European Commission</publisher-name>.</citation>
</ref>
<ref id="bibr18-1356389012460961">
<citation citation-type="book">
<collab>European Court of Auditors</collab> (<year>2007</year>) <source>Evaluating the EU Research and Technological Development (RTD) Framework Programmes</source>. <publisher-loc>Brussels</publisher-loc>: <publisher-name>European Commission</publisher-name>.</citation>
</ref>
<ref id="bibr19-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Feller</surname><given-names>I</given-names></name>
</person-group> (<year>2007</year>) <article-title>Mapping the frontiers of evaluation of public-sector R&amp;D programs</article-title>. <source>Science and Public Policy</source> <volume>34</volume>(<issue>10</issue>): <fpage>681</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr20-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Georghiou</surname><given-names>L</given-names></name>
<name><surname>Larédo</surname><given-names>P</given-names></name>
</person-group> (<year>2006</year>) <article-title>Evaluation of publicly funded research: recent trends and perspectives</article-title>. In: <collab>OECD</collab> (eds) <source>OECD Science, Technology and Industry Outlook</source>. <publisher-loc>Paris</publisher-loc>: <publisher-name>OECD</publisher-name>, <fpage>177</fpage>–<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr21-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Glänzel</surname><given-names>W</given-names></name>
<name><surname>Schubert</surname><given-names>A</given-names></name>
</person-group> (<year>2005</year>) <article-title>Analysing scientific networks through co-authorship</article-title>. In: <person-group person-group-type="editor">
<name><surname>Moed</surname><given-names>HF</given-names></name>
<name><surname>Glänzel</surname><given-names>W</given-names></name>
<name><surname>Schmoch</surname><given-names>U</given-names></name>
</person-group> (eds) <source>Handbook of Quantitative Science and Technology Research</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic Publications</publisher-name>, <fpage>257</fpage>–<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr22-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Godin</surname><given-names>B</given-names></name>
</person-group> (<year>2005</year>) <source>Measurement and Statistics on Science and Technology</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr23-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grupp</surname><given-names>H</given-names></name>
</person-group> (<year>2000</year>) <article-title>Indicator-assisted evaluation of R&amp;D programmes: possibilities, state of the art and case studies</article-title>. <source>Research Evaluation</source> <volume>8</volume>(<issue>2</issue>): <fpage>87</fpage>–<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr24-1356389012460961">
<citation citation-type="patent">
<person-group person-group-type="author">
<name><surname>Grupp</surname><given-names>H</given-names></name>
<name><surname>Schmoch</surname><given-names>U</given-names></name>
<name><surname>Kuntze</surname><given-names>U</given-names></name>
</person-group> (<year>1991</year>) <article-title>Patents as potential indicators of the utility of EC Research Programmes</article-title>. <source>Scientometrics</source> <volume>21</volume>(<issue>3</issue>): <fpage>417</fpage>–<lpage>45</lpage>.</citation>
</ref>
<ref id="bibr25-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Guba</surname><given-names>EG</given-names></name>
<name><surname>Lincoln</surname><given-names>YS</given-names></name>
</person-group> (<year>1989</year>) <source>Fourth Generation Evaluation</source>. <publisher-loc>Newbury Park, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr26-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gulbrandsen</surname><given-names>M</given-names></name>
<name><surname>Slipersaeter</surname><given-names>S</given-names></name>
</person-group> (<year>2007</year>) <article-title>The third mission and the entrepreneurial university model</article-title>. In: <person-group person-group-type="editor">
<name><surname>Bonaccorsi</surname><given-names>A</given-names></name>
<name><surname>Daraio</surname><given-names>C</given-names></name>
</person-group> (eds) <source>Universities and Strategic Knowledge Creation. Specialization and Performance in Europe</source>. <publisher-loc>Cheltenham</publisher-loc>: <publisher-name>Edwar Elgar</publisher-name>, <fpage>112</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr27-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guston</surname><given-names>DH</given-names></name>
</person-group> (<year>1999</year>) <article-title>Stabilizing the boundary between US politics and science: the role of the Office of Technology Transfer as a boundary organization</article-title>. <source>Social Studies of Science</source> <volume>29</volume>(<issue>1</issue>): <fpage>87</fpage>–<lpage>111</lpage>.</citation>
</ref>
<ref id="bibr28-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Guy</surname><given-names>K</given-names></name>
<name><surname>Nauwelaars</surname><given-names>C</given-names></name>
</person-group> (<year>2003</year>) <source>Benchmarking STI Policies in Europe: In Search of Good Practice</source>. <publisher-loc>Seville</publisher-loc>: <publisher-name>Institute for Prospective Technological Studies</publisher-name>.</citation>
</ref>
<ref id="bibr29-1356389012460961">
<citation citation-type="book">
<collab>HEFCE (Higher Education Funding Council for England)</collab> (<year>2007</year>) <source>Research Excellence Framework. Consultation on the Assessment and Funding of Higher Education Research Post 2009</source>. <publisher-loc>London</publisher-loc>: <publisher-name>HEFCE</publisher-name>.</citation>
</ref>
<ref id="bibr30-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Heller-Schuh</surname><given-names>B</given-names></name>
<name><surname>Barber</surname><given-names>M</given-names></name>
<name><surname>Henriques</surname><given-names>L</given-names></name>
<name><surname>Paier</surname><given-names>M</given-names></name>
<name><surname>Pontikakis</surname><given-names>D</given-names></name>
<name><surname>Scherngell</surname><given-names>T</given-names></name>
<name><surname>Veltri</surname><given-names>GA</given-names></name>
<name><surname>Weber</surname><given-names>M</given-names></name>
</person-group> (<year>2011</year>) <source>Analysis of Networks in European Framework Programmes (1984-2006)</source>. <publisher-loc>Luxembourg</publisher-loc>: <publisher-name>Publications Office of the European Union</publisher-name>.</citation>
</ref>
<ref id="bibr31-1356389012460961">
<citation citation-type="book">
<collab>IPTS</collab> (<year>2002</year>) <source>RTD Evaluation Toolbox</source>. <publisher-loc>Brussels</publisher-loc>: <publisher-name>Institute for Prospective Technological Studies</publisher-name>, EUR 20382N.</citation>
</ref>
<ref id="bibr32-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Joly</surname><given-names>PB</given-names></name>
<name><surname>Mangematin</surname><given-names>V</given-names></name>
</person-group> (<year>1996</year>) <article-title>Profile of public laboratories, industrial partnerships and organisation of R&amp;D: the dynamics of industrial relationships in a large research organization</article-title>. <source>Research Policy</source> <volume>25</volume>: <fpage>901</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr33-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Klerkx</surname><given-names>L</given-names></name>
<name><surname>Leeuwis</surname><given-names>C</given-names></name>
</person-group> (<year>2008</year>) <article-title>Delegation of authority in research funding to networks: experience with a multiple goal boundary organization</article-title>. <source>Science and Public Policy</source> <volume>35</volume>(<issue>3</issue>): <fpage>183</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr34-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhlmann</surname><given-names>S</given-names></name>
</person-group> (<year>1998</year>) <article-title>Moderation of policy-making? Science and technology policy evaluation beyond impact measurement: the case of Germany</article-title>. <source>Evaluation</source> <volume>4</volume>(<issue>2</issue>): <fpage>130</fpage>–<lpage>48</lpage>.</citation>
</ref>
<ref id="bibr35-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhlmann</surname><given-names>S</given-names></name>
</person-group> (<year>2003</year>) <article-title>Evaluation of research and innovation policies: a discussion of trends with examples from Germany</article-title>. <source>Journal of Technology Management</source> <volume>26</volume>(<issue>2–4</issue>): <fpage>131</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr36-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kuhlmann</surname><given-names>S</given-names></name>
<name><surname>Boekholt</surname><given-names>P</given-names></name>
<name><surname>Georghiou</surname><given-names>L</given-names></name>
<name><surname>Guy</surname><given-names>K</given-names></name>
<name><surname>Héraud</surname><given-names>J</given-names></name>
<name><surname>Larédo</surname><given-names>P</given-names></name>
<name><surname>Lemola</surname><given-names>T</given-names></name>
<name><surname>Loveridge</surname><given-names>D</given-names></name>
<name><surname>Luukkonen</surname><given-names>T</given-names></name>
<name><surname>Polt</surname><given-names>W</given-names></name>
<name><surname>Rip</surname><given-names>A</given-names></name>
<name><surname>Sanz Menéndez</surname><given-names>L</given-names></name>
<name><surname>Smits</surname><given-names>R</given-names></name>
</person-group> (<year>1999</year>) <source>Improving Distributed Intelligence in Complex Innovation Systems</source>. <publisher-loc>Karlsruhe</publisher-loc>: <publisher-name>Frauenhofer Institute – Systems and Innovation Research</publisher-name>.</citation>
</ref>
<ref id="bibr37-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Larédo</surname><given-names>P</given-names></name>
<name><surname>Mustar</surname><given-names>P</given-names></name>
</person-group> (<year>2000</year>) <article-title>Laboratory activity profiles: an exploratory approach</article-title>. <source>Scientometrics</source> <volume>47</volume>(<issue>3</issue>): <fpage>515</fpage>–<lpage>39</lpage>.</citation>
</ref>
<ref id="bibr38-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Latour</surname><given-names>B</given-names></name>
<name><surname>Woolgar</surname><given-names>S</given-names></name>
</person-group> (<year>1979</year>) <source>Laboratory Life. The Construction of Scientific Facts</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr39-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lepori</surname><given-names>B</given-names></name>
</person-group> (<year>2011</year>) <article-title>Coordination modes in public funding systems</article-title>. <source>Research Policy</source> <volume>40</volume>(<issue>3</issue>): <fpage>355</fpage>–<lpage>67</lpage>.</citation>
</ref>
<ref id="bibr40-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lepori</surname><given-names>B</given-names></name>
<name><surname>Barré</surname><given-names>R</given-names></name>
<name><surname>Filliatreau</surname><given-names>G</given-names></name>
</person-group> (<year>2008</year>) <article-title>New perspectives and challenges for the design and production of S&amp;T indicators</article-title>. <source>Research Evaluation</source> <volume>17</volume>(<issue>1</issue>): <fpage>33</fpage>–<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr41-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lepori</surname><given-names>B</given-names></name>
<name><surname>Dinges</surname><given-names>M</given-names></name>
<name><surname>Reale</surname><given-names>E</given-names></name>
<name><surname>Slipersaeter</surname><given-names>S</given-names></name>
<name><surname>Theves</surname><given-names>J</given-names></name>
<name><surname>Van den Besselaar</surname><given-names>P</given-names></name>
</person-group> (<year>2007</year>) <article-title>Comparing the evolution of national research policies: what patterns of change?</article-title> <source>Science and Public Policy</source> <volume>34</volume>(<issue>6</issue>): <fpage>372</fpage>–<lpage>88</lpage>.</citation>
</ref>
<ref id="bibr42-1356389012460961">
<citation citation-type="patent">
<person-group person-group-type="author">
<name><surname>Lissoni</surname><given-names>F</given-names></name>
<name><surname>Llerena</surname><given-names>P</given-names></name>
<name><surname>McKelvey</surname><given-names>M</given-names></name>
<name><surname>Bulat</surname><given-names>S</given-names></name>
</person-group> (<year>2008</year>) <article-title>Academic patenting in Europe: new evidence from the KEINS database</article-title>. <source>Research Evaluation</source> <volume>17</volume>(<issue>2</issue>): <fpage>87</fpage>–<lpage>102</lpage>.</citation>
</ref>
<ref id="bibr43-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Luukkonen</surname><given-names>T</given-names></name>
</person-group> (<year>1998</year>) <article-title>The difficulties in assessing the impact of EU Framework Programs</article-title>. <source>Research Policy</source> <volume>27</volume>(<issue>6</issue>): <fpage>599</fpage>–<lpage>610</lpage>.</citation>
</ref>
<ref id="bibr44-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Martin</surname><given-names>BR</given-names></name>
<name><surname>Irvine</surname><given-names>J</given-names></name>
</person-group> (<year>1983</year>) <article-title>Assessing basic research: some partial indicators of scientific progress in radio astronomy</article-title>. <source>Research Policy</source> <volume>12</volume>(<issue>2</issue>): <fpage>61</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr45-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Merkx</surname><given-names>F</given-names></name>
<name><surname>Van den Besselaar</surname><given-names>P</given-names></name>
</person-group> (<year>2008</year>) <article-title>Positioning indicators for cross-disciplinary challenges: the Dutch coastal defense research case</article-title>. <source>Research Evaluation</source> <volume>17</volume>(<issue>1</issue>): <fpage>4</fpage>–<lpage>16</lpage>.</citation>
</ref>
<ref id="bibr46-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Moed</surname><given-names>HF</given-names></name>
<name><surname>Glänzel</surname><given-names>W</given-names></name>
<name><surname>Schmoch</surname><given-names>U</given-names></name>
</person-group> (eds) (<year>2004</year>) <source>Handbook of Quantitative Science and Technology Research</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>.</citation>
</ref>
<ref id="bibr47-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mogoutov</surname><given-names>A</given-names></name>
<name><surname>Kahane</surname><given-names>B</given-names></name>
</person-group> (<year>2007</year>) <article-title>Data search strategy for science and technology emergence: a scalable and evolutionary query for nanotechnology tracking</article-title>. <source>Research Policy</source> <volume>36</volume>(<issue>6</issue>): <fpage>893</fpage>–<lpage>903</lpage>.</citation>
</ref>
<ref id="bibr48-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Molas-Gallart</surname><given-names>J</given-names></name>
</person-group> (<year>2011</year>) <article-title>Tracing ‘productive interactions’ to identify social impacts: an example from the social sciences</article-title>. <source>Research Evaluation</source> <volume>20</volume>(<issue>3</issue>): <fpage>219</fpage>–<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr49-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Molas-Gallart</surname><given-names>J</given-names></name>
<name><surname>Davies</surname><given-names>A</given-names></name>
</person-group> (<year>2005</year>) <article-title>Toward theory-led evaluation: the experience of european science, technology and innovation policies</article-title>. <source>American Journal of Evaluation</source> <volume>20</volume>(<issue>10</issue>): <fpage>1</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr50-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nederhof</surname><given-names>AJ</given-names></name>
</person-group> (<year>2006</year>) <article-title>Bibliometric monitoring of research performance in the social sciences and the humanities: a review</article-title>. <source>Scientometrics</source> <volume>66</volume>(<issue>1</issue>): <fpage>81</fpage>–<lpage>100</lpage>.</citation>
</ref>
<ref id="bibr51-1356389012460961">
<citation citation-type="book">
<collab>OECD</collab> (<year>1995</year>) <source>The Measurement of Human Resources Devoted to S&amp;T – Canberra Manual</source>. <publisher-loc>Paris</publisher-loc>: <publisher-name>OECD</publisher-name>.</citation>
</ref>
<ref id="bibr52-1356389012460961">
<citation citation-type="book">
<collab>OECD</collab> (<year>2002</year>) <source>Frascati Manual. Proposed Standard Practice for Surveys on Research and Experimental Development</source>. <publisher-loc>Paris</publisher-loc>: <publisher-name>OECD</publisher-name>.</citation>
</ref>
<ref id="bibr53-1356389012460961">
<citation citation-type="book">
<collab>OECD</collab> (<year>2005</year>) <source>Guidelines for Collecting and Interpreting Innovation Data — The Oslo Manual</source>. <publisher-loc>Paris</publisher-loc>: <publisher-name>OECD</publisher-name>.</citation>
</ref>
<ref id="bibr54-1356389012460961">
<citation citation-type="book">
<collab>OECD</collab> (<year>2006</year>) <source>Government R&amp;D Funding and Company Behaviour. Measuring Behavioural Additionality</source>. <publisher-loc>Paris</publisher-loc>: <publisher-name>OECD</publisher-name>.</citation>
</ref>
<ref id="bibr55-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>T</given-names></name>
</person-group> (<year>1995</year>) <source>Trust in Numbers: The Pursuit of Objectivity in Science and Public Life</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr56-1356389012460961">
<citation citation-type="book">
<collab>Proneos, OST, NIFU-Step and CERIS-CNR</collab> (<year>2009</year>) <source>Tools and Indicators for the Evaluation and Monitoring of European Research Framework Programmes</source>. <publisher-loc>Brussels</publisher-loc>: <publisher-name>European Commission</publisher-name>.</citation>
</ref>
<ref id="bibr57-1356389012460961">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Reale</surname><given-names>E</given-names></name>
<name><surname>Inzelt</surname><given-names>A</given-names></name>
<name><surname>Lepori</surname><given-names>B</given-names></name>
<name><surname>Van den Besselaar</surname><given-names>P</given-names></name>
</person-group> (<year>2011</year>) <article-title>Indicators for the evaluation of the internationalization of Government Funding Agencies: results from an exploratory and participatory project at European level</article-title>. <conf-name>ENID STI Indicator Conference</conf-name>, <conf-date>7–9 September</conf-date> <conf-loc>Rome, Italy</conf-loc>.</citation>
</ref>
<ref id="bibr58-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Roessner</surname><given-names>D</given-names></name>
</person-group> (<year>2000</year>) <article-title>Quantitative and qualitative methods and measures in the evaluation of research</article-title>. <source>Research Evaluation</source> <volume>8</volume>(<issue>2</issue>): <fpage>125</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr59-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogers</surname><given-names>J</given-names></name>
<name><surname>Jordan</surname><given-names>G</given-names></name>
</person-group> (<year>2011</year>) <article-title>Introduction to a special section on new approaches to predicting and tracing R&amp;D program effects for policy learning</article-title>. <source>Research Evaluation</source> <volume>20</volume>(<issue>4</issue>): <fpage>263</fpage>–<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr60-1356389012460961">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Ruegg</surname><given-names>R</given-names></name>
<name><surname>Feller</surname><given-names>I</given-names></name>
</person-group> (<year>2003</year>) <source>A Toolkit for Evaluating Public R&amp;D Investment: Findings from ATP’s First Decade</source>. NIST GCR 03-857.</citation>
</ref>
<ref id="bibr61-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scherngell</surname><given-names>T</given-names></name>
<name><surname>Barber</surname><given-names>M</given-names></name>
</person-group> (<year>2011</year>) <article-title>Distinct spatial characteristics of industrial and public research collaborations: evidence from the fifth EU Framework Programme</article-title>. <source>The Annals of Regional Science</source> <volume>46</volume>(<issue>2</issue>): <fpage>247</fpage>–<lpage>66</lpage>.</citation>
</ref>
<ref id="bibr62-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spaapen</surname><given-names>J</given-names></name>
<name><surname>Van Drooge</surname><given-names>L</given-names></name>
</person-group> (<year>2011</year>) <article-title>Introducing ‘productive interaction’ in social impact evaluation</article-title>. <source>Research Evaluation</source> <volume>20</volume>(<issue>3</issue>): <fpage>211</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr63-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stake</surname><given-names>RE</given-names></name>
<name><surname>Abma</surname><given-names>T</given-names></name>
</person-group> (<year>2005</year>) <article-title>Responsive evaluation</article-title>. In: <person-group person-group-type="editor">
<name><surname>Mathison</surname><given-names>S</given-names></name>
</person-group> (ed.) <source>Encyclopaedia of Evaluation</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>, <fpage>376</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr64-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stame</surname><given-names>N</given-names></name>
</person-group> (<year>2004</year>) <article-title>Theory-based evaluation and types of complexity</article-title>. <source>Evaluation</source> <volume>10</volume>(<issue>1</issue>): <fpage>58</fpage>–<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr65-1356389012460961">
<citation citation-type="book"><collab>Technopolis and OST</collab> (<year>2009</year>) <source>Bibliometric Profiling of Framework Programme Participants</source>. <publisher-loc>Brussels</publisher-loc>: <publisher-name>EPEC</publisher-name>.</citation>
</ref>
<ref id="bibr66-1356389012460961">
<citation citation-type="book">
<collab>Technopolis and OST Paris</collab> (<year>2004</year>) <source>Future Priorities for Community Research Based on Bibliometric Analysis of Publication Activity for the Five-Year Assessment (1999-2003) of Community Research Activities</source>. <publisher-loc>Brussels</publisher-loc>: <publisher-name>EPEC</publisher-name>.</citation>
</ref>
<ref id="bibr67-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thoenig</surname><given-names>J</given-names></name>
</person-group> (<year>2000</year>) <article-title>Evaluation as usable knowledge for public management reforms</article-title>. <source>Evaluation</source> <volume>6</volume>(<issue>2</issue>): <fpage>217</fpage>–<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr68-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tijssen</surname><given-names>R</given-names></name>
</person-group> (<year>2001</year>) <article-title>Global and domestic utilization of industrial relevant science: patent citation analysis of science–technology interactions and knowledge flows</article-title>. <source>Research Policy</source> <volume>30</volume>(<issue>1</issue>): <fpage>35</fpage>–<lpage>54</lpage>.</citation>
</ref>
<ref id="bibr69-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Trochim</surname><given-names>WM</given-names></name>
<name><surname>Marcus</surname><given-names>SE</given-names></name>
<name><surname>Masse</surname><given-names>LC</given-names></name>
<name><surname>Moser</surname><given-names>RP</given-names></name>
<name><surname>Weld</surname><given-names>PC</given-names></name>
</person-group> (<year>2008</year>) <article-title>The evaluation of large research initiatives: a participatory integrative mixed methods approach</article-title>. <source>American Journal of Evaluation</source> <volume>29</volume>: <fpage>8</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr70-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Van Raan</surname><given-names>AFJ</given-names></name>
</person-group> (<year>2004</year>) <article-title>Measuring science</article-title>. In: <person-group person-group-type="editor">
<name><surname>Moed</surname><given-names>HF</given-names></name>
<name><surname>Glänzel</surname><given-names>W</given-names></name>
<name><surname>Schmoch</surname><given-names>U</given-names></name>
</person-group> (eds) <source>Handbook of Quantitative Science and Technology Research</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>, <fpage>19</fpage>–<lpage>50</lpage>.</citation>
</ref>
<ref id="bibr71-1356389012460961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weiss</surname><given-names>C</given-names></name>
</person-group> (<year>2004</year>) <article-title>Theory-based evaluation: past, present, future</article-title>. <source>New Directions for Evaluation</source> <volume>76</volume>: <fpage>41</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr72-1356389012460961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Whitley</surname><given-names>R</given-names></name>
<name><surname>Glaser</surname><given-names>J</given-names></name>
</person-group> (<year>2007</year>) <source>The Changing Governance of the Sciences: The Advent of Research Evaluation Systems</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>