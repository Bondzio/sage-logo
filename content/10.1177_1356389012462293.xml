<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="editorial">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EVI</journal-id>
<journal-id journal-id-type="hwp">spevi</journal-id>
<journal-title>Evaluation</journal-title>
<issn pub-type="ppub">1356-3890</issn>
<issn pub-type="epub">1461-7153</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1356389012462293</article-id>
<article-id pub-id-type="publisher-id">10.1177_1356389012462293</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Editorial</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Editorial</article-title>
</title-group>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>18</volume>
<issue>4</issue>
<fpage>402</fpage>
<lpage>404</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p>This issue of <italic>Evaluation</italic> begins with some new thinking about complexity and realist philosophy of science by a scholar-practitioner from Australia; and ends with a report on some sophisticated in-house evaluation design work from within the European Commission. In between we have new thinking about evaluation use and usability from a social practice standpoint; discussions of ethical sensitivity amongst evaluators; and a social construction and social-mechanism perspective on Research and Development (R&amp;D) and industrial subsidies respectively. Not many years ago, many of the evaluation ideas in this issue were confined to specialist symposia and the fringe of evaluation conferences. It is encouraging to see how fast ‘new’ thinking has come to shape mainstream evaluation practice.</p>
<p>Gill Westhorp sets out to build bridges between complexity theory and a ‘realist’ philosophy of science, which she argues ‘are sometimes seen as competitive or mutually incompatible’. She does so with a particular interest in causation and causal attribution in complex adaptive systems. For Westhorp, realism and complexity can be bridged through ‘substantive’ theory, which both schools of thought rely on, and notions of ‘layering’ in complex systems. Concepts such as ‘nested’ systems, embedded programmes and emergent properties have been creeping into evaluation parlance in recent years, all building on a layered understanding of reality. Combining these two sets of ideas allows the author to suggest that different bodies of substantive theory can be applied to different layers in a system, fitting well with both complexity and realist understandings. She goes on to illustrate this in relation to early intervention policies for children in which attribution theory, judgement theory, social capital theory and social inclusion theory all have explanatory power at different levels. The usefulness of this approach will be taken further in a forthcoming article in which Westhorp will apply her thinking to an actual evaluation.</p>
<p>Murray Saunders revisits evaluation use from a distinctly European perspective. This has been an area of study hitherto strongest in North America. Reviewing the extensive US literature on evaluation use, Saunders comments that despite ‘identifying many potentially relevant factors’, the literature is less strong identifying what factors encourage ‘positive’ use. He sympathizes with this reality, and rather than seeing evaluation use as a search for critical success factors, he regards ‘use’ itself as highly contextualized. Saunders distinguishes between ‘contexts of use’ and ‘usability’; the former being concerned with how and where an evaluation is used and the latter with the inherent qualities of an evaluation that make it more or less usable. The author focuses his discussion around evaluation as a ‘social practice’, which he understands as routinized behaviours, knowledge and motivations. These practices are ‘embedded’ in particular contexts, which Saunders unpicks drawing on evaluation experience in Higher Education, EU Regional and Social Cohesion policy and in EU Research and Development.</p>
<p>Geoffroy Desautels and Steve Jacob observe that whilst evaluators talk a great deal about ethical issues and professional associations have drafted standards and guidelines to address ethical concerns, empirical research on the subject is still ‘quite rare’. This article reports on a Canadian study about ‘how evaluators identify ethical dilemmas’. More particularly, Desautels and Jacob are concerned with how the values and orientation of evaluators affect their sensitivity to evaluation challenges: ‘Do evaluators perceive the ethical issues with which they are confronted? If so, what factors influence ethical sensitivity?’ Twelve Canadian evaluators were each presented with six ‘typical’ ethical dilemmas. They were invited to rate these dilemmas in various ways and then asked to explain their judgements. The authors found that the values of the evaluator – whether they were ‘corporatist’ or ‘altruistic’ – partly accounted for how they responded to these dilemmas. But familiarity with norms and standards and general evaluation experience also affected sensitivity to ethical dilemmas. As the authors themselves note, there are limits to this study. However, it is suggestive of the range of important choices that evaluators regularly face. It should also encourage others in the evaluation community to focus research on the behaviour of evaluators rather than on the prescriptions that codes and guidance embody.</p>
<p>Debates about the use of indicators in evaluation are long-standing. Indeed, there are many who regard quantitative indicators as only loosely linked to evaluative practice unless they answer clear evaluation questions and are informed by relevant theory. Benedetto Lepori and Emanuela Reale revisit this debate within the R&amp;D domain. Despite various initiatives to increase their use, uptake of indicators has not been that widespread in R&amp;D. The authors discuss why this is, noting, for example, the contextualized nature of indicators and hence difficulties with hoped-for standardized measurement; and the socially constructed nature of R&amp;D programmes – shaped by the perspectives of human actors. However, Lepori and Reale argue that most advocates of indicators in R&amp;D have emphasized ‘summative’ evaluations that try to measure research outputs. Whilst the authors see the need for greater use of quantitative indicators in summative evaluations, they also argue that indicators have a formative purpose. This is consistent with a conceptualization of R&amp;D programmes as ‘socially constructed actor spaces’ in which many actors participate within what is essentially an open system. In this framework indicators can even help actors better understand an R&amp;D programme’s assumptions and ‘the validity and the advantages of public investment in R&amp;D’.</p>
<p>Seweryn Krupnik has looked at the results of investment subsidies widely used in industrial policy in the EU – and in his native Poland – to encourage productive investment and growth in eligible firms. Despite their popularity, evaluations and studies have shown their success to be limited. Krupnik adopts an ‘abductive’ and grounded-theory approach in which theory was developed iteratively during the course of data collection. (See Levin-Rozalis in <italic>Evaluation</italic> 6(4) for a useful introduction to ‘abduction’ and ‘retroduction’.) Rather than simply measuring effects, the author identifies the ‘social mechanisms’ that might explain the poor results of investment subsidies. These mechanisms are seen as rooted in the programme theories of the main actors – policy makers, beneficiary firms and journalists: ‘the analysis shows that the programme theories of stakeholders accounts for the persistence and positive evaluation of the policy’. Krupnik concludes by suggesting how independent evaluations in this area should be used to improve public policies and reduce unproductive subsidies.</p>
<p>In ‘A visit to the world of practice’ Teresa Fitzpatrick provides an insider’s view from within the European Commission of how to evaluate the effects of legislation. Fitzpatrick describes how one part of the Commission, the Directorate General for Internal Market and Services, set about developing an evaluation approach that could be applied to ‘non-spending’ policy instruments such as legislation. The author paints a picture of considerable diversity in the way different Directorates General within the Commission design their own evaluations in this area. An evaluation of legislation is understood broadly: ‘It should consider not just whether the legislation did what it was expected to do, but also what other effects may have happened as a result.’ Being formative to policy, it considers negative as well as positive changes. Teresa Fitzpatrick describes an initial attempt to use a standard linear, input/output model of legislation and the thinking behind a more dynamic, multi-level model. This was needed to accommodate the diversity of 27 Member States and the amount of discretion that various European actors have in how European legislation is implemented. One of the striking aspects of this article is the way a sensitive political setting influences evaluation design. For example, the Commission is not able to evaluate Member State actions but is still interested in understanding how Member State discretion overall is able to shape the success of legislation. The methods proposed are shaped to this end.</p>
<sig-block>
<sig><bold>Elliot Stern</bold></sig>
</sig-block>
</body>
</article>