<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LDX</journal-id>
<journal-id journal-id-type="hwp">spldx</journal-id>
<journal-id journal-id-type="nlm-ta">J Learn Disabil</journal-id>
<journal-title>Journal of Learning Disabilities</journal-title>
<issn pub-type="ppub">0022-2194</issn>
<issn pub-type="epub">1538-4780</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022219412452097</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022219412452097</article-id>
<title-group>
<article-title>Meta-Analysis and Inadequate Responders to Intervention</article-title>
<subtitle>A Reply</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Swanson</surname><given-names>H. Lee</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff1-0022219412452097">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022219412452097"><label>1</label>University of California, Riverside, Riverside, CA, USA</aff>
<author-notes>
<corresp id="corresp1-0022219412452097">H. Lee Swanson, University of California, Riverside, Educational Psychology and Special Education, School of Education, Riverside, CA 92521 Email: <email>Lee.Swanson@ucr.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>45</volume>
<issue>6</issue>
<fpage>570</fpage>
<lpage>575</lpage>
<permissions>
<copyright-statement>© Hammill Institute on Disabilities 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Hammill Institute on Disabilities</copyright-holder>
</permissions>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>November/December 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>A meta-analysis by <xref ref-type="bibr" rid="bibr16-0022219412452097">Tran, Sanchez, Arellano, and Swanson (2011)</xref> of the published RTI literature found that the magnitude of effect size (ES) between responders and low responders at posttest was significantly moderated by the pretest ES and the type of dependent measure administered, whereas no significant moderating effects were found in the mixed regression analyses for number of weeks of intervention, length of sessions, number of sessions, type of intervention (one-to-one vs. small-group instruction), and criteria for defining responders (cutoff, scores, discrepancy, benchmark). Overall, the synthesis questions whether the published evidence on RTI related to classifying responders and nonresponders at posttest has shown to be adequately separated from pretest learner characteristics. <xref ref-type="bibr" rid="bibr13-0022219412452097">Stuebing et al. (2012)</xref> provided an excellent critique of this metaanalysis and raised at least four major issues related to interpreting the outcomes. We appreciate the opportunity to respond to these concerns.</p>
<sec id="section1-0022219412452097">
<title>Concern 1: Is the pre-post effect size analytic framework appropriate for a meta-analysis of intervention response?</title>
<p>An important issue that <xref ref-type="bibr" rid="bibr13-0022219412452097">Stuebing, Fletcher, and Hughes (2012)</xref> raised was questioning our assumptions about the direction of effect sizes (ESs) related to differences between responders and nonresponders as a function of the treatment at pretest and posttest. As stated by Stuebing et al.,
<disp-quote>
<p><xref ref-type="bibr" rid="bibr16-0022219412452097">Tran et al. (2011)</xref> used hierarchical linear modeling (HLM) to predict the posttest effect size (ES) from the pretest (ES), finding that the magnitude of the ESs increased in some cases from pretest to posttest. Thus, the data do not support the notion that posttest scores as a function of RTI provide outcomes independent of pretest scores.</p>
</disp-quote></p>
<p>In addition, Stuebing et al. commented,
<disp-quote>
<p>It is difficult to see why effective interventions in any service delivery model, including RTI, would lead to predictions of ESs that are not larger at posttest than at pretest. In fact, the larger difference between adequate and inadequate responders at posttest seems consistent with typical RTI studies where students are rather homogenous at pretest (i.e., meet criteria for risk) but heterogeneous after intervention, with some responding adequately and others inadequately.</p>
</disp-quote></p>
<p>However, we feel that Stuebing et al.’s observation may have overlooked the context of our study in the introduction of the article. We asked whether children identified as responders and nonresponders <italic>at posttest</italic> were related to differences (the gap or ES) at pretest. The key ES we are referring to is the relationship of ES between responders/nonresponders at posttest and pretest within an experimental condition, and not the ES gain that occurs when a treatment condition is administered (although this was reported). More important, the focus of our synthesis was on the <italic>changes</italic> in the “variance” between posttest and pretest within the experimental condition and not the ES per se. We do not argue that mean levels of performance fail to change between responders and nonresponders from pretest to posttest, but rather we argue that the variance between the pretest and posttest should be reduced. Clearly, as we have stated elsewhere, “any reasonable treatment improves post-test scores” (<xref ref-type="bibr" rid="bibr15-0022219412452097">Swanson &amp; Lussier, 2001</xref>, p. 323) and within design studies have an upward bias because posttest standard deviations in some cases are inflated. However, we expected that if RTI procedures are controlling for the over-identification of reading disabilities (RD), there would be a reduction in the variance from posttest when compared to pretest. As we stated,
<disp-quote>
<p>Under most intervention circumstances where there is no ceiling or floor effects, pre-test and post-test variable standard deviations are expected to be similar (see Hunter &amp; Schmidt, 1990, pp. 250–252; also see <xref ref-type="bibr" rid="bibr2-0022219412452097">Carlson &amp; Schmidt, 1999</xref>, p. 853; for a review). However, one would expect that if the intensity of instruction identifies true responders from those with LD, then a significant reduction in standard deviations would occur at posttest. (<xref ref-type="bibr" rid="bibr16-0022219412452097">Tran, Sanchez, Arrelano, &amp; Swanson, 2011</xref>, pp. 284–285)</p>
</disp-quote></p>
<p>That is, if one of the key goals of RTI is to “reduce” the overidentification of children at risk of having a learning disability (LD), then the correlation between the two groups (ES) at pretest and posttest would be weak because the heterogeneity in the sample that exists at pretest would be reduced. This is based on the assumption that evidence-based treatments have been reliably administered and the majority of children with teaching deficits rather than disabilities are now “theoretically” responsive, thereby reducing the variance (overclassification) related to previous teaching outcomes. Thus, we assumed that greater variance should occur at pretest for the high-risk sample than at posttest. Stuebing et al. argue the opposite: Groups are more homogenous at pretest then posttest.</p>
<p>In our sampling of the published research, we did not find evidence for greater heterogeneity at posttest than pretest. In our study, the overall standard deviations were 1.04 at pretest and 1.11 at posttest, with an overall correlation between ES at pretest and ES posttest of .75 (p. 290). In fact, the standard errors across all measures at pretest and posttest were identical (<italic>SE</italic> = 0.03; see Tables 2 and 3 of <xref ref-type="bibr" rid="bibr16-0022219412452097">Tran et al., 2011</xref>). This is not a strong argument for the increase of heterogeneity from pretest to posttest, as indicated by Stuebing et al. Given Stuebing et al.’s concern about including three studies they deemed as reflecting a bias at pretest, we did a follow-up by dropping these studies from the correlational analysis. After taking those studies out of the analysis, the mean ES and <italic>SD</italic> at pretest were 0.60 and 0.95, and the mean ES and <italic>SD</italic> at posttest were 0.76 and 1.14, and the correlation was <italic>r</italic> = .78. Again, this is not a strong argument that ES differences between the two groups at pretest/posttest are independent, nor does it fit RTI predictions related to the overidentification notion that the variance between the two groups is reduced.</p>
<p>We might also add that in contrast to Stuebing et al.’s observation, our overall findings of RTI studies suggest nothing out of the ordinary from what has been found in the treatment literature. For example, <xref ref-type="bibr" rid="bibr2-0022219412452097">Carlson and Schmidt (1999)</xref>, in their review of experimental designs, stated,
<disp-quote>
<p>Whether <italic>treatment by subject interactions</italic> [italics added] (Cronbach &amp; Snow, 1977) or different exposure to the treatment actually occur has been difficult to demonstrate empirically. Consequently, under most circumstances (i.e., in the absence of ceiling or floor effects), pre-training and post training dependent variable standard deviations are expected to be similar. (p. 853)</p>
</disp-quote></p>
<p>Thus, if this is the general pattern related to experimental interventions, one needs to question whether RTI studies are that unique in dealing with individual differences. The practical validity of RTI (evidence that something different is really happening) would be supported if the standard deviations related to pretest and posttest individual differences varied substantially from what generally occurs in the intervention literature. For example, if one argued that an intervention substantially increased individual differences in performance (as suggested by Stuebing et al.), then this would be reflected in systematically larger posttest standard deviations when compared to pretest standard deviations. This does not appear to be the general pattern found in the results of currently published RTI studies.</p>
<p>I believe the essence of our difference here is that Stuebing et al. presuppose that we selected articles showing differences at pretest, when in fact we selected articles reporting responders and nonresponders at posttest, then required in our selection of articles that pretest scores also be reported (see p. 285, Selection Criterion 5). No stipulations were made on what the pretest scores should look like. We deleted from our synthesis only studies that did not report any pretest scores. Although we recognized potential biases in published literature, it would be our contention that studies showing no differences between responders and nonresponders at pretest would also be just as likely to be in our synthesis as not (Stuebing et al. attest to this fact). Therefore, the sample of studies selected and outcomes related to author-identified responders and nonresponders within an experimental condition reflect the nature of the primary studies. Although Stuebing et al. suggest that these results should have been accompanied by simulations, this is appropriate only when we can make sense of the individual values composing the “input” to the analysis (e.g., corrected covariance matrices). Although the combination simulations and meta-analysis are more common in some domains (neuroscience; e.g., <xref ref-type="bibr" rid="bibr10-0022219412452097">Ramsey, Spirites, &amp; Glymour, 2011</xref>; also see <xref ref-type="bibr" rid="bibr4-0022219412452097">Hadf &amp; Willams, 2009</xref>, for biases in simulation studies with correlations) than, say, RTI studies, the reader needs to be aware that simulations are subject to several artifacts (e.g., pooling data across individuals, when those samples are from different sample distributions, etc.), and given the results of how our meta-analysis turned out, we would have some grave concerns at how the covariance structure would be established.</p>
</sec>
<sec id="section2-0022219412452097">
<title>Concern 2: Can Cohen’s heuristic interpretation of <italic>d</italic> be used for dichotomized outcomes?</title>
<p>As stated by Stuebing et al.,
<disp-quote>
<p>Effect size <italic>d</italic> . . . assumes that both groups are selected from the same population of individuals and that prior to intervention, these groups have the same population mean and standard deviation (<italic>SD</italic>). With <italic>randomization</italic> [italics added], the expected difference between the two means is 0 in the absence of a treatment effect.</p>
</disp-quote></p>
<p>Stuebing et al. then describe biases when selection occurs for variables established at various points on the normal curve at pretest. The reader needs to recall that the ESs between responders and nonresponders within a treatment condition were computed at two points (pretest and posttest) and not between control and experimental conditions (no doubt a problem with some of the primary studies in general). Forgetting for a moment that our comparisons were made not between a treatment and control condition (what they refer to as the treatment effect) but rather between groups in the same treatment, there are several ways to respond to the issues raised by Stuebing et al.</p>
<p>First, their assumption that subgroups within an at-risk sample, via a randomized design, should or could start at 0 at pretest (i.e., ES at zero) does not fit the data. On this issue, <xref ref-type="bibr" rid="bibr12-0022219412452097">Shadish and Ragsdale (1996)</xref> make an interesting observation (in response to the findings on treatment outcomes of <xref ref-type="bibr" rid="bibr8-0022219412452097">Lipsey &amp; Wilson, 1993</xref>) related to random assignment versus nonrandomized studies and state that “on an average, however, randomized experiments yield an average standardized means difference statistic of <italic>d</italic> = .46 (SD = .28), trivially higher than the nonrandomized studies <italic>d</italic> = .41 (SD = .36), that is the difference is near zero” (p. 1291). More important, when they examined pretest differences, the correlation between pretest and posttest ESs was significant, being .53 for the studies with pretests (<italic>N</italic> = 81) and .39 for 54 randomized studies and .84 for the nonequivalent control group designs, that is, larger pretest ESs are associated with larger posttest effect sizes, in both designs (p. 1294). Thus, Stuebing et al.’s assumption is in conflict with some of the literature. Clearly, our data suggest that any classification of responder–nonresponder differences at posttest calls for an adjustment related to already-existing pretest differences. Simply, we would argue that posttest scores between responder–nonresponders within a treatment condition are in part a function of pretest effects (i.e., pretest sensitization as well as learner characteristics), and therefore an adjustment for pretest effects is critical when defining groups as responders at posttest (see Kim &amp; Willson, 2004, for a related discussion).</p>
<p>Second, although the point made by Stuebing et al. is accurate related to the preferred research designs in the general scheme of treatments, these design characteristics were not apparent in the RTI studies that we reviewed. Although we accept the notion that a stronger case can be made for classification at posttest with extensive intervention, when we worked backward from these studies (i.e., studies were selected when responders/nonresponders were identified at posttest, and then we analyzed pretest performance), we found that this classification could be tied to pretest performance. A casual analysis of Table 1 in the <xref ref-type="bibr" rid="bibr16-0022219412452097">Tran et al. (2011)</xref> article clearly shows that low responders performed well below responders on several normed-referenced measures at pretest. Although we agree with the assumption that children with RD are more accurately identified at risk after intense treatment, we found, however, that differences at pretest were fairly accurate at predicting differences at posttest.</p>
<p>Finally, the designation of responders versus nonresponders clearly implies a dichotomization or a categorical variable. Stuebing et al. critiqued our use of the <italic>d</italic>’ index on dichotomized data. No doubt Cohen’s <italic>d</italic> can be easily converted to a correlation coefficient (see What Works Clearing House [WWC] for other formulas as well as the limitations of correlations), and there are corrections when continuous data are dichotomized (<xref ref-type="bibr" rid="bibr11-0022219412452097">Sánchez-Meca, Marín-Martínez, &amp; Chacó-Moscoso, 2003</xref>), but the data reported in the RTI studies were in terms of separate means and <italic>SD</italic>s for responders and nonresponders at posttest. Some studies provided further gradations (mild responders, partial responders), but again there was a focus on the categorical variable of responders and nonresponders. We think their argument about whether Cohen’s <italic>d</italic> “as intended” and how we calculated Cohen’s <italic>d</italic> is convoluted. They make the curious argument that calculating group differences within a treatment conditions (as we did) is not as Cohen intended because the participants were not randomized at the beginning (or they did not start with treatment effects at zero). There is no discussion in <xref ref-type="bibr" rid="bibr3-0022219412452097">Cohen (1988)</xref> that ESs can be interpreted as related to treatment only if randomization occurs. Obviously, the point of reference for our analysis on group differences within a treatment and their focus on “treatment effect” when compared to a control condition are different. Either calculation is testing a departure from the null hypothesis. As stated by Cohen,
<disp-quote>
<p>Whether expressed as a difference between two population parameters or the departure of a population parameter from a constant or in an any other suitable way, the ES can itself be treated as a parameter which takes the value of zero when the null hypothesis is true and some other specific nonzero value when the null hypothesis is false. (p. 10)</p>
</disp-quote></p>
<p>Thus, we are testing whether two groups given the same intervention at pretest and posttest in the continuum is zero. Either way, the ES serves as an index for departure from the null hypothesis.</p>
<p>In general, Stuebing et al., as well as our research team, would agree that responsiveness is a continuous variable. In fact, we would argue that dichotomizing the data loses information and reduces statistical power, and potentially biases the estimates. Furthermore, in the grand scheme of things it jeopardizes the validity and efficiency of our meta-analysis because of the single cutoff point and/or inconsistent cutoff points of studies included in the synthesis. That being said, this was the primary data we had to work with.</p>
</sec>
<sec id="section3-0022219412452097">
<title>Concern 3: Is phonological awareness a good predictor of response to intervention?</title>
<p>As stated by Stuebing et al.,
<disp-quote>
<p>In a model where posttest effect sizes were predicted from pretest effect sizes as well as dummy coded vectors representing both the category of posttest variable and methodological variables (such as method used to determine intervention response), Tran et al. reported that the beta weight for the PA coded vector was close to 0 and was non-significant. Does this mean that PA is not important for reading acquisition?</p>
</disp-quote></p>
<p>We are not sure why this was the focus. It may be because other published syntheses have found this variable as critical, whereas other studies view its importance as overstated when other variables are included in the analysis. The critique by Stuebing et al. provides an interpretation of the outcomes related to the centering of our variables, especially dummy variables (the reader should also see <xref ref-type="bibr" rid="bibr1-0022219412452097">Bryk &amp; Raudenbush, 2002</xref>, p. 34, for a more in-depth rationale for centering binary variables). We do not disagree with the analysis by Stuebing et al. here. They provide the typical meta-analysis argument of mixing apples and oranges (that is why homogeneity was reported for the reader in Tables 2 and 3 of <xref ref-type="bibr" rid="bibr16-0022219412452097">Tran et al., 2011</xref>), but their concern about centering seems odd to us. Centering, in all discussions we are familiar with on HLM analyses, calls for such procedures. Regardless, clearly not finding a significant parameter estimate for phonological awareness in predicting posttest outcomes in the full conditional model has to do with the magnitude of the standard error, as well as real word reading, word attack, passage comprehension, and rapid naming speed superseding (partialing out) the contribution of phonological awareness in predicting posttest.</p>
<p>Stuebing et al. do suggest that we made the analysis more complicated than necessary. We did provide zero-order correlations between pretest and posttest ESs, which we assumed told a great deal of the story. The difficulty was that significant error (random effects) existed among the studies, even when pretest and classification variables were entered into the analysis. Thus, some model testing was necessary to ensure that the relationships of pretest and other moderating variables to posttest ESs were not merely related to random effects. Even at that, we could reduce this random effect by only 76% in the full model. Thus, approximately 25% of the explainable variance was left unaccounted for.</p>
</sec>
<sec id="section4-0022219412452097">
<title>Concern 4: Does this meta-analysis support the conclusion that RTI is not effective?</title>
<p>Stuebing et al. conclude that our “methods are not appropriate for determining the effectiveness of interventions based on RTI approaches, which is more appropriately determined via randomized control trials and syntheses of these studies.” We would not disagree with that statement if such studies existed at the time we did our meta-analysis. Stuebing et al. also state, “<xref ref-type="bibr" rid="bibr16-0022219412452097">Tran et al. (2011)</xref> conclude that response to intervention (RTI) conditions were not effective at mitigating learner characteristics related to pretest conditions. The evidence presented in support of this assertion was that pre and posttest effect sizes were substantially correlated.”</p>
<p>They also focus on one of our sentences: “[U]nfortunately, the validity of RTI procedures, particularly in comparison to other assessment approaches, has not been adequately established in the present synthesis of the literature” (p. 293). They indicate that we went way beyond our data in this case. It is important for the reader to note that this sentence follows several paragraphs, and we were asked to provide a broader context to the findings by the anonymous reviewers. However, we would not necessarily back off from these general observations because we were unable to find any studies that made systematic comparisons with RTI to other classification procedures using randomized control conditions.</p>
<p>Stuebing et al. also state, “What is not clear is why <xref ref-type="bibr" rid="bibr16-0022219412452097">Tran et al. (2011)</xref> did not conduct a much simpler meta-analysis of the correlations among pretests, posttests, and other individual characteristics measured at <italic>baseline</italic> [italics added]” (p. 9). Stuebing et al. are familiar with the studies we reviewed, and few actually provided a condition separate from the pretest conditions that captured “baseline.” A point made by Stuebing et al. is whether we took into consideration the timing of the classification (i.e., an assessment of the categorical label at different points along the intervention continuum). They are correct; we did not include this information in the analysis because it was redundant with classification criteria and length of treatment. Some studies (as indicated) may have defined children as responders and nonresponders somewhere between pretest and posttest, and we did not code this (as there were not enough studies). We did code intervention time and found that its effect (at least within this data set) was not reliable (nonsignificant) in its prediction of the magnitude of the posttest ES between responder and nonresponders. It seems to us, however, that the closer the time interval for classification of responder/nonresponder is to posttest, the stronger the correlation with posttest, which, again, is not a strong argument for increasing heterogeneity.</p>
</sec>
<sec id="section5-0022219412452097">
<title>Final observation</title>
<p>A final point alluded to by Stuebing et al. was “what” the ES should be in defining the success of RTI in identifying responders/nonresponders. We might rephrase the question as follows: “What would be a reasonable ES benchmark for a valid separation of the groups (responders/nonresponders) when pretest and other methodological variables are partialed in the analysis?” We recognize some caution is necessary against applying ES with the same rigidity that one would typically use in a statistical significance testing. Because responders/nonresponders are usually determined within an experimental treatment, it would be necessary to take into consideration the design artifact for ES (this would be subtracted from the outcomes) as well as retest sensitivity (see <xref ref-type="bibr" rid="bibr15-0022219412452097">Swanson &amp; Lussier, 2001</xref>, p. 323, for discussion). Furthermore, <xref ref-type="bibr" rid="bibr3-0022219412452097">Cohen (1988)</xref> intended the magnitude of ESs to serve as only a broad, general guideline, not to be used blindly. Unfortunately, the field of LD has not provided, to date, a consensus on the “benchmarks” for the overall, experimentally based ESs that includes the performance of responders and nonresponders in context. The best the field has to offer, to date, are comparisons between children with LD within an experimental condition and those with LD in a control condition. In the area of reading, the magnitude of the ES has not been impressive for children with RD. For example, the <xref ref-type="bibr" rid="bibr9-0022219412452097">National Reading Panel (2000)</xref> reported that the evidence between reading (phonics) instruction and control conditions for students with RD varied on reading measures from .24 to .52, with a mean of approximately .33 (see Appendix E, 2-159). In short, there is a conundrum. The conundrum we confront (as quoted in <xref ref-type="bibr" rid="bibr7-0022219412452097">Light, Singer, and Willet’s (1990)</xref> book titled <italic>By Design</italic>, is that “meta-analyses often reveal a sobering fact: effect sizes are not nearly as large as we might hope” (p. 195).</p>
<p>Regardless of these issues, we think one potential benchmark was established in an earlier meta-analysis of experimental studies (<xref ref-type="bibr" rid="bibr14-0022219412452097">Swanson, 1999</xref>), which found that the overall ES for measures of word recognition for children with RD, partialed for methodological variations within pretest/posttest control group designs and type of treatment, was about .57 when comparing children with RD in the treatment and the RD control group. This is not to argue that this number serves as a benchmark for determining treatment effectiveness, but it does provide a rough approximation when interpreting the practical significance of the ESs.</p>
<p>Stuebing et al. also provide a reasonable critique, stating that we did not define what an adequate design might entail within an RTI framework. That was not the purpose of our synthesis. We would suggest, however, that a reasonable design to identify responders/nonresponder, as typified in the medical literature (e.g., <xref ref-type="bibr" rid="bibr5-0022219412452097">Hewitt et al., 2011</xref>), may involve the following sequence: prescreening (finding children at risk), implementing an intervention that has an extensive evidence base, placing children in a maintenance condition to establish baseline, then randomly assigning participants to treatment conditions (conduct pretest here to ensure an adequate distribution or stratification of responders and nonresponders), and then a withdrawal period (posttesting) to determine stable responders and nonresponders. The design is more realistic in assuming that little variation in responders would occur in the “baseline” period but acknowledges extensive heterogeneity exists at the pretest conditions.</p>
</sec>
<sec id="section6-0022219412452097">
<title>Conclusion</title>
<p>Clearly, Stuebing et al. have provided an excellent review of our meta-analysis. Although we disagree with them on several points, we think an appropriate conclusion is that more work is needed in this area. Their ideal study (randomization and no biases between responders/nonresponders prior to RTI) would have perhaps changed our outcomes and conclusions. But in meta-analysis there must be a reliance on the best evidence from the primary studies available. It is evident that as we look at the data, current interventions do not appear powerful enough to completely eliminate pretest differences for children at risk for RD. Perhaps more recent studies have addressed some of the design issues raised by Stuebing et al., as well as ourselves. Hopefully, as we update our analysis, this will be the case.</p>
</sec>
</body>
<back>
<ack>
<p>The author thanks Danielle Stomel for her comments on a draft of the article.</p>
</ack>
<fn-group>
<fn fn-type="other">
<label>Author’s Note</label>
<p>Janette Klingner served as action editor on this interchange of the two articles.</p>
</fn>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This article was supported by an Institute of Education Sciences (IES) Grants R324B080002 and R324A090002. The opinions expressed in this article do not necessarily reflect the opinion or policies of IES.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0022219412452097">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bryk</surname><given-names>A. S.</given-names></name>
<name><surname>Raudenbush</surname><given-names>S. W.</given-names></name>
</person-group> (<year>2002</year>). <source>Hierarchical linear models: Applications and data analysis methods</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr2-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carlson</surname><given-names>K. D.</given-names></name>
<name><surname>Schmidt</surname><given-names>F. L.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Impact of experimental design on effect size: Findings from the research literature on training</article-title>. <source>Journal of Applied Psychology</source>, <volume>84</volume>, <fpage>851</fpage>–<lpage>862</lpage>.</citation>
</ref>
<ref id="bibr3-0022219412452097">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>). <source>Statistical power analysis for the behavioral sciences</source> (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hadf</surname><given-names>A. R.</given-names></name>
<name><surname>Willams</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Meta-analysis of correlations revisited: Attempted replication and extension of Field’s (2001) simulation studies</article-title>. <source>Psychological Methods</source>, <volume>14</volume>, <fpage>24</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr5-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hewitt</surname><given-names>D. J.</given-names></name>
<name><surname>Ho</surname><given-names>T.</given-names></name>
<name><surname>Galer</surname><given-names>B.</given-names></name>
<name><surname>Backonja</surname><given-names>M.</given-names></name>
<name><surname>Markovitz</surname><given-names>P.</given-names></name>
<name><surname>Gammaitoni</surname><given-names>A.</given-names></name>
<name><surname>. . . Wang</surname><given-names>H.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Impact of responder definition on the enriched enrollment randomized withdrawal trial design for establishing proof of concept in neuropathic pain</article-title>. <source>Pain</source>, <volume>152</volume>, <fpage>514</fpage>–<lpage>521</lpage>.</citation>
</ref>
<ref id="bibr6-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kim</surname><given-names>E. S.</given-names></name>
<name><surname>Willson</surname><given-names>V. L.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Evaluating pretest effects in pre-post studies</article-title>. <source>Educational and Psychological Measurement</source>, <volume>70</volume>, <fpage>744</fpage>–<lpage>759</lpage>.</citation>
</ref>
<ref id="bibr7-0022219412452097">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Light</surname><given-names>R. J.</given-names></name>
<name><surname>Singer</surname><given-names>J. D.</given-names></name>
<name><surname>Willet</surname><given-names>J. B.</given-names></name>
</person-group> (<year>1990</year>). <source>By design: Planning research on higher education</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr8-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lipsey</surname><given-names>M. W.</given-names></name>
<name><surname>Wilson</surname><given-names>D. B.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The efficacy of psychological, educational, and behavioral treatment: Confirmation from meta-analysis</article-title>. <source>American Psychologist</source>, <volume>48</volume>, <fpage>1181</fpage>–<lpage>1209</lpage>.</citation>
</ref>
<ref id="bibr9-0022219412452097">
<citation citation-type="book">
<collab>National Reading Panel</collab>. (<year>2000</year>). <source>Teaching children to read: An evidence-based assessment of the scientific research literature on reading and its implications for reading instruction</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Institute of Child Health and Human Development</publisher-name>.</citation>
</ref>
<ref id="bibr10-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ramsey</surname><given-names>J. D.</given-names></name>
<name><surname>Spirites</surname><given-names>P.</given-names></name>
<name><surname>Glymour</surname><given-names>C.</given-names></name>
</person-group> (<year>2011</year>). <article-title>On meta-analysis of imaging data and the mixture of records</article-title>. <source>Neuroimage</source>, <volume>57</volume>, <fpage>323</fpage>–<lpage>330</lpage>.</citation>
</ref>
<ref id="bibr11-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sánchez-Meca</surname><given-names>J.</given-names></name>
<name><surname>Marín-Martínez</surname><given-names>F.</given-names></name>
<name><surname>Chacó-Moscoso</surname><given-names>S.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Effect-size indices for dichotomized outcomes in meta-analysis</article-title>. <source>Psychological Methods</source>, <volume>8</volume>, <fpage>448</fpage>–<lpage>467</lpage>.</citation>
</ref>
<ref id="bibr12-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shadish</surname><given-names>W. R.</given-names></name>
<name><surname>Ragsdale</surname><given-names>K.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Random versus nonrandom assignment in control experiments: Do you get the same answer</article-title>. <source>Journal of Consulting and Clinical Psychology</source>, <volume>64</volume>, <fpage>1290</fpage>–<lpage>1305</lpage>.</citation>
</ref>
<ref id="bibr13-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stuebing</surname><given-names>K.</given-names></name>
<name><surname>Fletcher</surname><given-names>J.</given-names></name>
<name><surname>Hughes</surname><given-names>L.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Meta-analysis and inadequate responders to intervention: A response</article-title>. <source>Journal of Learning Disabilities</source>, <volume>45</volume>, <fpage>565</fpage>–<lpage>569</lpage>.</citation>
</ref>
<ref id="bibr14-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>H. L.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Reading research for students with LD: A meta-analysis in intervention outcomes</article-title>. <source>Journal of Learning Disabilities</source>, <volume>32</volume>, <fpage>504</fpage>–<lpage>532</lpage>.</citation>
</ref>
<ref id="bibr15-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>H. L.</given-names></name>
<name><surname>Lussier</surname><given-names>C.</given-names></name>
</person-group> (<year>2001</year>). <article-title>A selective synthesis of the experimental literature on dynamic assessment</article-title>. <source>Review of Educational Research</source>, <volume>71</volume>, <fpage>321</fpage>–<lpage>361</lpage>.</citation>
</ref>
<ref id="bibr16-0022219412452097">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tran</surname><given-names>L.</given-names></name>
<name><surname>Sanchez</surname><given-names>T.</given-names></name>
<name><surname>Arrelano</surname><given-names>B.</given-names></name>
<name><surname>Swanson</surname><given-names>H. L.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A meta-analysis of the RTI literature for children at risk for reading disabilities</article-title>. <source>Journal of Leaning Disabilities</source>, <volume>44</volume>, <fpage>283</fpage>–<lpage>295</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>