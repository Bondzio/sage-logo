<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">CTJ</journal-id>
<journal-id journal-id-type="hwp">spctj</journal-id>
<journal-id journal-id-type="nlm-ta">Clin Trials</journal-id>
<journal-title>Clinical Trials</journal-title>
<issn pub-type="ppub">1740-7745</issn>
<issn pub-type="epub">1740-7753</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1740774512463179</article-id>
<article-id pub-id-type="publisher-id">10.1177_1740774512463179</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Panel Discussion</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Panel discussion 1</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Dickersin</surname><given-names>Kay</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Williams</surname><given-names>George</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>George</surname><given-names>Stephen</given-names></name>
</contrib>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>9</volume>
<issue>6</issue>
<issue-title>University of Pennsylvania Conference on Statistical Issues in Clinical Trials: Emerging statistical issues in the conduct and monitoring of clinical trials</issue-title>
<fpage>696</fpage>
<lpage>704</lpage>
<permissions>
<copyright-statement>© The Author(s), 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">The Society for Clinical Trials</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p><bold>Kay Dickersin (Johns Hopkins)</bold>: What I’ve heard so far is that the statistical issues that we are dealing with are either related to doing good science or doing better science. To this end, we have heard about methods to address adherence and take into account concurrent therapies and the course of the disease, as well as the education and training of people who are doing trials, and who are sitting on Data and Safety Monitoring Boards (DSMBs). We’ve also heard about incorporating patient advocacy groups and consumers in the planning and carrying out of trials, to enhance the validity and relevance of the questions we are asking and the data we are collecting.</p>
<p>Another concept to consider in our discussions would be improving the efficiency of trials. It is not enough that randomized trials are the gold standard for testing interventions; we also want them to be efficient and a practical choice. We have heard about creating a stronger infrastructure for increasing efficiency, and also about accruing to trials in a more efficient manner. It has been argued that we can even achieve good science and also be more efficient simultaneously.</p>
<p>We have had a good mix of topics in this session and I will focus on putting these issues into a larger context. Comparative effectiveness research (CER) is today’s new buzz word, and in this regard, I was especially interested in a recent commentary by Drs. DeMets and Califf [<xref ref-type="bibr" rid="bibr1-1740774512463179">1</xref>], where they asked, ‘Where have all the academic clinical trialists gone?’ They make a comparison between industry-funded trials and academic trials, the latter presumably funded by National Institutes of Health (NIH) and nonprofit organizations. Before the recent discussions about what CER really means, I hadn’t really thought about whether these two funding streams are related to differences in whether a trial addresses doctors’ questions. In these discussions, people have been saying that commercial trials don’t answer the questions doctors really have. For one thing, they compare active treatment to placebo, because of the requirements of the Food and Drug Administration (FDA), when doctors’ questions are really about head-to-head comparisons of interventions. Furthermore, these days, we are interested in patient-centered outcomes research (PCOR) and neither commercial, NIH, nor not-for-profit trials have been addressing PCOR adequately.</p>
<p>One of our challenges as trialists is maintaining a balance in the types of trials that we do as well as their initiating group. By this I mean a balance in serving the goals of commercial interests (for example, finding drugs that have a really big effect or ‘blockbuster’ drugs), and serving the public’s interest. The public, which includes clinicians, payers, patients, and others who are interested in making sure we get the evidence to the patient, is less interested in profit and more interested in treatments that work (including for conditions lacking adequate treatments, which may be relatively rare). The public is also interested in trying to decrease the cost of treatment, overall, by finding out what really works.</p>
<p>A big challenge to those of us doing trials is the cost of doing trials and that cost is an impediment to trial initiation. Cost is a barrier whether we are sitting in a company that wants to do trials but does not want to invest a lot of money in drugs that don’t work, or whether we are hearing NIH say that they have finite resources; could we use another method to examine the question so they don’t have to invest so much in a trial?</p>
<p>Depending on whether we are doing trials within industry or within the public sector, we may attempt to reduce costs differently, often by fiddling with study design. Is it possible to modify the classic randomized controlled trial (RCT) design without jeopardizing the science? We’ve talked about using adaptive designs and modifying sample size, properly, in a preplanned fashion. But adaptive design has become a buzzword, as Dr Coffey said. Once methods become part of the self-taught researcher’s toolbox, people who maybe haven’t studied some of the theoretical underpinnings of the methods may influence design choice with less knowledge about the potential associated biases. For example, researchers may think that calling their study an adaptive trial means they can decide to change the planned sample size midstream. Consumers are onboard with adaptive design, at least in cancer, because the method appears to support getting answers faster. Like the researchers, however, many of them do not understand what adaptive design actually means.</p>
<p>There are other aspects of design that affect sample size and the cost of a trial; for example the choice of primary outcome. Surrogate outcomes, often used by industry, assume a causal pathway, which may or may not be correct, and could result in a waste of both time and money in the long run. Cause-specific mortality as a primary outcome, where relevant, may give us the positive results we are seeking, and reduce sample size compared to all-cause mortality, but we may not be getting the information we want about the balance between effectiveness and safety. This approach to reducing sample size is most often used by academic researchers and NIH.</p>
<p>We haven’t talked about noninferiority trials, but to date, this is a design issue that has been more important to commercial sources than academic sources. Network meta-analysis is a method that addresses questions the public wants answered about which treatment works best, and could reduce the need for multiple head-to-head trials by using evidence from direct and indirect comparisons. Methods development and testing is still underway; while an exciting direction for finding out what works, home-grown methods are already being implemented, misused, and misinterpreted.</p>
<p>Finally, as mentioned earlier by Dr Coffey, more and more people in academia are both doing trials and monitoring trials without formal training themselves or strong methods partners, especially with the emphasis on trials accorded by NIH’s Clinical Translational Science Awards and others. Many of you have probably sat on DSMBs where the members, and even the chairs, are making it up as they are going along, and you see the worrisome consequences of not having proper training in clinical trials.</p>
<p>A recent request for applications from NIH (RFA-HL-12-019: Pilot Studies to Develop and Test Novel, Low- Cost Methods for the Conduct of Clinical Trials) asked for applicants to propose trial designs that decrease the costs of doing trials, using methods we haven’t talked about today, methods that are challenging all of us to consider the trade-offs between science and cost. NIH is asking us to minimize trial infrastructure, minimize the number of visits, explore novel methods to obtain consent, and employ low-cost methods of monitoring. They suggest, for example, that we consider methods to obtain group consent to reduce the amount of one-to-one patient time required; methods for embedding randomization, consent, and outcome assessment into the electronic health record; and methods to incorporate trial recruitment into routine clinical care.</p>
<p>Finally, I think the role of the consumer as a research partner is increasingly discussed, especially with a focus on patient-centered outcomes. Susan Love’s Web site entitled the Avon Army of Women (<ext-link ext-link-type="uri" xlink:href="http://www.armyofwomen.org/">http://www.armyofwomen.org/</ext-link>) has recruited hundreds of thousands of women who say they are ready to participate in either observational studies or randomized trials about breast cancer. A large number of studies have already been initiated that recruit from this pool of willing and potentially eligible women. While these women represent a selected group, just in their willingness to be recruited, Web-based armies of self-identified potential participants may be another method of incorporating consumer volunteers into our studies and recruiting more quickly to clinical trials. The model of having a population that is ready, willing, and able to participate in studies, as an alternative to using electronic health records, is an active form of encouraging the public’s engagement in research and is an interesting direction for the modern RCT.</p>
<p><bold>George Williams (Amgen, Inc.)</bold>: In terms of Chris Coffey’s paper on adaptive designs, there are well-understood and less well-understood types of adaptive designs. The example Dr Coffey presented was a blinded sample size reestimation, which is generally well understood, while a less understood approach might be to use interim results on a treatment-specific basis to then proceed with the trial subsequently. Certainly in the less well-understood approaches, I very much agree with Dr Coffey’s comments that the continuing challenges to the wide adoption of such methods in clinical trials are both the statistical and logistical aspects. A recent paper referred to a survey of adaptive designs and some of the barriers to their use in pharmaceutical research, and the authors indicate that the statistical methods for adaptive designs are way ahead of the difficulties in the implementation of these designs, as we have heard emphasized this morning.[<xref ref-type="bibr" rid="bibr2-1740774512463179">2</xref>] There is clearly a need for continuing discussions with regulatory agencies about these designs, but I think even within the pharmaceutical industry, the need to engage the multidisciplinary group involved in drug development to understand what some of the issues are is equally critical.</p>
<p>The value and lengthy development of simulations for understanding the appropriate designs for adaptive trials and their implementation was highlighted. While Dr Coffey’s emphasis was in terms of NIH funding models for writing grant applications, in industry, there is a parallel scenario in that you definitely need time to do the kind of careful thinking that may push drug development times higher. So it is not really so different in the two fields.</p>
<p>I now turn to Dr Zhang’s paper on predicting accrual in ongoing trials. Several sophisticated and informative models were presented today, dealing with a very practical problem in the execution of clinical trials. Such models can certainly aid us in monitoring so that we can take appropriate actions to ensure the success of trials. It is nice that the models presented do have ways to recognize explicitly some of the randomness involved. I think the primary challenge in this approach is the constancy of the assumptions. I was very pleased to see that the models accommodated multicenter clinical trials, which is very frequently the approach for larger studies, as well as the allowance for nonhomogeneous processes during the course of the prediction. There are so many things that can change over time. I definitely see the advantage of the models in that they can actually handle the multisite situation and the varying of the assumptions over time.</p>
<p>The presentation was on recruitment, but retention is extremely important as well and we shouldn’t forget that as we think about accrual. Dr Zhang was dealing with accrual of patients, but of course, many trials are event-based and what is really important is the accurate prediction of the accrual of the events since we want to know when the study is going to be over. I wonder if this methodology could be applied to event rates.</p>
<p>Drs Kimmel and Troxel’s paper dealt with the important topic of adherence. I must admit I was intrigued by the lottery incentive, as well as the other kinds of tailored approaches that were mentioned. Of course there are all kinds of things you’d be curious about, as to what might influence the value of such incentives. I thought the paper reemphasized for us the challenges in interpreting observational research and the value of the randomized clinical trial. I am also wondering if there is a possibility that this kind of approach can actually help not just in medication adherence but also in visit adherence.</p>
<p><bold>Stephen George (Duke University)</bold>: Dr Coffey indicates that the statistical issues and principles of adaptive designs are mostly well known and developed, although there are certainly still methodology issues that need some work. However, we already know what to do in principle and the real problems are practical. One of the key principles is that the approaches must be defined in advance. There is no room for ad hoc procedures if you want to keep yourself out of trouble. Another point is that nearly all clinical trials are already adaptive, at least to some degree. For example, any group sequential trial is an adaptive design. The only kind of trial that wouldn’t be adaptive is one in which a fixed number of patients is entered and no analysis is done until the end of the trial no matter what happens before then. It is very unlikely that anybody is using this kind of design, especially for phase III trials in serious diseases. Adaptive designs are very appealing in principle since they use early data from the trial to inform the future conduct of the trial, including sample size, randomization balance, test statistics, end points, selection criteria, and so on. If the goal is to produce reliable results in less time with fewer patients, who wouldn’t want to do that?</p>
<p>But what is the added cost of such adaptive designs? These designs require increased planning, complicate the logistical issues, and require enhanced information technology and data management procedures. One thing I don’t think was mentioned explicitly by Dr Coffey was that if the rules for an adaptive design are known, once an actual decision is taken based on the rules, it is easy to reverse-engineer the interim results, at least within some range. Does this matter? It might, particularly if that range is narrow enough.</p>
<p>Slowly accruing trials are extremely common, for various reasons, particularly in the area of cancer. One reason is the extensive inclusion/exclusion criteria. One trial leads to the next, the eligibility criteria for the previous trial is the default with additions based on the details of the new trial. That is a serious problem that greatly contributes to accrual problems. There are other contributing reasons such as competing trials and slow Institutional Review Board (IRB) approval. Because of these problems, the National Cancer Institute (NCI) some years ago developed rules for terminating slowly accruing trials. Even without slow accrual, I think accurate prediction of accrual can be useful as was pointed out by Dr Zhang. The various approaches she used were complicated and the properties seemed reasonable, but is the added complexity really worth the effort? I am not completely convinced of this. It might be useful to extend the prediction to the number of events. The number of events is also a stochastic process but it is a filtered process based on a random sum, depending on when the patient enters the trial and the survival distribution. The computational details may be complicated but the principles are not.</p>
<p>Drs Kimmel and Troxel raised the issue of adherence with the questions addressed: how can adherence be improved, by how much, and at what cost? One issue that wasn’t addressed directly is the effect on efficacy. That is, we may know, or strongly suspect from general causal notions, that adherence is a good thing, but do we really know that increasing the adherence of those who were not adherent will improve outcome? Although we may think this is obvious in some of the situations presented, it surely can’t be taken for granted. I do agree strongly that the clinical trials that are being proposed by Kimmel and Troxel are the best way to assess these kinds of interventions. What I really like about this approach is bringing to the table behavioral economics. We know that the more traditional rational economics (i.e., that individuals act to maximize expected utility) just doesn’t work in many settings. So the approaches based on positive feedback, preferences, variable reinforcement, and so on are fascinating and I think are certainly worth studying in an attempt to increase adherence. The last design that was presented was a 2 × 2 factorial design looking at different levels of adherence, a very nice example of the type of trials that can be done in this setting.</p>
<p><bold>Chris Coffey (University of Iowa)</bold>: Dr George raised the issue of added costs for adaptive designs, and whether this added cost is worth it. However, I’m not sure that the issues that were brought up are specific to adaptive designs. I think a lot of the logistical issues raised, such as the need for better data management and improved drug supply are important in even standard clinical trials. In the past, we as a clinical trials community may have been able to sweep these issues under the rug. However, adaptive designs have brought these issues to the forefront and forced clinical trialists to address them to an extent that we have avoided for far too long. So, although an adaptive design does add complexity and cost, I am not sure that the added cost there is entirely due to the adaptive design. I think as a whole we need to develop better infrastructure. This leads directly to the point that Dr Dickersin raised, which was the need to develop better educational programs. I would like to pose a question for discussion by the panel or to others: Is there a current disconnect between what students get in a biostatistics training program versus what biostatisticians do on a day-to-day basis after graduation? I think the best way to answer this question is to ask a series of questions: what fraction of biostatistics graduates work on clinical trials and what percentage of your graduate training involved a discussion of clinical trial–related issues? What percentage of your graduate training was related to doing sample size calculations? What percentage of your training was related to doing group sequential methods? I think there is a growing disconnect between what is driving growth and demand in biostatistics and clinical trials versus what people are learning in their training programs. I think we need to more closely align biostatistics training programs with the demands and needs that are driving the increased demand for biostatisticians.</p>
<p><bold>Kay Dickersin (Johns Hopkins University)</bold>: Our PhD students in the clinical trials concentration have given us feedback repeatedly that they are not getting enough from the curriculum in terms of using statistics in clinical trials. We have not really understood their concerns. They have plenty of required courses that seem to address this issue – longitudinal analysis, experimental design, adaptive design, and so on. What I understand from our discussion here is that they probably mean ‘how does it happen in practice’? We must think of how to give them a practicum, a short simulated experience in doing a clinical trial, where they use these methods.</p>
<p><bold>George Williams (Amgen)</bold>: We definitely need more attention in our training programs to some practical experience. This could be done through consulting opportunities, or through program internships, and have tremendous value. Also programs that help improve communication skills (e.g., presentation skills, interactions with consultants) would be important additions.</p>
<p><bold>Xiaoxi Zhang (Pfizer)</bold>. I want to first address a common theme from both Dr Williams and Dr George – the need to monitor the number of events in trials with survival endpoints. There is existing and ongoing research work in this field, in which the accrual and event processes are jointly modeled [<xref ref-type="bibr" rid="bibr3-1740774512463179">3</xref><xref ref-type="bibr" rid="bibr4-1740774512463179"/><xref ref-type="bibr" rid="bibr5-1740774512463179"/><xref ref-type="bibr" rid="bibr6-1740774512463179"/>–<xref ref-type="bibr" rid="bibr7-1740774512463179">7</xref>]. The event process can either be modeled in a parametric or a nonparametric fashion. Nevertheless, one generally makes the assumption that the patients enrolled early in the trial are similar to those enrolled late in the trial. Dr Williams also raised the issue of addressing other accrual patterns in clinical trials. A flu medication trial, for example, will have a seasonal accrual pattern, with high enrollment during the flu season and low during the rest of the year. The methodology discussed today is more appropriately deemed as a principal framework for such topic. It will further benefit from tailoring in specific settings, for example, adding a cyclic component when modeling the accrual pattern of a flu medicine trial. In addition, prior information can be customized. During the break, I had conversations with a few colleagues, and comments were made that historical accrual in similar trials could serve as proper prior information, which is worth looking into. Dr George queried whether the additional complexities that are introduced in these accrual predictions are justified. In my opinion, quantification of uncertainties is one important contribution (among many) that the statistics discipline makes.</p>
<p><bold>Andrea Troxel (University of Pennsylvania)</bold>: One issue that was mentioned by each of the panelists, in slightly different ways, is the logistical challenge of trying to reduce costs and simplify processes. I would like to describe one of the initiatives underway at the University of Pennsylvania Center for Health Incentives (<ext-link ext-link-type="uri" xlink:href="http://www.cphi.upenn.edu/">http://www.cphi.upenn.edu/</ext-link>). A project called ‘The Way to Health’ will use a Web-based platform to facilitate the conduct of trials and also allow for general implementation of many of these processes. We are building a platform that has interfaces with many kinds of users: participants in trials, statisticians, study managers, principal investigators, and financial institutions. We are working to make this as smooth and as automated as possible, and therefore scalable, as we identify effective interventions that we can then implement much more broadly. The development work is ongoing and several pilot studies have recently begun randomizing patients, using different kinds of interventions in different disease settings. A useful feature of this platform is that it allows us to incorporate biometric devices located in subjects’ homes directly into the system. For example, we can enable automatic uploads of daily weight data from scales in subjects homes in a pedometer study to increase walking, or data from glucometers for diabetic subjects participating in studies to better control their blood sugar. The ‘Way to Health’ system has many potential applications, and we are trying to make it as general as possible to interface with the new biometric devices that are becoming more commonly used.</p>
<p>Dr Williams mentioned the distinction between visit adherence and medication adherence; another way to classify this is adherence with trial procedures versus adherence to the medical therapy in question. Our approach thus far has been to provide fixed financial incentives for visit adherence to everyone in the trial regardless of intervention arm in order to maintain participation in the study across all arms. There is certainly some concern, however, that subjects in the intervention arms may be more engaged and thus may be more likely to be more compliant with study procedures and visits overall. We have used fixed cash payments for attendance at study visits and have been reasonably successful with maintaining approximately equal participation levels across different arms. Different kinds of financial incentive interventions could be applied to trial adherence as well; as of now, we treat them separately so that we can evaluate the intervention effects on the primary outcomes of interest.</p>
<p>A related question, debated in behavioral economics, is whether to incentivize the processes for adherence or the outcomes themselves. In the warfarin trial described in our article, the process is taking of the pill and the outcome is the International Normalized Ratio (INR; a measure of the level of warfarin in the system). Our approach has been to try to do both where possible, but we prefer hard, well-defined outcomes. In the warfarin trial, the primary outcome is out-of-range INR, though we are also assessing daily adherence to pill-taking using the electronic monitors. In other trials, we are assessing adherence to statin therapy, but the primary outcome will be the reduction in cholesterol levels. It is not always clear, however, what the most effective target is for incentives. The question of fairness can also arise, since behaviors are clearly under the control of individuals, while actual biomarker levels may not be.</p>
<p><bold>Stephen George (Duke University)</bold>: I question whether incentivizing nonadherent subjects will influence the ultimate outcome of clinical benefit of some kind. It may simply influence behavior or some intermediate outcome but not the clinical benefit.</p>
<p><bold>Stephen Kimmel (University of Pennsylvania)</bold>: I completely agree. Just to give a specific example, again, going back to co-payments for medications, if you raise co-payments on medications, patients may think ‘this is ridiculous, I have to pay more for my medication now’. So theoretically, if you reduce co-payments patients will be happy, but there is actually some theoretical concern that if you make a medication free, it becomes of less value to the patient. So they say gee whiz this is free, it must not be as good as the one that costs US$7.00. I think you are right, it is always a two-sided test and that is why I think we need to formally test this rather than just use these interventions. The other comment I will make is in terms of thinking about patients in trials versus in the real world. We put all these resources and effort into developing drugs and trying to show that they work, and then we let them out into what I call ‘the wild west of medical practice’, and there are always concerns about how effective they really are. One of the missing links, although certainly not the only one, is adherence. For instance, using warfarin as an example, we know that patients in clinical trials have much better control over their INRs than patients even in specialized clinics and observational studies. Some of that is probably the physicians but it tends to be the same physicians participating in both of those types of studies, so it has to be something about the patients. One of those missing links is probably adherence. If we really want to maximize the bang for the buck of our carefully controlled clinical trials in terms of public health, we need to maximize everything we can to translate those findings into the real world, and some of that is going to be related to adherence.</p>
<p><bold>Benjamin French (University of Pennsylvania)</bold>: It seems like there may be ethical concerns when incentivizing an outcome rather than a behavior, because a behavior is directly controlled by the study subject, but the outcome may be influenced by factors, biological or otherwise, outside the subject’s control.</p>
<p><bold>James Neaton (University of Minnesota)</bold>: For many trials that are done one may be able to nest questions about the effectiveness of different adherence interventions. For such trials, it may be better to randomize sites to different interventions instead of individual patients. This may be a cost-efficient way of addressing some of these questions about optimizing adherence to interventions that we are doing. I applaud the notion that you are using hard end points and not self-reported adherence.</p>
<p><bold>James Fisch (New York University)</bold>: As someone who has been trained as a behaviorist, I’d like to point out that incentives are empirically defined; just because you think you are giving somebody money doesn’t necessarily mean that they will work for it. This is why I like the tailored approach because you identify the characteristics and the behavior of the individual and work with that as a means of adapting your adherence model.</p>
<p><bold>Stephen Kimmel (University of Pennsylvania)</bold>: I take your point and believe that we have to replace the word ‘incentives’ as a summary name for these approaches.</p>
<p><bold>Andrea Troxel (University of Pennsylvania)</bold>: One issue we are investigating is the relative effectiveness of cash, or a nonmonetary gift, or the chance to donate money to your favorite charity. There are many creative approaches we can take to broaden the applications to move away from strictly cash value. It may turn out that ultimately the cash value is what’s important, but there is some evidence that gift cards or lotteries rather than direct payments are more effective, in part because some of the value in these interventions for participants lies in their entertainment aspect in addition to the monetary value.</p>
<p><bold>Barbara Tilley (University of Texas)</bold>: Referring back to the presentations by Drs Kimmel and Troxel, we were doing a study of a dietary intervention in automotive industry workers and did some focus groups. One of the things we had planned was a lottery where the participants receive tickets for attending classes, with a big drawing at the end. The participants were infuriated with this concept. They argued that one of their group should not receive a big prize by chance for the good behavior of all. Their union attitudes just did not lend us to using a lottery as one of our incentives.</p>
<p><bold>Stephen Kimmel (University of Pennsylvania)</bold>: The different responses to incentives are interesting and probably not unexpected. The goal is just to try to improve the public health and the money is the method in which to do it but the idea is not to reward people for bad behavior, or slight people for good behavior. The point is to improve the health of the public and with that perspective I think it makes a lot of sense. But I certainly understand the automobile union perspective or the perspective that this isn’t fair, why are you paying people for bad behavior. The answer is we are not; we are trying to actually motivate them to behave well so that we all get healthier. If you want to take the real global perspective, your insurance premiums will go down so you will get something out of it too.</p>
<p><bold>Marc Buyse (International Drug Development Institute (IDDI), Belgium)</bold>: I would like to change the topic a bit and go back to adaptive designs. Dr Coffey said that adaptive designs are not always better and that is certainly true; there is, however, a specific class of adaptive designs that we know is always worse and that is the class of designs where the adaptation consists of increasing the sample size based on the observed treatment effect. There is a very nice article by Tsiatis and Mehta [<xref ref-type="bibr" rid="bibr8-1740774512463179">8</xref>] who showed that in fact such designs are always inefficient compared to group sequential designs. This is a result that is not well known and I think should be, because it sort of rules out the use of such adaptive designs (which are a very specific subset of adaptive designs). I think it is good to know that in at least those cases it is always worse in theory to use an adaptive design than to use a good old group sequential design.</p>
<p><bold>Janet Wittes (Statistics Collaborative)</bold>: I have two comments. With regard to the discussion of reverse engineering, I recently heard a webinar during which somebody recommended that we do more adaptive designs but because of reverse engineering the protocols should not include the actual design. He suggested including a ‘Methods’ section that simply says the study will have an adaptive component. I thought that was an interesting comment but I don’t know how such a protocol would pass review committees.</p>
<p>The other issue that I wanted to mention relates to topics addressed by both Drs Coffey and Williams. I have been involved in an adaptive trial where neither the statistician designing the trial, the contract research organization producing the data, nor the sponsor understood the importance of accurate interim data. In particular, they did not recognize that the delay in adjudicating events may depend on the complexity of the case. This has created a huge problem because the DSMB refuses to make recommendations about the adaptations, but others involved say the protocol specifies how to adapt. In light of this example, I want to emphasize Dr Coffey’s point that adaptive designs require a different kind of data management than fixed designs.</p>
<p><bold>Elizabeth Kumm (i3Statprobe)</bold>: A few years ago, I did a simulation for predicting events ahead of a trial that had accrued very fast and investigators were asking when can we look at the data? They wanted to know when we were going to get to 90% of our progression-free survival events. It was a much simpler model than discussed here. But I wanted to bring up two points; one is we very definitely had to make sure we put in a lag time in the model for when the data would be available, and to Dr Coffey’s point, I had to work very closely with the data management staff and build an entirely different data management structure, because the data management staff were used to waiting for the data lock to perform all of the edit checks and cleaning. That was the way their processes were set up.</p>
<p><bold>Stephen George (Duke University)</bold>: It has been my experience that even with a very simple adaptive design, a group sequential design, for example, you run into problems such as the adjudication of endpoints and the extent to which follow-up is up to date. In addition, you might have enough known events, but you also know that there are likely other events that have occurred but have not been reported. In more complicated adaptive designs, these issues will be exacerbated. We will need substantial improvements in our data management procedures.</p>
<p><bold>Bob Zhong (Johnson and Johnson)</bold>. There is a great deal of discussion about how much to specify in the protocol as to how you adjust the sample size. If you leave it to the DSMB, the approach is totally out of the control of the company. But, the DSMB will likely be ill informed on the overall development strategy of the compound. Making a recommendation in such an environment may produce an unrealistic sample size. Are there some ideas on how to balance the need to keep the sponsor ‘blinded’ and the need for the DSMB to make recommendations in context?</p>
<p><bold>George Williams (Amgen)</bold>: I would have thought that you would have provided very clear instructions to the DSMB.</p>
<p><bold>Stephen George Duke University)</bold>: Are you implying that the DSMB might be charged with designing part of the study?</p>
<p><bold>Bob Zhong (Johnson and Johnson)</bold>: If you specify everything in the Data Monitoring Committee (DMC) charter, that is a different story, since that DMC charter is also known to the company and they can still do the reverse engineering to obtain information about trial status.</p>
<p><bold>Stephen George (Duke University)</bold>: That is certainly an issue that can always happen but it happens in a standard group sequential design too. If you know the rules and you know that the trial wasn’t stopped, then you have narrowed down the possibilities. In that case, usually you haven’t narrowed the possibilities enough to be very informative, since by the usual rules, a ‘nonstopping’ decision simply means that the interim result wasn’t an extreme one. But in some of these adaptive designs, you could be getting into a situation where you know a lot more or you have narrowed that range down to an extent that it might affect future accrual, the types of patients that are accrued in the future or other things. I am just raising it as a possibility, I don’t have any proof of this, and nobody really does. It raises the potential for a kind of bias that isn’t covered in our models. The design and the particular decisions made in practice may affect the study in ways which haven’t been anticipated.</p>
<p><bold>Chris Coffey (University of Iowa)</bold>: Even when you talk about prespecifying an adaptive design, when you serve on a DSMB, things happen that you have to react to. I served on a DSMB that involved a seamless phase I/IIa design. The first stage involved a selection design to choose between several doses of a compound. The second stage involved taking the selected dose forward for comparison with placebo in a futility design. Consider the selection design used for the first stage. In a pure selection design, you simply take the winner. However, in this seamless design, there is an issue lurking in the background. What should be done if the larger dose is selected, but there is a potential safety concern. The safety concern might overrule the selection, and justify taking the smaller dose forward. That is certainly OK, but is a decision that would usually be made by the study investigators between phases of research. In a seamless design, once data start to accrue, the investigators should be removed from these types of decisions. To obtain investigator input into these decisions, extensive discussions may need to take place early on in a study between the investigators and the DSMB – before you get to the point where you are looking at data. Even if that is done adequately, it is the DSMB, not the investigators, who will now be responsible for making that decision. I don’t think that is consistent with the standard role of what a DSMB should be. We need to be careful when we start to implement trials with an adaptive design that we are not dramatically changing the role of the DSMB.</p>
<p>I want to go back to a comment that Dr Wittes made. The use of adaptive designs has added a level of complexity and infrastructure. For doing standard group sequential interim analyses, there is the concept of using an independent statistician or having someone independent of the study group presenting the analysis. Although it would further add complexity, I wonder whether this idea of using an independent statistician to perform the needed simulations for an adaptive design would be useful. This could potentially serve two purposes. First, the simulations would have more credibility if performed by an external group. Second, the use of an external group would force the study team to have to explain the design to someone else. That would help to ensure that it is not just one statistician at the institution or the organization doing the study who understands the design. This would obviously have huge cost and infrastructure implications, but would at least ensure that multiple people understood the design of the study – and would avoid the issues that Dr Wittes mentioned.</p>
<p><bold>Stephen George (Duke University)</bold>: It is often stated that the DSMB rules we come up with are guidelines but the full implication of that is seldom thought through. If there are really going to be firm rules, you don’t need a DSMB. If you are convinced ahead of time that all you need to do is decide based on whatever outcome you are looking at the time to increase or decrease the sample size, or take some other fully prespecified action, you don’t need a DSMB. You need a DSMB precisely because the situation in a clinical trial is more complicated than the statistical decision rules imply. Even with the risk of violating a rule because of ‘extenuating’ circumstances and causing the statistician all kinds of trouble, the primary obligation of a DSMB is to the patients on the study. We as statisticians design studies to control error rates very carefully, and this is very important, but we need to be aware that there are situations in which broader issues and concerns can override some of our carefully crafted rules.</p>
<p><bold>Jonas H. Ellenberg (University of Pennsylvania)</bold>: Going back to Dr Neaton’s point about the virtues of using a hard end point in the measure of adherence, I wonder if you could comment on the fact that in warfarin trials, INR is an imperfect measure of adherence. The variability in INR is huge, to some degree related to adherence. Missing a pill has a nontrivial impact on INR level. In addition, if you miss three pills in a row, that is a lot different than missing every other pill three times. How does a sensitive relationship between pill-taking and INR outcome play into the results that you are seeing?</p>
<p><bold>Andrea Troxel (University of Pennsylvania)</bold>: This is a difficult but interesting area. Adherence is a step on the path to better health; we are not concerned if patients are not adherent as long as their health outcomes are good. For example, if a patient can keep his or her INRs in range and doesn’t experience a stroke in five years, then it seems less important that he or she takes his or her medication. But obviously, these outcomes are correlated, and adherence is something that can be targeted in the near term. It is certainly the case that the pattern of adherence is important. If a patient is 80% adherent, and doesn’t go for long stretches with no medication, that may be sufficient to maintain good health; on the other hand, being largely compliant in some periods and then largely noncompliant in other periods may have detrimental effects on health status. Incorporating the pattern of nonadherence is something we can do at the analytic stage. An advantage of the electronic pillbox systems is that we receive daily adherence data. We can learn a lot by not only analyzing the repeated INRs, but also by relating those outcomes to the daily adherence information. We can look for patterns that will predict outcomes differently; similar patterns of mostly adherent days with occasional lapses may have one outcome profile, while patterns involving long periods of nonadherence may predict a different outcome profile. These analyses are complex due to questions of endogeneity and feedback between the adherence data and the outcome data on subsequent assessments, and we are working on development of statistical approaches to handle these features.</p>
</body>
<back>
<fn-group>
<fn fn-type="other">
<label>Participants</label>
<p><bold>Kay Dickersin</bold>: Johns Hopkins University</p>
<p><bold>George Williams</bold>: Amgen, Inc.</p>
<p><bold>Stephen George</bold>: Duke University</p>
<p><bold>Chris Coffey</bold>: University of Iowa</p>
<p><bold>Xiaoxi Zhang</bold>: Pfizer</p>
<p><bold>Andrea Troxel</bold>: University of Pennsylvania</p>
<p><bold>Stephen Kimmel</bold>: University of Pennsylvania</p>
<p><bold>Benjamin French</bold>: University of Pennsylvania</p>
<p><bold>James Neaton</bold>: University of Minnesota</p>
<p><bold>James Fisch</bold>: New York University</p>
<p><bold>Barbara Tilley</bold>: University of Texas</p>
<p><bold>Marc Buyse</bold>: International Drug Development Institute (IDDI), Belgium</p>
<p><bold>Janet Wittes</bold>: Statistics Collaborative</p>
<p><bold>Elizabeth Kumm</bold>: i3Statprobe</p>
<p><bold>Bob Zhong</bold>: Johnson and Johnson</p>
<p><bold>Jonas H Ellenberg</bold>: University of Pennsylvania</p></fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1740774512463179">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>DeMets</surname><given-names>D</given-names></name>
<name><surname>Califf</surname><given-names>R</given-names></name>
</person-group>. <article-title>A historical perspective on clinical trials innovation and leadership: Where have the academics gone?</article-title> <source>JAMA</source> <year>2012</year>; <volume>305</volume>: <fpage>713</fpage>–<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr2-1740774512463179">
<label>2.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Quinlan</surname><given-names>J</given-names></name>
<name><surname>Gaydos</surname><given-names>B</given-names></name>
<name><surname>Maca</surname><given-names>J</given-names></name>
<name><surname>Krams</surname><given-names>M</given-names></name>
</person-group>. <article-title>Barriers and opportunities for implementation of adaptive designs in pharmaceutical product development</article-title>. <source>Clin Trials</source> <year>2010</year>; <volume>7</volume>(<issue>2</issue>): <fpage>167</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr3-1740774512463179">
<label>3.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bagiella</surname><given-names>E</given-names></name>
<name><surname>Heitjan</surname><given-names>DF</given-names></name>
</person-group>. <article-title>Predicting analysis times in randomized clinical trials</article-title>. <source>Stat Med</source> <year>2001</year>; <volume>20</volume>: <fpage>2055</fpage>–<lpage>63</lpage>.</citation>
</ref>
<ref id="bibr4-1740774512463179">
<label>4.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ying</surname><given-names>GS</given-names></name>
<name><surname>Heitjan</surname><given-names>DF</given-names></name>
<name><surname>Chen</surname><given-names>TT</given-names></name>
</person-group>. <article-title>Nonparametric prediction of event times in randomized clinical trials</article-title>. <source>Clin Trials</source> <year>2004</year>; <volume>1</volume>: <fpage>352</fpage>–<lpage>61</lpage>.</citation>
</ref>
<ref id="bibr5-1740774512463179">
<label>5.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Donovan</surname><given-names>JM</given-names></name>
<name><surname>Elliott</surname><given-names>MR</given-names></name>
<name><surname>Heitjan</surname><given-names>DF</given-names></name>
</person-group>. <article-title>Predicting event times in clinical trials when treatment arm is masked</article-title>. <source>J Biopharm Stat</source> <year>2006</year>; <volume>16</volume>: <fpage>343</fpage>–<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr6-1740774512463179">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ying</surname><given-names>GS</given-names></name>
<name><surname>Heitjan</surname><given-names>DF</given-names></name>
</person-group>. <article-title>Weibull prediction of event times in clinical trials</article-title>. <source>Pharm Stat</source> <year>2008</year>; <volume>7</volume>: <fpage>107</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr7-1740774512463179">
<label>7.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X</given-names></name>
<name><surname>Long</surname><given-names>Q</given-names></name>
</person-group>. <article-title>Joint monitoring and prediction of accrual and event times in clinical trials</article-title>. <source>Biom J</source> <year>2012</year>; <volume>54</volume>: <fpage>735</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr8-1740774512463179">
<label>8.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tsiatis</surname><given-names>AA</given-names></name>
<name><surname>Mehta</surname><given-names>CR</given-names></name>
</person-group>. <article-title>On the inefficiency of the adaptive design for monitoring clinical trials</article-title>. <source>Biometrika</source> <year>2003</year>; <volume>90</volume>: <fpage>367</fpage>–<lpage>78</lpage>.</citation>
</ref></ref-list>
</back>
</article>