<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342012462751</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342012462751</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Regular Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Progress towards accelerating HOMME on hybrid multi-core systems</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Carpenter</surname>
<given-names>I.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012462751">1</xref>
<xref ref-type="corresp" rid="corresp1-1094342012462751"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Archibald</surname>
<given-names>R.K.</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342012462751">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Evans</surname>
<given-names>K.J.</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342012462751">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Larkin</surname>
<given-names>J.</given-names>
</name>
<xref ref-type="aff" rid="aff3-1094342012462751">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Micikevicius</surname>
<given-names>P.</given-names>
</name>
<xref ref-type="aff" rid="aff4-1094342012462751">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Norman</surname>
<given-names>M.</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342012462751">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rosinski</surname>
<given-names>J.</given-names>
</name>
<xref ref-type="aff" rid="aff5-1094342012462751">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schwarzmeier</surname>
<given-names>J.</given-names>
</name>
<xref ref-type="aff" rid="aff6-1094342012462751">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Taylor</surname>
<given-names>M.A.</given-names>
</name>
<xref ref-type="aff" rid="aff7-1094342012462751">7</xref>
</contrib>
<bio>
<title>Author biographies</title>
<p>
<italic>I. Carpenter </italic>received her Ph.D. from the University of Wisconsin, Madison in 1987. After post-docs at the University of California, Irvine and University of Minnesota, she joined Cray Research in 1992 as a computational chemist. She became technical lead for weather and climate applications at SGI in 2001, held the same position at Cray Inc. from 2003 to 2004 and then returned to SGI, where she managed the scientific applications and benchmarking group. She left SGI in 2010 to work at the National Center for Supercomputing Applications at Oak Ridge National Laboratory and is currently in the computational sciences group at the National Renewable Energy Laboratory.</p>
<p>
<italic>R.K. Archibald </italic>has been staff scientist at Oak Ridge National Laboratory for the past 5 years where he is currently a member of the Climate Change Science Institute. He received a B.Sc. degree with honors in physics and M.Sc. in applied mathematics from University of Alberta. He received a Ph.D. degree in mathematics from Arizona State University.</p>
<p>
<italic>K.J. Evans </italic>is a research staff member in the Computational Earth Sciences Group at Oak Ridge National Laboratory. She received her Ph.D. from Georgia Institute of Technology in 2000 and completed a post-doc at Los Alamos National Laboratory studying implicit numerical methods for phase change and fluid flows from 2004 to 2006. Her work includes high-resolution coupled climate model development and scalable numerical methods development in global climate models. She is a member of the American Meteorological Society, American Geophysical Union, and the Society for Applied and Industrial Mathematics.</p>
<p>
<italic>J. Larkin</italic> is a member of the Cray Supercomputing Center of Excellence, located at Oak Ridge National Laboratory, where he has worked since 2005. He has a B.Sc. degree in Computer Science from Furman University and a Master of Science degree in Computer Science from the University of Tennessee. He specializes in performance analysis and optimization of high-performance computing applications. He was a recipient of the Gordon Bell Prize in 2008 and 2009. He lives in Knoxville, TN with his wife, Carla, and son, Owen.</p>
<p>
<italic>P. Micikevicius</italic> is a Developer Technology Engineer at NVIDIA with a focus on parallel computing, specifically algorithms and optimization. Prior to joining NVIDIA, he was an assistant professor of Computer Science at Armstrong Atlantic State University as well as a research associate at the Media Convergence Laboratory at UCF. Paulius holds a Ph.D. in Computer Science from University of Central Florida and a B.S. in Computer Science from Midwestern State University. </p>
<p>
<italic>M. Norman</italic> is a Computational Climate Scientist with the Scientific Computing Group in the National Center for Computational Sciences. Attending North Carolina State University for both undergraduate and graduate work, Matt received a BS in Computer Science, a BS in Meteorology, and a minor in Mathematics, working on projects ranging from stability analysis of semi-implicit semi-Lagrangian schemes to data mining and analysis. For the MS in Atmospheric Sciences, the thesis title was “Investigation of Higher-Order Accuracy for a Conservative Semi-Lagrangian Discretization of the Atmospheric Dynamical Equations”. For the PhD in Atmospheric Sciences, the dissertation title was “Characteristics-Based Methods for Efficient Parallel Integration of the Atmospheric Dynamical Equations”. Most recently, Matt<sup>1</sup>s focus has been on striving for GPU (graphics processing unit) efficiency in the atmospheric dynamical core from the level of the algorithm choices more so than extensive memory optimization. Matt has also been working to gain efficiency with minimal coding effort by ensuring efficient use of the per-multiprocessor L1 cache. He is working with the Oak Ridge Leadership Computing Facility 3 (OLCF3) readiness for Community Earth System Model/Community Atmosphere Model/HOMME (High-Order Method Modeling Environment) along with user and project assistance in NCCS. In addition, he will continue work in dynamical core algorithm development thereby complimenting the GPU porting work</p>
<p>
<italic>J. Rosinski </italic>is a senior software engineer at NOAA/ESRL in Boulder, CO. His focus is performance analysis and code optimization for high-performance computing applications with a specialty in atmospheric modeling. He has held various positions in the computing industry and national research labs. He holds an M.Sc. degree in Atmospheric Science from the University of Wisconsin-Madison (1983), and an M.Sc. degree in Computer Science from the University of Colorado (2001).</p>
<p>
<italic>J. Schwarzmeier </italic>is a Senior Principal Engineer at Cray Inc. His focus is on performance analysis and modeling, numerical algorithms, and compiler optimizations. Before coming to Cray Inc. he spent 6 years at LANL working on magnetic fusion. He holds BS and PhD degrees in physics from the University of Wisconsin–Madison.</p>
<p>
<italic>M.A. Taylor </italic>received his Ph.D. from New York University’s Courant Institute of Mathematical Sciences in 1992. He worked as a post-doc and then a software engineer at the NCAR from 1992 to 1998. From 1998 to 2004, he was a staff member in the Computer and Computational Sciences Division at Los Alamos National Laboratory. Since 2004, he has been a Principal Member of the Technical Staff at Sandia National Laboratories, in the Computation, Computers, Information and Mathematics Center. He works on scalable numerical methods for climate modeling.</p>
</bio>
</contrib-group>
<aff id="aff1-1094342012462751">
<label>1</label>National Renewable Energy Laboratory, Golden, CO, USA</aff>
<aff id="aff2-1094342012462751">
<label>2</label>Oak Ridge National Laboratory, Oak Ridge, TN, USA</aff>
<aff id="aff3-1094342012462751">
<label>3</label>Cray Inc., Oak Ridge, TN, USA</aff>
<aff id="aff4-1094342012462751">
<label>4</label>NVIDIA, Santa Clara, CA, USA</aff>
<aff id="aff5-1094342012462751">
<label>5</label>National Oceanic and Atmospheric Administration, Boulder, CO, USA</aff>
<aff id="aff6-1094342012462751">
<label>6</label>Cray Inc., Chippewa Falls, WI, USA</aff>
<aff id="aff7-1094342012462751">
<label>7</label>Sandia National Laboratories, Albuquerque, NM, USA</aff>
<author-notes>
<corresp id="corresp1-1094342012462751">I. Carpenter, Computational Sciences Center, National Renewable Energy Laboratory, 1617 Cole Boulevard, MS 1622, Golden, CO 80401, USA. Email: <email>Ilene.Carpenter@nrel.gov</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2013</year>
</pub-date>
<volume>27</volume>
<issue>3</issue>
<issue-title>Special Issue section on CCDSC 2012 Workshop</issue-title>
<fpage>335</fpage>
<lpage>347</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The suitability of a spectral element based dynamical core (HOMME) within the Community Atmospheric Model (CAM) for GPU-based architectures is examined and initial performance results are reported. This work was done within a project to enable CAM to run at high resolution on next-generation, multi-petaflop systems. The dynamical core is the present focus because it dominates the performance profile of our target problem. HOMME enjoys good scalability due to its underlying cubed-sphere mesh with full two-dimensional decomposition and the localization of all computational work within each element. The thread blocking and code changes that allow HOMME to effectively use GPUs are described along with a rewritten vertical remapping scheme, which improves performance on both CPUs and GPUs. Validation of results in the full HOMME model is also described. We demonstrate that the most expensive kernel in the model executes more than three times faster on the GPU than the CPU. These improvements are expected to provide improved efficiency when incorporated into the full model that has been configured for the target problem. Remaining issues affecting performance include optimizing the boundary exchanges for the case of multiple spectral elements being computed on the GPU.</p>
</abstract>
<kwd-group>
<kwd>CAM</kwd>
<kwd>HOMME</kwd>
<kwd>GPU</kwd>
<kwd>scalability</kwd>
<kwd>tracer</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342012462751">
<title>1. Introduction</title>
<p>The Community Earth System Model (CESM) is a global climate model that simulates the Earth system with five fully coupled components: atmosphere, ocean, land surface, sea-ice and land-ice (<xref ref-type="bibr" rid="bibr3-1094342012462751">Gent et al., 2011</xref>). The Community Atmosphere Model (CAM) component (<xref ref-type="bibr" rid="bibr11-1094342012462751">Neale et al., 2010</xref>) within CESM supports a spectral element based dynamical core option, from HOMME, the High-Order Method Modeling Environment (<xref ref-type="bibr" rid="bibr2-1094342012462751">Dennis et al., 2005</xref>, <xref ref-type="bibr" rid="bibr1-1094342012462751">2011</xref>). A key motivation for the integration of HOMME into CAM was to improve CAM’s parallel scalability to enable efficient global simulations on over 100,000 processors (<xref ref-type="bibr" rid="bibr17-1094342012462751">Taylor et al., 2008</xref>). CESM is a community model, which is run in various configurations on a wide variety of computer platforms. High-resolution simulations are done on leadership class systems, which will evolve to include hybrid multi-core technologies. This paper describes initial work done to enable climate simulations with CESM to run on systems with NVIDIA graphical processing units (GPUs), which represents one of the architectures that will be available to climate modelers for running high-resolution simulations.</p>
<p>HOMME uses a cubed sphere grid as shown in <xref ref-type="fig" rid="fig1-1094342012462751">Figure 1</xref>, which allows for a more spatially uniform discretization than the latitude-longitude grids used in CAM’s finite volume dynamical core. The MPI parallel decomposition in HOMME is across spectral elements. In the standard central processing unit (CPU) version of HOMME, computations within each element are handled serially. The excellent scalability of HOMME on the largest computing platforms was a key factor driving the choice of HOMME for initial porting work to GPU-based architectures. The element-wise decomposition in HOMME and minimal communication across elements has enabled scalability to be demonstrated to over 100,000 processors at a horizontal resolution of approximately 1/8th degree (<xref ref-type="bibr" rid="bibr1-1094342012462751">Dennis et al., 2011</xref>). This resolution has an average equatorial grid spacing (based on the total number of degrees of freedom) of about 13 km.</p>
<fig id="fig1-1094342012462751" position="float">
<label>Figure 1.</label>
<caption>
<p>Cubed-sphere spectral discretization showing the element layout for and 8 x 8 x 6 element discretization. Within each element, additional spectral discretization occurs.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig1.tif"/>
</fig>
<p>Power consumption is becoming a key design constraint of high-performance computing (HPC) systems targeted for multi-petaflop and exaflop levels of performance. Originally designed for high computational intensity algorithms common in graphics processing, GPUs are highly parallel commodity computing devices that provide high compute performance per unit of power consumption. As such, GPUs offer promise for future exascale computing systems. End users will see power efficiency in terms of sustained performance of the application on the GPU per Watt consumed by the GPU. If an application is not well suited to GPUs, sustained performance per Watt of a GPU may not be advantageous over a CPU.</p>
<p>Individual processor cores in GPUs are optimized to provide high levels of performance for large amounts of data streaming through a memory hierarchy in single instruction multiple thread (SIMT) mode. Broadcasting the same instruction to many threads greatly reduces power consumption compared with individual instruction fetch and decode for each thread. Optimal performance on GPUs requires abundant low-level SIMT parallelism in addition to the high-level MPI and OpenMP parallelism needed to take advantage of current petascale systems without GPUs. Exposing all of these levels of parallelism will improve performance on future GPUs, other types of accelerators and many-core sockets. In addition to high computational rate, GPUs offer significant memory bandwidth advantage over contemporary CPUs. For example, an Opteron Istanbul socket in the Oak Ridge Leadership Computing Facility’s Jaguarpf system has peak memory bandwidth of 12.8 GB/s, whereas the NVIDIA Fermi C2050 socket has peak GDDR5 bandwidth of 115 GB/s (the peak is 144 GB/s before Error Correction Code is accounted for). Presently, we describe the methods used to expose this parallelism within HOMME using the PGI Compute Unified Device Architecture (CUDA) Fortran compiler, the performance achieved with different parallelization strategies and the next steps needed to efficiently run HOMME on a system with GPUs.</p>
<p>Previous work porting portions of atmospheric models to GPUs includes work on the shortwave radiation parameterization in CAM (<xref ref-type="bibr" rid="bibr6-1094342012462751">Kelly, 2010</xref>), the WSM5 microphysics scheme (<xref ref-type="bibr" rid="bibr9-1094342012462751">Michalakes and Vahharajani, 2008</xref>) and a computationally expensive chemical kinetics module within the Weather Research and Forecasting model (WRF) (<xref ref-type="bibr" rid="bibr8-1094342012462751">Linford et al., 2009</xref>). The dynamical core from the NIM weather forecast model has also been ported to run on NVIDIA GPUs (<xref ref-type="bibr" rid="bibr4-1094342012462751">Govett et al., 2010</xref>). These porting efforts either required substantial work to go from Fortran to CUDA C or the development of directive-based tools (<xref ref-type="bibr" rid="bibr4-1094342012462751">Govett et al., 2010</xref>). Our work on HOMME uses CUDA Fortran via the PGI Fortran compiler. Like the directive-based approach, this enables one to minimize changes to the code and preserves the ability of Fortran programmers and scientists to read, understand and modify the code as new methods are developed.</p>
</sec>
<sec id="section2-1094342012462751">
<title>2. Programming for acceleration using GPUs</title>
<p>NVIDIA’s CUDA enables GPU programming without any graphics knowledge, using languages such as C and Fortran (<xref ref-type="bibr" rid="bibr12-1094342012462751">Nickolls et al., 2008</xref>). Code is written from the point of view of a single scalar thread, with built-in variables for thread ID and execution configuration. Three general requirements to maximize GPU performance, in decreasing order of importance, are sufficient parallelism, coherent memory access, and coherent flow control.</p>
<p>GPUs simultaneously execute many threads, hiding latency by quickly swapping stalled threads for threads that are ready to execute. In the CUDA programming model, threads are grouped into programmer-defined thread blocks, where threads within a thread block can synchronize and communicate via shared memory. The strategy employed for decomposing parallel work from HOMME into thread blocks is discussed in Section 4.</p>
<p>GPUs implement a SIMT execution model: one instruction is applied to a group of 32 consecutively numbered threads (also known as a <italic>warp</italic>). Whenever control flow diverges within a warp (e.g. different threads within the warp execute different branches of a conditional clause), execution time is the sum of the two branches. The vertical remapping portion of the tracer advection code is one of HOMME’s most time consuming portions and contains many conditionals. The rewritten algorithm, which improves the performance on both CPUs and GPUs, is described in Section 4.</p>
<p>Memory accesses are processed per warp and are converted into the minimum number of discrete cache-lines. Memory throughput is maximized when threads in a warp access memory in a coherent way: addresses fall within a contiguous memory region. This generally means that the fastest-varying dimension of thread IDs should match up with the fastest-varying dimension of the data being accessed. Work assignment to threads, as well as thread grouping into thread blocks are chosen to maximize memory throughput.</p>
<p>This study used the C2050, a HPC-oriented GPU based on Fermi architecture, which is NVIDIA's most recent GPU architecture (<xref ref-type="bibr" rid="bibr13-1094342012462751">NVIDIA, 2009</xref>). The C2050 consists of 14 streaming multiprocessors (SMs), 768 kB of L2 cache, and 3 GB of GDDR5 global memory. Each SM operates at 1.15 GHz and contains 64 kB of memory that is configured into shared memory and L1 cache. Further details on Fermi architecture and code optimization can be found in <xref ref-type="bibr" rid="bibr14-1094342012462751">NVIDIA (2010</xref>) and <xref ref-type="bibr" rid="bibr10-1094342012462751">Micikevicius (2010</xref>). CUDA Fortran (PGI Fortran 10.8 compiler) and CUDA 3.1 were used for this work.</p>
</sec>
<sec id="section3-1094342012462751">
<title>3. Overview of the HOMME dynamical core</title>
<p>HOMME uses the spectral element method to discretize the horizontal (surface of the sphere) directions, coupled with the <xref ref-type="bibr" rid="bibr16-1094342012462751">Simmons and Burridge (1981</xref>) finite difference approximation in the vertical (radial) directions. The spectral element method is a continuous Galerkin finite-element method, where the integrals used in the Galerkin formulation are computed via a Gauss–Lobatto quadrature rule within each element. Within each element, HOMME uses an <italic>nv</italic> × <italic>nv</italic> quadrature grid and represents the unknown variables in terms of polynomial expansions of degree <italic>nv</italic> – 1. For efficiency, these polynomial expansions are represented by their nodal values at the <italic>nv</italic> × <italic>nv</italic> quadrature points. We denote the number of vertical layers in the model by <italic>nlev</italic>, and thus each element contains an <italic>nv</italic> × <italic>nv</italic> × <italic>nlev</italic> block of data.</p>
<p>HOMME decomposes each time step into components involving dynamics, tracer advection, forcing and dissipation. The forcing terms are computed by the CAM physics (the suite of subgrid physical parameterizations in CAM). Dissipation refers to the horizontal viscosity and mixing that is explicitly added to the model. The equations can be written in terms of a vector <italic>U</italic> containing all of the prognostic state variables and right-hand side terms representing the forcing (<italic>F</italic>), dynamics and tracers (<italic>A</italic>) and dissipation (<italic>D</italic>),<disp-formula id="disp-formula1-1094342012462751">
<mml:math id="mml-disp1-1094342012462751">
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mi>U</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>d</mml:mi>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>F</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi>A</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi>D</mml:mi>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula1-1094342012462751" xlink:href="10.1177_1094342012462751-eq1.tif"/>
</disp-formula>
</p>
<p>HOMME solves this equation in a time-split fully explicit form. For simplicity we illustrate this using the forward-Euler method to advance the solution from time <italic>t </italic>to time <italic>t </italic>+ 1:<disp-formula id="disp-formula2-1094342012462751">
<mml:math id="mml-disp2-1094342012462751">
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
<mml:mo stretchy="false">=</mml:mo>
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mi>t</mml:mi>
</mml:msup>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow>
</mml:mrow>
</mml:mrow>
<mml:mi>F</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mi>t</mml:mi>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula2-1094342012462751" xlink:href="10.1177_1094342012462751-eq2.tif"/>
</disp-formula>
<disp-formula id="disp-formula3-1094342012462751">
<mml:math id="mml-disp3-1094342012462751">
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">∗</mml:mo>
<mml:mo stretchy="false">∗</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">=</mml:mo>
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow>
</mml:mrow>
</mml:mrow>
<mml:mi>F</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula3-1094342012462751" xlink:href="10.1177_1094342012462751-eq3.tif"/>
</disp-formula>
<disp-formula id="disp-formula4-1094342012462751">
<mml:math id="mml-disp4-1094342012462751">
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mrow>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">=</mml:mo>
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">∗</mml:mo>
<mml:mo stretchy="false">∗</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow>
</mml:mrow>
</mml:mrow>
<mml:mi>F</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msup>
<mml:mi>U</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">∗</mml:mo>
<mml:mo stretchy="false">∗</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula4-1094342012462751" xlink:href="10.1177_1094342012462751-eq4.tif"/>
</disp-formula>
</p>
<p>For the steps involving forcing <italic>F</italic> and dissipation <italic>D</italic>, the equations are solved exactly as written above. The dissipation term is a hyper-viscosity operator that is computed by the <monospace>laplace_sphere_wk</monospace> subroutine. It is written in terms of divergence and gradient operators, which are computed by the <monospace>divergence_sphere_wk</monospace> and <monospace>gradient_sphere</monospace> subroutines, respectively. For the dynamics and tracer advection, represented by <italic>A</italic>, HOMME does not use the forward-Euler approach shown above, but instead use a family of <italic>N</italic>-stage Runge–Kutta methods which can be written as convex combinations of forward-Euler steps, each of which is implemented in the <monospace>euler_step</monospace> subroutine.</p>
<p>Owing to the nature of the spectral element method, each Euler step can be broken into two stages. The first stage evaluates all the terms needed by <italic>A </italic>using the polynomial expansions within each spectral element. This stage is completely local to the element and requires no inter-element communication. The second stage is to apply the finite-element inverse mass matrix (also referred to as direct stiffness summation or assembly) that comes from the Galerkin formulation of equations. In the spectral element method, the mass matrix is diagonal, so this step is also explicit, but it does require communication with adjacent elements. This is the only stage that involves communication, and is represented by buffer packing routines <monospace>edgepack</monospace>, <monospace>edgeunpack</monospace> and the communication subroutine <monospace>bndy_exchange</monospace>.</p>
<p>For the components of <italic>U</italic> representing velocity, temperature and surface pressure, <italic>A</italic> represents the atmospheric primitive equations. These equations are the compressible Euler equations in a rotating reference frame with the hydrostatic and shallow atmosphere approximations. For the tracer components of <italic>U</italic> (such as specific humidity, liquid water and ice, as well as additional quantities depending on the CAM configuration), <italic>A</italic> represents linear advection. For advection, HOMME uses the vertically Lagrangian approach from <xref ref-type="bibr" rid="bibr7-1094342012462751">Lin (2004</xref>). This decouples the computations into a horizontal (two-dimensional) advection step on floating Lagrangian vertical levels (computed in the <monospace>euler_step</monospace> subroutine), followed by a vertical remap step back to the reference vertical levels (implemented in the <monospace>remap_Q_kern</monospace> subroutine).</p>
</sec>
<sec id="section4-1094342012462751">
<title>4. Target problem and its performance profile</title>
<p>To resolve important sub-global features in the Earth system, climate modelers would like to be able to run models with high spatial resolution in conjunction with full atmospheric chemistry (100+ chemical constituents). Unfortunately, this is too computationally expensive to be practical today for production-length climate simulations. Thus, we select the combination of high spatial resolution (1/8th degree HOMME) and Mozart atmospheric chemistry (101 advected constituents) as a target problem for future multi-petaflop system and evaluate the benefit of using GPUs, which will be present in future leadership class computer systems.</p>
<p>Active chemistry in CAM on non-latitude/longitude grids (such as those used by HOMME) was still under development at the time this work began (work was needed to remove assumptions about the grid type from the legacy chemistry model), so a performance model for the target problem was created by running a set of jobs that enabled us to probe the performance of different parts of the target problem (dynamics, physics, and chemistry). This performance model was then used to generate a profile of the time spent in each routine for the target job. The performance model is based on profiles obtained on a Cray XT5 system for several jobs described below.</p>
<p>Three configurations of HOMME were run, each consisting of 345,600 elements, 26 vertical levels (<italic>nlev</italic>) and a 4 x 4 tensor product (<italic>nv</italic>) Gauss–Lobatto quadrature of grid points within each element. The number of elements selected corresponds to a 1/8th degree resolution HOMME configuration. The first configuration is the version of CAM using the HOMME dynamical core at high resolution as included in the CESM1, which employs three advected constituents. Also, the Held–Suarez problem (<xref ref-type="bibr" rid="bibr5-1094342012462751">Held et al., 1994</xref>) was run from a standalone version of HOMME at the same resolution, with both 3 and 101 advected constituents, 101 being the number of constituents that would be present with the Mozart atmospheric chemistry package.</p>
<p>It was considered whether larger <italic>nv</italic> in place of larger ne would improve GPU efficiency due to improved compute density for the matrix–vector multiply kernel of SE. However, tracer transport runtime is dominated by other operations that do not enjoy better compute density: packing and unpacking element edges into and out of process-wide buffers, MPI communication, and applying metric terms. Thus, sustained Gflops do not change appreciably on CPUs or GPUs. Also, the time step scales as roughly <inline-formula id="inline-formula1-1094342012462751">
<mml:math id="mml-inline1-1094342012462751">
<mml:mi>n</mml:mi>
<mml:msup>
<mml:mi>v</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1.7</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mi>n</mml:mi>
<mml:msup>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, and computational complexity grows as <inline-formula id="inline-formula2-1094342012462751">
<mml:math id="mml-inline2-1094342012462751">
<mml:mi>n</mml:mi>
<mml:msup>
<mml:mi>v</mml:mi>
<mml:mn>3</mml:mn>
</mml:msup>
<mml:mi>n</mml:mi>
<mml:msup>
<mml:mi>e</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>. With no appreciable increase in compute density, smaller time steps and larger costs per time step associated with larger <italic>nv</italic> cannot be compensated by improved GPU efficiency.</p>
<p>Also, because the Runge–Kutta time stepping is third-order accurate, temporal error dominates for <inline-formula id="inline-formula3-1094342012462751">
<mml:math id="mml-inline3-1094342012462751">
<mml:mi>n</mml:mi>
<mml:mi>v</mml:mi>
<mml:mo stretchy="false">≥</mml:mo>
<mml:mn>4</mml:mn>
</mml:math>
</inline-formula>. Error responds only to the decreased time step, meaning convergence with larger <italic>nv</italic> is not exponential but polynomial as with larger <italic>ne</italic>. For non-smooth flows, it is known that increased <italic>nv</italic> leads to larger Runge oscillations, which typically decreases accuracy. Therefore, regarding accuracy, runtime, and computational density, there is no net benefit in increasing <italic>nv</italic> instead of <italic>ne</italic> in HOMME using currently implemented time-stepping methods.</p>
<p>These three HOMME runs defined above allowed estimation of the time in the dynamics and physics subroutines for the target problem. To investigate the cost of the chemistry portion of the target problem, a 1/4th degree configuration of the finite volume version of CAM was run with and without Mozart chemistry. From this run one can determine the cost of the chemistry subroutines per column of grid points for each time step and estimate the time for this portion of the code for the higher-resolution problem.</p>
<p>The resulting profile (<xref ref-type="fig" rid="fig2-1094342012462751">Figure 2</xref>) shows all routines that account for greater than 1% of the projected runtime. The bars have been grouped to reflect the kernels that have been investigated or written as a part of this exercise. Routines in the HOMME dynamical core dominate the runtime and tracer advection (the bars grouped as vertical remap, Euler step and Laplace Sphere Weak in <xref ref-type="fig" rid="fig2-1094342012462751">Figure 2</xref>) dominates the dynamics because its cost increases linearly with the number of advected constituents. This number increases from 3 to 101 when atmospheric chemistry is included. The aggregate and individual time spent in the physical parameterization schemes is negligible (less than 1% of the projected runtime) so none is shown in <xref ref-type="fig" rid="fig2-1094342012462751">Figure 2</xref>. The time spent on atmospheric chemistry is dominated by the implicit solver within it, with other aspects of the Mozart chemistry package shown as ‘chemistry (outside of imp_sol)’ in <xref ref-type="fig" rid="fig2-1094342012462751">Figure 2</xref>.</p>
<fig id="fig2-1094342012462751" position="float">
<label>Figure 2.</label>
<caption>
<p>Performance model profile of target problem. This profile was generated for a case run on 4608 cores (75 elements per core).</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig2.tif"/>
</fig>
<p>The vertical remapping code within the advection has been optimized for the CPU as a part of this work. The current implementation is projected to be roughly 34% of the application runtime for the target problem. The <monospace>euler_step</monospace> and <monospace>laplace_sphere_wk</monospace> kernels are expected to account for more than 25% of the total runtime. They operate on very similar loop nests and data structures, which made extracting these kernels and modifying them for execution on a GPU fairly straightforward. More details about these kernels are presented in Section 5.</p>
<p>The MPI communication time is negligible, however the time spent packing and unpacking the buffers used in the boundary exchanges increases linearly with the number of advected constituents. Currently, these routines remain on the CPU.</p>
</sec>
<sec id="section5-1094342012462751">
<title>5. Tracer advection on GPUs</title>
<p>For HOMME’s tracer advection, the best performance from the multi-core CPU and latest generation GPU are presented by comparing the performance of standalone kernels for fixed problem sizes using all cores on the Opteron 2435 (Istanbul family) six-core CPU socket or the C2050 GPU. Performance rates for the GPU are reported including and excluding data transfer time. For all runs we use <italic>nv </italic>= 4, <italic>nlev </italic>= 26 and <italic>qsize </italic>= 101, where <italic>qsize</italic> equals the number of advected constituents. In our standalone kernels, the total number of elements computed varies from 6 to 384. One socket with all six cores was used for the CPU runs. For the GPU runs, we use one core to launch the GPU kernel. The kernels have been incorporated into the standalone HOMME code for validation and to enable continued refinement of profiles as portions of the calculation are accelerated. <xref ref-type="fig" rid="fig3-1094342012462751">Figure 3</xref> displays the calltree for the kernels that we accelerated in this study for our target problem.</p>
<fig id="fig3-1094342012462751" position="float">
<label>Figure 3.</label>
<caption>
<p>Calltree that terminates at the three kernels accelerated in this study.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig3.tif"/>
</fig>
<sec id="section6-1094342012462751">
<title>5.1. Euler step and Laplace sphere weak</title>
<p>As shown in Section 3, the time spent in the <monospace>euler_step</monospace> and <monospace>laplace_sphere_wk</monospace> routines (inclusively) is expected to account for more than 25% of the runtime on our problem of interest. These routines have similar loop nesting and data structures, which is illustrated by the pseudo-code in <xref ref-type="fig" rid="fig4-1094342012462751">Figure 4</xref>. We show that these loops provide sufficient parallelism for the GPU at the problem size of interest.</p>
<fig id="fig4-1094342012462751" position="float">
<label>Figure 4.</label>
<caption>
<p>Loop nesting pattern.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig4.tif"/>
</fig>
<p>Deciding how to decompose the computation among threads and how to group threads into thread blocks is key to parallelizing code for GPUs. For the both the <monospace>euler_step</monospace> and <monospace>laplace_sphere_wk</monospace> kernels, each advected constituent (<italic>q</italic>) was computed in a separate block with <monospace>nv x nv x nlev</monospace> threads per block, thus the target problem has 101 blocks, each with 4*4*26 threads. Expectations for future work include exploration of improvements to this thread blocking. In the future, some number of the advected constituents may be moved into the thread blocks to increase the thread count and some or all of the elements will be used as a part of the thread block grid.</p>
<p>The dominant data array for tracer advection is the array of tracers (<italic>Qdp</italic>) within each element derived type, which is dimensioned <monospace>nv x nv x nlev x qsize x timelevels</monospace>. Since Fortran arrays are column-major and the i and j indices are the innermost (<xref ref-type="fig" rid="fig4-1094342012462751">Figure 4</xref>), these dimensions result in a stride-1 access pattern on the CPU, making it both cache-friendly and amenable to Streaming SIMD Extensions (SSE) vectorization. Because derived-type support is limited on the GPU and not all time levels are needed for the GPU calculation, this data is condensed during transfer to the device into a device <italic>Qdp</italic> array dimensioned <monospace>nv x nv x nlev x qsize x nelem</monospace>. This device array compresses the non-contiguous <italic>Qdp</italic> array from every element into one contiguous array, and since the data for each <monospace>nv x nv x nlev</monospace> thread block is contiguous, on-GPU memory transactions are minimized with 128-byte, coalesced memory transactions. The few smaller arrays used within tracer advection are likewise dimensioned with <monospace>nv x nv</monospace> leading dimensions, providing desirable data layout for both the CPU and GPU.</p>
</sec>
<sec id="section7-1094342012462751">
<title>5.2. Moving parallelism down the calltree</title>
<p>In both <monospace>euler_step</monospace> and <monospace>laplace_sphere_wk</monospace> each thread calculates the <monospace>(i, j, k, q)</monospace>’th element of the appropriate array, where <monospace>i</monospace> and <monospace>j</monospace>, <monospace>k</monospace>, and <monospace>q</monospace> correspond to indices of <italic>nv</italic>, <italic>nlev</italic> and <italic>qsize</italic>, respectively. To achieve this it was necessary to move the <monospace>q</monospace> and <monospace>k</monospace> loops into the lowest level of the calltree. For example, where the original <monospace>laplace_sphere_wk</monospace> kernel is called <monospace>nelem x qsize x nlev</monospace> times, the new kernel is called only once per element (<monospace>nelem</monospace> times), and performs the calculations for all vertical levels and all advected constituents (<monospace>q</monospace>) in each call. This involved loop fission at the callsite, moving the <monospace>q</monospace> and <monospace>k</monospace> loops into the lowest-level functions, and increasing the dimensions of several temporary arrays. These changes degraded the CPU performance so it will be necessary to maintain both the original CPU code and the new GPU code to ensure that both use the best possible implementation.</p>
<p>Profiles showed significant time spent in low-level routines such as those that calculate gradient and divergence (<monospace>gradient_sphere</monospace> and <monospace>divergence_sphere</monospace>). These are used in <monospace>euler_step</monospace> and <monospace>laplace_sphere_wk</monospace> as well as in other parts of the code. Implementing these low-level routines on the GPU allows many higher-level routines to use the GPU with little or no change, however these low-level routines do too little work to offset the high cost of transferring data to and from the GPU. It is necessary to move higher up the call tree to increase the work done between data transfers over the PCIe bus, which connects the GPU to the CPU, and to allow temporary arrays used in the calculations to remain on the GPU. The low-level routines were easily modified to be device-only routines, which allows them to be used within the higher-level kernels.</p>
</sec>
<sec id="section8-1094342012462751">
<title>
<monospace>5.3. laplace_sphere_wk</monospace> details</title>
<p>The <monospace>laplace_sphere_wk</monospace> routine is a simple wrapper with one call to <monospace>divergence_sphere_wk</monospace> and one to <monospace>gradient_sphere</monospace>. Since <monospace>divergence_sphere</monospace> and <monospace>gradient_sphere</monospace> had previously been modified for use on the GPU, it was straightforward to build a simple routine that calls both of these kernels, leaving the intermediate data on the GPU between kernel invocations. This was relatively inefficient, as it involved launching two CUDA kernels per call to <monospace>laplace_sphere_wk</monospace>. This implementation was refined to make a <monospace>laplace_sphere_wk</monospace> CUDA kernel, which used two device-only kernels for the calculations. This basic implementation performed comparably to the original on one CPU core, but needed further optimizations. The first major optimization recognized that many of the data arrays are constant throughout the entire series of time steps of a HOMME experiment and therefore could be copied during initialization and left for the entire run. This significantly reduces the data transfer cost and allows several data structures to be shared with <monospace>euler_step</monospace>. In addition, the device-only kernels were modified to allow each thread to calculate only the single array element it needed and leave it in a register, as opposed to storing into a temporary array.</p>
<p>The code sample in <xref ref-type="fig" rid="fig5-1094342012462751">Figure 5</xref> shows relevant details of the current <monospace>laplace_sphere_wk</monospace> implementation. When compared with the original code, the divergence and gradient sphere kernels are essentially the bodies of the <monospace>j</monospace> and <monospace>i</monospace> loops. Several data structures were extended to accommodate the <monospace>k</monospace> and <monospace>q</monospace> loops, which were originally outside of <monospace>laplace_sphere_wk</monospace>. The only GPU-specific changes to this code are the calculation of indices, which replaces the original loops, and the caching of some arrays into the faster, shared memory.</p>
<fig id="fig5-1094342012462751" position="float">
<label>Figure 5.</label>
<caption>
<p>
<monospace>laplace_sphere_wk</monospace> pseudo-code</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig5.tif"/>
</fig>
<p>The graph in <xref ref-type="fig" rid="fig6-1094342012462751">Figure 6</xref> shows computational rate in Gflops/s for the current best implementation of <monospace>laplace_sphere_wk</monospace>, comparing the runtime using all six CPU cores to the runtime on one GPU and varying the total number of elements computed in the standalone kernel from 6 to 384. Each line represents the mean rate for 25 executions. Since the data required for laplace_sphere_wk is a subset of the data required by euler_step, no additional data needs to be copied to the device, so data transfer costs are ignored for this figure.</p>
<fig id="fig6-1094342012462751" position="float">
<label>Figure 6.</label>
<caption>
<p>Performance of <monospace>laplace_sphere_wk</monospace>.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig6.tif"/>
</fig>
</sec>
<sec id="section9-1094342012462751">
<title>
<monospace>5.4. euler_step</monospace> details</title>
<p>An <monospace>euler_step</monospace> CUDA kernel was created for the computational part of the routine, which could do both local calculations and call device-only routines for the lower-level calculations. Only the data needed for the boundary exchange, which involves MPI communication, is copied back to the CPU, which updates the necessary arrays on the next call to <monospace>euler_step</monospace>. Using asynchronous memory copies further improves data transfers by overlapping PCIe copies with kernel execution. As noted for <monospace>laplace_sphere_wk</monospace>, all but two arrays remain static throughout an entire HOMME simulation, so they can be copied before the first time step and left on the device for the remainder of the run.</p>
<p>The graph in <xref ref-type="fig" rid="fig7-1094342012462751">Figure 7</xref> shows the current status of <monospace>euler_step</monospace> performance, comparing the computational rate in Gflops/s using all six CPU cores of the Opteron Istanbul family processor to one C2050 GPU, varying the total number of elements from 6 to 384. <monospace>euler_step</monospace> has dramatically different performance characteristics from <monospace>laplace_sphere_wk</monospace> because it is much larger, both in terms of data transfer and computation. The large amount of data transferred emphasizes the importance of overlapping memory transfers with computation. Observe that the ‘GPU (sync)’ line, which represents the GPU kernel plus PCIe transfer time, never shows better performance than the ‘CPU’ line. If data transfer is not included (the GPU (kernel) line), the GPU computation is roughly six times faster than the CPU computation. When using asynchronous transfers, it becomes profitable to use a GPU for this kernel at around 100 elements per node and becomes more than 20% faster at 300 elements. It will be important to experiment with the number of elements per node when running the production code, as this will determine whether a GPU will be beneficial.</p>
<fig id="fig7-1094342012462751" position="float">
<label>Figure 7.</label>
<caption>
<p>Performance of <monospace>euler_step</monospace>.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig7.tif"/>
</fig>
<p>Because <xref ref-type="fig" rid="fig7-1094342012462751">Figure 7</xref> is based on a standalone kernel, the cost of data transfer is over-estimated, giving a worst-case performance scenario. By performing the MPI buffer packing on the device, rather than copying the entire array back to the CPU for packing, the amount of data returned to the CPU will be reduced by roughly a factor of <inline-formula id="inline-formula4-1094342012462751">
<mml:math id="mml-inline4-1094342012462751">
<mml:mrow>
<mml:mfrac>
<mml:mn mathvariant="monospace">1</mml:mn>
<mml:mrow>
<mml:msqrt>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="monospace">n</mml:mi>
<mml:mi mathvariant="monospace">e</mml:mi>
<mml:mi mathvariant="monospace">l</mml:mi>
<mml:mi mathvariant="monospace">e</mml:mi>
<mml:mi mathvariant="monospace">m</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:msqrt>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
</inline-formula>, corresponding to only transferring exterior element boundaries of the decomposition. The cross-over point is expected to be reduced depending upon the specific layout of the element domain decomposition and the blocking transfer size. In addition, not all of the tracer data is required for the rest of the dynamics or the CAM physics steps, so much of the data may remain resident throughout the simulation. For this reason, the final implementation is expected to require far fewer than the 100 elements <xref ref-type="fig" rid="fig7-1094342012462751">Figure 7</xref> implies.</p>
</sec>
<sec id="section10-1094342012462751">
<title>5.5. Vertical remapping</title>
<p>The mass and momentum variables in HOMME are conservatively remapped at the end of each time step following <xref ref-type="bibr" rid="bibr18-1094342012462751">Zerroukat et al. (2005</xref>, <xref ref-type="bibr" rid="bibr19-1094342012462751">2006</xref>). This approach uses a quadratic spline reconstruction with monotonicity constraints. This remap is applied to each tracer, for each vertical column of data. Computation for each column is independent of other columns, which immediately offers parallelism for <monospace>nv x nv x qsize x nelem</monospace> threads, but due to data dependence within each column, no parallelism is available in the <italic>k</italic>-dimension. Thus, there is sufficient parallelism to utilize the GPU efficiently if several elements are processed concurrently within a single vertical remap kernel. Consequently, one thread is dedicated to each column, for a total of <monospace>nv x nv x qsize x nelem</monospace> threads launched for the kernel. Threads operating on the same advected constituent q are grouped into a thread block. As a result, each kernel launch has <monospace>qsize</monospace> thread blocks, and each thread block is <monospace>nv x nv x nelem</monospace> threads. The best performance was achieved with six elements per block, <italic>nelem </italic>= 6, which makes each thread block have 4*4*6 = 96 threads. Since the vertical remapping computes on the array of tracers (<italic>Qdp</italic>), which is dimensioned <italic>nv</italic> x <italic>nv</italic> x <italic>nlev</italic> x <italic>qsize</italic> x <italic>nelem</italic>, the thread block does not compute on a single, contiguous block of memory due to the lack of the level index (<italic>k</italic>). Thus the access pattern from 96 threads of a thread block referencing <italic>Qdp</italic> is 16 contiguous 8-byte words of memory, following by a jump in address of <italic>nv</italic>*<italic>nv</italic>*<italic>nlev</italic>*<italic>qsize</italic> (for the next value of <italic>ie</italic>), followed by another 16 contiguous words of memory, etc. While this not uniformly stride one within a thread block, it is composed of all stride-one DRAM accesses, as each DRAM access of the Fermi C2050 is 128 bytes. This means that each thread block accesses memory optimally for the GPU architecture.</p>
<p>The vertical remap code was modified from the original version by inlining subroutine calls to create a flat kernel. This facilitated the GPU port by exposing sufficient SIMT parallelism across vertical columns. It also improved the CPU performance by about a factor of two. The kernel consists of one large loop nest of the parallel indices of the problem, and a number of temporary arrays were eliminated in favor of register-carried values. The included code sample in <xref ref-type="fig" rid="fig8-1094342012462751">Figure 8</xref> shows the most important features of the current implementation.</p>
<fig id="fig8-1094342012462751" position="float">
<label>Figure 8.</label>
<caption>
<p>Vertical remap pseudo-code.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig8.tif"/>
</fig>
<p>
<xref ref-type="fig" rid="fig9-1094342012462751">Figure 9</xref> shows computational rate in Gflops/s for a standalone vertical remapping program. On the GPU, both data movement and computation was done on chunks of six elements at a time without asynchronous data transfers. For the GPU we report kernel execution rates. The current implementation transfers the input for a chunk of elements to GPU, launches computation kernel, then transfers the output from GPU. Data transfers to/from GPU take approximately the same amount of time as computation, thus, transfer time can be hidden by overlapping the two. Future work will include pipelining the chunks, where input for chunk (<italic>k </italic>+ 1) is being transferred to GPU at the same as output for chunk (<italic>k </italic>– 1) is being transferred from GPU and computation on chunk <italic>k</italic> is executing.</p>
<fig id="fig9-1094342012462751" position="float">
<label>Figure 9.</label>
<caption>
<p>Performance of vertical remapping.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig9.tif"/>
</fig>
</sec>
</sec>
<sec id="section11-1094342012462751">
<title>6. Validation</title>
<p>A critical aspect of any code porting exercise is to ensure that ported code is validated, with no bugs inadvertently introduced. Compiler and architecture differences cause differences in the results at the level of machine rounding on any finite precision machine. And since the atmosphere is a chaotic fluid system, these differences will grow to large values after a relatively small number of simulated days, which is the so-called ‘butterfly’ effect. Ensuring correct code is of particular concern in situations such as porting a large code to a GPU, where substantial differences in the underlying hardware and programming model demand significant changes to the application code.</p>
<p>In a full atmospheric General Circulation Model (GCM) such as CAM, the port validation procedure outlined by <xref ref-type="bibr" rid="bibr15-1094342012462751">Rosinski and Williamson (1997</xref>), hereafter referred to as RW, is applied to verify the correctness of the modified code. The procedure they describe provides a metric to ascertain whether the growth of a difference between trusted and test application software is within bounds, and therefore the modified code likely to be free of bugs. In Condition 2 described in RW, the growth of this difference must be similar in both magnitude and shape to the growth of an artificial rounding-level perturbation applied to one of the model’s prognostic variables by the code using the ‘trusted’ software on ‘trusted’ hardware.</p>
<p>All test cases provided with HOMME are adiabatic, and all GPU porting effort thus far has been focused on the tracer advection portion of the application. The adiabatic HOMME configuration employs dissipation with no external forcing such as radiation and clouds. An adiabatic configuration also implies that there is no feedback from changes in tracer concentration into the non-linear dynamics. These two facts mean that an imposed perturbation in standalone HOMME will not grow nearly as rapidly as in a full GCM. However, the code validation principle is exactly the same as with the full model for condition 2 of RW: If the growth of an initial perturbation imposed at the level of machine floating-point roundoff is no greater than the growth of a difference between trusted (CPU) code and modified (GPU) code, this is strong evidence of a valid port.</p>
<p>
<xref ref-type="fig" rid="fig10-1094342012462751">Figure 10</xref> shows the growth of a machine-rounding perturbation applied to the initial state of the first advected constituent on the trusted (CPU) machine (solid curve), and the growth of the difference between the first advected constituent using the original CPU code, and its GPU counterpart (dashed curve). In each case the magnitude of the difference begins to decay after its initial growth over about 10 days. The eventual decay is due to the fact that there is no external forcing (physics) present, so internal dissipation forces the final state to a zonally uniform structure. Condition 2 from RW is satisfied since both curves show similar magnitude and shape, with the perturbation curve actually revealing greater RMS differences than the CPU-GPU curve. This result provides solid evidence that no inadvertent bugs were introduced as part of the GPU port.</p>
<fig id="fig10-1094342012462751" position="float">
<label>Figure 10.</label>
<caption>
<p>Validation curves for adiabatic test case from HOMME. The solid curve is the difference between the unmodified code run on the CPU and the unmodified code run on the CPU with a machine rounding-level perturbation added to the initial constituent field for tracer 1. The dashed curve is the difference between the unmodified code run on the CPU and the modified code run on the GPU. In each case the quantity plotted is the RMS difference of tracer 1 between runs, without any mass weighting.</p>
</caption>
<graphic xlink:href="10.1177_1094342012462751-fig10.tif"/>
</fig>
<p>The final aspect of HOMME code port validation is to verify that long-term climate statistics produced from a GPU run match those from a CPU run. This will be possible once the GPU version of the HOMME dynamical core is integrated into the CAM, so that physical parameterizations are enabled and realistic initial and boundary condition forcing datasets are available.</p>
</sec>
<sec id="section12-1094342012462751">
<title>7. Conclusions</title>
<p>Suitability of the HOMME dynamical core for acceleration on GPU-based architectures has been examined using a target problem, the choice of which was driven by the near-tem needs of climate modelers to produce high-resolution simulations that include atmospheric chemistry. This problem is a 1/8th degree horizontal resolution configuration of HOMME with 101 advected constituents, whereby tracer advection is projected to be the most computationally expensive part of the simulation. The most time-consuming parts of the tracer advection calculation in HOMME have been extracted and the performance of standalone kernels on GPUs has been analyzed.</p>
<p>Tracer advection is shown to be amenable to acceleration on GPUs provided that each MPI task, which is associated with one GPU, is responsible for computation of several elements. The current implementation of the tracer advection kernels in HOMME does not take advantage of the potential for sharing data between the accelerated kernels or overlapping data transfers and computation. This data sharing will allow more data to remain resident on the device, reducing the high cost of transferring data between the host and device. Asynchronous data transfers and execution may help further reduce data transfer costs for data which cannot remain on the device. Opportunities remain in each kernel to optimize the thread blocking parameters for better GPU utilization. These kernels were then reincorporated into the full HOMME code for validation.</p>
<p>Work will continue on GPU acceleration of additional portions of the tracer advection. Since the majority of this portion of the calculation involves the same data structures and high level of parallelism, much of the tracer advection can be accelerated without significantly increasing the data transfer costs. An important part of this work will include rewriting boundary exchange code, including the packing and unpacking of edge buffers, so that boundary exchanges involving edges connecting elements computed on the same GPU may be done on the GPU. This will substantially reduce the amount of data that must be transferred between CPU and GPU memory.</p>
<p>After work on the HOMME dynamical core is complete, GPU acceleration of the implicit solver from the Mozart chemistry package needs to be addressed. Because this code is generated by a pre-processor, it is expected that modifications to the pre-processor will be made so that it will generate code tailored to the GPU. Although there is much work left to be done toward accelerating HOMME, the work presented above demonstrates that enough parallelism exists to make GPU acceleration of atmospheric dynamics feasible for certain classes of problems.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure" id="fn1-1094342012462751">
<label>Funding</label>
<p>MAT and KJE have been supported by the DOE BER SciDAC project, ‘A Scalable and Extensible Earth System’. This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract Number DE-AC05-00OR22725.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dennis</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Edwards</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Evans</surname>
<given-names>KJ</given-names>
</name>
<etal/>
</person-group>. (<year>2011</year>) <article-title>CAM-SE: A scalable spectral element dynamical core for the Community Atmosphere Model</article-title>. <source>Int J High Perf Comput Appl</source>, <comment>to appear</comment>.</citation>
</ref>
<ref id="bibr2-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dennis</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Fournier</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Spotz</surname>
<given-names>WF</given-names>
</name>
<etal/>
</person-group>. (<year>2005</year>) <article-title>High resolution mesh convergence properties and parallel efficiency of a spectral element dynamical core</article-title>. <source>Int J High Perf Comput Appl</source> <volume>19</volume>: <fpage>225</fpage>–<lpage>235</lpage>.</citation>
</ref>
<ref id="bibr3-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gent</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Danabasoglu</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Donner</surname>
<given-names>LJ</given-names>
</name>
<etal/>
</person-group>. (<year>2011</year>) <article-title>The Community Climate System Model version 4</article-title>. <source>J Climate</source>, <comment>in press</comment>.</citation>
</ref>
<ref id="bibr4-1094342012462751">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Govett</surname>
<given-names>MW</given-names>
</name>
<name>
<surname>Middlecoff</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Henderson</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Running the NIM next-generation model on GPUs</article-title>. In <source>CCGRID, Proceedings of the10th IEE/ACM International Conference on Cluster, Cloud and Grid Computing</source>, pp. <fpage>792</fpage>–<lpage>796</lpage>.</citation>
</ref>
<ref id="bibr5-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Held</surname>
<given-names>IM</given-names>
</name>
<name>
<surname>Suarez</surname>
<given-names>MJ</given-names>
</name>
</person-group> (<year>1994</year>) <article-title>A proposal for the intercomparison of the dynamical cores of atmospheric general circulation models</article-title>. <source>Bull Am Meteorol Soc</source> <volume>73</volume>: <fpage>1825</fpage>–<lpage>1830</lpage>.</citation>
</ref>
<ref id="bibr6-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kelly</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>GPU Computing for Atmospheric Modeling</article-title>. <source>Comput Sci Engng</source> <volume>12</volume>: <fpage>26</fpage>–<lpage>33</lpage>.</citation>
</ref>
<ref id="bibr7-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lin</surname>
<given-names>SJ</given-names>
</name>
</person-group> (<year>2004</year>) <article-title>A vertically Lagrangian finite-volume dynamical core for global models</article-title>. <source>Mon Wea Rev</source> <volume>132</volume>: <fpage>2293</fpage>–<lpage>2397</lpage>.</citation>
</ref>
<ref id="bibr8-1094342012462751">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Linford</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Michalakes</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Sandu</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Vachharajani</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>Multi-core acceleration of chemical kinetics for simulation and prediction</article-title>. In <source>Proceedings of the Conference on High Performance Computing, Networking, Storage and Analysis (SC’09)</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, <fpage>2009</fpage>.</citation>
</ref>
<ref id="bibr9-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Michalakes</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Vachharajani</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>GPU acceleration of numerical weather prediction</article-title>. <source>Parallel Process Lett</source> <volume>18</volume>: <fpage>531</fpage>–<lpage>548</lpage>.</citation>
</ref>
<ref id="bibr10-1094342012462751">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Micikevicius</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2010</year>) <comment>Fundamental Optimizations. High Performance Computing with CUDA, Tutorial S03, Supercomputing 2010, New Orleans. Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.nvidia.com/content/PDF/sc_2010/CUDA_Tutorial/SC10_Fundamental_Optimizations.pdf">http://www.nvidia.com/content/PDF/sc_2010/CUDA_Tutorial/SC10_Fundamental_Optimizations.pdf</ext-link>.</citation>
</ref>
<ref id="bibr11-1094342012462751">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Neale</surname>
<given-names>RB et al</given-names>
</name>
</person-group>. (<year>2010</year>) <article-title>Description of the NCAR Community Atmosphere Model CAM 4.0. NCAR Technical Note</article-title>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.cesm.ucar.edu/models/cesm1.0/cam/docs/description/cam5_desc.pdf">http://www.cesm.ucar.edu/models/cesm1.0/cam/docs/description/cam5_desc.pdf</ext-link> </citation>
</ref>
<ref id="bibr12-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nickolls</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Buck</surname>
<given-names>I</given-names>
</name>
<name>
<surname>Garland</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Skadron</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Scalable parallel programming with CUDA</article-title>. <source>ACM Queue</source> <volume>6</volume>(<issue>2</issue>): <fpage>40</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr13-1094342012462751">
<citation citation-type="web">
<collab collab-type="author">NVIDIA</collab> (<year>2009</year>) <article-title>NVIDIA’s Next Generation CUDA Compute Architecture: Fermi</article-title>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf">http://www.nvidia.com/content/PDF/fermi_white_papers/NVIDIA_Fermi_Compute_Architecture_Whitepaper.pdf</ext-link>.</citation>
</ref>
<ref id="bibr14-1094342012462751">
<citation citation-type="web">
<collab collab-type="author">NVIDIA</collab> (<year>2010</year>) <comment>CUDA Best Practices Guide. Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://developer.download.nvidia.com/compute/cuda/3_2_prod/toolkit/docs/CUDA_C_Best_Practices_Guide.pdf">http://developer.download.nvidia.com/compute/cuda/3_2_prod/toolkit/docs/CUDA_C_Best_Practices_Guide.pdf</ext-link>.</citation>
</ref>
<ref id="bibr15-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rosinski</surname>
<given-names>JM</given-names>
</name>
<name>
<surname>Williamson</surname>
<given-names>DL</given-names>
</name>
</person-group> (<year>1997</year>) <article-title>The accumulation of rounding errors and port validation for global atmospheric models</article-title>. <source>SIAM J Sci Comput</source> <volume>18</volume>: <fpage>552</fpage>–<lpage>564</lpage>.</citation>
</ref>
<ref id="bibr16-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Simmons</surname>
<given-names>AJ</given-names>
</name>
<name>
<surname>Burridge</surname>
<given-names>DM</given-names>
</name>
</person-group> (<year>1981</year>) <article-title>An energy and angular-momentum conserving vertical finite difference scheme and hybrid vertical coordinates</article-title>. <source>Mon Wea Rev</source> <volume>109</volume>: <fpage>758</fpage>–<lpage>766</lpage>.</citation>
</ref>
<ref id="bibr17-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Taylor</surname>
<given-names>MA</given-names>
</name>
<name>
<surname>Edwards</surname>
<given-names>J</given-names>
</name>
<name>
<surname>St.-Cyr</surname>
<given-names>A</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Petascale atmospheric models for the Community Climate System Model: New developments and evaluation of scalable dynamic cores</article-title>. <source>J Phys Conf Series</source> <volume>125</volume>: <fpage>12</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr18-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zerroukat</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Wood</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Staniforth</surname>
<given-names>A</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>A monotonic and positive-definite filter for a Semi-Lagrangian Inherently Conserving and Efficient (SLICE) scheme</article-title>. <source>QJR Meteorol Soc</source> <volume>131</volume>: <fpage>2923</fpage>–<lpage>2936</lpage>.</citation>
</ref>
<ref id="bibr19-1094342012462751">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zerroukat</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Wood</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Staniforth</surname>
<given-names>A</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>The Parabolic Spline Method (PSM) for conservative transport problems</article-title>. <source>Int J Num Meth Fluids</source> <volume>51</volume>: <fpage>1297</fpage>–<lpage>1318</lpage>. <comment>DOI: 10.1002/fld.1154</comment>.</citation>
</ref>
</ref-list>
</back>
</article>