<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EDR</journal-id>
<journal-id journal-id-type="hwp">spedr</journal-id>
<journal-title>Educational Researcher</journal-title>
<issn pub-type="ppub">0013-189X</issn>
<issn pub-type="epub">1935-102X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/0013189X12464517</article-id>
<article-id pub-id-type="publisher-id">10.3102_0013189X12464517</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Feature Articles</subject>
<subj-group subj-group-type="heading">
<subject>2012 Presidential Address</subject>
</subj-group>
</subj-group>
</article-categories>
<title-group>
<article-title>Curricular Coherence and the Common Core State Standards for Mathematics</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Schmidt</surname><given-names>William H.</given-names></name>
<xref ref-type="aff" rid="aff1-0013189X12464517">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Houang</surname><given-names>Richard T.</given-names></name>
<xref ref-type="aff" rid="aff1-0013189X12464517">1</xref>
</contrib>
<aff id="aff1-0013189X12464517"><label>1</label>Michigan State University, East Lansing, MI</aff>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>41</volume>
<issue>8</issue>
<fpage>294</fpage>
<lpage>308</lpage>
<history>
<date date-type="received">
<day>7</day>
<month>5</month>
<year>2012</year>
</date>
<date date-type="rev-recd">
<day>19</day>
<month>6</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>6</day>
<month>8</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012 AERA</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>In this work, we explored the relationship of the Common Core State Standards in Mathematics (CCSSM) to student achievement. Building on techniques developed for the Third International Mathematics and Science Study (TIMSS), we found a very high degree of similarity between CCSSM and the standards of the highest-achieving nations on the 1995 TIMSS. A similar analysis revealed wide variation in the proximity of state standards in effect in 2009 to the CCSSM. Finally, we used regression and analysis of covariance techniques to assess the relationship between the proximity of a state’s standards to the CCSSM and performance on the 2009 National Assessment of Educational Progress (NAEP). After adjusting for cut-points on state assessments and controlling for state demographics related to socioeconomic status and poverty, we found that states with standards more like the CCSSM, on average, had higher NAEP scores.</p>
</abstract>
<kwd-group>
<kwd>correlational analysis</kwd>
<kwd>curriculum</kwd>
<kwd>achievement</kwd>
<kwd>NAEP</kwd>
<kwd>regression analyses</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>As 46 states move to implement the newly adopted Common Core State Standards for Mathematics (CCSSM) the hope among policy makers, teachers, and other educators is that they are, in fact, high quality standards that are internationally competitive, and that will lead to improved achievement for America’s children. This is certainly how they have been described, but some have questioned this characterization (<xref ref-type="bibr" rid="bibr6-0013189X12464517">Milgram and Stotsky, 2010</xref>; <xref ref-type="bibr" rid="bibr8-0013189X12464517">Porter, 2011</xref>; <xref ref-type="bibr" rid="bibr18-0013189X12464517">Stotsky and Wurman, 2010</xref>; <xref ref-type="bibr" rid="bibr21-0013189X12464517">Wurman and Evers, 2010</xref>; and most recently <xref ref-type="bibr" rid="bibr5-0013189X12464517">Loveless, 2012</xref>). The question remains: Do they have the focus, rigor, and coherence that the Third International Mathematics and Science Study (TIMSS) curriculum analysis said were the characteristics that distinguished the curricular standards of the top-achieving countries from those of other countries who do not fare as well in such international assessments, including the United States?</p>
<p>To address this pressing question, we conducted three analyses in order to characterize the focus, coherence, and rigor of the content defined by the CCSSM and also to anticipate their likely impact on student achievement. Our analyses included a comparison of the similarity of the content defined by the CCSSM to that of the mathematics standards of the highest-achieving TIMSS countries as well as to the content defined by the state standards in place before the adoption of the CCSSM; and an examination of the relationship between the proximity of state standards to the CCSSM and average eighth-grade mathematics performance on 2009 National Assessment of Educational Progress (NAEP). We acknowledge that such an approach can only simulate to what degree CCSSM will be related to student achievement after they are implemented in the nation’s classrooms, but it seems an avenue worth exploring, especially given the importance of the issue.</p>
<sec id="section1-0013189X12464517">
<title>The Origins of the Common Core</title>
<p>For more than 15 years, the United States has been concerned about the performance of its students when compared to other nations. The 1997 release of the original TIMSS data showed a downward trend of performance relative to other countries from 4th through 12th grade.</p>
<p>Recent international studies from both TIMSS and the Programme for International Student Assessment (PISA) convey the same message. Even our own national assessment tells much the same story, with only around 30% of 12th-graders being characterized as proficient in mathematics on the NAEP. <xref ref-type="bibr" rid="bibr13-0013189X12464517">Schmidt et al. (2001)</xref> has suggested that a significant factor related to such low performance is the nature of the curriculum—in terms of defining content coverage—both in its intended and implemented forms. Both are instantiations of the broader notion of opportunity to learn (OTL), which studies have shown to be related to academic achievement (<xref ref-type="bibr" rid="bibr12-0013189X12464517">Schmidt &amp; Maier, 2009</xref>).</p>
<p>In response to perceived problems of the existing U.S. mathematics curriculum, the Common Core State Standards (CCSS) were developed under the leadership of the National Governors Association (NGA) and the Council of Chief State School Officers (CCSSO) and were released in 2010. Originally, 49 states agreed to participate in the process and as of the time of the writing of this article, 46 states and the District of Columbia have officially adopted them as their new state standards. This breaks a long tradition (<xref ref-type="bibr" rid="bibr20-0013189X12464517">Will, 2012</xref>) and essentially places the U.S. in a comparable position to most countries in the developed world (<xref ref-type="bibr" rid="bibr13-0013189X12464517">Schmidt et al., 2001</xref>; <xref ref-type="bibr" rid="bibr11-0013189X12464517">Schmidt, Houang, &amp; Shakrani, 2009</xref>).</p>
<p>In 1997, TIMSS released results describing the Grades 1–8 content standards in mathematics for each of 39 countries (<xref ref-type="bibr" rid="bibr15-0013189X12464517">Schmidt, McKnight, Valverde, Houang, &amp; Wiley, 1997</xref>). Further analyses of the data led to a description of the national standards of those countries—referred to as the A+ countries—whose eighth-grade students performed at the top of the international distribution. Three characteristics were identified—focus, rigor, and coherence (<xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt, Wang, &amp; McKnight, 2005</xref>). We concentrate in this article on focus and coherence (although we do address rigor as well) as they relate to the Common Core State Standards. The measure of focus was defined in TIMSS as the number of topics covered at each grade that was also aggregated over the first eight grades, by counting the total number of topic-by-grade combinations covered in elementary and middle school. A topic–grade combination (<xref ref-type="bibr" rid="bibr10-0013189X12464517">Schmidt &amp; Houang, 2007</xref>) refers to coverage of a topic at a particular grade; for example, each topic could potentially be covered at each of eight grades. Greater focus as a characteristic of standards is thereby indicated by a relatively smaller number. Such an overall number (Grades 1–8) was calculated for the top-achieving TIMSS countries and was found to be 99 out of a possible 352 (44 topics × 8 grades).</p>
<p>The other characteristic identified was coherence. This was defined by <xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al. (2005)</xref> as standards that are “articulated over time as a sequence of topics and performances that are logical and reflect, where appropriate, the sequential and hierarchical nature of the disciplinary content from which the subject matter derives” (p. 528). In fact, they found that this was the most important of the three characteristics. This definition recognizes that coverage of topics is only part of the definition of coherence. The other and perhaps more central part of the definition centers on whether the sequence in which the topics are covered is consistent with the logical structure of mathematics. On the basis of TIMSS data from the top-achieving countries, <xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al. (2005)</xref> originally used a geometric shape (one approximating an upper-triangle) as a nonquantitative indicator of coherence (see <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref>). The geometric shape in effect captures both coverage and the pattern or sequence in which the topics were to be covered.</p>
<fig id="fig1-0013189X12464517" position="float">
<label>Figure 1.</label>
<caption><p>Mathematics topics intended at each grade by at least two thirds of the top-achieving countries</p></caption>
<graphic xlink:href="10.3102_0013189X12464517-fig1.tif"/>
</fig>
<p><xref ref-type="bibr" rid="bibr10-0013189X12464517">Schmidt and Houang (2007)</xref> went beyond graphical representation and identified quantitative indicators of both focus and coherence. They calculated measures for each of the countries and related them to achievement. Focus was defined as described above and coherence was operationalized as a count of the 99 possible topic–grade combinations defining intended coverage of the A+ countries as represented in <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref>. The analyses were done at the third, fourth, seventh, and eighth grades for 20 TIMSS countries at third grade up to 33 at eighth grade as not all countries participated at each grade level. The results were statistically significant at both seventh and eighth grades (<italic>p</italic> &lt; .027 and <italic>p</italic> &lt; .014) but only marginally so at fourth grade (<italic>p</italic> &lt; .069). They were not significant at third grade (<italic>p</italic> &lt; .122). The central finding was that focus and coherence were not significantly related to achievement separately but were in combination. As a result, <xref ref-type="bibr" rid="bibr10-0013189X12464517">Schmidt and Houang (2007)</xref> suggested that focus was so interlinked in conception with coherence that it was really a part of coherence itself. The results indicated that greater focus (coverage of fewer topic–grade combinations) and coherence (consistency with the international A+ model) in a country’s national standards were positively related to performance on the TIMSS mathematics test.</p>
</sec>
<sec id="section2-0013189X12464517">
<title>Results From Related Studies of CCSSM</title>
<p><xref ref-type="bibr" rid="bibr9-0013189X12464517">Porter, McMaken, Hwang, and Yang (2011)</xref> looked at the issue of focus comparing the Common Core State Standards with the standards of 27 states. Using a somewhat unique and different definition of focus from that used by <xref ref-type="bibr" rid="bibr13-0013189X12464517">Schmidt et al. (2001)</xref>, they considered not only the number of topics but also the cognitive demand associated with the topics as found in the standards. They found large variation among states and summarized their findings with respect to one index of focus as:
<disp-quote>
<p>Collectively, then, across states, the state content standards are substantially less focused using this criterion than is the Common Core. However, when taking the average across states, the difference is less dramatic, and the states are slightly more focused than is the Common Core. (<xref ref-type="bibr" rid="bibr9-0013189X12464517">Porter et al., 2011</xref>, p. 108)</p>
</disp-quote></p>
<p><xref ref-type="bibr" rid="bibr4-0013189X12464517">Cobb and Jackson (2011)</xref> in response to the Porter et al. study indicated:
<disp-quote>
<p>Unfortunately, the two measures of focus that Porter et al. constructed do not appear to be entirely compatible with the intent of the CCSSM developers . . . [by] contrast, the developers of the CCSSM appear to equate focus with a consistent emphasis on the different facets of central or core mathematical ideas. (p. 183)</p>
</disp-quote></p>
<p>Cobb and Jackson further indicated that the Common Core State Standards were focused “on a small number of core mathematical ideas at each grade” (p. 184), and in their view, were consistent with the definition used in the two Schmidt et al. articles cited previously. In addition, they point out that the developers of the Common Core State Standards “strove for coherence as well as focus” (p. 184). They concluded that Porter et al.’s (2011) analyses “would have been enhanced by a comparison of the CCSSM with state mathematics standards in terms of coherence as well as focus” (<xref ref-type="bibr" rid="bibr4-0013189X12464517">Cobb &amp; Jackson, 2011</xref>, p. 184).</p>
<p>In the present article, we pursue the suggestions noted by Cobb and Jackson in their review of the <xref ref-type="bibr" rid="bibr9-0013189X12464517">Porter et al. (2011)</xref> article. Using data on all 50 state standards as well as the Common Core State Standards for Mathematics, we compared the coherence and focus of the CCSSM to each of the 50 states concentrating only on Grades 1 through 8. We also compared the CCSSM to the internationally derived A+ model.</p>
<p>In addition, we explore one of the central questions surrounding the implementation of the CCSSM: Will this make a difference for student performance? As stated in the introduction, the need for the development of the CCSSM was driven in part by results that showed U.S. students’ performance in mathematics to be below par. In fact, most recently in 2009 PISA, the U.S. ranked 25th out of 34 Organisation for Economic Co-operation and Development (OECD) countries. The real answer to this important question must await full implementation of the CCSSM (only now underway) as well as the development of the Common Core State Assessments due to be introduced for the first time during the 2014–2015 school year.</p>
<p>The 2012 Brown Center Report on American Education (Brookings) took up the question of the CCSSM’s prospective impact in a section titled “Predicting the Effect of Common Core State Standards on Student Achievement.” The author concluded that “the empirical evidence suggests that the Common Core will have little effect on American students’ achievement” (<xref ref-type="bibr" rid="bibr5-0013189X12464517">Loveless, 2012</xref>, p. 14). Using quality ratings for state standards as judged by the Fordham Foundation (<xref ref-type="bibr" rid="bibr3-0013189X12464517">Carmichael, Martino, Porter-Magee, &amp; Wilson, 2010</xref>), Loveless, building on the work of <xref ref-type="bibr" rid="bibr19-0013189X12464517">Whitehurst (2009)</xref>, analyzed the relationship of those state standard ratings to gain scores on NAEP using the 2003–2009 data. Making adjustments for changes in demographic characteristics over the same time period, he found no relationship between Fordham’s ratings and state NAEP gains.</p>
<p>In the last section of this article, we also explore this same question, using different data and a different approach. Testing the hypothesis—supported at least in part by <xref ref-type="bibr" rid="bibr10-0013189X12464517">Schmidt and Houang’s (2007)</xref> work on international assessments—that coherence should be related to performance in mathematics for any well-developed test that focuses on the topics found in most K–8 curricula, we examined at the state level the relationship of a measure of coherence defined relative to the CCSSM for the 50 state standards to mathematics performance on the 2009 state NAEP assessment.</p>
</sec>
<sec id="section3-0013189X12464517">
<title>The Data: Measuring Coherence and Focus</title>
<p>For this study, we used the data characterizing the 50 state standards in place leading up to the 2008–2009 school year. These had been coded as to their content utilizing the methodology developed in TIMSS for the analysis of standards documents (<xref ref-type="bibr" rid="bibr15-0013189X12464517">Schmidt, McKnight, Valverde, et al., 1997</xref>). The data were originally collected as a part of two international studies done from 2005 to 2009 (Teacher Education and Development Study [TEDS] and Mathematics Teaching in the 21st Century [MT21]). Applying the same methodology, we coded the Common Core State Standards in Mathematics.</p>
<p>The coding procedure is based on an internationally developed content framework for K–12 mathematics that includes two dimensions—topics or content specifications and performance expectations (<xref ref-type="bibr" rid="bibr15-0013189X12464517">Schmidt, McKnight, Valverde, et al., 1997</xref>). The latter is what others have called cognitive demand (e.g., <xref ref-type="bibr" rid="bibr7-0013189X12464517">Porter, 2002</xref>). The topic dimension includes 260 specific topics organized hierarchically into 44 broader topics that then can be collapsed into 10 super-categories. For purposes of this article, we focus on the 44 broader topics. In addition to these topic designations, there are 29 categories describing cognitive demand in the framework that, like topics, can be collapsed into a smaller number. Analyses related to the standard’s coding procedures have indicated reasonably strong reliabilities of .7 or greater in TIMSS. This is consistent with what Porter has found using similar procedures and a similar framework (<xref ref-type="bibr" rid="bibr7-0013189X12464517">Porter, 2002</xref>).</p>
<p>Two approaches have been used to describe coherence. One is a graphical representation placing dots at the intersection of a topic and the grade (or grades) at which that topic is intended in the standards of a particular education system. Although visually powerful, this approach does not provide quantitative indicators of coherence or focus. The graphic is examined to determine if the coverage of topics at various grade levels reflects the internal logical structure of mathematics. This process applied to the top-achieving TIMSS countries’ national standards and vetted by a group of mathematicians led to an international model of coherence referred to as the A+ model (see <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref>). This model or any other model of coherence (such as the CCSSM) can be used as the standard by which to judge the coherence of any country, state, or education system’s standards. Using such a model as the standard, lack of coherence is indicated either when topics are being intended at grade levels outside of the model’s placement of those topics or when topics are not covered where intended by the model, that is, where they should be as indicated by the model. The A+ model has not been suggested as the sole definition of coherence, only as one such standard derived empirically and confirmed by mathematicians. It has served as an internationally benchmarked model and was used to define country-level coherence in the <xref ref-type="bibr" rid="bibr10-0013189X12464517">Schmidt and Houang (2007)</xref> article.</p>
<sec id="section4-0013189X12464517">
<title>Definition of the Measure of Congruence</title>
<p>Building on insights from the results of the <xref ref-type="bibr" rid="bibr10-0013189X12464517">Schmidt and Houang (2007)</xref> analyses, we developed a quantitative indicator that combined focus and coherence. Using a model of coherence such as the A+ or the CCSSM and conceptualizing it as a two-dimensional geometric figure defining content coverage across Grades 1–8, we developed a quantitative indicator that can be used to determine the degree of congruence of the geometric figure representing the content coverage of a different set of standards such as the state standards with the model. We use the term <italic>congruence</italic> in this article not in the strictest sense of two sets of standards being identical or coincidental throughout—a characterization that is dichotomous in nature—but as a degree of closeness or similarity. This led to the development of five indicators that were then combined into a total measure of congruence.</p>
<p>To operationalize this, we conceptualized the geometric representation of any particular set of standards (such as those of a state or the A+ standards) to be compared to the CCSSM as being composed of 44 row vectors. Each vector (of 0s or 1s), representing a topic and its coverage over eight grades, was summarized by five indicators. The five indicators were then summed for each topic and finally summed across the 44 topics to develop the total measure of congruence. Each of the five indicators characterized the magnitude of the different types of deviations from the CCSSM’s coverage of a topic. As a result, each indictor can also be summed over topics resulting in the measure of a particular type of deviation from the CCSSM.</p>
<p>The <italic>first indicator</italic> (I) was dichotomous—(0 or –1)—indicating whether a topic was introduced at an earlier grade from that of the grade in which it was introduced in the model of coherence, which, in this article, is the CCSSM. For every topic for which this was the case, a negative one was added to the overall indicator of early coverage for a set of standards. The <italic>second indicator</italic> (II), characterizing lack of focus, was a negative count of the number of times a topic was covered in the standards at a grade level for which it was not intended in the model—what might be called “sins of commission,” or intending coverage of a topic at grade levels not intended in the CCSSM. Every time this occurred a negative one was added to the topic indicator which was also summed over topics to represent the standards overall.</p>
<p>The <italic>third indicator (III)</italic> was a count of the number of times a topic was not covered in the standards at a grade level for which it was intended in the CCSSM—what might be called “sins of omission.” Again, every time this occurred a negative one was added to the topic indicator, which was also summed over topics. The <italic>fourth indicator</italic> (IV) characterized whether a topic was covered later in terms of the last grade for which the topic was intended in the standards from that specified by the CCSSM. The same coding as done for the early introduction of topics was used. The <italic>final indicator</italic> (V) concerned the intended coverage of a topic in cases where there was a “break” in coverage across the grades of a particular topic in the CCSSM but not in the standards. These were coded as indicator II was.</p>
<p>For each topic, the five indicators were summed to produce a negative value indicating the lack of congruence associated with that particular topic. The measure of congruence for a set of standards was then created by summing over topics the topic-specific measure of coherence. These were defined as deviations from the CCSSM and as a result yielded negative values for the measure of congruence unless the standards were coincidental with the CCSSM, in which case the measure of congruence was zero. For ease of interpretation, we converted the overall scale for measuring congruence to be positive, ranging from 0 to 1,000. For the rescaled measure of congruence, 1,000 indicated perfect agreement between the standards being analyzed and the model of coherence.</p>
</sec>
</sec>
<sec id="section5-0013189X12464517">
<title>Results</title>
<sec id="section6-0013189X12464517">
<title>Are the Common Core State Standards Coherent and Focused?</title>
<p>As <xref ref-type="bibr" rid="bibr4-0013189X12464517">Cobb and Jackson (2011)</xref> indicated, an important issue is whether the CCSSM are coherent and how they compare on this dimension to the 50 state standards that are either being replaced or are still in effect. Looking first at a visual representation, we note that <xref ref-type="fig" rid="fig2-0013189X12464517">Figure 2</xref> representing the CCSSM bears a strong resemblance to <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref> (A+ model), at least in terms of its general shape. From that point of view, together with the vetting done by mathematicians (several of whom were the same ones that did the original vetting of the A+), it can be suggested that the CCSSM are coherent and focused. Nevertheless, some of the specifics are different; for example, there are 32 topics in the A+ standards and 35 in the CCSSM (of the 44 possible topics in the TIMSS framework) and the ordering of some of the topics (rows) are not identical in the two figures.</p>
<fig id="fig2-0013189X12464517" position="float">
<label>Figure 2.</label>
<caption><p>Mathematics topics intended in the Common Core State Standards</p></caption>
<graphic xlink:href="10.3102_0013189X12464517-fig2.tif"/>
</fig>
<p>Although <xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al. (2005)</xref> suggested that there was not a single model of coherence, it would be very unlikely that alternative models of coherence would be very different from each other given the logical organization of the discipline. Rather, one would expect a strong degree of congruence, with differences involving those topics whose placement in the logical sequence of mathematics is not quite so determined and hence leaving some flexibility as to their placement. Some of the geometry topics would fall into this category as might some of the measurement and data topics.</p>
<p>To further examine the coherence and focus of the CCSSM, we compared them more formally with the international benchmark model of coherence—the A+. This comparison is represented in <xref ref-type="fig" rid="fig3-0013189X12464517">Figure 3</xref>, where the A+ topic coverage is mapped onto the CCSSM. The shaded area represents the topic–grade combinations defining what is expected in the CCSSM, whereas the dots represent the topic–grade combinations called for in the A+ standards.</p>
<fig id="fig3-0013189X12464517" position="float">
<label>Figure 3.</label>
<caption><p>Mathematics topics intended at each grade in top-achieving countries compared to the Common Core State Standards</p></caption>
<graphic xlink:href="10.3102_0013189X12464517-fig3.tif"/>
</fig>
<p>There were only three topics intended for coverage in the CCSSM that were introduced in an earlier grade by the A+ (indicator I). The differences between the two sets of standards were mostly captured by indicator III (the A+ not covering topics at particular grades as found in the CCSSM). The CCSSM introduced many topics in grades earlier than called for in the A+ standards. This was especially the case for the 12 topics introduced in Grade 1, 2, or 3 although half of them were geometry, measurement, or data topics. This was not as prevalent for topics introduced after Grade 3.</p>
<p>There were also four topics including systematic counting and probability (both covered only at seventh grade in the CCSSM), and real numbers and validation/justification (both covered in the CCSSM only at the eighth grade) that were not covered by the A+. Given that they are all intended only at a single grade and are not prerequisite for any other topics in the standards, their omission (although an inconsistency and accounted for in indicator III) does not likely have a serious impact on coherence per se. Rather, it is more the case of the appearance that the CCSSM included more rigorous content than the A+, at least in Grades 7 and 8 (which proves to be only somewhat the case as discussed later in this section).</p>
<p>There being no major differences between the two sets of standards, this provides further evidence that the CCSSM are coherent and very consistent with the international benchmark. Overall, the A+ had a total congruence value of 833 (out of 1,000), which implies an almost 85% degree of consistency with the CCSSM.</p>
<p>Although the measure of coherence integrates focus into the overall calculation, we now examine it separately in a way that is comparable to the measure of focus for the A+ (<xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al., 2005</xref>) and that addresses Cobb and Jackson’s concern that the Porter et al. definition of focus is not comparable and combines the concept of cognitive complexity with focus. <xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al. (2005)</xref> originally defined focus as the number of topics intended for coverage at each grade, which could also be combined into a single index by aggregating over grade levels. The smaller the value, the greater the focus—thus allowing the limited amount of instructional time in a year to be spent on a fewer number of topics. <xref ref-type="table" rid="table1-0013189X12464517">Table 1</xref> gives the number of topics intended at each grade level for the A+ model and the CCSSM. The CCSSM have focus at each of the grade levels that is consistent in number and pattern with that of the A+. The differences in the number of topics expected at each grade level vary by no more than three.</p>
<table-wrap id="table1-0013189X12464517" position="float">
<label>Table 1</label>
<caption><p>Number of Topics Intended for Coverage at Each Grade in Various Standards</p></caption>
<graphic alternate-form-of="table1-0013189X12464517" xlink:href="10.3102_0013189X12464517-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="8">Grade<hr/></th>
</tr>
<tr>
<th/>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
</tr>
</thead>
<tbody>
<tr>
<td>A+</td>
<td>5</td>
<td>9</td>
<td>12</td>
<td>16</td>
<td>21</td>
<td>20</td>
<td>22</td>
<td>21</td>
</tr>
<tr>
<td>Common Core</td>
<td>8</td>
<td>11</td>
<td>13</td>
<td>17</td>
<td>21</td>
<td>22</td>
<td>19</td>
<td>20</td>
</tr>
<tr>
<td>Current state averages</td>
<td>13</td>
<td>15</td>
<td>18</td>
<td>20</td>
<td>21</td>
<td>23</td>
<td>23</td>
<td>21</td>
</tr>
<tr>
<td>1995 state averages</td>
<td>12</td>
<td>17</td>
<td>21</td>
<td>26</td>
<td>28</td>
<td>32</td>
<td>32</td>
<td>34</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Counting the number of dots for each grade in <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref> results in a smaller set of values for the A+ model than what is indicated in <xref ref-type="table" rid="table1-0013189X12464517">Table 1</xref>. <xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al. (2005)</xref> in developing the model presented in <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref> indicated topic expectations at a grade level (the dots) when two thirds or more of the top-achieving countries listed it in their standards. As a result, <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref> does not represent a set of standards defining a full curriculum for the A+ countries but only the common part shared by two-thirds or more of them. The article also reported the average number of topics that were intended at each grade for the A+ countries, which in all cases were slightly more than the number of dots found in <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref> and are the values presented in <xref ref-type="table" rid="table1-0013189X12464517">Table 1</xref>. To be comparable to the CCSSM, the A+ model should represent a full complement of topics for a typical top-achieving country.</p>
<p>The additional one to three topics for Grades 1, 4, 5, 6, and 8 and five or six topics for Grades 2, 3, and 7 in the top-achieving countries’ standards that were needed to complete the picture of typical intended coverage were, however, not intended by the required two thirds or more to be included in <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref>. Between 4 and 10 potential additional topics that were intended for coverage at each grade by only half of the A+ countries (with one exception where the only potential additional topic was selected by just two of the countries) were also identified by <xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al. (2005)</xref>. Selecting from that list of topics we added the appropriate number of them to <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref> in order to have the number of topics intended at each grade be equal to the number of topics intended on average for the A+ countries. Given that the topics on the list were each chosen by one-half of the A+ countries, we first chose those topics that were consistent with those in the CCSSM and then randomly chose from the rest. The result is represented in <xref ref-type="fig" rid="fig4-0013189X12464517">Figure 4</xref>, which presents one possible example of standards defining complete topic coverage for Grades 1–8 consistent with what is typical for the A+ countries. The congruence between the A+ standards (we here use the phrase “A+ standards” so as to differentiate them from the A+ model depicted in <xref ref-type="fig" rid="fig1-0013189X12464517">Figure 1</xref>) and CCSSM is somewhat stronger, with a congruence value of 889 implying an almost 90% degree of consistency between the two sets of standards.<sup><xref ref-type="fn" rid="fn1-0013189X12464517">1</xref></sup> <xref ref-type="fig" rid="fig3-0013189X12464517">Figures 3</xref> and <xref ref-type="fig" rid="fig4-0013189X12464517">4</xref>, together with related indices, support the inference that the CCSSM are congruent and focused. In comparison to the A+, the CCSSM are world-class standards.</p>
<fig id="fig4-0013189X12464517" position="float">
<label>Figure 4.</label>
<caption><p>A possible set of standards based on a typical profile from the top-achieving countries</p></caption>
<graphic xlink:href="10.3102_0013189X12464517-fig4.tif"/>
</fig>
</sec>
<sec id="section7-0013189X12464517">
<title>Comparing the Coherence of CCSSM With State Standards</title>
<p>Those states that have adopted the CCSSM are now confronted with the reality of implementing them across districts and schools. Once adopted, it is relatively easy to declare that the CCSSM are the new standards for the state, but given that the districts and schools have presumably aligned themselves to the old standards, a challenging effort will be required, especially if the CCSSM differ greatly in focus and coherence from the state’s most recently implemented set of standards.</p>
<p>In this section we used state data from the TEDS and MT21 studies that were coded using the same methodology to estimate how much each state’s standards varied in coherence and focus from those of the CCSSM. <xref ref-type="table" rid="table2-0013189X12464517">Table 2</xref> presents the results, placing states into five categories based on the overall measure of congruence. On the measure of congruence ranging from 0 to 1,000 (and compared to the international (A+) standards’ score of 889), the states ranged from 662 to 826 with an average of 762 and a standard deviation of 33.5. States such as California, Florida, Georgia, Indiana, Michigan, and Washington, all of whom had standards that were given a grade of A by Fordham (<xref ref-type="bibr" rid="bibr3-0013189X12464517">Carmichael et al., 2010</xref>), ranked in the top 8 on the measure of congruence. Two anomalies stand out. Mississippi’s standards had a high degree of congruence with the CCSSM, but were given a grade of “C” by Fordham, whereas Massachusetts’s standards ranked 24th overall on the measure of congruence—which was not at a level that might be expected given the high praise their standards have often received. At the other extreme, the standards for Arizona, Iowa, Kansas, Louisiana, Nevada, New Jersey and Wisconsin were the least coherent and focused.</p>
<table-wrap id="table2-0013189X12464517" position="float">
<label>Table 2</label>
<caption><p>Degree of Congruence of State Standards as Compared to the Common Core State Standards for Mathematics</p></caption>
<graphic alternate-form-of="table2-0013189X12464517" xlink:href="10.3102_0013189X12464517-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<tbody>
<tr>
<td>Most like CCSSM</td>
<td>Alabama</td>
<td>California</td>
<td>Florida</td>
<td>Georgia</td>
<td>Indiana</td>
</tr>
<tr>
<td rowspan="9"><inline-graphic xlink:href="10.3102_0013189X12464517-img1.tif"/></td>
<td>Michigan</td>
<td>Minnesota</td>
<td>Mississippi</td>
<td>Oklahoma</td>
<td>Washington</td>
</tr>
<tr>
<td>IdahoUtah</td>
<td>North Dakota</td>
<td>Oregon</td>
<td>South Dakota</td>
<td>Tennessee</td>
</tr>
<tr>
<td>Utah</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Alaska</td>
<td>Arkansas</td>
<td>Colorado</td>
<td>Delaware</td>
<td>Hawaii</td>
</tr>
<tr>
<td>Massachusetts</td>
<td>New Mexico</td>
<td>New York</td>
<td>North Carolina</td>
<td>Ohio</td>
</tr>
<tr>
<td>Pennsylvania</td>
<td>South Carolina</td>
<td>Texas</td>
<td>Vermont</td>
<td>West Virginia</td>
</tr>
<tr>
<td>Connecticut</td>
<td>Illinois</td>
<td>Maine</td>
<td>Maryland</td>
<td>Missouri</td>
</tr>
<tr>
<td>Montana</td>
<td>Nebraska</td>
<td>New Hampshire</td>
<td>Virginia</td>
<td>Wyoming</td>
</tr>
<tr>
<td>Arizona</td>
<td>Iowa</td>
<td>Kansas</td>
<td>Kentucky</td>
<td>Louisiana</td>
</tr>
<tr>
<td>Least like CCSSM</td>
<td>Nevada</td>
<td>New Jersey</td>
<td>Rhode Island</td>
<td>Wisconsin</td>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
<p>The level of congruence was defined in part by the two major indices characterizing what we have termed “sins of omission” and “sins of commission.” The lower than expected value for Massachusetts’s state standards can be understood by examining these two components. For the measure characterizing the degree of consistency (III) for the intended coverage of the CCSSM topics at their appropriate grade levels, the Massachusetts standards were ranked 11th, among the 50 states, but on index II characterizing the degree of inconsistency related to coverage of topics at grade levels not found in the CCSSM, Massachusetts’s standards were ranked 35th. Thus, the dominant negative factor for Massachusetts’ standards was coverage of topics at a grade level for which such coverage was not intended in the CCSSM. This was also the case for Arizona, Kansas, Kentucky, Nevada, New Hampshire, New Jersey, and Rhode Island, which all except for New Hampshire, were among the states with the lowest degree of congruence with the CCSSM.</p>
<p>The other major contributor to the lower congruence—lack of coverage of topics at the grade level for which the CCSSM intended such coverage (III)—Florida, Georgia, North Carolina, Oregon, Texas, Washington, and Wyoming stand out. Six states whose state standards do not have intended topics specified for Grades 1 and 2 also stand out on this dimension and included Colorado, Illinois, Iowa, Maine, Montana, and Pennsylvania. Given the absence of stated intentions at these grades—unless we were to make assumptions about what teachers would do in such a case—the measure indicates correctly a serious lack of congruence with the CCSSM. Such “sins of omission” do not just reduce coherence at Grades 1 and 2 but also affect the coherence of topics for later grades in which CCSSM assumes coverage in Grades 1 or 2.</p>
<p>The measure of congruence with the CCSSM was defined by the sum over topics of the topic-specific measure of congruence (representing how deviant the topic coverage in the standards was from coverage in the CCSSM) defined for each of the 44 topics separately. Looking at the disaggregated data by topic allows us to identify topics that were the most consistent with the intended coverage as defined by the CCSSM as averaged across states. The values of the topic measure of congruence (expressed as deviation scores) ranged from 0 to −9.4, with zero indicating concordance in coverage between all the states and the CCSSM.</p>
<p>Several topics were especially consistent on average across states in coverage (a value on the index greater than −2.5) with the CCSSM’s intended coverage, including polygons and circles, data analysis, rational numbers, measurement units, and slope. On the other hand, several topics were not intended typically in the state standards in a way consistent with the CCSSM (values of −6 or lower). This becomes important for several reasons. First of all, these are topics for which change would likely be the most difficult to accommodate. The topics would need detailed specifications for states to guide districts and individual teachers, because the topics have not typically been part of current practice at the appropriate grades. This implies major adjustments in instruction. The topics included properties of whole number operations; geometric transformations; errors of measurement; 3-D geometry; primes and factorization (number theory); patterns, relations, and functions; probability; and congruence and similarity.</p>
<p>The lack of consistency with the topics of probability and the congruence and similarity of geometric figures is due mostly to their early coverage. Both topics are included only in the CCSSM at the seventh or eighth grade, which is similar to the A+ standards for congruence and similarity, but probability is not included until high school. Their placement in the A+ and CCSSM appropriately reflect their complexity and the need for coverage of prerequisite topics in order to correctly define them. In many states, these two topics are intended for coverage in elementary school, even in the first or second grade. If these topics are to be taken seriously when they are introduced, the necessary background topics need to have already been covered, which for these topics will not occur until sixth grade or later. Covering them in the earlier grades can take time away from the focused coverage of the essential topics that need to be covered in those early grades, but also can create confusion and misconceptions related to the absence of clear definitions, inhibiting the later learning of these topics.</p>
<p>The other way the data can be examined is with respect to how congruence varies by grade. Although the overall measure of congruence cannot be defined for individual grades because congruence must be defined over grades, the two components (II and III) discussed above can be combined to characterize coherence as it varies across Grades 1–8. The two components are deviations from the CCSSM in appropriate coverage of topics and inappropriate coverage of topics at each grade. The grade-level index combines the two but is defined in the percentage metric in order to account for the differences among grades in the number of intended topics in the CCSSM at each grade.</p>
<p>The values averaged over states ranged from 28 (at first grade) to 49 (at sixth grade). The greatest mismatch occurred in Grades 6 through 8. In the early elementary grades (1–4) the inconsistency was mostly due to the coverage of topics at grade levels not specified in the CCSSM—“sins of commission”—whereas in Grades 5–8 a mix of both types of misspecifications contributed about equally. These patterns vary appreciably across states and across grades within states. For example, North Dakota at first grade intended coverage of all of the relevant topics in the CCSSM and intended coverage of only 3 of the 36 topics (11%) not intended for coverage at first grade. At eighth grade by contrast, North Dakota intended coverage of 80% of the topics found in the CCSSM and about one-third of those not called for in the CCSSM. As a result, few general patterns were noted.</p>
</sec>
<sec id="section8-0013189X12464517">
<title>Comparing the Focus of CCSSM With State Standards</title>
<p>The previous sections describe the degree of congruence between the CCSSM and each of the 50 states as well as a composite of the states with respect to a measure of coherence. The measure of coherence incorporates the idea of focus, but in this section we look at a specific measure of focus—one that is consistent with <xref ref-type="bibr" rid="bibr16-0013189X12464517">Schmidt et al. (2005)</xref>, as well as <xref ref-type="bibr" rid="bibr4-0013189X12464517">Cobb and Jackson (2011)</xref>. That measure is simply a count of the number of topics intended for coverage at each grade. <xref ref-type="table" rid="table1-0013189X12464517">Table 1</xref> gives the average over the 50 states. In the early grades, the difference in focus clearly favors the CCSSM. Over the first five grades, the CCSSM called for the coverage of between 3 and 5 fewer topics (with the exception of Grade 5, where there was no difference) than the average for the 50 state standards. For Grades 5–8, there is a smaller difference in focus.</p>
<p>It is important to note that at the time of TIMSS (1995), an analysis of U.S. state standards painted an even greater lack of focus, leading to the designation of the U.S. curriculum “as a mile wide and an inch deep.” These values, also listed in <xref ref-type="table" rid="table1-0013189X12464517">Table 1</xref>, were drawn from <xref ref-type="bibr" rid="bibr14-0013189X12464517">Schmidt, McKnight, and Raizen (1997)</xref>. These values suggest the states on average have increased the focus of their standards, especially for fifth through eighth grade, where the differences ranged from 7 to 13 topics. The CCSSM as the new state standards for 46 states have moved one step further in the quest for focus.</p>
</sec>
<sec id="section9-0013189X12464517">
<title>Comparing the Cognitive Demand of CCSSM With State Standards</title>
<p>In the coding of the CCSSM and the 50 state standards, not only was each mathematics topic coded as to content but also as to the level of cognitive demand associated with that topic. This was done following the approach of <xref ref-type="bibr" rid="bibr15-0013189X12464517">Schmidt, McKnight, Valverde et al. (1997)</xref>. Their original framework had 29 categories ranging from recognizing equivalences to justifying, proving, and axiomatizing. For our purposes here, these were collapsed into broader categories representing four levels: (1) knowledge—memorizing definitions, (2) performing routine procedures, (3) solving routine problems, and (4) mathematics reasoning including nonroutine problem solving. These are very similar to those used by <xref ref-type="bibr" rid="bibr9-0013189X12464517">Porter et al. (2011)</xref> in analyzing the CCSSM. Unlike Porter et al., we do not consider cognitive demand as a part of focus. Rather, we consider cognitive demand as that which defines a topic’s depth. Cognitive demand, therefore, is related to the third characteristic of the A+ model—rigor. Going beyond Levels 1 and 2 for intended coverage of a topic is consistent with focus in that instead of covering more topics the standards call for coverage of the same topic that is deeper and that moves toward the underlying structure of mathematics to what <xref ref-type="bibr" rid="bibr2-0013189X12464517">Bruner (1995)</xref> describes as “the experience of going from a primitive and weak grasp of some subject to a stage in which he has a more refined and powerful grasp of it” (p. 334). Staying at Levels 1 and 2 does not encourage such a “powerful grasp” (p. 334).</p>
<p><xref ref-type="table" rid="table3-0013189X12464517">Table 3</xref> summarizes the coding of the cognitive demand of the CCSSM for the topics at each grade. These codings (at the level of the 29 categories) were then aggregated into the four broad categories and converted to percentages.</p>
<table-wrap id="table3-0013189X12464517" position="float">
<label>Table 3</label>
<caption><p>Percentage Coverage of Four Broad Categories of Cognitive Demand Associated With the Common Core State Standards for Mathematics</p></caption>
<graphic alternate-form-of="table3-0013189X12464517" xlink:href="10.3102_0013189X12464517-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="8">Grade<hr/></th>
<th/>
</tr>
<tr>
<th align="center">Cognitive demands</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">All grades</th>
</tr>
</thead>
<tbody>
<tr>
<td>Knowledge—memorizing definitions</td>
<td>40</td>
<td>47</td>
<td>29</td>
<td>31</td>
<td>35</td>
<td>22</td>
<td>27</td>
<td>25</td>
<td>30</td>
</tr>
<tr>
<td>Performing routine procedures</td>
<td>31</td>
<td>26</td>
<td>36</td>
<td>32</td>
<td>27</td>
<td>28</td>
<td>34</td>
<td>31</td>
<td>31</td>
</tr>
<tr>
<td>Solving routine problems</td>
<td>29</td>
<td>27</td>
<td>35</td>
<td>33</td>
<td>37</td>
<td>47</td>
<td>36</td>
<td>34</td>
<td>36</td>
</tr>
<tr>
<td>Mathematics reasoning including  solving nonroutine problems</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>0</td>
<td>3</td>
<td>3</td>
<td>9</td>
<td>3</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Across the eight grades, 61% of the intended level of cognitive demand associated with each topic was at the more basic “knowledge—memorizing definitions” and “performing of routine procedures” levels. Only 3% reached the highest level. These percentages varied by grade, with the largest proportion of knowledge and routine procedures occurring at grades one and two and the greatest concentration of “solving routine problems” at grade six. As one would expect, the greatest concentration on proving etc. was at the eighth grade, although even here it was only about 10% of the total.</p>
<p>Combining Levels 1 and 2 gives an estimate that is close to that given in <xref ref-type="bibr" rid="bibr9-0013189X12464517">Porter et al. (2011)</xref>. Because the definition of cognitive demand is not the same, one would not expect the percentages to be identical, but the general thrust is consistent—more than half of the intended cognitive demand was at the two lowest levels. The comparison between the two analyses yielded exactly the same percentage for routine problem solving (demonstrate understanding), but <xref ref-type="bibr" rid="bibr9-0013189X12464517">Porter et al. (2011)</xref> found a much higher incidence of Level 4 (although it too accounted for only about 10%).</p>
</sec>
<sec id="section10-0013189X12464517">
<title>Anticipating the Relationship of the Common Core State Standards and Achievement</title>
<p>Certainly part of the motivation for the development of the Common Core State Standards was to move the country in the direction of having coherent, focused, and rigorous standards for all students, as top-achieving countries have. The belief is that this would improve mathematics achievement for all students—essentially raising the entire achievement distribution. Unfortunately, determining the relationship between these new standards and achievement will take several years at the very least. The new tests being developed to assess performance relative to the CCSSM will first be available in the 2014–2015 school year.</p>
<p>In this section, we simulate what that relationship might look like using the 2009 state NAEP data for eighth grade. We do this in a novel way using our measure of congruence with the CCSSM coupled with several assumptions. As such, these analyses should be viewed as only exploratory in nature, merely suggesting the possibility of a relationship. We have analyzed the 50 state standards in place leading up to the time of the 2009 NAEP test. In effect, these would have been the standards students undergoing the assessment would have had over many if not all of their 8 years of schooling. What they actually experienced as the implemented curriculum in the classrooms within each of the states would presumably have been influenced by them. As shown in the previous section, each state’s standards differed in varying degrees from the CCSSM.</p>
<p>To simulate the relationship between the CCSSM and NAEP, we first assumed that variation in the degree of implementation of the CCSSM across states, were that known, would be related to mathematics achievement not only on the assessments being developed specifically for the CCSSM but also on a reasonably well defined mathematics test that is based on most of the same topics cumulatively covered over the first 8 years of school. NAEP, which is generally well respected, would certainly serve this role.</p>
<p>The problem is that such an analysis is not possible because the CCSSM are only now beginning to be implemented by the states; measures of the degree of implementation are simply not available. As a result, we make an additional assumption that the closer a state’s mathematics standards—as defined by the measure of congruence—are to the CCSSM, the more that state’s standards become reflective of the CCSSM and as a consequence the more likely content coverage at the classroom level would also have been more consistent with the CCSSM. In other words, the greater the value of the measure of congruence for a state’s standards the more like the CCSSM they are and consequently the more likely that classroom content coverage would also be consistent with the CCSSM. This then leads to our central hypothesis: Those states with standards that were more similar or closer to the CCSSM should be more likely to have had higher average scores on the 2009 state NAEP assessment. It is this relationship between the measure of congruence for state standards and average performance on state NAEP that we examine in this section as a means to explore the possible effect of the adoption of the CCSSM on student achievement.</p>
<p>To examine this relationship we plotted the 50 states and fit a simple linear regression model relating the degree of congruence to the state NAEP scores. We found a weak relationship that was not statistically significant, but on examining the residuals from the model and the scatter plot we found what appeared to be two distinct groups of states.</p>
<p>One group, Group A—the dominant one in terms of the number of states—ranged across the spectrum of values for both variables. The second group, Group B—composed of a much smaller set of states—was restricted to a limited region of the plot with low values on NAEP and high values on the measure of congruence with the CCSSM. As the boundaries of the two groups were not particularly well defined, we divided the scatter plot into four quadrants using the means of the two variables. Group B states were all found in the fourth quadrant, with above-average measures of congruence and below-average NAEP scores. The 13 states so identified were South Carolina, Florida, Georgia, Arkansas, Tennessee, Alabama, Mississippi, West Virginia, Oklahoma, New Mexico, Hawaii, California, and Michigan.</p>
<p>We then fitted the same linear regression model separately to each group of states and obtained a striking result. There was a statistically significant (<italic>p</italic> &lt; .006) positive association between the measure of congruence and NAEP scores for Group A (37 states). The <italic>R</italic><sup>2</sup> was .20 with an estimated regression coefficient of .08 (see <xref ref-type="table" rid="table4-0013189X12464517">Table 4</xref>). Fitting the same model to the group of 13 (Group B), we found a nonsignificant relationship. The residuals identified the state of Mississippi as an outlier. It was ranked last among the states on the NAEP test while in the top three on the measure of congruence. If such an extreme outlier is eliminated and the model is reestimated for group B, the <italic>R</italic><sup>2</sup> becomes .10, but the model is still not statistically significant (<italic>p</italic> &lt; .33), not surprisingly given only 10 degrees of freedom for error. The estimated value of the regression coefficient (.06) was essentially the same (at least within the bounds of error) as the corresponding coefficient for Group A, suggesting a parallel relationship between the two groups even though it was not significant for Group B (see <xref ref-type="fig" rid="fig5-0013189X12464517">Figure 5</xref>).</p>
<table-wrap id="table4-0013189X12464517" position="float">
<label>Table 4</label>
<caption><p>Estimated Results From Fitting Different Models Predicting 2008–2009 State NAEP Performance</p></caption>
<graphic alternate-form-of="table4-0013189X12464517" xlink:href="10.3102_0013189X12464517-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="center">Estimate</th>
<th align="center">Standard error</th>
<th align="center"><italic>p</italic></th>
<th align="center"><italic>R</italic>-squared</th>
<th align="center"><italic>p</italic></th>
<th align="center">No. of states</th>
</tr>
</thead>
<tbody>
<tr>
<td>Congruence measure (Group A)</td>
<td>0.08</td>
<td>0.03</td>
<td>&lt;.006</td>
<td>0.20</td>
<td>&lt;.006</td>
<td>37</td>
</tr>
<tr>
<td>Congruence measure (Group B)</td>
<td>0.06</td>
<td>0.06</td>
<td>&lt;.325</td>
<td>0.10</td>
<td>&lt;.325</td>
<td>12<sup><xref ref-type="table-fn" rid="table-fn2-0013189X12464517">a</xref></sup></td>
</tr>
<tr>
<td>Congruence measure ANCOVA</td>
<td>0.07</td>
<td>0.02</td>
<td>&lt;.008</td>
<td>0.59</td>
<td>&lt;.000</td>
<td>50</td>
</tr>
<tr>
<td>Congruence measure adjusted for cut points (Group A)</td>
<td>0.07</td>
<td>0.02</td>
<td>&lt;.000</td>
<td>0.32</td>
<td>&lt;.000</td>
<td>37</td>
</tr>
<tr>
<td>Congruence measure adjusted for cut points (Group B)</td>
<td>−0.01</td>
<td>0.04</td>
<td>&lt;.783</td>
<td>0.01</td>
<td>&lt;.783</td>
<td>13</td>
</tr>
<tr>
<td>Congruence measure adjusted for cut points ANCOVA</td>
<td>0.05</td>
<td>0.02</td>
<td>&lt;.001</td>
<td>0.62</td>
<td>&lt;.000</td>
<td>50</td>
</tr>
<tr>
<td>Congruence measure adjusted for cut points</td>
<td>0.04</td>
<td>0.02</td>
<td>&lt;.016</td>
<td>0.67</td>
<td>&lt;.000</td>
<td>50</td>
</tr>
<tr>
<td>  % federal funding</td>
<td>−0.95</td>
<td>0.26</td>
<td>&lt;.001</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>  % nonpoor white</td>
<td>0.33</td>
<td>0.05</td>
<td>&lt;.000</td>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Congruence measure adjusted for cut points ANCOVA</td>
<td>0.05</td>
<td>0.01</td>
<td>&lt;.000</td>
<td>0.78</td>
<td>&lt;.000</td>
<td>50</td>
</tr>
<tr>
<td>  % federal funding</td>
<td>−0.75</td>
<td>0.21</td>
<td>&lt;.001</td>
<td/>
<td/>
<td/></tr>
<tr>
<td>  % nonpoor white</td>
<td>0.19</td>
<td>0.05</td>
<td>&lt;.001</td>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013189X12464517">
<p><italic>Note</italic>: NAEP = National Assessment of Educational Progress; ANCOVA = analysis of covariance.</p>
</fn>
<fn id="table-fn2-0013189X12464517">
<label>a</label>
<p>Mississippi excluded.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<fig id="fig5-0013189X12464517" position="float">
<label>Figure 5.</label>
<caption><p>Scatter plot with estimated regression lines for Groups A and B relating congruence to 2009 National Assessment of Educational Progress (NAEP). Mississippi was excluded from the scatter plot.</p></caption>
<graphic xlink:href="10.3102_0013189X12464517-fig5.tif"/>
</fig>
<p>As a result, we fitted an analysis of covariance (ANCOVA) model to the data, treating the designation of the two groups as the design variable and the measure of congruence as the covariate. This model assumes parallelism of the two regression lines and has the advantage of a larger number of pooled degrees of freedom for error. The results of this analysis (which includes the state of Mississippi) are also summarized in <xref ref-type="table" rid="table4-0013189X12464517">Table 4</xref>.</p>
<p>The model was significant (<italic>p</italic> &lt; .0001) with an <italic>R</italic><sup>2</sup> = .59. The estimated regression coefficient was .07 (<italic>SE</italic> = .02, <italic>p</italic> &lt; .008) and the estimated contrast between the two groups was 14.67 (<italic>p</italic> &lt; .0001). This yields results consistent with analyzing the two groups separately but now fitting a common regression line for both groups indicating that the more congruent a state’s standards are with the CCSSM, the higher the predicted score for the states’ NAEP test. The only difference between the group of 13 states and the rest of the states is that they operate in general at a lower level of performance on NAEP, but the relationship to congruence with the CCSSM is essentially the same. This implies the more consistent that the coherence and focus of the state standards are, relative to the CCSSM, the higher the predicted average achievement. The relationship of the standards to achievement most likely occurs indirectly through the process of implementation. In this way, the relationship not only reflects the effect of the standards per se but also attendant state policies related to their implementation. Given the coherence of the CCSSM, these results also imply that degree of coherence and focus is related to higher mathematics achievement at the state level.</p>
</sec>
<sec id="section11-0013189X12464517">
<title>The Role of Implementation</title>
<p>A tantalizing question remains: What makes Group B so different from Group A? We defined the group of 13 by their membership in the fourth quadrant of the scatter plot defined by the measure of congruence and NAEP. These were states whose standards were above average in coherence and focus related to the CCSSM but whose NAEP scores were below average. The relationship of the measure of congruence and NAEP is essentially the same for both groups, but an almost 15-point gap in average achievement exists between them.</p>
<p>We believe that what distinguishes the two groups is the degree to which the state standards of each group were actually implemented in the state’s classrooms. More specifically we hypothesize that those states in the group of 13 which on average had highly challenging standards—ones that had a high degree of coherence and focus with the CCSSM—did not on average have them become reality in terms of the content coverage that was actually implemented in the classrooms.</p>
<p>State standards are only stated intentions. Their effect on student achievement is essentially indirect, influencing districts, schools, and teachers in terms of what content is actually covered in the classrooms. Consequently, we believe a critical variable to add to the previous analyses is a measure of the degree to which the topics in the state’s standards were actually being taught in the classrooms at the appropriate grades by the teachers. We also believe such an analysis would strengthen the relationship of the measure of congruence with NAEP across all states. Unfortunately, such a variable describing actual classroom content coverage was not available.</p>
<p>What was available were state policies and statistics related to state-level demographic characteristics that we hypothesize would likely influence the degree to which the stated intentions of the standards would become the implemented curriculum. Adding them to the models previously described might at least help to explain the 15-point difference between the two groups while validating the relationship between degree of congruence with the CCSSM and student achievement.</p>
<p>In analyzing state standards, we have focused only on characterizing content coverage as expected at each of Grades 1–8. We have done the same with respect to the CCSSM. A characterization of state-level policies related to the implementation of the standards, how they work, and what particular attributes of these policies have the greatest influence on actual classroom content coverage are important issues but beyond the scope of this article. For example, <xref ref-type="bibr" rid="bibr17-0013189X12464517">Schwille et al. (1983)</xref> defined one framework that identifies characteristics related to the implementation of standards such as authority, power, coherence, and prescriptiveness. In this article, we consider one such state policy but again focus only on content-related aspects of the policy.</p>
<p>State policy related to assessment is one such possible factor. Each state has its own assessment designed to measure student performance against the standards. One of the critical policy decisions each state makes is the determination of the cut point that defines student proficiency (other cut points such as basic and advanced are used but we focus on proficiency). That decision reflects technical, substantive, and political concerns. How high that cut point is made affects the number of children in the state who are found to be proficient.</p>
<p>Given the strong impact that this decision has on schools, especially in the era of accountability related to NCLB, districts, schools, and teachers pay careful attention to what content is covered on these tests. We postulate that higher cut points send a clear message to the teachers that the state expects them to fully implement the standards. Lower cut points by contrast send the opposite message to teachers—one that says, yes teach the standards but we do not expect your students to actually learn all of the material. The level of rigor represented by the cut point we believe influences the degree of implementation of a state’s standards. At the very least, it is a measure of accountability related to the implementation of the standards.</p>
<p>The difficulty is that across states these cut points are not standardized and so states vary as to their definition of proficiency. The relationship of the states’ defined cut points to state NAEP has been benchmarked onto a single scale (<xref ref-type="bibr" rid="bibr1-0013189X12464517">Bandeira de Mello, 2011</xref>). That scale defines for each state the equivalent value of the state’s cut point defining proficiency in terms of a common metric defined by state NAEP. It is these equivalence scores that we used as a measure of the strength of implementation.</p>
<p>We further converted each state’s equivalence score on NAEP into a proportion reflecting how close the state’s definition of proficiency was to NAEP’s definition of proficiency. The degree of congruence measure was then weighted by this proportion, the logic being that lower assessment cut points mitigate the impact of the standards as local districts and teachers see it. If the state’s definition of proficiency is equivalent to that of NAEP, the congruence measure remains the same. In other cases it reduces the measure of congruence proportionately to how far the state’s definition of proficiency differs from that of NAEP. It is that adjusted measure of congruence with the CCSSM that we used in the statistical model.</p>
<p>It is interesting to see the effect this modified metric has on state rankings of congruence with the CCSSM. An example is Massachusetts, which ranked 24th before, now ranks 3rd when taking into account their assessment policy with respect to the definition of proficiency. Their definition of proficiency was equivalent (actually slightly higher though not statistically significantly different) to that of NAEP. Minnesota remains at the top, but Michigan, which was in the group of 13 (Group B) and ranked 5th in the nation in terms of their standards’ closeness to the CCSSM, was now 28th after adjusting for the very low cut point on the Michigan Assessment (MEAP). Michigan, in fact, has just raised the cut point defining proficiency which had been set at the scale equivalent of only 39% correct. With only two exceptions, the rest of the 13 states dropped in their rankings, and others such as Alabama, Georgia, and Tennessee did so appreciably. For example, Tennessee dropped from 15th to 49th and Alabama from 8th to 36th. Overall, the average degree of congruence for the group of 13 dropped by almost 100 points, compared to roughly 75 points for the other 37 states.</p>
<p>Another factor that likely influences the degree of implementation of state standards is the percentage of students who come from low-income families and attend the state’s public schools. States with larger percentages of such children are likely to have a more difficult time implementing their standards, especially if the standards are quite challenging. All of the states in the group of 13 were ranked in the upper half of the distribution describing the degree of congruence with the rigorous CCSSM. In fact, 7 of the 13 had state standards ranked in the top 10. On the other hand, 10 of those states had a per capita personal income in 2007 that was at least some $5,000 less than the U.S. mean (a 13% difference), placing them among the 16 states nationwide with the lowest per capita income. Two related demographics including childhood poverty rates and the social class distribution of the families of white children attending the state’s public schools, painted much the same picture.</p>
<p>Title I and other federal programs have been designed to help ameliorate this situation by providing substantial federal funding and as such is one indicator reflecting the social class distribution of the state especially as it relates to poverty. Given the small sample size of 50 states, we used only two variables to take social class into account in the model—the percentage of revenue for the state’s public K–12 schools that came from the federal government in the 2008–2009 school year and the percentage of nonpoor whites whose children attended public school in the state.</p>
<p>Using these variables, we fitted two new models to the state NAEP data. First we examined the results from fitting the same models as before but with the adjusted (for the cut point) congruence measure. For group A, the <italic>R</italic><sup>2</sup> increased to 32% (<italic>p</italic> &lt; .0003) whereas for the 13 states as expected it was not significant. The result from fitting the ANCOVA model was statistically significant (<italic>R</italic><sup>2</sup> = .62, <italic>p</italic> &lt; .0001).</p>
<p>What is consistent is that the degree of congruence of the state standards with the CCSSM, either adjusted for the definition of the cut points or not, was related to performance on state NAEP. The inclusion of the assessment performance standard only makes the relationship stronger. For example, for the group of 37 states, taking into account the rigor of the cut points increased the percentage of variance accounted for by 12% (.20 vs. .32), with virtually identical estimated regression coefficients (.08 vs. .07). This suggests the importance of taking implementation into account when looking at the relationship of state standards and achievement. Furthermore, state policies related to assessment performance standards reduced the achievement gap between the two groups of states by just short of 12%.</p>
<p>Including the percentage of K–12 state funding that comes from the federal government and the percentage of nonpoor whites whose children are in the public schools together with the adjusted measure of congruence in the model was statistically significant (<italic>p</italic> &lt; .0001) and accounted for 67% of the variance over all 50 states. The estimated regression coefficient was .04 (<italic>p</italic> &lt; .016). Having the three factors hypothesized to be related to implementation in the same model resulted in a better-fitting model. These results further support the importance of the implemented curriculum—what content is actually covered in the states’ classrooms—in understanding the relationship of the standards and student achievement. However, the achievement gap, although reduced by about one-third, remains at around 10 points. Fitting this same model for all states but including the group designation (A vs. B) as the design variable in an ANCOVA (see <xref ref-type="table" rid="table4-0013189X12464517">Table 4</xref>) shows that the estimated difference between the two groups of states still remains although reduced by about one-half (<italic>p</italic> &lt; .0001 with an <italic>R</italic><sup>2</sup> = .78).</p>
</sec>
</sec>
<sec id="section12-0013189X12464517">
<title>Discussion</title>
<p>It is important to note the consistency between the Common Core State Standards for Mathematics and the internationally developed A+ standards. The measure of congruence on which this conclusion is based suggests that the CCSSM are both coherent and focused. They were also found to be rigorous as indicated by the consistency in topic coverage between the two sets of standards especially at eighth grade. As some have questioned the quality of the CCSSM, having data comparing them to an international benchmark developed from countries whose students perform well in international comparisons becomes important. Having substantiated their coherence and focus, it is important to now move to the important task of implementing them.</p>
<p>That task will be quite difficult, as the comparison of the coherence and focus of the CCSSM to previous state standards shows—the range in consistency going from around 60% to 80%. For some states, the road to travel is shorter, but for others it is very long. Two major types of problems emerged—“sins of omission”—by leaving out topics at a grade for which the CCSSM calls for their inclusion, and “sins of commission”—by covering topics at grades for which they are not intended in the CCSSM. Both types are present to differing degrees over the 50 states.</p>
<p>For some states, one of the two types of inconsistencies was more prevalent. Those furthest from congruence with the CCSSM seem to be there mostly because of covering more advanced topics too early, contributing not only to a lack of focus but to incoherence as well. Such topics are covered to a lesser depth than they should be primarily because of the lack of coverage of proper prerequisites. This perhaps reflects a “spiraled” curriculum in which topics are introduced in a very elementary form and built up in conceptual complexity over the grades. Congruence and similarity of geometric shapes is a good example.</p>
<p>An exploratory analysis of the likelihood that the CCSSM will actually improve the mathematics achievement of U.S. children once appropriately implemented is encouraging, if rather tentative. To interpret this as an indication of the likelihood that the CCSSM will be related to higher achievement requires adherence to a line of not unreasonable assumptions, but they are assumptions. The totality of the multiple analyses we have done suggests a statistically significant positive relationship between the degree of congruence between a state’s standards and the CCSSM and achievement as defined by the 2009 NAEP assessment, but is only an indication of correlation not of causality. On the other hand, combining these analyses with the strong degree of consistency that the CCSSM have with those of the countries whose eighth-grade students achieve at the highest levels, makes the likelihood of such a relationship even greater.</p>
<p>The relationship to 2009 NAEP achievement holds both for the group of 37 states and the group of 13 whether the analyses were done separately, pooled across the two groups, or as a single set of 50 states. In fact, the coefficient characterizing the relationship remained quite stable no matter the analysis. The relationship is even stronger when taking into account the likelihood of implementation as reflected by state policies related to setting the cut point of the state assessments and by key state demographics as related to the prevalence of low-income students attending the state’s public schools. As states are now moving to implement the CCSSM and move “closer” to the CCSSM, this implies a “hopeful” relationship.</p>
<p>The other intriguing question that remains unanswered is, What makes the 13 states so distinct? Those states below average on NAEP but above average in terms of consistency with the CCSSM, have essentially the same relationship of coherence and focus as defined by the CCSSM and student achievement. However, that relationship exists at a lower base level of achievement. Our hypothesis, and somewhat supported by the data, is that the 13 states had highly challenging standards that were poorly implemented. This occurs in part because of low definitions of proficiency on the state assessments, high levels of poverty, the absence of a critical mass of middle- to upper-class white children attending the public schools, and a low level of per capita personal income, all of which makes implementing challenging standards even more difficult to accomplish. Although this analysis was significant and reduced the achievement gap by almost one third, it is still the case that we do not fully understand the achievement gap. The relationship of the coherence of the states’ standards (defined relative to the CCSSM) and student achievement is the same but occurs in the presence of a persistent achievement gap between the two groups of states.</p>
<p>What is clear to us at least is that the new Common Core State Standards for Mathematics deserve to be seriously implemented. The consistency of them with the benchmark derived from standards of the top-achieving countries suggests that the goal of the authors’ that the CCSSM be consistent with the internationally benchmarked standards and as a result are coherent, focused, and rigorous has been achieved. One can quibble about some details, but the pattern is clear. The relationship of performance on state NAEP in 2009, and the closeness of the state standards in effect around 2009 to those of the new CCSSM is also encouraging.</p>
<p>It seems to us that it is time to stop debating their quality and to move to assuring that they define content coverage at the classroom level—that is, what is actually being taught and to all children. The evidence presented in this article seems, at least to the authors, to offer a vision of what can be. To not move in that direction and to continue to debate the issue is a mistake our children call ill afford.</p>
</sec>
</body>
<back>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0013189X12464517">
<label>1</label>
<p>Including all of the topics covered by at least one half of the countries so as to avoid choosing among topics results in a congruence value of 828 even though this results in more topics being intended than would be the case on average for the A+ countries.</p>
</fn>
</fn-group>
</notes>
<bio>
<title>Authors</title>
<p><bold>William H. Schmidt</bold> is a University Distinguished Professor at Michigan State University, College of Education, 238 Erickson Hall, East Lansing, Michigan 48824-1034; <italic><email>bschmidt@msu.edu</email></italic>. His research focuses on issues of academic content in K–12 schooling, assessment theory and the effects of curriculum on academic achievement, as well as educational policy related to mathematics, science, and testing in general.</p>
<p><bold>Richard T. Houang</bold> is the Director of Research for the Center of Study of Curriculum at Michigan State University, Room 230, College of Education, 620 Farm Lane, East Lansing, MI 48824; <italic><email>houang@msu.edu</email></italic>. His research focuses on methodologies in quantifying Mathematics and Science curriculum and relationships between curriculum and student achievement.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bandeira de Mello</surname><given-names>V.</given-names></name>
</person-group> (<year>2011</year>). <source>Mapping state proficiency standards onto the NAEP scales: Variation and change in state standards for reading and mathematics, 2005–2009</source> (NCES 2011-458). <collab>National Center for Education Statistics, Institute of Education Sciences, U.S. Department of Education</collab>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Government Printing Office</publisher-name>.</citation>
</ref>
<ref id="bibr2-0013189X12464517">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bruner</surname><given-names>J. S.</given-names></name>
</person-group> (<year>1995</year>). <article-title>On learning mathematics</article-title>. <source>Mathematics Teacher</source>, <volume>88</volume>, <fpage>330</fpage>–<lpage>335</lpage>.</citation>
</ref>
<ref id="bibr3-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Carmichael</surname><given-names>S. B.</given-names></name>
<name><surname>Martino</surname><given-names>G.</given-names></name>
<name><surname>Porter-Magee</surname><given-names>K.</given-names></name>
<name><surname>Wilson</surname><given-names>W. S.</given-names></name>
</person-group> (with <person-group person-group-type="author">
<name><surname>Fairchild</surname><given-names>D.</given-names></name>
<name><surname>Haydel</surname><given-names>E.</given-names></name>
<name><surname>Senechal</surname><given-names>D.</given-names></name>
<name><surname>Winkler M.</surname><given-names>A.</given-names></name>
</person-group>). (<year>2010</year>). <source>The State of State Standards—and the Common Core—in 2010</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Thomas B. Fordham Institute</publisher-name>.</citation>
</ref>
<ref id="bibr4-0013189X12464517">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cobb</surname><given-names>P.</given-names></name>
<name><surname>Jackson</surname><given-names>K.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Assessing the quality of the Common Core State Standards for Mathematics</article-title>. <source>Educational Researcher</source>, <volume>40</volume>, <fpage>183</fpage>–<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr5-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Loveless</surname><given-names>T.</given-names></name>
</person-group> (<year>2012</year>). <source>The 2012 Brown Center Report on American education: How well are American Students Learning?</source> <collab>With sections on predicting the effect of the Common Core State Standards, achievement gaps on the two NAEP tests, and misinterpreting international test scores</collab>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>The Brookings Institution</publisher-name>.</citation>
</ref>
<ref id="bibr6-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Milgram</surname><given-names>R. J.</given-names></name>
<name><surname>Stotsky</surname><given-names>S.</given-names></name>
</person-group> (<year>2010</year>). <source>Fair to middling: A national standards progress report</source> (Pioneer Institute White Paper No. 56). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Pioneer Institute</publisher-name>.</citation>
</ref>
<ref id="bibr7-0013189X12464517">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>A. C.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Measuring the content of instruction: Uses in research and practice</article-title>. <source>Educational Researcher</source>, <volume>31</volume>(<issue>7</issue>), <fpage>3</fpage>–<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr8-0013189X12464517">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>A.</given-names></name>
</person-group> (<year>2011</year>, <month>August</month>). <article-title>In Common Core, little to cheer about</article-title>. <source>Education Week</source>, <volume>30</volume>(<issue>37</issue>), <fpage>24</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr9-0013189X12464517">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>A.</given-names></name>
<name><surname>McMaken</surname><given-names>J.</given-names></name>
<name><surname>Hwang</surname><given-names>J.</given-names></name>
<name><surname>Yang</surname><given-names>R.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Common core standards: The new U.S. intended curriculum</article-title>. <source>Educational Researcher</source>, <volume>40</volume>, <fpage>103</fpage>–<lpage>116</lpage>.</citation>
</ref>
<ref id="bibr10-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>Houang</surname><given-names>R. T.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Lack of focus in the mathematics curriculum: Symptom or cause?</article-title> In <person-group person-group-type="editor">
<name><surname>Loveless</surname><given-names>T.</given-names></name>
</person-group> (Ed.), <source>Lessons learned: What international assessments tell us about math achievement</source> (pp. <fpage>65</fpage>–<lpage>84</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Brookings Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>Houang</surname><given-names>R. T.</given-names></name>
<name><surname>Shakrani</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <source>International lessons about national standards</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>The Thomas B. Fordham Institute</publisher-name>.</citation>
</ref>
<ref id="bibr12-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>Maier</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Opportunity to learn</article-title>. In <person-group person-group-type="editor">
<name><surname>Sykes</surname><given-names>G.</given-names></name>
<name><surname>Schneider</surname><given-names>B.</given-names></name>
<name><surname>Plank</surname><given-names>D. N.</given-names></name>
</person-group> (Eds.), <source>Handbook of education policy research</source> (pp. <fpage>541</fpage>–<lpage>559</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Rutledge for American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr13-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>McKnight</surname><given-names>C. C.</given-names></name>
<name><surname>Houang</surname><given-names>R. T.</given-names></name>
<name><surname>Wang</surname><given-names>H. A.</given-names></name>
<name><surname>Wiley</surname><given-names>D. E.</given-names></name>
<name><surname>Cogan</surname><given-names>L. S.</given-names></name>
<name><surname>Wolfe</surname><given-names>R. G.</given-names></name>
</person-group> (<year>2001</year>). <source>Why schools matter: A cross-national comparison of curriculum and learning</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr14-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>McKnight</surname><given-names>C.</given-names></name>
<name><surname>Raizen</surname><given-names>S.</given-names></name>
</person-group> (<year>1997</year>). <article-title>A splintered vision: An investigation of U.S. science and mathematics education</article-title>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer</publisher-name>.</citation>
</ref>
<ref id="bibr15-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>McKnight</surname><given-names>C.</given-names></name>
<name><surname>Valverde</surname><given-names>G. A.</given-names></name>
<name><surname>Houang</surname><given-names>R. T.</given-names></name>
<name><surname>Wiley</surname><given-names>D. E.</given-names></name>
</person-group> (<year>1997</year>). <source>Many visions, many aims, Volume I: A cross-national investigation of curricular intentions in school mathematics</source>. <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer</publisher-name>.</citation>
</ref>
<ref id="bibr16-0013189X12464517">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schmidt</surname><given-names>W. H.</given-names></name>
<name><surname>Wang</surname><given-names>H. C.</given-names></name>
<name><surname>McKnight</surname><given-names>C. C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Curriculum coherence: An examination of U.S. mathematics and science content standards from an international perspective</article-title>. <source>Journal of Curriculum Studies</source>, <volume>35</volume>, <fpage>528</fpage>–<lpage>529</lpage>.</citation>
</ref>
<ref id="bibr17-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schwille</surname><given-names>J. R.</given-names></name>
<name><surname>Porter</surname><given-names>A. C.</given-names></name>
<name><surname>Belli</surname><given-names>G.</given-names></name>
<name><surname>Floden</surname><given-names>R. E.</given-names></name>
<name><surname>Freeman</surname><given-names>D. J.</given-names></name>
<name><surname>Knappen</surname><given-names>L. B.</given-names></name>
<name><surname>. . . Schmidt</surname><given-names>W. H.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Teachers as policy brokers in the content of elementary school mathematics</article-title>. In <person-group person-group-type="editor">
<name><surname>Shulman</surname><given-names>L.</given-names></name>
<name><surname>Sykes</surname><given-names>G.</given-names></name>
</person-group> (Eds.), <source>Handbook of teaching and policy</source> (pp. <fpage>370</fpage>–<lpage>391</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Longman</publisher-name>.</citation>
</ref>
<ref id="bibr18-0013189X12464517">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stotsky</surname><given-names>S.</given-names></name>
<name><surname>Wurman</surname><given-names>Z.</given-names></name>
</person-group> (<year>2010</year>). <source>Common Core’s Standards still don’t make the grade: Why Massachusetts and California must regain control over their academic destinies</source> (Pioneer Institute White Paper No. 65). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Pioneer Institute</publisher-name>.</citation>
</ref>
<ref id="bibr19-0013189X12464517">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Whitehurst</surname><given-names>R.</given-names></name>
</person-group> (<year>2009</year>). <source>Don’t forget the curriculum</source> (Brown Center Letters on Education No. 3). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>The Brookings Institution</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.brookings.edu/papers/2009/1014_curriculum_whitehurst.aspx">http://www.brookings.edu/papers/2009/1014_curriculum_whitehurst.aspx</ext-link></citation>
</ref>
<ref id="bibr20-0013189X12464517">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Will</surname><given-names>G. F.</given-names></name>
</person-group> (<year>2012</year>, <month>March</month> <day>9</day>). <article-title>Those pesky things called laws [Opinion]</article-title>. <source>The Washington Post</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.washingtonpost.com">http://www.washingtonpost.com</ext-link></citation>
</ref>
<ref id="bibr21-0013189X12464517">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wurman</surname><given-names>Z.</given-names></name>
<name><surname>Evers</surname><given-names>B.</given-names></name>
</person-group> (<year>2010</year>, <month>July</month> <day>30</day>). <article-title>National standards would harm math curriculum</article-title>. <source>San Francisco Chronicle</source>, p. <fpage>A14</fpage>.</citation>
</ref>
</ref-list>
</back>
</article>