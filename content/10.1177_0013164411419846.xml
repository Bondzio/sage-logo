<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPM</journal-id>
<journal-id journal-id-type="hwp">spepm</journal-id>
<journal-title>Educational and Psychological Measurement</journal-title>
<issn pub-type="ppub">0013-1644</issn>
<issn pub-type="epub">1552-3888</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0013164411419846</article-id>
<article-id pub-id-type="publisher-id">10.1177_0013164411419846</article-id>
<title-group>
<article-title>Using IRT Trait Estimates Versus Summated Scores in Predicting Outcomes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Xu</surname><given-names>Ting</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411419846">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Stone</surname><given-names>Clement A.</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411419846">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0013164411419846"><label>1</label>University of Pittsburgh, Pittsburgh, PA, USA</aff>
<author-notes>
<corresp id="corresp1-0013164411419846">Ting Xu, Department of Psychology in Education, University of Pittsburgh, 5500 Wesley W. Posvar Hall, 230 S. Bouquet St., Pittsburgh, PA 15260, USA Email: <email>tix3@pitt.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2012</year>
</pub-date>
<volume>72</volume>
<issue>3</issue>
<fpage>453</fpage>
<lpage>468</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>It has been argued that item response theory trait estimates should be used in analyses rather than number right (NR) or summated scale (SS) scores. Thissen and Orlando postulated that IRT scaling tends to produce trait estimates that are linearly related to the underlying trait being measured. Therefore, IRT trait estimates can be more useful than summated scores when examining relationships between test scores and external variables. Also, when the model holds, IRT trait estimates possess an interval scale that is a property assumed for dependent variables by most statistical procedures used in educational research. The objective of this study was to use Monte Carlo methods to compare the performance of IRT trait estimates and SS scores in predicting outcome variables in the context of health and behavioral assessment. The use of scores based on the graded-response model versus summated scores was compared. Results indicated that IRT-based scores and summated scores are comparable when evaluating the relationships between test scores and outcome measures. Thus, applied researchers could use summated scores in predictive studies and circumvent evaluating the assumptions underlying use of IRT-based scores.</p>
</abstract>
<kwd-group>
<kwd>CTT</kwd>
<kwd>IRT</kwd>
<kwd>predictive validity</kwd>
<kwd>summated scores</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0013164411419846" sec-type="intro">
<title>Introduction</title>
<p>The study of relationships between estimates of individuals’ traits or behaviors and outcome variables are prevalent in health and behavioral sciences research. The estimates of the traits or behaviors are typically derived from set(s) of items with ordinal response scales (e.g., Likert items). Based on classical test theory (CTT), item responses for a scale or subscale are aggregated or summed to obtain scale or subscale scores (i.e., summated scores), and these scores may be used to assess the relationship between the test scores and outcome measures. Alternatively, item response theory (IRT) models can be used to scale the items and obtain estimates of the underlying trait being measured. These trait estimates may then be used to assess the relationship between test scores and outcome measures. However, the literature does not clearly indicate which type of scores should be used in practice by researchers.</p>
<p>Some researchers have discussed that the two types of scores are not always linearly related (<xref ref-type="bibr" rid="bibr9-0013164411419846">Harwell &amp; Gatti, 2001</xref>; <xref ref-type="bibr" rid="bibr20-0013164411419846">Thissen, Nelson, Rosa, &amp; McLeod, 2001</xref>). Specifically, summated scores are usually observed to be linearly related to the IRT trait estimates under the Rasch model but are less similar to the trait estimates obtained under other commonly used IRT models such as the graded-response (GR) model (<xref ref-type="bibr" rid="bibr9-0013164411419846">Harwell &amp; Gatti, 2001</xref>) or the combined three-parameter logistic and GR model (<xref ref-type="bibr" rid="bibr20-0013164411419846">Thissen et al., 2001</xref>).</p>
<p><xref ref-type="bibr" rid="bibr21-0013164411419846">Thissen and Orlando (2001)</xref> further discussed that since IRT scaling tends to produce trait estimates that are linearly related to the underlying trait being measured, IRT trait estimates may be more useful than CTT-based scores when examining the linear relationship between test scores and external variables (e.g., outcome measures). Theoretically, if the underlying trait being measured is linearly related to an external variable, the IRT trait estimates also have a linear relationship with that variable. Therefore, CTT-based scores may be less accurate in predicting an outcome measure. <xref ref-type="bibr" rid="bibr21-0013164411419846">Thissen and Orlando (2001)</xref> also discussed that a nonlinear relationship between summated scores and the underlying trait typically results from ceiling and/or floor effects, which can be accounted for by IRT scaling.</p>
<p><xref ref-type="bibr" rid="bibr9-0013164411419846">Harwell and Gatti (2001)</xref> argued that there are also theoretical advantages to IRT trait estimates over CTT-based (number right [NR] or summated scale [SS]) scores. NR and SS scores are essentially a summation of ordinally scaled variables and therefore do not possess an interval scale. There are two problems with the use of these scores in statistical analyses such as regression analyses: nonnormality and “incoherence” between the test score scale and the latent trait scale (<xref ref-type="bibr" rid="bibr9-0013164411419846">Harwell &amp; Gatti, 2001</xref>). Both problems may yield biased parameter estimates and misleading results. Alternatively, IRT can be used as a scaling model to transform item responses to trait estimates that possess an interval scale (<xref ref-type="bibr" rid="bibr9-0013164411419846">Harwell &amp; Gatti, 2001</xref>). If the IRT model is appropriate for the data, it is reasonable to expect that using IRT trait estimates would lead to improved prediction of outcome variables. <xref ref-type="bibr" rid="bibr7-0013164411419846">Ferrando and Chico (2007)</xref> further argued that maximum likelihood (ML) trait estimates based on IRT models better reflect the characteristics of test items and therefore provide more test information than either unweighted or weighted raw scores. The increased test information afforded by IRT models versus CTT applications implies the possibility of greater accuracy or precision in trait estimates versus CTT-based scores.</p>
<p>Finally, IRT models offer more flexibility in scaling sets of items with the same response scale (summated scales). By summing item responses to create total or subscale scores, the CTT model weights each item equally, an assumption necessary to estimate internal consistency reliability for a scale (<xref ref-type="bibr" rid="bibr11-0013164411419846">Li &amp; Wainer, 1998</xref>; <xref ref-type="bibr" rid="bibr17-0013164411419846">Raykov, 1997</xref>). Equal weighting of items also implies each item is equally related to the underlying trait being measured. Last, since the same response scale is used across the set of items, the CTT model assumes the response options have the same meaning across the set of items. Alternatively, a variety of IRT models may be applied to summated scales that address some of these constraints. The rating scale model discussed by <xref ref-type="bibr" rid="bibr1-0013164411419846">Andrich (1978)</xref> most closely approximates the summated score approach used with health and behavioral assessments. This model incorporates a common discrimination parameter and a common response scale across the set of items. Other models can be estimated that relax the assumption of a common discrimination parameter and/or relax the assumption of a common response scale (e.g., GR IRT model). The different IRT models can be estimated and compared to evaluate the extent to which these assumptions are met. Note that relaxing the assumption of a common discrimination parameter is equivalent to the case where factor loadings vary across items measuring the same trait.</p>
<p>Despite the theoretical advantages attributed to IRT traits estimates over CTT-based scores, these advantages are only derived if the IRT model is appropriate. IRT models are based on several assumptions (e.g., dimensionality, functional form of the model, and local independence), and these assumptions must be evaluated to validate the IRT application. This complicates the application of IRT models to summated scales since applied researchers must be familiar with methods used to evaluate the underlying assumptions. This is in contrast to the use of CTT-based scores since the “correctness” of the CTT model and inferences based on the use of raw scores is generally not evaluated (<xref ref-type="bibr" rid="bibr7-0013164411419846">Ferrando &amp; Chico, 2007</xref>).</p>
<p>Other arguments for the use of CTT-based scores are rooted in the scaling properties of summated scales and robustness of statistical analyses using summated scale scores. While each item (e.g., Likert item) reflects an ordinal scale, researchers have discussed that the aggregation of these items into a scale score measuring a construct may yield or closely approximate an interval scaled score (e.g., <xref ref-type="bibr" rid="bibr2-0013164411419846">Carifio &amp; Perla, 2007</xref>). Furthermore, many analyses using summated or raw scores are robust to the assumption of interval data (<xref ref-type="bibr" rid="bibr8-0013164411419846">Glass, Peckman, &amp; Sanders, 1972</xref>), although other researchers have discussed that the use of summated scores from Likert items can influence the assessment of relationships (e.g., <xref ref-type="bibr" rid="bibr18-0013164411419846">Russell &amp; Bobko, 1992</xref>).</p>
<p>To address the contradictory recommendations surrounding the use of CTT- versus IRT-based scores, previous research has compared parameter estimates from IRT- and CTT-based applications and found that IRT trait estimates and CTT scores are highly correlated for tests composed of dichotomous items (e.g., <xref ref-type="bibr" rid="bibr5-0013164411419846">Fan, 1998</xref>; <xref ref-type="bibr" rid="bibr10-0013164411419846">Lawson, 1991</xref>; <xref ref-type="bibr" rid="bibr12-0013164411419846">MacDonald &amp; Pauonen, 2002</xref>; <xref ref-type="bibr" rid="bibr16-0013164411419846">Ndalichako &amp; Rogers, 1997</xref>). For example, based on the data from the Texas Assessment of Academic Skills tests, <xref ref-type="bibr" rid="bibr5-0013164411419846">Fan (1998)</xref> compared the performance of the NR scores under CTT and the IRT scores under the one-, two-, and three-parameter IRT models for low and high ability groups, male and female samples, as well as random samples. The results indicated that the correlations between the two ability measures were very high (&gt;.97) under most conditions that were studied.</p>
<p>The focus of prior research was on the relative accuracy of the IRT- and CTT-based parameter estimates. However, there is a lack of empirical studies evaluating the robustness of using IRT trait estimates versus CTT-based scores in prediction studies. The results of a few empirical studies found that validity coefficients based on IRT trait estimates were not consistently better than those of CTT-based scores (<xref ref-type="bibr" rid="bibr3-0013164411419846">Dodd, Koch, &amp; De Ayala, 1989</xref>; <xref ref-type="bibr" rid="bibr6-0013164411419846">Ferrando, 1999</xref>; <xref ref-type="bibr" rid="bibr13-0013164411419846">McBride &amp; Martin, 1983</xref>; <xref ref-type="bibr" rid="bibr22-0013164411419846">Young, 1995</xref>). For example, <xref ref-type="bibr" rid="bibr13-0013164411419846">McBride and Martin (1983)</xref> found that the use of IRT ability estimates was associated with slightly higher validity coefficients than the use of NR scores when the number of items was equal to or smaller than 10. They also found that validity coefficients based on IRT scores were no better than those based on NR scores when the number of items was more than 30. <xref ref-type="bibr" rid="bibr3-0013164411419846">Dodd et al. (1989)</xref> compared the validity coefficients from IRT scores and those from CTT-based scores for a test composed of both multiple choice and essay items. The results showed that the IRT approach yielded slightly lower validity coefficients when compared with the CTT approaches. Finally, <xref ref-type="bibr" rid="bibr7-0013164411419846">Ferrando and Chico (2007)</xref> examined the external validity of scores based on the two-parameter logistic (2PL) model. Validity coefficients based on IRT trait estimates (2PL) were found to be similar to those based on NR scores. These researchers also found that the presence of “strong” floor or ceiling effects may provide an advantage to analyses using IRT-based scores since nonlinearity in the relationships exists in the tails of the score distributions. They suggested that further studies should be conducted and consider other factors such as different item distributions and different IRT models (e.g., GR models).</p>
<p>The purpose of this study was to extend previous research and evaluate the use of IRT- versus CTT-based scores in the context of health and behavioral assessments that incorporate polytomous items. Little research has been conducted examining the relationship between IRT- and CTT-based scores for tests based on these types of applications. Furthermore, much of the previous research has involved comparisons based on real data applications. The present study used simulation methods to compare the accuracy of the different scores, as well as the recovery of predicted outcomes or validity coefficients based on IRT trait estimates versus SS scores. Conditions were introduced in the study to evaluate the robustness of the different trait estimates under violations of CTT model assumptions: (a) equal item weights across items and (b) constant response scale across items. The advantages of a simulation study approach are that the true relationships between variables are known and factors that could affect the methods can be manipulated directly.</p>
</sec>
<sec id="section2-0013164411419846" sec-type="methods">
<title>Method</title>
<p>The manipulated factors of the simulation study included the following: (a) the number of items: 10, 20, and 40; (b) correlation of scores with the outcome measure: 0.3, 0.4, and 0.6; (c) range in item slopes: Uniform [1.2, 2.2] and Uniform [0.5, 2.9]; and (d) sample size: 250, 500, and 1,000. Number of items was manipulated since the number of items affects the reliability or precision of scores and this factor has been found to affect the comparison of IRT- and CTT-based scores (e.g., <xref ref-type="bibr" rid="bibr7-0013164411419846">Ferrando &amp; Chico, 2007</xref>). The range of item slopes was varied to relax the assumption that items were equally related to the trait being measured. Response scales for items were fixed at five categories to reflect scales typically used in health and behavioral assessments.</p>
<p>Trait estimates and outcome measures were generated from a standard bivariate normal distribution based on the specified correlation values. For the five-category response items, the GR model (<xref ref-type="bibr" rid="bibr19-0013164411419846">Samejima, 1969</xref>) was used to simulate ordinally scaled item responses:</p>
<p><disp-formula id="disp-formula1-0013164411419846">
<mml:math display="block" id="math1-0013164411419846">
<mml:mrow>
<mml:msubsup>
<mml:mi>P</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mo>*</mml:mo>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>θ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mi>exp</mml:mi>
<mml:mo stretchy="false">[</mml:mo>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>θ</mml:mi>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">]</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:mi>exp</mml:mi>
<mml:mo stretchy="false">[</mml:mo>
<mml:msub>
<mml:mi>α</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>θ</mml:mi>
<mml:mo>−</mml:mo>
<mml:msub>
<mml:mi>b</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">]</mml:mo>
</mml:mrow>
</mml:mfrac>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0013164411419846" xlink:href="10.1177_0013164411419846-eq1.tif"/>
</disp-formula></p>
<p>where <italic>x</italic> represents a response in or above a given category (<italic>x</italic> = <italic>j</italic> = 1, . . ., <italic>m<sub>i</sub></italic>), <italic>m</italic> is the number of thresholds, θ is the person trait parameter, α<sub><italic>i</italic></sub> is the slope parameter for item <italic>i</italic>, and <italic>b<sub>ij</sub></italic> is the threshold parameter for item <italic>i</italic> and response category <italic>j</italic>.</p>
<p>For the varying item slope condition, item slope values were randomly chosen from a distribution that reflected a wide range—Uniform [0.5, 2.9]. This condition yielded items that varied extensively in terms of their association with the underlying trait being measured and reflected a condition that violated the assumption of equal item weights underlying CTT-based scores. For the “constant” item slope condition, item slopes were randomly varied based on a narrow distribution—Uniform [1.2, 2.2]. Slopes were randomly varied over a narrow range as opposed to fixed at a common value to reflect more realistic conditions.</p>
<p>With regard to the threshold parameters for the GR model, the first threshold parameter for the GR model was obtained from a Uniform [−2.5, 0] distribution, and subsequent threshold parameters were obtained by adding a constant from the set [0.5, 1, 1.5]. The value of the constant was randomly determined based on the discrete probability distribution [1/3, 1/3, 1/3].</p>
<p><xref ref-type="table" rid="table1-0013164411419846">Table 1</xref> presents an example set of randomly generated item slope threshold parameters under the GR model. To illustrate the different response scales for the different items, these parameters were used to simulate item responses and a GR model was estimated in PARSCALE (<xref ref-type="bibr" rid="bibr15-0013164411419846">Muraki &amp; Bock, 1997</xref>). The model estimated in PARSCALE reflects a slightly different parameterization, where <italic>b<sub>ij</sub></italic> under the GR model is decomposed into <italic>b<sub>j</sub></italic> − <italic>c<sub>jk</sub></italic>. Under this parameterization, <italic>b<sub>j</sub></italic> is an item threshold parameter and <italic>c<sub>jk</sub></italic> is a category parameter for item <italic>j</italic> and category <italic>k</italic>. This alternative parameterization allows for isolating the item location along the trait scale from the category parameters associated with the response scale. The parameter estimates from PARSCALE for the constant slope condition are presented in <xref ref-type="table" rid="table2-0013164411419846">Table 2</xref>. As can be seen, the category parameter estimates, although similar for some items, vary considerably across the set of items.</p>
<table-wrap id="table1-0013164411419846" position="float">
<label>Table 1.</label>
<caption><p>Simulated Item Parameter Values for a Condition With Test Length of 10</p></caption>
<graphic alternate-form-of="table1-0013164411419846" xlink:href="10.1177_0013164411419846-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Item</th>
<th align="center">α</th>
<th align="center">β<sub>1</sub></th>
<th align="center">β<sub>2</sub></th>
<th align="center">β<sub>3</sub></th>
<th align="center">β<sub>4</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Varying slopes</td>
</tr>
<tr>
<td> 1</td>
<td>1.06</td>
<td>−2.36</td>
<td>−1.86</td>
<td>−1.36</td>
<td>−0.86</td>
</tr>
<tr>
<td> 2</td>
<td>2.67</td>
<td>−1.18</td>
<td>−0.68</td>
<td>−0.18</td>
<td>0.32</td>
</tr>
<tr>
<td> 3</td>
<td>2.23</td>
<td>−2.19</td>
<td>−0.69</td>
<td>0.81</td>
<td>2.31</td>
</tr>
<tr>
<td> 4</td>
<td>2.05</td>
<td>−1.07</td>
<td>−0.07</td>
<td>0.93</td>
<td>1.93</td>
</tr>
<tr>
<td> 5</td>
<td>0.98</td>
<td>−1.04</td>
<td>−0.04</td>
<td>0.96</td>
<td>1.96</td>
</tr>
<tr>
<td> 6</td>
<td>2.65</td>
<td>−1.92</td>
<td>−1.42</td>
<td>−0.92</td>
<td>−0.42</td>
</tr>
<tr>
<td> 7</td>
<td>2.40</td>
<td>−1.97</td>
<td>−1.47</td>
<td>−0.97</td>
<td>−0.47</td>
</tr>
<tr>
<td> 8</td>
<td>1.19</td>
<td>−2.36</td>
<td>−1.36</td>
<td>−0.36</td>
<td>0.64</td>
</tr>
<tr>
<td> 9</td>
<td>2.25</td>
<td>−1.36</td>
<td>−0.36</td>
<td>0.64</td>
<td>1.64</td>
</tr>
<tr>
<td> 10</td>
<td>1.08</td>
<td>−2.41</td>
<td>−1.91</td>
<td>−1.41</td>
<td>−0.91</td>
</tr>
<tr>
<td colspan="6">Constant slopes</td>
</tr>
<tr>
<td> 1</td>
<td>2.02</td>
<td>−2.19</td>
<td>−1.69</td>
<td>−1.19</td>
<td>−0.69</td>
</tr>
<tr>
<td> 2</td>
<td>1.77</td>
<td>−0.32</td>
<td>0.18</td>
<td>0.68</td>
<td>1.18</td>
</tr>
<tr>
<td> 3</td>
<td>1.72</td>
<td>−0.53</td>
<td>0.47</td>
<td>1.47</td>
<td>2.47</td>
</tr>
<tr>
<td> 4</td>
<td>1.29</td>
<td>−2.06</td>
<td>−1.07</td>
<td>−0.07</td>
<td>0.93</td>
</tr>
<tr>
<td> 5</td>
<td>1.51</td>
<td>−1.80</td>
<td>−1.30</td>
<td>−0.80</td>
<td>−0.30</td>
</tr>
<tr>
<td> 6</td>
<td>1.52</td>
<td>−2.25</td>
<td>−1.25</td>
<td>−0.25</td>
<td>0.75</td>
</tr>
<tr>
<td> 7</td>
<td>1.21</td>
<td>−2.49</td>
<td>−1.49</td>
<td>−0.49</td>
<td>0.51</td>
</tr>
<tr>
<td> 8</td>
<td>2.14</td>
<td>−0.03</td>
<td>0.47</td>
<td>0.97</td>
<td>1.47</td>
</tr>
<tr>
<td> 9</td>
<td>2.13</td>
<td>−2.21</td>
<td>−1.21</td>
<td>−0.21</td>
<td>0.79</td>
</tr>
<tr>
<td> 10</td>
<td>1.49</td>
<td>−1.42</td>
<td>−0.42</td>
<td>0.58</td>
<td>1.58</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table2-0013164411419846" position="float">
<label>Table 2.</label>
<caption><p>Estimated Item Parameter Values From PARSCALE for a Constant Slope Condition With Test Length of 10</p></caption>
<graphic alternate-form-of="table2-0013164411419846" xlink:href="10.1177_0013164411419846-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Item</th>
<th align="center">α</th>
<th align="center"><italic>b</italic></th>
<th align="center"><italic>c</italic><sub>1</sub></th>
<th align="center"><italic>c</italic><sub>2</sub></th>
<th align="center"><italic>c</italic><sub>3</sub></th>
<th align="center"><italic>c</italic><sub>4</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1.49</td>
<td>−1.21</td>
<td>0.74</td>
<td>0.31</td>
<td>−0.27</td>
<td>−0.77</td>
</tr>
<tr>
<td>2</td>
<td>1.47</td>
<td>0.68</td>
<td>1.43</td>
<td>0.52</td>
<td>−0.45</td>
<td>−1.49</td>
</tr>
<tr>
<td>3</td>
<td>1.54</td>
<td>0.17</td>
<td>2.10</td>
<td>0.63</td>
<td>−0.72</td>
<td>−2.01</td>
</tr>
<tr>
<td>4</td>
<td>1.54</td>
<td>−1.70</td>
<td>0.67</td>
<td>0.26</td>
<td>−0.22</td>
<td>−0.71</td>
</tr>
<tr>
<td>5</td>
<td>1.97</td>
<td>0.17</td>
<td>2.19</td>
<td>0.79</td>
<td>−0.74</td>
<td>−2.23</td>
</tr>
<tr>
<td>6</td>
<td>1.61</td>
<td>0.16</td>
<td>0.78</td>
<td>0.25</td>
<td>−0.26</td>
<td>−0.77</td>
</tr>
<tr>
<td>7</td>
<td>2.22</td>
<td>−0.37</td>
<td>1.45</td>
<td>0.49</td>
<td>−0.44</td>
<td>−1.49</td>
</tr>
<tr>
<td>8</td>
<td>2.15</td>
<td>0.01</td>
<td>0.79</td>
<td>0.25</td>
<td>−0.26</td>
<td>−0.78</td>
</tr>
<tr>
<td>9</td>
<td>2.23</td>
<td>−1.08</td>
<td>0.67</td>
<td>0.26</td>
<td>−0.22</td>
<td>−0.70</td>
</tr>
<tr>
<td>10</td>
<td>1.66</td>
<td>−0.51</td>
<td>0.80</td>
<td>0.21</td>
<td>−0.26</td>
<td>−0.74</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Note that item responses based on a common response scale across items could also be simulated with a rating scale model to compare the results with the model reflecting different response scales. However, as will be presented, the use of CTT-based scores was very robust to the condition where different response scales were simulated. Thus, it was not necessary to compare results under a condition where a common response scale was simulated.</p>
<p>Using the item and ability parameters, item responses were simulated and analyzed using MULTILOG to obtain IRT-based ML trait estimates (<inline-formula id="inline-formula1-0013164411419846">
<mml:math display="inline" id="math2-0013164411419846">
<mml:mover accent="true">
<mml:mi>θ</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:math>
</inline-formula>). SS scores were also obtained for each simulated case. Regression coefficients and predicted values for the outcome variable were then estimated. Note that simulated and estimated predictor and the outcome variables were all standardized (<italic>z</italic>). Therefore, estimated regression coefficients were equivalent to correlations between predictor and outcome measures. For each combination of conditions, 500 replications were performed.</p>
<p>The outcome measures included root mean square deviations (RMSDs) comparing predicted values for the outcome measure (<italic>z</italic>-score estimates) versus true values (<italic>z</italic> scores), and these values were calculated at high- (<inline-formula id="inline-formula2-0013164411419846">
<mml:math display="inline" id="math3-0013164411419846">
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mover accent="true">
<mml:mi>θ</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo stretchy="false">/</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo>&gt;</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>), middle- (<inline-formula id="inline-formula3-0013164411419846">
<mml:math display="inline" id="math4-0013164411419846">
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>≤</mml:mo>
<mml:mover accent="true">
<mml:mi>θ</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo stretchy="false">/</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo>≤</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>), and low-trait (<inline-formula id="inline-formula4-0013164411419846">
<mml:math display="inline" id="math5-0013164411419846">
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mover accent="true">
<mml:mi>θ</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo stretchy="false">/</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo>&lt;</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>) levels. Comparisons were made separately for the three groups to evaluate performance in the tails of the distribution where any nonlinearity in the relationship between the score and the latent trait would likely be observed. RMSDs comparing estimated regression coefficients with true values were also calculated. Note that although <xref ref-type="bibr" rid="bibr19-0013164411419846">Samejima’s (1969)</xref> GR model was used in the current study, a variant of the partial credit model (e.g., <xref ref-type="bibr" rid="bibr14-0013164411419846">Muraki, 1992</xref>) could also have been used and similar results would be expected.</p>
<p>Note that in this study the empirical validity coefficients all reflected disattenuated coefficients or coefficients that were not corrected for unreliability. While some predictive studies incorporate coefficients corrected for attenuation, in particular, predictive studies in employment testing contexts, most predictive validity studies in health and behavior assessment do not correct for attenuation in outcome measures. Furthermore, correcting for attenuation would not affect the relative comparison of conditions under study.</p>
</sec>
<sec id="section3-0013164411419846" sec-type="results">
<title>Results</title>
<p><xref ref-type="table" rid="table3-0013164411419846">Table 3</xref> presents average RMSDs comparing predicted values for the outcome measure versus true values across 500 replications for the varying item slope condition and sample size equal to 250. As noted before, this condition reflected items that varied in terms of their association with the underlying trait being measured and reflected a condition under which a model with item slope parameters may be preferred over a model with a single slope parameter for items. <xref ref-type="table" rid="table4-0013164411419846">Table 4</xref> reports average RMSDs for predicted versus true outcome measure values across 500 replications for the constant slope condition in which item slopes reflect a relatively narrow range.</p>
<table-wrap id="table3-0013164411419846" position="float">
<label>Table 3.</label>
<caption><p>RMSDs for Predicted Outcome Values (Varying Item Slopes; 250 Persons)</p></caption>
<graphic alternate-form-of="table3-0013164411419846" xlink:href="10.1177_0013164411419846-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="2">ρ = 0.3</th>
<th align="center" colspan="2">ρ = 0.4</th>
<th align="center" colspan="2">ρ = 0.6</th>
</tr>
<tr>
<th align="left">Number of Items</th>
<th align="center">Trait Level</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Traits Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>Low</td>
<td>0.95</td>
<td>0.94</td>
<td>0.92</td>
<td>0.92</td>
<td>0.82</td>
<td>0.83</td>
</tr>
<tr>
<td/>
<td>Medium</td>
<td>0.96</td>
<td>0.96</td>
<td>0.93</td>
<td>0.93</td>
<td>0.82</td>
<td>0.83</td>
</tr>
<tr>
<td/>
<td>High</td>
<td>0.95</td>
<td>0.96</td>
<td>0.92</td>
<td>0.92</td>
<td>0.82</td>
<td>0.83</td>
</tr>
<tr>
<td>20</td>
<td>Low</td>
<td>0.95</td>
<td>0.95</td>
<td>0.92</td>
<td>0.92</td>
<td>0.81</td>
<td>0.81</td>
</tr>
<tr>
<td/>
<td>Medium</td>
<td>0.95</td>
<td>0.96</td>
<td>0.92</td>
<td>0.92</td>
<td>0.81</td>
<td>0.82</td>
</tr>
<tr>
<td/>
<td>High</td>
<td>0.95</td>
<td>0.96</td>
<td>0.91</td>
<td>0.92</td>
<td>0.81</td>
<td>0.82</td>
</tr>
<tr>
<td>40</td>
<td>Low</td>
<td>0.94</td>
<td>0.95</td>
<td>0.91</td>
<td>0.91</td>
<td>0.81</td>
<td>0.81</td>
</tr>
<tr>
<td/>
<td>Medium</td>
<td>0.95</td>
<td>0.95</td>
<td>0.92</td>
<td>0.92</td>
<td>0.81</td>
<td>0.81</td>
</tr>
<tr>
<td/>
<td>High</td>
<td>0.95</td>
<td>0.96</td>
<td>0.91</td>
<td>0.91</td>
<td>0.80</td>
<td>0.81</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013164411419846">
<p><italic>Note</italic>. RMSD = root mean square deviation; IRT = item response theory.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table4-0013164411419846" position="float">
<label>Table 4.</label>
<caption><p>RMSDs for Predicted Outcome Values (Constant Item Slopes; 250 Persons)</p></caption>
<graphic alternate-form-of="table4-0013164411419846" xlink:href="10.1177_0013164411419846-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="2">ρ = 0.3</th>
<th align="center" colspan="2">ρ = 0.4</th>
<th align="center" colspan="2">ρ = 0.6</th>
</tr>
<tr>
<th align="left">Number of Items</th>
<th align="center">Trait Level</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Traits Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>Low</td>
<td>0.95</td>
<td>0.95</td>
<td>0.91</td>
<td>0.91</td>
<td>0.81</td>
<td>0.82</td>
</tr>
<tr>
<td/>
<td>Medium</td>
<td>0.96</td>
<td>0.96</td>
<td>0.93</td>
<td>0.93</td>
<td>0.83</td>
<td>0.84</td>
</tr>
<tr>
<td/>
<td>High</td>
<td>0.96</td>
<td>0.96</td>
<td>0.94</td>
<td>0.94</td>
<td>0.85</td>
<td>0.85</td>
</tr>
<tr>
<td>20</td>
<td>Low</td>
<td>0.94</td>
<td>0.95</td>
<td>0.91</td>
<td>0.92</td>
<td>0.81</td>
<td>0.82</td>
</tr>
<tr>
<td/>
<td>Medium</td>
<td>0.95</td>
<td>0.95</td>
<td>0.92</td>
<td>0.92</td>
<td>0.81</td>
<td>0.82</td>
</tr>
<tr>
<td/>
<td>High</td>
<td>0.95</td>
<td>0.96</td>
<td>0.91</td>
<td>0.92</td>
<td>0.82</td>
<td>0.84</td>
</tr>
<tr>
<td>40</td>
<td>Low</td>
<td>0.94</td>
<td>0.95</td>
<td>0.91</td>
<td>0.92</td>
<td>0.80</td>
<td>0.80</td>
</tr>
<tr>
<td/>
<td>Medium</td>
<td>0.96</td>
<td>0.96</td>
<td>0.92</td>
<td>0.92</td>
<td>0.80</td>
<td>0.81</td>
</tr>
<tr>
<td/>
<td>High</td>
<td>0.95</td>
<td>0.95</td>
<td>0.91</td>
<td>0.91</td>
<td>0.80</td>
<td>0.81</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013164411419846">
<p><italic>Note</italic>. RMSD = root mean square deviation; IRT = item response theory.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>When comparing <xref ref-type="table" rid="table3-0013164411419846">Tables 3</xref> and <xref ref-type="table" rid="table4-0013164411419846">4</xref>, it is apparent that there was no practical difference in the results for varying versus constant slope parameter conditions. There was essentially no effect of the number of items on the accuracy of the predicted outcome for either condition. The finding of no effect of number of items was surprising given that there is increased accuracy in scores as the number of items increases. In addition, there was essentially no effect of the type of score (IRT-based score vs. summated score) on the accuracy of the predicted outcome for either the varying slope condition or the constant slope condition. Furthermore, the level of the outcome variable (low, medium, or high) also had no effect on the accuracy of the predicted outcome. The only factor affecting the accuracy of the predicted outcome was the level of the correlation between the predictor and predicted outcome variables. As the correlation increased, the accuracy of the predicted outcome also increased. RMSDs for predicted versus true outcome measure values decreased from around .95 to .80 as the correlation increased from .30 to .60.</p>
<p>It should be noted that the size of the RMSDs given a <italic>z</italic>-score metric appear rather large. However, these values are consistent with an expectation based on the standard error of estimate (<italic>SEE</italic>) under the population model. One way of expressing the <italic>SEE</italic> is in terms of the Pearson correlation between the predictor and outcome measure:</p>
<p><disp-formula id="disp-formula2-0013164411419846">
<mml:math display="block" id="math6-0013164411419846">
<mml:mrow>
<mml:mi>S</mml:mi>
<mml:mi>E</mml:mi>
<mml:mi>E</mml:mi>
<mml:mo>=</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>−</mml:mo>
<mml:msup>
<mml:mi>ρ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>Y</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:msqrt>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0013164411419846" xlink:href="10.1177_0013164411419846-eq2.tif"/>
</disp-formula></p>
<p>where ρ is the Pearson correlation between the predictor and outcome measure and <inline-formula id="inline-formula5-0013164411419846">
<mml:math display="inline" id="math7-0013164411419846">
<mml:mrow>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>Y</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> is the variance of the outcome measure. Since the outcome measure in the current simulation study follows a standard normal distribution, <italic>SEE</italic> is expected to be .95, .92, and .80 for ρ = .30, .40, and .60, respectively.</p>
<p><xref ref-type="table" rid="table5-0013164411419846">Table 5</xref> illustrates the comparison of estimated correlations between the predictor and predicted outcome variables with true values for the varying item slope condition and the constant item slope condition. The values are compared for correlations based on the use of summated scores versus IRT-based scores with a sample size of 250. <xref ref-type="table" rid="table5-0013164411419846">Table 5</xref> demonstrates that RMSDs for the correlations increased as the true correlation increased, a finding that is not surprising. In general, use of summated scores as a predictor yielded smaller RMSDs for the correlations than using the IRT-based scores, but this effect decreased as the number of items increased. However, the differences between the RMSDs were more noticeable under the varying slope condition, a condition that was hypothesized to favor the use of IRT-based scores.</p>
<table-wrap id="table5-0013164411419846" position="float">
<label>Table 5.</label>
<caption><p>RMSDs for Correlations (250 Persons)</p></caption>
<graphic alternate-form-of="table5-0013164411419846" xlink:href="10.1177_0013164411419846-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">ρ = 0.3</th>
<th align="center" colspan="2">ρ = 0.4</th>
<th align="center" colspan="2">ρ = 0.6</th>
</tr>
<tr>
<th align="left">Number of Items</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Varying slopes</td>
</tr>
<tr>
<td> 10</td>
<td>0.079</td>
<td>0.070</td>
<td>0.092</td>
<td>0.075</td>
<td>0.120</td>
<td>0.085</td>
</tr>
<tr>
<td> 20</td>
<td>0.062</td>
<td>0.060</td>
<td>0.076</td>
<td>0.069</td>
<td>0.088</td>
<td>0.072</td>
</tr>
<tr>
<td> 40</td>
<td>0.066</td>
<td>0.065</td>
<td>0.067</td>
<td>0.061</td>
<td>0.074</td>
<td>0.059</td>
</tr>
<tr>
<td colspan="7">Constant slopes</td>
</tr>
<tr>
<td> 10</td>
<td>0.073</td>
<td>0.067</td>
<td>0.079</td>
<td>0.067</td>
<td>0.099</td>
<td>0.073</td>
</tr>
<tr>
<td> 20</td>
<td>0.063</td>
<td>0.063</td>
<td>0.067</td>
<td>0.064</td>
<td>0.072</td>
<td>0.062</td>
</tr>
<tr>
<td> 40</td>
<td>0.065</td>
<td>0.065</td>
<td>0.066</td>
<td>0.063</td>
<td>0.065</td>
<td>0.059</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0013164411419846">
<p><italic>Note</italic>. RMSD = root mean square deviation; IRT = item response theory.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The RMSD results for predicted outcomes for sample size of 500 and 1,000 were virtually equivalent to those for <italic>N</italic> = 250 and so are not presented. Thus, sample size does not affect this outcome measure. <xref ref-type="table" rid="table6-0013164411419846">Tables 6</xref> and <xref ref-type="table" rid="table7-0013164411419846">7</xref> present the RMSD results for correlation based on a sample size of 500 and 1,000 for the varying and constant slope conditions. As for the results based on <italic>N</italic> = 250, the RMSDs for the correlation increased as the true correlation between the predictor and outcome increased. Also, the summated score as a predictor yielded slightly smaller RMSDs for the correlation than using IRT-based scores, and as for <italic>N</italic> = 250, the effect decreased as the number of items increased. In comparing the results for <italic>N</italic> = 250 versus 1,000 (<xref ref-type="table" rid="table4-0013164411419846">Table 4</xref> vs. <xref ref-type="table" rid="table7-0013164411419846">Table 7</xref>), RMSDs are somewhat smaller for <italic>N</italic> = 1,000 under the constant and varying slope conditions. In addition, the differences observed between the varying slope and constant slope conditions when <italic>N</italic> = 250 were not found for <italic>N</italic> = 1,000.</p>
<table-wrap id="table6-0013164411419846" position="float">
<label>Table 6.</label>
<caption><p>RMSDs for Correlations (500 Persons)</p></caption>
<graphic alternate-form-of="table6-0013164411419846" xlink:href="10.1177_0013164411419846-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">ρ = 0.3</th>
<th align="center" colspan="2">ρ = 0.4</th>
<th align="center" colspan="2">ρ = 0.6</th>
</tr>
<tr>
<th align="left">Number of Items</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Varying slopes</td>
</tr>
<tr>
<td> 10</td>
<td>0.043</td>
<td>0.037</td>
<td>0.054</td>
<td>0.043</td>
<td>0.072</td>
<td>0.053</td>
</tr>
<tr>
<td> 20</td>
<td>0.037</td>
<td>0.034</td>
<td>0.041</td>
<td>0.035</td>
<td>0.053</td>
<td>0.040</td>
</tr>
<tr>
<td> 40</td>
<td>0.037</td>
<td>0.032</td>
<td>0.042</td>
<td>0.033</td>
<td>0.058</td>
<td>0.036</td>
</tr>
<tr>
<td colspan="7">Constant slopes</td>
</tr>
<tr>
<td> 10</td>
<td>0.051</td>
<td>0.040</td>
<td>0.062</td>
<td>0.043</td>
<td>0.078</td>
<td>0.052</td>
</tr>
<tr>
<td> 20</td>
<td>0.037</td>
<td>0.033</td>
<td>0.043</td>
<td>0.035</td>
<td>0.053</td>
<td>0.038</td>
</tr>
<tr>
<td> 30</td>
<td>0.035</td>
<td>0.031</td>
<td>0.040</td>
<td>0.032</td>
<td>0.049</td>
<td>0.034</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0013164411419846">
<p><italic>Note</italic>. RMSD = root mean square deviation; IRT = item response theory.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table7-0013164411419846" position="float">
<label>Table 7.</label>
<caption><p>RMSDs for Correlations (1,000 Persons)</p></caption>
<graphic alternate-form-of="table7-0013164411419846" xlink:href="10.1177_0013164411419846-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">ρ = 0.3</th>
<th align="center" colspan="2">ρ = 0.4</th>
<th align="center" colspan="2">ρ = 0.6</th>
</tr>
<tr>
<th align="left">Number of Items</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
<th align="center">Based on IRT Trait Estimates</th>
<th align="center">Based on Standardized Summated Scores</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Varying slopes</td>
</tr>
<tr>
<td> 10</td>
<td>0.061</td>
<td>0.053</td>
<td>0.068</td>
<td>0.054</td>
<td>0.117</td>
<td>0.090</td>
</tr>
<tr>
<td> 20</td>
<td>0.051</td>
<td>0.048</td>
<td>0.058</td>
<td>0.050</td>
<td>0.070</td>
<td>0.054</td>
</tr>
<tr>
<td> 40</td>
<td>0.046</td>
<td>0.042</td>
<td>0.052</td>
<td>0.045</td>
<td>0.065</td>
<td>0.047</td>
</tr>
<tr>
<td colspan="7">Constant slopes</td>
</tr>
<tr>
<td> 10</td>
<td>0.062</td>
<td>0.054</td>
<td>0.070</td>
<td>0.054</td>
<td>0.088</td>
<td>0.060</td>
</tr>
<tr>
<td> 20</td>
<td>0.053</td>
<td>0.048</td>
<td>0.055</td>
<td>0.046</td>
<td>0.070</td>
<td>0.052</td>
</tr>
<tr>
<td> 30</td>
<td>0.048</td>
<td>0.046</td>
<td>0.049</td>
<td>0.044</td>
<td>0.055</td>
<td>0.041</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0013164411419846">
<p><italic>Note</italic>. RMSD = root mean square deviation; IRT = item response theory.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section4-0013164411419846" sec-type="discussion">
<title>Discussion</title>
<p>When studying relationships between estimates of traits and outcomes in the social and behavioral sciences, researchers have the option of using estimates of the variables based on CTT or IRT. While researchers have considered the different scaling properties of CTT- versus IRT-based scores, the debate surrounding the use of ordinal or interval scaled scores in analyses does not appear resolved.</p>
<p>The current study used a Monte Carlo approach to compare the behavior of trait estimates obtained from CTT versus IRT measurement frameworks and their use as predictor variables in validity studies. Conditions were specifically manipulated to evaluate the robustness of the different trait estimates under various conditions and assumptions underlying the CTT model: (a) CTT model weighs each item equally, which in turn implies each item is equally related to the underlying trait being measured; (b) CTT model assumes the response options have the same meaning for all items since the same response scale is used across the set of items.</p>
<p>Results indicated that IRT- and CTT-based scores (SS) were very comparable in terms of predicting outcomes (e.g., validity coefficients) and that the use of CTT-based scores appear robust to violations in assumptions underlying the use of summated scores. While the findings of comparability between scores from the two frameworks are consistent with previous research, they were somewhat surprising given the explicit manipulation of the assumptions underlying CTT-based scores. Results further indicate that SS scores may have an advantage over IRT-based scores for scales of short length (10 items), particularly for smaller sample sizes (<italic>N</italic> = 250). Thus, for applied researchers, there may be no practical disadvantage to using CTT-based scores in prediction studies, and the one major advantage is that researchers do not need to engage in analyses required to evaluate the assumptions underlying the use of IRT-based scores. Researchers should be reassured that many types of analyses using summated scores are also robust to the assumption of interval data (e.g., <xref ref-type="bibr" rid="bibr4-0013164411419846">Dowling &amp; Midgley, 1991</xref>; <xref ref-type="bibr" rid="bibr8-0013164411419846">Glass et al., 1972</xref>).</p>
<p>Despite the finding of no practical advantage to IRT-based scores with ordinal scaled items, it may be possible to introduce conditions that generate more nonlinearity in the relationship between summated scores and the underlying trait being measured (<xref ref-type="bibr" rid="bibr7-0013164411419846">Ferrando &amp; Chico, 2007</xref>). Under such conditions, IRT-based scores may now have an advantage over summated scale scores. For example, items reflecting extremely high discrimination or sets of items measuring extreme levels of the trait should exhibit more nonlinearity. To test this prediction, a simulation condition was examined for highly discriminating items (values fixed at 6). For a 20-item test, <italic>N</italic> = 250, and a true validity coefficient equal to .3, RMSD values for predicted outcomes and correlations were very similar to the corresponding values in <xref ref-type="table" rid="table4-0013164411419846">Tables 4</xref> and <xref ref-type="table" rid="table5-0013164411419846">5</xref>. While nonlinearity may be a potential problem with binary items (e.g., <xref ref-type="bibr" rid="bibr7-0013164411419846">Ferrando &amp; Chico, 2007</xref>), nonlinearity appears to become even less relevant as the number of response categories increase. Finally, although it may be possible to introduce more extreme conditions to create an advantage to IRT-based scores, it is important to consider that such conditions are not typically associated with instruments used in practice.</p>
<p>In any simulation study, the results generalize only to the conditions that were studied and the context of the modeled population. Results from this study are intended to generalize to applications involving scales consisting of polytomous items (five-choice items). Further in that context the intent was to manipulate factors that would have practical value to applied researchers. While the conditions were intended to reflect practical values, it is possible that results do not generalize to other contexts.</p>
</sec>
</body>
<back>
<ack>
<p>The authors wish to thank the reviewers and editor for their comments and help in revising the article.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Andrich</surname><given-names>D.</given-names></name>
</person-group> (<year>1978</year>). <article-title>A rating scale formulation for ordered response categories</article-title>. <source>Psychometrika</source>, <volume>43</volume>, <fpage>561</fpage>-<lpage>573</lpage>.</citation>
</ref>
<ref id="bibr2-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carifio</surname><given-names>J.</given-names></name>
<name><surname>Perla</surname><given-names>R. J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Ten common misunderstandings, misconceptions, persistent myths and urban legends about Likert scales and Likert response formats and their antidotes</article-title>. <source>Journal of Social Sciences</source>, <volume>3</volume>(<issue>3</issue>), <fpage>106</fpage>-<lpage>116</lpage>.</citation>
</ref>
<ref id="bibr3-0013164411419846">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Dodd</surname><given-names>B. G.</given-names></name>
<name><surname>Koch</surname><given-names>W. R.</given-names></name>
<name><surname>De Ayala</surname><given-names>R. J.</given-names></name>
</person-group> (<year>1989</year>, <month>March</month>). <source>Validity of combined essay and multiple choice achievement scores: Classical versus IRT approaches</source>. <conf-name>Paper presented at the Annual Meeting of the American Educational Research Association</conf-name>, <conf-loc>San Francisco, CA</conf-loc>.</citation>
</ref>
<ref id="bibr4-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dowling</surname><given-names>G. R.</given-names></name>
<name><surname>Midgley</surname><given-names>D. F.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Using rank values as an interval scale</article-title>. <source>Psychology &amp; Marketing</source>, <volume>8</volume>, <fpage>37</fpage>-<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr5-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fan</surname><given-names>X.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Item response theory and classical test theory: An empirical comparison of their item/person statistics</article-title>. <source>Educational and Psychological Measurement</source>, <volume>58</volume>, <fpage>357</fpage>-<lpage>381</lpage>.</citation>
</ref>
<ref id="bibr6-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ferrando</surname><given-names>P. J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Likert scaling using continuous, censored and graded response models: Effects on criterion-related validity</article-title>. <source>Applied Psychological Measurement</source>, <volume>23</volume>, <fpage>161</fpage>-<lpage>175</lpage>.</citation>
</ref>
<ref id="bibr7-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ferrando</surname><given-names>P. J.</given-names></name>
<name><surname>Chico</surname><given-names>E.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The external validity of scores based on the two-parameter logistic model: Some comparisons between IRT and CTT</article-title>. <source>Psicológica</source>, <volume>28</volume>, <fpage>237</fpage>-<lpage>257</lpage>.</citation>
</ref>
<ref id="bibr8-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Glass</surname><given-names>G. V.</given-names></name>
<name><surname>Peckman</surname><given-names>P. D.</given-names></name>
<name><surname>Sanders</surname><given-names>J. R.</given-names></name>
</person-group> (<year>1972</year>). <article-title>Consequences of failure to meet assumptions underlying analyses of variance and covariance</article-title>. <source>Review of Educational Research</source>, <volume>42</volume>, <fpage>237</fpage>-<lpage>288</lpage>.</citation>
</ref>
<ref id="bibr9-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Harwell</surname><given-names>M. R.</given-names></name>
<name><surname>Gatti</surname><given-names>G. G.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Rescaling ordinal data to interval data in educational research</article-title>. <source>Review of Educational Research</source>, <volume>71</volume>, <fpage>105</fpage>-<lpage>131</lpage>.</citation>
</ref>
<ref id="bibr10-0013164411419846">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lawson</surname><given-names>S.</given-names></name>
</person-group> (<year>1991</year>). <article-title>One parameter latent trait measurement: Do the results justify the effort?</article-title> In <person-group person-group-type="editor">
<name><surname>Thompson</surname><given-names>B.</given-names></name>
</person-group> (Ed.), <source>Advances in educational research: Substantive findings, methodological developments</source> (Vol. <volume>1</volume>, pp. <fpage>159</fpage>-<lpage>168</lpage>). <publisher-loc>Greenwich, CT</publisher-loc>: <publisher-name>JAI Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0013164411419846">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Wainer</surname><given-names>H.</given-names></name>
</person-group> (<year>1998</year>). <source>Toward a coherent view of reliability in test theory</source> (<comment>Research Report RR-98-2</comment>). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr12-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>MacDonald</surname><given-names>P.</given-names></name>
<name><surname>Pauonen</surname><given-names>S. V.</given-names></name>
</person-group> (<year>2002</year>). <article-title>A Monte Carlo comparison of item and person statistics based on item response theory versus classical test theory</article-title>. <source>Educational and Psychological Measurement</source>, <volume>62</volume>, <fpage>921</fpage>-<lpage>943</lpage>.</citation>
</ref>
<ref id="bibr13-0013164411419846">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McBride</surname><given-names>J. R.</given-names></name>
<name><surname>Martin</surname><given-names>J. T.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Reliability and validity of adaptive ability tests in a military setting</article-title>. In <person-group person-group-type="editor">
<name><surname>Weiss</surname><given-names>D. J.</given-names></name>
</person-group> (Ed.), <source>New horizons in testing: Latent trait test theory and computerized adaptive testing</source> (pp. <fpage>224</fpage>-<lpage>236</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Muraki</surname><given-names>E.</given-names></name>
</person-group> (<year>1992</year>). <article-title>A generalized partial credit model: Application of an EM algorithm</article-title>. <source>Applied Psychological Measurement</source>, <volume>16</volume>, <fpage>159</fpage>-<lpage>176</lpage>.</citation>
</ref>
<ref id="bibr15-0013164411419846">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Muraki</surname><given-names>E.</given-names></name>
<name><surname>Bock</surname><given-names>R. D.</given-names></name>
</person-group> (<year>1997</year>). <source>PARSCALE 3: IRT based test scoring and item analysis for graded items and rating scales</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>Scientific Software International</publisher-name>.</citation>
</ref>
<ref id="bibr16-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ndalichako</surname><given-names>J. L.</given-names></name>
<name><surname>Rogers</surname><given-names>W. T.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Comparison of finite state score theory, classical test theory, and item response theory in scoring multiple-choice items</article-title>. <source>Educational and Psychological Measurement</source>, <volume>57</volume>, <fpage>580</fpage>-<lpage>589</lpage>.</citation>
</ref>
<ref id="bibr17-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Raykov</surname><given-names>T.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Estimation of composite reliability for congeneric measures</article-title>. <source>Applied Psychological Measurement</source>, <volume>21</volume>, <fpage>173</fpage>-<lpage>184</lpage>.</citation>
</ref>
<ref id="bibr18-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>C. J.</given-names></name>
<name><surname>Bobko</surname><given-names>P.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Moderated regression analyses and Likert scales: Too coarse for comfort</article-title>. <source>Journal of Applied Psychology</source>, <volume>77</volume>, <fpage>336</fpage>-<lpage>342</lpage>.</citation>
</ref>
<ref id="bibr19-0013164411419846">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Samejima</surname><given-names>F.</given-names></name>
</person-group> (<year>1969</year>). <source>Estimation of latent ability using a response pattern of graded scores</source> (<comment>Psychometric Monographs, No. 17</comment>). <publisher-loc>Richmond, VA</publisher-loc>: <publisher-name>Psychometric Society</publisher-name>.</citation>
</ref>
<ref id="bibr20-0013164411419846">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thissen</surname><given-names>D.</given-names></name>
<name><surname>Nelson</surname><given-names>L.</given-names></name>
<name><surname>Rosa</surname><given-names>K.</given-names></name>
<name><surname>McLeod</surname><given-names>L. D.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Item response theory for items scored in more than two categories</article-title>. In <person-group person-group-type="editor">
<name><surname>Thissen</surname><given-names>D.</given-names></name>
<name><surname>Wainer</surname><given-names>H.</given-names></name>
</person-group> (Eds.), <source>Test scoring</source> (pp. <fpage>141</fpage>-<lpage>184</lpage>). <publisher-loc>London, England</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr21-0013164411419846">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thissen</surname><given-names>D.</given-names></name>
<name><surname>Orlando</surname><given-names>M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Item response theory for items scored in two categories</article-title>. In <person-group person-group-type="editor">
<name><surname>Thissen</surname><given-names>D.</given-names></name>
<name><surname>Wainer</surname><given-names>H.</given-names></name>
</person-group> (Eds.), <source>Test scoring</source> (pp. <fpage>73</fpage>-<lpage>137</lpage>). <publisher-loc>London, England</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr22-0013164411419846">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Young</surname><given-names>J. W.</given-names></name>
</person-group> (<year>1995</year>). <article-title>A comparison of two adjustment methods for improving the prediction of law school grades</article-title>. <source>Educational and Psychological Measurement</source>, <volume>55</volume>, <fpage>558</fpage>-<lpage>571</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>