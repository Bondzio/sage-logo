<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ERG</journal-id>
<journal-id journal-id-type="hwp">sperg</journal-id>
<journal-title>Ergonomics in Design: The Quarterly of Human Factors Applications</journal-title>
<issn pub-type="ppub">1064-8046</issn>
<issn pub-type="epub">XXXX-XXXX</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1064804613491268</article-id>
<article-id pub-id-type="publisher-id">10.1177_1064804613491268</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Features</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Flight Deck Automation</article-title>
<subtitle>Invaluable Collaborator or Insidious Enabler?</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Geiselman</surname><given-names>Eric E.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Johnson</surname><given-names>Christopher M.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Buck</surname><given-names>David R.</given-names></name>
</contrib>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>21</volume>
<issue>3</issue>
<fpage>22</fpage>
<lpage>26</lpage>
<permissions>
<copyright-statement>© 2013 Human Factors and Ergonomics Society</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<p>We respond to claims that airline pilots may be losing their ability to manually control aircraft because overreliance on automation is eroding basic manual flying skills. We propose that better training is only a partial solution and that automation can be designed to better support human performance. We do not simply advocate more automation; rather, we envision a more context-aware automation design philosophy that promotes a more communicative and collaborative human-machine interface. Examples are used to illustrate the benefits of this approach. A companion piece to this article, which includes proposed mitigation interface designs, will be available in a subsequent issue of <italic>Ergonomics in Design</italic>.</p>
</abstract>
<kwd-group>
<kwd>automation</kwd>
<kwd>automation surprise</kwd>
<kwd>black swan</kwd>
<kwd>context-aware computing</kwd>
<kwd>crew resource management</kwd>
<kwd>operator complacency</kwd>
<kwd>threat and error management</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Today’s reliance on flight deck automation could be eroding manual flying skills.</p>
<p>Are airline pilots forgetting how to fly? Several recent news articles have used terms to this effect to posit that overreliance on flight deck automation, or “automation addiction,” has degraded pilots’ manual flight skills (e.g., <xref ref-type="bibr" rid="bibr5-1064804613491268">Lowy, 2011</xref>). We define <italic>automation</italic> here as the “autoflight” system used to translate aircraft horizontal and vertical path guidance into an audio-visual display interface for the pilot. The autopilot is a component of the autoflight system that replaces the pilot’s manual control input to follow the automated guidance.</p>
<p><xref ref-type="bibr" rid="bibr5-1064804613491268">Lowy (2011)</xref> cited recent accidents in which pilots lost control after automated systems disengaged unexpectedly. Interviewees called exclusively for competency-based strategies aimed at ensuring manual skill proficiency. However, they did not question the behavior of the automated systems that acted precisely as designed, and this oversight concerns us because we have identified safety-related issues that reside at the design level. Placing blame only on pilots for automation-related airline disasters fails to delve deep enough into the human–machine interface. In this article, we use real-world incident examples to explore a more robust analysis of the problem.</p>
<sec id="section1-1064804613491268">
<title>The Automation Role</title>
<p>Degradation of manual skill may be a problem associated with training and experience, but loss-of-control accidents can also be mitigated with better automation design that is intended to avoid automation surprises. Automation surprises are commonly caused by mode errors in which pilots fail to realize, or are surprised by, lack of awareness of automated flight modes (<xref ref-type="bibr" rid="bibr11-1064804613491268">Sarter, Woods, &amp; Billings, 1997</xref>). Automated systems are paramount in managing flight deck workload; indeed, for years, human factors/ergonomics experts have suggested that automation should be designed with inherent “etiquette” to effectively communicate its intentions and limitations – automation should adhere to crew resource management (CRM) principles (<xref ref-type="bibr" rid="bibr9-1064804613491268">Parasuraman &amp; Miller, 2004</xref>).</p>
<p>Considering the principles of human-centered design by which “automation systems must also monitor the human operators and every intelligent system element must know the intent of other intelligent system elements” (<xref ref-type="bibr" rid="bibr1-1064804613491268">Billings, 1997</xref>, p. 39), we feel that a context-aware computing approach can produce automation that is capable of adapting to its location of use and changes in the environment (<xref ref-type="bibr" rid="bibr2-1064804613491268">Dey, 2009</xref>) to become a better “team player.” We are not advocating that flight deck automation be an anthropometric agent; rather, we simply propose that access to existing data within context can be used to better tune interfaces to be more communicative and informative.</p>
<p>We use the terms <italic>etiquette</italic> and <italic>team player</italic> as they have been defined metaphorically by <xref ref-type="bibr" rid="bibr6-1064804613491268">Miller (2002)</xref> and <xref ref-type="bibr" rid="bibr3-1064804613491268">Klein, Woods, Bradshaw, Hoffman, and Feltovich (2004)</xref>, respectively, to denote the utility gain that may be realized when automation adheres to well-established standards of context-specific conduct. Automation interfaces should support crew members’ goals and mitigate breakdowns in team coordination (<xref ref-type="bibr" rid="bibr3-1064804613491268">Klein et al., 2004</xref>).</p>
<p>The following examples illustrate the threat posed by the <italic>dutiful but silent</italic> disposition of current automation interface designs.</p>
</sec>
<sec id="section2-1064804613491268">
<title>A Typical Scenario</title>
<p>An airliner is established in cruise with the autopilot tracking along a preprogrammed flight route. Under normal conditions, this is a low-workload situation, in which the crew monitor systems and comply with air traffic control (ATC) requests. ATC requests commonly include temporary modifications to the flight plan, such as “maintain present heading for conflicting traffic.” In response, the pilot flying engages heading mode to override the navigation mode, temporarily suspending the programmed flight routing. Before any preprogrammed turns are missed, ATC clears the crew to resume navigation along the original route; however, the crew fail to reselect navigation mode, and the aircraft passes a required turn. The crew members are surprised when ATC asks, “Acme Flight 123, where are you going?”</p>
<p>The problem here is that current automation does not indicate any mismatch between the distant goal (the destination) and the immediate goal (heading mode to avoid conflicting traffic). A more context-aware computing system could have detected the mismatch and alerted the crew before the turn, making the ATC notification unnecessary. An actual example was Northwest Airlines (NWA) Flight 188, which overflew its destination by 150 nautical miles (<xref ref-type="bibr" rid="bibr7-1064804613491268">National Transportation Safety Board [NTSB], 2009</xref>). One of the pilots programmed an incorrect communication frequency and failed to establish subsequent contact with ATC. The pilots overflew the planned descent point and failed to detect a subtle automatic switch from navigation to heading mode after passing the final waypoint. A more context-aware autoflight system could have used geographic information to alert the crew of the improper radio frequency and the missed descent point.</p>
<p>This hypothetical scenario happens frequently, but the rate of negative outcome is low because the aviation system includes layers of redundant safety measures that resist human error propagation (<xref ref-type="bibr" rid="bibr10-1064804613491268">Reason, 1990</xref>; <xref ref-type="bibr" rid="bibr13-1064804613491268">Wiegmann &amp; Shappell, 2003</xref>). Designers should, however, meticulously analyze automation-related behavior that propagates to catastrophic outcomes and strive to eliminate unexpected, nonsalient mode changes. Only when such analysis is not possible should the operator serve as a last line of defense.</p>
<p>A recent catastrophe is worthy of such interest and emphasizes the need for context-aware automation. The following example illustrates how real a threat automation behavior can be.</p>
</sec>
<sec id="section3-1064804613491268">
<title>Colgan Air Flight 3407</title>
<p>Colgan Air Flight 3407 crashed during an instrument approach to landing at Buffalo International Airport on February 12, 2009. The aircraft crashed into a residence five miles from the airport. All on board and one person on the ground perished. The investigation concluded that it was a loss-of-control accident (<xref ref-type="bibr" rid="bibr8-1064804613491268">NTSB, 2010</xref>), and the critical events were as follows.</p>
<p>ATC cleared the crew to descend to 2,300 feet and intercept the instrument approach course. The autoflight system was set to hold at 2,300 feet. Once this was established on the approach, the pilot flying kept the power levers near idle and slowed to flap-extension speed. As the landing gear lowered and flaps extended, the autoflight system continually pitched the aircraft nose-up to maintain altitude, causing the airspeed to decay. The indicated airspeed and low-speed cue converged, and the stick shaker engaged to alert of an impending stall. Simultaneously, the autoflight system disengaged – as designed.</p>
<p>In an apparent condition of surprise, the captain applied inappropriate nose-up inputs and failed to add power. The aircraft experienced a full aerodynamic stall, which activated the stick pusher (which is designed to lower the nose and regain airspeed to recover from the stall). However, the pilot counteracted the stick pusher and failed to recover.</p>
<p><disp-quote>
<p>This accident may have been avoided if the automation had incorporated a context-aware design solution similar to that suggested by the NTSB</p>
</disp-quote></p>
<p>The NTSB cited the captain’s failure to maintain proper airspeed and recover from the aerodynamic stall. Contributing factors include the fact that the power was kept at idle while the airplane was leveling off at 2,300 feet and the simultaneous configuration for landing. Other than the low-speed cue, the stick shaker was the first indication of the impending stall, and the crew members seemed to have been caught by surprise, likely because they had not noticed the decay of airspeed. This was an automation surprise.</p>
<p>The NTSB cited the stall-warning system as an inadequate line of defense. Crew members should have noticed that the aircraft was dangerously slow, but they did not. The NTSB concluded that the airspeed indicator lacked low-speed awareness features, such as an amber band above the low-speed cue and an aural warning in advance of the stick shaker, which (if present) may have elicited a timely response from the pilots. The automation had access to the data needed to make it “aware” of its low-energy state, but the interface did not convey the implications of that information to the crew before it was too late.</p>
<p>This accident may have been avoided if the automation had incorporated a context-aware design solution similar to that suggested by the NTSB. Simply stated, the system could have communicated its limitations more effectively and prompted the captain to check the airspeed – as the first officer should have. Some automated alert should have been triggered prior to stick shaker onset and the simultaneous autopilot disconnect. Instead, the autopilot stopped flying the aircraft, startled the crew, and did not afford them adequate time to respond confidently.</p>
<boxed-text id="boxed-text1-1064804613491268">
<title>Not More Automation . . . Better Automation</title>
<p> Of all airline crashes, the most tragic are those stemming from otherwise inconsequential errors – often a result of operator complacency. The set of circumstances can be eerily similar: no mechanical failures, routine weather conditions, and normal procedures, yet a crew enables a situation in which an extraordinary level of effort, skill, or outside intervention is called upon to recover from self-imposed errors (if recovery is possible at all).</p>
<p>For Colgan Air Flight 3407, an egregious lack of attention led to a rare loss of control, low-altitude stall, and crash. The investigation showed that the stall was recoverable, yet the crash still occurred. In response, the media asked whether airline pilots have forgotten how to fly because of overreliance on automation. We believe an “automation addiction” diagnosis is partial at best.</p>
<p>Accordingly, we take a perspective different from that of those who propose only additional manual skill training and evaluation as the solution. Of course, skill degradation should be avoided, but we believe the automation interface can be designed to deliver preventative measures prior to events that necessitate recovery. As a group of experienced airline pilots, crew resource management instructors, and human factors researchers, we believe that there are design opportunities to trap initiating errors through the application of context-aware computing techniques and use of automation interfaces that are more informative tools – tools capable of communicating goal inconsistency, system limitations, and recognition of uncertainty.</p>
<p>Our motivation for writing this piece is to encourage a dialogue within the design community, asking whether enough has been done to deliver holistic solutions in the form of automation interfaces that support complex systems. To resist the negative effects of well-understood human limitations and error-prone states, such as complacency, we suggest that automation be designed to help operators maintain vigilance and that interfaces become more coordinated defensive measures.</p>
<p>We follow this article with a call for context-aware logic to improve safety, which will address the problems identified earlier. Specifically, we outline concepts that we believe have the potential to mitigate insidious enablers of human error and propose design solutions that may have mitigated the extraordinary “black swan” event of the Air France Flight 447 loss-of-control accident. Black swans are highly consequential, off-nominal events that occur outside of regular expectancy and involve nonsalient interface changes that evade perception and catch pilots by surprise (<xref ref-type="bibr" rid="bibr12-1064804613491268">Wickens, Hooey, Gore, Sebok, &amp; Koenicke, 2009</xref>). The complexity involved in the Air France crash deserves careful analysis and thorough consideration. We believe this crash could have been averted if a more collaborative interface had been available to the aircrew.</p>
<p>We enthusiastically encourage the design community to challenge our assertions by submitting alternative views and ideas. In the end, the most important objective is to ensure that any potential safety advantage afforded by thoughtful interface design not be overlooked.</p>
</boxed-text>
<p>Analyzing the pilot-autopilot interaction during the Colgan Air Flight 3407 events, one can appreciate the need for better automation. Specifically, proper CRM protocol requires a surrendering pilot (the autopilot in this case) to perform a positive transfer-of-control procedure by receiving confirmation from the pilot taking the controls (e.g., “my controls”) prior to releasing control. Unfortunately, autopilots do not adhere to this simple but vital aspect of flight deck etiquette, which is among the first standard operating procedures taught to all pilots. Instead, current autopilots abandon their flying duty at critical, high-workload phases of flight without warning, creating unnecessary emergencies.</p>
<p>To put this situation into perspective, imagine the reaction of a captain suddenly faced with the transfer of control from a first officer in the landing flare during strong crosswind conditions without prior warning. Such a showing of poor etiquette should never be tolerated by a human pilot, so why should we accept similar behavior from automated systems?</p>
</sec>
<sec id="section4-1064804613491268">
<title>Common Themes</title>
<p>These incidents illustrate automation surprise events that have much in common. In no case did the automation fail. The automation did exactly what it was programmed to do, and the aircraft performed exactly as expected. With the exception of NWA Flight 188, these aircraft appeared to ATC to be in complete compliance with all directives.</p>
<p>During the events of Colgan Air Flight 3407, it appears that the crew lacked the skill to recover from the circumstances into which the autoflight system led them, and the NTSB agrees that skill weakness likely contributed to exacerbation of a low-energy flight condition. The evidence leads reasonable people to implicate automation overreliance, manual skill degradation, or both (<xref ref-type="bibr" rid="bibr5-1064804613491268">Lowy, 2011</xref>). Accordingly, the proposed mitigation strategy becomes, in part, a matter of pilot training and evaluation. It may appear that there is no downside to pursuing this fix; however, manual operation is fatiguing, and it requires constant attention absorbed by the psychomotor control loop, which reduces the availability of attentional resources for system monitoring, procedure management, and communication.</p>
<p>The focus of the analysis needs to take into account subsurface variables within the system that cause pilots to be surprised by their automation. Interface designers and human factors/ergonomics professionals need to investigate the nature of automation surprises and develop resilient mitigation strategies at the interface design level. A logical starting point is to ask what the present state of automation is and what it is not.</p>
<sec id="section5-1064804613491268">
<title>Flight deck automation now . . .</title>
<list id="list1-1064804613491268" list-type="bullet">
<list-item><p>is a subject of supervisory control;</p></list-item>
<list-item><p>has roles but not responsibility;</p></list-item>
<list-item><p>requires that roles be assigned and managed by humans;</p></list-item>
<list-item><p>is typically reliable, until it reaches its design limitations;</p></list-item>
<list-item><p>requires that humans maintain awareness of its design limitations; and</p></list-item>
<list-item><p>is a tool, not an additional crew member.</p></list-item>
</list>
</sec>
</sec>
<sec id="section6-1064804613491268">
<title>The Real Problem</title>
<p>Complacency causes pilots to fail to maintain an appropriate level of vigilance in the face of developing threats. In the foregoing examples, the pilots were surprised by where the automation led them. The automation performed as designed, but the pilots failed to manage its limitations. They were complacent because automation normally does its job very well. Therefore, a challenge to the design community is to build interfaces that can help to mitigate automation surprises by reducing operator complacency. Again, this is not a solicitation for more automation; it is a call for better automation. Automation surprises can be mitigated by assigning more vigilance and communication capability through modifications of current automation that incorporate intelligent data management solutions.</p>
<p>Although designers may never assign automation a level of responsibility equal to that of a human pilot, it can reasonably be assigned the role of assisting humans by mitigating the threat of complacency. A context-aware design philosophy will facilitate the development of automation that works more seamlessly with crews by adhering to well-established principles of CRM. Collaborative team members share common goals, communicate intentions, and confess limitations to the team when appropriate; automation should be no different.</p>
<p>We propose a design shift from the current state of automation – which has limited capability with singular goals and subtasks assigned by pilots – toward automation that is a more effective communicator. As presented by <xref ref-type="bibr" rid="bibr1-1064804613491268">Billings (1997)</xref>, “flight deck automation should be managed to support the pilots’ goal hierarchy” (p. 122).</p>
<p>What flight deck automation could be:</p>
<list id="list2-1064804613491268" list-type="order">
<list-item><p>“Aware” of progress toward common goals,</p></list-item>
<list-item><p>suggestive of crew actions that will support common goals,</p></list-item>
<list-item><p>inquisitive and/or informative when unacceptable uncertainty prevails,</p></list-item>
<list-item><p>a reducer of uncertainty by communicating intentions and limitations, and</p></list-item>
<list-item><p>not an additional crew member . . . but perhaps a better collaborator.</p></list-item>
</list>
</sec>
<sec id="section7-1064804613491268">
<title>The Way Forward</title>
<p>The common goal is to complete a flight safely and efficiently. Current automation is vaguely “aware” of this goal as it calculates and displays flight plan information, such as time and fuel remaining en route; however, it can do more with the available data. The interface can become reactive as a context-aware collaborator, which is an approach that affords automation a place within the team (<xref ref-type="bibr" rid="bibr4-1064804613491268">Lee, 2008</xref>).</p>
<p>The concepts illustrated here represent a small sample of automation issues that promote a change in design philosophy. We believe that context-aware computing has the potential to produce an autopilot that is worthy of its name. As is expected of human pilots, automation should maintain “awareness” throughout the flight in support of effective decision making. A <italic>true</italic> automated pilot should be aware of goals, seek clarification, offer timely information regarding limitations, and unambiguously communicate intended changes. Given the highly structured nature of aviation and the expanse of data-processing functionality, these capabilities are technologically feasible and afford significant safety benefits.</p>
<p>As next-generation technologies populate the flight deck, normal reliance on automation will increase significantly. Accordingly, the roles and responsibilities of human operators will shift from manual control toward the more cognitive processes of judgment and decision making. Thus, it is likely that training will also place less emphasis on manual skill. In the absence of a design philosophy in which the vigilance role of the automation is emphasized, future functionality has real potential to increase the negative outcomes initiated by complacency and automation surprise.</p>
<p>Training, albeit critical, is only one of the components required for a solution to the problems discussed here. A holistic mitigation approach focused upstream from manual recovery (the last line of defense) requires that the entire system capability be considered. Context-aware automation may go a long way toward preventing situations in which manual skill makes the difference between recovery and disaster.</p>
</sec>
</body>
<back>
<bio>
<p><bold>Eric E. Geiselman</bold> is an engineering research psychologist at the U.S. Air Force Research Laboratory’s 711th Human Performance Wing, in the Human Effectiveness Directorate. He spent eight years as an airline pilot and eventually became a crew resource management instructor. He holds an MA in experimental psychology from the University of Dayton and is pursuing a doctorate in systems engineering at the Air Force Institute of Technology.</p>
<p><bold>Christopher M. Johnson</bold> holds a PhD in industrial and systems engineering (human factors and ergonomics) from the University of Wisconsin-Madison and is a certified flight instructor. He holds an MS in human factors as well as a professional pilot certification from the University of Illinois. His dissertation work was funded by the Federal Aviation Administration to research weather-related decision making and to develop high-fidelity weather simulation in scenario-based virtual training.</p>
<p><bold>David R. Buck</bold> has more than 14 years of experience and more than 6,000 flight hours in FAR Part 121 Airline Operations, both as a current pilot and as a human factors senior facilitator. He has written and delivered annual training courseware to thousands of crew members and has conducted more than 400 formal crew debriefs. He is a member of his airline’s Accident GO Team and of the Association for Aviation Psychology and is president of NavCom Consulting, LLC.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1064804613491268">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Billings</surname><given-names>C. E.</given-names></name>
</person-group> (<year>1997</year>). <source>Aviation automation: The search for a human-centered approach</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr2-1064804613491268">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dey</surname><given-names>A. K.</given-names></name>
</person-group> (<year>2009</year>). <source>Context-aware computing in ubiquitous computing fundamentals</source> (<person-group person-group-type="editor">
<name><surname>Krumm</surname><given-names>J.</given-names></name>
</person-group>, ed.). <publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>Chapman and Hall/CRC Press</publisher-name>.</citation>
</ref>
<ref id="bibr3-1064804613491268">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Klein</surname><given-names>G.</given-names></name>
<name><surname>Woods</surname><given-names>D. D.</given-names></name>
<name><surname>Bradshaw</surname><given-names>J. M.</given-names></name>
<name><surname>Hoffman</surname><given-names>R.</given-names></name>
<name><surname>Feltovich</surname><given-names>P.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Ten challenges for making automation a “team player” in joint human-agent activity</article-title>. <source>IEEE Intelligent Systems</source>, <volume>9</volume>(<issue>6</issue>), <fpage>91</fpage>–<lpage>95</lpage>.</citation>
</ref>
<ref id="bibr4-1064804613491268">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>A review of a pivotal <italic>Human Factors</italic> article: “Humans and automation: Use, misuse, disuse, abuse.”</article-title> <source>Human Factors</source>, <volume>50</volume>, <fpage>404</fpage>–<lpage>410</lpage>.</citation>
</ref>
<ref id="bibr5-1064804613491268">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lowy</surname><given-names>J.</given-names></name>
</person-group> (<year>2011</year>, <month>August</month> <day>31</day>). <article-title>Automation in the sky dulls airline-pilot skill</article-title>. <source>Daily News Los Angeles</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.dailynews.com/business/ci_18792474">http://www.dailynews.com/business/ci_18792474</ext-link></citation>
</ref>
<ref id="bibr6-1064804613491268">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Miller</surname><given-names>C. A.</given-names></name>
</person-group> (<year>2002</year>). <source>Definitions and dimensions of etiquette: The AAAI Fall Symposium on Etiquette for Human-Computer Work</source> (Tech. Rep. FS-02-02). <publisher-loc>Menlo Park, CA</publisher-loc>: <publisher-name>AAAI</publisher-name>.</citation>
</ref>
<ref id="bibr7-1064804613491268">
<citation citation-type="book">
<collab>National Transportation Safety Board</collab>. (<year>2009</year>). <source>Operational Factors/Human Performance Group chairman’s factual report</source> (DCA10IA001). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr8-1064804613491268">
<citation citation-type="book">
<collab>National Transportation Safety Board</collab>. (<year>2010</year>). <source>Aviation accident report: Loss of control on approach Colgan Air, Inc. operating as Continental Connection Flight 3407 Bombardier DHC-8-400, N200WQ Clarence Center New York, February 12, 2009</source> (NTSB/AAR-10/01 PB2010-910401). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr9-1064804613491268">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parasuraman</surname><given-names>R.</given-names></name>
<name><surname>Miller</surname><given-names>C.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Trust and etiquette in high-criticality automated systems</article-title>. <source>Communications of the ACM</source>, <volume>47</volume>(<issue>4</issue>), <fpage>51</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr10-1064804613491268">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Reason</surname><given-names>J.</given-names></name>
</person-group> (<year>1990</year>). <source>Human error</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-1064804613491268">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sarter</surname><given-names>N. B.</given-names></name>
<name><surname>Woods</surname><given-names>D. D.</given-names></name>
<name><surname>Billings</surname><given-names>C. E.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Automation surprises</article-title>. In <person-group person-group-type="editor">
<name><surname>Salvendy</surname><given-names>G.</given-names></name>
</person-group> (Ed.), <source>Handbook of human factors and ergonomics</source> (<edition>2nd ed.</edition>, pp. <fpage>1926</fpage>–<lpage>1943</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr12-1064804613491268">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
<name><surname>Hooey</surname><given-names>B. L.</given-names></name>
<name><surname>Gore</surname><given-names>B. F.</given-names></name>
<name><surname>Sebok</surname><given-names>A.</given-names></name>
<name><surname>Koenicke</surname><given-names>C. S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Identifying black swans in NextGen: Predicting human performance in off-nominal conditions</article-title>. <source>Human Factors</source>, <volume>51</volume>, <fpage>638</fpage>–<lpage>651</lpage>.</citation>
</ref>
<ref id="bibr13-1064804613491268">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wiegmann</surname><given-names>D. A.</given-names></name>
<name><surname>Shappell</surname><given-names>S. A.</given-names></name>
</person-group> (<year>2003</year>). <source>A human error approach to aviation accident analysis: The Human Factors Analysis and Classification System</source>. <publisher-loc>Burlington, VT</publisher-loc>: <publisher-name>Ashgate</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>