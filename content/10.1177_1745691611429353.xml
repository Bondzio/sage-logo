<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PPS</journal-id>
<journal-id journal-id-type="hwp">sppps</journal-id>
<journal-id journal-id-type="nlm-ta">Perspect Psychol Sci</journal-id>
<journal-title>Perspectives on Psychological Science</journal-title>
<issn pub-type="ppub">1745-6916</issn>
<issn pub-type="epub">1745-6924</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1745691611429353</article-id>
<article-id pub-id-type="publisher-id">10.1177_1745691611429353</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Special Section: Sizes of Our Science</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Bite-Size Science and Its Undesired Side Effects</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Bertamini</surname><given-names>Marco</given-names></name>
<xref ref-type="aff" rid="aff1-1745691611429353">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Munafò</surname><given-names>Marcus R.</given-names></name>
<xref ref-type="aff" rid="aff2-1745691611429353">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-1745691611429353"><label>1</label>University of Liverpool</aff>
<aff id="aff2-1745691611429353"><label>2</label>University of Bristol</aff>
<author-notes>
<corresp id="corresp1-1745691611429353">Marco Bertamini, University of Liverpool, Bedford Street South, School of Psychology, Liverpool, L697ZA, UK E-mail: <email>M.Bertamini@liv.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2012</year>
</pub-date>
<volume>7</volume>
<issue>1</issue>
<fpage>67</fpage>
<lpage>71</lpage>
<permissions>
<copyright-statement>© Association for Psychological Science 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>Short and rapid publication of research findings has many advantages. However, there is another side of the coin that needs careful consideration. We argue that the most dangerous aspect of a shift toward “bite-size” publishing is the relationship between study size and publication bias. Findings based on a single study or a study based on a limited sample size are more likely to be false positive, because the false positive rate remains constant, whereas the true positive rate (the power) declines as sample size declines. Pressure on productivity and on novelty value further exacerbates the problem.</p>
</abstract>
<kwd-group>
<kwd>allied field</kwd>
<kwd>behavioral</kwd>
<kwd>cognition</kwd>
<kwd>perception</kwd>
<kwd>publishing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p><disp-quote>
<p>“Delay is preferable to error”</p>
<attrib>Thomas Jefferson</attrib>
</disp-quote></p>
<sec id="section1-1745691611429353">
<title>“The Culture Demands Shorter Reports”</title>
<p>In journals from all areas of psychology, there is a trend toward short and rapid publications. A recent example is a new type of article in the <italic>Journal of Experimental Psychology: Human Perception and Performance</italic> called “observations” (2,000 words maximum). In the words of the incoming Editor, James Enns, “The culture demands shorter reports and rapid publication” (<xref ref-type="bibr" rid="bibr18-1745691611429353">Price, 2011</xref>, p. 68).</p>
<p>Those in the field have claimed that there are many advantages for short reports: faster communication of results; ease of assimilation; ease of access for people outside the field; ease of processing for editors and reviewers; and more dynamic exchange of fresh ideas, even if they may turn out to be wrong. In addition to these advantages, there are practical considerations affecting this trend. The main one is an increased pressure on researchers to produce quantifiable output. Despite an ostensible emphasis on quality, quantity is still important, as it contributes to résumé length, exposure, name recognition, and summary statistics of productivity; after all, the units used in most citation analyses are individual articles. In their article, <xref ref-type="bibr" rid="bibr9-1745691611429353">Ledgerwood and Sherman (2012</xref>, this issue) have provided a table of pros and cons of the short format, and we share some of their concerns.</p>
<p>In this article, we first address the issue of whether article impact should be adjusted for number of pages. We then discuss some of the more general problems of the short article format. After that, we turn to the link between study size and publication bias, which is, in our opinion, the single most serious source of danger. Finally, in our conclusions, we suggest that evaluating journal impact in terms of citation should be balanced with some measure of publication bias and that replication of unexpected findings should be encouraged during the editorial process.</p>
</sec>
<sec id="section2-1745691611429353">
<title>Haslam’s Citation Analysis</title>
<p>Some empirical evidence seems to suggest that the trend toward short articles is beneficial. In particular, <xref ref-type="bibr" rid="bibr4-1745691611429353">Haslam (2010)</xref> has shown that, when adjusted for length, short articles are cited more frequently than other articles. Based on this, Haslam concluded that “short articles appear to be somewhat more efficient in generating scientific influence” and that “journals [..] might be well advised to add a short-article format” (p. 264).</p>
<p>Short articles are often, though not always, single study articles and tend to include smaller samples. This single-study model is broadly consistent with the neuroscience model advocated also by <xref ref-type="bibr" rid="bibr16-1745691611429353">Park (2009)</xref>. Park also touches on forcing people to be concise and on an increased use of the Internet to store data and stimuli. These are separate issues, but the idea that “bite-size” publishing results in greater impact is more problematic.</p>
<p>There are some technical problems with a citation impact adjusted for length, and it should not be taken as a bona fide superior measure of impact. If the same findings can be written in either a short or long format, and assuming that the two articles would get cited equally, the impact per page would be higher for the short article, but it would be misleading to say that the short article has achieved any greater impact than the long one. Moreover, suppose I conduct two studies providing converging evidence for the same conclusion and I can publish them in one long article or in two short articles. My colleagues A. Friend and A. Foe always cite all my work because it is relevant for what they do. They will cite either one long or two short articles in all their publications. Based on their citations, each of the three articles would have the same impact, but on a per-page measure, the shorter articles are more influential. This would be purely because of how we measure impact, not because of a difference in influence.</p>
<p>As an example of a short but highly influential article, <xref ref-type="bibr" rid="bibr4-1745691611429353">Haslam (2010)</xref> cites <xref ref-type="bibr" rid="bibr24-1745691611429353">Watson and Crick’s (1953)</xref> report on the structure of DNA. However, that was a theoretical piece proposing a specific model. The debate about format in psychology is focused on empirical reports, and different considerations may apply in the absence of data. This issue could be explored by looking at impact for nonempirical articles. However, the three journals analyzed by Haslam (<italic>Psychological Science, Cognition, Journal of Experimental Social Psychology</italic>) publish mainly empirical findings and, therefore, do not lend themselves to this analysis.</p>
</sec>
<sec id="section3-1745691611429353">
<title>Problems Associated With Short Reports</title>
<p>Part of the argument for the short format is that, other things being equal, shorter articles are easier and quicker to read. But assuming a fixed amount of studies are carried out, shorter articles may mean more articles. If a journal were to switch to the short-article format while keeping the yearly number of articles fixed, then that might shift articles onto other journals, and onto new journals. The growth in the number of peer-reviewed articles and in the number of scientific journals over time is well documented (e.g., <xref ref-type="bibr" rid="bibr8-1745691611429353">Larsen &amp; von Ins, 2010</xref>; <xref ref-type="bibr" rid="bibr14-1745691611429353">Neff &amp; Olden, 2010</xref>). The increase in articles creates more work for the editors, for the reviewers, and perhaps most importantly, for anyone researching the topic. The change to the short format may be meant to help reviewers, but it may backfire as there could be more requests to review (<xref ref-type="bibr" rid="bibr5-1745691611429353">Huth, 1986</xref>). The same can be said about the way short articles speed up processing of a manuscript by allowing editors to decide quickly about acceptance or rejection. If rejected papers are resubmitted after minor changes to a different journal, and if we consider the community of reviewers as a whole, then this quick turnaround strategy will not lighten their workload.</p>
<p>As authors, we may all get mildly annoyed at reviewers asking for an extra experiment or just more evidence to be convinced (<xref ref-type="bibr" rid="bibr17-1745691611429353">Ploegh, 2011</xref>). On the other hand, this kind of back and forth correspondence between peers has a long and illustrious tradition in the history of science. Often an extra experiment does change the conclusions of a paper, therefore avoiding the publication of a partial or distorted result.</p>
<p>If replication is a cornerstone of the scientific method, then an advantage of multiexperiment papers is that replication is inherent and usually rather stringently defined (because the experiments come from the same lab). This latter point is important—replication is no “silver bullet,” and loose definitions of replication can result in initial false positive findings being propagated in the literature (<xref ref-type="bibr" rid="bibr23-1745691611429353">Sullivan, 2007</xref>).</p>
<p>Short articles may also be more prone to what has been described as “citation amnesia” (<xref ref-type="bibr" rid="bibr3-1745691611429353">Garfield, 1982</xref>; <xref ref-type="bibr" rid="bibr19-1745691611429353">Robinson &amp; Goodman, 2011</xref>), especially when the authors have to meet tight word count criteria. A finding is bound to sound more newsworthy when the discussion of previous relevant work is less detailed, therefore, there is pressure on authors not to go into great depth when researching and discussing previous work. A bit of ignorance helps in discovering “new” things.</p>
</sec>
<sec id="section4-1745691611429353">
<title>Publication Bias</title>
<p>We want to turn to what, in our opinion, is a crucial issue of the implications of short-article publishing: the link between study size and publication bias.</p>
<p>This bias is nicely captured by a funnel plot (<xref ref-type="fig" rid="fig1-1745691611429353">Fig. 1</xref>), which has become an important tool in biomedical research (<xref ref-type="bibr" rid="bibr21-1745691611429353">Sterne &amp; Egger, 2001</xref>). A short article tends to contain less data than a long article. The smaller dataset (in particular, small sample size) implies more variability in the results, and this is the variability that is shown in a funnel plot: Precision (which is usually measured as the inverse of the standard error and is directly related to study size) is on the <italic>y</italic> axis and effect size is on the <italic>x</italic> axis. Larger studies will be more accurate and will show a narrow range of estimates around the true population effect. Smaller, less accurate studies will give a wider distribution of estimates, although if all are published, these should still center on the true population effect. Therefore, this funnel should be symmetrical if there is no bias operating. Often it is not. The asymmetry implies a disproportionally large number of published effects in one direction. These deviations from the true effect are concentrated at the bottom of the funnel because it is more likely to obtain large deviations from small studies (because of the law of large numbers). Small studies that generate either null results or results that are the opposite of what the author(s) predicted will be systematically censored (i.e., there will be publication bias), and this will be reflected in an asymmetrical funnel. Formal regression-based tests exist to assess this (<xref ref-type="bibr" rid="bibr2-1745691611429353">Egger, Smith, Schneider, &amp; Minder, 1997</xref>).</p>
<fig id="fig1-1745691611429353" position="float">
<label>Fig. 1.</label>
<caption>
<p>A funnel plot is a scatterplot where each datapoint represents a single published study, with effect size plotted against precision (typically the inverse of the individual study standard error). Effect size may be a difference (d), a correlation coefficient (r), or other metric such as an odds ratio. The effect size may reflect either a positive or negative association, on either side of the point where there is no difference). The dashed vertical line shows the pooled effect size estimate when all studies are combined (e.g., in a meta-analysis). In the example shown, the pooled effect size estimate is not null, and therefore the line is to the left of the location where the difference is zero. More precise studies (e.g., larger) give a narrower distribution of effect size estimates, while less precise studies (e.g., smaller) give a wider distribution of estimates, with both centered on the true poputation effect. The theoretical distribution of effect sizes, therefore, should be a symmetrical funnel, centered on the true population effect (left panel). The pooled estimate will differ from the population effect if it is affected by publication bias (right panel). Some small studies that show either no effect or an effect opposite to that anticipated are missing, and the shape of the funnel is distorted. The presence of this asymmetry can be tested formally using regression-based methods (<xref ref-type="bibr" rid="bibr2-1745691611429353">Egger et al., 1997</xref>).</p>
</caption>
<graphic xlink:href="10.1177_1745691611429353-fig1.tif"/>
</fig>
<p>A more subtle problem is that results in small studies are more likely to be false positives if they are statistically significant. This is because the false positive rate remains constant at 5%, whereas the true positive rate (the power) depends on sample size. Therefore, as power/sample size decreases, the ratio of true positives to false positives among those studies that achieve statistical significance also decreases (<xref ref-type="bibr" rid="bibr22-1745691611429353">Sterne &amp; Smith, 2001</xref>). We know that studies in psychology are endemically underpowered (<xref ref-type="bibr" rid="bibr20-1745691611429353">Sedlmeier &amp; Gigerenzer, 1989</xref>), and this, therefore, increases the risk of false positives. A culture of brief, single-study reports based on small datasets will inevitably lead to even greater contamination of the literature by false positive findings.</p>
<p>False positives are a problem because “erroneous ideas much more easily enter the literature than leave it” (<xref ref-type="bibr" rid="bibr10-1745691611429353">McManus, 2002</xref>, p. 322). (McManus was writing about the myths that left-handers die younger and suffer more from immune disorders.)</p>
<p>A publication bias may originate from several sources: dishonesty is probably the least likely. The main issue is how journals lead to selective publication: significant effects and, in particular, results that are perceived as “newsworthy” are more likely to be submitted and more likely to be accepted for publication (<xref ref-type="bibr" rid="bibr1-1745691611429353">Dickersin, Chan, Chalmers, Sacks, &amp; Smith, 1987</xref>). That is, extreme values have greater novelty value. Use of multiple analyses on the same dataset and selective reporting of statistics can also produce an inflated effect (<xref ref-type="bibr" rid="bibr15-1745691611429353">Nieuwenhuis, Forstmann, &amp; Wagenmakers, 2011</xref>).</p>
<p>The existence of publication biases can lead to an artificially inflated sense of the strength (or even presence) of an effect from a casual inspection of the literature. Moreover, the extent to which an individual study overestimates any underlying true effect correlates with the impact factor of a journal (<xref ref-type="bibr" rid="bibr13-1745691611429353">Munafò, Stothart, &amp; Flint, 2009</xref>). This may seem astonishing and hard to explain, but one should consider that high impact factor journals do seek novel and ground-breaking findings and that they often publish short articles. In the words of <xref ref-type="bibr" rid="bibr13-1745691611429353">Munafò et al. (2009)</xref>, “We also note that journals with high impact factors tend to publish studies with high bias scores and small sample sizes” (p. 120). Pressure to achieve “impact” may also be stronger in some countries. There is a stronger tendency for individual studies to overestimate the true effect in articles from authors based in North America than in articles from authors based in Europe; this also seems to correlate with the amount of government R&amp;D funding in individual countries (<xref ref-type="bibr" rid="bibr12-1745691611429353">Munafò, Attwood, &amp; Flint, 2008</xref>).</p>
<p>One of the authors has received correspondence from <italic>Psychological Science</italic> saying that they hope to find manuscripts that “report new discoveries that will make our readership sit up and take notice.” These, a priori, are more likely to be false positives (<xref ref-type="bibr" rid="bibr6-1745691611429353">Ioannidis, 2005</xref>). Flukes tend to meet this criterion by their nature (they are surprising and different from what other people tend to find). In part this is unavoidable, but what is relevant here is that bite-size articles make this problem worse. We are all aware of the need for results to be replicated. Long articles with multiple experiments show whether an effect can be replicated and supported by converging evidence.</p>
<p>Using simulations, <xref ref-type="bibr" rid="bibr6-1745691611429353">Ioannidis (2005)</xref> has demonstrated a number of interesting aspects of the problem. For example, he found that the smaller the studies; the “hotter” a scientific field; and the greater the flexibility in designs, definitions, outcomes, and analyses used, the less likely the published research findings are true. He concluded that “statistical significance testing in the report of <italic>a single study</italic> gives only a partial picture, without knowing how much testing has been done outside the report and in the relevant field at large” (p. 701, italics ours). Indeed, there are far more statistically significant findings present in most literatures than we would expect even if the effects reported are indeed real (<xref ref-type="bibr" rid="bibr7-1745691611429353">Ioannidis, 2011</xref>).</p>
<p>We assumed that short articles include less data than long articles. However, one could simply force more data into a smaller space. It is not controversial to say that articles should be concise. To support our assumption of a correlation between sample size and article length, we looked at all articles published in <italic>Cognition</italic> in 2007 (65 regular articles and 35 brief articles). The individuals in the samples were adults, infants, or animals of other species.</p>
<p>The total sample size was, on average, 122 for regular articles and 95 for brief articles. Medians were 80 and 52, respectively. Of the 35 brief articles, 20 reported a single study. The total sample size was, on average, 80 for the single-study brief articles and 102 for the others. Therefore, the link between length and amount of data seems to exist. Brief articles in <italic>Cognition</italic> often include multiple studies, and at more than 11 pages on average, they would probably be full articles in some other journal. What we call brief articles may, therefore, be relative. Nevertheless the correlation between article length and study size seems to exist.</p>
</sec>
<sec id="section5-1745691611429353">
<title>Conclusions</title>
<p>By far the most popular and influential measure of quality for journals is their Impact Factor. This is computed on the basis of citations, and therefore should reflect influence in the field. As critics have often pointed out, it does not distinguish between citations that confirm and extend the original findings and citations that criticize and debunk them. In this sense, journals have little formal incentive to minimize bias in the effects reported and to minimize false alarms, although we have no doubt that every good editor is trying to do that. Including a measure of publication bias in the evaluation of a journal would provide a more direct incentive. By combining different measures, such as number of articles, Impact Factor, and publication bias, we could arrive at better measures of the quality of a journal in reporting interesting but replicable and valid results. At the moment, there are sophisticated tools to count articles published and number of citations. However, it is much more difficult to measure bias and the frequency of false alarms. Systematic and large-scale meta-analyses are the standard tools, but they will not be enough without some increase in published replication studies, and they are not feasible if there are insufficient comparable studies on a particular topic. Unfortunately, the culture of mere replication (and publication of failures to replicate) is not well established in psychology. Although psychology is not unique, a few other disciplines fare better. These are important differences between fields, because we can estimate false alarms (imperfectly) after a field has matured to a necessary degree, but not before.</p>
<p>Despite increased interest in bibliometrics, there is also a growing consciousness of the limitations of any individual index, especially because as soon as one index achieves the status of the measure of choice, with practical implications, authors and institutions will start to adapt and play the system. In the United Kingdom, after an initial intention to rely heavily on metrics for the research exercise that compares all Departments in the country (the Research Assessment Exercise now renamed Research Excellence Framework), the project has been revised to focus on quality and not only on quantity (<ext-link ext-link-type="uri" xlink:href="http://www.hefce.ac.uk/research/ref/">www.hefce.ac.uk/research/ref/</ext-link>). Each author submits only four publications for evaluation by a panel of peers, which partially removes the incentive to seek quantity at the (potential) expense of quality.</p>
<p>In summary, the shift toward shorter articles including fewer studies, the reduced space for locating results in the context of previous findings, and the pressure on productivity and novelty all combine to create a potentially damaging effect on the scientific literature. It is interesting that in fields such as genetic association studies, in which the false-positives problem is well-recognized, independent replication in the same study is now a requirement for publication in many high- quality journals (<xref ref-type="bibr" rid="bibr11-1745691611429353">Munafò, 2010</xref>). Perhaps this is an option that should be at least considered in psychological science.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dickersin</surname><given-names>K.</given-names></name>
<name><surname>Chan</surname><given-names>S.</given-names></name>
<name><surname>Chalmers</surname><given-names>T.</given-names></name>
<name><surname>Sacks</surname><given-names>H.</given-names></name>
<name><surname>Smith</surname><given-names>H.</given-names><suffix>Jr.</suffix></name>
</person-group> (<year>1987</year>). <article-title>Publication bias and clinical trials</article-title>. <source>Controlled Clinical Trials</source>, <volume>8</volume>, <fpage>343</fpage>–<lpage>353</lpage>. doi:10.1016/0197-2456(87)90155-3<pub-id pub-id-type="doi">10.1016/0197-2456(87)90155-3</pub-id></citation>
</ref>
<ref id="bibr2-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Egger</surname><given-names>M.</given-names></name>
<name><surname>Smith</surname><given-names>G. D.</given-names></name>
<name><surname>Schneider</surname><given-names>M.</given-names></name>
<name><surname>Minder</surname><given-names>C.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Bias in meta-analysis detected by a simple, graphical test</article-title>. <source>British Medical Journal</source>, <volume>315</volume>, <fpage>629</fpage>–<lpage>634</lpage>.</citation>
</ref>
<ref id="bibr3-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Garfield</surname><given-names>E.</given-names></name>
</person-group> (<year>1982</year>). <article-title>More on the ethics of scientific publication: Abuses of authorship attribution and citation amnesia undermine the reward system of science</article-title>. <source>Current Contents</source>, <volume>30</volume>, <fpage>5</fpage>–<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr4-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haslam</surname><given-names>N.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Bite-size science: Relative impact of short article formats</article-title>. <source>Perspectives on Psychological Science</source>, <volume>5</volume>, <fpage>263</fpage>–<lpage>264</lpage>. doi:10.1177/1745691610369466<pub-id pub-id-type="doi">10.1177/1745691610369466</pub-id></citation>
</ref>
<ref id="bibr5-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Huth</surname><given-names>E. J.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Irresponsible authorship and wasteful publication</article-title>. <source>Annals of Internal Medicine</source>, <volume>104</volume>, <fpage>257</fpage>–<lpage>259</lpage>.</citation>
</ref>
<ref id="bibr6-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Why most published research findings are false</article-title>. <source>PLoS Medicine</source>, <volume>2</volume>, <fpage>e124</fpage>.</citation>
</ref>
<ref id="bibr7-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Excess significance bias in the literature on brain volume abnormalities</article-title>. <source>Archives of General Psychiatry</source>, <volume>68</volume>, <fpage>773</fpage>–<lpage>801</lpage>. doi:10.1001/archgenpsychiatry.2011.28<pub-id pub-id-type="doi">10.1001/archgenpsychiatry.2011.28</pub-id></citation>
</ref>
<ref id="bibr8-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Larsen</surname><given-names>P. O.</given-names></name>
<name><surname>von Ins</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The rate of growth in scientific publication and the decline in coverage provided by Science Citation Index</article-title>. <source>Scientometrics</source>, <volume>84</volume>, <fpage>575</fpage>–<lpage>603</lpage>. doi:10.1007/s11192-010-0202-z<pub-id pub-id-type="doi">10.1007/s11192-010-0202-z</pub-id></citation>
</ref>
<ref id="bibr9-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ledgerwood</surname><given-names>A.</given-names></name>
<name><surname>Sherman</surname><given-names>J. W.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Short, sweet, and problematic? The rise of the short report in psychological science</article-title>. <source>Perspectives on Psychological Science</source>, <volume>7</volume>, <fpage>xxx</fpage>–<lpage>xxx</lpage>.</citation>
</ref>
<ref id="bibr10-1745691611429353">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McManus</surname><given-names>I. C.</given-names></name>
</person-group> (<year>2002</year>). <source>Right hand, left hand</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Weidenfeld &amp; Nicolson</publisher-name>.</citation>
</ref>
<ref id="bibr11-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Munafò</surname><given-names>M. R.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Credible genetic associations?</article-title> <source>International Journal of Molecular Epidemiology and Genetics</source>, <volume>1</volume>, <fpage>31</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr12-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Munafò</surname><given-names>M. R.</given-names></name>
<name><surname>Attwood</surname><given-names>A. S.</given-names></name>
<name><surname>Flint</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Relationship between geographical location of research group and effect size estimate</article-title>. <source>Psychological Medicine</source>, <volume>38</volume>, <fpage>1213</fpage>–<lpage>1214</lpage>. doi:10.1017/S003329170800353X<pub-id pub-id-type="doi">10.1017/S003329170800353X</pub-id></citation>
</ref>
<ref id="bibr13-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Munafò</surname><given-names>M. R.</given-names></name>
<name><surname>Stothart</surname><given-names>G.</given-names></name>
<name><surname>Flint</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Bias in genetic association studies and impact factor</article-title>. <source>Molecular Psychiatry</source>, <volume>14</volume>, <fpage>119</fpage>–<lpage>120</lpage>. doi:10.1038/mp.2008.77<pub-id pub-id-type="doi">10.1038/mp.2008.77</pub-id></citation>
</ref>
<ref id="bibr14-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Neff</surname><given-names>B. D.</given-names></name>
<name><surname>Olden</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Not so fast: Inflation in impact factors contributes to apparent improvements in journal quality</article-title>. <source>BioScience</source>, <volume>60</volume>, <fpage>455</fpage>. doi:10.1525/bio.2010.60.6.9<pub-id pub-id-type="doi">10.1525/bio.2010.60.6.9</pub-id></citation>
</ref>
<ref id="bibr15-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nieuwenhuis</surname><given-names>S.</given-names></name>
<name><surname>Forstmann</surname><given-names>B. U.</given-names></name>
<name><surname>Wagenmakers</surname><given-names>E. J.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Erroneous analyses of interactions in neuroscience: A problem of significance</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>, <fpage>1105</fpage>–<lpage>1107</lpage>. doi:10.1038/nn.2886<pub-id pub-id-type="doi">10.1038/nn.2886</pub-id></citation>
</ref>
<ref id="bibr16-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Park</surname><given-names>D. C.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Publishing in the psychological sciences</article-title>. <source>Perspectives on Psychological Science</source>, <volume>4</volume>, <fpage>36</fpage>–<lpage>37</lpage>. doi:10.1111/j.1745-6924.2009.01103.x<pub-id pub-id-type="doi">10.1111/j.1745-6924.2009.01103.x</pub-id></citation>
</ref>
<ref id="bibr17-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ploegh</surname><given-names>H.</given-names></name>
</person-group> (<year>2011</year>). <article-title>End the wasteful tyranny of reviewer experiments</article-title>. <source>Nature</source>, <volume>472</volume>, <fpage>391</fpage>.</citation>
</ref>
<ref id="bibr18-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Price</surname><given-names>M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Restoring glory</article-title>. <source>Monitor</source>, <volume>42</volume>, <fpage>68</fpage>. doi:10.1038/472391a<pub-id pub-id-type="doi">10.1038/472391a</pub-id></citation>
</ref>
<ref id="bibr19-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Robinson</surname><given-names>K. A.</given-names></name>
<name><surname>Goodman</surname><given-names>S. N.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A systematic examination of the citation of prior research in reports of randomized, controlled trials</article-title>. <source>Annals of Internal Medicine</source>, <volume>154</volume>, <fpage>50</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr20-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sedlmeier</surname><given-names>P.</given-names></name>
<name><surname>Gigerenzer</surname><given-names>G.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Do studies of statistical power have an effect on the power of studies?</article-title> <source>Psychological Bulletin</source>, <volume>105</volume>, <fpage>309</fpage>–<lpage>316</lpage>. doi:10.1037/0033-2909.105.2.309<pub-id pub-id-type="doi">10.1037/0033-2909.105.2.309</pub-id></citation>
</ref>
<ref id="bibr21-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sterne</surname><given-names>J. A. C.</given-names></name>
<name><surname>Egger</surname><given-names>M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Funnel plots for detecting bias in meta-analysis: Guidelines on choice of axis</article-title>. <source>Journal of Clinical Epidemiology</source>, <volume>54</volume>, <fpage>1046</fpage>–<lpage>1104</lpage>.</citation>
</ref>
<ref id="bibr22-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sterne</surname><given-names>J. A. C.</given-names></name>
<name><surname>Smith</surname><given-names>G. D.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Sifting the evidence—What’s wrong with significance tests?</article-title> <source>British Medical Journal</source>, <volume>322</volume>, <fpage>226</fpage>–<lpage>231</lpage>. doi:10.1136/bmj.322.7280.226<pub-id pub-id-type="doi">10.1136/bmj.322.7280.226</pub-id></citation>
</ref>
<ref id="bibr23-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sullivan</surname><given-names>P. F.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Spurious genetic associations</article-title>. <source>Biological Psychiatry</source>, <volume>61</volume>, <fpage>1121</fpage>–<lpage>1126</lpage>. doi:10.1016/j.biopsych.2006.11.010<pub-id pub-id-type="doi">10.1016/j.biopsych.2006.11.010</pub-id></citation>
</ref>
<ref id="bibr24-1745691611429353">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Watson</surname><given-names>J. D.</given-names></name>
<name><surname>Crick</surname><given-names>F. H. C.</given-names></name>
</person-group> (<year>1953</year>). <article-title>Molecular structure of nucleic acids: A structure for deoxyribose nucleic acid</article-title>. <source>Nature</source>, <volume>171</volume>, <fpage>737</fpage>–<lpage>738</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>