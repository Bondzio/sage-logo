<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPA</journal-id>
<journal-id journal-id-type="hwp">spepa</journal-id>
<journal-title>Educational Evaluation and Policy Analysis</journal-title>
<issn pub-type="ppub">0162-3737</issn>
<issn pub-type="epub">1935-1062</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/0162373712454327</article-id>
<article-id pub-id-type="publisher-id">10.3102_0162373712454327</article-id>
<title-group>
<article-title>The Academic Effects of Summer Instruction and Retention in New York City</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Mariano</surname><given-names>Louis T.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Martorell</surname><given-names>Paco</given-names></name>
</contrib>
<aff id="aff1-0162373712454327">RAND Corporation</aff>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>35</volume>
<issue>1</issue>
<fpage>96</fpage>
<lpage>117</lpage>
<permissions>
<copyright-statement>© 2012 AERA</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>This article examines the impacts of summer instruction and test-based grade retention in New York City. We use a research design that exploits test score cutoffs used in assignment to these treatments. We find modest positive effects of summer instruction on English language arts (ELA) achievement for students assigned to summer instruction because of poor ELA performance but find little evidence of positive effects of summer instruction on math outcomes. After netting out estimates of differential test score growth within grades across years, the estimated effects of grade retention are substantial and positive through seventh grade on both math and ELA outcomes, suggesting that the additional year of instruction in fifth grade leads to improvements in subsequent grade achievement.</p>
</abstract>
<kwd-group>
<kwd>student retention</kwd>
<kwd>summer instruction</kwd>
<kwd>regression discontinuity</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>IN efforts to increase accountability for student performance, numerous states and districts have instituted test-based promotion policies that require students to score at a certain level on standards-based assessments to advance to the next grade. A key component of such policies is the assignment of students at risk of being retained in grade to summer school for additional instruction, which allows struggling students an additional opportunity to gain the required understanding of the curriculum required to be promoted with their peers. A recent review by <xref ref-type="bibr" rid="bibr27-0162373712454327">Marsh, Gershwin, Kirby, and Xia (2009)</xref> identified six states and 13 large school districts that assign low-performing students to mandatory summer school and retain students who do not demonstrate sufficient improvement during the summer.</p>
<p>Despite their prevalence, considerable uncertainty remains concerning the efficacy of these types of programs. Although the literature shows that summer school participants make gains during the summer (<xref ref-type="bibr" rid="bibr5-0162373712454327">Cooper, Charlton, Valentine, &amp; Muhlenbruck, 2000</xref>), less is known about the effect of summer school on achievement in later grades. In addition, there is debate about the magnitude of gains made by students in summer school, with better designed studies’ showing smaller gains than less rigorous studies (<xref ref-type="bibr" rid="bibr5-0162373712454327">Cooper et al., 2000</xref>).</p>
<p>The use of standards-based assessments in promotion decisions continues to be highly controversial. As advocated by former president <xref ref-type="bibr" rid="bibr4-0162373712454327">Clinton (1998)</xref>, proponents view such use as a necessity for ensuring students master the material of the current grade and preparing them to successfully engage at the next level, with the long-term goal of leaving high school equipped for college and with the skills needed for employment. Opponents point to a large literature (<xref ref-type="bibr" rid="bibr12-0162373712454327">Holmes, 1989</xref>; <xref ref-type="bibr" rid="bibr21-0162373712454327">Jimerson, 2001</xref>) that associates discretionary retention with negative outcomes including poorer academic performance and increased risk of dropout. However, more recent work has cast doubts on prior conclusions regarding the impact of retention (<xref ref-type="bibr" rid="bibr2-0162373712454327">Allen, Chen, Willson, &amp; Hughes, 2009</xref>), with studies of test-based retention showing mixed results (<xref ref-type="bibr" rid="bibr9-0162373712454327">Greene &amp; Winters, 2007</xref>; <xref ref-type="bibr" rid="bibr15-0162373712454327">Hughes, Chen, Thoemmes, &amp; Kwok, 2010</xref>; <xref ref-type="bibr" rid="bibr19-0162373712454327">Jacob &amp; Lefgren, 2004</xref>, <xref ref-type="bibr" rid="bibr20-0162373712454327">2009</xref>), underscoring the value of additional research.</p>
<p>In this article, we separately estimate the causal effects of summer school and grade retention on future academic achievement for fifth-grade students in New York City Department of Education (NYCDOE) schools. Since 2004, NYCDOE has had a policy whereby students scoring less than a specified cutoff on the fifth-grade spring assessment in either English language arts (ELA) or math are assigned to summer instruction.<sup><xref ref-type="fn" rid="fn1-0162373712454327">1</xref></sup> Summer school students scoring less than a summer assessment cutoff are then eligible to be retained. NYCDOE’s promotion policy is of particular interest because it may serve as an example for districts considering similar initiatives. It focuses on identifying underperforming students early in the year and providing these students additional instructional services throughout the year. This focus on early identification and provision of services during the school year and the summer aligns it with recommendations of the National Research Council (<xref ref-type="bibr" rid="bibr11-0162373712454327">Heubert &amp; Hauser, 1999</xref>) for enhancing the validity and fairness of test-based student promotion (<xref ref-type="bibr" rid="bibr22-0162373712454327">Kirby, McCombs, &amp; Mariano, 2009</xref>). In addition, as the nation’s largest district, NYCDOE holds more students to test-based policies than any other.</p>
<p>Our empirical strategy involves comparing the outcomes of students falling just less than or greater than the treatment cutoffs. These students likely will be similar in terms of baseline observed and unobserved characteristics but differ in the probability of receiving the treatments. This suggests that fuzzy regression discontinuity (FRD) can be used to identify the effects of summer instruction and grade retention (<xref ref-type="bibr" rid="bibr10-0162373712454327">Hahn, Todd, &amp; van der Klaauw, 2001</xref>; <xref ref-type="bibr" rid="bibr23-0162373712454327">D. Lee, 2008</xref>) for students scoring close to the treatment assignment cutoffs.</p>
<p>This article builds on prior studies using FRD methods to study these treatments (<xref ref-type="bibr" rid="bibr9-0162373712454327">Greene &amp; Winters, 2007</xref>; <xref ref-type="bibr" rid="bibr19-0162373712454327">Jacob &amp; Lefgren, 2004</xref>; <xref ref-type="bibr" rid="bibr29-0162373712454327">Matsudaira, 2008</xref>) in several ways. First, we use same-grade comparisons of postretention outcomes to test directly a central hypothesis made by advocates of the policy, namely, that retention will lead to improved mastery of the curriculum standards in subsequent grades. Second, we utilize an anomaly in test administration to isolate the effect of summer school instruction from the effect of subsequent grade retention. Finally, we adjust FRD treatment estimates for the influence of other reform efforts and potential incongruencies in testing that might confound estimates of the effect of retention. Although we think this is a crucial step in credibly estimating the effect of grade retention, it is important to note that the adjustments we make rely on stronger assumptions than FRD analyses usually require, thus reducing the level of internal validity from that commonly associated with FRD estimates.</p>
<p>Our analyses reveal a modest positive effect of summer instruction on sixth-grade ELA among students assigned to summer school for doing poorly on the ELA exam. However, the estimates for math are smaller and sensitive to modeling choices. The estimated impacts of grade retention are substantial and positive through seventh grade in both math and ELA, suggesting that students benefit from the additional instruction entailed in spending another year in fifth grade.</p>
<sec id="section1-0162373712454327">
<title>Background</title>
<sec id="section2-0162373712454327">
<title>Prior Literature on the Impact of Summer School</title>
<p>A review and meta-analysis of the literature by <xref ref-type="bibr" rid="bibr5-0162373712454327">Cooper et al. (2000)</xref> find that most studies on the effect of summer remedial programs report positive estimates, with an average effect size of 0.26. However, estimates from their meta-analysis restricted to comparison groups at least matching on prior academic achievement yielded a smaller average effect size of 0.14. Even these studies could be subject to selection bias if summer school participants differ systematically from nonparticipants in ways that are difficult to observe.</p>
<p>Focusing on test-based summer remediation, two recent studies use a unidimensional FRD design to examine the effect on future grade outcomes. <xref ref-type="bibr" rid="bibr19-0162373712454327">Jacob and Lefgren (2004)</xref> find positive effects of summer instruction for both math and reading 2 years after summer school. <xref ref-type="bibr" rid="bibr29-0162373712454327">Matsudaira (2008)</xref> finds overall effect sizes of about 0.12 standard deviations for both math and reading after 2 years, although these estimates vary considerably across grades.</p>
<p>These FRD studies highlight two analytical challenges in estimating effects of summer instruction. One concerns the multidimensionality of treatment assignment in that scoring less than the cutoff in either math or ELA triggers assignment to summer remediation, and all of their analyses exclude students who were assigned to summer school because of scoring less than both the math and ELA cutoffs. The second challenge is that some summer school attendees are eventually retained, and subsequent outcomes for these students reflect both the summer and retention treatments, implying that grade retention could confound estimates of summer school efficacy. <xref ref-type="bibr" rid="bibr19-0162373712454327">Jacob and Lefgren (2004)</xref> subtract retention effect estimates to gain an estimate of the pure summer school effect under the assumption that the two effects are additively separable. <xref ref-type="bibr" rid="bibr29-0162373712454327">Matsudaira (2008)</xref> treats retained student outcomes as missing, noting that “implementation of this retention aspect of the policy, however, appears to have been rather lax” (p. 846). In contrast, our study uses data from a cohort in which a grading anomaly led to very few students’ being retained at the cut score for summer school assignment, allowing us to estimate cleanly the effect of summer school without confounding from retention.</p>
</sec>
<sec id="section3-0162373712454327">
<title>Prior Literature on Grade Retention</title>
<p>A large literature on the effect of grade retention (summarized in meta-analyses by <xref ref-type="bibr" rid="bibr12-0162373712454327">Holmes, 1989</xref>; <xref ref-type="bibr" rid="bibr21-0162373712454327">Jimerson, 2001</xref>), including a study on an earlier New York City policy (<xref ref-type="bibr" rid="bibr14-0162373712454327">House, 1998</xref>), finds that retention is negatively associated with academic performance. However, this work has been criticized as having serious methodological flaws (cf. <xref ref-type="bibr" rid="bibr1-0162373712454327">Alexander, Entwisle, &amp; Dauber, 2003</xref>; <xref ref-type="bibr" rid="bibr2-0162373712454327">Allen et al., 2009</xref>; <xref ref-type="bibr" rid="bibr13-0162373712454327">Hong &amp; Raudenbush, 2005</xref>; <xref ref-type="bibr" rid="bibr25-0162373712454327">Lorence, Dworkin, Toenjes, &amp; Hill, 2002</xref>) relating to the difficulty in establishing a viable comparison group for retained students.</p>
<p>Several recent studies specifically examine the effects of test-based retention policies and use clearly defined retention eligibility criteria to generate credible control groups for retained students. <xref ref-type="bibr" rid="bibr19-0162373712454327">Jacob and Lefgren (2004)</xref> examine grade retention in Chicago using the single-subject FRD methods discussed above and find initial small positive effects in reading and math for third graders that dissipate after the 2nd year and 2nd-year effects for sixth graders that were negative but not statistically significant. Another study (<xref ref-type="bibr" rid="bibr37-0162373712454327">Roderick &amp; Nagaoka, 2005</xref>) finds similar results in Chicago using growth curve and two-stage probit least squares models for students falling just greater than and less than the promotion cutoff, although their analysis does find significant negative sixth-grade effects. <xref ref-type="bibr" rid="bibr9-0162373712454327">Greene and Winters (2007)</xref> find large positive effects 2 years postretention using data from Florida and an FRD design, but their estimates may be confounded with summer school effects because they use performance on the spring assessment score as an instrumental variable even though the spring scores are also used for summer school assignment. <xref ref-type="bibr" rid="bibr15-0162373712454327">Hughes et al. (2010)</xref> use data on Texas first graders and propensity scoring techniques, finding that retention increases the likelihood of passing third-grade reading and math assessments by about 80%.</p>
<p>These disparate findings underscore the value of additional analyses. This is all the more true given that the only FRD estimates not confounded by possible impacts of summer school come from one city (Chicago), and results in other cities could be different.</p>
<p>A crucial consideration for any analysis of retention is that retained students are henceforth no longer in the same grade as their promoted counterparts. Thus, estimates of the impact of retention would be confounded by factors that have differential effects across academic cohorts. An example could be an intervention given only in the grade of retention in the year following the retention decision. Another example is that retained and promoted students do not take the same test under the exact same conditions, opening the door to bias from test equating or contextual circumstances. For example, <xref ref-type="bibr" rid="bibr19-0162373712454327">Jacob and Lefgren (2004)</xref> point out that negative sixth-grade effects might be explained by promoted students’ taking a high-stakes test and retained students’ taking a low-stakes test. We directly address this potential confounding and are unaware of any prior attempts to do so.</p>
</sec>
<sec id="section4-0162373712454327">
<title>Same-Grade Versus Same-Age Comparisons</title>
<p>Retaining a student involves providing him or her with an additional year of instruction in the retained grade, delaying exposure to material covered in subsequent grades until after the material in the retained grade has been seen a second time when he or she is a year older, and changing his or her cohort and classroom peers. The appropriateness of same-age or same-grade comparisons to estimate the short-run achievement effects of retention depends on whether a given comparison captures the effects of these treatments while not capturing the influence of other factors.</p>
<p>Same-grade analyses compare grade-level outcomes of retained students to those of their promoted peers who were assessed a year earlier. They inform whether spending an additional year in the retained grade improves performance in subsequent grades and rely on valid equating of outcomes within grade, across years. Same-grade comparisons capture the effect of an additional year of instruction in the retained grade on achievement. This is important because the additional year of instruction is a key component of the retention treatment.</p>
<p>In addition, same-grade comparisons capture any natural achievement growth that accompanies maturation. This is potentially an important limitation of same-grade comparisons because it is not reasonable to attribute achievement gains owing solely to maturation to the retention intervention. However, we suspect that any pure maturation effect on achievement is likely small. In support of this view is the finding that among fifth graders, vertically equated test scores increase steadily during the academic year but then cease or even partially reverse themselves during the summer (<xref ref-type="bibr" rid="bibr35-0162373712454327">Northwest Evaluation Association, 2011</xref>). This suggests that aging is unlikely to lead to large increases in achievement when not accompanied with formal instruction.</p>
<p>Estimates based on same-age comparisons, including the FRD studies in Chicago and Florida cited above, compare retained and promoted students at the same age across adjacent grades. They address whether retention improves a failing student’s achievement growth trajectory. The main limitation of same-age comparisons is that they do not capture the effect of the extra year of instruction that is part of the retention treatment and instead capture only that the retained student spends the subsequent year in the retained grade. Same-age comparisons also may reflect differences in the curriculum students face at the time of assessment. In particular, promoted students are exposed to more advanced material, helping them answer harder test items and improving vertically equated test performance. However, this effect may be fleeting because retained students also see the more advanced material eventually.</p>
<p>Evaluating the choice between same-age and same-grade comparisons also is informed by considering the goals of the promotion policy. The ultimate goal of retention is to improve students’ life outcomes. To examine this question, longer run analyses of effects on, for instance, wages, would be needed. Same-age comparisons are naturally appropriate for such questions. However, unlike same-age comparisons on short-run achievement outcomes, same-age comparisons on postschooling outcomes do capture the total effect of the retention treatment, in particular the effect of a year of additional instruction in the retained grade.</p>
<p>Because we do not have data on long-run outcomes, we instead focus on short-run effects on achievement. Policymakers such as New York City mayor Michael Bloomberg articulate that the primary interim goal of allowing an extra year to master the material in the retained grade is to position the student for greater academic success in the next grade and subsequent grades (<xref ref-type="bibr" rid="bibr33-0162373712454327">NYCDOE, 2009</xref>). Same-grade comparisons address whether this policy goal is being met; positive same-grade achievement effects of retention indicate that retention may have long-run positive effects on students, whereas the absence of such effects would suggest retention is not worth the cost.</p>
</sec>
</sec>
<sec id="section5-0162373712454327">
<title>New York City’s Student Promotion Policy</title>
<sec id="section6-0162373712454327">
<title>Policy Description</title>
<p>NYCDOE implemented an ambitious reform initiative that included a new assessment-based promotion policy for general education students in Grade 3 in the 2003–2004 school year. This policy was extended to fifth graders in the fall of 2004; students in charter schools, special education students, and early English language learners are excluded from the policy (see <xref ref-type="bibr" rid="bibr31-0162373712454327">McCombs, Kirby, Marsh, &amp; DiMartino, 2009</xref>, for a comprehensive discussion of the policy and institutional context in New York City at the time of the study). The policy was fully extended to seventh grade in 2006–2007 and by the fall of 2009 was in place for Grades 3 through 8. The key policy feature is the reliance on standardized test scores for grade promotion decisions. Students in the lowest performance category (Level 1: shows serious academic difficulties) on either the math or the ELA spring assessments are at risk of being retained in grade under the policy, whereas those who score in the next-highest category (Level 2: meets some of the standards or partially meets the standards) on both subjects are eligible for promotion. Students at Level 3 and higher are considered proficient.</p>
<p>The policy complements the threat of retention by placing considerable emphasis on an early intervention structure for struggling students. At the beginning of each school year, schools identify students who potentially are at risk for retention. This group automatically includes students retained the prior year and typically includes students scoring at Level 1 or in the lower half of Level 2 on a prior year’s spring assessment; upper Level 2 students may also receive these services as resources allow. Thus, both promoted and retained students near the Level 2 cutoff receive supplemental services, a fact that is important for our research design. These students receive a variety of additional instructional services both within and outside the classroom during the school day, as well as additional services outside of regular school hours (<xref ref-type="bibr" rid="bibr31-0162373712454327">McCombs et al., 2009</xref>).</p>
<p>The policy provides students multiple attempts to demonstrate proficiency required for promotion. Students with Level 1 scores on the math or ELA spring assessment can demonstrate Level 2 proficiency through a portfolio review in June. Those who do not demonstrate Level 2 proficiency attend the city’s summer instructional program, which was updated with the onset of the policy and rebranded Summer Success Academy (SSA). SSA meets 4 days a week, 4.5 hours per day, in July and early August, providing approximately 20 additional days of academic instruction in ELA and math with maximum class sizes of 15 students. Students attending SSA due to scoring Level 1 in only one subject still receive instruction in both subjects, although students may receive more focused instruction in their deficient subject as resources permit (<xref ref-type="bibr" rid="bibr16-0162373712454327">Ikemoto, McCombs, DiMartino, &amp; Naftel, 2009</xref>).</p>
<p>At SSA’s conclusion, students take a summer assessment in the subjects in which they scored Level 1 on the spring assessment. Those not demonstrating Level 2 on the summer assessment may pass through a summer portfolio review, and those not demonstrating Level 2 at the end of this process remain eligible to be retained in grade. A final promotion decision is made by the student’s principal and community superintendent.</p>
<p>In the spring of 2006, the NYCDOE assessments in third, fifth, sixth, and seventh grades were replaced with the statewide assessment (which also replaced prior state assessments in fourth and eighth grade). Level 2 cutoff scores were not available for the new assessment prior to the start of the 2006 SSA, so NYCDOE used interim cutoff scores to determine SSA eligibility. The interim cutoff scores turned out to be higher than the true Level 2 cutoffs released later that summer. SSA attendees from the 2005–2006 cohort whose scores were greater than the true spring Level 2 cutoffs were all promoted because they had actually met the promotion criteria under the policy on the spring assessment; that is, all SSA attendees with spring scores between the true Level 2 cutoff and the interim Level 2 cutoff were promoted. We return to this anomaly below in the discussion of the analysis samples we use.</p>
<p>Two other features of the educational landscape in New York City during the study period bear mention. First, NYCDOE concurrently instituted a variety of other reforms as part of its Children First initiative (<xref ref-type="bibr" rid="bibr31-0162373712454327">McCombs et al., 2009</xref>). Second, the state spring assessments had to be recalibrated for the 2010 administration (<xref ref-type="bibr" rid="bibr34-0162373712454327">New York State Education Department, 2010</xref>); although our available data run through 2008, the recalibration raises the potential that testing incongruencies across years may have occurred during our study period. Factors such as these could have generated differential achievement trends, either across grades or within a grade across years, and such external drift could bias estimates of retention effects. Below we describe a difference-in-difference approach to minimize this potential source of bias.</p>
</sec>
<sec id="section7-0162373712454327">
<title>Data</title>
<p>This article uses administrative data from the 2004–2005 and 2005–2006 cohorts of fifth-grade students attending NYCDOE schools. Each cohort has about 60,000 students. For students in these two cohorts, the data files provided by NYCDOE include information about fifth-grade math and ELA spring and summer assessment scores, which students participated in SSA, and an indicator for whether a student was retained in fifth grade. The outcome measures we use are scores on subsequent spring assessments. NYCDOE provided such information through the 2008 spring assessment, allowing students to be tracked up to 3 years beyond the initial fifth-grade year for the 2004–2005 cohort and 2 years for the 2005–2006 cohort. Scores on these assessments are not vertically scaled, so it is not possible to estimate grade retention effects using same-age comparisons. Below we express outcomes in standard deviation units of the state assessment scale (40 for both subjects) for ease of interpretability. Note that the variability of these outcomes in New York City is consistently smaller than 40, so our effect size estimates may be viewed as conservative in magnitude when compared to effect sizes from other studies.<sup><xref ref-type="fn" rid="fn2-0162373712454327">2</xref></sup></p>
<p>In addition to information about test scores and treatment status, the data also contain background information about students and schools. The student-level measures include gender, race/ethnicity, free and reduced-price lunch status, and attendance. School-level variables, which are available for the entire school and for fifth grade only, include enrollment, grades served, aggregates of each of the individual student measures, and a series of teacher characteristics. This information, 58 baseline variables in total, is used to generate baseline statistical controls in the models described below.</p>
<p>We use two samples in the analysis; <xref ref-type="table" rid="table1-0162373712454327">Table 1</xref> shows descriptive statistics for the two analytic samples. The sample we use to examine the effect of grade retention consists of students in the 2004–2005 fifth-grade cohort who attended SSA. Because we have data through 2008, we observe sixth- and seventh-grade outcomes for students retained at the end of the summer program as well as for promoted students. In particular, the sample includes 3,426 summer students for whom we observe sixth-grade scores and 3,367 students for whom we observe seventh-grade scores; our analyses include students who were assigned to special education after being retained but exclude 98 double retainees. These retention analyses do not include the 2005–2006 cohort because, as we describe below, the procedure we use to correct for external drift in test scores over time cannot be implemented for the 2005–2006 cohort.</p>
<table-wrap id="table1-0162373712454327" position="float">
<label>Table 1</label>
<caption><p>Sample Summary Statistics</p></caption>
<graphic alternate-form-of="table1-0162373712454327" xlink:href="10.3102_0162373712454327-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Grade retention analysis<hr/></th>
<th align="center">Summer Success Academy analysis<hr/></th>
</tr>
<tr>
<th/>
<th align="center">Sample (2004–2005 cohort)</th>
<th align="center">Sample (2005–2006 cohort)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full sample</td>
<td><italic>n</italic> = 4,046</td>
<td><italic>n</italic> = 57,889</td>
</tr>
<tr>
<td>Level 1 on math or ELA</td>
<td>0.568</td>
<td>0.148</td>
</tr>
<tr>
<td> Treated</td>
<td>0.379</td>
<td>0.125</td>
</tr>
<tr>
<td> Days absent</td>
<td>16.8</td>
<td>11.3</td>
</tr>
<tr>
<td/>
<td>(13.6)</td>
<td>(11.3)</td>
</tr>
<tr>
<td> English as a second language</td>
<td>0.133</td>
<td>0.066</td>
</tr>
<tr>
<td> Black</td>
<td>0.521</td>
<td>0.329</td>
</tr>
<tr>
<td> Hispanic</td>
<td>0.426</td>
<td>0.376</td>
</tr>
<tr>
<td> White</td>
<td>0.024</td>
<td>0.145</td>
</tr>
<tr>
<td> Other race</td>
<td>0.029</td>
<td>0.150</td>
</tr>
<tr>
<td> Free/reduced-price lunch</td>
<td>0.952</td>
<td>0.830</td>
</tr>
<tr>
<td> Male</td>
<td>0.461</td>
<td>0.474</td>
</tr>
<tr>
<td> High need school (75%)</td>
<td>0.508</td>
<td>0.266</td>
</tr>
<tr>
<td> Low need school (25%)</td>
<td>0.492</td>
<td>0.734</td>
</tr>
<tr>
<td> Sixth-grade scores observed</td>
<td>0.844</td>
<td>0.862</td>
</tr>
<tr>
<td> Seventh-grade scores observed</td>
<td>0.827</td>
<td/>
</tr>
<tr>
<td>Level 2 on ELA</td>
<td><italic>n</italic> = 3,591</td>
<td><italic>n</italic> = 53,229</td>
</tr>
<tr>
<td> Level 1 on math</td>
<td>0.513</td>
<td>0.074</td>
</tr>
<tr>
<td> Treated</td>
<td>0.332</td>
<td>0.070</td>
</tr>
<tr>
<td> Sixth-grade scores observed</td>
<td>0.850</td>
<td>0.863</td>
</tr>
<tr>
<td> Seventh-grade scores observed</td>
<td>0.835</td>
<td/>
</tr>
<tr>
<td>Level 2 on math</td>
<td><italic>n</italic> = 1,922</td>
<td><italic>n</italic> = 52,064</td>
</tr>
<tr>
<td> Level 1 on ELA</td>
<td>0.091</td>
<td>0.053</td>
</tr>
<tr>
<td> Treated</td>
<td>0.067</td>
<td>0.048</td>
</tr>
<tr>
<td> Sixth-grade scores observed</td>
<td>0.890</td>
<td>0.866</td>
</tr>
<tr>
<td> Seventh-grade scores observed</td>
<td>0.878</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0162373712454327">
<p><italic>Note.</italic> ELA = English language arts.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>For the analysis of the impact of SSA, the sample consists of all fifth-grade general education students in the 2005–2006 cohort who took the spring assessment, and the outcome we use is sixth-grade achievement. We use this cohort for the SSA analysis because, as explained above, the interim cutoff used for SSA assignments was greater than the eventual cutoff used to determine grade retention eligibility, which implies that the effects of SSA for students in this cohort who scored close to the interim Level 2 cutoff are not contaminated by the effect of grade retention. Conversely, students in the 2004–2005 cohort who barely missed the Level 2 cutoff on the spring assessment faced a much higher probability of being retained.</p>
</sec>
</sec>
<sec id="section8-0162373712454327" sec-type="methods">
<title>Method</title>
<sec id="section9-0162373712454327">
<title>Empirical Specification</title>
<p>To implement the FRD design, we obtain instrumental variables estimates of the treatment effect θ by estimating the following system of equations:</p>
<p><disp-formula id="disp-formula1-0162373712454327">
<mml:math display="block" id="math1-0162373712454327">
<mml:mrow>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>θ</mml:mi>
<mml:msub>
<mml:mi>W</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>f</mml:mi>
<mml:mi>Y</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>ε</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0162373712454327" xlink:href="10.3102_0162373712454327-eq1.tif"/>
</disp-formula></p>
<p><disp-formula id="disp-formula2-0162373712454327">
<mml:math display="block" id="math2-0162373712454327">
<mml:mrow>
<mml:msub>
<mml:mi>W</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>π</mml:mi>
<mml:msub>
<mml:mi>T</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>f</mml:mi>
<mml:mi>w</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>v</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0162373712454327" xlink:href="10.3102_0162373712454327-eq2.tif"/>
</disp-formula></p>
<p>where <italic>Y</italic><sub><italic>i</italic></sub> is the outcome for student <italic>i</italic>, <italic>W</italic><sub><italic>i</italic></sub> denotes treatment status, <italic>T</italic><sub><italic>i</italic></sub> is an indicator for treatment eligibility (i.e., scoring Level 1 on math or ELA) that serves as an instrumental variable for treatment status, and ε<sub><italic>i</italic></sub> and υ<sub><italic>i</italic></sub> are residuals. The flexible functions <italic>f</italic><sub><italic>Y</italic></sub> (<italic>X</italic><sub><italic>i</italic></sub>) and <italic>f</italic><sub><italic>W</italic></sub> (<italic>X</italic><sub><italic>i</italic></sub>) are used to describe the relationship between the assignment variable <italic>X</italic><sub><italic>i</italic></sub> and the outcome and treatment status, respectively (we describe below how we generate one-dimensional assignment variables from the math and ELA scores). As written, the parameter of interest, θ, is constant, but in practice it likely varies across students. Our design focuses on students near the cutoff, and our results are not necessarily informative about the average effect in the population. Instead, our approach identifies the effect for students scoring close to the Level 2 cutoff whose treatment status is affected by scoring less than Level 2 on either math or ELA (<xref ref-type="bibr" rid="bibr17-0162373712454327">Imbens &amp; Angrist, 1994</xref>).</p>
<p>This approach will deliver consistent estimates provided that unobservable determinants of <italic>Y</italic><sub><italic>i</italic></sub> do not change discontinuously at the Level 2 cutoff. One way this assumption would be violated is if students (or their teachers) had control over the exact score and could manipulate whether they scored just greater or less than the Level 2 cutoff. This type of gaming was impossible for SSA because the cutoffs were not yet established and highly unlikely for retention eligibility. A more subtle way this assumption could be violated is if missing the Level 2 cutoffs affects downstream outcomes. This is a particular concern for assignment to summer instruction because this could also affect the likelihood of grade retention, so we capitalize on the unique circumstances of the 2005–2006 interim-level scores to avoid this issue. It is also a concern for retention because external drift in test scores could differentially affect retained students. Below we describe how we account for this drift.</p>
</sec>
<sec id="section10-0162373712454327">
<title>Estimation</title>
<p>To estimate the model described above, we follow the approach suggested in <xref ref-type="bibr" rid="bibr18-0162373712454327">Imbens and Lemieux (2008)</xref>. In particular, we model <italic>f</italic><sub><italic>Y</italic></sub> (<italic>X</italic><sub><italic>i</italic></sub>) and <italic>f</italic><sub><italic>W</italic></sub> (<italic>X</italic><sub><italic>i</italic></sub>) as local polynomial regression models, wherein only observations within a bandwidth <italic>h</italic> of the treatment threshold are included in the analysis, low-order polynomial terms <inline-formula id="inline-formula1-0162373712454327">
<mml:math display="inline" id="math3-0162373712454327">
<mml:mrow>
<mml:msubsup>
<mml:mi>X</mml:mi>
<mml:mi>i</mml:mi>
<mml:mrow>
<mml:mi>p</mml:mi>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> are included as regression covariates, and coefficients are allowed to differ on either side of the treatment threshold (<italic>X</italic><sub><italic>i</italic></sub> = 0). The math and ELA scale scores are standardized so that the running variable for both subjects is in the same units and also in the same units as the outcomes (the estimates, available on request, are nearly identical when using the unstandardized scale scores as running variables).</p>
<p>For bandwidth and polynomial choices, we implement the cross-validation (CV) procedure recommended by <xref ref-type="bibr" rid="bibr26-0162373712454327">Ludwig and Miller (2007)</xref> and <xref ref-type="bibr" rid="bibr18-0162373712454327">Imbens and Lemieux (2008)</xref> that optimizes bandwidth choice on minimizing average mean squared error near the threshold. Within the CV procedure, we consider polynomial candidates up to the fourth degree of <italic>X</italic><sub><italic>i</italic></sub> at each candidate bandwidth to simultaneously choose bandwidth and polynomial specification.<sup><xref ref-type="fn" rid="fn3-0162373712454327">3</xref></sup> We separately calculate the optimal bandwidth and polynomial degree for each outcome, sample, and assignment variable.</p>
<p>As a sensitivity test, we also report estimates using a fixed specification that remains constant across outcomes and samples, where <italic>f</italic>() is modeled as a quadratic function of the assignment variable and the bandwidth is two standard deviations of the assignment variable (i.e., <italic>h</italic> = 2 and <italic>p</italic> = 2). Both the CV-based and the fixed specifications are estimated with and without inclusion of baseline covariates, although adjusting for these covariates should not affect the estimates provided that students just greater and less than the cutoff are similar aside from treatment status. Finally, we report estimated standard errors that adjust for “clustering” in a test score cell that arises from random misspecification of the <italic>f</italic>() functions (<xref ref-type="bibr" rid="bibr24-0162373712454327">Lee &amp; Card, 2008</xref>).</p>
</sec>
<sec id="section11-0162373712454327">
<title>Approaches for Addressing Assignment Variable Multidimensionality</title>
<p>In our context the assignment variable consists of both math and ELA test scores because students must score Level 2 on both subjects to avoid the treatments. There are several ways to implement a regression discontinuity design in this context (<xref ref-type="bibr" rid="bibr36-0162373712454327">Reardon &amp; Robinson, 2012</xref>; <xref ref-type="bibr" rid="bibr38-0162373712454327">Wong, Steiner, &amp; Cook, 2010</xref>). The first approach (used by <xref ref-type="bibr" rid="bibr8-0162373712454327">Gill et al., 2009</xref>; <xref ref-type="bibr" rid="bibr20-0162373712454327">Jacob &amp; Lefgren, 2009</xref>; <xref ref-type="bibr" rid="bibr28-0162373712454327">Martorell &amp; McFarlin, 2011</xref>) transforms the multidimensional assignment index into a one-dimensional index by using the minimum of the math and reading scores as the assignment variable. To see how this is done, denote the math and ELA scores, scaled by their respective standard deviations and recentered to be zero at the Level 2 cutoff, by <italic>X</italic><sub><italic>im</italic></sub> and <italic>X</italic><sub><italic>ie</italic></sub>. Defined in this way, treatment eligibility <italic>T</italic><sub><italic>i</italic></sub> is determined by whether <inline-formula id="inline-formula2-0162373712454327">
<mml:math display="inline" id="math4-0162373712454327">
<mml:mrow>
<mml:msubsup>
<mml:mi>X</mml:mi>
<mml:mi>i</mml:mi>
<mml:mrow>
<mml:mtext>MIN</mml:mtext>
</mml:mrow>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mtext>MIN</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>e</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> is negative, and <inline-formula id="inline-formula3-0162373712454327">
<mml:math display="inline" id="math5-0162373712454327">
<mml:mrow>
<mml:msubsup>
<mml:mi>X</mml:mi>
<mml:mi>i</mml:mi>
<mml:mrow>
<mml:mtext>MIN</mml:mtext>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> is a one-dimensional assignment variable.</p>
<p>Our second approach, used by <xref ref-type="bibr" rid="bibr19-0162373712454327">Jacob and Lefgren (2004)</xref> and <xref ref-type="bibr" rid="bibr29-0162373712454327">Matsudaira (2008)</xref>, limits the sample to students scoring greater than the treatment threshold on a given subject and uses the score on the other subject as the assignment variable. For instance, in the sample restricted to students with <italic>X</italic><sub><italic>im</italic></sub> &gt; 0, whether <italic>X</italic><sub><italic>ie</italic></sub> is less than zero determines treatment eligibility so it can be used as the assignment variable in the FRD setup described above. Because students in this sample are assigned to treatment solely on the basis of ELA performance, we characterize this as the ELA-only model. Similarly, in the model using the math score as the assignment variable and a sample limited to students scoring Level 2 on ELA is called the math-only model.</p>
<p>Rather than assume that the effects are the same for students assigned to treatment for scoring Level 1 on math or ELA, the minimum-score approach instead captures the average effect across both the math and ELA treatment thresholds (<xref ref-type="bibr" rid="bibr38-0162373712454327">Wong et al., 2010</xref>). An advantage of this approach is therefore that it delivers a useful summary measure of the treatment effect. Moreover, it includes students who are just less than the cutoff on both subjects, whereas these students would be excluded from both the math- and the ELA-only models. However, the single average effect at the threshold might miss heterogeneity that would be uncovered by comparing estimates from the math- and ELA-only models. Thus, we think it is useful to report the results of the minimum-score, math-only, and ELA-only models. Moreover, although it might seem intuitive that the minimum-score models would produce estimates that lie between the subject-specific univariate estimates, this is not always the case (as is seen in the result below) because the minimum-score model includes students excluded from both of the univariate models and also because observations in the univariate models can enter the local polynomial at their maximum assignment score.</p>
</sec>
<sec id="section12-0162373712454327">
<title>Balancing External Drift</title>
<p>The RD estimates discussed above produce an estimate of the effect of being retained into the subsequent cohort. This includes the effect of retention and any additional influence that may cause cohort-over-cohort differences in outcomes. Differences in the implementation of other reform efforts and incongruencies in testing could partially explain estimated effects from the RD models; we refer to these cohort-over-cohort differences as <italic>external drift.</italic> One area of potential concern is that the New York State math and ELA assessments were recalibrated in 2010 (<xref ref-type="bibr" rid="bibr6-0162373712454327">CTB/McGraw-Hill, 2010a</xref>, <xref ref-type="bibr" rid="bibr7-0162373712454327">2010b</xref>). That recalibration implied that it had become easier. Although the data we use from these assessments span 2006 to 2008, predating the recalibration, correcting for external drift will account for any differences in relative difficulty of the assessments that may have existed. To account for the possibility of cohort-level differences in our retention effect estimates, we institute a difference-in-difference approach wherein we estimate the external drift net of retention and subtract it from the RD retention estimates, yielding an effect estimate for retention alone. Note that it is not necessary to parse the influence of each source of cohort-level differences on the RD estimate, only to estimate the aggregate nonretention influence and remove it from the retention effect estimate.</p>
<p>We identify external drift as the average expected scale score increase on an outcome assessment by virtue of being in the retained-into cohort (B) instead of the original cohort (A) among students of equal proficiency prior to policy treatment. For example, suppose that absent any treatment under the promotion policy, Cohort B students were expected to score 10 scale score points higher than baseline equivalent Cohort A students on the sixth-grade ELA assessment. Those 10 points, the external drift in this example, would be absorbed into an FRD retention effect estimate for that outcome and would need to be subtracted to identify a proper retention effect. Because the retention treatment commences at the start of the fifth-grade school year and incoming Cohort B Level 1 and 2 students may receive these same services under the policy, we use the fourth-grade spring assessments as appropriate baseline measures prior to policy treatment. Similarly, we use the lowest Level 3 Cohort A and B students at baseline to estimate the drift because they are the untreated students closest to those retained.<sup><xref ref-type="fn" rid="fn4-0162373712454327">4</xref></sup></p>
<p>To estimate external drift, we restrict the sample to low Level 3 students who are in 2004–2005 and 2005–2006 fifth-grade cohorts and estimate <inline-formula id="inline-formula4-0162373712454327">
<mml:math display="inline" id="math6-0162373712454327">
<mml:mrow>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mrow>
<mml:mi>icjg</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>α</mml:mi>
<mml:msubsup>
<mml:mi>D</mml:mi>
<mml:mi>i</mml:mi>
<mml:mrow>
<mml:mn>2006</mml:mn>
</mml:mrow>
</mml:msubsup>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mrow>
<mml:mi>icj</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>β</mml:mi>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>u</mml:mi>
<mml:mrow>
<mml:mi>icj</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</inline-formula>, where Y<sub><italic>icjg</italic></sub> is the standardized scale score for student <italic>i</italic> in cohort <italic>c</italic>, subject <italic>j</italic> (math or ELA), and Grade <italic>g</italic> (6 or 7); <inline-formula id="inline-formula5-0162373712454327">
<mml:math display="inline" id="math7-0162373712454327">
<mml:mrow>
<mml:msubsup>
<mml:mi>D</mml:mi>
<mml:mi>i</mml:mi>
<mml:mrow>
<mml:mn>2006</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:mrow>
</mml:math>
</inline-formula> is dummy for being in the 2005–2006 cohort; and X′<sub><italic>icj</italic></sub> is a vector of 56 baseline covariates. The parameter α is the external drift term. To help ensure that baseline variables are fully balanced, we estimate the regression via weighted least squares, where the weights depend on the propensity score of being in the 2005–2006 cohort (students in the 2005–2006 cohort receive a weight of 1, and students in the 2004–2005 cohort receive a weight equal to <italic>p / (1 − p)</italic>, where <italic>p</italic> is the estimated probability of being in the 2005–2006 cohort, conditional on the baseline covariates).<sup><xref ref-type="fn" rid="fn5-0162373712454327">5</xref></sup> This procedure is called doubly robust regression (<xref ref-type="bibr" rid="bibr3-0162373712454327">Bang &amp; Robins, 2005</xref>) because if either the propensity score model or the regression model is correct, a consistent estimate of the external drift is produced. It is important to note that this method is not fully immune from omitted variable bias; as such, the resulting drift-adjusted retention estimates will not feature the same high internal validity of the original RD estimates.</p>
<p>To estimate the effect of retention absent external drift, we subtract the external drift estimate from the retention effects obtained from the RD models (in results available on request, we find very similar point estimates when subtracting the drift estimate from the outcomes of retained students prior to estimating the RD models). Let <italic>se</italic><sub><italic>r</italic></sub> and <italic>se</italic><sub><italic>e</italic></sub> represent the standard error of the RD and drift estimates, respectively. We estimate the standard error of the drift-adjusted retention estimate as <inline-formula id="inline-formula6-0162373712454327">
<mml:math display="inline" id="math8-0162373712454327">
<mml:mrow>
<mml:msqrt>
<mml:mrow>
<mml:msubsup>
<mml:mrow>
<mml:mtext mathvariant="bold">se</mml:mtext>
</mml:mrow>
<mml:mtext mathvariant="bold">r</mml:mtext>
<mml:mn mathvariant="bold">2</mml:mn>
</mml:msubsup>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:msubsup>
<mml:mrow>
<mml:mtext mathvariant="bold">se</mml:mtext>
</mml:mrow>
<mml:mtext mathvariant="bold">e</mml:mtext>
<mml:mn mathvariant="bold">2</mml:mn>
</mml:msubsup>
</mml:msqrt>
</mml:mrow>
</mml:math>
</inline-formula>.<sup><xref ref-type="fn" rid="fn6-0162373712454327">6</xref></sup></p>
<p>The fourth-grade baseline assessments measures are not equated between 2004–2005 and 2005–2006, the transition between the former and current state assessments. This precludes making the external drift adjustment for the 2005–2006 fifth-grade cohort. For that reason, we concentrate on the 2004–2005 fifth-grade cohort in estimating the effect of retention.</p>
<p>Underlying this drift adjustment is the assumption that the external drift for Cohort A students at the fifth-grade summer Level 2 threshold is the same as for fourth-grade students at just greater than the Level 3 threshold. To the extent that assumption is incorrect, the drift-adjusted estimates may be biased in either direction. Because the retention estimates depend on the validity of the drift correction, below we discuss the results from several robustness checks used to gauge the success of our method.</p>
</sec>
</sec>
<sec id="section13-0162373712454327" sec-type="results">
<title>Results</title>
<p>This section presents results of our analysis. We first discuss evidence for the validity of the RD models, then describe the discontinuity of treatment eligibility at the Level 2 cutoffs, and then present the estimated treatment effects.</p>
<sec id="section14-0162373712454327">
<title>RD Validity</title>
<p>The assumptions underlying the validity of the research design imply that baseline covariates trend smoothly through the treatment threshold (<xref ref-type="bibr" rid="bibr18-0162373712454327">Imbens &amp; Lemieux, 2008</xref>). To test this implication, we generated a single-index summary measure of the available baseline covariates for each student achievement outcome variable by finding the linear combination of the covariates most predictive of outcome (i.e., for each outcome, we estimated a regression of the outcome on the baseline covariates and calculated the predicted values from this regression). We then estimated the discontinuity in this summary measure at the treatment threshold. For all samples and local polynomial specifications used in the analyses presented below, the estimated discontinuities are small in magnitude and not statistically significant (not displayed). This evidence suggests that observable baseline variables do not differ on either side of the treatment threshold, which is consistent with a valid regression discontinuity design.<sup><xref ref-type="fn" rid="fn7-0162373712454327">7</xref></sup></p>
<p>Another implication of a valid RD design is that the distribution of the assignment variable trends smoothly through the treatment threshold (<xref ref-type="bibr" rid="bibr32-0162373712454327">McCrary, 2008</xref>). We estimated the discontinuity in the number of observations at the cut score (not reported) and found no evidence that the distribution of the assignment variables was discontinuous.</p>
</sec>
<sec id="section15-0162373712454327">
<title>First-Stage Estimates</title>
<p><xref ref-type="fig" rid="fig1-0162373712454327">Figure 1a</xref> shows the likelihood of being in SSA as a function of the minimum of the standardized math and ELA spring assessment scores for the 2005–2006 cohort. Each dot represents the fraction of students in a particular test score cell who attended SSA, and the curves represent the local polynomial regression fit from the CV (solid line) or the fixed (dashed line) specification without covariates. The white and black dots represent test score cells populated by students whose lowest scores are in ELA or math, respectively; grey dots refer to cells that include both types of students. Point estimates from Equation 2 for the discontinuity in the fraction attending SSA at the Level 2 cutoff ranges from about 0.69 to 0.74 across local polynomial specifications and are precisely estimated (standard errors are about 0.05).</p>
<fig id="fig1-0162373712454327" position="float">
<label>Figure 1.</label>
<caption><p>Chance of attending Summer Success Academy or being retained at the Level 2 cutoff.</p>
<p><italic>Note.</italic> White dots are observed English language arts assignment values, black dots are observed math assignment values, and gray dots are observed values in both subjects. Lines are fitted local polynomial specifications corresponding to the sixth-grade English language arts outcome: solid = cross-validation, dashed = fixed.</p></caption>
<graphic xlink:href="10.3102_0162373712454327-fig1.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig1-0162373712454327">Figure 1b</xref> shows analogous results for grade retention, depicting the fraction of students in the 2004–2005 cohort who were retained as a function of their minimum standardized summer assessment score. The point estimates of the discontinuity in the fraction retained range from 0.57 to 0.61, with standard errors less than 0.02. Thus, for both treatments, the fraction of treated students falls markedly at the Level 2 cutoff. Using the ELA-only or math-only models (omitted for brevity) instead of the minimum score produces substantially the same figure.</p>
</sec>
<sec id="section16-0162373712454327">
<title>The Effect of Summer Instruction</title>
<p><xref ref-type="table" rid="table2-0162373712454327">Table 2</xref> displays RD estimates of the effect of SSA attendance the summer after fifth grade on spring sixth-grade assessment outcomes for the 2005–2006 cohort. The upper panel presents the SSA effect estimates on ELA achievement in sixth grade. In the rows corresponding to each of the three RD models, we present the estimated effect sizes (with standard errors); just below them, the bandwidth and polynomial degree used in the local polynomial specification; and the number of observations underlying the estimates. When considering SSA assignment determined by spring ELA performance only, the first row of <xref ref-type="table" rid="table2-0162373712454327">Table 2</xref> indicates positive statistically significant effects of SSA ranging from 0.063 to 0.094, suggesting a modest positive effect on ELA achievement. Consistent with these small but positive estimates, <xref ref-type="fig" rid="fig2-0162373712454327">Figure 2a</xref> shows that ELA achievement has a small decrease at the Level 2 cutoff. Furthermore, the regression fit for both the local polynomial and the baseline specifications appears to approximate the underlying data quite closely, which provides reassurance that these estimates are not an artifact of model misspecification.</p>
<table-wrap id="table2-0162373712454327" position="float">
<label>Table 2</label>
<caption><p>Effects of Summer Success Academy Summer Instruction on Future Spring Assessment Outcomes: 2005–2006 Fifth-Grade Cohort</p></caption>
<graphic alternate-form-of="table2-0162373712454327" xlink:href="10.3102_0162373712454327-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="center" colspan="4">Outcome: ELA, sixth grade</th>
</tr>
</thead>
<tbody>
<tr>
<td>ELA only</td>
<td>0.094<xref ref-type="table-fn" rid="table-fn2-0162373712454327">**</xref> (0.019)</td>
<td>0.089<xref ref-type="table-fn" rid="table-fn2-0162373712454327">**</xref> (0.021)</td>
<td>0.082<xref ref-type="table-fn" rid="table-fn2-0162373712454327">**</xref> (0.021)</td>
<td>0.063<xref ref-type="table-fn" rid="table-fn2-0162373712454327">**</xref> (0.026)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.5; <italic>p</italic> = 2; <italic>n</italic> = 29,989</td>
<td align="center" colspan="2"><italic>h</italic> = 2; <italic>p</italic> = 2; <italic>n</italic> = 36,481</td>
</tr>
<tr>
<td>Math only</td>
<td>0.011 (0.010)</td>
<td>0.007 (0.008)</td>
<td>0.003 (0.014)</td>
<td>–0.002 (0.010)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.5; <italic>p</italic> = 1; <italic>n</italic> = 9,321</td>
<td align="center" colspan="2"><italic>h</italic> = 2, <italic>p</italic> = 2, <italic>n</italic> = 37,873</td>
</tr>
<tr>
<td>Minimum score</td>
<td>0.044 (0.058)</td>
<td>0.035 (0.055)</td>
<td>0.046 (0.089)</td>
<td>0.036 (0.085)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 2.0; <italic>p</italic> = 1; <italic>n</italic> = 43,030</td>
<td align="center" colspan="2"><italic>h</italic> = 2, <italic>p</italic> = 2, <italic>n</italic> = 43,030</td>
</tr>
<tr>
<th align="left">Model</th>
<th align="center" colspan="4">Outcome: math, sixth grade</th>
</tr>
<tr>
<td>ELA only</td>
<td>0.044 (0.037)</td>
<td>0.038 (0.039)</td>
<td>0.071 (0.049)</td>
<td>0.039 (0.055)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; <italic>p</italic> = 1; <italic>n</italic> = 13,069</td>
<td align="center" colspan="2"><italic>h</italic> = 2, <italic>p</italic> = 2, <italic>n</italic> = 36,322</td>
</tr>
<tr>
<td>Math only</td>
<td>0.120<xref ref-type="table-fn" rid="table-fn2-0162373712454327">**</xref> (0.029)</td>
<td>0.105<xref ref-type="table-fn" rid="table-fn2-0162373712454327">**</xref> (0.030)</td>
<td>0.049 (0.032)</td>
<td>0.034 (0.033)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.5; <italic>p</italic> = 1; <italic>n</italic> = 29,368</td>
<td align="center" colspan="2"><italic>h</italic> = 2, <italic>p</italic> = 2, <italic>n</italic> = 37,707</td>
</tr>
<tr>
<td>Minimum score</td>
<td>–0.002 (0.140)</td>
<td>–0.001 (0.013)</td>
<td>–0.015 (0.130)</td>
<td>–0.025 (0.113)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; <italic>p</italic> = 1; <italic>n</italic> = 19,219</td>
<td align="center" colspan="2"><italic>h</italic> = 2; <italic>p</italic> = 2; <italic>n</italic> = 42,832</td>
</tr>
<tr>
<td>Baseline covariates?</td>
<td align="center">No</td>
<td align="center">Yes</td>
<td align="center">No</td>
<td align="center">Yes</td>
</tr>
<tr>
<td>Local polynomial specification</td>
<td align="center" colspan="2">Cross-validation</td>
<td align="center" colspan="2">Fixed</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0162373712454327">
<p><italic>Note.</italic> ELA = English language arts; <italic>h</italic> = bandwidth; <italic>p</italic> = polynomial degree; <italic>n</italic> = sample size within bandwidth. Effect estimates are presented in standard deviations of the assessment reporting scale with standard errors in parentheses.</p></fn>
<fn id="table-fn3-0162373712454327">
<label>*</label>
<p><italic>p</italic>&lt;0.05; ** <italic>p</italic>&lt;0.01.</p></fn>
</table-wrap-foot>
</table-wrap>
<fig id="fig2-0162373712454327" position="float">
<label>Figure 2.</label>
<caption><p>Average sixth-grade English language arts spring assessment outcomes by 2005–2006 fifth-grade spring assignment score.</p>
<p><italic>Note.</italic> White dots are observed English language arts assignment values, black dots are observed math assignment values, and gray dots are observed values in both subjects. Lines are fitted local polynomial specifications: solid = cross-validation, dashed = fixed.</p></caption>
<graphic xlink:href="10.3102_0162373712454327-fig2.tif"/>
</fig>
<p>The estimates of SSA effect on sixth-grade assessment outcomes that restricts the sample to those attending for math only (second row) are small and not statistically significant. The estimates in which the assignment variable is the minimum of the math and ELA scores (third row) are positive and lie in between the estimates that use either the math or the ELA score as the assignment variable but are fairly small in magnitude and are not statistically significant. The small estimated treatment effects from these two classes of models are consistent with the visual evidence in <xref ref-type="fig" rid="fig2-0162373712454327">Figures 2b</xref> and <xref ref-type="fig" rid="fig2-0162373712454327">2c</xref>, which show ELA achievement as a function of the math score (for the sample that met Level 2 on ELA) or as a function of the minimum of the math and ELA scores, respectively. A noteworthy feature of <xref ref-type="fig" rid="fig2-0162373712454327">Figure 2c</xref> is that outcomes that map to math minimum scores are higher than outcomes that map to nearby ELA minimum scores (as can be seen by the black dots’ being above the white dots), essentially producing two disjoint signals. The fitted regression lines from the minimum-score models balance between these two signals.</p>
<p>The lower panel of <xref ref-type="table" rid="table2-0162373712454327">Table 2</xref> shows estimates for math. When the sample is restricted to SSA assignment for math only (second row), estimates from the CV local polynomial specification are positive and imply a treatment effect of 0.11 to 0.12 standard deviations, but estimates from the fixed specification are close to zero and not statistically significant. The visual evidence in <xref ref-type="fig" rid="fig3-0162373712454327">Figure 3b</xref> suggests that the fixed specification (dashed curve) fits the data better near the cut score than the CV specification (solid curve) and is therefore more consistent with the absence of an effect of SSA. These results highlight the need to evaluate the sensitivity of RD estimates to the choice of local polynomial and by including multiple specifications. Estimates from the ELA-only model (second row) and the minimum score model (third row) are small and not statistically significant across the regression specifications. This is consistent with the visual evidence in <xref ref-type="fig" rid="fig3-0162373712454327">Figures 3a</xref> and <xref ref-type="fig" rid="fig3-0162373712454327">3c</xref>, which shows no indication that math achievement behaves discontinuously at the Level 2 cutoff.</p>
<fig id="fig3-0162373712454327" position="float">
<label>Figure 3.</label>
<caption><p>Average sixth-grade math spring assessment outcomes by 2005–2006 fifth-grade spring assignment score.</p>
<p><italic>Note.</italic> White dots are observed English language arts assignment values, black dots are observed math assignment values, and gray dots are observed values in both subjects. Lines are fitted local polynomial specifications: solid = cross-validation, dashed = fixed.</p></caption>
<graphic xlink:href="10.3102_0162373712454327-fig3.tif"/>
</fig>
<p>Overall, these results provide evidence that attending SSA after scoring Level 1 in ELA has a modest positive effect on sixth-grade ELA achievement among students scoring Level 2 or better in math, but we otherwise do not find robust evidence that attending SSA generates improvements in sixth-grade achievement.</p>
</sec>
<sec id="section17-0162373712454327">
<title>The Influence of External Drift on Retention Outcomes</title>
<p><xref ref-type="table" rid="table3-0162373712454327">Table 3</xref> provides doubly robust regression estimates of external drift for ELA and math outcomes in both sixth and seventh grade. As described above, these estimates are based on the experiences of students in the relevant adjacent cohorts who are most similar to those treated under the promotion policy without being treated themselves. Because the three FRD models we use rely on different sets of treated students, we produce drift estimates that are specific to each model; that is, for the ELA-only approach, we identify those students most similar to the treated using baseline ELA scores, taking the analogous approach for the math-only model and using baseline scores in both subjects for the minimum-score approach.</p>
<table-wrap id="table3-0162373712454327" position="float">
<label>Table 3</label>
<caption><p>Estimates of Postretention External Drift: 2004–2005 Fifth-Grade Cohort</p></caption>
<graphic alternate-form-of="table3-0162373712454327" xlink:href="10.3102_0162373712454327-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">ELA outcomes<hr/></th>
</tr>
<tr>
<th align="left">Model</th>
<th align="center">Sixth grade</th>
<th align="center">Seventh grade</th>
</tr>
</thead>
<tbody>
<tr>
<td>ELA only</td>
<td>–0.003 (0.008)</td>
<td>0.073<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.007)</td>
</tr>
<tr>
<td>Math only</td>
<td>0.116<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.010)</td>
<td>0.177<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.009)</td>
</tr>
<tr>
<td>Minimum score</td>
<td>0.035<xref ref-type="table-fn" rid="table-fn5-0162373712454327">*</xref> (0.017)</td>
<td>0.122<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.017)</td>
</tr>
<tr>
<th/>
<th align="center" colspan="2">Math outcomes<hr/></th>
</tr>
<tr>
<th align="left">Model</th>
<th align="center">Sixth grade</th>
<th align="center">Seventh grade</th>
</tr>
<tr>
<td>ELA only</td>
<td>0.109<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.009)</td>
<td>0.089<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.009)</td>
</tr>
<tr>
<td>Math only</td>
<td>0.151<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.011)</td>
<td>0.158<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.010)</td>
</tr>
<tr>
<td>Minimum score</td>
<td>0.088<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.018)</td>
<td>0.125<xref ref-type="table-fn" rid="table-fn5-0162373712454327">**</xref> (0.018)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0162373712454327">
<p><italic>Note.</italic> ELA = English language arts. Effect estimates are presented in standard deviations of the assessment reporting scale with standard errors in parentheses.</p></fn>
<fn id="table-fn5-0162373712454327">
<label>*</label>
<p><italic>p</italic>&lt;0.05; ** <italic>p</italic>&lt;0.01</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The results confirm the presence of an external drift across all models and outcomes, with the exception of sixth-grade ELA outcomes using the ELA-only model. In each case the drift is positive, implying that when taking the relevant assessment 1 year later the external drift results in a higher expected score. The magnitude of the drift ranges from 0.04 to 0.18 standard deviations on the assessment reporting scale. Because the FRD estimates identify the effects of retention plus external drift, we subtract these external drift estimates from the FRD results to produce estimates of retention alone.</p>
</sec>
<sec id="section18-0162373712454327">
<title>The Effect of the Retention</title>
<p><xref ref-type="table" rid="table4-0162373712454327">Table 4</xref> shows drift-adjusted estimates of the effect of being retained in fifth grade on sixth- and seventh-grade achievement. With respect to ELA performance (upper two panels), retention effect estimates are positive and statistically significant across all three FRD models for both outcome years. Retained students score 0.43 to 0.56 standard deviations higher on the sixth-grade spring ELA assessment than expected had they been promoted, results that remain fairly consistent across regression specifications. Similarly, the retention effects on the seventh-grade ELA assessment ranged from 0.37 to 0.60. Seventh-grade effects were slightly lower in the ELA-only model and slightly higher in the math-only and minimum-score models, but no statistically significant differences in the magnitude of effects were detected across outcome years. <xref ref-type="fig" rid="fig4-0162373712454327">Figure 4</xref> illustrates the (pre-drift-corrected) effects of retention on seventh-grade ELA achievement, with scores falling sharply at the Level 2 cutoff under each FRD model.</p>
<table-wrap id="table4-0162373712454327" position="float">
<label>Table 4</label>
<caption><p>Drift-Corrected Effects of Retention on Future Spring Assessment Outcomes: 2004–2005 Fifth-Grade Cohort</p></caption>
<graphic alternate-form-of="table4-0162373712454327" xlink:href="10.3102_0162373712454327-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="center" colspan="4">Outcome: ELA, sixth grade</th>
</tr>
</thead>
<tbody>
<tr>
<td>ELA only</td>
<td>0.429<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref> (0.067)</td>
<td>0.481<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.142)</td>
<td>0.524<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.056)</td>
<td>0.556<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.097)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.0; p = 1; <italic>n</italic> = 452</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 556</td>
</tr>
<tr>
<td>Math only</td>
<td>0.371<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.052)</td>
<td>0.357<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.056)</td>
<td>0.409<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.062)</td>
<td>0.385<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.068)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.5; p = 1; <italic>n</italic> = 2,652</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 2,717</td>
</tr>
<tr>
<td>Minimum score</td>
<td>0.498<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.084)</td>
<td>0.481<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.077)</td>
<td>0.524<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.102)</td>
<td>0.495<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.09)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.5; p = 1; <italic>n</italic> = 3,158</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 3,315</td>
</tr>
<tr>
<th/>
<th align="center" colspan="4">Outcome: ELA, seventh grade</th>
</tr>
<tr>
<td>ELA only</td>
<td>0.369<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.060)</td>
<td>0.421<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.120)</td>
<td>0.415<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.069)</td>
<td>0.435<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.123)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.0; p = 1; <italic>n</italic> = 448</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 553</td>
</tr>
<tr>
<td>Math only</td>
<td>0.513<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.056)</td>
<td>0.5<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.061)</td>
<td>0.565<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.066)</td>
<td>0.54<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.074)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.5; p = 1; <italic>n</italic> = 2,609</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 2,673</td>
</tr>
<tr>
<td>Minimum score</td>
<td>0.583<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.091)</td>
<td>0.568<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.084)</td>
<td>0.602<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.108)</td>
<td>0.585<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.096)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 1.5; p = 1; <italic>n</italic> = 3,105</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 3,261</td>
</tr>
<tr>
<th/>
<th align="center" colspan="4">Outcome: math, sixth grade</th>
</tr>
<tr>
<td>ELA only</td>
<td>0.095 (0.098)</td>
<td>0.257<xref ref-type="table-fn" rid="table-fn7-0162373712454327">*</xref> (0.131)</td>
<td>0.183 (0.190)</td>
<td>0.3<xref ref-type="table-fn" rid="table-fn7-0162373712454327">*</xref> (0.132)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; p = 1; <italic>n</italic> = 400</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 561</td>
</tr>
<tr>
<td>Math only</td>
<td>0.395<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.047)</td>
<td>0.406<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.045)</td>
<td>0.405<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.053)</td>
<td>0.407<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.059)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; p = 1; <italic>n</italic> = 1,992</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 2,700</td>
</tr>
<tr>
<td>Minimum score</td>
<td>0.442<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.092)</td>
<td>0.431<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.085)</td>
<td>0.431<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.089)</td>
<td>0.428<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.091)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; p = 1; <italic>n</italic> = 2,079</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 3,303</td>
</tr>
<tr>
<th/>
<th align="center" colspan="4">Outcome: math, seventh grade</th>
</tr>
<tr>
<td>ELA only</td>
<td>0.041 (0.102)</td>
<td>0.257 (0.153)</td>
<td>0.118 (0.157)</td>
<td>0.175 (0.146)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; p = 1; <italic>n</italic> = 394</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 555</td>
</tr>
<tr>
<td>Math only</td>
<td>0.442<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.048)</td>
<td>0.453<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.046)</td>
<td>0.47<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.053)</td>
<td>0.473<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.058)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; p = 1; <italic>n</italic> = 1,957</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 2,654</td>
</tr>
<tr>
<td>Minimum score</td>
<td>0.458<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.075)</td>
<td>0.454<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.072)</td>
<td>0.395<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.086)</td>
<td>0.399<xref ref-type="table-fn" rid="table-fn7-0162373712454327">**</xref>** (0.085)</td>
</tr>
<tr>
<td/>
<td align="center" colspan="2"><italic>h</italic> = 0.75; p = 1; <italic>n</italic> = 2,049</td>
<td align="center" colspan="2"><italic>h</italic> = 2; p = 2; <italic>n</italic> = 3,243</td>
</tr>
<tr>
<td>Baseline covariates?</td>
<td align="center">No</td>
<td align="center">Yes</td>
<td align="center">No</td>
<td align="center">Yes</td>
</tr>
<tr>
<td>Local polynomial Specification</td>
<td align="center" colspan="2">Cross-validation</td>
<td align="center" colspan="2">Fixed</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0162373712454327">
<p><italic>Note.</italic> ELA = English language arts; <italic>h</italic> = bandwidth; <italic>p</italic> = polynomial degree; <italic>n</italic> = sample size within bandwidth. Effect estimates are presented in standard deviations of the assessment reporting scale with standard errors in parentheses.</p></fn>
<fn id="table-fn7-0162373712454327">
<label>*</label>
<p><italic>p</italic>&lt;0.05; ** <italic>p</italic>&lt;0.01</p></fn>
</table-wrap-foot>
</table-wrap>
<fig id="fig4-0162373712454327" position="float">
<label>Figure 4.</label>
<caption><p>Average seventh-grade English language arts spring assessment outcomes by 2004–2005 fifth-grade summer assignment score.</p>
<p><italic>Note.</italic> White dots are observed English language arts assignment values, black dots are observed math assignment values, and gray dots are observed values in both subjects. Lines are fitted local polynomial specifications: solid = cross-validation, dashed = fixed.</p></caption>
<graphic xlink:href="10.3102_0162373712454327-fig4.tif"/>
</fig>
<p>The estimates of the effect of retention on math achievement are shown in the lower two panels of <xref ref-type="table" rid="table4-0162373712454327">Table 4</xref>. The math-only and minimum-score models suggest that retention improves performance on the sixth-grade spring math assessment by 0.40 to 0.44 standard deviations of the reporting scale. These estimates persist into seventh grade, with effects ranging from 0.40 to 0.47 standard deviations higher on the seventh-grade assessment than expected absent fifth-grade retention and no observed sensitivity to the local polynomial specifications. <xref ref-type="fig" rid="fig5-0162373712454327">Figures 5b</xref> and <xref ref-type="fig" rid="fig5-0162373712454327">5c</xref> illustrate the sharp decrease in seventh-grade math outcomes at the Level 2 cutoff.</p>
<fig id="fig5-0162373712454327" position="float">
<label>Figure 5.</label>
<caption><p>Average seventh-grade math spring assessment outcomes by 2004–2005 fifth-grade summer assignment score.</p>
<p><italic>Note.</italic> White dots are observed English language arts assignment values, black dots are observed math assignment values, and gray dots are observed values in both subjects. Lines are fitted local polynomial specifications: solid = cross-validation, dashed = fixed.</p></caption>
<graphic xlink:href="10.3102_0162373712454327-fig5.tif"/>
</fig>
<p>The estimated effects of retention on math are smaller in the ELA-only model and are more sensitive to the regression specification, ranging from 0.10 to 0.30 for sixth grade and 0.04 to 0.26 for seventh grade; they are statistically significant in only some specifications. In particular, the ELA-only estimates for math are sensitive to the inclusion of baseline controls (adding covariates increases the magnitude of the effects). This sensitivity is not surprising given the visual evidence in <xref ref-type="fig" rid="fig5-0162373712454327">Figure 5a</xref>, where there is considerable variability in the average outcome in seventh-grade math scores near the Level 2 cutoff, resulting in a lack of concordance in the fit of the CV (solid line) and fixed (dashed line) specifications. This variability is likely driven by the relatively small number of students in the ELA-only models. The estimates also appear to be somewhat sensitive to the inclusion of baseline covariates; however, the estimates that include baseline covariates are larger, implying that any potential misspecification bias that may exist causes the retention effect to be underestimated.</p>
<p>Overall, these results suggest strong positive effects of retention on same-grade outcomes two grades removed from retention, with the exception of math outcomes for those retained for ELA only. Because these estimates require not only the usual FRD assumptions but also the (stronger) assumptions underlying the adjustment for external factors such as concurrent reform efforts and incongruencies in testing, we conducted multiple robustness checks to evaluate the validity of the external drift correction. First, we estimated the drift correction for other baseline proficiency levels. We did this for students at low Level 3 and greater at baseline because these students would not have received the supplemental remedial services that may confound the drift estimates based on Level 2 students.<sup><xref ref-type="fn" rid="fn8-0162373712454327">8</xref></sup> The drift adjustment estimates are smaller when using high Level 3 or Level 4 baseline proficiency levels, implying our estimated retention effects would be larger if we were to use these adjustments. Therefore, we think that drift adjustments used in <xref ref-type="table" rid="table3-0162373712454327">Table 3</xref> based on low Level 3 students only (i.e., those within two standard errors of the Level 2/3 cutoff) are most appropriate. Second, as a check on whether the doubly robust method can identify a credible comparison group in this context, we used it to estimate the impact of retention (without drift adjustment) for students with fifth-grade summer scores within 0.5 standard deviations less than Level 2. The results (available on request) did not differ significantly from the unadjusted FRD retention estimates, with point estimates ranging 0.03 greater to 0.13 less than the FRD estimates.<sup><xref ref-type="fn" rid="fn9-0162373712454327">9</xref></sup></p>
<p>As we note above, these same-grade estimates incorporate the effect of the retained student’s having had an additional year of instruction, which is a defining feature of the retention intervention. Nonetheless, the same-age effect that is net of the effect of an additional year of instruction is also of interest. An important limitation of our data is that they do not have the vertically equated scores necessary to conduct a same-age analysis. To assess how much of the estimated retention effects are due to an additional year of instruction, we compared published estimates of growth on a vertically scaled test experienced by a test norming sample (<xref ref-type="bibr" rid="bibr35-0162373712454327">Northwest Evaluation Association, 2011</xref>) to our estimated retention effects. For ELA, the estimated retention effects from the minimum score model are about 0.1 to 0.25 larger than average growth experienced by fifth graders (0.3). For math, the estimates from the minimum score model are about equal to or 0.04 larger than average growth for fifth graders (0.4).</p>
<p>Taken literally, these results imply that the additional instruction might explain a substantial portion of our estimated retention effects and that same-age effects would be smaller than what we report. For math, the same-age effects would be close to zero, whereas for ELA, a substantial treatment effect would remain even after netting out the effect of additional instruction. However, this simple back-of-the-envelope calculation has some important limitations. First, the growth patterns for the Northwest Evaluation Association norming sample may be different from those in New York City. A subtler but more important limitation is that estimates of the gains from a year of instruction are confounded by the fact that students are exposed to a more advanced curriculum in the next grade. Retained students, on the other hand, receive additional instruction in the same grade. Thus, the 1-year gains experienced by fifth graders in a national sample could overstate the gains retained students would experience exclusively as a result of receiving additional instruction in the same grade.</p>
</sec>
</sec>
<sec id="section19-0162373712454327" sec-type="discussion">
<title>Discussion</title>
<p>The above analyses enhance our understanding of the near-term academic effects of two treatments critical to a test-based, standards-based promotion policy. In particular, we use multidimensional FRD methods to obtain credible estimates of the causal effect of summer instruction and grade retention for fifth-grade students in New York City who have test scores close to the cutoffs used to make assignments to these treatments. Our analysis of summer instruction directly estimates the pure summer instruction effect by focusing on a student cohort for which there is no confounding due to the effect of subsequent retention. The estimated effects of retention are the first to adjust for the potential bias arising from external drift in test scores, which could bias same-age or same-grade comparisons.</p>
<p>The findings suggest that SSA has a modest positive effect on sixth-grade ELA achievement of 0.06 to 0.09 standard deviations for students assigned to SSA because of missing the Level 2 cutoff on ELA only, but we do not find robust evidence that attending SSA generates improvement in sixth-grade math achievement. In addition, we find no evidence that summer instruction resulted in improved achievement in ELA for students assigned to summer instruction for math only. It is possible that the lack of future grade effects in ELA when assigned for math, or in math when assigned for ELA, may be attributable to the high-stakes nature of SSA. However, the results with respect to math outcomes among those assigned for math are counter to expectations given the prior literature that, although effect sizes vary, consistently indicates positive effects. In particular, our estimates are somewhat smaller than those for fifth graders reported in <xref ref-type="bibr" rid="bibr29-0162373712454327">Matsudaira (2008)</xref>, who used a similar research design to analyze students attending summer school in the period from 1999 to 2002. The reason for this deviation is unclear and merits further study.</p>
<p>The estimates of the effect of grade retention indicate that the additional year of fifth-grade instruction leads to substantially better sixth-grade math and ELA achievement. It is important to note that we do not find any evidence that these effects decay in seventh grade. These improvements take into account the differential classroom and testing experiences of the retained students, which were found to partially explain observed outcome differences at the Level 2 cutoff. A further consideration has to do with promoted students’ tending to score near the bottom of the sixth- and seventh-grade distribution of scores. If their test performance appears to be better than what their achievement really is due to floor effects in the tests, our estimated retention effects could be understated.</p>
<p>One important consideration for evaluating these results is that they may not generalize beyond the study population. We focus on specific grades and cohorts for identification purposes, and our research design allows us to estimate only a local effect of the treatments for students scoring close to the Level 2 cutoff whose treatment status is affected by barely making or missing Level 2. However, this is a particular policy-relevant subgroup because it consists of students whom policymakers deem to have the academic skills just at the margin for needing the remedial intervention and whose treatment status is actually affected by the policy. These results would be pertinent for considering the likely effect of a change in the Level 2 cutoff score.</p>
<p>Although we find only limited evidence of positive effects of SSA on student achievement, it is also important to point out that these effects are not the only measure of the value of SSA. In the context of a test-based promotion policy, another value of SSA is that it provides additional opportunity to demonstrate meeting curriculum standards, which is a widely accepted tenant of fairness and validity in test-based promotion (<xref ref-type="bibr" rid="bibr11-0162373712454327">Heubert &amp; Hauser, 1999</xref>).</p>
<p>For grade retention, the short-run positive effects we find suggest that a key goal of the policy, namely, that an extra year of instruction in the same grade leads to improved achievement in later grades, is being met. This establishes the potential for meeting the longer term academic goals of the policy but makes no guarantees. Although we find no evidence that these effects diminish in the 2nd year after retention, earlier research (reviewed in <xref ref-type="bibr" rid="bibr2-0162373712454327">Allen et al., 2009</xref>; <xref ref-type="bibr" rid="bibr12-0162373712454327">Holmes, 1989</xref>) suggests that near-term same-grade effects tend to diminish over time. Thus, a crucial question for future research is whether grade retention has long-run benefits such as improved educational attainment and even success in the labor market.<sup><xref ref-type="fn" rid="fn10-0162373712454327">10</xref></sup></p>
<p>Another important consideration is which mechanisms might drive these retention effects. As we note above, a substantial portion of the retention effect is likely due to the additional instructional time in fifth grade. Another possibility is that retention might lead to better alignment of one’s academic skills to those of his or her peers. However, we did not find evidence that effects in high-retention schools differed from those in lower retention schools even though retention would have a larger effect on one’s relative rank in the classroom achievement distribution in higher achieving schools where retention is rare. Finally, it may be that the accompaniment of remedial services with the retention intervention leads to improved achievement. An important area for future research is to unpack the black box estimates we report.</p>
<p>Finally, even if the positive effects of retention we find are indicative of improved long-run outcomes, it is crucial to consider the program’s costs. In one sense, it is not surprising that we find that an extra year of instruction leads to improved achievement in same-grade outcomes. A crucial question that future research should address is whether these short-run and any long-run benefits justify the costs of the policy relative to reasonable alternatives. Because retention entails an extra year of instruction, these costs could be considerable, both for taxpayers and for students who may wind up spending more time in school then they otherwise would, which carries an opportunity cost in terms of less time in the workforce.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: The research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant 305A090039 to the RAND Corporation. Any opinions, findings, conclusions, or recommendations expressed are those of the authors and do not represent views of the Institute, the U.S. Department of Education, or the RAND Corporation.</p></fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0162373712454327">
<label>1.</label>
<p>Prior to policy inception, students were still expected to demonstrate Level 2 performance to be promoted, and spring Level 1 students did go to summer school. What changed with inception of this policy was the enforcement of the promotion standards.</p></fn>
<fn fn-type="other" id="fn2-0162373712454327">
<label>2.</label>
<p>The city standard deviation in the 2004–2005 cohort was about 30 for English language arts and 35 for math, and for the 2005–2006 cohort, it was 24 for English language arts and 33 for math.</p></fn>
<fn fn-type="other" id="fn3-0162373712454327">
<label>3.</label>
<p>We choose the <italic>h</italic> and <italic>p</italic> jointly because the optimal bandwidth likely varies across <italic>p.</italic> In contrast, <italic>p</italic> has typically been chosen via standard regression modeling options (e.g., <xref ref-type="bibr" rid="bibr29-0162373712454327">Matsudaira, 2008</xref>).</p></fn>
<fn fn-type="other" id="fn4-0162373712454327">
<label>4.</label>
<p>Although not all Cohort B students scoring in the upper range of Level 2 would receive treatment at the beginning of fifth grade, individual student detail of treatment received was not available to determine which upper Level 2 students were not treated and appropriate to include. Lowest Level 3 is defined as within one standard error of the lowest Level 3 score. For the minimum-score approach, students were included only if they scored lowest Level 3 in both subjects.</p></fn>
<fn fn-type="other" id="fn5-0162373712454327">
<label>5.</label>
<p>Propensity scores were generated via generalized boosting methods (<xref ref-type="bibr" rid="bibr30-0162373712454327">McCaffrey, Ridgeway, &amp; Morral, 2004</xref>). As a sensitivity check, generating propensity scores using logistic regression and nonweighted regression results produced nearly identical drift estimates.</p></fn>
<fn fn-type="other" id="fn6-0162373712454327">
<label>6.</label>
<p>Here we rely on the fact that both the RD retention estimate and the drift estimate are regression coefficients with asymptotically normal distributions and assume they are independent in calculating the variance of their difference because they are estimated on different samples. Any correlation due to a common random component driving the drift we are trying to purge is likely to be positive (i.e., increasing drift will increase the RD retention estimate). Because we are interested in the variance of the difference in the RD estimate and the drift estimate, such a positive correlation would make our estimated standard errors unduly conservative.</p></fn>
<fn fn-type="other" id="fn7-0162373712454327">
<label>7.</label>
<p>We also examined whether there were discontinuities in each of the individual baseline variables. For the SSA treatment, we found no evidence of individual discontinuities. For the retention outcome, some estimates suggested that retention-eligible students near the threshold came from schools that had a higher percentage of male students (four tenths of 1%) and between 60 and 90 fewer total students. Neither variable demonstrated discontinuities across all samples and polynomial specifications. To assess whether this imbalance affects the results, we estimated models with and without controlling for baseline covariates. If anything, the results we find here are a bit stronger when we control for these variables, suggesting that any nonsmoothness in baseline does not account for these results (as evidenced in <xref ref-type="table" rid="table2-0162373712454327">Tables 2</xref> and <xref ref-type="table" rid="table4-0162373712454327">4</xref>).</p></fn>
<fn fn-type="other" id="fn8-0162373712454327">
<label>8.</label>
<p>The drift adjustment is somewhat larger when using Level 2 students (0.1–0.3 compared to 0.0–.16 when using low Level 3 students). However, these estimates might partially reflect the impact of additional remedial services (<xref ref-type="bibr" rid="bibr31-0162373712454327">McCombs, Kirby, Marsh, &amp; DiMartino, 2009</xref>).</p></fn>
<fn fn-type="other" id="fn9-0162373712454327">
<label>9.</label>
<p>The comparison group for the doubly robust estimates of retention comprised promoted summer Level 1 students within the same bandwidth below the promotion threshold. It is known that an omitted variable exists in this case because the reason for promoting these Level 1 comparison students is not in the data set. Thus, to the extent the doubly robust estimates differ from the fuzzy regression discontinuity estimates, it is reasonable to expect that the doubly robust estimates would be lower. This omitted variable is not at issue for the drift estimates.</p></fn>
<fn fn-type="other" id="fn10-0162373712454327">
<label>10.</label>
<p><xref ref-type="bibr" rid="bibr20-0162373712454327">Jacob and Lefgren (2009)</xref> find that early-grade retention does not affect dropout but that eighth-grade retention increases it. However, the same patterns might not appear in New York City.</p></fn>
</fn-group>
</notes>
<bio>
<title>Authors</title>
<p>LOUIS T. MARIANO is a Senior Statistician, RAND Corporation, 1200 S. Hayes Street, Arlington, VA 22202; <email>loum@rand.org</email>. His reseearch interests include evaluation of the impact of school reform efforts using experimental and quasi-experimental designs and the advancement of associated statistical methodology.</p>
<p>PACO MARTORELL is an Economist, RAND Corporation, 1776 Main Street, Santa Monica, California 90401; <email>paco@rand.org</email>. He is also a Research Fellow at the Texas Schools Project at the University of Texas at Dallas. His areas of specialty are labor economics, the economics of education, and program evaluation. He has conducted studies on both K-12 and higher education policy questions.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alexander</surname><given-names>K. L.</given-names></name>
<name><surname>Entwisle</surname><given-names>D. R.</given-names></name>
<name><surname>Dauber</surname><given-names>S. L.</given-names></name>
</person-group> (<year>2003</year>). <source>On the success of failure: A reassessment of the effects of retention in the primary school grades</source> (<edition>2nd ed.</edition>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Allen</surname><given-names>C.</given-names></name>
<name><surname>Chen</surname><given-names>Q.</given-names></name>
<name><surname>Willson</surname><given-names>V.</given-names></name>
<name><surname>Hughes</surname><given-names>J. N.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Quality of design moderates effects of grade retention on achievement: A meta-analytic, multi-level analysis</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>31</volume>, <fpage>480</fpage>–<lpage>499</lpage>.</citation>
</ref>
<ref id="bibr3-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bang</surname><given-names>H.</given-names></name>
<name><surname>Robins</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Doubly-robust estimation in missing data and causal inference models</article-title>. <source>Biometrics</source>, <volume>61</volume>, <fpage>962</fpage>–<lpage>973</lpage>.</citation>
</ref>
<ref id="bibr4-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clinton</surname><given-names>W. J.</given-names></name>
</person-group> (<year>1998</year>, <month>February</month> <day>23</day>). <article-title>Memorandum on helping schools end social promotions</article-title>. <source>Weekly Compilation of Presidential Documents</source>, <volume>34</volume>(<issue>9</issue>), <fpage>310</fpage>–<lpage>312</lpage>.</citation>
</ref>
<ref id="bibr5-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cooper</surname><given-names>H.</given-names></name>
<name><surname>Charlton</surname><given-names>K.</given-names></name>
<name><surname>Valentine</surname><given-names>J. C.</given-names></name>
<name><surname>Muhlenbruck</surname><given-names>L.</given-names></name>
</person-group> (<year>2000</year>). <source>Making the most of summer school: A meta-analytic and narrative review</source>. <publisher-loc>Malden, MA</publisher-loc>: <publisher-name>Monographs of the Society for Research in Child Development</publisher-name>.</citation>
</ref>
<ref id="bibr6-0162373712454327">
<citation citation-type="book">
<collab>CTB/McGraw-Hill</collab>. (<year>2010a</year>). <source>New York State testing program 2010: English language arts, Grades 3–8</source> (<comment>Technical Report</comment>). <publisher-loc>Monterey, CA</publisher-loc>: <publisher-name>CTB/McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr7-0162373712454327">
<citation citation-type="book">
<collab>CTB/McGraw-Hill</collab>. (<year>2010b</year>). <source>New York State testing program 2010: Mathematics, Grades 3–8</source> (<comment>Technical Report</comment>). <publisher-loc>Monterey, CA</publisher-loc>: <publisher-name>CTB/McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr8-0162373712454327">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Gill</surname><given-names>B.</given-names></name>
<name><surname>Lockwood</surname><given-names>J. R.</given-names></name>
<name><surname>Martorell</surname><given-names>P.</given-names></name>
<name><surname>Setodji</surname><given-names>C. M.</given-names></name>
<name><surname>Booker</surname><given-names>K.</given-names></name>
<name><surname>Vernez</surname><given-names>G.</given-names></name>
<name><surname>Garet</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2009</year>). <source>An exploratory analysis of adequate yearly progress, identification for improvement, and student achievement in two states and three cities</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education</publisher-name>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.eric.ed.gov/PDFS/ED507477.pdf">http://www.eric.ed.gov/PDFS/ED507477.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr9-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Greene</surname><given-names>J. P.</given-names></name>
<name><surname>Winters</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Revisiting grade retention: An evaluation of Florida’s test-based promotion policy</article-title>. <source>Education Finance and Policy</source>, <volume>2</volume>(<issue>4</issue>), <fpage>319</fpage>–<lpage>340</lpage>.</citation>
</ref>
<ref id="bibr10-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hahn</surname><given-names>J.</given-names></name>
<name><surname>Todd</surname><given-names>P.</given-names></name>
<name><surname>van der Klaauw</surname><given-names>W.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Identification and estimation of treatment effects with a regression-discontinuity design</article-title>. <source>Econometrica</source>, <volume>69</volume>(<issue>1</issue>), <fpage>201</fpage>–<lpage>209</lpage>.</citation>
</ref>
<ref id="bibr11-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Heubert</surname><given-names>J. P.</given-names></name>
<name><surname>Hauser</surname><given-names>R. M.</given-names></name>
</person-group> (Eds.). (<year>1999</year>). <source>High stakes: Testing for tracking, promotion and graduation</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academies Press</publisher-name>.</citation>
</ref>
<ref id="bibr12-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Holmes</surname><given-names>C. T.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Grade level retention effects: Meta-analysis of research studies</article-title>. In <person-group person-group-type="editor">
<name><surname>Shepard</surname><given-names>L. A.</given-names></name>
<name><surname>Smith</surname><given-names>M. L.</given-names></name>
</person-group> (Eds.), <source>Flunking grades: Research and policies on retention</source> (pp. <fpage>16</fpage>–<lpage>33</lpage>). <publisher-loc>London, UK</publisher-loc>: <publisher-name>Falmer</publisher-name>.</citation>
</ref>
<ref id="bibr13-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hong</surname><given-names>G.</given-names></name>
<name><surname>Raudenbush</surname><given-names>S. W.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Effects of kindergarten retention policy on children’s cognitive growth in reading and mathematics</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>27</volume>(<issue>3</issue>), <fpage>205</fpage>–<lpage>224</lpage>.</citation>
</ref>
<ref id="bibr14-0162373712454327">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>House</surname><given-names>E. R.</given-names></name>
</person-group> (<year>1998</year>, <month>November</month>). <source>The predictable failure of Chicago’s student retention program</source>. <article-title>Paper prepared for the Conference on Rethinking Retention to Help All Students Succeed, Chicago, IL</article-title>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.designsforchange.org/pdfs/house.pdf">http://www.designsforchange.org/pdfs/house.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr15-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hughes</surname><given-names>J. N.</given-names></name>
<name><surname>Chen</surname><given-names>Q.</given-names></name>
<name><surname>Thoemmes</surname><given-names>F.</given-names></name>
<name><surname>Kwok</surname><given-names>O.</given-names></name>
</person-group> (<year>2010</year>). <article-title>An investigation of the relationship between retention in first grade and performance on high stakes tests in third grade</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>32</volume>(<issue>2</issue>), <fpage>166</fpage>–<lpage>182</lpage>.</citation>
</ref>
<ref id="bibr16-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ikemoto</surname><given-names>G. S.</given-names></name>
<name><surname>McCombs</surname><given-names>J. S.</given-names></name>
<name><surname>DiMartino</surname><given-names>C.</given-names></name>
<name><surname>Naftel</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Implementation of the policy: Saturday and summer schools</article-title>. In <person-group person-group-type="editor">
<name><surname>McCombs</surname><given-names>J. S.</given-names></name>
<name><surname>Kirby</surname><given-names>S. N.</given-names></name>
<name><surname>Mariano</surname><given-names>L. T.</given-names></name>
</person-group> (Eds.), <source>Ending social promotion without leaving children behind: The case of New York City</source> (pp. <fpage>81</fpage>–<lpage>99</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>RAND</publisher-name>.</citation>
</ref>
<ref id="bibr17-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Imbens</surname><given-names>G.</given-names></name>
<name><surname>Angrist</surname><given-names>J.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Identification and estimation of local average treatment effects</article-title>. <source>Econometrica</source>, <volume>62</volume>(<issue>2</issue>), <fpage>467</fpage>–<lpage>475</lpage>.</citation>
</ref>
<ref id="bibr18-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Imbens</surname><given-names>G.</given-names></name>
<name><surname>Lemieux</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Regression discontinuity designs: A guide to practice</article-title>. <source>Journal of Econometrics</source>, <volume>142</volume>(<issue>2</issue>), <fpage>615</fpage>–<lpage>635</lpage>.</citation>
</ref>
<ref id="bibr19-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>B. A.</given-names></name>
<name><surname>Lefgren</surname><given-names>L.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Remedial education and student achievement: A regression-discontinuity analysis</article-title>. <source>Review of Economics and Statistics</source>, <volume>86</volume>, <fpage>226</fpage>–<lpage>244</lpage>.</citation>
</ref>
<ref id="bibr20-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jacob</surname><given-names>B. A.</given-names></name>
<name><surname>Lefgren</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The effect of grade retention on high school completion</article-title>. <source>American Economic Journal: Applied Economics</source>, <volume>1</volume>(<issue>3</issue>), <fpage>33</fpage>–<lpage>58</lpage>.</citation>
</ref>
<ref id="bibr21-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jimerson</surname><given-names>S. R.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Meta-analysis of grade retention research: Implications for practice in the 21st century</article-title>. <source>School Psychology Review</source>, <volume>30</volume>(<issue>3</issue>), <fpage>420</fpage>–<lpage>438</lpage>.</citation>
</ref>
<ref id="bibr22-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kirby</surname><given-names>S. N.</given-names></name>
<name><surname>McCombs</surname><given-names>J. S.</given-names></name>
<name><surname>Mariano</surname><given-names>L. T.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Conclusions and policy implications</article-title>. In <person-group person-group-type="editor">
<name><surname>McCombs</surname><given-names>J. S.</given-names></name>
<name><surname>Kirby</surname><given-names>S. N.</given-names></name>
<name><surname>Mariano</surname><given-names>L. T.</given-names></name>
</person-group> (Eds.), <source>Ending social promotion without leaving children behind: The case of New York City</source> (pp. <fpage>183</fpage>–<lpage>194</lpage>) <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>RAND</publisher-name>.</citation>
</ref>
<ref id="bibr23-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Randomized experiments from non-random selection in U.S. House elections</article-title>. <source>Journal of Econometrics</source>, <volume>142</volume>(<issue>2</issue>), <fpage>675</fpage>–<lpage>697</lpage>.</citation>
</ref>
<ref id="bibr24-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>D. S.</given-names></name>
<name><surname>Card</surname><given-names>D. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Regression discontinuity inference with specification error</article-title>. <source>Journal of Econometrics</source>, <volume>142</volume>(<issue>2</issue>), <fpage>655</fpage>–<lpage>674</lpage>.</citation>
</ref>
<ref id="bibr25-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lorence</surname><given-names>J. A.</given-names></name>
<name><surname>Dworkin</surname><given-names>G.</given-names></name>
<name><surname>Toenjes</surname><given-names>L. A.</given-names></name>
<name><surname>Hill</surname><given-names>A. N.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Grade retention and social promotion in Texas 1994–99: Academic achievement among elementary school students</article-title>. In <person-group person-group-type="editor">
<name><surname>Ravitch</surname><given-names>D.</given-names></name>
</person-group> (Ed.), <source>Brookings papers on education policy 2002</source> (pp. <fpage>13</fpage>–<lpage>67</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Brookings Institution Press</publisher-name>.</citation>
</ref>
<ref id="bibr26-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ludwig</surname><given-names>J.</given-names></name>
<name><surname>Miller</surname><given-names>D. L.</given-names></name>
</person-group> (<year>2007</year>). <source>Does Head Start improve children’s life chances? Evidence from a regression discontinuity design</source> (<comment>Working Paper 11702</comment>). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research</publisher-name>.</citation>
</ref>
<ref id="bibr27-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Marsh</surname><given-names>J. A.</given-names></name>
<name><surname>Gershwin</surname><given-names>D.</given-names></name>
<name><surname>Kirby</surname><given-names>S. N.</given-names></name>
<name><surname>Xia</surname><given-names>N.</given-names></name>
</person-group> (<year>2009</year>). <source>Retaining students in grade: Lessons learned regarding policy design and implementation</source>. <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>RAND</publisher-name>.</citation>
</ref>
<ref id="bibr28-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Martorell</surname><given-names>P.</given-names></name>
<name><surname>McFarlin</surname><given-names>I.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Help or hindrance: The effects of college remediation on academic and labor market outcomes</article-title>. <source>Review of Economics and Statistics</source>, <volume>93</volume>(<issue>2</issue>), <fpage>436</fpage>–<lpage>454</lpage>.</citation>
</ref>
<ref id="bibr29-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Matsudaira</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Mandatory summer school and student achievement</article-title>. <source>Journal of Econometrics</source>, <volume>142</volume>(<issue>2</issue>), <fpage>829</fpage>–<lpage>850</lpage>.</citation>
</ref>
<ref id="bibr30-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McCaffrey</surname><given-names>D. F.</given-names></name>
<name><surname>Ridgeway</surname><given-names>G.</given-names></name>
<name><surname>Morral</surname><given-names>A. R.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Propensity score estimation with boosted regression for evaluating causal effects in observational studies</article-title>. <source>Psychological Methods</source>, <volume>9</volume>(<issue>4</issue>), <fpage>403</fpage>–<lpage>425</lpage>.</citation>
</ref>
<ref id="bibr31-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McCombs</surname><given-names>J. S.</given-names></name>
<name><surname>Kirby</surname><given-names>S. N.</given-names></name>
<name><surname>Marsh</surname><given-names>J. A.</given-names></name>
<name><surname>DiMartino</surname><given-names>C.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Context and conceptual framework for understanding New York City’s promotion policy</article-title>. In <person-group person-group-type="editor">
<name><surname>McCombs</surname><given-names>J. S.</given-names></name>
<name><surname>Kirby</surname><given-names>S. N.</given-names></name>
<name><surname>Mariano</surname><given-names>L. T.</given-names></name>
</person-group> (Eds.), <source>Ending social promotion without leaving children behind: The case of New York City</source> (pp. <fpage>17</fpage>–<lpage>32</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>RAND</publisher-name>.</citation>
</ref>
<ref id="bibr32-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McCrary</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Manipulation of the running variable in the regression discontinuity design: A density test</article-title>. <source>Journal of Econometrics</source>, <volume>142</volume>(<issue>2</issue>), <fpage>698</fpage>–<lpage>714</lpage>.</citation>
</ref>
<ref id="bibr33-0162373712454327">
<citation citation-type="gov">
<collab>New York City Department of Education</collab>. (<year>2009</year>, <month>August</month> <day>10</day>). <source>Mayor unveils plan to end social promotion in all remaining grades from three to eight (News Release)</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://schools.nyc.gov/Offices/mediarelations/NewsandSpeeches/2009-2010/20090810_social_promotion.htm">http://schools.nyc.gov/Offices/mediarelations/NewsandSpeeches/2009-2010/20090810_social_promotion.htm</ext-link></comment></citation>
</ref>
<ref id="bibr34-0162373712454327">
<citation citation-type="gov">
<collab>New York State Education Department</collab>. (<year>2010</year>, <month>July</month> <day>19</day>). <source>Regents approve scoring changes to Grade 3-8 math and English tests (News Release)</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.oms.nysed.gov/press/Regents_Approve_Scoring_Changes.html">http://www.oms.nysed.gov/press/Regents_Approve_Scoring_Changes.html</ext-link></comment></citation>
</ref>
<ref id="bibr35-0162373712454327">
<citation citation-type="web">
<collab>Northwest Evaluation Association</collab>. (<year>2011</year>). <source>RIT scale norms study</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.nwea.org/sites/www.nwea.org/files/resources/NWEA_2011_RIT_Scale_Norms_0.pdf">http://www.nwea.org/sites/www.nwea.org/files/resources/NWEA_2011_RIT_Scale_Norms_0.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr36-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reardon</surname><given-names>S. F.</given-names></name>
<name><surname>Robinson</surname><given-names>J. P.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Regression discontinuity designs with multiple rating-score variables</article-title>. <source>Journal of Research on Educational Effectiveness</source>, <volume>5</volume>(<issue>1</issue>), <fpage>83</fpage>–<lpage>104</lpage>.</citation>
</ref>
<ref id="bibr37-0162373712454327">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Roderick</surname><given-names>M.</given-names></name>
<name><surname>Nagoka</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Retention under Chicago’s high-stakes testing program: Helpful, harmful, or harmless?</article-title> <source>Educational Evaluation and Policy Analysis</source>, <volume>27</volume>(<issue>4</issue>), <fpage>309</fpage>–<lpage>340</lpage>.</citation>
</ref>
<ref id="bibr38-0162373712454327">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wong</surname><given-names>V. C.</given-names></name>
<name><surname>Steiner</surname><given-names>P. M.</given-names></name>
<name><surname>Cook</surname><given-names>T. D.</given-names></name>
</person-group> (<year>2010</year>). <source>Analyzing regression-discontinuity designs with multiple assignment variables: A comparative study of four estimation methods</source> (<comment>Working Paper 10-02</comment>). <publisher-loc>Evanston, IL</publisher-loc>: <publisher-name>Northwestern University Institute for Policy Research</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>