<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="book-review">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JRI</journal-id>
<journal-id journal-id-type="hwp">spjri</journal-id>
<journal-title>Journal of Research in International Education</journal-title>
<issn pub-type="ppub">1475-2409</issn>
<issn pub-type="epub">1741-2943</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1475240911431665</article-id>
<article-id pub-id-type="publisher-id">10.1177_1475240911431665</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Book reviews</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Book review: Evaluation in distance education and e-learning: The unfolding model</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Jones</surname><given-names>Geraldine</given-names></name>
</contrib>
<aff id="aff1-1475240911431665">University of Bath, UK</aff>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2012</year>
</pub-date>
<volume>11</volume>
<issue>1</issue>
<fpage>106</fpage>
<lpage>110</lpage>
<product>
<person-group person-group-type="author">
<name><surname>Ruhe</surname><given-names>Valerie</given-names></name>
<name><surname>Zumbo</surname><given-names>Bruno</given-names></name>
</person-group> <source>Evaluation in distance education and e-learning: The unfolding model</source> <publisher-loc>New York</publisher-loc>: <publisher-name>The Guilford Press</publisher-name>, <year>2009</year></product>
<permissions>
<copyright-statement>© The Author(s) 2011</copyright-statement>
<copyright-year>2011</copyright-year>
<copyright-holder content-type="sage">IBO, Sage Ltd.</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p>This book sets out to make a case for a professional approach to the evaluation of distance and e-learning courses. Its aims are twofold; to establish a model (the Unfolding Model) that can be adapted to local needs and act as a road map to guide course evaluation, and to offer practical tools and guidance for conducting an evaluation including examples of how findings can be reported. The authors are specialists in evaluation and educational measurement and have an interest in the validity and validation of assessment. The book is dedicated to Sam Messick, an expert in assessment validity whose framework inspired the Unfolding Model. The authors are based at the University of British Columbia in Canada and write predominantly from a North American perspective, focusing on the evaluation of academic oriented distance and e-learning programmes.</p>
<p>In the opening chapter Ruhe and Zumbo establish the drivers for a revised approach to evaluation. These include the growth in distance learning and e-learning provision, technological change, increasing stakeholder expectations and the need for continuous improvement. They distinguish between distance education and e-learning as two separate contexts, although this separation is somewhat questionable considering contemporary blended and hybrid courses where distance almost always implies some e-learning component. The authors propose their Unfolding Model as a way of framing a more theory-driven approach to evaluation by applying Messick’s ideas to program evaluation. They contend that the Unfolding Model goes beyond learner satisfaction by foregrounding the key elements of relevance, cost–benefit, underlying values and unintended consequences. Chapters 2 and 3 go on to chart the history of program and distance education evaluation respectively, thus providing the background for the development of the Unfolding Model.</p>
<p>Chapter 2 reviews the theory and practice of program evaluation encompassing health, social and educational contexts. The authors see program evaluation as applied research and a professional practice. Using Alkin and Christie’s root (accountability and social inquiry) and branch (methods, values, consequences) model, the authors critically map out the landscape and history of program evaluation practice, signposting the key players and distinguishing a diversity of philosophies, theories and values that underpin practice. Alkin and Christie’s classification is successfully used to categorize models that are predominantly methods-driven, values-driven or consequences-driven. Ruhe and Zumbo are somewhat critical of methods-driven approaches because they claim that a single question can narrow the focus of an evaluation, thus detracting from a ‘comprehensive assessment of the overall merit and worth of programs’ (p. 27). They observe that classical approaches to evaluation foreground methods and evidence, while more contemporary approaches bring multiple perspectives and stakeholder participation to the fore, making them more values-oriented. The chapter concludes by drawing parallels between Alkin and Christie’s model and Messick’s framework, arguing that the strength in Messick’s approach is that it can potentially integrate a variety of approaches to evaluation, for example by acknowledging that all evaluations are to some extent values-driven.</p>
<p>Chapter 3 reviews evaluation theory and practice in distance education and e-learning spanning the previous twenty years. Evaluation models and practices are classified based on the emphasis the authors claim they place on evidence, values or unintended consequences. Somewhat confusingly, initially we are told that 12 models were chosen for review but later it transpires that 17 are presented. However this chapter does provide a useful review, covering both strengths and weaknesses of the 17 models surveyed and a comparison matrix (p. 57) offers a succinct summary. The authors’ purpose ‘is to show that the dimensions of Messick’s framework are recurring themes in distance education [evaluation] models’ (p. 57). They assert that an analysis of underlying values in distance learning is an essential component of evaluation, as technologies and innovative pedagogical approaches can bring with them implicit sets of values. Both positive and negative unintended consequences, they argue, are also important to an evaluation as these inevitably arise in complex environments. A trend in more recent studies to include values and consequences is highlighted, and the overall adoption of a more ‘professional’ approach to evaluation is observed.</p>
<p>Messick’s four-quadrant framework is discussed in more detail in Chapter 4. The authors seek to justify the use of this framework, grounded in assessment validity, for a different purpose; that of program evaluation. The synergy between test validity and program evaluation is asserted by arguing that both contexts share a similar conception of validity. Messick, the authors report, sees validity as a ‘unified, four faceted conception of functional worth or value’ (p. 92) in which evidence, underlying values and unintended consequences are important components. The authors contend that the Unfolding Model is similarly motivated but reconceived for program evaluation. This chapter is useful in raising awareness about the nature and possible sources of unintended consequences, and therefore the importance of their inclusion in evaluation studies. The readers’ attention is also helpfully drawn to three ways in which underlying values can be revealed; in rhetoric (value-laden or over-blown language), in underlying assumptions or theories about how things are supposed to work, and in ideologies exhibited/espoused by stakeholder groups. Another significant contribution that the authors claim is made by Messick’s framework and their own Unfolding Model is the call to explore multiple perspectives to illuminate underlying values in the context in question. Surprisingly however, no mention is made of <xref ref-type="bibr" rid="bibr2-1475240911431665">Stake’s (1980)</xref> responsive approach to evaluation, which advocates accommodating multiple stakeholder perspectives, though in a rather more participative way.</p>
<p>The main thrust of Chapter 5 is to unpack the Unfolding Model, itemising evaluation data sources and providing advice on methods of data collection. The Unfolding Model is put forward as a generic ‘road map’ suitable for any evaluation study but, for me, rather than a guide to planning a study it seems more like a toolkit where the reader is invited to select from a menu of data sources. The authors view ‘each course as a case or unit of analysis and each course evaluation as a mixed methods case study’. Their definition makes evaluation look a lot like intrinsically motivated case study research (<xref ref-type="bibr" rid="bibr3-1475240911431665">Stake, 2003</xref>).</p>
<p>Chapter 5 goes on to advise the reader to keep their study manageable by setting boundaries that are mindful of:</p>
<list id="list1-1475240911431665" list-type="bullet">
<list-item><p>technologies and delivery mechanisms;</p></list-item>
<list-item><p>the needs of stakeholders;</p></list-item>
<list-item><p>constraints of time and resource;</p></list-item>
<list-item><p>limited opportunity to access data.</p></list-item>
</list>
<p>The authors stress the need to ensure the credibility of a study through adopting rigorous approaches and following good ethical practice. They do, however, seem somewhat biased in their belief that quantitative methods, in particular for example random sampling and good response rates, will bolster the credibility of evaluations.</p>
<p>A rather lengthy Chapter 6 deals with the collection of scientific evidence, advocating a mixed-methods approach. The upper two quadrants of the Unfolding Model provide guidance about what data to collect. The top left deals with data about outcomes, grades, completion rates, learner satisfaction and instructor competencies, while the top right deals with data about, for instance, relevance including meaningfulness to learner, contribution to society, transfer of theory into practice, and cost and benefits. A rather simplified ‘how to’ guide to various methods of data collection is offered. Although some strengths and weaknesses of the various methods are mentioned, overall a somewhat superficial overview is achieved. In some instances much detail is provided. For example there are sample survey questions and interview protocols, specific online survey tools are recommended and checklists of evaluation criteria for learning objects are offered. However, tips such as ‘one way to save time is to use the Voice to Text feature in Microsoft Office’ seem questionable. My advice would be for the novice evaluator to look to other sources for a more comprehensive introduction to research methods.</p>
<p>Chapter 7 turns to the lower quadrants of the Unfolding Model. It begins by highlighting sources of data for surfacing the underlying values associated with a course. These include, for instance, rhetoric in documentation, standards expressed as targets, course goals and objectives, theories and ideologies about how learning takes place, and the roles of stakeholders. The authors point out the possibility of conflicting values held by different stakeholder groups. The reader is advised to use survey or interview questions to surface underlying values and is strongly encouraged to use the examples provided. Next the reader’s attention is turned to ways of investigating any positive or negative unintended consequences associated with a distance or e-learning course. These include consideration of the degree of fit between different aspects of a course, such as between the outline/relevance and the implementation, among values held by stakeholders, between the technology and pedagogical approach, the pedagogical approach/media and the subject/discipline, and between learners’ needs and the course implementation.</p>
<p>This chapter also includes a literature review of standards frameworks and benchmarks. Although interesting, the review is exclusively North America-centric and so of limited value for those located in other geographical contexts. There is also a brief summary, with examples, of the principal theories of learning; I was somewhat surprised to see here human–computer interaction (HCI) presented as an example of a theory of learning based on constructivism.</p>
<p>Finally the reader is usefully offered advice for enhancing the credibility of evaluation findings, including random/stratified sampling of interviewees, an awareness of interviewer bias, triangulation by other independent researchers and an awareness of the Hawthorne effect (where individuals can act differently because of interest shown in them by a significant other). Examples of the sort of recommendations that might result from an evaluation and the guidelines provided for reporting an evaluation will also be useful for the novice evaluator. More emphasis and advice regarding establishing good communication channels and trust with the participants of an evaluation and the recipients of the report, however, including how this is integrated into an evaluation plan, would have been welcomed.</p>
<p>Chapter 8 presents two sample evaluation reports; one of a distance learning computing course and the other of an e-learning writing course. The distinction made between the distance learning and e-learning contexts seems somewhat contrived, as the distance education course is delivered via software installed on a PC. Both evaluations reviewed web pages and course materials, and collected data on learner satisfaction, delivery method, instructor interaction, student support, feedback and assessment. The findings were analysed to determine how well the course worked and the extent of the gap between the intended and the actual course implementation. Although the evaluations were largely based on student satisfaction surveys and interviews, the example reports do show how unintended consequences can support the formulation of contextualized recommendations for improvements in course design and delivery.</p>
<p>Chapter 9 provides a summary of all previous chapters. The authors reiterate their key message, namely that comprehensive evaluation of distance and e-learning programs requires consideration of three key areas; evidence, values and consequences. They assert that the Unfolding Model is both comprehensive and flexible, and thus claim that it has the potential to evolve to suit future e-learning and Web 2.0 environments. Readers are invited to build on and adapt the model to accommodate changes in distance and e-learning practice.</p>
<p>Overall the style of the book is that of an introductory text. Throughout the book, grey ‘call out’ boxes provide useful summaries of the main points introduced in the text. This helps the reader to assimilate the main ideas even while skim reading. Key terms introduced in the first chapter are emboldened to make an explicit link to the glossary. Each chapter begins with a recap of all the previous chapters, no doubt to keep the reader orientated. This can, however, make the text seem rather repetitive; I certainly found this, and the occasional typographic errors, rather tedious at times.</p>
<p>The authors claim that ‘this book is for those who want to know how well distance learning courses really work and for those who want to design the very best distance education courses’ (p. ix). The book does present a professional and systematic approach to evaluation, and prepares readers for using the Unfolding Model to guide their own evaluation studies of distance or e-learning courses. Oriented toward the novice evaluator, the book provides a basic overview of research methods and report writing for evaluation. However, I have concerns that this could lead to a ‘cook book’ approach to planning an evaluation, which may detract from evaluator responsiveness to the context in question. There is an overarching emphasis on what data to collect as prescribed by the Unfolding Model and methods of collection. I would prefer more attention to have been placed on how to ensure the evaluation is fit for purpose, for example by providing more advice about the position of the evaluator and the development of context-specific questions to guide the study. I concur with <xref ref-type="bibr" rid="bibr1-1475240911431665">Cousins (2009)</xref>, who argues that ‘evaluation method is not like a piece of cutlery that you can simply select for its adaptability to the food you want to eat … It is as much about art as it is about science’.</p>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="bibr1-1475240911431665">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cousins</surname><given-names>G</given-names></name>
</person-group> (<year>2009</year>) <source>Researching Learning in Higher Education: An Introduction to Contemporary Methods and Approaches</source>. <publisher-loc>Abingdon</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr2-1475240911431665">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stake</surname><given-names>RE</given-names></name>
</person-group> (<year>1980</year>) <article-title>Program evaluation, particularly responsive evaluation</article-title>. In: <person-group person-group-type="editor">
<name><surname>Dockrell</surname><given-names>WB</given-names></name>
<name><surname>Hamilton</surname><given-names>D</given-names></name>
</person-group> (eds) <source>Rethinking Educational Research</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Hodder and Stoughton</publisher-name></citation>
</ref>
<ref id="bibr3-1475240911431665">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stake</surname><given-names>RE</given-names></name>
</person-group> (<year>2003</year>) <article-title>Case studies</article-title>. In: <person-group person-group-type="editor">
<name><surname>Denzin</surname><given-names>NK</given-names></name>
<name><surname>Lincoln</surname><given-names>YS</given-names></name>
</person-group> (eds) <source>Strategies Of Qualitative Inquiry</source>, <edition>2nd edn.</edition> <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>, <fpage>134</fpage>–<lpage>164</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>