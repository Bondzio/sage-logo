<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PID</journal-id>
<journal-id journal-id-type="hwp">sppid</journal-id>
<journal-title>Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering</journal-title>
<issn pub-type="ppub">0954-4070</issn>
<issn pub-type="epub">2041-2991</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0954407012441882</article-id>
<article-id pub-id-type="publisher-id">10.1177_0954407012441882</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Original Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Driver’s head pose estimation using a hierarchical classification on an effective feature space</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Ghaffari</surname><given-names>Ali</given-names></name>
<xref ref-type="aff" rid="aff1-0954407012441882">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Rezvan</surname><given-names>Mahdieh</given-names></name>
<xref ref-type="aff" rid="aff1-0954407012441882">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Khodayari</surname><given-names>Alireza</given-names></name>
<xref ref-type="aff" rid="aff2-0954407012441882">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Sadati</surname><given-names>Seyed Hossein</given-names></name>
<xref ref-type="aff" rid="aff2-0954407012441882">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Vahidi-Shams</surname><given-names>Afra</given-names></name>
<xref ref-type="aff" rid="aff1-0954407012441882">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0954407012441882">
<label>1</label>Mechtronics Engineering Department, Islamic Azad University, South Tehran Branch, Tehran, Iran</aff>
<aff id="aff2-0954407012441882">
<label>2</label>Mechanical Engineering Department, K. N. Toosi University of Technology, Tehran, Iran</aff>
<author-notes>
<corresp id="corresp1-0954407012441882">Alireza Khodayari, Mechanical Engineering Department, K. N. Toosi University of Technology, Pardis St., Mollasadra Ave, Vanak Sq., Tehran, Iran. Email: <email>arkhodayari@dena.kntu.ac.ir</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>9</month>
<year>2012</year>
</pub-date>
<volume>226</volume>
<issue>9</issue>
<fpage>1233</fpage>
<lpage>1242</lpage>
<history>
<date date-type="received">
<day>13</day>
<month>8</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>2</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© IMechE 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Institution of Mechanical Engineers</copyright-holder>
</permissions>
<abstract>
<p>This paper introduces a direct accountability technique for the real-time estimation of the head’s position and orientation. Unlike existing works which rely on feature extraction either in the image domain or in three-dimensional space, our proposed approach based on a hierarchical expert classification will estimate the head pose for real-time applications, specifically for driver assistance systems. We proposed a method for feature extraction which aims at keeping only the angle-related features, independent of identity. We then used a hierarchical algorithm for classification which divides the whole space into smaller spaces. Moreover, we made a performance study aiming at evaluating the accuracy of the proposed approach. Experimental results show that the proposed features are able to describe the pose of the driver face image successfully, with a performance in agreement with the actual poses. This approach is not only useful for automobile safety and security but also can help severely handicapped people and can be used in collision prevention systems and other driving applications.</p>
</abstract>
<kwd-group>
<kwd>Head pose estimation</kwd>
<kwd>human–machine interface</kwd>
<kwd>intelligent automotive systems</kwd>
<kwd>driver behaviors surveillance</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0954407012441882" sec-type="intro">
<title>Introduction</title>
<p>Techniques related to the detection and recognition of the human face play important roles in many applications such as the human–computer interface, video surveillance, and more recently, human–robot interactions. Head poses are certainly quite an important indicator of a person’s focus of attention. Determining a face pose in a driving application is a complex problem for which numerous methods have been devised.<sup><xref ref-type="bibr" rid="bibr1-0954407012441882">1</xref>,<xref ref-type="bibr" rid="bibr2-0954407012441882">2</xref></sup> A historical study done on the subject for driver assistance systems is in fact an indication of how rapidly these systems are being developed. Several different techniques have been proposed for vision pose estimation in recent years.<sup><xref ref-type="bibr" rid="bibr3-0954407012441882">3</xref><xref ref-type="bibr" rid="bibr4-0954407012441882"/><xref ref-type="bibr" rid="bibr5-0954407012441882"/>–<xref ref-type="bibr" rid="bibr6-0954407012441882">6</xref></sup> It is noted that one of the main application domains is in fact the so-called advanced driver assistance.</p>
<p>In machine vision applications, head pose estimation is normally related to the state of a human head orientation. In addition to biological appearance, an ideal head pose estimator, as with any other facial vision techniques, must be invariant against many factors. Such factors could include physical phenomena such as the facial expression, camera distortion, lighting, and the presence of accessories such as hats and glasses.</p>
<p>In machine vision, the detection of facial features and tracking in video sequences are indeed two of the main challenging problems.<sup><xref ref-type="bibr" rid="bibr7-0954407012441882">7</xref></sup> In a general sequence, detection of the face in a frame is normally followed by an automatic extraction of facial features in video sequences. Among the many approaches proposed to detect faces in static scenes are principal component analysis,<sup><xref ref-type="bibr" rid="bibr8-0954407012441882">8</xref>,<xref ref-type="bibr" rid="bibr9-0954407012441882">9</xref></sup> and clustering and neural nets.<sup><xref ref-type="bibr" rid="bibr10-0954407012441882">10</xref></sup> Although these techniques may result in good detection accuracy rates, they are usually computationally expensive, in addition to their inability to offer a real-time performance.</p>
<p>In recent years, there have been various approaches in the literature for head pose estimation. The approaches proposed in the literature can be generally classiﬁed into two main categories, namely the three-dimensional (3D)<sup><xref ref-type="bibr" rid="bibr11-0954407012441882">11</xref></sup> and two-dimensional (2D) head pose estimation approaches.<sup><xref ref-type="bibr" rid="bibr12-0954407012441882">12</xref></sup> Another method of categorizing estimation techniques relates to the approach used to estimate the head position.<sup><xref ref-type="bibr" rid="bibr13-0954407012441882">13</xref></sup> These methods can be described in the following main categories.</p>
<list id="list1-0954407012441882" list-type="bullet">
<list-item>
<p>Appearance Template Methods make a comparison of a group of image samples, each taken at a discrete pose, so as to find the most similar view.</p>
</list-item>
<list-item>
<p>Detector Array Methods focus on the training of an ensemble of head detectors, each corresponding to a certain pose, assigning a discrete pose to the detector having the greatest support.</p>
</list-item>
<list-item>
<p>Nonlinear Regression Methods make use of nonlinear interpolation to devise a functional mapping from the image or feature space to a measurement space for the head pose.</p>
</list-item>
<list-item>
<p>Manifold Embedding Methods models with low dimension manifolds representing the continuous changes in head pose. When a new image arrives, it can be embedded into these manifolds, then used for embedding the template matching or interpolation.</p>
</list-item>
<list-item>
<p>Flexible Models which set a non-rigid (flexible) model onto the facial geometry of every individual inside the image plane. The head pose is then estimated by proper comparisons made at the feature levels or by using the model parameters.</p>
</list-item>
<list-item>
<p>Geometric Methods make use of the feature locations as the nose tip, mouth, and eyes in order to find the pose from their relative configuration.<sup><xref ref-type="bibr" rid="bibr14-0954407012441882">14</xref></sup></p>
</list-item>
<list-item>
<p>Tracking Methods detect the possible head changes using the movement observed from one frame to the next.</p>
</list-item>
<list-item>
<p>Hybrid Methods make use of a combination of one or more of the previous methods to remedy the limitations inherent in any single approach.</p>
</list-item>
</list>
<p>It is noted that for a proper design in our method, the system should be able to make head pose estimations using a single camera. The system should also be able to give the proper estimates for a continuous range of head orientations in the more important directions in driving, namely, the yaw and pitch, within reasonable time (approximately 9 m/s or faster). In driving situations, the orientation of the head in roll is not as important as that in pitch and yaw; therefore, we introduce our method for the estimation of the head position in just yaw and pitch and disregard the roll. Finally, the performance of the proposed method must be invariant of the driver as well as the operating conditions. What we are interested in is the development of full intelligent interfaces, serving as the first step in enabling computers to interact with people.</p>
<p>The remainder of this paper is organized as follows. This Introduction has reviewed some techniques related to the present work. The next section presents a description for the methodology of the work. In this section, the complete details of the face detection, normalization, classification, and feature extraction methodology are described. The proposed methodology and the results obtained are presented in the third section. The last section presents the conclusions and scope for future studies.</p>
</sec>
<sec id="section2-0954407012441882">
<title>Detailed design and implementation</title>
<p>The main purpose of our study is to provide estimations for head pose parameters using images obtained from video sequences. In other words, we track the head pose configuration in two directions over time. In this section, we propose an approach that creates a feature space from the video images containing the angle-only related features. We then introduce our hierarchical method for classification. <xref ref-type="fig" rid="fig1-0954407012441882">Figure 1</xref> shows the algorithm flowchart.</p>
<fig id="fig1-0954407012441882" position="float">
<label>Figure 1.</label>
<caption>
<p>The flowchart of the proposed algorithm.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig1.tif"/>
</fig>
<p>Our framework consists of these stages: 1. Image enhancement; 2. Face detection; 3. Features extraction; 4. Pose estimation using classification. These steps are illustrated in the following sections.</p>
<sec id="section3-0954407012441882">
<title>Image enhancement algorithm</title>
<p>The light level condition is the major restriction in our work. What is required is that the lighting for the images used must in fact be normal. If, therefore, the faces to be detected happen to be too bright or too dark, we increase the robustness against the light variations by applying a proper light compensation/correction preprocessing technique. We use the ‘Retinex’ which is a well-known algorithm for image enhancement. The Retinex is aimed at obtaining the balance between the human vision and machine vision system in addition to providing color constancy.<sup><xref ref-type="bibr" rid="bibr15-0954407012441882">15</xref></sup> The initial idea behind Retinex was introduced by Land<sup><xref ref-type="bibr" rid="bibr16-0954407012441882">16</xref></sup> as a model of lightness and color perception of human vision. It is evident that Retinex is not only a model, but can also serve as an algorithm used for image enhancement. After Land, Jobson and his colleagues<sup><xref ref-type="bibr" rid="bibr17-0954407012441882">17</xref></sup> defined a single-scale Retinex (SSR),which is an implementation of what became known as the center/surround Retinex. The single-scale Retinex is given by the relation</p>
<p>
<disp-formula id="disp-formula1-0954407012441882">
<label>(1)</label>
<mml:math display="block" id="math1-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mi>log</mml:mi>
<mml:msub>
<mml:mrow>
<mml:mi>I</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mtext>log</mml:mtext>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>×</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>I</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0954407012441882" xlink:href="10.1177_0954407012441882-eq1.tif"/>
</disp-formula>
</p>
<p>where <italic>I</italic><sub><italic>i</italic></sub>(<italic>x,y</italic>) is the image distribution in the <italic>i</italic>th color band, <italic>F</italic>(<italic>x,y</italic>) is the normalized surround function given as</p>
<p>
<disp-formula id="disp-formula2-0954407012441882">
<label>(2)</label>
<mml:math display="block" id="math2-0954407012441882">
<mml:mrow>
<mml:mo>∫</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>∫</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi>F</mml:mi>
</mml:mrow>
</mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi>dxdy</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0954407012441882" xlink:href="10.1177_0954407012441882-eq2.tif"/>
</disp-formula>
</p>
<p>It is noted that the image distribution is in fact the product of a scene’s reflectance and illumination, i.e.</p>
<p>
<disp-formula id="disp-formula3-0954407012441882">
<label>(3)</label>
<mml:math display="block" id="math3-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>I</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>S</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>i</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-0954407012441882" xlink:href="10.1177_0954407012441882-eq3.tif"/>
</disp-formula>
</p>
<p>where <italic>S</italic><sub><italic>i</italic></sub>(<italic>x,y</italic>) is the spatial distribution of illumination and <italic>r</italic><sub><italic>i</italic></sub>(<italic>x,y</italic>) is the distribution of scene reflectance. There exist a variety of surround functions. We use the Gaussian formula with absolute parameter <italic>c</italic> as</p>
<p>
<disp-formula id="disp-formula4-0954407012441882">
<label>(4)</label>
<mml:math display="block" id="math4-0954407012441882">
<mml:mrow>
<mml:mi>F</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mi>exp</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mo>−</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>÷</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-0954407012441882" xlink:href="10.1177_0954407012441882-eq4.tif"/>
</disp-formula>
</p>
<p>Depending on the scale used, SSR can either provide dynamic range compression (small scale) or tonal rendition (large scale). Superposition of weighted difference scale SSR provides a balance between these two effects resulting in a multi-scale Retinex (MSR) given by</p>
<p>
<disp-formula id="disp-formula5-0954407012441882">
<label>(5)</label>
<mml:math display="block" id="math5-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>MSRi</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:msub>
<mml:mrow>
<mml:mi>ω</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>R</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ni</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-0954407012441882" xlink:href="10.1177_0954407012441882-eq5.tif"/>
</disp-formula>
</p>
<p>where <italic>N</italic> is the number of scales, <italic>R</italic><sub><italic>ni</italic></sub> is the <italic>i</italic>th component of the <italic>n</italic>th scale. An immediate question about MSR is the number of scales required, scale values, and weight values. The work of Bian and Zhang<sup><xref ref-type="bibr" rid="bibr18-0954407012441882">18</xref></sup> showed that for most of the images, three scales are adequate, and that the weights can be equal. They showed that generally, fixed scales of 15, 80, and 250 can be used, or scales of a fixed size, using part of the image of fixed portion of image size, can be used. These are more of an experimental nature than theoretical since we do not know the size of the image in the real scenes or situations. The weights can be adjusted to weigh more on dynamic range compression or on color rendition.<sup><xref ref-type="bibr" rid="bibr18-0954407012441882">18</xref></sup></p>
</sec>
<sec id="section4-0954407012441882">
<title>Face detection</title>
<p>Edge detection is used here followed by filling holes so as to separate objects in each frame and find the face-like regions. At this stage, it so happens that there may exist several face-like regions in the image block. In such cases, we select the area with the maximum pixels as the face region. With 1200 test images of 15 subjects, background complexities, and lighting conditions, the rate for the correct face detection is found to be 86% in this work. It is worth mentioning that the proposed system does not guarantee whether the identified face region is accurate enough for the next stages.</p>
</sec>
<sec id="section5-0954407012441882">
<title>Features extraction</title>
<p>A face detector has been utilized for the detection of the facial region in the earlier stage. At this point now, the edge detection is performed on the images in four directions, followed by a dilation process on the four images and an averaging process was then done for the final images obtained. The resulting final image is the image belonging to the feature space, a space termed as the pose feature space. This pose is what is used for the ultimate classification. Sample feature spaces are shown in <xref ref-type="fig" rid="fig2-0954407012441882">Figure 2</xref> for a number of subjects with different poses.</p>
<fig id="fig2-0954407012441882" position="float">
<label>Figure 2.</label>
<caption>
<p>The feature spaces for some different subjects from various angles.</p>
<p>Source: Gourier et al.,<sup><xref ref-type="bibr" rid="bibr19-0954407012441882">19</xref></sup> with permission.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig2.tif"/>
</fig>
</sec>
<sec id="section6-0954407012441882">
<title>Dimension reduction by principal component analysis (PCA)</title>
<p>The next stages in our method are prototype fabrication, the definition of a calibration method, and then study of a tracking framework for the driver’s head pose. We normalize feature vectors from previous steps and calculate the average of training images to create the prototype for each face pose. Then we use PCA to reduce the dimensions of the obtained vectors. Afterwards, features are stored in the new dataset so that the features of new frame can be compared by one nearest neighbor classifier.</p>
<p>PCA<sup><xref ref-type="bibr" rid="bibr20-0954407012441882">20</xref>,<xref ref-type="bibr" rid="bibr21-0954407012441882">21</xref></sup> is a general method used in an image processing system. With the <italic>N</italic> sample of images {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>,…,<italic>x</italic><sub><italic>N</italic></sub>} in an <italic>n</italic>-dimensional space <italic>R</italic><sup><italic>n</italic></sup> used as the training set, PCA performs a linear mapping or transformation <italic>W</italic><sup><italic>T</italic></sup>, where <italic>W</italic><italic>R</italic><sup><italic>n×m</italic></sup> is a matrix with orthonormal columns. The mapping is done from an <italic>R</italic><sup><italic>n</italic></sup> space using the original image to an <italic>R</italic><sup><italic>m</italic></sup> feature space, in which <italic>m </italic>&lt; <italic>n</italic>. The coordinates of the new image vectors, known as the feature vectors, are given by</p>
<p>
<disp-formula id="disp-formula6-0954407012441882">
<label>(6)</label>
<mml:math display="block" id="math6-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>Y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>W</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mi>k</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo>,</mml:mo>
<mml:mo>…</mml:mo>
<mml:mo>,</mml:mo>
<mml:mi>N</mml:mi>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula6-0954407012441882" xlink:href="10.1177_0954407012441882-eq6.tif"/>
</disp-formula>
</p>
<p>The total scatter matrix <italic>S</italic><sub><italic>T</italic></sub> is defined as</p>
<p>
<disp-formula id="disp-formula7-0954407012441882">
<label>(7)</label>
<mml:math display="block" id="math7-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>S</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:munderover>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mi>N</mml:mi>
</mml:mrow>
</mml:munderover>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mi>μ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>X</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>k</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mi>μ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula7-0954407012441882" xlink:href="10.1177_0954407012441882-eq7.tif"/>
</disp-formula>
</p>
<p>where µ R<sup>n</sup> is the mean of all images.</p>
<p>Once the linear transformation <italic>W</italic><sup><italic>T</italic></sup> is performed, the transformed feature vectors {<italic>y</italic><sub>1</sub>,<italic>y</italic><sub>2</sub>,…, <italic>y</italic><sub><italic>N</italic></sub>} have the scatter <italic>W</italic><sup><italic>T</italic></sup><italic>S</italic><sub><italic>T</italic></sub><italic>W</italic>. The PCA aims at maximizing the projection <italic>W</italic><sub><italic>opt</italic></sub> of the determinant of the total scatter matrix of the projected images. The <italic>W</italic><sub><italic>opt</italic></sub> matrix is given by the relation</p>
<p>
<disp-formula id="disp-formula8-0954407012441882">
<label>(8)</label>
<mml:math display="block" id="math8-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>W</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>opt</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>argmaxW</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mi>W</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msup>
<mml:msub>
<mml:mrow>
<mml:mi>S</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>T</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>W</mml:mi>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>…</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>w</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula8-0954407012441882" xlink:href="10.1177_0954407012441882-eq8.tif"/>
</disp-formula>
</p>
<p>where {<italic>w</italic><sub><italic>i</italic></sub>|<italic>i</italic> = 1, 2, … , <italic>m</italic>} is the <italic>m</italic> eigenvectors in <italic>R</italic><sup><italic>n</italic></sup> of <italic>S</italic><sub><italic>T</italic></sub> associated with the largest <italic>m</italic> eigenvalues. These eigenvectors represent the original image vectors, which are known as the principal components or the eigenfaces in the relevant literature.</p>
</sec>
<sec id="section7-0954407012441882">
<title>Classification</title>
<p>The purpose of this section is to perform pattern recognition, i.e. the recognition and classification of the image based on its describing features. The process of pose estimation will be completed at the end of this stage. In this article, neural networks of the multilayer perceptron (MLP) type are used for classification.</p>
</sec>
</sec>
<sec id="section8-0954407012441882">
<title>Experimental set-up and results</title>
<p>We implement our feature space for head pose estimation and present the results on the public head pose database (Pointing’04 database<sup><xref ref-type="bibr" rid="bibr19-0954407012441882">19</xref></sup>).</p>
<sec id="section9-0954407012441882">
<title>Dataset</title>
<p>The Pointing’04 database consists of 15 sets of images, with and without wearing glasses and with various skin colors. Each set contains two series of 93 images of the same person, and the 93 head poses are determined by the yaw and pitch angles, which vary from −90° to +90°. One subject with various head poses is shown in <xref ref-type="fig" rid="fig3-0954407012441882">Figure 3</xref>.</p>
<fig id="fig3-0954407012441882" position="float">
<label>Figure 3.</label>
<caption>
<p>Sample images from the Pointing’04 head pose data base.</p>
<p>Source: Gourier et al.,<sup><xref ref-type="bibr" rid="bibr19-0954407012441882">19</xref></sup> with permission.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig3.tif"/>
</fig>
</sec>
<sec id="section10-0954407012441882">
<title>Cropping based on nose-tip method</title>
<p>We first crop each image based on the nose-tip method. Background influence is generally avoided. Therefore, to reduce such influence, the cropping process for the image patches is not performed by centering on the nose-tips. Rather, the nose-tip position is shifted from the center of the cropped image based on the known head poses in order to remove the background to a high degree.<sup><xref ref-type="bibr" rid="bibr22-0954407012441882">22</xref></sup> As shown in <xref ref-type="fig" rid="fig4-0954407012441882">Figure 4</xref>, by specifying the nose-tip location (<italic>x</italic><sub><italic>n</italic></sub>,<italic>y</italic><sub><italic>n</italic></sub>) and the ground truth yaw and pitch angle (<italic>θ</italic>,ϕ), the center of the image patch (of size <italic>W</italic> × <italic>H</italic>) is determined by</p>
<p>
<disp-formula id="disp-formula9-0954407012441882">
<label>(9)</label>
<mml:math display="block" id="math9-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi>p</mml:mi>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>W</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mn>180</mml:mn>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula9-0954407012441882" xlink:href="10.1177_0954407012441882-eq9.tif"/>
</disp-formula>
</p>
<p>
<disp-formula id="disp-formula10-0954407012441882">
<label>(10)</label>
<mml:math display="block" id="math10-0954407012441882">
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>c</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mi>y</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi>p</mml:mi>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mrow>
<mml:mi>H</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mi>ϕ</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mrow>
<mml:mn>180</mml:mn>
</mml:mrow>
</mml:mfrac>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula10-0954407012441882" xlink:href="10.1177_0954407012441882-eq10.tif"/>
</disp-formula>
</p>
<p>where <italic>p</italic> denotes the percentage of shift from the center.</p>
<fig id="fig4-0954407012441882" position="float">
<label>Figure 4.</label>
<caption>
<p>Cropping image patch according to the location of the nose-tip and head pose.</p>
<p>Source: Gourier et al.,<sup><xref ref-type="bibr" rid="bibr19-0954407012441882">19</xref></sup> with permission.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig4.tif"/>
</fig>
</sec>
<sec id="section11-0954407012441882">
<title>Edge detection</title>
<p>The face-cropped images must be changed so that the image is independent of the features related to the identity, keeping only the features related to the angle. Such an image encompassing only the face features related to the angles constitutes our feature space. The edge detection process is performed by convolving the image with an edge operator (a 2D filter known as mask). In the present study, edge detection is performed in four directions on the images by using the horizontal Sobel operator, vertical Sobel operator, Robert’s operator in the 45° direction, and Robert’s operator in the 135° direction.</p>
</sec>
<sec id="section12-0954407012441882">
<title>Edge dilation on edge detected images</title>
<p>It is noted that the edges can change depending on various environmental conditions such as lighting and partial head displacement in different images of the same angle, hence making different results in the edge detection. With such considerations, a method is needed to dilate the edges in the edge detected image. For this purpose, a random window of size 6 × 6 is selected from the parts of images which include the edge information and it is convolved with the images resulting from each edge detector. This process is repeated several times by choosing different windows from different parts of the image. The obtained images are then averaged together. This results in the dilation of the image edges. Finally, the four images resulting from all the operators after dilation are put together and averaged pixel by pixel.</p>
</sec>
<sec id="section13-0954407012441882">
<title>Classification</title>
<p>The process of pattern recognition is performed for the classification. Considering the large number of classes in the case of head pose estimation on the Pointing’04 database, in the following experiments the total problem space is divided into a number of smaller spaces in order to increase the accuracy of the estimation.</p>
<sec id="section14-0954407012441882">
<title>Experiment 1:</title>
<p>The first experiment is performed on the images that contain only the yaw changes with 13 angles, with the pitch angles unchanged. The purpose of this experiment is to review the ability of the method presented in this article to separate the extracted features when only the yaw classification is considered. The test includes seven stages, each corresponding to one of the pitch angles. Each stage contained 13 angles, making the problem a thirteen-class type.</p>
<p>To solve this thirteen-class problem by the proposed method, it is necessary to train the classifier by making use of the images of eight subjects as the training set and the images of seven subjects as the test set, all selected randomly. The experiment was repeated with 8, 10, and 12 training samples. <xref ref-type="fig" rid="fig5-0954407012441882">Figure 5</xref> shows the result of this experiment obtained from 20 runs.</p>
<fig id="fig5-0954407012441882" position="float">
<label>Figure 5.</label>
<caption>
<p>Yaw rotation estimation problem in seven stages corresponding to seven pitch rotations each, i.e. as a 13-class problem containing all (13) horizontal head poses. Results are shown for 8, 10, and 12 training samples.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig5.tif"/>
</fig>
</sec>
<sec id="section15-0954407012441882">
<title>Experiment 2:</title>
<p>In order to evaluate the efficiency of our feature extraction technique in pitch only, in the next experiment the images differing only in their pitch angles were used, keeping yaw angles unchanged. The simulation is done in 13 stages, each corresponding to one of the yaw angles with seven pitch angles. The problem is then of a seven-class type in this case.</p>
<p>The number of training and test images selected for this experiment is eight and seven, respectively, all selected randomly. <xref ref-type="fig" rid="fig6-0954407012441882">Figure 6</xref> shows the results of this experiment performed in 20 runs for 8, 10, and 12 training image samples.</p>
</sec>
<sec id="section16-0954407012441882">
<title>Experiment 3:</title>
<p>This experiment concerns the head pose estimation with the proposed method as applied on the whole database space with a different classification. In this experiment, the images are classified by two separate classifiers in both the yaw and pitch directions. These two classifier experts perform the classification process in parallel on the feature space created from the images.</p>
<p>The two expert systems are 7-class and 13-class classifiers responsible for the pitch and yaw directions, respectively. The end result is the presented angle for the test image. The result of this experiment for 20 runs shows a 36.73% correct estimation rate for the images of 8 subjects used as the training set and 7 subjects used as the test set, as depicted in <xref ref-type="table" rid="table1-0954407012441882">Table 1</xref>. This result shows that the classification method used in experiment 3 offers better estimations for the yaw than the pitch angles, although overall, this method outperforms the existing methods, as shown in <xref ref-type="table" rid="table1-0954407012441882">Table 1</xref>.</p>
<?breakt b07?>
<table-wrap id="table1-0954407012441882" position="float">
<label>Table 1.</label>
<caption>
<p>Performance evaluation and comparison table of the hierarchical model</p>
</caption>
<graphic alternate-form-of="table1-0954407012441882" xlink:href="10.1177_0954407012441882-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left" colspan="3">Experiment 3<hr/></th>
<th align="left" colspan="3">Hierarchical classification<hr/></th>
<th align="left" colspan="6">Comparing methods<hr/></th>
</tr>
<tr>
<th align="left">Metric</th>
<th align="left">Mean</th>
<th align="left">Min</th>
<th align="left">Max</th>
<th align="left">Mean</th>
<th align="left">Min</th>
<th align="left">Max</th>
<th align="left">PNMF<sup><xref ref-type="bibr" rid="bibr9-0954407012441882">9</xref></sup></th>
<th align="left">HOSVD<sup><xref ref-type="bibr" rid="bibr24-0954407012441882">24</xref></sup></th>
<th align="left">NMF<sup><xref ref-type="bibr" rid="bibr9-0954407012441882">9</xref></sup></th>
<th align="left">LAAM<sup><xref ref-type="bibr" rid="bibr25-0954407012441882">25</xref></sup></th>
<th align="left">NN-based<sup><xref ref-type="bibr" rid="bibr26-0954407012441882">26</xref></sup></th>
<th align="left">PCA<sup><xref ref-type="bibr" rid="bibr9-0954407012441882">9</xref></sup></th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean yaw error (degree)</td>
<td>9.97°</td>
<td>7.7°</td>
<td>12.73°</td>
<td>4.77°</td>
<td>3.83°</td>
<td>5.9°</td>
<td>11.2°</td>
<td align="left">12.9°</td>
<td>10.3°</td>
<td>8.5°</td>
<td align="left">9.5°</td>
<td>13.73°</td>
</tr>
<tr>
<td>Mean pitch error (degree)</td>
<td>13.78°</td>
<td>11.2°</td>
<td>16.64°</td>
<td>7.23°</td>
<td>5.21°</td>
<td>9.38°</td>
<td>12.8°</td>
<td align="left">17.9°</td>
<td>15.9°</td>
<td>10.1°</td>
<td align="left">9.7°</td>
<td>14.78°</td>
</tr>
<tr>
<td>Yaw classification</td>
<td>64.67%</td>
<td>61.26%</td>
<td>69.51%</td>
<td>81.48%</td>
<td>80.56%</td>
<td>83.33%</td>
<td/>
<td align="left">49.25%</td>
<td>50.4%</td>
<td>60.8%</td>
<td align="left">66.3%</td>
<td>55.20%</td>
</tr>
<tr>
<td>Pitch classification</td>
<td>57.23%</td>
<td>53.57%</td>
<td>62.91%</td>
<td>61.11%</td>
<td>55.56%</td>
<td>68.59%</td>
<td/>
<td align="left">54.84%</td>
<td>43.9%</td>
<td>61.7%</td>
<td align="left">52%</td>
<td align="left">57.99%</td>
</tr>
<tr>
<td>Yaw classification (within 15<sup>o</sup>)</td>
<td>89.21%</td>
<td>86.26%</td>
<td>93.42%</td>
<td>95.8%</td>
<td>87.65%</td>
<td>98.3%</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Pitch classification (within 15<sup>o</sup>)</td>
<td>84.28%</td>
<td>81.59%</td>
<td>88.18%</td>
<td>89.95%</td>
<td>87.11%</td>
<td>95.06%</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Classification</td>
<td>36.73%</td>
<td>33.24%</td>
<td>43.68%</td>
<td>59.7%</td>
<td>51.73%</td>
<td>65.54%</td>
<td>50.6%</td>
<td align="left">49.3%</td>
<td>47.8%</td>
<td/>
<td/>
<td>45.9%</td>
</tr>
<tr>
<td>classification (within 15<sup>o</sup>)</td>
<td>84.18%</td>
<td>80.21%</td>
<td>87.08%</td>
<td>91.09%</td>
<td>82.41%</td>
<td>94.44%</td>
<td/>
<td>81~84%</td>
<td/>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="fig6-0954407012441882" position="float">
<label>Figure 6.</label>
<caption>
<p>Pitch rotation estimation problem in 13 stages corresponding to 13 yaw rotations each as a 7-class problem containing all (7) vertical head poses. Results are shown for 8, 10, and 12 training samples.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig6.tif"/>
</fig>
</sec>
</sec>
<sec id="section17-0954407012441882">
<title>Hierarchical classification algorithm</title>
<p>This is an experiment intended to examine a different methodology for the classification, setting grounds to assess our method on a set of images that includes 72 angles of the Pointing’04 database.</p>
<p>Considering the large number of classes in the case of head poses estimation on the Pointing’04 database, we proposed a hierarchical classification algorithm so as to increase the accuracy of the estimation. The hierarchical classification is an algorithm based on the well-known ‘divide and conquer (D&amp;C)’ algorithm, as described elsewhere.<sup><xref ref-type="bibr" rid="bibr23-0954407012441882">23</xref></sup> In computer science, divide and conquer is an important algorithm design paradigm based on multi-branched recursion. This algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type, until these become simple enough to be solved directly. The solutions to the sub-problems are then combined to give a solution to the original problem.<sup><xref ref-type="bibr" rid="bibr23-0954407012441882">23</xref></sup> Therefore, in this method the whole space is divided into smaller ones in the three steps below.</p>
<p>First, the problem is separated into two two-class problems for the up–down and right–left cases. An expert is used to classify each (with a 95% correct classification rate). By applying these two classifiers, the primary space is partitioned into four parts. Each of these four areas is then looked at as a left–right two-class problem by classifiers of a second-level classifier expert type, resulting in an approximately 95% correct rate. In the final level then, to estimate the angle, each of the eight regions created is considered as a nine-class problem, classified by classifiers of a third-level expert type. This hierarchical procedure is depicted in <xref ref-type="fig" rid="fig7-0954407012441882">Figure 7</xref>.</p>
<fig id="fig7-0954407012441882" position="float">
<label>Figure 7.</label>
<caption>
<p>Hierarchical classification process for the estimation of the head pose.</p>
<p>Source: Gourier et al.,<sup><xref ref-type="bibr" rid="bibr19-0954407012441882">19</xref></sup> with permission.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig7.tif"/>
</fig>
<p>Application of this hierarchical method converts the complex space of the original problem into smaller problems so that the MLP classification results will increase the performance from a very low rate (about 15%) with one whole space to an acceptable level with the divided space. <xref ref-type="fig" rid="fig8-0954407012441882">Figure 8</xref> illustrates the ‘zoning’ procedure.</p>
<fig id="fig8-0954407012441882" position="float">
<label>Figure 8.</label>
<caption>
<p>Regions for the space of head positions.</p>
<p>Source: Gourier et al.,<sup><xref ref-type="bibr" rid="bibr19-0954407012441882">19</xref></sup> with permission.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig8.tif"/>
</fig>
<p>In this experiment, like the previous ones, the images of eight subjects from the total 15 are used for training of the classifier, while the remaining 7 are used as the test images, all selected randomly. <xref ref-type="fig" rid="fig9-0954407012441882">Figure 9</xref> illustrates the results of this experiment obtained in 20 runs.</p>
<fig id="fig9-0954407012441882" position="float">
<label>Figure 9.</label>
<caption>
<p>Performance chart for expert classifiers.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig9.tif"/>
</fig>
<p>
<xref ref-type="table" rid="table1-0954407012441882">Table 1</xref> shows the performance evaluation and comparison of the model in experiment 3 and the hierarchical model with those presented in the literature. The performance is assessed by the mean error in both the pitch and yaw directions. In addition, the classification rates are calculated in the yaw and pitch. The classification rates were also calculated by disregarding the estimation error of up to 15°.</p>
<p>The results of <xref ref-type="table" rid="table1-0954407012441882">Table 1</xref> illustrate that the hierarchical method outperforms the method of experiment 3. It is also observed that the results of our proposed method are considerably better than those of the PNMF,<sup><xref ref-type="bibr" rid="bibr9-0954407012441882">9</xref></sup> NMF,<sup><xref ref-type="bibr" rid="bibr9-0954407012441882">9</xref></sup> PCA,<sup><xref ref-type="bibr" rid="bibr9-0954407012441882">9</xref></sup> HOSVD,<sup><xref ref-type="bibr" rid="bibr24-0954407012441882">24</xref></sup> LAAM,<sup><xref ref-type="bibr" rid="bibr25-0954407012441882">25</xref></sup> and NN-based<sup><xref ref-type="bibr" rid="bibr26-0954407012441882">26</xref></sup> methods reported in the literature.</p>
<p>Although we reported the results with the MLP, we tested our method using the RBF as well. The results show that the estimation rate in the classification method is independent of the type of neural net used.</p>
</sec>
<sec id="section18-0954407012441882">
<title>Experiment for real-time video</title>
<p>In order to demonstrate the feasibility and applicability of the method proposed in this study, we performed an experiment to evaluate the performance of our method in the estimation of the head poses from the images in a video format database. This experiment shows that the performance of our method is quite feasible within the acceptable time frame required for image processing.</p>
<p>The time needed by the algorithm is directly related to the image quality. The minimum and maximum processing times needed in our work were observed to be about 20 ms and 34 ms, respectively, with an average of about 28 ms, a value less than the average required processing time of 30 ms reported in the literature.<sup><xref ref-type="bibr" rid="bibr13-0954407012441882">13</xref></sup></p>
<p>We also examined our proposed method in real driving situations with different drivers. The experiments we performed involved a certain vehicular testbed including a monocular camera mounted on the dashboard of the car in front of the driver. Furthermore, we designed a graphic user interface in Matlab to see the functionality of the model simultaneously.</p>
<p>It must be mentioned that a fair evaluation of the results in a real-world application requires certain devices such as calibrated cameras or accurate simulators, as indeed done elsewhere.<sup><xref ref-type="bibr" rid="bibr27-0954407012441882">27</xref>,<xref ref-type="bibr" rid="bibr28-0954407012441882">28</xref></sup> In addition, as stated by some authors,<sup><xref ref-type="bibr" rid="bibr13-0954407012441882">13</xref>,<xref ref-type="bibr" rid="bibr29-0954407012441882">29</xref>,<xref ref-type="bibr" rid="bibr30-0954407012441882">30</xref></sup> certain notion of the ground truth for the driver’s head pose is ideally needed for the validation of the system. Without a thorough and detailed understanding and knowledge of the true intentions of a driver in a driving situation, it is quite difficult to obtain an actual ground truth. In the absence of such a sound ground truth, we have made use of some actual driving data with a scaled frame applied on the program graphical user interface (GUI) for several hundred frames of the video sequences. This procedure, in effect, resulted in a ground setting which could be used as our ground truth in our ‘real-world’ driving experiment.</p>
<p>
<xref ref-type="fig" rid="fig10-0954407012441882">Figure 10</xref> shows some of the drivers who participated in the experiment to examine the ‘real-world’ applicability of our proposed method.</p>
<fig id="fig10-0954407012441882" position="float">
<label>Figure 10.</label>
<caption>
<p>Some drivers participated in the driving experiment.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig10.tif"/>
</fig>
<p>
<xref ref-type="fig" rid="fig11-0954407012441882">Figure 11</xref> shows the real-time execution of the computer program written based on the proposed algorithm for the estimation of the head pose.</p>
<fig id="fig11-0954407012441882" position="float">
<label>Figure 11.</label>
<caption>
<p>Real-time execution of the computer program written based on the proposed algorithm for the estimation of the head pose.</p>
</caption>
<graphic xlink:href="10.1177_0954407012441882-fig11.tif"/>
</fig>
</sec>
</sec>
<sec id="section19-0954407012441882" sec-type="conclusions">
<title>Conclusion</title>
<p>In this work, an effective algorithm for real-time head pose estimation based on a novel feature space and a hierarchical classification technique was proposed. This method is useful for driver assistance systems and video-based surveillance systems. While the real-time speed is a critical requirement in such applications, it was shown that our model performs fast with high accuracy. The main focus of this article is whether the driver is paying adequate attention to the driving, so that in the event he is not, some vocal warning can be issued to alert the driver. When the head direction is deviated for too long, the proposed algorithm provides a judgment to this effect.</p>
<p>There are a number of advantages associated with the proposed technique. First, the method of feature extraction in the image domain is independent of identity, keeping only the features related to the angles. Furthermore, the technique can handle cases in which the feature extraction method does not require the appearance of facial components on the sequences so clearly. Thus, our algorithm can detect those incomplete faces resulted from great occlusions and large orientations. Second, the hierarchical technique offers a stable performance for all orientations in the head pose space, requiring only one camera for each position, in contrast with those systems requiring more cameras with certain locations for the driver head pose estimation. In essence, the proposed methodology can be considered as a part of an advanced driver assistant system. It is also useful for collision prevention systems, severely handicapped people, and other driving applications.</p>
<p>The main restriction in using the proposed approach is the high similarity between head positions in each of the eight regions of our zoning procedure, an area open to further study in our future work. In addition we plan to add some means of veriﬁcation to be used in security systems.</p>
</sec>
</body>
<back>
<ack>
<p>The authors wish to express their thanks to ICPR’04 and the Face and Gesture Recognition Working group for providing the Pointing04 dataset which is used in this research.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0954407012441882">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wu</surname><given-names>J</given-names></name>
<name><surname>Trivedi</surname><given-names>MM</given-names></name>
</person-group>. <article-title>A two-stage head pose estimation framework and evaluation</article-title>. <source>Pattern Recognit</source> <year>2008</year>; <volume>41</volume>(<issue>3</issue>): <fpage>1138</fpage>–<lpage>1158</lpage>.</citation>
</ref>
<ref id="bibr2-0954407012441882">
<label>2.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Guo</surname><given-names>K</given-names></name>
<name><surname>Yu</surname><given-names>G</given-names></name>
<name><surname>Li</surname><given-names>Z</given-names></name>
</person-group>. <article-title>A new algorithm for analyzing driver’s attention state</article-title>. In: <conf-name>IEEE intelligent vehicles symposium</conf-name>, <year>2009</year>, <conf-loc>Xi’an, People’s Republic of China</conf-loc>, pp.<fpage>21</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr3-0954407012441882">
<label>3.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mamatha</surname><given-names>MN</given-names></name>
<name><surname>Ramachandran</surname><given-names>S</given-names></name>
</person-group>. <article-title>Automatic eyewinks interpretation system using face orientation recognition for human-machine interface</article-title>. <source>Int J Comput Sci Network Security</source> <year>2009</year>; <volume>9</volume>(<issue>5</issue>): <fpage>155</fpage>–<lpage>163</lpage>.</citation>
</ref>
<ref id="bibr4-0954407012441882">
<label>4.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Murphy-Chutorian</surname><given-names>E</given-names></name>
<name><surname>Trivedi</surname><given-names>M</given-names></name>
</person-group>. <article-title>Hybrid head orientation and position estimation (HyHOPE): A system and evaluation for driver support</article-title>. In: <conf-name>IEEE intelligent vehicles symposium</conf-name>, <year>2008</year>, <conf-loc>Eindhoven, the Netherlands</conf-loc>, pp.<fpage>512</fpage>–<lpage>517</lpage>.</citation>
</ref>
<ref id="bibr5-0954407012441882">
<label>5.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Baskar</surname><given-names>LD</given-names></name>
<name><surname>De Schutter</surname><given-names>B</given-names></name>
<name><surname>Hellendoorn</surname><given-names>H</given-names></name>
</person-group>. <article-title>Model-based predictive traffic control for intelligent vehicles: dynamic speed limits and dynamic lane allocation</article-title>. In: <conf-name>IEEE intelligent vehicles symposium</conf-name>, <year>2008</year>, <conf-loc>Eindhoven, the Netherlands</conf-loc>, pp.<fpage>174</fpage>–<lpage>179</lpage>.</citation>
</ref>
<ref id="bibr6-0954407012441882">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Murphy-Chutorian</surname><given-names>E</given-names></name>
<name><surname>Trivedi</surname><given-names>MM</given-names></name>
</person-group>. <article-title>Head pose estimation and augmented reality tracking: an integrated system and evaluation for monitoring driver awareness</article-title>. <source>IEEE Trans Intell Transp Syst</source> <year>2010</year>; <volume>11</volume>(<issue>2</issue>): <fpage>300</fpage>–<lpage>311</lpage>.</citation>
</ref>
<ref id="bibr7-0954407012441882">
<label>7.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Doshi</surname><given-names>A</given-names></name>
<name><surname>Trivedi</surname><given-names>MM</given-names></name>
</person-group>. <article-title>On the roles of eye gaze and head dynamics in predicting driver’s intent to change lanes</article-title>. <source>IEEE Trans Intell Transp Syst</source> <year>2009</year>; <volume>10</volume>(<issue>3</issue>): <fpage>453</fpage>–<lpage>462</lpage>.</citation>
</ref>
<ref id="bibr8-0954407012441882">
<label>8.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>Y</given-names></name>
<name><surname>Gong</surname><given-names>S</given-names></name>
<name><surname>Sherrah</surname><given-names>J</given-names></name>
<name><surname>Liddell</surname><given-names>H</given-names></name>
</person-group>. <article-title>Support vector machine based multi-view face detection and recognition</article-title>. <source>Image Vision Comput</source> <year>2004</year>; <volume>22</volume>(<issue>5</issue>): <fpage>413</fpage>–<lpage>427</lpage>.</citation>
</ref>
<ref id="bibr9-0954407012441882">
<label>9.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>X</given-names></name>
<name><surname>Lu</surname><given-names>H</given-names></name>
<name><surname>Luo</surname><given-names>H</given-names></name>
</person-group>. <article-title>A new representation method of head images for head pose estimation</article-title>. In: <conf-name>IEEE international conference on image processing</conf-name>, <year>2009</year>, <conf-loc>Cairo, Egypt</conf-loc>, pp.<fpage>3585</fpage>–<lpage>3588</lpage>.</citation>
</ref>
<ref id="bibr10-0954407012441882">
<label>10.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Voit</surname><given-names>M</given-names></name>
<name><surname>Nickel</surname><given-names>K</given-names></name>
<name><surname>Stiefelhagen</surname><given-names>R</given-names></name>
</person-group>. <article-title>Neural network-based head pose estimation and multiview fusion</article-title>. In: <source>Multimodal technologies for perception of humans</source>, <year>2007</year>, <publisher-loc>Berlin, Germany</publisher-loc>, pp.<fpage>291</fpage>–<lpage>298</lpage>.</citation>
</ref>
<ref id="bibr11-0954407012441882">
<label>11.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wang</surname><given-names>JG</given-names></name>
<name><surname>Sung</surname><given-names>E</given-names></name>
</person-group>. <article-title>EM enhancement of 3D head pose estimated by point at infinity</article-title>. <source>Image Vision Comput</source> <year>2007</year>; <volume>25</volume>(<issue>12</issue>): <fpage>1864</fpage>–<lpage>1874</lpage>.</citation>
</ref>
<ref id="bibr12-0954407012441882">
<label>12.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Xiao</surname><given-names>J</given-names></name>
<name><surname>Baker</surname><given-names>S</given-names></name>
<name><surname>Matthews</surname><given-names>I</given-names></name>
<name><surname>Kanade</surname><given-names>T</given-names></name>
</person-group>. <article-title>Real-time combined 2D+3D active appearance models</article-title>. In: <conf-name>IEEE conference on computer vision and pattern recognition</conf-name>, <year>2004</year>, <conf-loc>Washington, USA</conf-loc>, pp.<fpage>535</fpage>–<lpage>542</lpage>.</citation>
</ref>
<ref id="bibr13-0954407012441882">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Murphy-Chutoria</surname><given-names>E</given-names></name>
<name><surname>Trivedi</surname><given-names>MM</given-names></name>
</person-group>. <article-title>Head pose estimation in computer vision: a survey</article-title>. <source>IEEE Trans Pattern Analysis Machine Intell</source> <year>2009</year>; <volume>31</volume>(<issue>4</issue>): <fpage>607</fpage>–<lpage>626</lpage>.</citation>
</ref>
<ref id="bibr14-0954407012441882">
<label>14.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Ghaffari</surname><given-names>A</given-names></name>
<name><surname>Rezvan</surname><given-names>M</given-names></name>
<name><surname>Khodayari</surname><given-names>A</given-names></name>
<name><surname>Vahidi-Shams</surname><given-names>A</given-names></name>
</person-group>. <article-title>A robust head pose tracking and estimating aprouch for driver assistant system</article-title>. In: <conf-name>IEEE international conference on vehicular electronics and safety</conf-name>, <year>2011</year>, <conf-loc>Beijing, People’s Republic of China</conf-loc>, pp.<fpage>180</fpage>–<lpage>186</lpage>.</citation>
</ref>
<ref id="bibr15-0954407012441882">
<label>15.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Joshi</surname><given-names>KR</given-names></name>
<name><surname>Kamathe</surname><given-names>RS</given-names></name>
</person-group>. <article-title>Quantification of retinex in enhancement of whether degraded image</article-title>. In: <conf-name>International conference on audio, language and image processing</conf-name>, <year>2008</year>, <conf-loc>Shanghai, People’s Republic of China</conf-loc>, pp.<fpage>1229</fpage>–<lpage>1233</lpage>.</citation>
</ref>
<ref id="bibr16-0954407012441882">
<label>16.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Land</surname><given-names>E</given-names></name>
</person-group>. <article-title>An alternative technique for the computation of the designator in the retinex theory of color vision</article-title>. <source>Proc Natl Acad Sci USA</source> <year>1986</year>; <volume>83</volume>(<issue>10</issue>): <fpage>3078</fpage>–<lpage>3080</lpage>.</citation>
</ref>
<ref id="bibr17-0954407012441882">
<label>17.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jobson</surname><given-names>DJ</given-names></name>
<name><surname>Rahman</surname><given-names>Z</given-names></name>
<name><surname>Woodell</surname><given-names>GA</given-names></name>
</person-group>. <article-title>Properties and performance of a center/surround retinex</article-title>. <source>IEEE Trans Image Processing</source> <year>1997</year>; <volume>6</volume>(<issue>3</issue>): <fpage>451</fpage>–<lpage>462</lpage>.</citation>
</ref>
<ref id="bibr18-0954407012441882">
<label>18.</label>
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Bian</surname><given-names>Z</given-names></name>
<name><surname>Zhang</surname><given-names>Y</given-names></name>
</person-group>. <article-title>Retinex image enhancement techniques: algorithm, application and advantages</article-title>. <comment>Final project report for EE264 image processing and reconstruction</comment>, <year>2002</year>.</citation>
</ref>
<ref id="bibr19-0954407012441882">
<label>19.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Gourier</surname><given-names>N</given-names></name>
<name><surname>Hall</surname><given-names>D</given-names></name>
<name><surname>Crowley</surname><given-names>J</given-names></name>
</person-group>. <article-title>Estimating face orientation from robust detection of salient facial structures</article-title>. In: <conf-name>International workshop on visual observation of deictic gestures</conf-name>, <year>2004</year>, <conf-loc>Cambridge, UK</conf-loc>, pp.<fpage>17</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr20-0954407012441882">
<label>20.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turk</surname><given-names>M</given-names></name>
<name><surname>Pentland</surname><given-names>A</given-names></name>
</person-group>. <article-title>Eigenfaces for recognition</article-title>. <source>J Cognitive Neurosci</source> <year>1991</year>; <volume>3</volume>(<issue>1</issue>): <fpage>71</fpage>–<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr21-0954407012441882">
<label>21.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Pentland</surname><given-names>A</given-names></name>
<name><surname>Moghaddam</surname><given-names>B</given-names></name>
<name><surname>Stamer</surname><given-names>T</given-names></name>
</person-group>. <article-title>View-based and modular eigenspaces for face recognition</article-title>. In: <conf-name>IEEE computer society conference on computer vision and pattern recognition</conf-name>, <year>1994</year>, <conf-loc>Seattle, WA, USA</conf-loc>, pp.<fpage>84</fpage>–<lpage>91</lpage>.</citation>
</ref>
<ref id="bibr22-0954407012441882">
<label>22.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tu</surname><given-names>J</given-names></name>
<name><surname>Fu</surname><given-names>Y</given-names></name>
<name><surname>Huang</surname><given-names>TS</given-names></name>
</person-group>. <article-title>Locating nose-tips and estimating head poses in images by tensorposes</article-title>. <source>IEEE Trans Circuits Syst Video Technol</source> <year>2009</year>; <volume>19</volume>(<issue>1</issue>): <fpage>90</fpage>–<lpage>102</lpage>.</citation>
</ref>
<ref id="bibr23-0954407012441882">
<label>23.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cormen</surname><given-names>TH</given-names></name>
<name><surname>Leiserson</surname><given-names>CE</given-names></name>
<name><surname>Rivest</surname><given-names>RL</given-names></name>
<name><surname>Stein</surname><given-names>C</given-names></name>
</person-group>. <source>Introduction to algorithms</source>, <edition>2nd ed.</edition> <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>, <year>2001</year>.</citation>
</ref>
<ref id="bibr24-0954407012441882">
<label>24.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Tu</surname><given-names>J</given-names></name>
<name><surname>Fu</surname><given-names>Y</given-names></name>
<name><surname>Hu</surname><given-names>Y</given-names></name>
<name><surname>Huang</surname><given-names>T</given-names></name>
</person-group>. <article-title>Evaluation of head pose estimation for studio data</article-title>. In: <conf-name>International workshop on classification of events activities and relationships</conf-name>, <year>2006</year>, <conf-loc>Southampton, UK</conf-loc>, pp.<fpage>281</fpage>–<lpage>290</lpage>.</citation>
</ref>
<ref id="bibr25-0954407012441882">
<label>25.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Gourier</surname><given-names>N</given-names></name>
<name><surname>Maisonnasse</surname><given-names>J</given-names></name>
<name><surname>Hall</surname><given-names>D</given-names></name>
<name><surname>Crowley</surname><given-names>J</given-names></name>
</person-group>. <article-title>Head pose estimation on low resolution images</article-title>. In: <conf-name>International workshop on classification of events activities and relationships</conf-name>, <year>2006</year>, <conf-loc>Southampton, UK</conf-loc>, pp.<fpage>281</fpage>–<lpage>290</lpage>.</citation>
</ref>
<ref id="bibr26-0954407012441882">
<label>26.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Stiefelhagen</surname><given-names>R</given-names></name>
</person-group>. <article-title>Estimating head pose with neural networks – results on the Pointing’04 ICPR workshop evaluation data</article-title>. In: <conf-name>Pointing 2004 workshop: visual observation of deictic gestures</conf-name>, <year>2004</year>, <conf-loc>Cambridge, UK</conf-loc>.</citation>
</ref>
<ref id="bibr27-0954407012441882">
<label>27.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Beauchemin</surname><given-names>S</given-names></name>
<name><surname>Varcheie</surname><given-names>P</given-names></name>
<name><surname>Gagnon</surname><given-names>L</given-names></name>
<etal/>
</person-group>. <article-title>COBVIS-D: A computer vision system for describing the cephalo-ocular behavior of drivers in a driving simulator</article-title>. In: <conf-name>International conference on Image analysis and recognition</conf-name>, <year>2009</year>, <conf-loc>Halifax, Canada</conf-loc>, pp.<fpage>604</fpage>–<lpage>615</lpage>.</citation>
</ref>
<ref id="bibr28-0954407012441882">
<label>28.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Z</given-names></name>
<name><surname>He</surname><given-names>G</given-names></name>
</person-group>. <article-title>A vehicle anti-theft and alarm system based on computer vision</article-title>. In: <conf-name>IEEE international conference on vehicular electronics and safety</conf-name>, <year>2005</year>, <conf-loc>Xi’an, People’s Republic of China</conf-loc>, pp.<fpage>326</fpage>–<lpage>330</lpage>.</citation>
</ref>
<ref id="bibr29-0954407012441882">
<label>29.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Doshi</surname><given-names>A</given-names></name>
<name><surname>Trivedi</surname><given-names>MM</given-names></name>
</person-group>. <article-title>Attention estimation by simultaneous observation of viewer and view</article-title>. In: <conf-name>IEEE computer society conference on computer vision and pattern recognition workshops</conf-name>, <year>2010</year>, <conf-loc>San Francisco, CA, USA</conf-loc>, pp.<fpage>21</fpage>–<lpage>27</lpage>.</citation>
</ref>
<ref id="bibr30-0954407012441882">
<label>30.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Tran</surname><given-names>C</given-names></name>
<name><surname>Trivedi</surname><given-names>MM</given-names></name>
</person-group>. <article-title>Driver assistance for ’keeping hands on the wheel and eyes on the road</article-title>. In: <conf-name>IEEE international conference on vehicular electronics and safety</conf-name>, <year>2009</year>, <conf-loc>Pune, India</conf-loc>, pp.<fpage>97</fpage>–<lpage>101</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>