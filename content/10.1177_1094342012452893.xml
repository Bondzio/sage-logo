<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342012452893</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342012452893</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Approximate weighted matching on emerging manycore and multithreaded architectures</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Halappanavar</surname>
<given-names>Mahantesh</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012452893">1</xref>
<xref ref-type="corresp" rid="corresp1-1094342012452893"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Feo</surname>
<given-names>John</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012452893">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Villa</surname>
<given-names>Oreste</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012452893">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Tumeo</surname>
<given-names>Antonino</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012452893">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pothen</surname>
<given-names>Alex</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342012452893">2</xref>
</contrib>
<bio>
<title>Author’s Biographies</title>
<p>
<bold>Mahantesh Halappanavar</bold> is a Research Scientist with the Computational Sciences and Mathematics Division at the Pacific Northwest National Laboratory. He received his MS and PhD degrees in Computer Science from the Old Dominion University in 2003 and 2009, respectively. His work focuses on parallel graph algorithms and spans several application areas including analysis of electric power grids, statistical textual analysis, numerical linear algebra, information security and machine learning. He explores the interplay of algorithm design, architectural features, and input characteristics targeting massively multithreaded architectures such as the Cray XMT and emerging multicore and manycore platforms.</p>
<p>
<bold>John Feo</bold> is the Director of the Center for Adaptive Supercomputer Software at the Pacific Northwest Laboratory. He received his PhD in Computer Science from The University of Texas at Austin. He began his career at Lawrence Livermore National Laboratory where he managed the Computer Science Group and was the principal investigator of the Sisal Language Project. He then joined Tera Computer Company (now Cray Inc) where he was a principal engineer and product manager for the MTA-1 and MTA-2, the first two generations of the Cray's multithreaded architecture. After a short 2-year ‘sabbatical' at Microsoft where he led a software group developing a next-generation virtual reality platform, he joined PNNL. His research interests are parallel programming, graph algorithms, multithreaded architectures, functional languages, and performance studies. He has held academic positions at UC Davis and is an adjunct faculty at Washington State University.</p>
<p>
<bold>Oreste Villa</bold> is a Research Scientist at the Pacific Northwest National Laboratory (PNNL) with a research focus on computer architectures and simulation, accelerators for scientific computing and irregular applications. He joined PNNL in May 2008 after receiving his PhD from Politecnico di Milano for his research on ‘Designing and Programming Advanced Multicore Architectures'. While earning his PhD, he was an intern student at PNNL, conducting research in programming techniques and algorithms for advanced multicore architectures, cluster fault tolerance and virtualization techniques for HPC. He received a MS degree in Electronic Engineering in 2003 from the University of Cagliari in Italy and an ME degree in 2004 in Embedded Systems Design from the University of Lugano in Switzerland.</p>
<p>
<bold>Antonino Tumeo</bold> received the MS degree in Informatics Engineering, in 2005, and the PhD degree in Computer Engineering, in 2009, from Politecnico di Milano in Italy. Since February 2011, he has been a Research Scientist at Pacific Northwest National Laboratory (PNNL). He joined PNNL in 2009 as a post doctoral research associate. Previously, he was a post-doctoral researcher at Politecnico di Milano. His research interests are modeling and simulation of high performance architectures, hardware-software codesign, FPGA prototyping and GPGPU computing.</p>
<p>
<bold>Alex Pothen</bold> received his PhD from Cornell University and is a Professor of Computer Science at Purdue University. His research interests include combinatorial scientific computing, high-performance computing and bioinformatics. He served as the Director of the CSCAPES Institute which developed combinatorial algorithms for enabling computational science on Petascale computers, with funding from the Office of Science of the U.S. Department of Energy from 2006–2012. He serves on the editorial boards of <italic>Journal of the ACM</italic> and <italic>SIAM Review</italic>, the flagship journals of the Association for Computing Machinery and the Society for Industrial and Applied Mathematics.</p>
</bio>
</contrib-group>
<aff id="aff1-1094342012452893">
<label>1</label>Pacific Northwest National Laboratory, Richland, WA, USA</aff>
<aff id="aff2-1094342012452893">
<label>2</label>Purdue University, West Lafayette, IN, USA</aff>
<author-notes>
<corresp id="corresp1-1094342012452893">Mahantesh Halappanavar, Pacific Northwest National Laboratory, 902 Battelle Boulevard, P.O.Box 999, MSIN J4-30, Richland, WA 99352, USA Email: <email>mahantesh.halappanavar@pnl.gov</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>26</volume>
<issue>4</issue>
<fpage>413</fpage>
<lpage>430</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Graph matching is a prototypical combinatorial problem with many applications in high-performance scientific computing. Optimal algorithms for computing matchings are challenging to parallelize. Approximation algorithms are amenable to parallelization and are therefore important to compute matchings for large-scale problems. Approximation algorithms also generate nearly optimal solutions that are sufficient for many applications. In this paper we present multithreaded algorithms for computing half-approximate weighted matching on state-of-the-art multicore (Intel Nehalem and AMD Magny-Cours), manycore (Nvidia Tesla and Nvidia Fermi), and massively multithreaded (Cray XMT) platforms. We provide two implementations: the first uses shared work queues and is suited for all platforms; and the second implementation, based on dataflow principles, exploits special features available on the Cray XMT. Using a carefully chosen dataset that exhibits characteristics from a wide range of applications, we show scalable performance across different platforms. In particular, for one instance of the input, an R-MAT graph (RMAT-G), we show speedups of about <inline-formula id="inline-formula3-1094342012452893">
<mml:math id="mml-inline3-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula> on <inline-formula id="inline-formula4-1094342012452893">
<mml:math id="mml-inline4-1094342012452893">
<mml:mn>48</mml:mn>
</mml:math>
</inline-formula> cores of an AMD Magny-Cours, <inline-formula id="inline-formula5-1094342012452893">
<mml:math id="mml-inline5-1094342012452893">
<mml:mn>7</mml:mn>
</mml:math>
</inline-formula> on <inline-formula id="inline-formula6-1094342012452893">
<mml:math id="mml-inline6-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> cores of Intel Nehalem, <inline-formula id="inline-formula7-1094342012452893">
<mml:math id="mml-inline7-1094342012452893">
<mml:mn>3</mml:mn>
</mml:math>
</inline-formula> on Nvidia Tesla and <inline-formula id="inline-formula8-1094342012452893">
<mml:math id="mml-inline8-1094342012452893">
<mml:mn>10</mml:mn>
</mml:math>
</inline-formula> on Nvidia Fermi relative to one core of Intel Nehalem, and <inline-formula id="inline-formula9-1094342012452893">
<mml:math id="mml-inline9-1094342012452893">
<mml:mn>60</mml:mn>
</mml:math>
</inline-formula> on <inline-formula id="inline-formula10-1094342012452893">
<mml:math id="mml-inline10-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula> processors of Cray XMT. We demonstrate strong as well as weak scaling for graphs with up to a billion edges using up to 12,800 threads. We avoid excessive fine-tuning for each platform and retain the basic structure of the algorithm uniformly across platforms. An exception is the dataflow algorithm designed specifically for the Cray XMT. To the best of the authors' knowledge, this is the first such large-scale study of the half-approximate weighted matching problem on multithreaded platforms. Driven by the critical enabling role of combinatorial algorithms such as matching in scientific computing and the emergence of informatics applications, there is a growing demand to support irregular computations on current and future computing platforms. In this context, we evaluate the capability of emerging multithreaded platforms to tolerate latency induced by irregular memory access patterns, and to support fine-grained parallelism via light-weight synchronization mechanisms. By contrasting the architectural features of these platforms against the Cray XMT, which is specifically designed to support irregular memory-intensive applications, we delineate the impact of these choices on performance.</p>
</abstract>
<kwd-group>
<kwd>approximation</kwd>
<kwd>dataflow</kwd>
<kwd>GPU</kwd>
<kwd>graph algorithms</kwd>
<kwd>manycore</kwd>
<kwd>matching</kwd>
<kwd>multicore</kwd>
<kwd>weighted matching</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342012452893">
<title>1 Introduction</title>
<p>A matching <inline-formula id="inline-formula12-1094342012452893">
<mml:math id="mml-inline12-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> in a graph <inline-formula id="inline-formula13-1094342012452893">
<mml:math id="mml-inline13-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is a subset of edges such that no two edges in <inline-formula id="inline-formula14-1094342012452893">
<mml:math id="mml-inline14-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> are incident on the same vertex. In other words, a matching is a pairing of vertices between the two endpoints of matched edges. Based on its objective function, there are several variants of the matching problem. For example, a maximum (cardinality) matching maximizes the number of matched edges in <inline-formula id="inline-formula15-1094342012452893">
<mml:math id="mml-inline15-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula>; a maximum weighted matching maximizes the sum or product of the weights of matched edges; and a bottleneck matching has an objective function to maximize the minimum weight of a matched edge in <inline-formula id="inline-formula16-1094342012452893">
<mml:math id="mml-inline16-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula>. The algorithms can be further classified as optimal or approximate. In this paper, we are interested in the weighted matching problem with an objective function to maximize the sum of weights of matched edges. In particular, we focus on a half-approximate weighted matching that guarantees a solution that is at least half the weight of a solution computed by an optimal algorithm. Formally, given a graph <inline-formula id="inline-formula17-1094342012452893">
<mml:math id="mml-inline17-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> with a weight function <inline-formula id="inline-formula18-1094342012452893">
<mml:math id="mml-inline18-1094342012452893">
<mml:mi>w</mml:mi>
<mml:mo stretchy="false">:</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="bold">R</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
</mml:msup>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula19-1094342012452893">
<mml:math id="mml-inline19-1094342012452893">
<mml:mi>V</mml:mi>
</mml:math>
</inline-formula> is a set of vertices, <inline-formula id="inline-formula20-1094342012452893">
<mml:math id="mml-inline20-1094342012452893">
<mml:mi>E</mml:mi>
</mml:math>
</inline-formula> is a set of edges, and each edge has a positive real number as its weight, and a matching <inline-formula id="inline-formula21-1094342012452893">
<mml:math id="mml-inline21-1094342012452893">
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">⊆</mml:mo>
<mml:mi>E</mml:mi>
</mml:math>
</inline-formula>, the weight of the matching is given by <inline-formula id="inline-formula22-1094342012452893">
<mml:math id="mml-inline22-1094342012452893">
<mml:mi>w</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>M</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:msub>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>e</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>M</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
<mml:mi>w</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>e</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula23-1094342012452893">
<mml:math id="mml-inline23-1094342012452893">
<mml:mi>e</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>E</mml:mi>
</mml:math>
</inline-formula> is an edge. We present a serial half-approximate algorithm (Manne and Bisseling, 2007), referred to as the Locally-Dominant Algorithm, in Section 3.1. Parallel implementations of this algorithm are presented in Section 4. A novel implementation based on dataflow principles is presented in Section 4.2. The dataflow algorithm addresses some of the limitations of the algorithm based on shared work queues.</p>
<p>Matching is a fundamental combinatorial problem with many applications in scientific computing. For instance: weighted as well as unweighted matchings are used in the solution of sparse linear systems to place large matrix elements on or close to the diagonal (<xref ref-type="bibr" rid="bibr12-1094342012452893">Duff and Koster, 1999</xref>; <xref ref-type="bibr" rid="bibr21-1094342012452893">Li and Demmel, 1998</xref>; <xref ref-type="bibr" rid="bibr32-1094342012452893">Schenk et al., 2007</xref>); weighted matchings are used in the computation of a sparse basis for the null space or column space of a under-determined matrices (<xref ref-type="bibr" rid="bibr29-1094342012452893">Pinar et al., 2006</xref>); maximum matching is used in the computation of the block-triangular form of a matrix (BTF) (<xref ref-type="bibr" rid="bibr30-1094342012452893">Pothen and Fan, 1990</xref>); approximate weighted matchings are used in multilevel graph algorithms for partitioning and clustering during the coarsening phase (<xref ref-type="bibr" rid="bibr19-1094342012452893">Karypis and Kumar, 1995</xref>); weighted matchings are used in image processing using shape contexts (<xref ref-type="bibr" rid="bibr7-1094342012452893">Belongie et al., 2002</xref>). In addition to scientific computing, matching also plays a key enabling role in bioinformatics, network switch design, web technologies, etc. These applications drive the need for efficient parallel implementations of matching algorithms. The need is justified not only by the emergence of large-scale problems where time-to-solution is important, but also by the stagnation in the performance of serial processing (<xref ref-type="bibr" rid="bibr3-1094342012452893">Asanovic et al., 2006</xref>).</p>
<p>While desirable, there are considerable challenges in implementing graph algorithms on traditional distributed-memory parallel processors and these have been well documented in scientific literature (<xref ref-type="bibr" rid="bibr23-1094342012452893">Lumsdaine et al., 2007</xref>; <xref ref-type="bibr" rid="bibr16-1094342012452893">Hendrickson and Berry, 2008</xref>). Some of the key challenges are: (i) efficient distribution of work among processors is determined by the structural and numerical properties of the input. Load balanced partitioning of work is particularly challenging in emerging applications in informatics (<xref ref-type="bibr" rid="bibr1-1094342012452893">Abou-Rjeili and Karypis, 2006</xref>); (ii) a lack of coarse-grained parallelism in many graph algorithms leads to fine-grained synchronization resulting in poor utilization of resources; and (iii) a small amount of computation per communicated word makes amortization of the communication cost difficult. Fine-grained synchronization can also lead to low computation per communicated word. As a consequence of these performance limiting challenges, shared-memory platforms provide an attractive alternative for implementing and executing graph algorithms (<xref ref-type="bibr" rid="bibr10-1094342012452893">Cong and Bader, 2007</xref>; <xref ref-type="bibr" rid="bibr28-1094342012452893">Nieplocha et al., 2007</xref>). However, efficient implementation of graph algorithms on shared-memory platforms is also challenging. Some of the challenges are similar to those encountered on distributed-memory platforms: irregular memory-access patterns that result in poor utilization of system resources; low amounts of work per accessed word of memory that make amortization of memory access costs difficult; and the need for fine-grained synchronization between threads that lead to poor utilization of system resources.</p>
<p>In order to address these challenges, we focus on three key hardware--software co-design features: (i) hardware multithreading to tolerate latencies arising not only from memory operations but also from synchronization between threads; (ii) dynamic, rather than static, load-balancing through scheduling and shared work queues; and (iii) extended memory semantics using tag bits to enable fine-grained synchronization. While we focus on these three features, we note that there are several other components that are needed to deliver scalable performance such as: runtime support, programming models, and efficient atomic memory operations. In order to evaluate these features effectively we provide results on five shared-memory platforms that are chosen to cover a broad spectrum of features: traditional multicore platforms with deep cache hierarchies, emerging manycore platforms (graphics processing units [GPUs]), and a non-traditional massively multithreaded cache-less architecture. These platforms are discussed in Section 3.2, and experimental results are provided in Section 5.</p>
<p>Coinciding with the manycore revolution in computer architecture is a revolution in informatics applications that is being enabled by the emergence of large-scale real-world data from scientific experiments as well as Internet phenomena such as social networks and connectivity of web pages. While the graphs arising in scientific computing tend to be fairly regular in structure, those arising in emerging applications such as social network analysis tend to be characterized by power law degree distribution, small world phenomenon of short average distance between any two vertices in a graph, and dense clustering within groups of vertices. In order to represent the characteristics of these graphs we experiment with a carefully chosen set of synthetic graphs generated using the R-MAT algorithm and a few instances from real-world data. The details are provided in Section 3.3.</p>
<p>The key contributions of this paper area as follows:<list list-type="order">
<list-item>
<p>A detailed discussion of multithreaded implementations of the half-approximate weighted matching algorithm including a novel dataflow-based implementation on the Cray XMT.</p>
</list-item>
<list-item>
<p>Presentation of scalable experimental results on emerging multicore (AMD Magny-Cours, Intel Nehalem), manycore (Nvidia Tesla and Nvidia Fermi), and massively multithreaded (Cray XMT) platforms.</p>
</list-item>
<list-item>
<p>Presentation of insights from considering algorithm design, input characteristics, and hardware features in tandem.</p>
</list-item>
</list>To the best of our knowledge, for the half-approximate weighted matching problem: this is the first large-scale study on diverse shared-memory parallel platforms, first implementation on GPUs, and the first work to consider the interplay of algorithm design and hardware features using large-scale graphs of up to a billion edges carefully generated and selected from real-world data. In the past, only a limited set of results have only been presented on distributed-memory platforms (<xref ref-type="bibr" rid="bibr14-1094342012452893">Halappanavar, 2009</xref>) and shared-memory platforms (<xref ref-type="bibr" rid="bibr25-1094342012452893">Manne and Bisseling, 2007</xref>). The input in earlier experiments has been limited to a few classes of graphs.</p>
</sec>
<sec id="section2-1094342012452893">
<title>2 Related work</title>
<p>Matching is an important combinatorial problem and has been studied extensively by researchers. Detailed discussions on matching can be found in <xref ref-type="bibr" rid="bibr22-1094342012452893">Lovasz (1986</xref>), <xref ref-type="bibr" rid="bibr27-1094342012452893">Monien et al. (2000</xref>), and <xref ref-type="bibr" rid="bibr33-1094342012452893">Schrijver (2003</xref>). Approximation algorithms for matching have also been studied extensively. <xref ref-type="bibr" rid="bibr4-1094342012452893">Avis (1983</xref>) proposed a greedy half-approximate algorithm for weighted matching based on sorting. Edges are sorted in a non-increasing order of their weights. A maximal (an approximation guarantee of half for cardinality) matching is then computed by repeatedly adding the current heaviest edge to the matching and removing all its neighbors from the graph. A strict global order for processing edges renders this approach unsuitable for parallel implementation. <xref ref-type="bibr" rid="bibr31-1094342012452893">Preis (1999</xref>) proposed a half-approximate algorithm for weighted matching based on searching instead of sorting. Preis' algorithm starts from an arbitrary edge and processes its neighbors. If one of its neighbors is heavier, then it progresses along that edge. The search stops when a locally dominant edge, an edge heavier than all of its neighbors, is reached. Preis' algorithm computes a half-approximate matching in linear time. Preis' algorithm, as proposed, relies on graph searches and complex book-keeping and is unsuitable for parallel implementation. However, Preis' work was seminal and laid the foundation for subsequent research by others. In particular, the work of Drake and Hougardy that followed the work of Preis is noteworthy for its simplicity. Their half-approximate algorithm for weighted matching is based on the simple idea of growing paths. Two temporary sets of matched edges are maintained. The algorithm starts the search from an arbitrary vertex and selects the heaviest neighbor. This neighbor is added to the first set of matching and the vertex (and all edges incident on it) is deleted from the graph. The search continues from other endpoint of the matched edge, but this time the heaviest edge is added to the second set of matched edges. The search continues until all of the vertices in the graph have been visited. The heavier of the two sets is returned as the final solution. Although simple, this algorithm is also unsuitable for parallel implementation (<xref ref-type="bibr" rid="bibr11-1094342012452893">Drake and Hougardy, 2003</xref>).</p>
<p>
<xref ref-type="bibr" rid="bibr17-1094342012452893">Hoepman (2004</xref>) developed a distributed variant of Preis' algorithm by assuming one vertex per processor. <xref ref-type="bibr" rid="bibr25-1094342012452893">Manne and Bisseling (2007</xref>) adapted this algorithm for a practical distributed-memory implementation with multiple vertices (subgraph) per processor. Building on (Manne and Bisseling's) work, efficient distributed-memory parallel algorithm and scalable implementation was described by <xref ref-type="bibr" rid="bibr14-1094342012452893">Halappanavar (2009</xref>) and <xref ref-type="bibr" rid="bibr8-1094342012452893">Catalyurek et al. (2011</xref>). In this paper, we provide multithreaded implementations of the same algorithm that we refer to as the locally dominant algorithm in the paper.</p>
<p>A class of optimal algorithms for computing matchings is based on the technique of augmentation, in which a matching computed at an intermediate step is augmented by using special paths known as augmenting paths (<xref ref-type="bibr" rid="bibr22-1094342012452893">Lovasz, 1986</xref>). Matching algorithms based on the technique of augmentation perform searches in a graph either in a breadth-first or a depth-first manner. In this context, we cite related work on breadth-first search, <inline-formula id="inline-formula24-1094342012452893">
<mml:math id="mml-inline24-1094342012452893">
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula>-connectivity, and all-pairs shortest-path computations. Several shared-memory, as well as distributed-memory, implementations exist for these graph kernels. Some of the important references include the seminal work of <xref ref-type="bibr" rid="bibr5-1094342012452893">Bader and Madduri (2006</xref>) on Cray MTA-2 (a predecessor of XMT), <xref ref-type="bibr" rid="bibr28-1094342012452893">Nieplocha et al. (2007</xref>) on XMT, <xref ref-type="bibr" rid="bibr10-1094342012452893">Cong and Bader (2007</xref>) on shared-memory platforms, <xref ref-type="bibr" rid="bibr2-1094342012452893">Agarwal et al. (2010</xref>) on emerging multicore platforms, <xref ref-type="bibr" rid="bibr34-1094342012452893">Yoo et al. (2005</xref>) on BlueGene/L platform, and <xref ref-type="bibr" rid="bibr16-1094342012452893">Hendrickson and Berry (2008</xref>) on cross-architecture comparisons and computational challenges.</p>
<p>Several papers on implementation of graph algorithms on general-purpose GPUs exist. Foremost among them is the work of <xref ref-type="bibr" rid="bibr15-1094342012452893">Harish and Narayanan (2007</xref>). Subsequent work was presented by <xref ref-type="bibr" rid="bibr20-1094342012452893">Katz and Kider (2008</xref>), <xref ref-type="bibr" rid="bibr24-1094342012452893">Luo et al. (2010</xref>), and most recently by <xref ref-type="bibr" rid="bibr18-1094342012452893">Hong et al. (2011</xref>). In the work of <xref ref-type="bibr" rid="bibr15-1094342012452893">Harish and Narayanan (2007</xref>) we observe that the cost of breadth-first search is <inline-formula id="inline-formula25-1094342012452893">
<mml:math id="mml-inline25-1094342012452893">
<mml:mi>O</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mi>L</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula26-1094342012452893">
<mml:math id="mml-inline26-1094342012452893">
<mml:mi>L</mml:mi>
</mml:math>
</inline-formula> represents the number of levels (the longest distance from the source to any vertex), and <inline-formula id="inline-formula27-1094342012452893">
<mml:math id="mml-inline27-1094342012452893">
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula28-1094342012452893">
<mml:math id="mml-inline28-1094342012452893">
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> are the number of vertices and edges, respectively. While an efficient implementation of breadth-first search has a cost of <inline-formula id="inline-formula29-1094342012452893">
<mml:math id="mml-inline29-1094342012452893">
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, the implementation of Harish and Narayanan is negatively impacted by architectural limitations of older generation of GPUs. The implementation of Harish and Narayanan, and a similar approach of others, is therefore suitable only for instances with small values of <inline-formula id="inline-formula30-1094342012452893">
<mml:math id="mml-inline30-1094342012452893">
<mml:mi>L</mml:mi>
</mml:math>
</inline-formula>. In contrast, our GPU implementation of approximate matching uses an efficient approach that is similar to implementation on other multithreaded platforms.</p>
</sec>
<sec id="section3-1094342012452893">
<title>3 Preliminaries</title>
<p>We provide relevant background information in this section. The information is organized into three subsections on: serial algorithm for the half-approximate weighted matching problem, hardware platforms, and the dataset used for experiments.</p>
<sec id="section4-1094342012452893">
<title>3.1 Locally dominant algorithm for half-approximate weighted matching</title>
<p>We now describe a generic queue-based implementation of the half-approximate algorithm for weighted matching. The algorithm was proposed by <xref ref-type="bibr" rid="bibr25-1094342012452893">Manne and Bisseling (2007</xref>) as a serial variant of Hoepman's algorithm (<xref ref-type="bibr" rid="bibr17-1094342012452893">Hoepman, 2004</xref>). We provide the details in Algorithm 1, which takes as input a graph <inline-formula id="inline-formula31-1094342012452893">
<mml:math id="mml-inline31-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and returns as output a matching <inline-formula id="inline-formula32-1094342012452893">
<mml:math id="mml-inline32-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula>. In order to simplify the description of parallel algorithms, we divide the execution into two phases: Phase 1 and Phase 2.</p>
<p>
<table-wrap id="table4-1094342012452893" position="float">
<label>Algorithm 1</label>
<caption>
<p>Serial queue-based implementation. <italic>Input</italic>: Graph <inline-formula id="inline-formula33-1094342012452893">
<mml:math id="mml-inline33-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. <italic>Output</italic>: A matching <inline-formula id="inline-formula34-1094342012452893">
<mml:math id="mml-inline34-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> represented in vector mate. <italic>Data structures</italic>: a queue, <inline-formula id="inline-formula35-1094342012452893">
<mml:math id="mml-inline35-1094342012452893">
<mml:mi>Q</mml:mi>
</mml:math>
</inline-formula>, that consists of unprocessed matched vertices; a vector candidate of size <inline-formula id="inline-formula36-1094342012452893">
<mml:math id="mml-inline36-1094342012452893">
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> that contains the i.d. of the current-heaviest neighbor of each vertex.</p>
</caption>
<graphic alternate-form-of="table4-1094342012452893" xlink:href="10.1177_1094342012452893-table4.tif"/>
<table>
<tbody>
<tr>
<td> 1:</td>
<td>
<bold>procedure</bold> S<sc>erial</sc>-Q<sc>ueue (</sc>
<inline-formula id="inline-formula37-1094342012452893">
<mml:math id="mml-inline37-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, mate)</td>
<td>
</td>
</tr>
<tr>
<td> 2:</td>
<td> <bold>for each</bold> <inline-formula id="inline-formula38-1094342012452893">
<mml:math id="mml-inline38-1094342012452893">
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>V</mml:mi>
</mml:math>
</inline-formula> <bold>do</bold>
</td>
<td>▷Initialization</td>
</tr>
<tr>
<td> 3:</td>
<td>  mate[<inline-formula id="inline-formula39-1094342012452893">
<mml:math id="mml-inline39-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula40-1094342012452893">
<mml:math id="mml-inline40-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 4:</td>
<td>  candidate[<inline-formula id="inline-formula41-1094342012452893">
<mml:math id="mml-inline41-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula42-1094342012452893">
<mml:math id="mml-inline42-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 5:</td>
<td> <inline-formula id="inline-formula43-1094342012452893">
<mml:math id="mml-inline43-1094342012452893">
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 6:</td>
<td> <bold>for each</bold> <inline-formula id="inline-formula44-1094342012452893">
<mml:math id="mml-inline44-1094342012452893">
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>V</mml:mi>
</mml:math>
</inline-formula> <bold>do</bold>
</td>
<td>▷<bold>Phase 1</bold>
</td>
</tr>
<tr>
<td> 7:</td>
<td>  P<sc>rocess</sc> V<sc>ertex</sc>
<inline-formula id="inline-formula45-1094342012452893">
<mml:math id="mml-inline45-1094342012452893">
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 8:</td>
<td> <bold>while</bold> <inline-formula id="inline-formula46-1094342012452893">
<mml:math id="mml-inline46-1094342012452893">
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">≠</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> <bold>do</bold>
</td>
<td>▷ <bold>Phase 2</bold>
</td>
</tr>
<tr>
<td> 9:</td>
<td>  <italic>u</italic> ← <sc>front</sc>(<italic>Q</italic>)</td>
<td>
</td>
</tr>
<tr>
<td>10:</td>
<td>  <inline-formula id="inline-formula47-1094342012452893">
<mml:math id="mml-inline47-1094342012452893">
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">←</mml:mo>
<mml:mi>Q</mml:mi>
<mml:mi mathvariant="normal">∖</mml:mi>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td>11:</td>
<td>  <bold>for each</bold> <inline-formula id="inline-formula48-1094342012452893">
<mml:math id="mml-inline48-1094342012452893">
<mml:mi>v</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>a</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">∖</mml:mo>
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>e</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> do</td>
<td>
</td>
</tr>
<tr>
<td>12:</td>
<td>   <bold>if</bold> candidate[<inline-formula id="inline-formula49-1094342012452893">
<mml:math id="mml-inline49-1094342012452893">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>] = <inline-formula id="inline-formula50-1094342012452893">
<mml:math id="mml-inline50-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula> <bold>then</bold>
</td>
<td>▷Process <inline-formula id="inline-formula51-1094342012452893">
<mml:math id="mml-inline51-1094342012452893">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula> only if <inline-formula id="inline-formula52-1094342012452893">
<mml:math id="mml-inline52-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula> is its candidate mate</td>
</tr>
<tr>
<td>13:</td>
<td>    P<sc>rocess</sc>V<sc>ertex</sc>
<inline-formula id="inline-formula53-1094342012452893">
<mml:math id="mml-inline53-1094342012452893">
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>v</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
</tbody>
</table>
</table-wrap>
</p>
<p>In Phase 1, each vertex <inline-formula id="inline-formula54-1094342012452893">
<mml:math id="mml-inline54-1094342012452893">
<mml:mi>v</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>V</mml:mi>
</mml:math>
</inline-formula> is processed by making a call to Procedure P<sc>rocess</sc>V<sc>ertex</sc>, which is given in Algorithm 2. In Procedure P<sc>rocess</sc>V<sc>ertex</sc>, for a given vertex <inline-formula id="inline-formula55-1094342012452893">
<mml:math id="mml-inline55-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>, all of its neighbors are scanned to find the current-heaviest neighbor that has not already been matched. When duplicate weights exist, it is important to break ties consistently to prevent deadlocks. We use vertex indices, which are guaranteed to be unique, to break ties consistently. The identity of the heaviest neighbor for each vertex is stored in vector candidate (line <inline-formula id="inline-formula56-1094342012452893">
<mml:math id="mml-inline56-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> in Algorithm 2). After setting the candidate mate or vertex <inline-formula id="inline-formula57-1094342012452893">
<mml:math id="mml-inline57-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>, say to vertex <inline-formula id="inline-formula58-1094342012452893">
<mml:math id="mml-inline58-1094342012452893">
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula>, we check whether the candidate mate for <inline-formula id="inline-formula59-1094342012452893">
<mml:math id="mml-inline59-1094342012452893">
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> is also set to <inline-formula id="inline-formula60-1094342012452893">
<mml:math id="mml-inline60-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>: candidate[candidate[<inline-formula id="inline-formula61-1094342012452893">
<mml:math id="mml-inline61-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>]] = <inline-formula id="inline-formula62-1094342012452893">
<mml:math id="mml-inline62-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>. If true, we have found a <italic>locally dominant</italic> edge in <inline-formula id="inline-formula63-1094342012452893">
<mml:math id="mml-inline63-1094342012452893">
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>. We add this edge to <inline-formula id="inline-formula64-1094342012452893">
<mml:math id="mml-inline64-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> (Lines <inline-formula id="inline-formula65-1094342012452893">
<mml:math id="mml-inline65-1094342012452893">
<mml:mn>10</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula66-1094342012452893">
<mml:math id="mml-inline66-1094342012452893">
<mml:mn>11</mml:mn>
</mml:math>
</inline-formula>), and the two vertices <inline-formula id="inline-formula67-1094342012452893">
<mml:math id="mml-inline67-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula68-1094342012452893">
<mml:math id="mml-inline68-1094342012452893">
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> to the queue (line <inline-formula id="inline-formula69-1094342012452893">
<mml:math id="mml-inline69-1094342012452893">
<mml:mn>12</mml:mn>
</mml:math>
</inline-formula>). Some of the vertices might end up not having any candidates available to match. <table-wrap id="table5-1094342012452893" position="float">
<label>Algorithm 2</label>
<caption>
<p>ProcessVertex</p>
</caption>
<graphic alternate-form-of="table5-1094342012452893" xlink:href="10.1177_1094342012452893-table5.tif"/>
<table>
<tbody>
<tr>
<td> 1:</td>
<td>
<bold>procedure</bold> P<sc>rocess</sc>V<sc>ertex</sc>
<inline-formula id="inline-formula70-1094342012452893">
<mml:math id="mml-inline70-1094342012452893">
<mml:mtext> </mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 2:</td>
<td> <inline-formula id="inline-formula71-1094342012452893">
<mml:math id="mml-inline71-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula72-1094342012452893">
<mml:math id="mml-inline72-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="normal">∞</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 3:</td>
<td> <inline-formula id="inline-formula73-1094342012452893">
<mml:math id="mml-inline73-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>d</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula74-1094342012452893">
<mml:math id="mml-inline74-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 4:</td>
<td> <bold>for</bold> <bold>each</bold> <inline-formula id="inline-formula75-1094342012452893">
<mml:math id="mml-inline75-1094342012452893">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>a</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> <bold>do</bold>
</td>
<td>
</td>
</tr>
<tr>
<td> 5:</td>
<td>
<bold>if</bold> (mate[<inline-formula id="inline-formula76-1094342012452893">
<mml:math id="mml-inline76-1094342012452893">
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula>] = <inline-formula id="inline-formula77-1094342012452893">
<mml:math id="mml-inline77-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>) <bold>AND</bold> (<inline-formula id="inline-formula78-1094342012452893">
<mml:math id="mml-inline78-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula79-1094342012452893">
<mml:math id="mml-inline79-1094342012452893">
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>) <bold>then</bold>
</td>
<td>▷ Use vertex i.d. to break ties</td>
</tr>
<tr>
<td> 6:</td>
<td>   <inline-formula id="inline-formula80-1094342012452893">
<mml:math id="mml-inline80-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula81-1094342012452893">
<mml:math id="mml-inline81-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 7:</td>
<td>   <inline-formula id="inline-formula82-1094342012452893">
<mml:math id="mml-inline82-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>d</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula83-1094342012452893">
<mml:math id="mml-inline83-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 8:</td>
<td> candidate[<inline-formula id="inline-formula84-1094342012452893">
<mml:math id="mml-inline84-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula85-1094342012452893">
<mml:math id="mml-inline85-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
</mml:math>
</inline-formula> <inline-formula id="inline-formula86-1094342012452893">
<mml:math id="mml-inline86-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>d</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 9:</td>
<td>
<bold>if</bold> candidate[candidate[<inline-formula id="inline-formula87-1094342012452893">
<mml:math id="mml-inline87-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>]] = <inline-formula id="inline-formula88-1094342012452893">
<mml:math id="mml-inline88-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> <bold>then</bold>
</td>
<td>▷ Found a locally dominant edge</td>
</tr>
<tr>
<td>10:</td>
<td>  mate[<inline-formula id="inline-formula89-1094342012452893">
<mml:math id="mml-inline89-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula90-1094342012452893">
<mml:math id="mml-inline90-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
</mml:math>
</inline-formula> candidate[<inline-formula id="inline-formula91-1094342012452893">
<mml:math id="mml-inline91-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>]</td>
<td>
</td>
</tr>
<tr>
<td>11:</td>
<td>  mate[candidate[<inline-formula id="inline-formula92-1094342012452893">
<mml:math id="mml-inline92-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>]] <inline-formula id="inline-formula93-1094342012452893">
<mml:math id="mml-inline93-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
</mml:math>
</inline-formula> <inline-formula id="inline-formula94-1094342012452893">
<mml:math id="mml-inline94-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td>12:</td>
<td>  <inline-formula id="inline-formula95-1094342012452893">
<mml:math id="mml-inline95-1094342012452893">
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">←</mml:mo>
<mml:mi>Q</mml:mi>
<mml:mo stretchy="false">∪</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
</inline-formula>candidate[<inline-formula id="inline-formula96-1094342012452893">
<mml:math id="mml-inline96-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>]<inline-formula id="inline-formula97-1094342012452893">
<mml:math id="mml-inline97-1094342012452893">
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>▷Only need to process vertices in the queue.</td>
</tr>
</tbody>
</table>
</table-wrap>
</p>
<p>The execution of Phase 2 begins when every vertex has been processed via a call to Procedure P<sc>rocess</sc>V<sc>ertex</sc>, which happens at the completion of Phase 1. In Phase 2, we iterate until the queue becomes empty (line <inline-formula id="inline-formula98-1094342012452893">
<mml:math id="mml-inline98-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> in Algorithm 1). Note that at least one edge (the heaviest edge in <inline-formula id="inline-formula99-1094342012452893">
<mml:math id="mml-inline99-1094342012452893">
<mml:mi>G</mml:mi>
</mml:math>
</inline-formula>) will get matched in Phase 1 and, therefore, <inline-formula id="inline-formula100-1094342012452893">
<mml:math id="mml-inline100-1094342012452893">
<mml:mi>Q</mml:mi>
</mml:math>
</inline-formula> is non-empty if <inline-formula id="inline-formula101-1094342012452893">
<mml:math id="mml-inline101-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> is non-empty. During each iteration of the <bold>while</bold> loop on line <inline-formula id="inline-formula102-1094342012452893">
<mml:math id="mml-inline102-1094342012452893">
<mml:mn>10</mml:mn>
</mml:math>
</inline-formula>, we process vertices matched in previous iterations while adding new vertices to the queue that become eligible as edges get matched. Note that we only need to process vertices for which the candidate was set to one of the matched vertices (line <inline-formula id="inline-formula103-1094342012452893">
<mml:math id="mml-inline103-1094342012452893">
<mml:mn>12</mml:mn>
</mml:math>
</inline-formula> in Algorithm 1). This is achieved by adding the newly matched vertices to the queue and checking whether any of their unmatched neighbors point to them. If so, those neighbors will have to find new candidates for matching. The algorithm will terminate when the queue becomes empty. The matching is stored in vector mate. An index <inline-formula id="inline-formula104-1094342012452893">
<mml:math id="mml-inline104-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> of this vector represents vertex <inline-formula id="inline-formula105-1094342012452893">
<mml:math id="mml-inline105-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>, and the vertex <inline-formula id="inline-formula106-1094342012452893">
<mml:math id="mml-inline106-1094342012452893">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">m</mml:mi>
<mml:mi mathvariant="normal">a</mml:mi>
<mml:mi mathvariant="normal">t</mml:mi>
<mml:mi mathvariant="normal">e</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">[</mml:mo>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula> represents the other endpoint of the matched edge <inline-formula id="inline-formula107-1094342012452893">
<mml:math id="mml-inline107-1094342012452893">
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>t</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula>.</p>
<p>An indication of the amount of work completed up to a certain stage in the execution of Algorithm 2, and the amount of work at a given step of execution, is given by the size of queue (<inline-formula id="inline-formula108-1094342012452893">
<mml:math id="mml-inline108-1094342012452893">
<mml:mi>Q</mml:mi>
</mml:math>
</inline-formula>) with respect to time. At the end of Phase 1, we expect several vertices (at least two) to be added to the queue. In practice, we observe a large percentage of edges being matched in Phase 1. Let us consider the end of Phase 1 as the first iteration. The iterations of the while loop on line <inline-formula id="inline-formula109-1094342012452893">
<mml:math id="mml-inline109-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> in Algorithm 1 will be considered as the subsequent iterations with the following modification. During each iteration of the while loop, all of the elements of <inline-formula id="inline-formula110-1094342012452893">
<mml:math id="mml-inline110-1094342012452893">
<mml:mi>Q</mml:mi>
</mml:math>
</inline-formula> are processed. Let newly matched vertices be added to a temporary queue. At the end of the iteration, elements of the temporary queue are moved to <inline-formula id="inline-formula111-1094342012452893">
<mml:math id="mml-inline111-1094342012452893">
<mml:mi>Q</mml:mi>
</mml:math>
</inline-formula>. The size of <inline-formula id="inline-formula112-1094342012452893">
<mml:math id="mml-inline112-1094342012452893">
<mml:mi>Q</mml:mi>
</mml:math>
</inline-formula> provides an indication of the number of edges that get matched at each iteration (or a step of the algorithm). The queue sizes for three different inputs are illustrated in <xref ref-type="fig" rid="fig1-1094342012452893">Figure 1</xref>(c). We observe that the queue size decreases roughly by half after each iteration, and has the largest size at for iteration <inline-formula id="inline-formula113-1094342012452893">
<mml:math id="mml-inline113-1094342012452893">
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula> (at the end of Phase 1).</p>
<fig id="fig1-1094342012452893" position="float">
<label>Figure 1.</label>
<caption>
<p>Characteristics of data(<inline-formula id="inline-formula114-1094342012452893">
<mml:math id="mml-inline114-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula>). (a) Degree distribution of the three RMAT graphs. The last datapoint for RMAT-B with degree = 38,143 and frequency = 1 is not shown. The plot is on a log--log scale. (b) Distribution of local clustering coefficients of the three RMAT graphs. (c) Queue sizes during the execution of the algorithm. Iterations are as defined in Section 3.1. The black dashed trend line (exponential) indicates that the amount of work roughly decreases by half after each iteration.</p>
</caption>
<graphic xlink:href="10.1177_1094342012452893-fig1.tif"/>
</fig>
<fig id="fig2-1094342012452893" position="float">
<label>Figure 2.</label>
<caption>
<p>Scaling on Magny-Cours: Performance of queue-based implementation on AMD Magny-Cours with up to 48 cores. The threads are pinned to the cores such that the total memory bandwidth is maximized. The black dashed lines represent linear scaling.</p>
</caption>
<graphic xlink:href="10.1177_1094342012452893-fig2.tif"/>
</fig>
<p>The cost of Algorithm 1 is given by <inline-formula id="inline-formula116-1094342012452893">
<mml:math id="mml-inline116-1094342012452893">
<mml:mi>O</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula117-1094342012452893">
<mml:math id="mml-inline117-1094342012452893">
<mml:mi mathvariant="normal">Δ</mml:mi>
</mml:math>
</inline-formula> is the maximum degree in <inline-formula id="inline-formula118-1094342012452893">
<mml:math id="mml-inline118-1094342012452893">
<mml:mi>G</mml:mi>
</mml:math>
</inline-formula>. The worst case happens when a vertex points to all of its neighbors unsuccessfully, and in order to determine the current heaviest neighbor needs to check its entire list of neighbors. However, the runtime can be improved to <inline-formula id="inline-formula119-1094342012452893">
<mml:math id="mml-inline119-1094342012452893">
<mml:mi mathvariant="normal">Θ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> if the adjacency list for each vertex is maintained in a non-increasing order of edge weights. Under such an assumption, the current heaviest neighbor of a vertex can be computed in constant time. In practice, we observe that the benefits from ordered adjacency lists is about <inline-formula id="inline-formula120-1094342012452893">
<mml:math id="mml-inline120-1094342012452893">
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula> relative to unordered lists (<xref ref-type="fig" rid="fig5-1094342012452893">Figure 5</xref>). However, the cost of sorting adjacency lists is considerably more than matching. Therefore, sorting is not a viable optimization strategy. In this paper, we use sorted lists only to demonstrate its effectiveness and to simplify the explanation of the dataflow algorithm (Algorithm 4) presented in Section 4.2.</p>
<p>The performance of approximation algorithms can be measured based on the quality and runtime of the algorithms. Quality can be measured in terms of the cardinality and weight of approximate matchings with respect to optimal matchings. Approximation algorithms have been demonstrated to compute high-quality matchings in a fraction of the time taken by optimal algorithms (<xref ref-type="bibr" rid="bibr14-1094342012452893">Halappanavar, 2009</xref>). When compared with other approximation algorithms, the locally dominant algorithm performs better in both quality and runtime. Experimental results comparing different approximation algorithms are provided by <xref ref-type="bibr" rid="bibr14-1094342012452893">Halappanavar (2009</xref>). The details of parallel implementations of the locally-dominant algorithm are provided in Section 4, and experimental results are provided in Section 5.</p>
</sec>
<sec id="section5-1094342012452893">
<title>3.2 Hardware platforms</title>
<p>The five platforms selected in this study, Opteron, Nehalem, Tesla, Fermi, and XMT, represent a broad spectrum of capabilities: clock speeds ranging from 0.5 GHz (XMT) to about 3.0 GHz (Nehalem); hardware multithreading ranging from none (Opteron) to 128 threads per processor (XMT); cache hierarchies ranging from none/flat (XMT) to three levels (Nehalem); generation of memory interconnect ranging from DDR1 (XMT) to GDDR5 (Fermi); advanced architectural features such as branch prediction and speculative execution on modern architectures to simple features of the XMT; and control structures ranging from fully autonomous (multiple instruction multiple data [MIMD]) processors (Opteron) to a 32-way single instruction multiple thread (SIMT) (Fermi). Different platforms employ different techniques for performance. For example, while the XMT uses massive multithreading to tolerate latency from memory operations, Nehalem employs deep cache hierarchies, advanced branch prediction, and two-way multithreading to tolerate latencies. While some architectures are extremes, others are more balanced. For example, compare the Nehalem with two-way hyperthreading and three levels of caches with the XMT that has 128-way threading and a cache-less memory structure. These contrasting hardware features offer a rich environment to compare performance impact of different features with respect to each other. Key architectural features of these five platforms are summarized in <xref ref-type="table" rid="table1-1094342012452893">Table 1</xref>. Considering the scope and length of this paper we provide brief descriptions as applicable to irregular applications in general and graph algorithms in particular. References that provide further details are also provided in the table.</p>
<table-wrap id="table1-1094342012452893" position="float">
<label>Table 1.</label>
<caption>
<p>Key architectural features: a summary of key features of the platforms used in this paper.</p>
</caption>
<graphic alternate-form-of="table1-1094342012452893" xlink:href="10.1177_1094342012452893-table1.tif"/>
<table>
<thead>
<tr>
<th>Platform:</th>
<th>Opteron 6176 SE (Magny-Cours)</th>
<th>Xeon E5540 (Nehalem)</th>
<th>Tesla C1060 (Tesla)</th>
<th>Tesla C2050 (Fermi)</th>
<th>ThreadStorm-2 (XMT)</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Processing units</td>
</tr>
<tr>
<td> Clock (GHz) </td>
<td>2.30</td>
<td>2.53 (base)</td>
<td>1.3</td>
<td>1.15</td>
<td>0.5</td>
</tr>
<tr>
<td> Sockets </td>
<td>4</td>
<td>2</td>
<td>30 SMs</td>
<td>14 SMs</td>
<td>128</td>
</tr>
<tr>
<td> Cores/socket </td>
<td>12</td>
<td>4</td>
<td>8 SPs/SM</td>
<td>32 SPs/SM</td>
<td>1</td>
</tr>
<tr>
<td> Threads/core </td>
<td>1</td>
<td>2</td>
<td align="center">–</td>
<td align="center">–</td>
<td>128</td>
</tr>
<tr>
<td> Total threads </td>
<td>48</td>
<td>16</td>
<td>240 SPs</td>
<td>448 SPs</td>
<td>16,384</td>
</tr>
<tr>
<td> Interconnect </td>
<td>HyperTransport-3</td>
<td>QPI</td>
<td>PCIe2</td>
<td>PCIe2</td>
<td>Seastar2</td>
</tr>
<tr>
<td> Interconnect (GB/s) </td>
<td>25.6</td>
<td>25.6</td>
<td>8</td>
<td>8</td>
<td>4.8</td>
</tr>
<tr>
<td colspan="6">Memory system</td>
</tr>
<tr>
<td> Cache structure </td>
<td>L1/L2/L3<inline-formula id="inline-formula121-1094342012452893">
<mml:math id="mml-inline121-1094342012452893">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mo>†</mml:mo>
</mml:msup>
</mml:math>
</inline-formula>
</td>
<td>L1/L2/L3<inline-formula id="inline-formula122-1094342012452893">
<mml:math id="mml-inline122-1094342012452893">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mo>†</mml:mo>
</mml:msup>
</mml:math>
</inline-formula>
</td>
<td align="center">–</td>
<td>L1/L2<inline-formula id="inline-formula123-1094342012452893">
<mml:math id="mml-inline123-1094342012452893">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mo>†</mml:mo>
</mml:msup>
</mml:math>
</inline-formula>
</td>
<td>cache-less</td>
</tr>
<tr>
<td> L1 (KB)/core: Inst/Data </td>
<td>64/64</td>
<td>32/32</td>
<td align="center">–</td>
<td>48/SM<inline-formula id="inline-formula124-1094342012452893">
<mml:math id="mml-inline124-1094342012452893">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
</mml:math>
</inline-formula>
</td>
<td align="center">–</td>
</tr>
<tr>
<td> L2 (KB)/core </td>
<td>512</td>
<td>256</td>
<td align="center">–</td>
<td>768/SM</td>
<td align="center">–</td>
</tr>
<tr>
<td> L3 (MB)/socket </td>
<td>12</td>
<td>8</td>
<td align="center">–</td>
<td align="center">–</td>
<td align="center">–</td>
</tr>
<tr>
<td> Memory/socket (GB) </td>
<td>64</td>
<td>12</td>
<td>4</td>
<td>3</td>
<td>8</td>
</tr>
<tr>
<td> Total memory (GB) </td>
<td>256</td>
<td>24</td>
<td>3</td>
<td>3</td>
<td>1,024</td>
</tr>
<tr>
<td> Peak bandwidth (GB/s) </td>
<td>42.7 (DDR3)</td>
<td>25.6 (DDR3)</td>
<td>102 (GDDR3)</td>
<td>144 (GDDR5)</td>
<td>86.4 (DDR1)</td>
</tr>
<tr>
<td colspan="6">Software</td>
</tr>
<tr>
<td> C Compiler </td>
<td>GCC 4.1.2</td>
<td>Intel 11.1</td>
<td>NVCC (CUDA 4.0)</td>
<td>NVCC (CUDA 4.0)</td>
<td>Cray C 6.5.0</td>
</tr>
<tr>
<td> Flags </td>
<td>-O3</td>
<td>-fast</td>
<td>-O3</td>
<td>-O3</td>
<td>-par</td>
</tr>
<tr>
<td> Thread scheduling </td>
<td>static</td>
<td>static</td>
<td>dynamic</td>
<td>dynamic</td>
<td>block-dynamic</td>
</tr>
<tr>
<td>Reference </td>
<td>A</td>
<td>B</td>
<td>C</td>
<td>D</td>
<td>E</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1094342012452893">
<p>
<inline-formula id="inline-formula125-1094342012452893">
<mml:math id="mml-inline125-1094342012452893">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mo>†</mml:mo>
</mml:msup>
</mml:math>
</inline-formula>Shared cache.</p>
</fn>
</table-wrap-foot>
<table-wrap-foot>
<fn id="table-fn2-1094342012452893">
<p>
<inline-formula id="inline-formula126-1094342012452893">
<mml:math id="mml-inline126-1094342012452893">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula> kB of memory is configured as <inline-formula id="inline-formula127-1094342012452893">
<mml:math id="mml-inline127-1094342012452893">
<mml:mn>48</mml:mn>
</mml:math>
</inline-formula> kB of L1 cache and <inline-formula id="inline-formula128-1094342012452893">
<mml:math id="mml-inline128-1094342012452893">
<mml:mn>16</mml:mn>
</mml:math>
</inline-formula> kB as shared memory.</p>
</fn>
<fn id="table-fn3-1094342012452893">
<p>A: see http://www.amd.com/us/products/embedded/processors/opteron/Pages/opteron-6100-series.aspx</p>
</fn>
<fn id="table-fn4-1094342012452893">
<p>B: see <xref ref-type="bibr" rid="bibr26-1094342012452893">Molka et al. (2009</xref>)</p>
</fn>
<fn id="table-fn5-1094342012452893">
<p>C: see http://www.nvidia.com/docs/IO/43395/BD-04111-001_v06.pdf</p>
</fn>
<fn id="table-fn6-1094342012452893">
<p>D: see http://www.nvidia.com/docs/IO/43395/Tesla_C2050_Board_Specification.pdf</p>
<p>E: see <xref ref-type="bibr" rid="bibr13-1094342012452893">Feo et al. (2005</xref>)</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The AMD Opteron platform (Magny-Cours) is a 4-socket 12-core system with <inline-formula id="inline-formula129-1094342012452893">
<mml:math id="mml-inline129-1094342012452893">
<mml:mn>256</mml:mn>
</mml:math>
</inline-formula> GB of system memory. Each 12-core socket consists of two 6-core dies on a single package (multichip module) with separate memory controllers. The sockets are interconnected using HyperTransport-3 technology. Given the large size and non-uniform memory access (NUMA) costs it is important to consider allocation of memory and how software threads get pinned to physical cores.</p>
<p>The Intel Xeon platform (Nehalem) is a 2-socket 4-core system with <inline-formula id="inline-formula130-1094342012452893">
<mml:math id="mml-inline130-1094342012452893">
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula> GB of system memory. Each core supports simultaneous multithreading (SMT), allowing two threads to share the instruction pipeline. This feature is commercially known as hyperthreading. In order to assess the benefits of hyperthreading, we pin two threads to the same core using the capabilities provided by the Intel compiler. In addition to hyperthreading, Nehalem provides several advanced architectural features such as a new cache coherency protocol (MESIF), out-of-order execution, sophisticated branch prediction, and high-bandwidth interconnect (QPI). Accordingly, we observe superior single-thread (serial) performance on this platform.</p>
<p>Tesla is the codename for the GPU used in Tesla C1060 boards targeted for general computational purposes. The basic unit of computation on Tesla is a streaming multiprocessor (SM) that consists of: an instruction unit, eight streaming processors (SPs), two special function units (SFUs), a double precision unit and a scratchpad memory (local memory) of <inline-formula id="inline-formula131-1094342012452893">
<mml:math id="mml-inline131-1094342012452893">
<mml:mn>16</mml:mn>
</mml:math>
</inline-formula> kB. SPs are the arithmetic-logic units (ALUs) that perform basic single precision and integer computations. The instruction unit fetches an instruction for a group of <inline-formula id="inline-formula132-1094342012452893">
<mml:math id="mml-inline132-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula> threads, called a <italic>warp</italic>, which is then executed in a SIMT fashion for multiple clock cycles using various resources of an SM. An instruction executes for a minimum of four clock cycles when simple single-precision operations are used. Each SM maintains up to <inline-formula id="inline-formula133-1094342012452893">
<mml:math id="mml-inline133-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula> warps (<inline-formula id="inline-formula134-1094342012452893">
<mml:math id="mml-inline134-1094342012452893">
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>024</mml:mn>
</mml:math>
</inline-formula> threads) that are simultaneously active. Multiple SMs are integrated in a single GPU and connected through a set of <inline-formula id="inline-formula135-1094342012452893">
<mml:math id="mml-inline135-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula>-bit wide GDDR3 memory controllers, to an external random access memory (named global memory) located on the board. In Tesla C1060, <inline-formula id="inline-formula136-1094342012452893">
<mml:math id="mml-inline136-1094342012452893">
<mml:mn>30</mml:mn>
</mml:math>
</inline-formula> SMs (for a total of <inline-formula id="inline-formula137-1094342012452893">
<mml:math id="mml-inline137-1094342012452893">
<mml:mn>240</mml:mn>
</mml:math>
</inline-formula> SPs) are interconnected through <inline-formula id="inline-formula138-1094342012452893">
<mml:math id="mml-inline138-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> memory controllers to <inline-formula id="inline-formula139-1094342012452893">
<mml:math id="mml-inline139-1094342012452893">
<mml:mn>4</mml:mn>
</mml:math>
</inline-formula> GB of global memory at a speed of <inline-formula id="inline-formula140-1094342012452893">
<mml:math id="mml-inline140-1094342012452893">
<mml:mn>800</mml:mn>
</mml:math>
</inline-formula> MHz. Thus, obtaining an overall memory bus width of <inline-formula id="inline-formula141-1094342012452893">
<mml:math id="mml-inline141-1094342012452893">
<mml:mn>512</mml:mn>
</mml:math>
</inline-formula> bits and reaching a peak bandwidth of <inline-formula id="inline-formula142-1094342012452893">
<mml:math id="mml-inline142-1094342012452893">
<mml:mn>102</mml:mn>
</mml:math>
</inline-formula> GB/s. When a warp issues a long latency operation to the global memory, another active warp is switched on the execution resources of the SM to cover this latency.</p>
<p>A GPU is programmed as an accelerator with its own memory pool, so the data needs to be moved to its global memory before and after the computation. This procedure is performed through a PCI-Express 2.0 bus that allows a peak bandwidth of <inline-formula id="inline-formula143-1094342012452893">
<mml:math id="mml-inline143-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> GB/s in both the directions. This bus may present a significant bottleneck for the overall performance of a GPU-accelerated algorithm. For obtaining high performance on Tesla, it is important to coalesce memory accesses (i.e., performing multiple operations with a single transaction). In Tesla, the maximum size of a transaction is <inline-formula id="inline-formula144-1094342012452893">
<mml:math id="mml-inline144-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula> bytes. To coalesce memory operations, memory accesses by threads in a half-warp (memory operations are issued for a group of <inline-formula id="inline-formula145-1094342012452893">
<mml:math id="mml-inline145-1094342012452893">
<mml:mn>16</mml:mn>
</mml:math>
</inline-formula> threads) should be sequential and aligned. If these rules are not respected, multiple memory transactions get issued that reduce the effective bandwidth available. Thus, irregular applications are challenging to implement on Tesla.</p>
<p>Fermi is the latest generation of NVIDIA GPUs. In comparison to the previous generations of GPUs, Fermi retains general architectural principles and programming model. However, there are several significant changes compared to Tesla. The principal differences are in the organization of SMs and memory hierarchy. The SMs in Fermi are composed of <inline-formula id="inline-formula146-1094342012452893">
<mml:math id="mml-inline146-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula> SPs, <inline-formula id="inline-formula147-1094342012452893">
<mml:math id="mml-inline147-1094342012452893">
<mml:mn>4</mml:mn>
</mml:math>
</inline-formula> SFUs and <inline-formula id="inline-formula148-1094342012452893">
<mml:math id="mml-inline148-1094342012452893">
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula> instruction units. The instruction units fetch instructions from two different warps (still composed of <inline-formula id="inline-formula149-1094342012452893">
<mml:math id="mml-inline149-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula> threads) which are then simultaneously issued on a group of <inline-formula id="inline-formula150-1094342012452893">
<mml:math id="mml-inline150-1094342012452893">
<mml:mn>16</mml:mn>
</mml:math>
</inline-formula> SPs for a minimum of <inline-formula id="inline-formula151-1094342012452893">
<mml:math id="mml-inline151-1094342012452893">
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula> clock cycles. However, when double-precision operations are issued, only one warp can be active at a given point of time. The number of warps-in-flight for each SM has been increased to <inline-formula id="inline-formula152-1094342012452893">
<mml:math id="mml-inline152-1094342012452893">
<mml:mn>48</mml:mn>
</mml:math>
</inline-formula>. Each SM includes <inline-formula id="inline-formula153-1094342012452893">
<mml:math id="mml-inline153-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula> kB of on-chip memory which can be configured as <inline-formula id="inline-formula154-1094342012452893">
<mml:math id="mml-inline154-1094342012452893">
<mml:mn>48</mml:mn>
</mml:math>
</inline-formula> kB of shared memory (scratchpad) and <inline-formula id="inline-formula155-1094342012452893">
<mml:math id="mml-inline155-1094342012452893">
<mml:mn>16</mml:mn>
</mml:math>
</inline-formula> kB of L1 cache, or as <inline-formula id="inline-formula156-1094342012452893">
<mml:math id="mml-inline156-1094342012452893">
<mml:mn>16</mml:mn>
</mml:math>
</inline-formula> kB of shared memory and <inline-formula id="inline-formula157-1094342012452893">
<mml:math id="mml-inline157-1094342012452893">
<mml:mn>48</mml:mn>
</mml:math>
</inline-formula> kB of L1 cache. The L1 caches of different SMs are not cache coherent. The SMs in Fermi are connected to a <inline-formula id="inline-formula158-1094342012452893">
<mml:math id="mml-inline158-1094342012452893">
<mml:mn>768</mml:mn>
</mml:math>
</inline-formula> kB L2 cache. The line size for L1 and L2 caches is <inline-formula id="inline-formula159-1094342012452893">
<mml:math id="mml-inline159-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula> bytes. While the rules for memory accesses remain similar to Tesla, the size of each memory transaction has been increased to <inline-formula id="inline-formula160-1094342012452893">
<mml:math id="mml-inline160-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula> bytes to match the line size of L1 and L2 caches. Caches have the potential to improve the performance of uncoalesced accesses to memory. Subsequent accesses to unaligned data already loaded in one of the caches are serviced at the throughput of the respective caches. In addition, L2 plays an important role when atomic memory operations are issued. In this paper, we use a Tesla C2050 board equipped with a Fermi processor comprising of <inline-formula id="inline-formula161-1094342012452893">
<mml:math id="mml-inline161-1094342012452893">
<mml:mn>14</mml:mn>
</mml:math>
</inline-formula> SMs (for a total of <inline-formula id="inline-formula162-1094342012452893">
<mml:math id="mml-inline162-1094342012452893">
<mml:mn>448</mml:mn>
</mml:math>
</inline-formula> SPs). The <inline-formula id="inline-formula163-1094342012452893">
<mml:math id="mml-inline163-1094342012452893">
<mml:mn>14</mml:mn>
</mml:math>
</inline-formula> SMs are connected to six <inline-formula id="inline-formula164-1094342012452893">
<mml:math id="mml-inline164-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula>-bit memory controllers for a total bus-width of <inline-formula id="inline-formula165-1094342012452893">
<mml:math id="mml-inline165-1094342012452893">
<mml:mn>384</mml:mn>
</mml:math>
</inline-formula> bits. The C2050 hosts <inline-formula id="inline-formula166-1094342012452893">
<mml:math id="mml-inline166-1094342012452893">
<mml:mn>3</mml:mn>
</mml:math>
</inline-formula> GB of GDDR5 memory at <inline-formula id="inline-formula167-1094342012452893">
<mml:math id="mml-inline167-1094342012452893">
<mml:mn>1.5</mml:mn>
</mml:math>
</inline-formula> GHz, which allows a peak bandwidth of <inline-formula id="inline-formula168-1094342012452893">
<mml:math id="mml-inline168-1094342012452893">
<mml:mn>144</mml:mn>
</mml:math>
</inline-formula> GB/s. While Fermi supports error correcting code (ECC) for all levels of the memory hierarchy, it also provides the ability to disable ECC through a software switch. The software infrastructure on Fermi allows setting the sizes of L1 caches and local memories. It also allows the user to bypass L1 caches altogether. However, L2 caches cannot be bypassed since all memory transactions pass through them. We show performance differences arising from disabling ECC and L1 caches in <xref ref-type="fig" rid="fig4-1094342012452893">Figure 4</xref>. In summary, Fermi is a superior alternative to Tesla for irregular algorithms such as the matching algorithm. Accordingly, we observe considerable improvements in performance.</p>
<fig id="fig3-1094342012452893" position="float">
<label>Figure 3.</label>
<caption>
<p>Scaling on Nehalem: Performance of queue-based implementation on Nehalem for the three variants of RMAT graphs (<inline-formula id="inline-formula169-1094342012452893">
<mml:math id="mml-inline169-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula>). The two threads per core lines indicate the use of hyperthreads. The threads are pinned to cores such that the total memory bandwidth is maximized. The black dashed lines represent linear scaling.</p>
</caption>
<graphic xlink:href="10.1177_1094342012452893-fig3.tif"/>
</fig>
<fig id="fig4-1094342012452893" position="float">
<label>Figure 4.</label>
<caption>
<p>Scaling on GPUs: (a) Relative performance of GPUs on synthetic graphs(<inline-formula id="inline-formula269-1094342012452893">
<mml:math id="mml-inline269-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>23</mml:mn>
</mml:math>
</inline-formula>). (b) performance on real-world instances. The runtimes are scaled with the performance of one core (thread) of the host machine (Intel Nehalem). The bars represent performance on 12 cores (CPU(12 T)), 24 cores (CPU(24 T)), Tesla, and Fermi respectively. The first two bars for Fermi represent results with ECC turned off, followed by two bars with ECC turned on. The two bars with a suffix L2 represent runtimes where caching on L1 is disabled.</p>
</caption>
<graphic xlink:href="10.1177_1094342012452893-fig4.tif"/>
</fig>
<p>The Cray XMT platform used in this paper consists of <inline-formula id="inline-formula170-1094342012452893">
<mml:math id="mml-inline170-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula> Threadstorm (MTA2) processors interconnected using a high-bandwidth 3D torus network (Cray SeaStar2). Each MTA2 processor consists of <inline-formula id="inline-formula171-1094342012452893">
<mml:math id="mml-inline171-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula> thread streams (hardware threads) and a very long instruction pipeline (VLIW) with <inline-formula id="inline-formula172-1094342012452893">
<mml:math id="mml-inline172-1094342012452893">
<mml:mn>21</mml:mn>
</mml:math>
</inline-formula> stages. In contrast to SMT on the Nehalem, MTA2 uses interleaved scheduling of threads: at each cycle, an instruction is chosen from a different thread that is ready. MTA2 uses massive multithreading as a means to tolerate latency from memory operations as well as latencies generated from synchronization of threads. In contrast to a multilevel cache hierarchy on traditional platforms, the memory system on XMT is a flat cacheless system. The virtual global address space is built from physically distributed memory modules of <inline-formula id="inline-formula173-1094342012452893">
<mml:math id="mml-inline173-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> GB of DDR-1 memory on each processor, resulting in a total memory of <inline-formula id="inline-formula174-1094342012452893">
<mml:math id="mml-inline174-1094342012452893">
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula> TB from <inline-formula id="inline-formula175-1094342012452893">
<mml:math id="mml-inline175-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula> processors. Memory accesses are made uniform by using a hardware hashing mechanism that maps data randomly to memory modules in block sizes of <inline-formula id="inline-formula176-1094342012452893">
<mml:math id="mml-inline176-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula> bytes. Each word (<inline-formula id="inline-formula177-1094342012452893">
<mml:math id="mml-inline177-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula> bits) of memory has special tag bits that are used to provide light-weight synchronization among threads. Details of extended memory semantics using these bits are presented in Section 4.2. The average latency of a memory access is <inline-formula id="inline-formula178-1094342012452893">
<mml:math id="mml-inline178-1094342012452893">
<mml:mn>600</mml:mn>
</mml:math>
</inline-formula> cycles with a worst-case latency of <inline-formula id="inline-formula179-1094342012452893">
<mml:math id="mml-inline179-1094342012452893">
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>000</mml:mn>
</mml:math>
</inline-formula> cycles. A <inline-formula id="inline-formula180-1094342012452893">
<mml:math id="mml-inline180-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula>-processor system has a sustained bandwidth of <inline-formula id="inline-formula181-1094342012452893">
<mml:math id="mml-inline181-1094342012452893">
<mml:mn>86.4</mml:mn>
</mml:math>
</inline-formula> GB/s. Unlike Nehalem, XMT lacks advanced architectural features such as branch prediction. It relies on multithreading to tolerate latencies from different sources including branches. A slow clock speed coupled with a long instruction pipeline results in a considerably slow serial performance. However, relatively better performance of XMT for irregular problems demonstrates the effectiveness of multithreading and large memory bandwidth over faster clock speeds and deeper cache hierarchies.</p>
</sec>
<sec id="section6-1094342012452893">
<title>3.3 Datasets</title>
<p>Our dataset comprises three instances of synthetic graphs and three instances of real-world data. Synthetic graphs are generated using the recursive matrix multiplication (R-MAT) algorithm proposed by <xref ref-type="bibr" rid="bibr9-1094342012452893">Chakrabarti and Faloutsos (2006</xref>). Different classes of graphs can be generated by varying the four input parameters for probability in the R-MAT algorithm. While a large number of combinations are possible, we find that the following three input parameters cover a broad range of characteristics: (i) <bold>RMAT-ER:</bold> (0.25, 0.25, 0.25, 0.25), (ii) <bold>RMAT-G:</bold> (0.45, 0.15, 0.15, 0.25), and (iii) <bold>RMAT-B:</bold> (0.55, 0.15, 0.15, 0.15). RMAT-ER stands for a class of Erdős--Rényi random graphs. The other two classes of graphs (RMAT-G and RMAT-B) have properties similar to real-world graphs with skewed normal distributions for vertex degrees and small-world phenomenon of short average distance between pairs of vertices. The size of the graph generated by R-MAT algorithm is specified as a power of two, where the number of vertices is given by <inline-formula id="inline-formula185-1094342012452893">
<mml:math id="mml-inline185-1094342012452893">
<mml:msup>
<mml:mn>2</mml:mn>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula> and the number of edges is some multiple of the number of vertices (we use <inline-formula id="inline-formula186-1094342012452893">
<mml:math id="mml-inline186-1094342012452893">
<mml:mn>8</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> for our experiments). For the input parameter <inline-formula id="inline-formula187-1094342012452893">
<mml:math id="mml-inline187-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, we use a range of values from <inline-formula id="inline-formula188-1094342012452893">
<mml:math id="mml-inline188-1094342012452893">
<mml:mn>23</mml:mn>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula189-1094342012452893">
<mml:math id="mml-inline189-1094342012452893">
<mml:mn>27</mml:mn>
</mml:math>
</inline-formula> on different platforms. The different sizes and key properties of the graphs used for experiments are summarized in <xref ref-type="table" rid="table2-1094342012452893">Table 2</xref>. The Table represents the numbers for graphs generated on the Cray XMT using our own implementation of the R-MAT algorithm. However, for experiments on the Magny-Cours platform, we generate the graphs in memory using SNAP (Small-world Network Analysis and Partitioning) toolkit version 0.3 (<xref ref-type="bibr" rid="bibr6-1094342012452893">Bader and Madduri, 2008</xref>). While the number of vertices remain the same, there are minor differences in the number of edges and connectivity due to differences in the pseudo-random numbers used. When cross comparisons between architectures are made, the graphs are generated on the XMT, saved to disk in plain text format, and reused on the other platforms. A positive integer weight is associated with each edge. For our experiments we assigned random weights ranging from zero to number of vertices.</p>
<table-wrap id="table2-1094342012452893" position="float">
<label>Table 2.</label>
<caption>
<p>Synthetic graphs: Details of the graphs used for experiments. Graphs with <inline-formula id="inline-formula190-1094342012452893">
<mml:math id="mml-inline190-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">≤</mml:mo>
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula> were generated on the XMT, stored in a text file, and used on all of the platforms. Larger graphs with <inline-formula id="inline-formula191-1094342012452893">
<mml:math id="mml-inline191-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">≥</mml:mo>
<mml:mn>25</mml:mn>
</mml:math>
</inline-formula> were generated separately on Magny-Cours and XMT. The details shown here are for those generated on the XMT. They vary by a small margin from those generated on Magny-Cours.</p>
</caption>
<graphic alternate-form-of="table2-1094342012452893" xlink:href="10.1177_1094342012452893-table2.tif"/>
<table>
<thead>
<tr>
<th>Graph</th>
<th>Scale</th>
<th>Number of vertices</th>
<th>Number of edges</th>
<th>Maximum degree</th>
<th>Variance</th>
<th>% Isolated vertices</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="5">RMAT-ER</td>
<td>23</td>
<td>8,388,608</td>
<td>67,108,782</td>
<td>39</td>
<td>16.00</td>
<td>0</td>
</tr>
<tr>
<td>24</td>
<td>16,777,216</td>
<td>134,217,654</td>
<td>42</td>
<td>16.00</td>
<td>0</td>
</tr>
<tr>
<td>25</td>
<td>33,554,432</td>
<td>268,435,385</td>
<td>41</td>
<td>16.00</td>
<td>0</td>
</tr>
<tr>
<td>26</td>
<td>67,108,864</td>
<td>536,870,837</td>
<td>48</td>
<td>16.00</td>
<td>0</td>
</tr>
<tr>
<td>27</td>
<td>134,217,728</td>
<td>1,073,741,753</td>
<td>43</td>
<td>16.00</td>
<td>0</td>
</tr>
<tr>
<td rowspan="5">RMAT-G</td>
<td>23</td>
<td>8,388,608</td>
<td>67,081,539</td>
<td>999</td>
<td>390.25</td>
<td>2.11</td>
</tr>
<tr>
<td>24</td>
<td>16,777,216</td>
<td>134,181,095</td>
<td>1,278</td>
<td>415.72</td>
<td>2.33</td>
</tr>
<tr>
<td>25</td>
<td>33,554,432</td>
<td>268,385,483</td>
<td>1,489</td>
<td>441.99</td>
<td>2.56</td>
</tr>
<tr>
<td>26</td>
<td>67,108,864</td>
<td>536,803,101</td>
<td>1,800</td>
<td>469.43</td>
<td>2.81</td>
</tr>
<tr>
<td>27</td>
<td>134,217,728</td>
<td>1,073,650,024</td>
<td>2,160</td>
<td>497.88</td>
<td>3.06</td>
</tr>
<tr>
<td rowspan="5">RMAT-B</td>
<td>23</td>
<td>8,388,608</td>
<td>66,738,577</td>
<td>26,949</td>
<td>6,834.34</td>
<td>29.22</td>
</tr>
<tr>
<td>24</td>
<td>16,777,216</td>
<td>133,658,229</td>
<td>38,143</td>
<td>8,085.64</td>
<td>30.81</td>
</tr>
<tr>
<td>25</td>
<td>33,554,432</td>
<td>267,592,474</td>
<td>54,974</td>
<td>9,539.17</td>
<td>32.34</td>
</tr>
<tr>
<td>26</td>
<td>67,108,864</td>
<td>535,599,280</td>
<td>77,844</td>
<td>11,213.79</td>
<td>33.87</td>
</tr>
<tr>
<td>27</td>
<td>134,217,728</td>
<td>1,071,833,624</td>
<td>111,702</td>
<td>13,165.52</td>
<td>35.37</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The overall sizes of the three graph variants are similar, but they vary widely with respect to degree distribution and clustering coefficient. The local clustering coefficient of a vertex is given by the ratio of the actual number of edges between its neighbors to the maximum number of edges possible between the neighbors. The clustering coefficient of a graph is the average of local clustering coefficients over all the vertices. Degree distribution and local clustering coefficient for the three variants at <inline-formula id="inline-formula192-1094342012452893">
<mml:math id="mml-inline192-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula> are shown in <xref ref-type="fig" rid="fig1-1094342012452893">Figure 1</xref>. As expected, RMAT-ER has a mean value of <inline-formula id="inline-formula193-1094342012452893">
<mml:math id="mml-inline193-1094342012452893">
<mml:mn>16</mml:mn>
</mml:math>
</inline-formula> and small variance resembling normal distribution. The other two variants have a large distribution with RMAT-B having a maximum degree of 38,143. The average clustering coefficient for ER, G, and B are <inline-formula id="inline-formula195-1094342012452893">
<mml:math id="mml-inline195-1094342012452893">
<mml:msup>
<mml:mn>10</mml:mn>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>7</mml:mn>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula196-1094342012452893">
<mml:math id="mml-inline196-1094342012452893">
<mml:msup>
<mml:mn>12</mml:mn>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>6</mml:mn>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, and <inline-formula id="inline-formula197-1094342012452893">
<mml:math id="mml-inline197-1094342012452893">
<mml:msup>
<mml:mn>34</mml:mn>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>5</mml:mn>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, respectively. We also observe that the number of isolated vertices (with zero degree) is high for RMAT-B, over <inline-formula id="inline-formula198-1094342012452893">
<mml:math id="mml-inline198-1094342012452893">
<mml:mn>30</mml:mn>
<mml:mi mathvariant="normal">%</mml:mi>
</mml:math>
</inline-formula> of vertices are isolated. In our experiments, we remove duplicate edges and self-loops (this is what leads to a variation in the number of edges between different classes of graphs), but retain isolated vertices. Given the large variation in graph properties, we expect significant impact not only on the parallel performance of algorithms, but also on the behavior of the sequential algorithm (in terms of the number of edges that get matched). In particular, we expect a large variation in the degree distribution in RMAT-B to have an adverse effect on the performance of GPUs caused by load imbalances. We also note that in our experiments we randomly shuffle the vertex indices in the graphs as generated by R-MAT algorithm. The motivation for this shuffling is to prevent vertices with larger degrees getting concentrated towards vertices with smaller indices (the top-left quadrant of the R-MAT algorithm). This shuffling also makes the synthetic graphs realistic since we do not expect correlation between vertex identities to its degree.</p>
<p>A small set of graphs from real-world data are chosen to enable comparative study of our implementations on the GPUs with existing literature. The properties of real-world dataset are summarized in <xref ref-type="table" rid="table3-1094342012452893">Table 3</xref>.</p>
<table-wrap id="table3-1094342012452893" position="float">
<label>Table 3.</label>
<caption>
<p>Real-world graphs obtained from the Stanford Large Network Dataset Collection maintained by Jure Leskovec and the collection from DIMACS Challenge-9. Duplicate entries are removed and random weights are assigned to the first two instances.</p>
</caption>
<graphic alternate-form-of="table3-1094342012452893" xlink:href="10.1177_1094342012452893-table3.tif"/>
<table>
<thead>
<tr>
<th>Graph</th>
<th>Number of vertices</th>
<th>Number of edges</th>
<th>Maximum degree</th>
<th>Average degree</th>
<th>Variance</th>
<th>% Isolated vertices</th>
</tr>
</thead>
<tbody>
<tr>
<td>cit-patent</td>
<td>3,774,768</td>
<td>33,037,894</td>
<td>793</td>
<td>8.75</td>
<td>110.06</td>
<td>0</td>
</tr>
<tr>
<td>soc-live</td>
<td>4,847,571</td>
<td>42,851,237</td>
<td>20,333</td>
<td>17.68</td>
<td>2704.35</td>
<td>0.02</td>
</tr>
<tr>
<td>Usa-Roadmap</td>
<td>23,947,347</td>
<td>28,854,312</td>
<td>9</td>
<td>2.41</td>
<td>0.86</td>
<td>0</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
</sec>
<sec id="section7-1094342012452893">
<title>4 Parallel implementation</title>
<p>In this section we provide details of parallel implementations of the locally dominant algorithm for half-approximate weighted matching that was introduced in Section 3.1. We first describe a generic queue-based implementation suitable for all of the platforms, and then present a novel dataflow algorithm targeted specifically for the Cray XMT. The dataflow algorithm is motivated by general weaknesses of the shared work-queue approach such as the need for synchronization and variation in the amount of work that can be done in parallel leading to loss of performance.</p>
<sec id="section8-1094342012452893">
<title>4.1 Parallel queue-based implementation</title>
<p>The parallel queue-based implementation, illustrated in Algorithm 3, is a parallel implementation of the locally dominant algorithm introduced in Section 3.1. Similar to the serial version, we divide the parallel algorithm into two phases. Phase 1 of the algorithm (lines <inline-formula id="inline-formula199-1094342012452893">
<mml:math id="mml-inline199-1094342012452893">
<mml:mn>5</mml:mn>
</mml:math>
</inline-formula>--<inline-formula id="inline-formula200-1094342012452893">
<mml:math id="mml-inline200-1094342012452893">
<mml:mn>18</mml:mn>
</mml:math>
</inline-formula> in Algorithm 3) consists of two parallel sections. Each section iterates once over all of the vertices in a graph. While the first section (line <inline-formula id="inline-formula201-1094342012452893">
<mml:math id="mml-inline201-1094342012452893">
<mml:mn>5</mml:mn>
</mml:math>
</inline-formula>) finds a candidate mate for each vertex, the second section (line <inline-formula id="inline-formula202-1094342012452893">
<mml:math id="mml-inline202-1094342012452893">
<mml:mn>15</mml:mn>
</mml:math>
</inline-formula>) checks for locally dominant edges and adds them to the matching. Endpoints of matched edges (matched vertices) are added to a queue (<inline-formula id="inline-formula203-1094342012452893">
<mml:math id="mml-inline203-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>). Phase 2 of the algorithm (lines <inline-formula id="inline-formula204-1094342012452893">
<mml:math id="mml-inline204-1094342012452893">
<mml:mn>21</mml:mn>
</mml:math>
</inline-formula>--<inline-formula id="inline-formula205-1094342012452893">
<mml:math id="mml-inline205-1094342012452893">
<mml:mn>27</mml:mn>
</mml:math>
</inline-formula> in algorithm 3) is executed several times by iterating over all the vertices in the queue until no new edges get matched (the queue becomes empty).<table-wrap id="table6-1094342012452893" position="float">
<label>Algorithm 3</label>
<caption>
<p>Parallel queue-based implementation. <italic>Input</italic>: Graph <inline-formula id="inline-formula206-1094342012452893">
<mml:math id="mml-inline206-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. <italic>Output</italic>: A matching <inline-formula id="inline-formula207-1094342012452893">
<mml:math id="mml-inline207-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> represented in vector mate.<italic> Data structures</italic>: a queue <inline-formula id="inline-formula208-1094342012452893">
<mml:math id="mml-inline208-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> of vertices for processing in the current step, and a queue <inline-formula id="inline-formula209-1094342012452893">
<mml:math id="mml-inline209-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> of vertices that will be processed in the next step. Both the queues contain matched vertices; a vector candidate of size <inline-formula id="inline-formula210-1094342012452893">
<mml:math id="mml-inline210-1094342012452893">
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> that contains the i.d. of the current heaviest neighbor of each vertex.</p>
</caption>
<graphic alternate-form-of="table6-1094342012452893" xlink:href="10.1177_1094342012452893-table6.tif"/>
<table>
<tbody>
<tr>
<td> 1:</td>
<td>
<bold>procedure</bold> P<sc>arallel</sc>-Q<sc>ueue(</sc>
<inline-formula id="inline-formula211-1094342012452893">
<mml:math id="mml-inline211-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, mate)</td>
<td>
</td>
</tr>
<tr>
<td> 2:</td>
<td> <inline-formula id="inline-formula212-1094342012452893">
<mml:math id="mml-inline212-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 3:</td>
<td> <inline-formula id="inline-formula213-1094342012452893">
<mml:math id="mml-inline213-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 4:</td>
<td> — — <bold>Phase-1</bold> — — </td>
<td> </td>
</tr>
<tr>
<td> 5:</td>
<td> <bold>for each</bold> <inline-formula id="inline-formula214-1094342012452893">
<mml:math id="mml-inline214-1094342012452893">
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>V</mml:mi>
</mml:math>
</inline-formula> in <monospace>parallel</monospace> <bold>do</bold>
</td>
<td>▷OpenMP pragmas/CUDA kernels</td>
</tr>
<tr>
<td> 6:</td>
<td>  mate[<inline-formula id="inline-formula215-1094342012452893">
<mml:math id="mml-inline215-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula216-1094342012452893">
<mml:math id="mml-inline216-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 7:</td>
<td>  candidate[<inline-formula id="inline-formula217-1094342012452893">
<mml:math id="mml-inline217-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula218-1094342012452893">
<mml:math id="mml-inline218-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 8:</td>
<td>  <inline-formula id="inline-formula219-1094342012452893">
<mml:math id="mml-inline219-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula220-1094342012452893">
<mml:math id="mml-inline220-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="normal">∞</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 9:</td>
<td>  <inline-formula id="inline-formula221-1094342012452893">
<mml:math id="mml-inline221-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>d</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula222-1094342012452893">
<mml:math id="mml-inline222-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td>10:</td>
<td>  <bold>for</bold> <bold>each</bold> <inline-formula id="inline-formula223-1094342012452893">
<mml:math id="mml-inline223-1094342012452893">
<mml:mi>v</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>a</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> <bold>do</bold>
</td>
<td>
</td>
</tr>
<tr>
<td>11:</td>
<td>
<bold>if</bold> (mate[<inline-formula id="inline-formula224-1094342012452893">
<mml:math id="mml-inline224-1094342012452893">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>] = <inline-formula id="inline-formula225-1094342012452893">
<mml:math id="mml-inline225-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>) <monospace>AND</monospace> (<inline-formula id="inline-formula226-1094342012452893">
<mml:math id="mml-inline226-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula227-1094342012452893">
<mml:math id="mml-inline227-1094342012452893">
<mml:mo stretchy="false">&lt;</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>v</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>) <bold>then</bold>
</td>
<td>▷Use vertex i.d.s to break ties.</td>
</tr>
<tr>
<td>12:</td>
<td>    <inline-formula id="inline-formula228-1094342012452893">
<mml:math id="mml-inline228-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula229-1094342012452893">
<mml:math id="mml-inline229-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mi>w</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>v</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td>13:</td>
<td>    <inline-formula id="inline-formula230-1094342012452893">
<mml:math id="mml-inline230-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>d</mml:mi>
</mml:math>
</inline-formula> <inline-formula id="inline-formula231-1094342012452893">
<mml:math id="mml-inline231-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td>14:</td>
<td>  candidate[<inline-formula id="inline-formula232-1094342012452893">
<mml:math id="mml-inline232-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula233-1094342012452893">
<mml:math id="mml-inline233-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
</mml:math>
</inline-formula> <inline-formula id="inline-formula234-1094342012452893">
<mml:math id="mml-inline234-1094342012452893">
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>w</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi mathvariant="normal">_</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>d</mml:mi>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td>15:</td>
<td> <bold>for each</bold> <inline-formula id="inline-formula235-1094342012452893">
<mml:math id="mml-inline235-1094342012452893">
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>V</mml:mi>
</mml:math>
</inline-formula> in <monospace>parallel</monospace> <bold>do</bold>
</td>
<td>▷OpenMP pragmas/CUDA kernels</td>
</tr>
<tr>
<td>16:</td>
<td>
<bold>if</bold> candidate[candidate[<inline-formula id="inline-formula236-1094342012452893">
<mml:math id="mml-inline236-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>]] = <inline-formula id="inline-formula237-1094342012452893">
<mml:math id="mml-inline237-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula> <bold>then</bold>
</td>
<td>▷Found a <italic>locally dominant</italic> edge</td>
</tr>
<tr>
<td>17:</td>
<td>   mate[<inline-formula id="inline-formula238-1094342012452893">
<mml:math id="mml-inline238-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>] <inline-formula id="inline-formula239-1094342012452893">
<mml:math id="mml-inline239-1094342012452893">
<mml:mo stretchy="false">←</mml:mo>
</mml:math>
</inline-formula> candidate[<inline-formula id="inline-formula240-1094342012452893">
<mml:math id="mml-inline240-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula>]</td>
<td>
</td>
</tr>
<tr>
<td>18:</td>
<td>   <inline-formula id="inline-formula241-1094342012452893">
<mml:math id="mml-inline241-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
<mml:mo stretchy="false">←</mml:mo>
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
<mml:mo stretchy="false">∪</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>▷Use atomic memory operation</td>
</tr>
<tr>
<td>19:</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>20:</td>
<td> — — <bold>Phase-2</bold> — — </td>
<td>
</td>
</tr>
<tr>
<td>21:</td>
<td> <bold>while</bold> <inline-formula id="inline-formula242-1094342012452893">
<mml:math id="mml-inline242-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
<mml:mo stretchy="false">≠</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> <bold>do</bold>
</td>
<td>
</td>
</tr>
<tr>
<td>22:</td>
<td>
<bold>for each</bold> <inline-formula id="inline-formula243-1094342012452893">
<mml:math id="mml-inline243-1094342012452893">
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> in <monospace>parallel</monospace> do</td>
<td>▷OpenMP pragmas/CUDA kernels</td>
</tr>
<tr>
<td>23:</td>
<td>   <bold>for</bold> <bold>each</bold> <inline-formula id="inline-formula244-1094342012452893">
<mml:math id="mml-inline244-1094342012452893">
<mml:mi>v</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>a</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi mathvariant="normal">∖</mml:mi>
</mml:math>
</inline-formula> <bold>do</bold>
</td>
<td>
</td>
</tr>
<tr>
<td>24:</td>
<td>
<bold>if</bold> candidate[<inline-formula id="inline-formula245-1094342012452893">
<mml:math id="mml-inline245-1094342012452893">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>] = <inline-formula id="inline-formula246-1094342012452893">
<mml:math id="mml-inline246-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula> <bold>then</bold>
</td>
<td>▷Process <inline-formula id="inline-formula247-1094342012452893">
<mml:math id="mml-inline247-1094342012452893">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula> only if <inline-formula id="inline-formula248-1094342012452893">
<mml:math id="mml-inline248-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula> is its candidate mate </td>
</tr>
<tr>
<td>25:</td>
<td>    P<sc>rocess</sc>V<sc>ertex</sc>
<inline-formula id="inline-formula249-1094342012452893">
<mml:math id="mml-inline249-1094342012452893">
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>v</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td>26:</td>
<td>  <inline-formula id="inline-formula250-1094342012452893">
<mml:math id="mml-inline250-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
<mml:mo stretchy="false">←</mml:mo>
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>
</td>
<td>▷The new set of matched vertices</td>
</tr>
<tr>
<td>27:</td>
<td>  <inline-formula id="inline-formula251-1094342012452893">
<mml:math id="mml-inline251-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
<mml:mo stretchy="false">←</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
</tbody>
</table>
</table-wrap>
</p>
<p>On the multicore platforms, the algorithm is parallelized using OpenMP as the programming model. We use compressed sparse row (CSR) format (data structure) to store graphs in memory. In this format the neighborhood of each vertex is stored contiguously in memory. Thus, the benefit from caching exists when adjacency lists are accessed on platforms with cache hierarchies. The shared data structures for which the threads need to synchronize accesses are the two queue data structures, <inline-formula id="inline-formula252-1094342012452893">
<mml:math id="mml-inline252-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula253-1094342012452893">
<mml:math id="mml-inline253-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>. Different threads simultaneously add those vertices to a queue that get matched in a given iteration of the algorithm. One way of implementing synchronous queue operations is to use critical sections. However, critical sections result in severe loss of performance. On the x86 platforms (Magny-Cours and Nehalem) we use an intrinsic atomic operation <monospace>__sync_fetch_and_add()</monospace>to find a unique position to add to the queue (the tail of the queue) for each thread. A similar atomic operation, <monospace>int_fetch_add(),</monospace> is provided on the XMT. The use of two queues (<inline-formula id="inline-formula254-1094342012452893">
<mml:math id="mml-inline254-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula255-1094342012452893">
<mml:math id="mml-inline255-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>) leads to better performance: while from <inline-formula id="inline-formula256-1094342012452893">
<mml:math id="mml-inline256-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> we efficiently process (read) the vertices matched in the previous iteration, we enqueue (write) vertices matched in the current iteration in <inline-formula id="inline-formula257-1094342012452893">
<mml:math id="mml-inline257-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>. The threads synchronize at the end of each iteration, and in serial, we assign the contents of <inline-formula id="inline-formula258-1094342012452893">
<mml:math id="mml-inline258-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula259-1094342012452893">
<mml:math id="mml-inline259-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> (via pointer swap) and set <inline-formula id="inline-formula260-1094342012452893">
<mml:math id="mml-inline260-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> to empty (lines <inline-formula id="inline-formula261-1094342012452893">
<mml:math id="mml-inline261-1094342012452893">
<mml:mn>26</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula262-1094342012452893">
<mml:math id="mml-inline262-1094342012452893">
<mml:mn>27</mml:mn>
</mml:math>
</inline-formula> in Algorithm 3).</p>
<p>The use of shared work queues is similar to the use of queues in a level-synchronous breadth-first search to store and process the frontier. However, given that the vertices that need to be processed in the next iteration are determined based on current matching and edge weights, generic prefetching techniques to exploit caches are not applicable for this algorithm. The amount of parallelism at any given iteration depends on the size of the queue and the parallel efficiency depends on the pattern and speed of atomic operations to add new elements to the queue. The size of the queue as the algorithm progresses for different inputs is captured in <xref ref-type="fig" rid="fig1-1094342012452893">Figure 1</xref>(c).</p>
<sec id="section9-1094342012452893">
<title>4.1.1 CUDA implementation</title>
<p>Parallel implementation on GPUs is similar to the OpenMP implementation on multicore platforms. Again, we divide the algorithm into two phases and have three GPU kernel routines for Algorithm 3. The first routine assigns the heaviest neighbor for each vertex, the second routine checks whether a locally dominant edge is found, and if so, adds that edge to <inline-formula id="inline-formula263-1094342012452893">
<mml:math id="mml-inline263-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> and the two endpoints to the queue. The third kernel call combines the functionality of finding the heaviest neighbor and a test for local dominance of an edge in Phase 2. Instead of processing one vertex, as shown in Algorithm 3, we process a set of vertices stored in <inline-formula id="inline-formula264-1094342012452893">
<mml:math id="mml-inline264-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>. The newly matched vertices are added to <inline-formula id="inline-formula265-1094342012452893">
<mml:math id="mml-inline265-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>. We use the atomic memory operation <monospace>atomicAdd()</monospace>on the GPUs. Depending on the size of a queue, the grid dimensions are determined on the host machine that places the kernel call. Within the kernel, each thread processes a vertex based on its thread i.d. At the beginning, all of the data structures are copied to GPU memory and they reside there for the entire duration of the algorithm. After each iteration, only the size of <inline-formula id="inline-formula266-1094342012452893">
<mml:math id="mml-inline266-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> is sent back to the host machine that decides if a kernel call needs to be made and the sizes of grid dimensions of such a call. Owing to a lack of spatial and temporal locality of memory accesses in graphs, achieving high performance on GPUs is challenging.</p>
<p>Three factors affect the performance of our implementation on GPUs:<list list-type="roman-lower">
<list-item>
<p>
<italic>Load balancing within a warp</italic>: The same instruction is issued for all of the <inline-formula id="inline-formula267-1094342012452893">
<mml:math id="mml-inline267-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula> threads in a warp, and therefore, the performance of a warp is limited by the slowest thread even when other threads finish their work ahead of time. For example, the thread processing the vertex with the largest degree might be the slowest. An approach to solve this imbalance is to block the work into groups and use independent threads for different blocks. We observed about <inline-formula id="inline-formula268-1094342012452893">
<mml:math id="mml-inline268-1094342012452893">
<mml:mn>20</mml:mn>
</mml:math>
</inline-formula>% improvement for breadth-first search in RMAT-B with this approach. Since the overhead for blocking is small, there was no performance penalty for well-balanced inputs for RMAT-ER. While such an approach is well suited for straightforward algorithms such as breadth-first search, it is not suitable for algorithms such as matching that need synchronization between threads that process the adjacency list of a vertex.</p>
</list-item>
<list-item>
<p>
<italic>Coalescing memory accesses</italic>: On Tesla, memory accesses by threads of the same half-warp should be aligned and to contiguous blocks in memory. On Fermi, caches reduce overheads of non-sequential accesses only if threads of the same warp access data from the same cache line. However, in our implementation the vertices get added to the queue in a random order and, thus, each thread in a warp will access random memory locations. An approach to address this problem would be to maintain the work queues in some order. However, the potential benefits may not be worth the effort to reorder the queues.</p>
</list-item>
<list-item>
<p>
<italic>Branch divergence</italic>: Since all of the threads in a warp share the same instruction, if any one of these threads in a warp takes a different branch of an if condition, all of the other threads are forced to wait for its completion. In other words, both the branches of an if condition are executed serially. The GPU kernel call to process a vertex has several if conditions that check whether a neighbor has already been matched or not, and whether it is the current-heaviest neighbor or not. Restructuring of an algorithm and implementation might alleviate this problem to a certain extent. Future architectural improvements such as branch prediction will also have a pronounced impact on performance.</p>
</list-item>
</list>
</p>
<p>To summarize, solving each of these problems is a significantly hard challenge by itself, and in concert, they determine the overall performance of a GPU implementation. In our experimental results, presented in <xref ref-type="fig" rid="fig4-1094342012452893">Figure 4</xref>, we observe the adverse impact of load imbalances in RMAT-B relative to RMAT-ER, and Cit-Patent relative to Soc-Live. The compressed data structures used in our implementation alleviate irregular accesses by storing adjacency lists of a vertex in a contiguous block. In particular, the benefits are evident in Phase 1 when vertices are processed in this order, but in Phase 2 vertices are processed in a random order and therefore lead to poor performance. Optimization for branch divergence is challenging.</p>
</sec>
</sec>
<sec id="section10-1094342012452893">
<title>4.2 Parallel dataflow-based implementation</title>
<p>In this section we provide a novel dataflow-based implementation of the locally dominant algorithm. The XMT supports light-weight synchronization by associating every word of memory (8 bytes) with the following additional bits: a full/empty bit, a pointer forwarding bit, and two trap bits. Of particular interest to this study is the full/empty bit. XMT provides several extended memory semantic operations to manipulate the full/empty bit. The four key operations that are sufficient to implement the matching algorithm (and several other graph algorithms) are:</p>
<list list-type="bullet">
<list-item>
<p>
<monospace>purge:</monospace> sets the full/empty bit to empty and the value to zero;</p>
</list-item>
<list-item>
<p>
<monospace>readff:</monospace> reads a memory location only when the full/empty bit is full and leaves the bit full when read finishes;</p>
</list-item>
<list-item>
<p>
<monospace>readfe:</monospace> reads a memory location only when the full/empty bit is full and leaves the bit empty when read finishes;</p>
</list-item>
<list-item>
<p>
<monospace>writeef:</monospace> writes to a memory location only if the full/empty bit is empty, and flips the bit to full when the write finishes.</p>
</list-item>
</list>
<p>While a ‘single-producer single-consumer’ model can be efficiently implemented using <monospace>writeef</monospace> and <monospace>readfe,</monospace> a ‘single-producer multiple-consumer’ model can be implemented using <monospace>writeef</monospace> and <monospace>readff</monospace>. A dataflow implementation using the ‘single-producer multiple-consumer’model is presented in Algorithm 4. The general structure and behavior of the dataflow-based algorithm is similar to the queue-based algorithm, except that threads do not have a shared work-queue to synchronize their operations. Instead, each thread will work on a block of vertices (line <inline-formula id="inline-formula270-1094342012452893">
<mml:math id="mml-inline270-1094342012452893">
<mml:mn>8</mml:mn>
</mml:math>
</inline-formula> in Algorithm 4) by making a call to Procedure P<sc>rocess</sc>V<sc>ertex</sc>D<sc>f</sc> (presented in Algorithm 5) for each vertex that has not been processed before. For simplicity of description, we assume that the adjacency lists are maintained in a non-increasing order (vertex-i.d.s are used to break ties consistently).</p>
<p>In Procedure P<sc>rocess</sc>V<sc>ertex</sc>D<sc>f</sc>, the thread processing vertex <inline-formula id="inline-formula271-1094342012452893">
<mml:math id="mml-inline271-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> sets the candidate mate of <inline-formula id="inline-formula272-1094342012452893">
<mml:math id="mml-inline272-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> to its current-heaviest neighbor and attempts to read if that neighbor wants to match with <inline-formula id="inline-formula273-1094342012452893">
<mml:math id="mml-inline273-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>. If a positive answer is observed from this neighbor, then the thread adds this edge to the current set of matched edges and proactively sets a negative response to its remaining neighbors (lines <inline-formula id="inline-formula274-1094342012452893">
<mml:math id="mml-inline274-1094342012452893">
<mml:mn>10</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula275-1094342012452893">
<mml:math id="mml-inline275-1094342012452893">
<mml:mn>11</mml:mn>
</mml:math>
</inline-formula>). If a negative answer is observed from this neighbor, then the thread proceeds to the next heaviest neighbor. The number of vertices to be processed is generally much larger than the number of threads available on a system. Therefore, passively waiting for another thread to complete processing the desired vertex could lead to a dead-lock due to exhaustion of resources. While this problem can be addressed in multiple ways, we use recursion in combination with an additional data structure state that maintains a state (processed or not) for each vertex. The recursive call is shown in line <inline-formula id="inline-formula276-1094342012452893">
<mml:math id="mml-inline276-1094342012452893">
<mml:mn>5</mml:mn>
</mml:math>
</inline-formula> of Algorithm 5.</p>
<p>In the absence of a shared work queue, the algorithm unravels efficiently on the XMT. Extended memory semantic operations are executed efficiently on the XMT. When the tag bits are in a desired state, memory operations complete in a single clock cycle. Otherwise, the request is returned to the ready queue for a retry. Similar to general memory operations, XMT tolerates the latencies arising from these operations via multithreading. Thus, in comparison to the queue-based implementation, we observe superior performance of the dataflow algorithm in our experiments. The results are presented in <xref ref-type="fig" rid="fig5-1094342012452893">Figure 5</xref>.</p>
<fig id="fig5-1094342012452893" position="float">
<label>Figure 5.</label>
<caption>
<p>Scaling on the XMT: Performance of three implementations of the matching algorithm: queue, queue-sorted and dataflow (left to right respectively) on three variants of RMAT graphs at different scales detailed in <xref ref-type="table" rid="table2-1094342012452893">Table 2</xref>. Both strong and weak scaling are captured in the plots; both axes are in logarithmic scale. We used block-dynamic scheduling and requested a maximum of 100 streams (threads) per processor. The black dashed lines represent linear scaling.</p>
</caption>
<graphic xlink:href="10.1177_1094342012452893-fig5.tif"/>
</fig>
<p>
<table-wrap id="table7-1094342012452893" position="float">
<label>Algorithm 4</label>
<caption>
<p>Parallel dataflow-based implementation. <italic>Input</italic>: Graph <inline-formula id="inline-formula277-1094342012452893">
<mml:math id="mml-inline277-1094342012452893">
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>V</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. <italic>Output</italic>: A matching <inline-formula id="inline-formula278-1094342012452893">
<mml:math id="mml-inline278-1094342012452893">
<mml:mi>M</mml:mi>
</mml:math>
</inline-formula> represented in vector mate. <italic>Data structures</italic>: a vector candidate of size <inline-formula id="inline-formula279-1094342012452893">
<mml:math id="mml-inline279-1094342012452893">
<mml:mn>2</mml:mn>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula>. For an edge <inline-formula id="inline-formula280-1094342012452893">
<mml:math id="mml-inline280-1094342012452893">
<mml:msub>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>v</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> the response from <inline-formula id="inline-formula281-1094342012452893">
<mml:math id="mml-inline281-1094342012452893">
<mml:mi>u</mml:mi>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula282-1094342012452893">
<mml:math id="mml-inline282-1094342012452893">
<mml:mi>v</mml:mi>
</mml:math>
</inline-formula>, and vice versa, is stored in candidate; a vector state of size <inline-formula id="inline-formula283-1094342012452893">
<mml:math id="mml-inline283-1094342012452893">
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>V</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
</mml:math>
</inline-formula> represents if a vertex has been processed or not (zero implies an unprocessed state, a number <inline-formula id="inline-formula284-1094342012452893">
<mml:math id="mml-inline284-1094342012452893">
<mml:mo stretchy="false">&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula> implies that it has been processed).</p>
</caption>
<graphic alternate-form-of="table7-1094342012452893" xlink:href="10.1177_1094342012452893-table7.tif"/>
<table>
<tbody>
<tr>
<td> 1:</td>
<td>
<bold>procedure</bold> P<sc>arallel-</sc>D<sc>ataflow</sc> (<italic>G</italic>(<italic>V</italic>, <italic>E</italic>), mate)</td>
<td>
</td>
</tr>
<tr>
<td> 2:</td>
<td>
<bold>for each</bold> <italic>u</italic> ∊ <italic>V</italic> in <monospace>parallel</monospace> <bold>do</bold>
</td>
<td> </td>
</tr>
<tr>
<td> 3:</td>
<td>   mate[<italic>u</italic>] ← <inline-formula id="inline-formula285-1094342012452893">
<mml:math id="mml-inline285-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">∅</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
<tr>
<td> 4:</td>
<td>   state[<italic>u</italic>] ← 0</td>
<td>
</td>
</tr>
<tr>
<td> 5:</td>
<td>
<bold>for each</bold> <italic>e<sub>u, v</sub>
</italic> ∊ <italic>E</italic> in <monospace>parallel</monospace> <bold>do</bold>
</td>
<td> </td>
</tr>
<tr>
<td> 6:</td>
<td>
<monospace>writeef</monospace>(candidate [<italic>u</italic> → <italic>v</italic>], 0)</td>
<td>▷ Set full/empty bit to empty and value to zero</td>
</tr>
<tr>
<td> 7:</td>
<td>
<monospace>writeef</monospace>(candidate [<italic>v</italic> → <italic>u</italic>], 0)</td>
<td>
</td>
</tr>
<tr>
<td> 8:</td>
<td>
<bold>for each</bold> <italic>u</italic> ∊ <italic>V</italic> in <monospace>parallel</monospace> <bold>do</bold>
</td>
<td>
</td>
</tr>
<tr>
<td> 9:</td>
<td>
<italic>Processed</italic> ← <monospace>int_fetch_add</monospace> (state[<italic>u</italic>], 1)</td>
<td>
</td>
</tr>
<tr>
<td>10:</td>
<td>   <bold>if</bold> <italic>Processed</italic> = 0 <bold>then</bold>
</td>
<td>▷ Process only if <italic>u</italic> has not been processed before</td>
</tr>
<tr>
<td>11:</td>
<td>   P<sc>rocess</sc>V<sc>ertex</sc>D<sc>F</sc>(<italic>u</italic>)</td>
<td>
</td>
</tr>
</tbody>
</table>
</table-wrap>
</p>
<p>
<table-wrap id="table8-1094342012452893" position="float">
<label>Algorithm 5</label>
<caption>
<p>ProcessVertexDF</p>
</caption>
<graphic alternate-form-of="table8-1094342012452893" xlink:href="10.1177_1094342012452893-table8.tif"/>
<table>
<tbody>
<tr>
<td> 1:</td>
<td>
<bold>procedure</bold> P<sc>rocess</sc>V<sc>ertex</sc>D<sc>F</sc>(<italic>s</italic>)</td>
<td>
</td>
</tr>
<tr>
<td> 2:</td>
<td>
<bold>for</bold> <bold>each</bold> <inline-formula id="inline-formula286-1094342012452893">
<mml:math id="mml-inline286-1094342012452893">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>a</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> in non-increasing order of weights <bold>do</bold>
</td>
<td>
</td>
</tr>
<tr>
<td> 3:</td>
<td>
<monospace>writeef</monospace>(candidate [<inline-formula id="inline-formula287-1094342012452893">
<mml:math id="mml-inline287-1094342012452893">
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula>], <inline-formula id="inline-formula288-1094342012452893">
<mml:math id="mml-inline288-1094342012452893">
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula>)</td>
<td>▷ Set full/empty bit to full and value to 1</td>
</tr>
<tr>
<td> 4:</td>
<td>
<bold>if</bold> <monospace>int_fetch_add</monospace> (state[t], 1) = 0 <bold>then</bold>
</td>
<td>
</td>
</tr>
<tr>
<td> 5:</td>
<td>    P<sc>rocess</sc>V<sc>ertex</sc>D<sc>F</sc>(<italic>t</italic>)</td>
<td>▷ Proactively process the other end</td>
</tr>
<tr>
<td> 6:</td>
<td>
<bold>if</bold> <monospace>readff</monospace> (candidate [<inline-formula id="inline-formula289-1094342012452893">
<mml:math id="mml-inline289-1094342012452893">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>])=<inline-formula id="inline-formula290-1094342012452893">
<mml:math id="mml-inline290-1094342012452893">
<mml:mn>1</mml:mn>
</mml:math>
</inline-formula> <bold>then</bold>
</td>
<td>▷ Wait until full/empty bit becomes full</td>
</tr>
<tr>
<td> 7:</td>
<td>    mate[<italic>s</italic>] ← <italic>t</italic>
</td>
<td>▷Found a locally dominant edge</td>
</tr>
<tr>
<td> 8:</td>
<td>    mate[<italic>t</italic>] ← <italic>s</italic>
</td>
<td>
</td>
</tr>
<tr>
<td> 9:</td>
<td>    <monospace>break</monospace>
</td>
<td>
</td>
</tr>
<tr>
<td>10:</td>
<td>
<bold>for</bold> <bold>each</bold> <inline-formula id="inline-formula291-1094342012452893">
<mml:math id="mml-inline291-1094342012452893">
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>p</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi>a</mml:mi>
<mml:mi>d</mml:mi>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> not already processed <bold>do</bold>
</td>
<td>▷ Set the value to zero for remaining neighbors of <inline-formula id="inline-formula292-1094342012452893">
<mml:math id="mml-inline292-1094342012452893">
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula>
</td>
</tr>
<tr>
<td>11:</td>
<td>   candidate<inline-formula id="inline-formula293-1094342012452893">
<mml:math id="mml-inline293-1094342012452893">
<mml:mo stretchy="false">[</mml:mo>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">→</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>p</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo stretchy="false">←</mml:mo>
<mml:mn>0</mml:mn>
</mml:math>
</inline-formula>
</td>
<td>
</td>
</tr>
</tbody>
</table>
</table-wrap>
</p>
</sec>
</sec>
<sec id="section11-1094342012452893">
<title>5 Experimental results</title>
<p>In this section, we provide experimental results on the performance of the parallel half-approximate algorithms on five platforms presented in Section 3.2. Three classes of synthetic graphs of various sizes are used as input for experiments across the platforms, and three instances of real-world data are used as input on the GPU platforms. Details on the dataset are provided in Section 3.3. We provide strong and weak scaling results on the Magny-Cours and XMT platforms for R-MAT graphs up to a billion edges (<inline-formula id="inline-formula294-1094342012452893">
<mml:math id="mml-inline294-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula295-1094342012452893">
<mml:math id="mml-inline295-1094342012452893">
<mml:mn>27</mml:mn>
</mml:math>
</inline-formula>). In addition, on the XMT we provide results for three variants of implementation: queue-based, queue-based with sorted adjacency lists, and dataflow-based. We provide strong scaling results on the Nehalem (<inline-formula id="inline-formula296-1094342012452893">
<mml:math id="mml-inline296-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula>). Results from GPU platforms are normalized with the performance of one core of the host platform. In general, we avoid excessive fine-tuning of software for any given architecture. Instead, we strive for uniformity of the algorithm and code where possible. An exception to this goal is the dataflow algorithm specially targeted for the XMT. We now present the results and brief discussion on different aspects of performance.</p>
<sec id="section12-1094342012452893">
<title>5.1 General observations</title>
<p>Basic insight on the performance differences for the three classes of graphs (RMAT-ER, RMAT-G, and RMAT-B) can be obtained by considering the cardinality as the algorithm progresses. The cardinality of matching (the number of edges in matching) represents the amount of work that can be parallelized and, therefore, determines the utilization of system resources. The cardinality, determined by the size of <inline-formula id="inline-formula297-1094342012452893">
<mml:math id="mml-inline297-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>, at each iteration in the execution of the algorithm, for the three classes of graphs (with <inline-formula id="inline-formula298-1094342012452893">
<mml:math id="mml-inline298-1094342012452893">
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">S</mml:mi>
<mml:mi mathvariant="normal">C</mml:mi>
<mml:mi mathvariant="normal">A</mml:mi>
<mml:mi mathvariant="normal">L</mml:mi>
<mml:mi mathvariant="normal">E</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>24</mml:mn>
</mml:math>
</inline-formula>) is provided in <xref ref-type="fig" rid="fig1-1094342012452893">Figure 1</xref>(c). We provide the size of <inline-formula id="inline-formula299-1094342012452893">
<mml:math id="mml-inline299-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>C</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> on the <inline-formula id="inline-formula300-1094342012452893">
<mml:math id="mml-inline300-1094342012452893">
<mml:mi>Y</mml:mi>
</mml:math>
</inline-formula>-axis in log-scale and the iterations are provided on the <inline-formula id="inline-formula301-1094342012452893">
<mml:math id="mml-inline301-1094342012452893">
<mml:mi>X</mml:mi>
</mml:math>
</inline-formula>-axis. It can be observed that an upper bound on the work roughly decreases by half after each iteration. The final cardinality of the three matchings as a percentage of the number of vertices is as follows: <inline-formula id="inline-formula302-1094342012452893">
<mml:math id="mml-inline302-1094342012452893">
<mml:mn>94.12</mml:mn>
</mml:math>
</inline-formula>% for RMAT-ER, <inline-formula id="inline-formula303-1094342012452893">
<mml:math id="mml-inline303-1094342012452893">
<mml:mn>81.70</mml:mn>
</mml:math>
</inline-formula>% (<inline-formula id="inline-formula304-1094342012452893">
<mml:math id="mml-inline304-1094342012452893">
<mml:mn>83.46</mml:mn>
</mml:math>
</inline-formula>% when isolated vertices are removed from the denominator) for RMAT-G, and <inline-formula id="inline-formula305-1094342012452893">
<mml:math id="mml-inline305-1094342012452893">
<mml:mn>44.24</mml:mn>
</mml:math>
</inline-formula>% (<inline-formula id="inline-formula306-1094342012452893">
<mml:math id="mml-inline306-1094342012452893">
<mml:mn>63.94</mml:mn>
</mml:math>
</inline-formula>% when isolated vertices are removed from the denominator) for RMAT-B. The cardinality of matching after Phase 1 as a percentage of the final cardinality is as follows: <inline-formula id="inline-formula307-1094342012452893">
<mml:math id="mml-inline307-1094342012452893">
<mml:mn>53.14</mml:mn>
</mml:math>
</inline-formula>% (ER), <inline-formula id="inline-formula308-1094342012452893">
<mml:math id="mml-inline308-1094342012452893">
<mml:mn>46.33</mml:mn>
</mml:math>
</inline-formula>% (G), and <inline-formula id="inline-formula309-1094342012452893">
<mml:math id="mml-inline309-1094342012452893">
<mml:mn>36.06</mml:mn>
</mml:math>
</inline-formula>% (B). We observe that the cardinality of matching for RMAT-B is much smaller than that for others. In addition, RMAT-B has more iterations with smaller amounts of work towards the end of the execution (refer to <xref ref-type="fig" rid="fig1-1094342012452893">Figure 1</xref>(c)). A longer tail with less work towards the end leads to inefficient use of resources on platforms such XMT and GPUs that have a large number of threads.</p>
</sec>
<sec id="section13-1094342012452893">
<title>5.2 Magny-Cours</title>
<p>Strong and weak scaling results on Magny-Cours are presented in <xref ref-type="fig" rid="fig2-1094342012452893">Figure 2</xref>. In our experiments, we pin the threads to cores using the environment variable <monospace>GOMP_CPU_AFFINITY</monospace> by using a round-robin scheme to place each consecutive thread on a different socket and a different chip within a socket. For example, thread-0 would be pinned to chip-0 on socket-0, followed by thread-1 to chip-0 on socket-1, …, thread-4 to chip-1 on socket-0, and so on. Note that each socket has two chips, and each chip has six cores. In addition, we use the command <monospace>numactl --interleave</monospace> to enable NUMA-aware memory allocation on the system. We observe excellent scalability for all three classes of graphs at different problem sizes. We observe minor degradation of performance at 48 cores, particularly for smaller graph sizes. While the Magny-Cours platform provides excellent scalability, the absolute runtime as compared with Nehalem and XMT is slower by a factor of <inline-formula id="inline-formula310-1094342012452893">
<mml:math id="mml-inline310-1094342012452893">
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula>.</p>
</sec>
<sec id="section14-1094342012452893">
<title>5.3 Nehalem</title>
<p>Strong scaling results on Nehalem are presented in <xref ref-type="fig" rid="fig3-1094342012452893">Figure 3</xref>. In our experiments, we used the environment variable <monospace>KMP_CPU_AFFINITY</monospace> to pin the threads to cores. Similar to Magny-Cours, we scatter the threads across the system to maximize available memory bandwidth. In order to evaluate the benefits of hyperthreads, we pin two threads to each core (shown with red lines in the Figure). We observe a performance gain of <inline-formula id="inline-formula311-1094342012452893">
<mml:math id="mml-inline311-1094342012452893">
<mml:mn>1.2</mml:mn>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula312-1094342012452893">
<mml:math id="mml-inline312-1094342012452893">
<mml:mn>1.4</mml:mn>
</mml:math>
</inline-formula> relative to the performance of one thread per core. The black dashed lines indicate linear speedup. We observe excellent scaling on Nehalem, which is enabled by the advanced architectural features coupled with the large memory bandwidth.</p>
</sec>
<sec id="section15-1094342012452893">
<title>Tesla and Fermi</title>
<p>We provide performance results on the two GPU platforms, Tesla and Fermi, in <xref ref-type="fig" rid="fig4-1094342012452893">Figure 4</xref>. The bars represent speedup relative to the performance of one core of the host platform. The host consists a 2-socket 6-core Intel Nehalem with 24 GBs of system memory. Note that the host processor is a new platform relative to the quad-core Intel Nehalem platform presented earlier. In our implementation, we transfer the graph data structures, as well as all of the other supporting data structures to the GPU memory at the beginning of the execution. The data structures remain in the GPU memory during the entire execution. For each iteration of the <bold>while</bold> loop, the size of the queue <inline-formula id="inline-formula313-1094342012452893">
<mml:math id="mml-inline313-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> is sent back to the host, which swaps the queue headers, determines the grid sizes and invokes a GPU kernel. In order to fit larger graphs on the GPU memory, the graph data structures are maintained in <inline-formula id="inline-formula314-1094342012452893">
<mml:math id="mml-inline314-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula>-bit primitive data types (integers for vertex i.d.s and doubles for weight). The data structures on the host machine are also maintained in <inline-formula id="inline-formula315-1094342012452893">
<mml:math id="mml-inline315-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula>-bit primitives. Note that while the results on GPUs and the host machine are obtained using <inline-formula id="inline-formula316-1094342012452893">
<mml:math id="mml-inline316-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula>-bit primitives, the results on x86 (Opteron and Nehalem) and XMT are obtained using <inline-formula id="inline-formula317-1094342012452893">
<mml:math id="mml-inline317-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula>-bit primitives. On the host, we used Intel 11.1 compilers with <monospace>-fast</monospace> switch, and used the environment variable <monospace>KMP_CPU_AFFINITY</monospace> to pin the threads to cores such that the total memory bandwidth is maximized.</p>
<p>From the results presented in <xref ref-type="fig" rid="fig4-1094342012452893">Figure 4</xref> we make the following observations.</p>
<list list-type="roman-lower">
<list-item>
<p>A significant speedup on Fermi relative to Tesla is enabled by efficient atomic memory operations that are executed in L2 on Fermi compared with the main memory in Tesla, and a larger number of cores although at a slightly lower clock speed.</p>
</list-item>
<list-item>
<p>A significant performance gain on Fermi compared with 12-core and 24-core runs on the host machine. While we observe improvements in the absolute times for single core runs, the excellent scaling observed on Nehalem (<xref ref-type="fig" rid="fig3-1094342012452893">Figure 3</xref>) is not repeated on the host machine. The relative increase in the number of cores without a corresponding increase in memory bandwidth and small problem sizes are the reasons for this lack of scaling on the host platform.</p>
</list-item>
<list-item>
<p>A large variation in vertex-degree (RMAT-B) has an adverse impact on performance for both Tesla and Fermi.</p>
</list-item>
<list-item>
<p>Disabling L1 caches improves performance for well-balanced work loads (RMAT-ER). This behavior can be attributed to the irregularity of memory accesses that thrash on a relatively small L1 cache.</p>
</list-item>
</list>
</sec>
<sec id="section16-1094342012452893">
<title>5.4 XMT</title>
<p>Strong and weak scaling results on the XMT using the three classes of graphs and three variants of implementation are presented in <xref ref-type="fig" rid="fig5-1094342012452893">Figure 5</xref>. Results for the queue-based implementation are provided on the left column. We observe good scaling for up to <inline-formula id="inline-formula318-1094342012452893">
<mml:math id="mml-inline318-1094342012452893">
<mml:mn>32</mml:mn>
</mml:math>
</inline-formula> processors, and see a degradation of scaling for <inline-formula id="inline-formula319-1094342012452893">
<mml:math id="mml-inline319-1094342012452893">
<mml:mn>64</mml:mn>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula320-1094342012452893">
<mml:math id="mml-inline320-1094342012452893">
<mml:mn>128</mml:mn>
</mml:math>
</inline-formula> processors. The middle column shows results for queue-based implementation where the adjacency lists are maintained in a sorted (non-increasing) order. We observe that sorting speeds the runtime by a factor of two with respect to the first variant (unsorted). However, sorting itself is much more expensive than matching. We present the results here for pedagogical reasons only. Results for the dataflow implementation are presented in the column on right.</p>
<p>In our experiments, we request about <inline-formula id="inline-formula321-1094342012452893">
<mml:math id="mml-inline321-1094342012452893">
<mml:mn>100</mml:mn>
</mml:math>
</inline-formula> threads (thread-streams) on each processor and use block-dynamic scheduling of threads. The flat cacheless memory structure, extended memory semantics, and compiler-driven optimizations make programming on the XMT the easiest among the five platforms.</p>
<p>We observe that the dataflow implementation scales better than the queue-based implementation. Owing to large amounts of concurrency, we observe memory hot-spotting on the queue, <inline-formula id="inline-formula322-1094342012452893">
<mml:math id="mml-inline322-1094342012452893">
<mml:msub>
<mml:mi>Q</mml:mi>
<mml:mi>N</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>, that results in a loss of performance. The autonomy induced from the absence of shared work-queues enable the dataflow algorithm to scale better.</p>
<p>In order to determine the influence of the range of edge-weights on performance, we experimented with different ranges of weights. While the queue-based implementation was sensitive to the range, the dataflow-based implementation was stable and provided about a factor of two speedup for smaller ranges of weights over larger ranges. In summary, we observe excellent speedups on the XMT for all three variants of the implementation on all three classes of inputs.</p>
</sec>
</sec>
<sec id="section17-1094342012452893">
<title>6 Conclusions and future work</title>
<p>We presented multithreaded implementations for the half-approximate weighted matching problem on state-of-the-art multicore and manycore platforms, and a massively multithreaded architecture. Using a carefully chosen set of synthetic and real-world graphs we demonstrated scalable performance across the platforms. Matching is an important graph problem with numerous applications in scientific computing. Using irregular nature of this problem, we explored several architectural features that enable efficient execution of irregular problems. We also presented a novel dataflow algorithms for the approximate matching problem that is suitable for architectures such as the Cray XMT.</p>
<p>Based on our current work and experimental results, we make the following broad conclusions.</p>
<list list-type="bullet">
<list-item>
<p>The structure of a graph, such as degree distributions and local clustering, as well as its numerical properties, such as the range of edge weights, play a dominant role in determining the performance of serial and parallel algorithms. This is evident from our results using inputs with different characteristics. These effects are pronounced on Fermi with about a four fold difference in performance between RMAT-ER and RMAT-B for a similar size of input.</p>
</list-item>
<list-item>
<p>Compared with cache hierarchies, multithreading provides an efficient means to tolerate latencies resulting from irregular memory accesses and synchronization between threads. Relatively superior performance of XMT relative to Opteron (about five times faster clock speed) supports this conclusion.</p>
</list-item>
<list-item>
<p>Architectural support for light-weight synchronization will enable efficient implementation of fine-grained parallelism in applications that lack coarse-grained parallelism. Graph algorithms in particular will benefit from these features. As observed in our experiments, a large improvement in the performance of Fermi over Tesla is enabled in part by efficient atomic memory operations. Tag bits and extended memory semantics on the XMT enable a different kind of thinking for algorithm design as well as better performance (dataflow algorithm).</p>
</list-item>
<list-item>
<p>When supported by hardware features, for example efficient atomic memory operations, CUDA provides a relatively simple programming model with considerable performance benefits for some types of inputs (for example, RMAT-ER). However, achieving good performance for difficult inputs on these systems remains challenging.</p>
</list-item>
</list>
<p>By contrasting and comparing the performance of approximate matching on different platforms, we expect the insights from this work to benefit the design and use of future generations of manycore architectures.</p>
<p>In the near future, we plan to extend this work along the following directions. We plan to implement optimal algorithms for weighted and unweighted matching problems on shared-memory platforms. Optimal algorithms have numerous applications but are challenging to implement. We plan to extend the work on GPUs along two directions: capability to handle load imbalances within a warp due to variation in vertex-degrees and scalable implementations on multi-GPU systems. We plan to explore NUMA-aware implementations that are relevant to current and emerging architectures. Finally, we plan to combine the work presented in this paper with our earlier work on distributed-memory systems using a hybrid programming approach.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We acknowledge many contributions from Florin Dobrian, Assefaw Gebremedhin, Fredrik Manne, and Umit Catalyurek. We also thank the anonymous referees for their valuable comments that helped us improve the manuscript.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure" id="fn1-1094342012452893">
<label>Funding</label>
<p>This work was supported by the Center for Adaptive Super Computing Software - MultiThreaded Architectures (CASS-MT) at the U.S. Department of Energy's Pacific Northwest National Laboratory. PNNL is operated by the Battelle Memorial Institute (contract number DE-ACO6-76RL01830). Additional funding for this project was provided by the U.S. Department of Energy through the CSCAPES Institute (grant numbers DE-FC02-08ER25864 and DE-FC02-06ER2775).</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Abou-Rjeili</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Karypis</surname>
<given-names>G</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Multilevel algorithms for partitioning power-law graphs</article-title>. In <source>Proceedings of the 20th International Parallel and Distributed Processing Symposium, 2006 (IPDPS 2006)</source>.</citation>
</ref>
<ref id="bibr2-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Agarwal</surname>
<given-names>V</given-names>
</name>
<name>
<surname>Petrini</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Pasetto</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Bader</surname>
<given-names>DA</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Scalable graph exploration on multicore processors</article-title>. In <source>Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis (SC'10)</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, pp. <fpage>1</fpage>–<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr3-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Asanovic</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Bodik</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Catanzaro</surname>
<given-names>BC</given-names>
</name>
<etal/>
</person-group>. (<year>2006</year>) <source>The Landscape of Parallel Computing Research: A View from Berkeley</source>. <comment>Technical report, Electrical Engineering and Computer Sciences, University of California at Berkeley</comment>.</citation>
</ref>
<ref id="bibr4-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Avis</surname>
<given-names>D</given-names>
</name>
</person-group> (<year>1983</year>) <article-title>A survey of heuristics for the weighted matching problem</article-title>. <source>Network</source> <volume>13</volume>: <fpage>475</fpage>–<lpage>493</lpage>.</citation>
</ref>
<ref id="bibr5-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bader</surname>
<given-names>DA</given-names>
</name>
<name>
<surname>Madduri</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Designing multithreaded algorithms for breadth-first search and st-connectivity on the Cray MTA-2</article-title>. In <source>Proceedings of the 2006 International Conference on Parallel Processing (ICPP '06)</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, pp. <fpage>523</fpage>–<lpage>530</lpage>.</citation>
</ref>
<ref id="bibr6-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bader</surname>
<given-names>DA</given-names>
</name>
<name>
<surname>Madduri</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>SNAP, small-world network analysis and partitioning: an open-source parallel graph framework for the exploration of large-scale networks</article-title>. In <source>Proceedings of the 2008 International Parallel and Distributed Processing Symposium (IPDPS 2008)</source>, <publisher-loc>Miami, FL</publisher-loc>.</citation>
</ref>
<ref id="bibr7-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Belongie</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Malik</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Puzicha</surname>
<given-names>J</given-names>
</name>
</person-group> (<year>2002</year>) <article-title>Shape matching and object recognition using shape contexts</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>24</volume>: <fpage>509</fpage>–<lpage>522</lpage>.</citation>
</ref>
<ref id="bibr8-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Catalyurek</surname>
<given-names>U</given-names>
</name>
<name>
<surname>Feo</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Gebremedhin</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Halappanavar</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Pothen</surname>
<given-names>A</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Multithreaded algorithms for graph coloring. Submitted to a journal. Invited presentation at</article-title> <source>SIAM Conference on Computational Science and Engineering (CSE11)</source>.</citation>
</ref>
<ref id="bibr9-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chakrabarti</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Faloutsos</surname>
<given-names>C</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Graph mining: laws, generators, and algorithms</article-title>. <source>ACM Computer Surveys</source> <volume>38</volume>: <fpage>2</fpage>.</citation>
</ref>
<ref id="bibr10-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cong</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Bader</surname>
<given-names>D</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Techniques for Designing Efficient Parallel Graph Algorithms for SMPs and Multicore Processors</article-title>. <source>lecture notes in computer science</source>, <volume>vol. 4742</volume>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>, pp. <fpage>137</fpage>–<lpage>147</lpage>.</citation>
</ref>
<ref id="bibr11-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Drake</surname>
<given-names>DE</given-names>
</name>
<name>
<surname>Hougardy</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2003</year>) <article-title>A simple approximation algorithm for the weighted matching problem</article-title>. <source>Information Processing Letters</source> <volume>85</volume>: <fpage>211</fpage>–<lpage>213</lpage>.</citation>
</ref>
<ref id="bibr12-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Duff</surname>
<given-names>IS</given-names>
</name>
<name>
<surname>Koster</surname>
<given-names>J</given-names>
</name>
</person-group> (<year>1999</year>) <article-title>The design and use of algorithms for permuting large entries to the diagonal of sparse matrices</article-title>. <source>SIAM Journal of Matrix Analysis Applications</source> <volume>20</volume>: <fpage>889</fpage>–<lpage>901</lpage>.</citation>
</ref>
<ref id="bibr13-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Feo</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Harper</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Kahan</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Konecny</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>ELDORADO</article-title>. In <source>CF '05: Proceedings of the 2nd Conference on Computing Frontiers</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, pp. <fpage>28</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr14-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Halappanavar</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2009</year>) <source>Algorithms for Vertex-weighted Matching in Graphs</source>. <publisher-name>PhD thesis</publisher-name>, <publisher-loc>Old Dominion University, Norfolk, VA</publisher-loc>.</citation>
</ref>
<ref id="bibr15-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Harish</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Narayanan</surname>
<given-names>PJ</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Accelerating large graph algorithms on the GPU using CUDA</article-title>. In <source>Proceedings of the 14th International Conference on High Performance Computing (HiPC'07)</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>, pp. <fpage>197</fpage>–<lpage>208</lpage>.</citation>
</ref>
<ref id="bibr16-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hendrickson</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Berry</surname>
<given-names>JW</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Graph analysis with high-performance computing</article-title>. <source>Computing in Science and Engineering</source> <volume>10</volume>(<issue>2</issue>): <fpage>14</fpage>–<lpage>19</lpage>.</citation>
</ref>
<ref id="bibr17-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hoepman</surname>
<given-names>J-H</given-names>
</name>
</person-group> (<year>2004</year>) <article-title>Simple distributed weighted matchings</article-title>. <source>Computing Research Repository (CoRR)</source> <comment>cs.DC/0410047</comment>. </citation>
</ref>
<ref id="bibr18-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Hong</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Kim</surname>
<given-names>SK</given-names>
</name>
<name>
<surname>Oguntebi</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Olukotun</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Accelerating CUDA graph algorithms at maximum warp</article-title>. In : Calin Cascaval and Pen-Chung Yew (eds) <source>Proceedings of the 16th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP 2011</source>, San Antonio, TX, USA, February 12--16. ACM pp. <fpage>267</fpage>–<lpage>276</lpage>. </citation>
</ref>
<ref id="bibr19-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Karypis</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Kumar</surname>
<given-names>V</given-names>
</name>
</person-group> (<year>1995</year>) <article-title>Analysis of multilevel graph partitioning</article-title>. In <source>Supercomputing '95: Proceedings of the 1995 ACM/IEEE Conference on Supercomputing (CDROM)</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, p. <fpage>29</fpage>.</citation>
</ref>
<ref id="bibr20-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Katz</surname>
<given-names>GJ</given-names>
</name>
<name>
<surname>Kider</surname>
<given-names>JT</given-names>
<suffix>Jr</suffix>
</name>
</person-group> (<year>2008</year>) <article-title>All-pairs shortest-paths for large graphs on the GPU</article-title>. In <source>Proceedings of the 23 rd ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware (GH '08)</source>. <publisher-loc>Aire-la-Ville, Switzerland</publisher-loc>: <publisher-name>Eurographics Association</publisher-name>, pp. <fpage>47</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr21-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>XS</given-names>
</name>
<name>
<surname>Demmel</surname>
<given-names>JW</given-names>
</name>
</person-group> (<year>1998</year>) <article-title>Making sparse Gaussian elimination scalable by static pivoting</article-title>. In <source>Supercomputing '98: Proceedings of the 1998 ACM/IEEE conference on Supercomputing (CDROM)</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, pp. <fpage>1</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr22-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Lovasz</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>1986</year>) <source>Matching Theory (North-Holland Mathematics Studies)</source>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Elsevier Science Ltd</publisher-name>.</citation>
</ref>
<ref id="bibr23-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lumsdaine</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Gregor</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Hendrickson</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Berry</surname>
<given-names>JW</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Challenges in parallel graph processing</article-title>. <source>Parallel Processing Letters</source> <volume>17</volume>: <fpage>5</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr24-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Luo</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Wong</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Hwu</surname>
<given-names>W-m</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>An effective GPU implementation of breadth-first search</article-title>. In <source>Proceedings of the 47th Design Automation Conference (DAC '10)</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, pp. <fpage>52</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr25-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Manne</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Bisseling</surname>
<given-names>RH</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>A parallel approximation algorithm for the weighted maximum matching problem</article-title>. In <source>The Seventh International Conference on Parallel Processing and Applied Mathematics</source>, pp. <fpage>708</fpage>–<lpage>717</lpage>.</citation>
</ref>
<ref id="bibr26-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Molka</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Hackenberg</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Schone</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Muller</surname>
<given-names>MS</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>Memory performance and cache coherency effects on an Intel Nehalem multiprocessor system</article-title>. In <source>PACT '09: Proceedings of the 2009 18th International Conference on Parallel Architectures and Compilation Techniques</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, pp. <fpage>261</fpage>–<lpage>270</lpage>.</citation>
</ref>
<ref id="bibr27-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Monien</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Preis</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Diekmann</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>Quality matching and local improvement for multilevel graph-partitioning</article-title>. <source>Parallel Computing</source> <volume>26</volume>: <fpage>1609</fpage>–<lpage>1634</lpage>.</citation>
</ref>
<ref id="bibr28-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Nieplocha</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Márquez</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Feo</surname>
<given-names>J</given-names>
</name>
<etal/>
</person-group>. (<year>2007</year>) <article-title>Evaluating the potential of multithreaded platforms for irregular scientific computations</article-title>. In <source>CF '07: Proceedings of the 4th International Conference on Computing Frontiers</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, pp. <fpage>47</fpage>–<lpage>58</lpage>.</citation>
</ref>
<ref id="bibr29-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pinar</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Chow</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Pothen</surname>
<given-names>A</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Combinatorial algorithms for computing column space bases that have sparse inverses</article-title>. <source>Electronic Transactions on Numerical Analysis</source> <volume>22</volume>: <fpage>122</fpage>–<lpage>145</lpage>.</citation>
</ref>
<ref id="bibr30-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pothen</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Fan</surname>
<given-names>C-J</given-names>
</name>
</person-group> (<year>1990</year>) <article-title>Computing the block triangular form of a sparse matrix</article-title>. <source>ACM Transactions on Mathematical Software</source> <volume>16</volume>: <fpage>303</fpage>–<lpage>324</lpage>.</citation>
</ref>
<ref id="bibr31-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Preis</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>1999</year>) <article-title>Linear time <inline-formula id="inline-formula323-1094342012452893">
<mml:math id="mml-inline323-1094342012452893">
<mml:mrow>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mn>2</mml:mn>
</mml:mfrac>
</mml:mrow>
</mml:math>
</inline-formula>-approximation algorithm for maximum weighted matching in general graphs</article-title>. In <source>16th Annual Symposium on Theoretical Aspects of Computer Science (STACS)</source>, pp. <fpage>259</fpage>–<lpage>269</lpage>.</citation>
</ref>
<ref id="bibr32-1094342012452893">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schenk</surname>
<given-names>O</given-names>
</name>
<name>
<surname>Wächter</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Hagemann</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Matching-based preprocessing algorithms for the solution of saddle-point problems in large-scale nonconvex interior-point optimization</article-title>. <source>Computer Optimization Application</source> <volume>36</volume>: <fpage>321</fpage>–<lpage>341</lpage>.</citation>
</ref>
<ref id="bibr33-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Schrijver</surname>
<given-names>A</given-names>
</name>
</person-group> (<year>2003</year>) <source>Combinatorial Optimization - Polyhedra and Efficiency</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr34-1094342012452893">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Yoo</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Chow</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Henderson</surname>
<given-names>K</given-names>
</name>
<name>
<surname>McLendon</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Hendrickson</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Catalyurek</surname>
<given-names>U</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>A scalable distributed parallel breadth-first search algorithm on BlueGene/L</article-title>. In <source>Proceedings of the 2005 ACM/IEEE Conference on Supercomputing (SC '05)</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>, p. <fpage>25</fpage>.</citation>
</ref>
</ref-list>
</back>
</article>