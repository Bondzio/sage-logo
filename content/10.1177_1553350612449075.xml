<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SRI</journal-id>
<journal-id journal-id-type="hwp">spsri</journal-id>
<journal-id journal-id-type="nlm-ta">Surg Innov</journal-id>
<journal-title>Surgical Innovation</journal-title>
<issn pub-type="ppub">1553-3506</issn>
<issn pub-type="epub">1553-3514</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1553350612449075</article-id>
<article-id pub-id-type="publisher-id">10.1177_1553350612449075</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Technology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>What Do Surgeons See</article-title>
<subtitle>Capturing and Synchronizing Eye Gaze for Surgery Applications</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Atkins</surname><given-names>M. Stella</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff1-1553350612449075">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Tien</surname><given-names>Geoffrey</given-names></name>
<degrees>MSc</degrees>
<xref ref-type="aff" rid="aff1-1553350612449075">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Khan</surname><given-names>Rana S.A.</given-names></name>
<degrees>MD</degrees>
<xref ref-type="aff" rid="aff2-1553350612449075">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Meneghetti</surname><given-names>Adam</given-names></name>
<degrees>MD</degrees>
<xref ref-type="aff" rid="aff2-1553350612449075">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Zheng</surname><given-names>Bin</given-names></name>
<degrees>MD, PhD</degrees>
<xref ref-type="aff" rid="aff3-1553350612449075">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-1553350612449075"><label>1</label>Simon Fraser University, Burnaby, British Columbia, Canada</aff>
<aff id="aff2-1553350612449075"><label>2</label>University of British Columbia, Vancouver, British Columbia, Canada</aff>
<aff id="aff3-1553350612449075"><label>3</label>University of Alberta, Edmonton, Alberta, Canada</aff>
<author-notes>
<corresp id="corresp1-1553350612449075">M. Stella Atkins, School of Computing Science, Simon Fraser University, 8888 University Drive, Burnaby, British Columbia, Canada, V5A 1S6. Email: <email>stella@cs.sfu.ca</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2013</year>
</pub-date>
<volume>20</volume>
<issue>3</issue>
<fpage>241</fpage>
<lpage>248</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Recording eye motions in surgical environments is challenging. This study describes the authors’ experiences with performing eye-tracking for improving surgery training, both in the laboratory and in the operating room (OR). Three different eye-trackers were used, each with different capabilities and requirements. For monitoring eye gaze shifts over the room scene in a simulated OR, a head-mounted system was used. The number of surgeons’ eye glances on the monitor displaying patient vital signs was successfully captured by this system. The resolution of the head-mounted eye-tracker was not sufficient to obtain the gaze coordinates in detail on the surgical display monitor. The authors then selected a high-resolution eye-tracker built in to a 17-inch computer monitor that is capable of recording gaze differences with resolution of 1° of visual angle. This system enables one to investigate surgeons’ eye–hand coordination on the surgical monitor in the laboratory environment. However, the limited effective tracking distance restricts the use of this system in the dynamic environment in the real OR. Another eye-tracker system was found with equally high level of resolution but with more flexibility on the tracking distance, as the eye-tracker camera was detached from the monitor. With this system, the surgeon’s gaze during 11 laparoscopic procedures in the OR was recorded successfully. There were many logistical challenges with unobtrusively integrating the eye-tracking equipment into the regular OR workflow and data processing issues in the form of image compatibility and data validation. The experiences and solutions to these challenges are discussed.</p>
</abstract>
<kwd-group>
<kwd>eye-tracking</kwd>
<kwd>eye–hand coordination</kwd>
<kwd>vigilance</kwd>
<kwd>human factors in surgery</kwd>
<kwd>cognition assessment</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1553350612449075" sec-type="intro">
<title>Introduction</title>
<p>Successful performance in surgery requires a surgeon to possess sophisticated skills for continual acquisition of visual information from complex and dynamic environments.<sup><xref ref-type="bibr" rid="bibr1-1553350612449075">1</xref>,<xref ref-type="bibr" rid="bibr2-1553350612449075">2</xref></sup> Quantifying the eye motion behavior as in Vickers’s studies on athletes provides information into mental readiness and expertise for performing various sport activities in the most competitive environment.<sup><xref ref-type="bibr" rid="bibr3-1553350612449075">3</xref></sup> Eye-tracking studies of anesthetists in a simulated operating room (OR) show that pupil size and heart rate are related to workload.<sup><xref ref-type="bibr" rid="bibr4-1553350612449075">4</xref></sup> Studies on laparoscopic surgeon’s eye motions reveal differences between novices and expert surgeons.<sup><xref ref-type="bibr" rid="bibr5-1553350612449075">5</xref>,<xref ref-type="bibr" rid="bibr6-1553350612449075">6</xref></sup> Richstone et al<sup><xref ref-type="bibr" rid="bibr5-1553350612449075">5</xref></sup> showed that eye metrics such as pupil diameter, blink rates, and fixation rates could be used as an objective measurement of surgical skill. They used Eyelink II (SR Research Ltd, Kanata, Ontario, Canada) head-mounted eye-trackers, which sample the wearer’s eye gaze at a high frequency of 250 times per second, to obtain eye gaze motion data in a simulated surgical environment and in the real OR, but did not record where the surgeon was actually looking.<sup><xref ref-type="bibr" rid="bibr5-1553350612449075">5</xref></sup> Others have shown that knowing where to look would improve overall surgical performance of residents more than by receiving verbal feedback from an instructor or just watching the video.<sup><xref ref-type="bibr" rid="bibr7-1553350612449075">7</xref></sup> We too are concerned with evaluating surgical performance.</p>
<p>Our long-term objective is to incorporate eye-tracking technologies into the field of surgical education in order to improve skills of surgeons performing image-guided surgical procedures. Laparoscopy is one type of image-guided surgery on the abdominal cavity, where the surgeon’s viewpoint is directed and controlled by an assistant through a laparoscope. We aim to prove the value of using eye-tracking videos of expert surgeons to teach residents where to look during key steps of laparoscopic cholecystectomy and maintain vigilance on patient safety while engaging in surgical tasks. We hypothesized that gaze tracking of the surgeon could prove useful, especially to provide objective measures of the surgeon’s vigilance in the OR and in measuring the surgeon’s eye–hand coordination on the operating scene.</p>
<p>To achieve our research goals, we required eye-trackers that could capture the appropriate information for the task. We found we needed 3 different eye-trackers: in laboratory studies to measure the vigilance of surgeons,<sup><xref ref-type="bibr" rid="bibr8-1553350612449075">8</xref><xref ref-type="bibr" rid="bibr9-1553350612449075"/>-<xref ref-type="bibr" rid="bibr10-1553350612449075">10</xref></sup> during real surgeries in the OR, and in follow-up laboratory studies.<sup><xref ref-type="bibr" rid="bibr11-1553350612449075">11</xref>,<xref ref-type="bibr" rid="bibr12-1553350612449075">12</xref></sup> We report on our experiences of using these eye-trackers with surgeons, and the difficulties in obtaining high-quality eye gaze data in the real OR setting.</p>
</sec>
<sec id="section2-1553350612449075">
<title>Background to Eye-Tracker Hardware</title>
<p>Most commercial eye-trackers rely on locating the reflection of an infrared beam of light from the subject’s eyes with an infrared-sensitive camera. The eye motion metrics calculated by the hardware contain samples over time of the gaze location in space, the corresponding fixations, the pupil diameter (if visible), and a parameter indicating if the eye gaze data are valid for one or for both eyes. Most eye-trackers require calibration before use, where the subject stares at known locations. Modern eye-trackers include head pose estimation to improve the accuracy of the gaze location, around 0.5° of visual angle, and gaze location samples are taken at 30 to 1000 Hz. There are 2 major types of eye-trackers: head-mounted eye-trackers, where the camera is mounted on eye glasses and the so-called “remote” eye-trackers, where the camera is fixed near a computer screen. An excellent review of eye-tracker hardware is given in Duchowski’s book.<sup><xref ref-type="bibr" rid="bibr13-1553350612449075">13</xref></sup></p>
<p>We use 3 different eye-trackers: a lightweight head-mounted eye-tracker (Locarna Systems, Inc, Victoria, British Columbia, Canada), a remote computer-attached remote eye-tracker (Tobii 1750, Tobii Technology AB, Danderyd, Sweden), and another remote tracker that is separable from a computer screen (Tobii X50, Tobii Technology AB). These eye-trackers are described in detail below.</p>
<p>For the 3 surgical applications described in this article, <xref ref-type="table" rid="table1-1553350612449075">Table 1</xref> summarizes the eye-tracker most suited to the research goals of the study.</p>
<table-wrap id="table1-1553350612449075" position="float">
<label>Table 1.</label>
<caption>
<p>Common Research Goals and Scenarios of Gaze Studies in the Surgical Environment and Recommendation for Choosing an Appropriate Eye-Tracker.</p>
</caption>
<graphic alternate-form-of="table1-1553350612449075" xlink:href="10.1177_1553350612449075-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Research Goal</th>
<th align="center">Scenario and Requirement</th>
<th align="center">Recommended Eye-Trackers</th>
</tr>
</thead>
<tbody>
<tr>
<td>Situation awareness/vigilance</td>
<td>Freedom to move head and eyes to scan objects around the operating room; low resolution on screen; sampling rate 30 Hz acceptable</td>
<td>Head-mounted, lightweight (like Locarna head-mounted)</td>
</tr>
<tr>
<td>Surgical performance in simulated tasks</td>
<td>Accurate gaze location on screen; subject at constant fixed distance about 60 cm to screen; sampling rate minimum 50 Hz</td>
<td>Remote eye-tracker, like Tobii 1750, or Tobii X50</td>
</tr>
<tr>
<td>Surgical performance during live surgical tasks</td>
<td>Accurate gaze location on screen; surgeon further than 70 cm from screen because of concern about patient safety; sampling rate minimum 50 Hz</td>
<td>Remote eye-tracker that has separable cameras that can be moved close to the surgeon, like Tobii X50</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section3-1553350612449075">
<title>Study 1: Surgeon’s Vigilance in Simulation Setting</title>
<sec id="section4-1553350612449075">
<title>Motivation</title>
<p>In study 1, we wanted to record how aware the surgeon was of the patient’s vital signs during a demanding laparoscopic surgery, and whether novices detected changes in the patient’s condition more frequently than experts.<sup><xref ref-type="bibr" rid="bibr9-1553350612449075">9</xref></sup> In this study, we were primarily interested in how frequently the surgeon was able to shift their focus of attention from the monitor displaying the surgery site to the monitor displaying the patient’s vital signs and were also interested to test the association between blinks and general workloads of surgeons assessed by a paper assessment instrument (National Aeronautics and Space Administration Task Load Index, NASA TLX) rather than their surgical performance, so we were not interested in details on where exactly they were looking on the surgical screen.</p>
<p>Therefore we could not use a camera integrated with the surgical display monitor, as that would not reveal the gazes on the vitals monitor. Furthermore, the user can move their head freely only using head-mounted eye-tracker systems, a strong requirement in this study (all remote eye-trackers allow only very small head movements of the subject, as with large movements, the sclera of the eye cannot be seen, and the gaze position is lost). So we selected the head-mounted lightweight eye-tracker (PT-Mini, Locarna Systems Inc, Victoria, British Columbia, Canada) to fulfill the task goal (<xref ref-type="fig" rid="fig1-1553350612449075">Figure 1</xref>).</p>
<fig id="fig1-1553350612449075" position="float">
<label>Figure 1.</label>
<caption>
<p>The Locarna PT-Mini head-mounted eye-tracker.</p>
</caption>
<graphic xlink:href="10.1177_1553350612449075-fig1.tif"/>
</fig>
</sec>
<sec id="section5-1553350612449075">
<title>Technology</title>
<p>The Locarna PT-Mini eye-tracker consists of 2 synchronized cameras recording video at 30 Hz. One camera capturing the scene of room can record the image at 720 × 480 resolution. The other camera aiming toward the wearer’s pupil records eye motion at 352 × 240 resolution (<xref ref-type="fig" rid="fig1-1553350612449075">Figure 1</xref>). The recorded video streams are postprocessed using Locarna’s Pictus software (Locarna System Inc, Victoria, British Columbia, Canada) to produce the eye gaze coordinates and fixation locations.</p>
<p>This eye-tracking setup involves a lightweight head-mounted Locarna eye-tracker to study participants’ gaze positions between a computer screen and nearby physical objects on different viewing planes. As we did not require very high sampling rates for this study, we chose to use the head-mounted eye-tracker, which samples at 30 Hz. We found that this sampling rate was sufficient to catch fixations of 80 ms or more on the vital signs display.</p>
<p>The fixation parameters used for this study are minimum duration of 3 video frames (100 ms) with a maximum radius of 40 pixels relative to the video frame recorded from the scene camera, which is equivalent to about 3.5° of visual angle. Moving fixations were handled using velocity-based algorithms similar to those described by Salvucci and Goldberg.<sup><xref ref-type="bibr" rid="bibr14-1553350612449075">14</xref></sup> Fixations produced from the Locarna Pictus software were annotated as gazing on the main laparoscopic display, the vitals display, or elsewhere. Consecutive fixations on the same item were then collapsed into a single extended dwell. Any change in which item is being dwelled upon is classified as a saccade to the new item.</p>
</sec>
<sec id="section6-1553350612449075">
<title>Main Findings</title>
<p>A total of 23 surgeons (13 experts and 10 novices) participated in the study in a laboratory setting using a computer-based simulator (SurgicalSim VR, METI Inc, Sarasota, FL), to create virtual cholecystectomy cases and a patient simulator for the patient. The setup and details are given elsewhere.<sup><xref ref-type="bibr" rid="bibr8-1553350612449075">8</xref></sup></p>
<p>Compared with expert surgeons, novices performed surgical tasks in the simulated cases with satisfactory results. However, they concentrated intently on the surgical task and performed few eye glances to the anesthesia monitor displaying the patient’s vital signs. In contrast, experts glanced more frequently at the anesthesia monitor and with longer fixation time.<sup><xref ref-type="bibr" rid="bibr9-1553350612449075">9</xref></sup> Furthermore, we identified that surgeons who blinked more frequently and with longer duration blinks, reported a lighter workload.<sup><xref ref-type="bibr" rid="bibr10-1553350612449075">10</xref></sup> These results show promise for using eye-tracking technology to measure surgeons’ vigilance during an operation. Eye-tracking observations can lead to inferences of surgeons’ behaviors for patient safety, which may surpass measures on surgeon’s technical skills.</p>
</sec>
<sec id="section7-1553350612449075">
<title>Technology Limitations</title>
<p>The quality of recording with head-mounted eye-trackers seemed variable for different wearers, leading to noisy data. The calibration process was potentially lengthy and intrusive, as the 2 cameras on the headgear had to be manually focused by twisting a barrel for each new wearer. Since the gaze data were only recorded for postprocessing, there was no way to detect and correct adverse events that occurred during recording, such as an instance of a sudden head movement that caused the position of the gear to shift on the wearer’s head.</p>
<p>Although the PT-Mini allowed us to track the simulated surgical environment directly from the wearer’s point of view, it did so at a relatively low resolution, which was insufficient for our future OR studies.</p>
</sec>
</sec>
<sec id="section8-1553350612449075">
<title>Study 2: Surgeon’s Eye–Hand Coordination in Laboratory Setting</title>
<sec id="section9-1553350612449075">
<title>Motivation</title>
<p>In study 2, our goal was to investigate the surgeon’s eye–hand coordination in performing laparoscopic tasks. Explicitly, we investigated the temporal relationship between the gaze locking on the target and initiation of hand movement toward the target. In a previous study, we showed that novices exhibit more tool-tracking rather than target-locking behaviors than expert surgeons, in a computer-based virtual training environment.<sup><xref ref-type="bibr" rid="bibr6-1553350612449075">6</xref></sup> In this study, we would like to see if such eye-tracking behavior also will be revealed in a more realistic, physical training environment. As a high-resolution image was required to locate the surgeon’s gaze on different surgical targets, we used the Tobii 1750 to display and record the surgeon’s gaze.</p>
</sec>
<sec id="section10-1553350612449075">
<title>Technology</title>
<p>The Tobii 1750 consists of a 17-inch 1280 × 1024 LCD monitor and 2 built-in infrared cameras at the bottom of the monitor to capture the operators’ eye gaze on the monitor, recording at 50 Hz (<xref ref-type="fig" rid="fig2-1553350612449075">Figure 2</xref>). With the Tobii 1750’s remote tracking ability, a surgeon’s eye motions can in theory be monitored unobtrusively while standing at a comfortable viewing distance, usually 60 to 70 cm. Surgeons do not need to wear special goggles, hence avoiding interference with surgical tasks. Surgical video images can later be played back with superimposed eye-tracking signals in high resolution.</p>
<fig id="fig2-1553350612449075" position="float">
<label>Figure 2.</label>
<caption>
<p>View of the setup of the training box and the Tobii 1750 eye-trackers.</p>
<p>The eye-tracker emitters are seen as 2 round purple lights mounted below the display. The display shows the task materials inside the box.</p>
</caption>
<graphic xlink:href="10.1177_1553350612449075-fig2.tif"/>
</fig>
<p>The surgical task was created with a standard laparoscopic training system (Laparoscopic Trainer, 3-D Technical Services, <bold>Franklin, OH</bold>), which consists of a closed box containing the task materials such as a peg board or suture-tying rubbers, illuminated with a camera, with the inside image displayed on a display monitor. Graspers are inserted through ports and held with handles outside the box. These are used to manipulate the materials inside the box, viewed on the display monitor. The subject performs the task standing up, as in live laparoscopic surgeries.</p>
<p>The task scene was captured with a TV tuner card (Hauppauge Computer Works, Inc, Hauppauge, NY) using a NTSC composite video connection at 352 × 288 pixels and was displayed directly on the Tobii 1750 using Clearview 2.7.0’s “external video” stimulus.</p>
</sec>
<sec id="section11-1553350612449075">
<title>Main Findings</title>
<p>Fourteen subjects were recruited to perform a simple laparoscopic task, including 9 subtasks. We found a consistent delay of about 100 ms between commencement of eye movement to the target and commencement of the tool movement to the target.<sup><xref ref-type="bibr" rid="bibr11-1553350612449075">11</xref>,<xref ref-type="bibr" rid="bibr12-1553350612449075">12</xref></sup> The target locking was initiated earlier in tasks that required a higher level of precision (ie, reaching and grasping the object) compared with low task requirements (ie, bringing home the instrument).</p>
</sec>
<sec id="section12-1553350612449075">
<title>Comment on Technology</title>
<p>The Tobii 1750 can be used directly to display and record the eye gaze of the operator of the training box on the display monitor, because the monitor can be easily arranged to be at about 60 cm distance from the operator in the laboratory setting.</p>
</sec>
</sec>
<sec id="section13-1553350612449075">
<title>Study 3: Recording Surgeon’s Gaze During Live Laparoscopic Surgeries in the Operating Room</title>
<sec id="section14-1553350612449075">
<title>Motivation</title>
<p>Knowledge gained from aforementioned studies in the laboratory provides an excellent foundation for our effort of recording eye-tracking of surgeon in the OR. To create high-quality eye-tracking videos for teaching surgical skills, we want to acquire precise data of the surgeon’s eye gaze on the surgical monitor during real laparoscopic surgeries. This is a necessary step to our goal of integrating eye-tracking into surgical educational programs aiming to improve surgeons’ vigilance and eye–hand coordination.</p>
</sec>
<sec id="section15-1553350612449075">
<title>Technology</title>
<p>Because of this requirement for high-resolution eye-tracking and the difficulty of sterilizing a tethered head-mounted device for the OR, we abandoned the Locarna PT-Mini and initially tried to use our remote eye-tracker, the Tobii 1750. In our laboratory setting, the Tobii 1750 worked well—the surgeon stood in front of the training box while looking at the monitor, and the eye gaze was recorded with valid gaze location signals.</p>
<p>However, we encountered many logistical difficulties when we moved to the actual OR. There was so much equipment around the patient (<xref ref-type="fig" rid="fig3-1553350612449075">Figure 3</xref>) that the monitor could not be moved over the patient. If the surgeon stood to one side of the patient, the distance to the monitor would be at least 150 cm—far beyond the usual 70 cm maximum recording range of the Tobii 1750. Therefore, we used a Tobii X50 eye-tracker, similar to the Tobii 1750 but with the camera separated from the display monitor.</p>
<fig id="fig3-1553350612449075" position="float">
<label>Figure 3.</label>
<caption>
<p>Setup in the operating room with the usual surgery display monitor and the eye-tracker camera below on an adjustable sliding shelf.</p>
</caption>
<graphic xlink:href="10.1177_1553350612449075-fig3.tif"/>
</fig>
<p>We placed the Tobii X50 on an adjustable shelf at a different location than the monitor (<xref ref-type="fig" rid="fig3-1553350612449075">Figure 3</xref>). We covered the shelf with a sterile drape to follow hospital regulations, and the cables were kept strictly out of the operating field. Once the initial setup had been completed, we were then able to slide the sterilized shelf and camera into position over the patient to perform the eye calibration and recording.</p>
</sec>
<sec id="section16-1553350612449075">
<title>Display Setup Challenges</title>
<p>In the typical OR scenario at our institution, the endoscopic video from a 90° field-of-view laparoscope is fed into a Stryker video capturing/processing unit, which is connected to the surgical display via a VGA (video graphics array) connection. The surgeon’s main display is a 19-inch LCD SV-2 monitor with 1280 × 1024 native resolutions (5:4 aspect ratio). This setup is shown schematically in <xref ref-type="fig" rid="fig4-1553350612449075">Figure 4</xref> as the blue connections.</p>
<fig id="fig4-1553350612449075" position="float">
<label>Figure 4.</label>
<caption>
<p>Stylized representation of the hardware connections.</p>
<p>The blue lines show our usual surgery connections, and the red lines show the extra eye-tracking connections.</p>
</caption>
<graphic xlink:href="10.1177_1553350612449075-fig4.tif"/>
</fig>
<p>For safety concerns in case of an eye-tracking personal computer (PC) failure, we were not allowed to interfere with the typical everyday usage, by inserting the eye-tracking PC into the regular connection loop. Therefore, when introducing the eye-tracking PC, the connections above are left unchanged, and we had to develop a parallel display path, shown as the red path in <xref ref-type="fig" rid="fig4-1553350612449075">Figure 4</xref>. The Stryker video router provides a NTSC S-video signal to the capture card of the eye-tracking PC, which we equipped with a Hauppauge HVR 1600 video capture card, at 720 × 480 input resolution (3:2 aspect ratio). The eye-tracking PC scaled the signal to the native resolution of the display monitor (5:4 aspect ratio), and sent the signal (with possible delays) to the display via a digital video interface (DVI) connection (<xref ref-type="fig" rid="fig4-1553350612449075">Figure 4</xref>).</p>
<p>The SV-2 monitor has a number of inputs on its rear panel and has controls at the front that can switch input modes at one touch of a button, so we could easily switch input signals between the VGA route direct from the laparoscope, or via the eye-tracker PC.</p>
<p>To avoid conversion delay problems or the disaster of lost signals, after we calibrated the surgeon using the DVI connection, we switched the monitor to its VGA input as soon as we started recording, so that the surgeon could operate using the usual interface.</p>
</sec>
<sec id="section17-1553350612449075">
<title>Scene-to-Screen Conversion</title>
<p>Having surgeon’s eye-tracking recorded in the OR with the surgical video, we also need to display videos to trainees with a teaching purpose. We found that the displayed image from the original VGA input differed from our resized DVI image, because of the differences in aspect ratio between the S-video capture card and the display monitor, so the view seen by the surgeons while operating was not necessarily the same view displayed by the eye-tracking PC for recording. Because the eye-tracking data are recorded under the assumption that the operator sees what is displayed from the PC, it was imperative to determine any differences between the image displayed directly from the Stryker unit and the image that had been additionally processed by the eye-tracking PC.</p>
<p>To solve this problem, we performed a test recording on calibration grids of known dimensions, and then scaled our eye gaze data to properly match what the surgeon saw on the display during the operation. Two printed grids were used to observe and measure the Stryker video unit’s scaling and display characteristics. A circular grid with several inscribed circles was used to observe the scaling method used by Stryker (<xref ref-type="fig" rid="fig5-1553350612449075">Figure 5a</xref>), and a grid with uniformly spaced concentric squares was used to estimate field-of-view adjustments made for different Stryker video output modes (<xref ref-type="fig" rid="fig5-1553350612449075">Figure 5b</xref>).</p>
<fig id="fig5-1553350612449075" position="float">
<label>Figure 5.</label>
<caption>
<p>Calibration grid samples: (a) circle pattern grid and (b) square pattern grid.</p>
</caption>
<graphic xlink:href="10.1177_1553350612449075-fig5.tif"/>
</fig>
<p>Nonuniform scaling of the image to fill the LCD panel’s dimensions was the main focus of this procedure. Since it was infeasible for us to determine exactly the nature of the video stream from the laparoscope before it reached the Stryker video unit, we used the display seen using the VGA connection as the baseline display mode.</p>
<p>Once the laparoscope and video equipment were set up, the circular calibration grid was placed in front of the laparoscope. A photograph of the LCD screen was taken for each of VGA and DVI inputs, as well as a frame grab of the S-video feed captured by the eye-tracking PC. The circular grid was replaced by the square grid without moving any other components and the process was repeated.</p>
<p>With the circular grid (<xref ref-type="fig" rid="fig5-1553350612449075">Figure 5a</xref>), it was observed visually that the circles became identically vertically elongated in the surgical display, using both Stryker direct VGA input, and DVI inputs via the eye-tracking PC software. However, the grid appeared circular in the captured video. When a screenshot of the captured video was nonuniformly scaled to the LCD’s native resolution using an image editing tool (horizontal resolution × 720/480, vertical resolution × 1024/480), the circle became similarly elongated. From this we concluded that the scaling performed by Stryker for the VGA display is the same as the scaling done by Tobii Clearview for the full-screen eye gaze stimulus over DVI.</p>
<p>Therefore, what the surgeon saw was what the eye-tracker software assumed the surgeon saw, so we did not need to correct any of the inputs because of this re-sizing effect.</p>
<p>Using the square grid, it was found that the horizontal field of view for the DVI and VGA inputs was slightly different. More specifically, the image captured from S-video and displayed using DVI had a wider field of view in the same display area, causing the image seen using DVI (and thus assumed to be seen by Clearview) to be horizontally compressed compared with using the VGA input. Using the square grid (<xref ref-type="fig" rid="fig5-1553350612449075">Figure 5b</xref>), we measured the DVI field width at the top of the screen to be approximately 137 mm from the center, and the field width of the VGA input was approximately 127 mm from the center. The vertical field of view was found to be the same for both inputs. Because of the narrower field of the VGA input, what is seen on the display during the operation is always a subset of the frame recorded in Clearview but is offset by a linear factor as the gaze moves to the periphery of the screen.</p>
<p>To make a correspondence between a Clearview-recorded pixel (DVI input) to the pixel stimulus seen on the display during the live operation (VGA input), the following transformation is applied to the recorded gaze point’s <italic>X</italic> coordinate:</p>
<p>
<disp-formula id="disp-formula1-1553350612449075">
<mml:math display="block" id="math1-1553350612449075">
<mml:mrow>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mrow>
<mml:mtext>VGA</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>X</mml:mi>
<mml:mrow>
<mml:mtext>DVI</mml:mtext>
</mml:mrow>
</mml:msub>
<mml:mo>−</mml:mo>
<mml:mn>360</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>×</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>127</mml:mn>
<mml:mo>/</mml:mo>
<mml:mn>137</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo>+</mml:mo>
<mml:mn>360</mml:mn>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-1553350612449075" xlink:href="10.1177_1553350612449075-eq1.tif"/>
</disp-formula>
</p>
<p>where 360 is the value of one half of the video capture’s horizontal resolution.</p>
<p>The data in Clearview’s internal data file cannot be altered. The transformation is thus used on Clearview-exported plaintext files in GZD (gaze data), FXD (fixation data), and CMD (combined gaze/fixation data), for our custom gaze analysis software.</p>
</sec>
</sec>
<sec id="section18-1553350612449075">
<title>Lessons Learned</title>
<p>Several types of eye-tracking systems are currently available for studying surgeons’ gaze during surgical procedures, but each has limitations, requiring a careful selection for different research objectives. When studying surgeons’ vigilance over the entire operating room, we can use head-mounted, lightweight eye-trackers to give more freedom to the surgeons. A custom purpose-built setup with a wireless capability would be appreciated in the OR, as the tethered cables of our head mounted eye-tracker can be a hazard in the live OR environment.</p>
<p>Although the Tobii eye-tracker is able to track eye gaze under small head movements, the operating surgeons (either out of habit or by necessity) often make large movements, causing Tobii to lose track of the surgeons’ eyes, leading to intervals of missing or unreliable data in our recordings. These events could be identified by adding a Webcam to the data collection, so we would have a view of the surgeon’s face and actions.</p>
<p>After overcoming these technical difficulties, we successfully recorded 11 cases in the OR with the surgical video plus the synchronized eye-tracking. We have created surgical videos augmented with the expert eye gaze, for future analysis and use in training, as such videos have recently been reported to be successful in surgery training.<sup><xref ref-type="bibr" rid="bibr7-1553350612449075">7</xref></sup></p>
</sec>
</body>
<back>
<ack><p>We thank the Canadian Natural Sciences and Engineering Research Council (NSERC), the Royal College of Physicians and Surgeons in Canada (RCPSC), and MITACS for funding this project.</p></ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared the following potential conflicts of interest with respect to the research, authorship, and/or publication of this article: This project has been funded by the NOSCAR (Natural Orifice Surgery Consortium for Assessment and Research) research grant in 2007.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article:</p>
<p>This project has been funded by the Canadian Natural Sciences and Engineering Research Council (NSERC), the Royal College of Physicians and Surgeons in Canada (RCPSC), and MITACS research grant.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1553350612449075">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anastakis</surname><given-names>DJ</given-names></name>
<name><surname>Hamstra</surname><given-names>SJ</given-names></name>
<name><surname>Matsumoto</surname><given-names>ED</given-names></name>
</person-group>. <article-title>Visual-spatial abilities in surgical training</article-title>. <source>Am J Surg</source>. <year>2000</year>;<volume>179</volume>:<fpage>469</fpage>-<lpage>471</lpage>.</citation>
</ref>
<ref id="bibr2-1553350612449075">
<label>2.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cuschieri</surname><given-names>A</given-names></name>
</person-group>. <article-title>Visual displays and visual perception in minimal access surgery</article-title>. <source>Semin Laparosc Surg</source>. <year>1995</year>;<volume>13</volume>:<fpage>209</fpage>-<lpage>214</lpage>.</citation>
</ref>
<ref id="bibr3-1553350612449075">
<label>3.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vickers</surname><given-names>JN</given-names></name>
</person-group>. <source>Perception, Cognition, and Decision Training: The Quiet Eye in Action</source>. <publisher-loc>Champaign, IL</publisher-loc>: <publisher-name>Human Kinetics</publisher-name>; <year>2007</year>.</citation>
</ref>
<ref id="bibr4-1553350612449075">
<label>4.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schulz</surname><given-names>CM</given-names></name>
<name><surname>Schneider</surname><given-names>E</given-names></name>
<name><surname>Fritz</surname><given-names>L</given-names></name><etal/>
</person-group>. <article-title>Eye tracking for assessment of workload: a pilot study in an anaesthesia simulator environment</article-title>. <source>Br J Anaesth</source>. <year>2011</year>;<volume>106</volume>:<fpage>44</fpage>-<lpage>50</lpage>.</citation>
</ref>
<ref id="bibr5-1553350612449075">
<label>5.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Richstone</surname><given-names>L</given-names></name>
<name><surname>Schwartz</surname><given-names>MJ</given-names></name>
<name><surname>Seideman</surname><given-names>C</given-names></name>
<name><surname>Cadeddu</surname><given-names>J</given-names></name>
<name><surname>Marshall</surname><given-names>S</given-names></name>
<name><surname>Kavoussi</surname><given-names>LR</given-names></name>
</person-group>. <article-title>Eye metrics as an objective assessment of surgical skill</article-title>. <source>Ann Surg</source>. <year>2010</year>;<volume>252</volume>:<fpage>177</fpage>-<lpage>182</lpage>.</citation>
</ref>
<ref id="bibr6-1553350612449075">
<label>6.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Law</surname><given-names>B</given-names></name>
<name><surname>Atkins</surname><given-names>MS</given-names></name>
<name><surname>Kirkpatrick</surname><given-names>A</given-names></name><etal/>
</person-group>. <article-title>Eye gaze patterns differentiate skill in a virtual laparoscopic training environment</article-title>. In <conf-name>ACM Proceedings of Eye Tracking Research and Applications (ETRA ‘04)</conf-name>; <year>2004</year>:<fpage>41</fpage>-<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr7-1553350612449075">
<label>7.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wilson</surname><given-names>MR</given-names></name>
<name><surname>Vine</surname><given-names>S</given-names></name>
<name><surname>Bright</surname><given-names>E</given-names></name>
<name><surname>Masters</surname><given-names>RS</given-names></name>
<name><surname>Defriend</surname><given-names>D</given-names></name>
<name><surname>McGrath</surname><given-names>JS</given-names></name>
</person-group>. <article-title>Gaze training enhances laparoscopic technical skill acquisition and multi-tasking performance: a randomized, controlled study</article-title>. <source>Surg Endosc</source>. <year>2011</year>;<volume>25</volume>:<fpage>3731</fpage>-<lpage>3739</lpage>.</citation>
</ref>
<ref id="bibr8-1553350612449075">
<label>8.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Tien</surname><given-names>G</given-names></name>
<name><surname>Atkins</surname><given-names>MS</given-names></name>
<name><surname>Zheng</surname><given-names>B</given-names></name>
<name><surname>Swindells</surname><given-names>C</given-names></name>
</person-group>. <article-title>Measuring situation awareness of surgeons in laparoscopic training</article-title>. In <conf-name>ACM Proceedings of Eye Tracking Research &amp; Applications (ETRA ‘10)</conf-name>; <year>2010</year>:<fpage>149</fpage>-<lpage>152</lpage>.</citation>
</ref>
<ref id="bibr9-1553350612449075">
<label>9.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>B</given-names></name>
<name><surname>Tien</surname><given-names>G</given-names></name>
<name><surname>Atkins</surname><given-names>MS</given-names></name><etal/>
</person-group>. <article-title>Surgeon’s vigilance in the operating room</article-title>. <source>Am J Surg</source>. <year>2011</year>;<volume>201</volume>:<fpage>667</fpage>-<lpage>671</lpage>.</citation>
</ref>
<ref id="bibr10-1553350612449075">
<label>10.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zheng</surname><given-names>B</given-names></name>
<name><surname>Jiang</surname><given-names>X</given-names></name>
<name><surname>Tien</surname><given-names>G</given-names></name>
<name><surname>Meneghetti</surname><given-names>A</given-names></name>
<name><surname>Panton</surname><given-names>ON</given-names></name>
<name><surname>Atkins</surname><given-names>MS</given-names></name>
</person-group>. <article-title>Workload assessment of surgeons: correlation between NASA TLX and blinks</article-title> [published online ahead of print <month>April</month> <day>24</day>, <year>2012</year>]. <source>Surg Endosc</source>. doi:<pub-id pub-id-type="doi">10.1007/s00464-012-2268-6</pub-id>.</citation>
</ref>
<ref id="bibr11-1553350612449075">
<label>11.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Tien</surname><given-names>G</given-names></name>
<name><surname>Atkins</surname><given-names>MS</given-names></name>
<name><surname>Zheng</surname><given-names>B</given-names></name>
</person-group>. <article-title>Measuring gaze overlap on videos between multiple observers</article-title>. In <conf-name>ACM Proceedings of Eye Tracking Research &amp; Applications (ETRA ‘12)</conf-name>; <year>2012</year>:<fpage>309</fpage>-<lpage>312</lpage>.</citation>
</ref>
<ref id="bibr12-1553350612449075">
<label>12.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Atkins</surname><given-names>MS</given-names></name>
<name><surname>Jiang</surname><given-names>X</given-names></name>
<name><surname>Tien</surname><given-names>G</given-names></name><etal/>
</person-group>. <article-title>Saccadic delays on targets while watching videos</article-title>. In <conf-name>ACM Proceedings of Eye Tracking Research &amp; Applications (ETRA ‘12)</conf-name>; <year>2012</year>:<fpage>405</fpage>-<lpage>409</lpage>.</citation>
</ref>
<ref id="bibr13-1553350612449075">
<label>13.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Douchowski</surname><given-names>AT</given-names></name>
</person-group>. <source>Eye Tracking Methodology: Theory and Practice</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2007</year>.</citation>
</ref>
<ref id="bibr14-1553350612449075">
<label>14.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Salvucci</surname><given-names>DD</given-names></name>
<name><surname>Goldberg</surname><given-names>JH</given-names></name>
</person-group>. <article-title>Identifying fixations and saccades in eye-tracking protocols</article-title>. In <conf-name>ACM Proceedings of Eye Tracking Research &amp; Applications (ETRA ‘00)</conf-name>; <year>2000</year>:<fpage>71</fpage>-<lpage>78</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>