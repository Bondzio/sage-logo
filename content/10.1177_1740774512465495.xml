<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">CTJ</journal-id>
<journal-id journal-id-type="hwp">spctj</journal-id>
<journal-id journal-id-type="nlm-ta">Clin Trials</journal-id>
<journal-title>Clinical Trials</journal-title>
<issn pub-type="ppub">1740-7745</issn>
<issn pub-type="epub">1740-7753</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1740774512465495</article-id>
<article-id pub-id-type="publisher-id">10.1177_1740774512465495</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Meta-analysis of clinical trial safety data in a drug development program: Answers to frequently asked questions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Berlin</surname><given-names>Jesse A</given-names></name>
<xref ref-type="aff" rid="aff1-1740774512465495">a</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Crowe</surname><given-names>Brenda J</given-names></name>
<xref ref-type="aff" rid="aff2-1740774512465495">b</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Whalen</surname><given-names>Edward</given-names></name>
<xref ref-type="aff" rid="aff3-1740774512465495">c</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Xia</surname><given-names>H Amy</given-names></name>
<xref ref-type="aff" rid="aff4-1740774512465495">d</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Koro</surname><given-names>Carol E</given-names></name>
<xref ref-type="aff" rid="aff5-1740774512465495">e</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kuebler</surname><given-names>Juergen</given-names></name>
<xref ref-type="aff" rid="aff6-1740774512465495">f</xref>
</contrib>
</contrib-group>
<aff id="aff1-1740774512465495"><label>a</label>Janssen Research &amp; Development, Titusville, NJ, USA</aff>
<aff id="aff2-1740774512465495"><label>b</label>Lilly Corporate Center, Eli Lilly and Company, Indianapolis, IN, USA</aff>
<aff id="aff3-1740774512465495"><label>c</label>Pfizer, Inc., NY, USA</aff>
<aff id="aff4-1740774512465495"><label>d</label>Amgen, Inc., Thousand Oaks, CA, USA</aff>
<aff id="aff5-1740774512465495"><label>e</label>Worldwide Epidemiology Department, GlaxoSmithKline, Collegeville, PA, USA</aff>
<aff id="aff6-1740774512465495"><label>f</label>CSL Behring GmbH, Marburg, Germany</aff>
<author-notes>
<corresp id="corresp1-1740774512465495">Jesse A Berlin, Janssen Research &amp; Development, 1125 Trenton-Harbourton Road, PO Box 200, Titusville, NJ 08560, USA. Email: <email>jberlin@its.jnj.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2013</year>
</pub-date>
<volume>10</volume>
<issue>1</issue>
<fpage>20</fpage>
<lpage>31</lpage>
<permissions>
<copyright-statement>© The Author(s), 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">The Society for Clinical Trials</copyright-holder>
</permissions>
<abstract>
<sec id="section1-1740774512465495">
<title>Background</title>
<p>Meta-analyses of clinical trial safety data have risen in importance beyond regulatory submissions. During drug development, sponsors need to recognize safety signals early and adjust the development program accordingly, so as to facilitate the assessment of causality. Once a product is marketed, sponsors add postapproval clinical trial data to the body of information to help understand existing safety concerns or those that arise from other postapproval data sources, such as spontaneous reports.</p>
</sec>
<sec id="section2-1740774512465495">
<title>Purpose</title>
<p>This article focuses on common questions encountered when designing and performing a meta-analysis of clinical trial safety data. Although far from an exhaustive set of questions, they touch on some basic and often misunderstood features of conducting such meta-analyses.</p>
</sec>
<sec id="section3-1740774512465495">
<title>Methods</title>
<p>The authors reviewed the current literature and used their combined experience with regulatory and other uses of meta-analysis to answer common questions that arise when performing meta-analyses of safety data.</p>
</sec>
<sec id="section4-1740774512465495">
<title>Results</title>
<p>We addressed the following topics: choice of studies to pool, effects of the method of ascertainment, use of patient-level data compared to trial-level data, the need (or not) for multiplicity adjustments, heterogeneity of effects and sources of it, and choice of fixed effects versus random effects.</p>
</sec>
<sec id="section5-1740774512465495">
<title>Limitations</title>
<p>The list of topics is not exhaustive and the opinions offered represent only our perspective; we recognize that there may be other valid perspectives.</p>
</sec>
<sec id="section6-1740774512465495">
<title>Conclusions</title>
<p>Meta-analysis can be a valuable tool for evaluating safety questions, but a number of methodological choices need to be made in designing and conducting any meta-analysis. This article provides advice on some of the more commonly encountered choices.</p>
</sec>
</abstract>
</article-meta>
</front>
<body>
<sec id="section7-1740774512465495" sec-type="intro">
<title>Introduction</title>
<p>Meta-analyses of clinical trials are increasingly used to identify and evaluate potential drug safety concerns. Examples of recent meta-analyses evaluating the safety profile of marketed drugs include one conducted on cardiovascular outcomes of the antidiabetes drug, rosiglitazone, and the assessment of suicidal ideation/suicide attempt of antidepressants in pediatric subjects [<xref ref-type="bibr" rid="bibr1-1740774512465495">1</xref><xref ref-type="bibr" rid="bibr2-1740774512465495"/><xref ref-type="bibr" rid="bibr3-1740774512465495"/>–<xref ref-type="bibr" rid="bibr4-1740774512465495">4</xref>].</p>
<p>Meta-analyses typically combine two or more independent trials (often with somewhat different designs and conducted for different reasons) into the same analytic framework. They may focus on producing a single-point estimate of a treatment effect, or the focus may be on explaining variability among study results. Although meta-analyses have been part of the medical literature for some time, these complexities lead to many questions that still arise during the process of planning and conducting them.</p>
<p>In an earlier article, Crowe <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr5-1740774512465495">5</xref>] presented an overview of the framework and planning of meta-analysis in a drug development program but did not provide details regarding practical issues arising during implementation. In this article, a major focus will be on common analytical topics, such as choice of studies to pool, effects of the method of ascertainment, use of patient-level data compared with trial-level data, the need (or not) for multiplicity adjustments, heterogeneity of effects and sources of it, and choice of fixed effects versus random effects. The emphasis of the article is on situations that arise during the course of a drug development program, mostly premarketing. However, in some situations, we also include discussion of published studies.</p>
<p>The article is written in question-and-answer format and is intended for readers interested in applying meta-analyses to clinical trial safety data. It does not assume experience with such statistical methods. These topics clearly do not represent an exhaustive list of all potential issues with meta-analysis of adverse events. The topics were selected initially based on the authors’ experience and by polling their coworkers. They reflect the most commonly asked questions in conducting meta-analyses of clinical trial safety data in drug development.</p>
<p>We also note here that the topics are not all covered in equivalent detail. Some questions lend themselves to a reasonably thorough discussion in this article, and others would fill an entire article (or more) by themselves. For the latter, we instead provide an overview and provide relevant references that can be used to find more detailed information.</p>
</sec>
<sec id="section8-1740774512465495">
<title>Question 1: what studies should be pooled/combined in the meta-analysis?</title>
<p>The terms ‘pool’ and ‘pooled analysis’ are only loosely defined in the literature. The term ‘pool’, for instance, can refer both to an electronic database comprising individual patient data (IPD) from trials or to a grouping of studies used to address a specific research question. In this article, we use the term ‘pool’ in the latter sense. Furthermore, we use the term ‘pooled analysis’ as a generic term that covers any type of combined analysis across more than one study, that is, from a simple, unstratified analysis to a sophisticated individual patient-level data meta-analysis. We generally recommend anyone doing a pooled analysis to strive for analyses that are stratified by ‘study’ [<xref ref-type="bibr" rid="bibr6-1740774512465495">6</xref>].</p>
<p>Several guidance documents have been written that provide advice on which studies to include in combined analyses. These include International Conference on Harmonization (ICH) M4E, the Food and Drug Administration’s (FDA) guidance on premarketing risk assessment, and the Council for International Organizations of Medical Sciences VI (CIOMS VI) report [<xref ref-type="bibr" rid="bibr7-1740774512465495">7</xref><xref ref-type="bibr" rid="bibr8-1740774512465495"/>–<xref ref-type="bibr" rid="bibr9-1740774512465495">9</xref>]. Decisions regarding what studies to combine depend greatly on the specific questions to be answered. Often there are several questions, and these might require different subsets of studies or subjects. Pools may be based on a particular type of control, such as placebo or active comparators (all or a subset of these). Other possibilities for pools include a particular dose route or regimen, concomitant therapy, or particular disease states. Subgroups of patients to be included in a pool may be based on particular age groups, geographies, ethnicity groups, or severity of disease. We emphasize the important point that all studies should be referenced and summarized to some degree, even if they are not part of the primary meta-analysis. This needs to be known in the interest of completeness and full transparency. The reader can then evaluate the decisions made in the conduct of the analysis and the effects of those decisions on the results and conclusions.</p>
<p>Typically, sponsors will want to evaluate all placebo-controlled studies for potential inclusion in a pooled analysis, as placebo-controlled studies are useful for isolating the effect of the new therapy. Another important pool might include all patients who received the investigational drug (as a single cohort without comparator group) – a useful pool for accounting for all events and estimating event rates for infrequent events, which can then be compared to external reference population rates. However, an important caveat when comparing to external population rates is that such comparisons are often limited by the availability of event rates for a specific subset of the population that is comparable to the trial population under consideration. For example, if the underlying disease increases the risk of a particular event, comparisons with an external reference could be biased against the study drug. Conversely, if enrollment criteria are such that high-risk patients are excluded from trials, the on-study rates could appear to be artificially low.</p>
<p>Evaluation of appropriateness for inclusion in a pool should include consideration of several features, such as dose, duration, methods of eliciting adverse events (active vs passive), and population, as it is generally most appropriate to combine data from studies that are similar (though certainly not identical) with respect to these features. We note, however, that such similarity may not be required for pooling, if the (adverse) effects of treatment don’t depend on the trial characteristics being considered. Studies that will often be excluded from a pool include phase I pharmacokinetic and pharmacodynamic studies (as they are typically of short duration and conducted in healthy subjects or in patients with incurable end-stage disease who have confounding symptoms) and studies that cannot or will not provide IPD (e.g., some academic research collaborative studies), when such data are required for the analyses. Note that limitation of an analysis to studies that provide patient-level data introduces the possibility that the included studies are no longer representative, in terms of results, of all studies. Additional analyses may be defined by subpools (beyond placebo-controlled studies and all treated patients, for example, long duration studies, or studies with active comparators). It is also sometimes important to define pools for analysis based on subject characteristics, for example, subgroups of subjects within a pool, depending on the questions to be answered.</p>
<p>As an example, suppose some studies (or arms) were conducted at a higher or lower dose than the sponsor is proposing for the marketing label. If the goal for those analyses is to characterize adverse events from labeled indications, it would be sensible to exclude these higher or lower dose arms from the main grouping of placebo-controlled studies. However, one might choose to combine the high-dose studies in a different pool to help assess what could happen in an overdose situation. Other aspects of dosing regimens may also be explored in separate analyses, for example, studies that use flexible dosing schemes, such as titrating dosage to treatment effect. In general, we recommend excluding dose arms that are lower than the proposed dose for marketing, as these may dilute the effects seen at the higher marketed dose. However, in some situations, events may occur in the lower dose studies that should not be ignored. Including low-dose and high-dose studies may help understand the dose–response relationship.</p>
<p>Another somewhat common situation is that a safety signal was detected during phase II studies that resulted in a change in ascertainment of a particular adverse event in phase III studies (e.g., an adjudication process was put in place or active elicitation of the event of interest was introduced). In this case, in addition to the main grouping of placebo-controlled studies, one might want to create a grouping of phase III studies for which the ascertainment of the event of interest was prespecified. This approach has two advantages: First, studies with consistent ascertainment can be analyzed together, and second, this approach excludes the studies that generated the hypothesis being tested [<xref ref-type="bibr" rid="bibr10-1740774512465495">10</xref>]. Although this approach addresses type I error that can be introduced by including the hypothesis-generating study or studies in the analysis, it sacrifices statistical power and discards data from what may have been studies in a narrow, closely monitored population, which may also be at differential risk due to exposure to the compound (compared with a broader population).</p>
<p>In situations in which the goal is to confirm, rather than simply to generate hypotheses, an a priori analysis plan is essential to facilitate the interpretation of the results. Ideally, the analysis plan should be finalized <italic>before</italic> results are available for the trials that will be included. This ability (requirement, really) to plan has become the rule in the setting of cardiovascular safety trials that are now being conducted to support development of new therapies for type 2 diabetes. In this context, sponsors are now planning a meta-analysis of premarketing (phase III) efficacy trials and from various combinations of pre- and postmarketing cardiovascular outcomes trials. The aim of these analyses is to rule out an unacceptable increase in cardiovascular risk compared with standard treatments. The relative risks to be ruled out (1.8 to get approval, 1.3 after approval) are prespecified in the analysis plan before results are available from any of the trials to be included in the meta-analysis. Appropriate control over type I error is generally built into these plans [<xref ref-type="bibr" rid="bibr11-1740774512465495">11</xref>].</p>
<p>Publication bias is a well-established phenomenon that can affect the validity of summary estimates of treatment effects based solely on published literature. It will generally <italic>not</italic> be relevant to meta-analyses of a drug that is in development with no approved indications but will be a concern for marketed products. A full discussion of this issue is well beyond the scope of this article, but specifically for the purpose of safety assessment, another challenge is that some trials may simply be ‘silent’ on the question of adverse effects. Studies may also have been done that are too short to assess the occurrence of some common events that occur only later in follow-up or are too small to observe less common events. Pools of studies that avoid these potential biases are also useful to examine.</p>
<p>Decisions regarding what to pool should be clearly stated in a Program Safety Analysis Plan (PSAP) or its equivalent, including the rationale for including/excluding studies [<xref ref-type="bibr" rid="bibr5-1740774512465495">5</xref>]. Careful a priori documentation of pooling choices together with justification of the choices will avoid concerns about ‘cherry picking’.</p>
<p>One way of creating an analysis pool would be to include only studies that are very similar, in order to remove heterogeneity to help reduce variability in the resulting estimates. On the other hand, heterogeneity of designs and populations could be embraced by being more broadly inclusive of studies, and could then be analyzed to inform the clinical questions of interest (see question 5). In the context of such exploratory analyses, some formal structure is helpful. <xref ref-type="table" rid="table1-1740774512465495">Table 1</xref> presents a schematic that can be used as a concise overview of important study features. The schematic shows, for each study, the doses and controls used in it (indicated with an X). It shows the basic layout of exposures and can include other study features, such as study duration and disease state. Such information can guide what studies to combine. For example, if one has questions regarding certain doses or if only studies of a sufficient duration will capture an event with longer latency, such a table will help to readily identify appropriate and inappropriate studies for such analysis. This information also allows an assessment of how one study-level factor might be confounded with another. For example, <xref ref-type="table" rid="table1-1740774512465495">Table 1</xref> makes it clear that head-to-head comparisons of doses, as well as direct comparisons of the two controls, are available only in study 2. <xref ref-type="table" rid="table1-1740774512465495">Table 1</xref> is meant only as an example of how one could create a logical framework for subsequent analyses. Many other aspects of the design and conduct of trials should also be carefully examined in determining the appropriateness of which trials to pool. A recent publication addressed many of these aspects [<xref ref-type="bibr" rid="bibr12-1740774512465495">12</xref>].</p>
<table-wrap id="table1-1740774512465495" position="float">
<label>Table 1.</label>
<caption><p>Schematic of study features. Additional features can be included as needed.</p></caption>
<graphic alternate-form-of="table1-1740774512465495" xlink:href="10.1177_1740774512465495-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Study</th>
<th align="left">Dose A</th>
<th align="left">Dose B</th>
<th align="left">Control Y</th>
<th align="left">Control Z</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>X</td>
<td/>
<td>X</td>
<td/>
</tr>
<tr>
<td>2</td>
<td>X</td>
<td>X</td>
<td>X</td>
<td>X</td>
</tr>
<tr>
<td>3</td>
<td/>
<td>X</td>
<td/>
<td>X</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>An additional topic of potential interest in the evaluation of safety studies is the use of indirect comparisons, implemented as network or mixed-treatment meta-analyses. A network meta-analysis, for example, has been conducted to investigate cardiovascular safety of nonsteroidal anti-inflammatory drugs [<xref ref-type="bibr" rid="bibr13-1740774512465495">13</xref>]. These methods also could be used to study dose effects when different doses have been used in different studies [<xref ref-type="bibr" rid="bibr14-1740774512465495">14</xref><xref ref-type="bibr" rid="bibr15-1740774512465495"/>–<xref ref-type="bibr" rid="bibr16-1740774512465495">16</xref>]. We have chosen to retain a focus on direct comparisons in the context of this particular article, just to avoid making the article unwieldy.</p>
</sec>
<sec id="section9-1740774512465495">
<title>Question 2: how does the method of ascertainment impact the quality of a meta-analysis?</title>
<p>The method of ascertainment of a safety outcome (or harm) directly impacts the response of patients and/or investigators and, therefore, also the observed event rates (CIOMS VI) [<xref ref-type="bibr" rid="bibr9-1740774512465495">9</xref>]. Consequently, differences in the methods of ascertainment, for example, passive collection of adverse events versus active elicitation with a specific data collection form for that particular event, are one potential cause of heterogeneity of observed incidence rates across trials. While a comparison of study-specific incidence rates for signs of ascertainment difference is recommended (FDA; Guidance for Industry and Premarketing Risk Assessment) [<xref ref-type="bibr" rid="bibr8-1740774512465495">8</xref>], adequate planning and standardization across trials is important to reduce heterogeneity. We note, though, that variability in baseline (control) rates does not necessarily translate into variability in <italic>relative</italic> rates.</p>
<p>In some instances, the assessment of the importance of an adverse event may change during development as a result of newly emerging information. These changes may trigger the need to collect more detailed information, for example, for an adverse event of special interest. For example, in trials of drugs that cross the blood–brain barrier, prospective use of a tool to assess suicidality could be beneficial, rather than relying on post hoc identification of possibly related codes for passively collected adverse events. Careful consideration needs to be given to the approaches to coping with these changes. Post hoc adjudication of previously collected event information may be considered as one method to reduce bias in the context of a meta-analysis. Such post hoc adjudication was used in the meta-analyses of suicidality cited earlier [<xref ref-type="bibr" rid="bibr2-1740774512465495">2</xref><xref ref-type="bibr" rid="bibr3-1740774512465495"/>–<xref ref-type="bibr" rid="bibr4-1740774512465495">4</xref>]. Strict criteria were applied using previously collected data. However, the value of post hoc adjudication may be limited because important, detailed clinical information may be missing. If post hoc adjudication is necessary, we recommend utilizing an external, independent adjudication committee that is blinded to the treatment assignment and charged with adjudicating adverse events of special interest across trials in a development program. Sometimes, a signal is identified in phase II studies, and both standardized data collection (ascertainment) and adjudication are prospectively planned for phase III studies. Whether to adjudicate phase II trials retrospectively will depend on whether they are considered as hypothesis generating or hypothesis testing in the meta-analysis context [<xref ref-type="bibr" rid="bibr10-1740774512465495">10</xref>]. Both the methods of ascertainment and the approaches to address differences should be documented in the PSAP, along with identifying a process, if any, for adjudication by an independent committee.</p>
</sec>
<sec id="section10-1740774512465495">
<title>Question 3: what are the advantages of using IPD (vs aggregate patient data)?</title>
<p>Many published meta-analyses are based only on aggregate-level summary data, often drawn from published studies. In contrast, there is growing attention being paid to the potential advantages of using patient-level data. In fact, both individual-level and aggregate-level approaches should be considered for meta-analyses, as most of the important analytic principles, for example, stratification by study, apply to both approaches. A useful (but not the only) approach to distinguishing between the two approaches is to refer to meta-analyses based on IPD as ‘IPD meta-analyses’, and to those based on aggregate patient (APD) data as ‘APD meta-analyses’. Again, these abbreviations are just a convenient way to make the distinction clear in a succinct way.</p>
<p>For analyses that do not <italic>require</italic> patient-level data, including all relevant studies (those for which IPD data are available and those with only APD) improves precision. Furthermore, including all relevant studies may also reduce bias that could be introduced by limiting the analysis to those where patient-level data are available. Of course, this recommendation to include all studies only applies if the studies are addressing sufficiently similar clinical questions and have compatible endpoint definitions, and so on.</p>
<p>In general, access to patient-level data provides greater flexibility and is desirable compared with using only summary-level information. Whatever can be done in an APD meta-analysis can also be done with an IPD meta-analysis, but the converse is not true. One might wonder why IPD are not always utilized in a drug development program. There are two main reasons. The first is that the integration that is required to provide the database is labor intensive and needs substantial planning, especially if done in retrospect. The second is that sometimes summary statistics may be the only information available for some studies of interest to the analysts, including the sponsor. For example, studies of a new therapeutic approach may have been conducted by an academic cooperative group that does not share patient-level data, or the drug of interest may have been included as an active control by another sponsor. In some situations, for example, when there is no need to reevaluate any aspects of the individual-level data (e.g., calculating time-to-event, or recoding adverse events), and when the question being addressed does not involve subgroups defined by patient characteristics, an APD meta-analysis will give the same answer as the corresponding IPD meta-analysis [<xref ref-type="bibr" rid="bibr17-1740774512465495">17</xref>].</p>
<p>Even under the assumption of careful and thorough standards management, an evolution of standards and, therefore, differences among trials are to be expected during the course of the development of any drug. As a simple example, we consider changes and updates to coding systems and dictionaries. When adverse event information is reported based on, for example, Medical Dictionary for Regulatory Activities (MedDRA), the same term can be mapped to a different preferred term, and/or differences at higher levels of the hierarchy may occur. Access to IPD allows mapping all data to a common version of MedDRA and, therefore, will increase consistency of terminology and definitions across trials. Generally speaking, access to IPD permits adjustment for differences among trials by creating common variables. Standard medical definitions of specific adverse events should be used, when possible. In the absence of a commonly accepted definition, it is important to meet with the relevant regulatory authorities to get agreement on a nonstandard definition, including a clear specification of the operational aspects of actually implementing the definition. In some situations, events may have been reported differently in different trials. For example, age categories for stratification of results may have been defined using different category boundaries, or different threshold hemoglobin values may have been used to define ‘anemia’. In such situations, access to patient-level data allows standardization across trials that would not have been possible based on published (or other aggregate-level) data alone. Definitions of censoring events may also be standardized using IPD, again not an option when limited to aggregate-level data.</p>
<p>In addition, careful consideration should be given to specification of a ‘common’ set of patient-level covariates, with the same definitions, so subgroup analyses across trials can be performed. As an example of the critical value of such data, we consider events with long latency, such as certain cancers. In such cases, detailed medication and procedural usage information during the period from study drug discontinuation to follow-up can help reduce the potential for uncontrolled confounding, or at least identify potential postrandomization confounders between study drug and the event, which can then be controlled analytically.</p>
<p>Access to IPD permits a number of analyses that would not typically be possible based solely on aggregate-level data. With patient-level data, it is possible to define outcomes based on combinations of variables that each define specific events but that collectively may indicate a common adverse event mechanism. As an example, one might define a composite variable by including as an adverse event a combination of weight loss or appetite reduction. One might also want to check on concordance of different measures of the same underlying problem (e.g., weight loss as an adverse event or a decrease in measured body weight beyond a specified threshold). Finally, post hoc analyses of outcomes that require adjudication can sometimes be derived, as in the case of suicide event grading according to Columbia Classification Algorithm of Suicide Assessment (C-CASA criteria) [<xref ref-type="bibr" rid="bibr18-1740774512465495">18</xref>].</p>
<p>Patient-level data also permit defining time-to-event variables and conducting the appropriate analysis, which again would not be easily accomplished using aggregate-level data, particularly when those data appear in the form of simple proportions. With patient-level data, Kaplan–Meier time-to-event graphs can be utilized to help identify events with changing hazard rates over time (e.g., there could be a separation in cardiovascular events in the first few months that goes away with a longer follow-up period), or events that have a latent period (e.g., cancer). Time-to-event analyses, such as Cox proportional hazards models, can be used to estimate incidence at a time after treatment starts and hazard ratios with confidence intervals (CIs) and corresponding p-values can be calculated for treatment effects. Even in situations in which there are published hazard ratios with variance estimates (which could be combined using standard techniques), the assumptions that are needed to make valid inferences, in particular, noninformative censoring and proportional hazards, might not be met. In some situations, access to patient-level data might permit examination of relevant assumptions (e.g., proportional hazards), but even patient-level data may not be sufficient to confirm that censoring is noninformative.</p>
<p>Often, patient-level data can be used to perform other types of exposure-adjusted analyses in which adverse event incidence rates are calculated per patient-years of follow-up. This analysis takes the duration of drug exposure into account and is most appropriate for events that occur at a constant rate over the study period. This assumption may, of course, not always be valid. For example, in an analysis of the risk of cancer deaths with an immunosuppressive agent used in rheumatoid arthritis patients, 50,000 patients followed for 5 weeks is not as informative (for the long-term risk) as 1000 patients followed for 5 years, despite the fact that the number of patient-years of follow-up will be similar for the two situations. There are various reasons for using events/person-year. One is the sheer volume of safety analyses needed in a drug development program. Simplifications can be made to events/person-year, but not for Cox regression. Constant baseline hazard will often be a realistic assumption, given that many trials are of relatively short duration. Constant treatment effect (proportional hazards) is often unrealistic, and standard Cox models do not address that situation either, at least not without additional modeling complexity.</p>
<p>Importantly, the analysis of IPD permits estimating effects for subgroups of patients defined by subject-level characteristics. (We note, though, that if subgroup results had been reported in the individual studies it might have been possible to combine them in a meta-analysis.) This flexibility also matters as the program accumulates studies and needs to look at certain subgroups as unexpected events emerge. As an example, consider an analysis by Szczech <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr19-1740774512465495">19</xref>], in which the authors were able to examine the specific effects of induction therapy (essentially, prophylactic treatment to prevent rejection of transplanted kidneys) in subgroups of patients at high risk for kidney transplant failure. Although not strictly a ‘safety’ question, this example illustrates the potential importance of the flexibility provided by access to patient-level data. In this situation, the authors were able to detect an interaction between treatment and a patient-level characteristic that was not possible to identify with only aggregate-level data. One of the patient characteristics of special interest was the panel reactive antibody (PRA) level, an indicator of immune system sensitization. After 2 years of follow-up, the effect of induction therapy differed in sensitized and nonsensitized patients (p = 0.03 for the interaction, using IPD). The rate ratio at 2 years was 0.12 (CI: 0.03–0.44, p = 0.002) in sensitized patients (85 patients in total, with 15 failures) and 0.74 (CI: 0.50–1.09, p = 0.13) in nonsensitized patients (511 patients in total, with 100 failures). This interaction was still significant at 5 years of follow-up (p = 0.009 for the interaction), with a rate ratio of 0.20 (CI: 0.09–0.47, p &lt; 0.001) in sensitized patients (85 patients in total, with 33 failures) and 0.97 (CI: 0.71–1.32, p &gt; 0.2) in nonsensitized patients (510 patients in total, with 163 failures). Importantly, this interaction was <italic>not</italic> consistently detected in analyses performed at the aggregate level, using the ‘percent of sensitized patients’ as a study-level characteristic, in an attempt to capture the individual patient characteristic. The inability to identify this interaction from the aggregate-level analysis is an example of ecological bias [<xref ref-type="bibr" rid="bibr19-1740774512465495">19</xref>,<xref ref-type="bibr" rid="bibr20-1740774512465495">20</xref>]. Although it might be important to confirm such subgroup results in an independent data set, the patient-level analyses strongly suggest that induction therapy is effective in the 14% of patients who are sensitized. If confirmed, these results could mean that induction therapy could be targeted to the group in which it is highly effective, while avoiding needless treatment and potential toxicity in other patients.</p>
<p>An additional caveat is needed in performing subgroup analyses. In some situations, an imbalance in covariates at baseline may be introduced due to differential dropout in the drug and control groups, especially in subgroups because of the smaller numbers. Even if data on the known and measured imbalanced factors are available, the distribution and potential imbalance of unmeasured pertinent risk factors in study subgroups would remain unknown with unpredictable impact on the study findings.</p>
<p>The difficulties with aggregate-level data for the detection of subgroup differences are not limited to ecological bias. Lambert <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr21-1740774512465495">21</xref>], in a simulation study, point out that the aggregate-level analyses often have low statistical power to detect interactions (also known as effect modification). Similarly, Schmid <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr22-1740774512465495">22</xref>] performed analyses of two datasets and showed that subgroup differences of interest were detected by patient-level analyses but not by aggregate-level analyses [<xref ref-type="bibr" rid="bibr22-1740774512465495">22</xref>]. In their first analysis, treatment effects were homogeneous across studies, and therefore, metaregression identified no interactions. Analysis of IPD from the same studies discovered an important modifier of treatment effect. Their second investigation found metaregression to be effective for detecting treatment interactions with study-level factors (no patient-level data were available for this analysis) in meta-analyses with at least 10 studies, with heterogeneous treatment effects, or significant overall treatment effects.</p>
<p>Of note, having access only to aggregate-level data does not rule out exploration of treatment-effect modifiers. Examining potential sources of heterogeneity of study findings, such as dose, duration, or other study-level factors, can still shed light on important clinical questions.</p>
</sec>
<sec id="section11-1740774512465495">
<title>Question 4: should we adjust for multiple looks and/or multiple endpoints in the context of meta-analysis?</title>
<p>Adjustments should be considered and, if agreed upon for some prespecified events of interest, written into the PSAP. Several fundamental issues arise when attempting to make statistical adjustments for repeated looks at safety data. First, we have multiplicity from multiple meta-analyses over time as studies finish and are added to the cumulative database; see Hu <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr23-1740774512465495">23</xref>] for more on this issue. Second, we don’t know how many different types of events will occur in clinical trials during the course of development and postapproval marketing of the product (and, thus, how many variables will be compared between groups).</p>
<p>We believe it’s necessary to take the objective of the analysis into account when deciding on the type of, and need for, multiplicity adjustment. The approaches for different types of multiplicity adjustment are determined by whether hypotheses about adverse events are prospectively defined. As described in the Safety Planning, Evaluation, and Reporting Team (SPERT) manuscript [<xref ref-type="bibr" rid="bibr5-1740774512465495">5</xref>], so-called tier 1 events are those events for which a prespecified hypothesis has been defined (e.g., to rule out an effect of a certain magnitude for assessing a particular risk, that is, a noninferiority test) as opposed to those events for which a hypothesis has not been prespecified (e.g., analyze all adverse events without knowing the number of types of adverse events in advance). Generally, we recommend that one considers performing formal adjustment for multiple looks for tier 1 events and for multiple endpoints for other events, with caveats noted below.</p>
<p>As an example of a tier 1 event, with diabetes treatments and cardiovascular events, a sponsor has specific requirements from the FDA regarding what to test [<xref ref-type="bibr" rid="bibr24-1740774512465495">24</xref>]. (Specifically, the sponsor needs to rule out a relative risk of 1.8 for conditional approval, and 1.3 for final approval). The confidence level for that <italic>specific</italic> outcome, as a tier 1 event, may need to be adjusted for multiple looks, but that adjustment can be considered to be separate from non-tier 1 events because it needs to be met for the drug to move forward. It is an event of interest that is important regardless of the specific side effect profile of the study drug and has a role analogous to a primary analysis in the efficacy setting.</p>
<p>These principles are general. In specific situations, we have to bear in mind the types of tests that we may do. For some events of interest, we may want to show that they occur at a lower rate on study drug than in the control group, for example, tests for differences between groups, such as gastrointestinal (GI) side effects for a Cox 2 inhibitor. Because the tests look for a benefit relative to the control, standard multiplicity adjustments should pertain in their usual form. One can conceive of this testing for a benefit as having the goal of essentially ‘ruling out’ a relative risk as high as 1.0. This is similar, in principle, to the noninferiority situation, in which, the goal is to rule out an <italic>increase</italic> in risk of a certain magnitude (as in the diabetes setting, discussed above). In both these situations, the upper bound of the CI is the relevant quantity. Adjustment makes the CI wider, making it more difficult to meet the goal. Looking in the other direction, for medically very important events, that is, mortality, a finding of <italic>increased</italic> risk that is nominally statistically significant might be viewed as important, <italic>regardless of the results of any type of adjustment for multiplicity</italic>. If no adjustment is made, the sponsor needs to weigh the lack of adjustment against the effects of making an incorrect decision based on what may be a false positive for an event that will stop the program.</p>
<p>In general, the issue of multiple looks is a controversial one, especially in drug safety, where adjustment for multiplicity is not commonly done. The decision of whether to adjust involves certain pragmatic issues. If 200 drug/adverse event associations are tested with a two-sided 0.05-level test, on average 5 might show a statistically significant drug effect by chance. However, if 10 associations were found to be significant in a given trial, knowing which 5 are drug related and which ones are due to chance is difficult. Adjustment will not affect the point estimate, but will affect our perception of the strength of evidence associated with a given risk estimate. However, in many situations, evaluating safety signals is so hampered by low power, lack of a priori definitions, and numerous sources of extraneous variability that it will often be helpful to recall that there is value in trying not to miss a safety signal, as long as it is recognized that initial detection is not always the same as proving that a given adverse event is definitively related to a given drug. In other words, our concern might be better directed at reducing false negative findings in drug safety given the known limitations of our tools.</p>
<p>Non-tier 1 events are events that do not have a prespecified hypothesis. The goal of analyzing these events is risk quantification or signal detection, that is, to screen for events that merit further investigation. For small studies, adjusting for multiple events may not be very important. However, as the database size increases, adjusting for multiple endpoints may be very helpful for the team, in order to help them sort out what is likely to be a true risk versus what is less likely. Statistical methods that might be useful include the ‘Double False Discovery Rate’ multiplicity adjustment procedure of Mehrotra and Heyse [<xref ref-type="bibr" rid="bibr25-1740774512465495">25</xref>] and the Bayesian hierarchical model of Berry and Berry [<xref ref-type="bibr" rid="bibr26-1740774512465495">26</xref>]. Xia <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr27-1740774512465495">27</xref>] expanded the Berry and Berry method by comparing the performance of different methods and providing some useful tools for implementation.</p>
<p>The preceding considerations affect how a sponsor approaches multiplicity in the development setting and need attention in the drug program’s PSAP. After approval, many of the same issues arise, especially if there are phase IV commitments. Additional sources of potential events of interest come in the postapproval setting through, for example, observational studies, spontaneous reports, medical literature, and third-party findings. Their impact on the PSAP and multiplicity adjustments needs assessment on a case-by-case basis.</p>
</sec>
<sec id="section12-1740774512465495">
<title>Question 5: what is heterogeneity and what are sources of heterogeneity?</title>
<p>Heterogeneity refers to differences among studies and/or study results. Heterogeneity can generally be classified in three ways: clinical heterogeneity, methodological heterogeneity, and statistical heterogeneity [<xref ref-type="bibr" rid="bibr28-1740774512465495">28</xref>]. Clinical heterogeneity refers to differences among trials in their patient selection (e.g., disease conditions under investigation, eligibility criteria, patient characteristics, or geographic differences), interventions (e.g., duration, dosing, and nature of the control), and outcomes (e.g., definitions of endpoints, follow-up duration, cut-off points for scales). The methodological heterogeneity refers to the differences in study design (e.g., the mechanism of randomization) and in study conduct (e.g., allocation concealment, blinding, extent and handling of withdrawals, and loss to follow-up or analysis methods). Decisions about what constitutes clinical heterogeneity and methodological heterogeneity do not involve any calculation and are based on judgment. On the other hand, statistical heterogeneity represents a notion that individual studies may have results that are not numerically consistent with each other, and the variation is more than what is expected on the basis of sampling variability alone. Statistical heterogeneity may be caused by known clinical and methodological differences among trials, by unknown trial (clinical or methodological) characteristics, or it may be due to chance. <xref ref-type="fig" rid="fig1-1740774512465495">Figure 1</xref> illustrates the variability of treatment effects for a binary outcome from study to study in the two cases of limited (left side) and large (right side) variation across studies. How these different types of variation affect the choice of statistical model is described in more detail under question 6. Essentially, homogeneous data (left side) may be analyzed using methods that require a broader set of model assumptions.</p>
<fig id="fig1-1740774512465495" position="float">
<label>Figure 1.</label>
<caption><p>Effect size variation across studies.</p>
</caption>
<graphic xlink:href="10.1177_1740774512465495-fig1.tif"/>
</fig>
<p>The main ways of detecting or estimating statistical heterogeneity are graphical, for example, forest plots (see above) or statistical (e.g., Cochran’s Q, I<sup>2</sup>, test of interaction). The left graph in <xref ref-type="fig" rid="fig1-1740774512465495">Figure 1</xref> shows a similar effect for each study but with varying degrees of certainty (precision) as seen from the different lengths of the CIs for each study. The figure on the right has a similar level of variability in the CI widths but a much greater variation in estimated differences from study to study. In this second case, fixed-effect models, which assume similar effects across studies, will likely not suffice, which leads to consideration of random-effects models, or of an analysis that somehow models sources of heterogeneity. Clearly, heterogeneity statistics should not be viewed in a vacuum. Certainly, when all studies are on the same side of the null, but there is heterogeneity in terms of magnitude of effect, is different from variability around the null, especially if there is an apparent bimodal distribution.</p>
<p>It is important to be mindful that absence of statistical heterogeneity does not necessarily mean absence of clinical heterogeneity or the absence of differential treatment effects, in the following sense. The <italic>global</italic> (i.e., omnibus) statistical test of heterogeneity may fail to detect differing treatment effects due to lack of power, especially when the number of studies included is small or the event is rare. (Specifically, we’re referring to a test such as Cochran’s Q, which has degrees of freedom equal to the number of studies minus 1.) Alternatively, when the included studies are large, the statistical test may detect small (not clinically important) among-study differences. The guiding principle should be to evaluate the association of clinical differences among studies with treatment effects, rather than to rely on an overall global statistical test of heterogeneity. Tests of <italic>specific</italic> trial characteristics (e.g., disease severity) may yield statistically significant results, indicating an association between treatment effects and that specific characteristic, even when a global test fails to reject the homogeneity assumption. Importantly, clinical heterogeneity may not always result in statistical heterogeneity. In some situations, consistency of results across diverse studies may represent robust, generalizable treatment effects.</p>
</sec>
<sec id="section13-1740774512465495">
<title>Question 6: is it sufficient to use fixed-effect models when combining studies or do we need to consider random-effects models?</title>
<p>Fixed-effect models assume that the true underlying population effect is the same in each study, that is, differences among the studies are not due to systematic differences among studies but rather due to sampling variability. However, in many situations, the true underlying population effects may, in fact, differ from study to study, that is, there are ‘real’ differences in effect sizes among studies. Random-effects models assume that the true underlying population effects differ from study to study and that the true individual study effects follow a statistical distribution. The goal of the analysis is then to estimate the overall mean and variance of the distribution of true study effects. If the observed study effects do not appear to follow any known statistical distribution, overall mean treatment-effect estimates based on that statistical model may be invalid. In some situations, it may not be appropriate to produce a single overall treatment-effect estimate; investigating the variability may then become the principal objective of the analysis. See Borenstein <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr29-1740774512465495">29</xref>] for a succinct overview.</p>
<p>It is often assumed that it is inappropriate to combine studies that have quite different baseline event rates. Differences in baseline (control) rates, however, do not necessarily translate into differences in relative rates across studies. Furthermore, when treatment effects do appear heterogeneous, it may still be possible to choose a metric/scale that leads to treatment effects that are homogeneous enough to be interpretable. Risk differences tend to be more heterogeneous than odds ratios or relative risks [<xref ref-type="bibr" rid="bibr30-1740774512465495">30</xref>], a point that is also made in an FDA’s draft guidance for industry on noninferiority trials [<xref ref-type="bibr" rid="bibr31-1740774512465495">31</xref>]. For binary outcomes, changing the scale from an absolute measure (like risk difference) to a relative measure, such as odds ratio or relative risk might reduce statistical heterogeneity and make the data sufficiently homogeneous to produce a meaningful combined estimate on that ratio scale. The combined result can then be converted, for example, from an odds ratio to a risk difference, to help with clinical interpretability. Because the constant odds ratio model implies that the effect size must vary on the risk difference scale, a decision is needed on whether to estimate the baseline (control) event rate from the external data or from the data included in the actual meta-analysis. These two approaches have different implications for the analysis, especially variance estimation [<xref ref-type="bibr" rid="bibr32-1740774512465495">32</xref>]. Including covariates in a regression or metaregression model is another way to potentially reduce heterogeneity [<xref ref-type="bibr" rid="bibr32-1740774512465495">32</xref>].</p>
<p>As a general practice, for important adverse events, it is valuable to understand and study the factors, if any, that are associated with heterogeneity as these associations may indicate that those specific factors modify the impact of treatment. In such situations, an investigation of causes of heterogeneity is an essential step in moving from a combined effect estimate to application to particular populations and individuals. Ideally, we should study patient-level factors as potential modifiers of treatment effect <italic>within</italic> studies using patient-level data (see question 3) because using study-level summary statistics can be biased by ecological bias and lack of power. For more information, see Berlin <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr20-1740774512465495">20</xref>], Schmid <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr22-1740774512465495">22</xref>], or Lambert <italic>et al</italic>. [<xref ref-type="bibr" rid="bibr21-1740774512465495">21</xref>]. In any analysis of causes of heterogeneity, care should be taken, as analyses of a given factor can easily be confounded by other factors, such as design features.</p>
<p>As suggested above, tests of heterogeneity should not be used as the sole way to decide on whether a fixed-effect model or a random-effects model should be utilized. The statistical operating characteristics of such an approach are undefined. After all, the fixed-effect model is a special case of the random-effects model when the among-study variance is zero or close to zero. The decision should be made in principle by considering the following:</p>
<list id="list1-1740774512465495" list-type="bullet">
<list-item><p>Do you expect a common effect or not? In a development program, for a single indication, with studies using a common protocol (or very similar protocols), with the same data collection methods, definitions, and so on, a fixed-effect model would likely be appropriate. When combining data across different populations, with different data collection methods and so on, the default might be random effects, but with additional analyses performed, exploring design features as predictors of effects.</p></list-item>
<list-item><p>Do you have enough data to do a good job of estimating a random-effects model? Although random-effects models can arguably be considered as a default to start in many situations, caution needs to be exercised in implementing such models when there are few events or events are sparse. For example, in the early phase of drug development when very few studies are completed, it might not be reasonable to consider random-effects models [<xref ref-type="bibr" rid="bibr33-1740774512465495">33</xref>,<xref ref-type="bibr" rid="bibr34-1740774512465495">34</xref>].</p></list-item>
<list-item><p>Generally, random-effects models give more weight to small studies and less weight to large studies, as compared to fixed-effect models. In the presence of high heterogeneity, one should consider the appropriateness of the resulting move toward more equality of weights for very large and very small studies, as such a move may not always be desirable. Poole and Greenland present an example in which meta-analyses of small studies and large studies give different results. They note that this situation might arise from publication bias, with small studies getting published only when they show statistically significant findings [<xref ref-type="bibr" rid="bibr35-1740774512465495">35</xref>] (implying larger effect sizes). In such a situation, it might be inappropriate to upweight the published smaller studies, which are likely to be a biased subset of all small studies.</p></list-item></list>
<p>Bayesian statistics, originating from Bayes’ theorem, allow one to express a belief about a treatment–outcome relationship by specifying some prior probability distribution before incorporating new data. This belief can then be updated taking the current data into account. The Bayesian method provides a framework for synthesizing all available information in a formal, consistent, and coherent manner. The concept of meta-analysis seems to fit the Bayesian philosophy very well. We learn as we accumulate the data, and today’s posterior becomes tomorrow’s prior.</p>
<p>The Bayesian approach is known for its capability and flexibility to deal with heterogeneity through complex modeling. Bayesian models are available under both the fixed- and random-effects assumptions. One option to decide on whether to follow a fixed- or random-effects model under the Bayesian framework is to utilize model selection criteria such as the Deviance Information Criterion (DIC) [<xref ref-type="bibr" rid="bibr36-1740774512465495">36</xref>]. A smaller DIC value represents a preferred model.</p>
<p>Additionally, it is helpful in the rare event setting that Bayesian inferences are based on the full ‘exact’ posterior distributions, relaxing the assumption of normality of the outcome required by many classical methods, and do not require continuity correction in dealing with zero-event trials. Furthermore, with the Bayesian method, it is straightforward to assess the posterior probability of clinically important differences on different scales (e.g., risk difference, odds ratio, or relative risk), so the meta-analysis results can be easily interpreted in the public health context.</p>
</sec>
<sec id="section14-1740774512465495">
<title>Concluding remarks</title>
<p>Meta-analysis of randomized clinical trials has been increasingly used to evaluate safety concerns in drug development. We addressed some (though certainly not all) of the more common questions that arise regarding the conduct of meta-analysis in this context. Giving thought to these questions up front allows teams to improve planning and enhances data capture, and enhances transparency and interpretation of the results. The answers presented in this article reflect our current experience and thinking on meta-analysis in a drug development program. Furthermore, the experience has also made it clear that many questions continue to emerge. We expect that as more people engage in meta-analysis of safety data, methods and best practices will evolve.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This work received no funding from any public, for-profit, or not-for-profit sources, other than the support of the authors’ time spent on manuscript preparation, which was provided by their employers.</p>
</fn>
<fn fn-type="conflict">
<label>Conflict of interest</label>
<p>Jesse Berlin is a full-time employee of Janssen Research &amp; Development. Brenda Crowe is a full-time employee of Eli Lilly and Company. Edward Whalen is a full-time employee of Pfizer, Inc. H Amy Xia is a full-time employee of Amgen, Inc. Carol Koro is a full-time employee of GlaxoSmithKline. Juergen Kuebler is a full-time employee of CSL Behring GmbH.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1740774512465495">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nissen</surname><given-names>SE</given-names></name>
<name><surname>Wolski</surname><given-names>K</given-names></name>
</person-group>. <article-title>Effect of rosiglitazone on the risk of myocardial infarction and death from cardiovascular causes</article-title>. <source>N Engl J Med</source> <year>2007</year>; <volume>356</volume>(<issue>24</issue>): <fpage>2457</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr2-1740774512465495">
<label>2.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bridge</surname><given-names>JA</given-names></name>
<name><surname>Iyengar</surname><given-names>S</given-names></name>
<name><surname>Salary</surname><given-names>CB</given-names></name>
<etal/>
</person-group>. <article-title>Clinical response and risk for reported suicidal ideation and suicide attempts in pediatric antidepressant treatment: A meta-analysis of randomized controlled trials</article-title>. <source>JAMA</source> <year>2007</year>; <volume>297</volume>(<issue>15</issue>): <fpage>1683</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr3-1740774512465495">
<label>3.</label>
<citation citation-type="gov">
<collab>United States Food and Drug Administration</collab>. <article-title>Relationship between psychotropic drugs and pediatric suicidality</article-title>, <ext-link ext-link-type="uri" xlink:href="http://www.fda.gov/ohrms/dockets/ac/04/briefing/2004-4065b1-10-tab08-hammads-review.pdf">http://www.fda.gov/ohrms/dockets/ac/04/briefing/2004-4065b1-10-tab08-hammads-review.pdf</ext-link></citation>
</ref>
<ref id="bibr4-1740774512465495">
<label>4.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hammad</surname><given-names>TA</given-names></name>
<name><surname>Laughren</surname><given-names>T</given-names></name>
<name><surname>Racoosin</surname><given-names>J</given-names></name>
</person-group>. <article-title>Suicidality in pediatric patients treated with antidepressant drugs</article-title>. <source>Arch Gen Psychiatry</source> <year>2006</year>; <volume>63</volume>(<issue>3</issue>): <fpage>332</fpage>–<lpage>39</lpage>.</citation>
</ref>
<ref id="bibr5-1740774512465495">
<label>5.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Crowe</surname><given-names>BJ</given-names></name>
<name><surname>Xia</surname><given-names>HA</given-names></name>
<name><surname>Berlin</surname><given-names>JA</given-names></name>
<etal/>
</person-group>. <article-title>Recommendations for safety planning, data collection, evaluation and reporting during drug, biologic and vaccine development: A report of the safety planning, evaluation, and reporting team</article-title>. <source>Clin Trials</source> <year>2009</year>; <volume>6</volume>(<issue>5</issue>): <fpage>430</fpage>–<lpage>40</lpage>.</citation>
</ref>
<ref id="bibr6-1740774512465495">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Berlin</surname><given-names>JA</given-names></name>
<name><surname>Laird</surname><given-names>NM</given-names></name>
<name><surname>Sacks</surname><given-names>HS</given-names></name>
<name><surname>Chalmers</surname><given-names>TC</given-names></name>
</person-group>. <article-title>A comparison of statistical methods for combining event rates from clinical trials</article-title>. <source>Stat Med</source> <year>1989</year>; <volume>8</volume>(<issue>2</issue>): <fpage>141</fpage>–<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr7-1740774512465495">
<label>7.</label>
<citation citation-type="gov">
<collab>International Conference on Harmonisation (ICH)</collab>. <article-title>Guidance for industry – M4E: The CTD – Efficacy</article-title>, <ext-link ext-link-type="uri" xlink:href="http://www.fda.gov/downloads/RegulatoryInformation/Guidances/UCM129865.pdf">http://www.fda.gov/downloads/RegulatoryInformation/Guidances/UCM129865.pdf</ext-link></citation>
</ref>
<ref id="bibr8-1740774512465495">
<label>8.</label>
<citation citation-type="gov">
<collab>United States Food and Drug Administration</collab>. <article-title>Guidance for industry: Premarketing risk assessment</article-title>, <ext-link ext-link-type="uri" xlink:href="http://www.fda.gov/downloads/RegulatoryInformation/Guidances/ucm126958.pdf">http://www.fda.gov/downloads/RegulatoryInformation/Guidances/ucm126958.pdf</ext-link></citation>
</ref>
<ref id="bibr9-1740774512465495">
<label>9.</label>
<citation citation-type="book"><collab>Council for International Organizations of Medical Sciences (CIOMS) Working Group VI</collab>. <source>Management of safety information from clinical trials</source>. <publisher-name>CIOMS</publisher-name>, <publisher-loc>Geneva</publisher-loc>, <year>2005</year>.</citation>
</ref>
<ref id="bibr10-1740774512465495">
<label>10.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hennekens</surname><given-names>CH</given-names></name>
<name><surname>Hebert</surname><given-names>PR</given-names></name>
<name><surname>Schneider</surname><given-names>WR</given-names></name>
<etal/>
</person-group>. <article-title>Academic perspectives on the United States Food and Drug Administration’s guidance for industry on diabetes mellitus</article-title>. <source>Contemp Clin Trials</source> <year>2010</year>; <volume>31</volume>(<issue>5</issue>): <fpage>411</fpage>–<lpage>13</lpage>.</citation>
</ref>
<ref id="bibr11-1740774512465495">
<label>11.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ibrahim</surname><given-names>JG</given-names></name>
<name><surname>Chen</surname><given-names>MH</given-names></name>
<name><surname>Xia</surname><given-names>HA</given-names></name>
<name><surname>Liu</surname><given-names>T</given-names></name>
</person-group>. <article-title>Bayesian meta-experimental design: Evaluating cardiovascular risk in new antidiabetic therapies to treat type 2 diabetes</article-title>. <source>Biometrics</source> <year>2012</year>; <volume>68</volume>(<issue>2</issue>): <fpage>578</fpage>–<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr12-1740774512465495">
<label>12.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hammad</surname><given-names>TA</given-names></name>
<name><surname>Pinheiro</surname><given-names>SP</given-names></name>
<name><surname>Neyarapally</surname><given-names>GA</given-names></name>
</person-group>. <article-title>Secondary use of randomized controlled trials to evaluate drug safety: A review of methodological considerations</article-title>. <source>Clin Trials</source> <year>2011</year>; <volume>8</volume>(<issue>5</issue>): <fpage>559</fpage>–<lpage>70</lpage>.</citation>
</ref>
<ref id="bibr13-1740774512465495">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Trelle</surname><given-names>S</given-names></name>
<name><surname>Reichenbach</surname><given-names>S</given-names></name>
<name><surname>Wandel</surname><given-names>S</given-names></name>
<etal/>
</person-group>. <article-title>Cardiovascular safety of non-steroidal anti-inflammatory drugs: Network meta-analysis</article-title>. <source>BMJ</source> <year>2011</year>; <volume>342</volume>: <fpage>c7086</fpage>.</citation>
</ref>
<ref id="bibr14-1740774512465495">
<label>14.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cipriani</surname><given-names>A</given-names></name>
<name><surname>Furukawa</surname><given-names>TA</given-names></name>
<name><surname>Churchill</surname><given-names>R</given-names></name>
<name><surname>Barbui</surname><given-names>C</given-names></name>
</person-group>. <article-title>Validity of indirect comparisons in meta-analysis</article-title>. <source>Lancet</source> <year>2007</year>; <volume>369</volume>(<issue>9558</issue>): <fpage>270</fpage>–<lpage>71</lpage>; author reply 271.</citation>
</ref>
<ref id="bibr15-1740774512465495">
<label>15.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ioannidis</surname><given-names>JP</given-names></name>
</person-group>. <article-title>Indirect comparisons: The mesh and mess of clinical trials</article-title>. <source>Lancet</source> <year>2006</year>; <volume>368</volume>(<issue>9546</issue>): <fpage>1470</fpage>–<lpage>72</lpage>.</citation>
</ref>
<ref id="bibr16-1740774512465495">
<label>16.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fu</surname><given-names>H</given-names></name>
<name><surname>Price</surname><given-names>K</given-names></name>
<name><surname>Nilsson</surname><given-names>M</given-names></name>
<name><surname>Ruberg</surname><given-names>S</given-names></name>
</person-group>. <article-title>Identifying Potential Adverse Events Dose-Response Relationships Via Bayesian Indirect And Mixed Treatment Comparison Models</article-title>. <source>J Biopharm Stat</source>, in press.</citation>
</ref>
<ref id="bibr17-1740774512465495">
<label>17.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Olkin</surname><given-names>I</given-names></name>
<name><surname>Sampson</surname><given-names>A</given-names></name>
</person-group>. <article-title>Comparison of meta-analysis versus analysis of variance of individual patient data</article-title>. <source>Biometrics</source> <year>1998</year>; <volume>54</volume>(<issue>1</issue>): <fpage>317</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr18-1740774512465495">
<label>18.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Posner</surname><given-names>K</given-names></name>
<name><surname>Oquendo</surname><given-names>MA</given-names></name>
<name><surname>Gould</surname><given-names>M</given-names></name>
<name><surname>Stanley</surname><given-names>B</given-names></name>
<name><surname>Davies</surname><given-names>M</given-names></name>
</person-group>. <article-title>Columbia Classification Algorithm of Suicide Assessment (C-CASA): Classification of suicidal events in the FDA’s pediatric suicidal risk analysis of antidepressants</article-title>. <source>Am J Psychiatry</source> <year>2007</year>; <volume>164</volume>(<issue>7</issue>): <fpage>1035</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr19-1740774512465495">
<label>19.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Szczech</surname><given-names>LA</given-names></name>
<name><surname>Berlin</surname><given-names>JA</given-names></name>
<name><surname>Feldman</surname><given-names>HI</given-names></name>
</person-group>. <article-title>The effect of antilymphocyte induction therapy on renal allograft survival. A meta-analysis of individual patient-level data. Anti-Lymphocyte Antibody Induction Therapy Study Group</article-title>. <source>Ann Intern Med</source> <year>1998</year>; <volume>128</volume>(<issue>10</issue>): <fpage>817</fpage>–<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr20-1740774512465495">
<label>20.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Berlin</surname><given-names>JA</given-names></name>
<name><surname>Santanna</surname><given-names>J</given-names></name>
<name><surname>Schmid</surname><given-names>CH</given-names></name>
<name><surname>Szczech</surname><given-names>LA</given-names></name>
<name><surname>Feldman</surname><given-names>HI</given-names></name>
</person-group>. <article-title>Individual patient-versus group-level data meta-regressions for the investigation of treatment effect modifiers: Ecological bias rears its ugly head</article-title>. <source>Stat Med</source> <year>2002</year>; <volume>21</volume>(<issue>3</issue>): <fpage>371</fpage>–<lpage>87</lpage>.</citation>
</ref>
<ref id="bibr21-1740774512465495">
<label>21.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lambert</surname><given-names>PC</given-names></name>
<name><surname>Sutton</surname><given-names>AJ</given-names></name>
<name><surname>Abrams</surname><given-names>KR</given-names></name>
<name><surname>Jones</surname><given-names>DR</given-names></name>
</person-group>. <article-title>A comparison of summary patient-level covariates in meta-regression with individual patient data meta-analysis</article-title>. <source>J Clin Epidemiol</source> <year>2002</year>; <volume>55</volume>(<issue>1</issue>): <fpage>86</fpage>–<lpage>94</lpage>.</citation>
</ref>
<ref id="bibr22-1740774512465495">
<label>22.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schmid</surname><given-names>CH</given-names></name>
<name><surname>Stark</surname><given-names>PC</given-names></name>
<name><surname>Berlin</surname><given-names>JA</given-names></name>
<name><surname>Landais</surname><given-names>P</given-names></name>
<name><surname>Lau</surname><given-names>J</given-names></name>
</person-group>. <article-title>Meta-regression detected associations between heterogeneous treatment effects and study-level, but not patient-level, factors</article-title>. <source>J Clin Epidemiol</source> <year>2004</year>; <volume>57</volume>(<issue>7</issue>): <fpage>683</fpage>–<lpage>97</lpage>.</citation>
</ref>
<ref id="bibr23-1740774512465495">
<label>23.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hu</surname><given-names>M</given-names></name>
<name><surname>Cappelleri</surname><given-names>JC</given-names></name>
<name><surname>Lan</surname><given-names>KK</given-names></name>
</person-group>. <article-title>Applying the law of iterated logarithm to control type I error in cumulative meta-analysis of binary outcomes</article-title>. <source>Clin Trials</source> <year>2007</year>; <volume>4</volume>(<issue>4</issue>): <fpage>329</fpage>–<lpage>40</lpage>.</citation>
</ref>
<ref id="bibr24-1740774512465495">
<label>24.</label>
<citation citation-type="gov">
<collab>United States Food and Drug Administration</collab>. <article-title>Guidance for industry: Diabetes mellitus – Evaluating cardiovascular risk in new antidiabetic therapies to treat type 2 diabetes</article-title>, <ext-link ext-link-type="uri" xlink:href="http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/ucm071627.pdf">http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/ucm071627.pdf</ext-link></citation>
</ref>
<ref id="bibr25-1740774512465495">
<label>25.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mehrotra</surname><given-names>DV</given-names></name>
<name><surname>Heyse</surname><given-names>JF</given-names></name>
</person-group>. <article-title>Use of the false discovery rate for evaluating clinical safety data</article-title>. <source>Stat Methods Med Res</source> <year>2004</year>; <volume>13</volume>(<issue>3</issue>): <fpage>227</fpage>–<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr26-1740774512465495">
<label>26.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Berry</surname><given-names>SM</given-names></name>
<name><surname>Berry</surname><given-names>DA</given-names></name>
</person-group>. <article-title>Accounting for multiplicities in assessing drug safety: A three-level hierarchical mixture model</article-title>. <source>Biometrics</source> <year>2004</year>; <volume>60</volume>(<issue>2</issue>): <fpage>418</fpage>–<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr27-1740774512465495">
<label>27.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Xia</surname><given-names>HA</given-names></name>
<name><surname>Ma</surname><given-names>H</given-names></name>
<name><surname>Carlin</surname><given-names>BP</given-names></name>
</person-group>. <article-title>Bayesian hierarchical modeling for detecting safety signals in clinical trials</article-title>. <source>J Biopharm Stat</source> <year>2011</year>; <volume>21</volume>(<issue>5</issue>): <fpage>1006</fpage>–<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr28-1740774512465495">
<label>28.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>SG</given-names></name>
</person-group>. <article-title>Why sources of heterogeneity in meta-analysis should be investigated</article-title>. <source>BMJ</source> <year>1994</year>; <volume>309</volume>(<issue>6965</issue>): <fpage>1351</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr29-1740774512465495">
<label>29.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Borenstein</surname><given-names>M</given-names></name>
<name><surname>Hedges</surname><given-names>LV</given-names></name>
<name><surname>Higgins</surname><given-names>JPT</given-names></name>
<name><surname>Rothstein</surname><given-names>HR</given-names></name>
</person-group>. <article-title>A basic introduction to fixed-effect and random effects models for meta-analysis</article-title>. <source>Res Synth Meth</source> <year>2010</year>; <volume>1</volume>: <fpage>97</fpage>–<lpage>111</lpage>.</citation>
</ref>
<ref id="bibr30-1740774512465495">
<label>30.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Engels</surname><given-names>EA</given-names></name>
<name><surname>Schmid</surname><given-names>CH</given-names></name>
<name><surname>Terrin</surname><given-names>N</given-names></name>
<name><surname>Olkin</surname><given-names>I</given-names></name>
<name><surname>Lau</surname><given-names>J</given-names></name>
</person-group>. <article-title>Heterogeneity and statistical significance in meta-analysis: An empirical study of 125 meta-analyses</article-title>. <source>Stat Med</source> <year>2000</year>; <volume>19</volume>(<issue>13</issue>): <fpage>1707</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr31-1740774512465495">
<label>31.</label>
<citation citation-type="gov">
<collab>United States Food and Drug Administration</collab>. <article-title>FDA’s draft guidance for industry non-inferiority clinical trials</article-title>, <ext-link ext-link-type="uri" xlink:href="http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/UCM202140.pdf">http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/UCM202140.pdf</ext-link></citation>
</ref>
<ref id="bibr32-1740774512465495">
<label>32.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Localio</surname><given-names>AR</given-names></name>
<name><surname>Margolis</surname><given-names>DJ</given-names></name>
<name><surname>Berlin</surname><given-names>JA</given-names></name>
</person-group>. <article-title>Relative risks and confidence intervals were easily computed indirectly from multivariable logistic regression</article-title>. <source>J Clin Epidemiol</source> <year>2007</year>; <volume>60</volume>(<issue>9</issue>): <fpage>874</fpage>–<lpage>82</lpage>.</citation>
</ref>
<ref id="bibr33-1740774512465495">
<label>33.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sweeting</surname><given-names>MJ</given-names></name>
<name><surname>Sutton</surname><given-names>AJ</given-names></name>
<name><surname>Lambert</surname><given-names>PC</given-names></name>
</person-group>. <article-title>What to add to nothing? Use and avoidance of continuity corrections in meta-analysis of sparse data</article-title>. <source>Stat Med</source> <year>2004</year>; <volume>23</volume>(<issue>9</issue>): <fpage>1351</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr34-1740774512465495">
<label>34.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sweeting</surname><given-names>MJ</given-names></name>
<name><surname>Sutton</surname><given-names>AJ</given-names></name>
<name><surname>Lambert</surname><given-names>PC</given-names></name>
</person-group>. <article-title>Correction</article-title>. <source>Stat Med</source> <year>2006</year>; <volume>25</volume>: <fpage>2700</fpage>.</citation>
</ref>
<ref id="bibr35-1740774512465495">
<label>35.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Poole</surname><given-names>C</given-names></name>
<name><surname>Greenland</surname><given-names>S</given-names></name>
</person-group>. <article-title>Random-effects meta-analyses are not always conservative</article-title>. <source>Am J Epidemiol</source> <year>1999</year>; <volume>150</volume>(<issue>5</issue>): <fpage>469</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr36-1740774512465495">
<label>36.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spiegelhalter</surname><given-names>DJ</given-names></name>
<name><surname>Best</surname><given-names>NG</given-names></name>
<name><surname>Carlin</surname><given-names>PB</given-names></name>
<name><surname>Van der Linde</surname><given-names>A</given-names></name>
</person-group>. <article-title>Bayesian measures of model complexity and fit</article-title>. <source>J R Stat Soc Ser B</source> <year>2002</year>; <volume>64</volume>(<issue>4</issue>): <fpage>583</fpage>–<lpage>639</lpage>.</citation>
</ref></ref-list>
</back>
</article>