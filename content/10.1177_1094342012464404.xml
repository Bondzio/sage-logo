<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342012464404</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342012464404</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Regular Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Pragmatic optimizations for better scientific utilization of large supercomputers</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Dubey</surname>
<given-names>Anshu</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012464404">1</xref>
<xref ref-type="aff" rid="aff2-1094342012464404">2</xref>
<xref ref-type="corresp" rid="corresp1-1094342012464404"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Calder</surname>
<given-names>Alan C.</given-names>
</name>
<xref ref-type="aff" rid="aff3-1094342012464404">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Daley</surname>
<given-names>Christopher</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012464404">1</xref>
<xref ref-type="aff" rid="aff2-1094342012464404">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fisher</surname>
<given-names>Robert T.</given-names>
</name>
<xref ref-type="aff" rid="aff4-1094342012464404">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Graziani</surname>
<given-names>C.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012464404">1</xref>
<xref ref-type="aff" rid="aff2-1094342012464404">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jordan</surname>
<given-names>George C.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012464404">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lamb</surname>
<given-names>Donald Q.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012464404">1</xref>
<xref ref-type="aff" rid="aff2-1094342012464404">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Reid</surname>
<given-names>Lynn B.</given-names>
</name>
<xref ref-type="aff" rid="aff5-1094342012464404">5</xref>
<xref ref-type="aff" rid="aff6-1094342012464404">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Townsley</surname>
<given-names>Dean M.</given-names>
</name>
<xref ref-type="aff" rid="aff7-1094342012464404">7</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Weide</surname>
<given-names>Klaus</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012464404">1</xref>
</contrib>
<bio>
<title>Author biographies</title>
<p>
<italic>Anshu Dubey</italic> is the Associate Director of the Flash Center for Computational Science at the University of Chicago, and ANL. She also leads the CS/Applications group, which develops, maintains and distributes the FLASH code. She received her PhD in Computer Science from Old Dominion University and a BTech in Electrical Engineering from Indian Institute of Technology, New Delhi.</p>
<p>
<italic>Alan C. Calder</italic> is an assistant professor at Stony Brook University where he works on numerically modeling thermonuclear supernovae and other nuclear astrophysics phenomena. His PhD is from Vanderbilt University and he had research appointments at the National Center for Supercomputing Applications and the University of Chicago.</p>
<p>
<italic>Christopher Daley</italic> is a scientific programmer at the Flash Center for Computational Science. He received his MSc in High Performance Computing from the University of Edinburgh and his BSc in Physics from the University of Surrey.</p>
<p>
<italic>Robert T. Fisher</italic> is an assistant professor of physics at the University of Massachusetts Dartmouth. Previously, he was a research scientist at the ASC/Flash Center at the University of Chicago, and postdoctoral research fellow at LLNL. He received his PhD in physics from the University of California at Berkeley, and a BS in physics from Caltech.</p>
<p>
<italic>C. Graziani</italic> is a Senior Research Associate in the Department of Astronomy and Astrophysics at the University of Chicago. He received his PhD in physics from the University of Chicago and a BSc in applied physics from Columbia University.</p>
<p>
<italic>George C. Jordan, IV</italic> is a Research Scientist in the Flash Center for Computational Science at the University of Chicago. He received an MS and PhD in physics from Clemson University.</p>
<p>
<italic>Donald Q. Lamb</italic> is the Robert A Millikan Distinguished Service Professor in the Department of Astronomy and Astrophysics and the Enrico Fermi Institute, and the Director of the Flash Center for Computational Science at the University of Chicago.</p>
<p>
<italic>Lynn B. Reid</italic> is a Principal Modeler at NTEC Environmental Technology in Western Australia. While at the Flash Center, she was a member of the team who created FLASH3. She received her BSE from Princeton University, an MSc from the University of Dundee, and a DSc from the Massachusetts Institute of Technology.</p>
<p>
<italic>Dean M. Townsley</italic> is an assistant Professor in the Department of Physics and Astronomy at the University of Alabama. He was a research scientist at the Joint Institute for Nuclear Astrophysics while at the University of Chicago. He received his PhD in physics from the University of California, Santa Barbara, and his BS from Florida State University.</p>
<p>
<italic>Klaus Weide</italic> received Masters and doctoral degrees (1992) in physics from the University of Göttingen in Germany. He joined the Flash Center in 2006 and since then has been working on extending, porting, maintaining and supporting the Center’s FLASH code.</p>
</bio>
</contrib-group>
<aff id="aff1-1094342012464404">
<label>1</label>Flash Center for Computational Science, Astronomy and Astrophysics, Computation Institute, The University of Chicago, Chicago, IL, USA</aff>
<aff id="aff2-1094342012464404">
<label>2</label>Argonne National Laboratory, Argonne, IL, USA</aff>
<aff id="aff3-1094342012464404">
<label>3</label>Department of Physics and Astronomy, Stony Brook University, USA</aff>
<aff id="aff4-1094342012464404">
<label>4</label>Department of Physics, University of Massachusetts Dartmouth, USA</aff>
<aff id="aff5-1094342012464404">
<label>5</label>NTEC Environmental Technology, Subiaco WA, Australia</aff>
<aff id="aff6-1094342012464404">
<label>6</label>University of Western Australia, Crawley WA, Australia</aff>
<aff id="aff7-1094342012464404">
<label>7</label>Department of Physics and Astronomy, The University of Alabama, USA</aff>
<author-notes>
<corresp id="corresp1-1094342012464404">Anshu Dubey, Flash Center for Computational Science, Astronomy &amp; Astrophysics, Computation Institute, The University of Chicago, 5747 South Ellis Avenue, Chicago, IL 60637, USA. Email: <email>dubey@flash.uchicago.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2013</year>
</pub-date>
<volume>27</volume>
<issue>3</issue>
<issue-title>Special Issue section on CCDSC 2012 Workshop</issue-title>
<fpage>360</fpage>
<lpage>373</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Advances in modeling and algorithms, combined with growth in computing resources, have enabled simulations of multiphysics–multiscale phenomena that can greatly enhance our scientific understanding. However, on currently available high-performance computing (HPC) resources, maximizing the scientific outcome of simulations requires many trade-offs. In this paper we describe our experiences in running simulations of the explosion phase of Type Ia supernovae on the largest available platforms. The simulations use FLASH, a modular, adaptive mesh, parallel simulation code with a wide user base. The simulations use multiple physics components: hydrodynamics, gravity, a sub-grid flame model, a three-stage burning model, and a degenerate equation of state. They also use Lagrangian tracer particles, which are then post-processed to determine the nucleosynthetic yields. We describe the simulation planning process, and the algorithmic optimizations and trade-offs that were found to be necessary. Several of the optimizations and trade-offs were made during the course of the simulations as our understanding of the challenges evolved, or when simulations went into previously unexplored physical regimes. We also briefly outline the anticipated challenges of, and our preparations for, the next-generation computing platforms.</p>
</abstract>
<kwd-group>
<kwd>FLASH</kwd>
<kwd>supercomputer</kwd>
<kwd>optimizations</kwd>
<kwd>SN Ia</kwd>
<kwd>GCD model</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342012464404">
<title>1. Introduction</title>
<p>Advances in modeling and algorithms, combined with growth in computing resources, have made simulations extremely useful in enhancing scientific understanding. However, even the largest computing platforms available today do not allow simulations of multiphysics–multiscale phenomena at full physical fidelity. To maximize the scientific return of simulations on the available resources, many trade-offs are necessary. For example, use of adaptive mesh refinement (AMR) allows higher spatial resolution at points of interest in the physical domain than would a uniform mesh with the same memory footprint. The costs of AMR are higher algorithmic complexity and more expensive communication between processors. Another example is use of sub-grid models when direct numerical simulations of some physics in an application would require significantly higher spatial resolution than the smallest affordable mesh spacing. Researchers at the Flash Center for Computational Science at the University of Chicago have been facing these challenges in the context of simulations that study the deflagration and detonation phases in the gravitationally confined detonation (GCD) model of Type Ia supernovae (SNe Ia) (<xref ref-type="bibr" rid="bibr25-1094342012464404">Plewa et al., 2004</xref>; <xref ref-type="bibr" rid="bibr24-1094342012464404">Plewa, 2007</xref>; <xref ref-type="bibr" rid="bibr16-1094342012464404">Jordan et al., 2008</xref>; <xref ref-type="bibr" rid="bibr23-1094342012464404">Meakin et al., 2009</xref>). In this paper we describe our approach to achieving optimal use of available supercomputing resources for these studies.</p>
<p>The Type Ia supernova (SN Ia) simulations were conducted on the IBM Power and Blue Gene series of platforms at Lawrence Livermore National Laboratory (LLNL), Lawrence Berkeley National Laboratory (LBNL) and Argonne National Laboratory (ANL). The simulations use FLASH, a component-based AMR code developed, maintained and distributed by the Flash Center. Because SNe Ia are complex multiphysics and multiscale events, their simulations require many different solvers. These include an Eulerian hydrodynamics solver for reactive flow in the presence of strong shocks, an elliptic solver for the Newtonian self-gravity, an equation of state (EOS) for degenerate stellar matter, a model for the nuclear energy release, and a model of flame propagation that accounts for sub-grid effects. In addition, the nucleosynthetic products are computed by using Lagrangian tracer particles. The time histories of all essential physical quantities associated with the tracer particles are saved during simulations, which are then post-processed to compute the nucleosynthetic yields.</p>
<p>Instead of concentrating on absolute metrics of performance, such as flop rate or spatial resolution, our optimization choices are made with the goal of obtaining the greatest scientific understanding. We want the highest fidelity possible, which generally means the highest spatial resolution possible. But given a certain amount of memory per processor, maximizing the spatial resolution at points of interest in the computational domain requires limiting the areas of highest resolution. It is also necessary to minimize the memory footprints of individual solvers. Often these compromises result in a lower performance with respect to standard metrics such as flop rate. The payoff, however, is scientific simulations that achieve higher fidelity by better utilizing available computer platforms.</p>
<p>In this paper, we detail the simulation planning process, and the algorithmic optimizations and trade-offs that were found to be necessary to realize these high-quality scientific simulations. The paper is organized as follows: Section 2 explains the physical processes included in the simulations; for completeness, Section 3 describes FLASH, the code used to perform the simulations; Section 4 describes the solvers and models employed in the calculations; Section 5 discusses the planning process we carried out to prepare for the simulations, and Section 6 describes the code modifications and optimizations we made before and during the simulations. Section 7 discusses future prospects for these kinds of simulations on the new HPC architectures currently being deployed and expected in the future; and finally, Section 8 summarizes our acquired knowledge and experience-based conclusions.</p>
</sec>
<sec id="section2-1094342012464404">
<title>2. Physical processes</title>
<p>SNe Ia are thought to be white dwarf stars in binary systems that explode due to a thermonuclear runaway. The following sequence of events is believed to produce the runaway. The companion star transfers matter onto the white dwarf, the core of which is composed of roughly equal amounts of carbon (C) and oxygen (O). The resulting increase in the mass of the white dwarf raises the temperature and density in its core, eventually leading to a smoldering phase that lasts ~ 1000 years and causes the core to become convective. Finally, ignition occurs at one or more points in this convective core. Recent studies suggest that the point or points where ignition occurs are likely to be off-center (<xref ref-type="bibr" rid="bibr36-1094342012464404">Woosley et al., 2004</xref>; <xref ref-type="bibr" rid="bibr19-1094342012464404">Kuhlen et al., 2006</xref>; <xref ref-type="bibr" rid="bibr37-1094342012464404">Zingale et al., 2009</xref>).</p>
<p>Our simulations treat the explosive nuclear combustion that follow ignition. The regions of burning C/O grow, and soon become Rayleigh-Taylor unstable. This instability initiates a phase of buoyancy-driven turbulent nuclear combustion via an ordinary flame that is referred to as the <italic>deflagration phase</italic>. During this phase, the flame front is subsonic, allowing time for the star to expand and its density to decrease due to the energy injected into it by the nuclear burning. However, while the nuclear energy powers the SN Ia explosion, it is the radioactive decay of <sup>56</sup>Ni that heats the explosion ejecta, making the SN Ia visible. Pure deflagration models do not generate enough <sup>56</sup>Ni to account for the observed light curves and spectra. For this reason, most scientists in the field now think that the deflagration wave must somehow turn into a detonation wave that incinerates the entire star (<xref ref-type="bibr" rid="bibr17-1094342012464404">Khokhlov, 1991</xref>; <xref ref-type="bibr" rid="bibr15-1094342012464404">Höflich et al., 2002</xref>; <xref ref-type="bibr" rid="bibr13-1094342012464404">Gamezo et al., 2004</xref>; <xref ref-type="bibr" rid="bibr25-1094342012464404">Plewa et al., 2004</xref>; <xref ref-type="bibr" rid="bibr24-1094342012464404">Plewa, 2007</xref>; <xref ref-type="bibr" rid="bibr27-1094342012464404">Röpke and Niemeyer, 2007</xref>; <xref ref-type="bibr" rid="bibr26-1094342012464404">Röpke et al., 2007</xref>; <xref ref-type="bibr" rid="bibr16-1094342012464404">Jordan et al., 2008</xref>). During this <italic>detonation phase</italic>, the supersonic burning front burns material in the core of the star that would have remained unburnt if there were only a deflagration phase. Hence more nuclear energy is released, and more <sup>56</sup>Ni created, producing SNe Ia with kinetic energies and luminosities similar to those observed.</p>
<p>In the GCD model, off-center ignition at one or more points produces a hot bubble of ash that rises buoyantly and breaks through the surface of the star, spreads rapidly across the stellar surface, converges at the opposite point, and initiates a detonation (<xref ref-type="bibr" rid="bibr25-1094342012464404">Plewa et al., 2004</xref>; <xref ref-type="bibr" rid="bibr21-1094342012464404">Livne et al., 2005</xref>; Plewa, <xref ref-type="bibr" rid="bibr24-1094342012464404">2007</xref>; <xref ref-type="bibr" rid="bibr27-1094342012464404">Röpke and Niemeyer, 2007</xref>; <xref ref-type="bibr" rid="bibr32-1094342012464404">Townsley et al., 2007</xref>; <xref ref-type="bibr" rid="bibr16-1094342012464404">Jordan et al., 2008</xref>; <xref ref-type="bibr" rid="bibr23-1094342012464404">Meakin et al., 2009</xref>). We focus on this model in the simulations described here.</p>
</sec>
<sec id="section3-1094342012464404">
<title>3. The FLASH code</title>
<p>FLASH is a massively parallel multiphysics code, developed and maintained by the Flash Center at the University of Chicago. FLASH is modular and extensible, and has a wide base of users within and outside the Center. FLASH was an informal acceptance test for the IBM BG/L and a formal acceptance test for the IBM BG/P, and is a formal acceptance test for the IBM BG/Q. FLASH applications can utilize either a uniformly discretized grid or a block-structured AMR grid in which the spacing between grid points is dictated by the resolution requirements of the physical model in different regions of the domain.</p>
<p>The discretization, and most of the solvers in FLASH, work in an Eulerian framework. The physical domain is discretized into <italic>cells </italic>and cells are grouped to form blocks. A union of blocks spans the entire computational domain. Each block, surrounded by appropriate layers of ghost cells, presents a self-contained computational domain to the Eulerian physics solvers in the code (<xref ref-type="bibr" rid="bibr12-1094342012464404">Fryxell et al., 2000</xref>; <xref ref-type="bibr" rid="bibr8-1094342012464404">Dubey et al., 2009</xref>). Most FLASH simulations use blocks with either 8<sup>3</sup> or 16<sup>3 </sup>cells; the SN Ia simulations use 16<sup>3</sup>. FLASH is organized into distinct code modules called <italic>units </italic>which, when put together, form the code basis for a simulation. In general, the physics solvers view the unit managing the mesh (the <italic>Grid </italic>unit) as a black box.</p>
<p>The AMR capabilities in the current release version of FLASH (version 4.0) are provided by the PARAMESH library package (<xref ref-type="bibr" rid="bibr22-1094342012464404">MacNeice et al., 2000</xref>). PARAMESH logically organizes its blocks in an octree structure; each branch of the tree terminates in 2<italic>
<sup>d</sup> </italic>leaf blocks, where <italic>d </italic>is the dimension of the mesh. <xref ref-type="fig" rid="fig1-1094342012464404">Figure 1</xref> shows an example of a 2D mesh with the corresponding octree, where all blocks except 2, 6, 10, 14 and 18 are leaf blocks. In FLASH, a simulation only evolves on the leaf blocks, and the refinement ratio is always a factor of two. When the mesh is refined, new higher resolution leaf blocks are created. When the mesh is derefined, the lowest level leaf blocks go out of existence and the parents of the old leaf blocks become the new leaf blocks. During the course of the simulation, blocks can refine or derefine based on user-specified criteria. The blocks are ordered according to a Morton space-filling curve (<xref ref-type="bibr" rid="bibr34-1094342012464404">Warren and Salmon, 1993</xref>) to optimize spatial proximity of neighboring blocks. This curve is cut into pieces whose number is equal to the number of processors.</p>
<fig id="fig1-1094342012464404" position="float">
<label>Figure 1.</label>
<caption>
<p>A two-dimensional adaptive mesh and the corresponding octree. Labels in boldface indicate a parent/ancestor node, those not in boldface are active leaf nodes.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig1.tif"/>
</fig>
</sec>
<sec id="section4-1094342012464404">
<title>4. Models and solvers</title>
<p>To make up each simulation application, FLASH combines multiple units. For example, there are units that handle physics such as hydrodynamics or gravity; code infrastructure, such as the grid and I/O; monitoring processes; and setup of the simulation’s initial and boundary conditions. The units may have multiple implementations from which the user can select when setting up the simulation. Here we describe the physics solvers that are used in the GCD simulations.</p>
<sec id="section5-1094342012464404">
<title>4.1. Hydrodynamics and equation of state</title>
<p>The hydrodynamics solver included in the SN Ia simulations is a directionally split piecewise-parabolic method (PPM) solver (<xref ref-type="bibr" rid="bibr6-1094342012464404">Colella and Woodward, 1984</xref>; <xref ref-type="bibr" rid="bibr35-1094342012464404">Woodward and Colella, 1984</xref>): a higher-order version of the method developed by <xref ref-type="bibr" rid="bibr14-1094342012464404">Godunov (1959</xref>). PPM in FLASH uses the Direct Eulerian formulation, and the technique for allowing non-ideal equations of state described in <xref ref-type="bibr" rid="bibr5-1094342012464404">Colella and Glaz (1985</xref>) and <xref ref-type="bibr" rid="bibr12-1094342012464404">Fryxell et al. (2000</xref>). For multidimensional problems, FLASH uses second-order operator splitting (<xref ref-type="bibr" rid="bibr29-1094342012464404">Strang, 1968</xref>).</p>
<p>Godunov’s method uses a finite-volume spatial discretization of the Euler equations together with an explicit forward time difference. Time-advanced fluxes at cell boundaries are computed using the numerical solution to Riemann’s shock tube problem at each boundary. Initial conditions for each Riemann problem are determined by assuming the non-advanced solution to be piecewise-constant in each cell. Using the Riemann solution has the effect of introducing explicit nonlinearity into the difference equations, and permits the calculation of sharp shock fronts and contact discontinuities, without introducing significant non-physical oscillations into the flow. PPM improves on Godunov’s method by representing the flow variables with piecewise-parabolic functions.</p>
<p>The EOS describing equilibrium conditions for degenerate stellar plasma includes contributions from radiation, completely ionized nuclei, and relativistically degenerate electrons and positrons (<xref ref-type="bibr" rid="bibr30-1094342012464404">Timmes and Swesty, 2000</xref>). The pressure and internal energy are calculated as the sum over these respective components. This EOS implementation in FLASH uses a table of the Helmholtz free energy and its derivatives, and employs a thermodynamically consistent interpolation scheme, obviating the need to perform costly calculations during the course of a simulation. The Helmholtz table contains roughly 3.14 × 10<sup>5</sup> 8-byte reals, for a total of 2.5 MB of memory.</p>
</sec>
<sec id="section6-1094342012464404">
<title>4.2. Gravity solver</title>
<p>The GCD simulations solve Poisson’s equation to calculate the gravitational potential. FLASH provides two options for solving the equation: when the matter distribution is sufficiently spherically symmetric that the gravitational field may be described by a truncated multipole expansion, a solver based on the multipole method is sufficient; for non-symmetric distributions a multigrid-based solver is available. Though the multigrid solver is more general, it is significantly more expensive computationally.</p>
<p>The multipole method computes the center of mass and takes spherical harmonic expansions about the center of mass, truncating the expansion at relatively low multipole values. The truncation index of <italic>l<sub>max</sub>
</italic> is chosen to maintain the error in the calculation of self-gravity within acceptable bounds. Because of the radial dependence of the multipole moments of the source function, the moments for individual cells are tabulated as functions of distance from the center of mass in discrete bins. At the end of tabulation, global sums complete the computation.</p>
</sec>
<sec id="section7-1094342012464404">
<title>4.3. Flame model</title>
<p>The thermonuclear flame is treated with an Advection-Diffusion-Reaction (ADR) model with a reactive ash in nuclear statistical equilibrium (NSE) (<xref ref-type="bibr" rid="bibr18-1094342012464404">Khokhlov, 1995</xref>; <xref ref-type="bibr" rid="bibr33-1094342012464404">Vladimirova et al., 2006</xref>; <xref ref-type="bibr" rid="bibr4-1094342012464404">Calder et al., 2007</xref>; <xref ref-type="bibr" rid="bibr32-1094342012464404">Townsley et al., 2007</xref>). The thickness of the flame front is in the millimeter to centimeter range during most of the deflagration phase, whereas the spacing of the mesh at its highest resolution is in kilometers. This difference of many orders of magnitude in spatial scales eliminates any possibility of a direct numerical simulation of the flame (even with AMR), necessitating the use of the ADR flame-capturing scheme. The ADR scheme requires an input flame speed, and FLASH enhances tabulated laminar flame speeds according to a model that accounts for the effect of sub-grid scale turbulent nuclear burning (<xref ref-type="bibr" rid="bibr32-1094342012464404">Townsley et al., 2007</xref>). The flame model can potentially cause computational load imbalance because the processors holding the portions of the physical domain with the flame front and ash must perform significantly more computations than the rest of the processors, and the flame front and ash typically occupy a small fraction of the domain.</p>
</sec>
<sec id="section8-1094342012464404">
<title>4.4. Lagrangian tracer particles</title>
<p>The FLASH SN Ia simulations include a description of the energy released by thermonuclear burning, but memory considerations prohibit calculating the nucleosynthetic products of the explosion inline. Note that these products do not impact the simulations; these detailed nuclear abundances are needed in order to be able to meaningfully compare the results of the simulations with observations. Massless Lagrangian tracer particles (<xref ref-type="bibr" rid="bibr7-1094342012464404">Dubey et al., 2011</xref>; <xref ref-type="bibr" rid="bibr9-1094342012464404">Dubey et al., 2012</xref>) are therefore used to record the density and temperature history of every bit of matter in the star. This enables the nucleosynthetic products of the explosion to be determined by post-processing the density and temperature histories of the tracer particles using a detailed reaction network.</p>
<p>The particle positions are initialized so that their spatial distribution traces the matter density distribution. By Liouville’s theorem, their evolving distribution will continue to trace the evolving matter density at later times. Time integration along the Eulerian velocity field uses the second-order accurate Runge–Kutta method. For parallel efficiency, particles are computationally placed on the processor that contains the blocks with which they share their spatial position.</p>
</sec>
</sec>
<sec id="section9-1094342012464404">
<title>5. Simulation planning</title>
<p>In view of the Flash Center’s ambitious scientific agenda, the Center’s SN Ia simulations are invariably designed to push the limits of available computational resources. Obtaining scientifically meaningful results on the available computer platforms within the allocated time requires considerable ingenuity, combined with a careful consideration of trade-offs. For example, the range of scales associated with the physical phenomena modeled in the GCD simulations is huge: the whole star is modeled using a mesh with resolution on order of a kilometer, while the flame thickness ranges from 1 mm to a few cm. Moreover, the flame front and ash, whose dynamics are computationally intensive, is highly localized. Similarly, it is desirable to populate the physically interesting sections of the domain with a sufficient number of tracer particles to give accurate information on the nucleosynthetic yield, but it is very difficult to predict a priori which particles will wind up in these interesting sections at the end of the simulation.</p>
<sec id="section10-1094342012464404">
<title>5.1. Grid considerations</title>
<p>In AMR hydrodynamic computations using explicit methods, two critical parameters related to the computational cost are the finest mesh resolution and the volume filling fraction of the blocks at the highest spatial resolutions, which we call the “fill factor”. By definition, if the finest blocks occupy a total volume <italic>V</italic>
<sub>ref</sub> and the total domain size is <italic>V</italic>, the fill factor is simply <italic>V</italic>
<sub>ref</sub> /<italic>V</italic>. Finest mesh resolution is critical for an explicit calculation because the explicit total computational cost is simply proportional to the total number of space-time mesh points evolved. Consequently, uniformly doubling the resolution everywhere in space increases the computational cost by the fourth power of the spatial resolution: three powers for the spatial dimensions, and the fourth due to the explicit timestep. The fill factor is critical because it characterizes the fraction of the domain refined at full resolution, and allows us to plan the expense of high-resolution runs from coarser pathfinder calculations, as we outline below in section 5.3.</p>
<p>AMR presents its own challenges. Complex fluid dynamical flows develop wherever nuclear reactions are occurring, due to the nuclear energy the reactions release. At the beginning of the deflagration phase, these reactions occur in a tiny part of the star, but by the end of this phase, they occur throughout a large fraction of the star. The available memory must be allocated to provide finer resolution in these parts of the computational domain, which are limited at the beginning of the simulation but are large by the end. Furthermore, because the hydrodynamic equations are treated explicitly in time, stability considerations limit the size of the timestep as spatial resolution increases, in turn increasing the time to calculate the solution. Some versions of AMR (<xref ref-type="bibr" rid="bibr2-1094342012464404">Berger and Colella, 1989</xref>) allow for variable timesteps, or subcycling, on the coarse and finer meshes, which can mitigate the computational cost of finer timesteps. However, for sufficiently large fill factors, subcycling does not significantly reduce the computation cost enough to justify the added complexity in FLASH (<xref ref-type="bibr" rid="bibr10-1094342012464404">Dursi and Zingale, 2005</xref>). During the deflagration phase, because the regions of nuclear burning occupy a large volume filling factor except for the earliest times, they dominate the total computational cost of the simulations. As a result, we employ only synchronous timestepping. All of these factors, ranging from fundamental physics to the memory layout of the hardware, play important roles in determining the resolution and physics fidelity in the simulation, given a specified count of CPU hours available for the production simulations.</p>
</sec>
<sec id="section11-1094342012464404">
<title>5.2. Simulation cost and timesteps</title>
<p>The total computational cost of an explicit calculation is simply the total number of space-time mesh points evolved, multiplied by the computational effort per timestep, and including additional overheads such as I/O and communication. Planning full-scale adaptive mesh refinement calculations therefore requires several key ingredients, including estimates for the physical time of evolution <italic>t</italic>
<sub>run</sub> required to reach the simulation goal, the timestep Δ<italic>t</italic> on the finest mesh, and the fill factor of the finest grids. The total number of timesteps <italic>N</italic>
<sub>steps</sub> taken is simply <italic>t</italic>
<sub>run</sub>/Δ<italic>t</italic>, assuming fixed timestep Δ<italic>t</italic>.</p>
<p>Of these parameters, the timestep Δ<italic>t</italic> is largely set by fundamental physics constraints, and it falls into place quickly. First the finest spatial resolution of the full-scale simulation is specified, which is determined by physical considerations given the tight constraints imposed by the fourth-power dependence of the computational cost on the resolution. Further, given the finest spatial resolution of the calculation, the timestep is determined by the Courant condition, which is in turn fixed by the maximum signal speed (the largest absolute velocity plus sound speed) on the domain. The non-relativistic dynamical assumption is well-justified here, because the highest bulk fluid speeds achieved during the simulation are of order the Chapman–Jouguet speed of the supersonic detonation front, about 0.03<italic>c</italic>, and the gravitational redshift factor <italic>GM</italic>/(<italic>c</italic>
<sup>2</sup>
<italic>r</italic>) ~ 1 × 10<sup>−3</sup> at the surface of the white dwarf. Therefore, the timestep is ideally set by physical considerations, and easily determined given the finest spatial resolution. For instance, in the SN Ia simulations, the timestep in the initially subsonic burning front is determined by the finest spatial cell size divided by the sound speed at the center of the white dwarf, multiplied by the Courant number.</p>
</sec>
<sec id="section12-1094342012464404">
<title>5.3. Pathfinder simulations</title>
<p>Determining the computational effort and overheads in simulation planning require 2D and 3D pathfinder simulations. Here, our considerable experience in designing test problems for the verification of solution correctness is invaluable. Knowledge gained there can be used equally effectively to design a test run that provides a rough idea of computational effort, communication overheads and load distribution. The performance numbers obtained from 2D pathfinder calculations and 3D test problems are then combined and extrapolated to make an initial estimate of resource requirements for the simulations. As a full simulation gets underway, we monitor the actual use of resources and adjust our estimates accordingly.</p>
<p>In contrast to the timestep, both the fill factor and the physical time of evolution <italic>t</italic>
<sub>run</sub> are themselves determined by the outcome of the complex hydrodynamical evolution of the simulation, and therefore subject to greater uncertainty. In the initial plan, these estimates have to rely upon simpler 2D pathfinder calculations, which also help in determining the parameters to be used in the run.</p>
</sec>
<sec id="section13-1094342012464404">
<title>5.4. Planning example</title>
<p>We provide here an example of a planning exercise for a 3D GCD simulation at 4-km resolution. Table <xref ref-type="table" rid="table1-1094342012464404">1</xref> shows numbers available from the 2D simulations, and an incomplete 3D simulation used to estimate required resources. The simulation is broken into three phases, from start to bubble breakout to detonation, each requiring roughly 1 s of physical evolution time, though each with distinctly different physical characteristics and resource requirements. The resource estimates shown are given in terms of block count (with each block consisting of 16<sup>3</sup> cells) and fill factors.</p>
<table-wrap id="table1-1094342012464404" position="float">
<label>Table 1.</label>
<caption>
<p>Estimates of block count (with each block count consisting of 16<sup>3</sup> cells) and fill factor in simulation planning. Numbers with * are extrapolated.</p>
</caption>
<graphic alternate-form-of="table1-1094342012464404" xlink:href="10.1177_1094342012464404-table1.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th colspan="2">2D pathfinder</th>
<th colspan="2">3D test</th>
</tr>
<tr>
<th>Phase</th>
<th>blocks</th>
<th>fill factor</th>
<th>blocks</th>
<th>fill factor</th>
</tr>
</thead>
<tbody>
<tr>
<td>Start</td>
<td>510</td>
<td>0.003</td>
<td>12,248</td>
<td>0.0006</td>
</tr>
<tr>
<td>Bubble breakout</td>
<td>1134</td>
<td>0.006</td>
<td>222,167</td>
<td>0.0085</td>
</tr>
<tr>
<td>Detonation</td>
<td>6162</td>
<td>0.035</td>
<td>*214,844</td>
<td>*0.0012</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Based upon the physical time of roughly 1 s for each phase, and an estimated timestep of 1.3 × 10<sup>−4</sup> s, each phase requires roughly 7700 timesteps. In addition, the CPU cost to advance one cell per timestep is 160 × 10<sup>−6</sup> s. The cost of each phase is simply the number of cells advanced multiplied by the CPU cost to advance one cell per timestep. Consequently, from measurements in the incomplete 3D run, we projected needing 0.63 million CPU hours for the full calculation on the available IBM Power5 platform at LLNL. This preliminary estimate assumed the full 3D simulation would be taken completely to the detonation phase, and showed that this plan would barely have been possible with the available computational resources. However, in the presence of AMR, and in taking the solvers to previously uncharted physical regimes, more stringent resource limits began to show early in the simulations, necessitating trade-offs and optimizations described in the next section.</p>
</sec>
</sec>
<sec id="section14-1094342012464404">
<title>6. Optimizations and trade-offs</title>
<p>Given the CPU and memory limitations of the available supercomputers, several trade-offs were necessary in order to optimize computational speed and obtain scientifically useful results.</p>
<sec id="section15-1094342012464404">
<title>6.1. Gravity solver</title>
<p>The volume-based hydrodynamic solver in FLASH is strictly conservative: by design it rigorously conserves local mass, momentum, and energy. This is physically desirable, and also serves as a diagnostic: loss of conservation signals a bug. Self-gravity, as a source term, is not locally conservative, since it represents an acceleration term that depends non-locally on the entire mass configuration. However, self-gravity is <italic>globally</italic> conservative, in that it conserves total momentum and energy (including nuclear energy released by burning) of all the gravitating mass in the isolated domain. We used strict conservation of momentum as a diagnostic of correct behavior of the gravity solver: the solver was regarded as broken unless total momentum was conserved. We also monitored energy, but anomalous energy fluctuations can be produced by bugs in various source terms, such as burning, whereas momentum anomalies are specific to gravitational physics in this problem.</p>
<p>The momentum of the mass configuration is the total mass multiplied by the velocity of the center of mass. Since the initial configuration is static, conservation of total momentum is equivalent to immobility of the center of mass. We established an acceptable bound for center-of-mass motion: any anomalous acceleration of the star should result in no more displacement of the center of mass than one maximally refined grid element during the course of the entire simulation.</p>
<p>The multipole solver computes contributions to the gravitational field up to multipoles of order <italic>l<sub>max</sub>
</italic>. The verification/optimization question we addressed was: what is an appropriate choice for <italic>l<sub>max</sub>
</italic>? Since the buoyant nuclear burning bubble ignites off-center, it rises toward the surface, destroying the spherical symmetry of the star’s initial density distribution. A monopole (<italic>l<sub>max</sub>
</italic> = 0) or low-<italic>l<sub>max</sub>
</italic> limit on the order of multipole moments computed by the solver cannot correctly capture the asymmetry of this configuration. This loss of physical fidelity due to inadequate choice of <italic>l<sub>max</sub>
</italic> manifests itself as motion of the center of mass. Through experimentation we found that the minimum value of <italic>l<sub>max</sub>
</italic> needed to keep the center of mass stationary at the specified level was 8, which made the time to compute the gravitational potential prohibitively expensive. Replacement of the multipole solver with an alternative solver, such as FLASH’s multigrid solver, would be equally expensive.</p>
<p>If, however, the ignition point was placed on the <italic>z</italic>-axis, it would rise vertically, and higher moments would need fewer terms because of the azimuthal symmetry around the <italic>z</italic>-axis: at each <italic>l</italic>, the <italic>m</italic> = 0 term would dominate the other 2<italic>l</italic>, non-azimuthally-symmetric <italic>m</italic> = 0 terms. This geometrical rotation reduced the computational complexity of the implementation from <italic>O</italic>(<inline-formula id="inline-formula1-1094342012464404">
<mml:math id="mml-inline1-1094342012464404">
<mml:msubsup>
<mml:mi>l</mml:mi>
<mml:mrow>
<mml:mi>m</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>x</mml:mi>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula>) to <italic>O</italic>(<italic>l<sub>max</sub>
</italic>), with a constant multiplication factor. With this optimization, the simulation would be within reach but the gravity solve was still far too expensive.</p>
<p>The subgrid sampling used by the multipole solver was found to constitute overkill for the GCD simulations. When there are steep density gradients near the origin, subgrid sampling minimizes the errors introduced by varying grid geometry. While a galaxy cluster or star cluster simulation might exhibit the density cusps requiring this sub-sampling, the SN Ia simulations do not. Subgrid sampling increases the cost of computing the gravitational potential by a factor of roughly <italic>N</italic>
<sup>3</sup>, where <italic>N</italic> is the finer discretization of subsampling. In the GCD simulation, we could exploit the physics of the problem to reduce <italic>N</italic> by a factor of four without compromising the accuracy.</p>
<p>Another possible optimization would have been consolidation of MPI Allreduce calls which were used to collect the moments. <xref ref-type="table" rid="table2-1094342012464404">Table 2</xref> shows the cost of individual routines in the gravity unit with and without these two optimizations. The table clearly shows that subsampling was the dominant cost of the calculations.</p>
<table-wrap id="table2-1094342012464404" position="float">
<label>Table 2.</label>
<caption>
<p>Cost in seconds of individual routines in computing the gravitational potential.</p>
</caption>
<graphic alternate-form-of="table2-1094342012464404" xlink:href="10.1177_1094342012464404-table2.tif"/>
<table>
<thead>
<tr>
<th>Subroutine</th>
<th>No optimization</th>
<th>Compressed mpi allreduce</th>
<th>Remove Subsampling</th>
</tr>
</thead>
<tbody>
<tr>
<td>gravity</td>
<td>293.137</td>
<td>289.414</td>
<td>18.424</td>
</tr>
<tr>
<td>mpole moments</td>
<td>14.245</td>
<td>13.277</td>
<td>13.320</td>
</tr>
<tr>
<td>mpole potential</td>
<td>275.510</td>
<td>272.752</td>
<td>1.724</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section16-1094342012464404">
<title>6.2. Flame model</title>
<p>The flame and ash computations presented a performance problem in the form of load imbalance. All of the calculations related to the nuclear burning occur in the flame front and the ash, which initially occupy a small fraction of the computational domain and hence a small number of CPUs. The load imbalance is especially pronounced during the deflagration phase when the initial burning bubble grows rapidly and rises to the stellar surface. The percentage of the blocks that are involved in the flame and ash calculations is initially negligible and eventually peaks at about 10 – 20%.</p>
<p>Our treatment of the flame front and ash requires finding the NSE, which in turn involves computation of the EOS. The EOS calculation in the presence of degenerate stellar matter takes roughly the same order of magnitude of computational effort as a hydrodynamic step in a single cell. As a result, blocks that contain the flame front and/or ash take much longer to compute this portion of the physical model than do those outside the flame front. The spatial proximity of the neighboring blocks (due to Morton ordering) exacerbates the load imbalance by causing these time-consuming blocks to lie on a few processors. In a test run with 3600 timesteps on four processors, the maximum time taken by any processor in the Flame unit was 325 s, while the minimum was 84 s. In the production runs, the imbalance was worse.</p>
<p>One of the possible options to reduce the load imbalance caused by the flame front and ash would be to overlap the flame and ash computations with those corresponding to some other physical operator. Another option would be to weight the blocks during the Morton ordering process to have a more even load distribution among processors. In this instance, neither of these options works very well. Overlap with another physical operator is not possible because of the block’s dependence on spatial guard cells. The guard cells surround a block with ghost cells that contain the corresponding values from neighboring blocks necessary for computing the physical stencil in the current block. At the end of each physics operator application in FLASH, a guard cell fill process is required in order to ensure state consistency with the active cells in the block. In a parallel environment, the fill is effectively a synchronization step amongst all processors, which eliminates any possibility of overlapping computations of two different operators.</p>
<p>The option of weighting blocks along the Morton curve also fails due to the same guard cell synchronization issue described above. In PARAMESH, load balance is achieved by distributing blocks among processors in such a way that each processor has roughly the same number of leaf blocks. This distribution is important because the solution evolves in time on leaf blocks, but not on parent or ancestor blocks in the octree. Because of the synchronization imposed by the guard-cell filling that occurs between the application of physics operators, a change in the block distribution that benefits one operator often has a detrimental effect on another operator. In the GCD simulations, for example, a load balance optimal for flame and ash computations causes an imbalance to appear in hydrodynamics and gravitational potential computations. Those two operators together account for a much greater fraction of computation time (50–60%) than does flame front evolution (0–20%), and therefore a block distribution that is helpful for the flame and ash calculations is not a very good option for the rest of the simulation’s calculations.</p>
<p>Similar to the case of the EOS, it was possible to vastly increase the efficiency of calculating the NSE state by investigating physical conditions in this particular simulation. Since it is possible to precompute the NSE for the whole problem ahead of the simulation, we prepared a tabulation of the answer for the expected range of physical parameters, and then interpolated (second order) within that table during the simulation. The tables occupy about 71 MB of memory. This method eliminates the load imbalance, and has the additional advantage of being a fixed-cost operation rather than an iterative one.</p>
</sec>
<sec id="section17-1094342012464404">
<title>6.3. Lagrangian tracer particles</title>
<p>Lagrangian tracer particles are used to determine the density and temperature history of matter in the star, which is needed for the post-processing that determines the nucleosynthetic yield. The initial distribution of the particles is based upon the density distribution in the domain, as described in Section 4.4. The global count of the particles used in the simulations is quite modest, about one million, requiring a few hundred megabytes of storage globally. This requirement is a small fraction of the several hundred gigabytes of global memory used by the entire simulation. Under normal circumstances, the storage requirements of the particles do not present any difficulty. However, in the GCD simulations, this extremely modest storage requirement becomes a bottleneck.</p>
<p>The difficulty is that the particles, being placed proportional to mass, tend to lie primarily on the blocks with the highest densities. This scheme works at the beginning of the simulation because the blocks with low densities are not refined. But as the bubble breaks out of the star and hot ash flows over the stellar surface, causing low-density regions to refine, the blocks with high density that remain unrefined have most of the particles. Because we use the Morton space-filling curve, more and more of these unrefined blocks with large numbers of particles begin to congregate on fewer and fewer processors, until one or more processors run out of memory.</p>
<p>Figure <xref ref-type="fig" rid="fig2-1094342012464404">2</xref> shows conceptually the behavior of the distribution of blocks and the congregation of particles that occurs in the GCD simulations. The mapping of blocks to processors is color coded: blocks with the same color reside on the same processor. Here, initially all processors have one block and 16 particles each. After one refinement step (which refines a single block), the distribution changes. Now three of the processors have two leaf blocks each, while one processor has one leaf and one parent block. The distribution of particles is more uneven. One of the processors now has 32 particles, effectively doubling the storage required for particles. Further refinements exacerbate the problem.</p>
<fig id="fig2-1094342012464404" position="float">
<label>Figure 2.</label>
<caption>
<p>The effect of refinement on particles count. The blocks with same color reside on the same processor. The particles associated with a block reside on the same processor as the block. At left, the initial state of the particle distribution is shown. At right, after one refinement step.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig2.tif"/>
</fig>
<p>Because it is not possible to predict which blocks will remain unrefined, the most obvious solution, which would be to change the initial distribution of particles, is not an option. The other conceptually simple solution would be to drop or ignore the particles in the unrefined blocks, since they are not of much interest in the post-processing for nucleosynthesis. While an attractive solution from a runtime consideration, this approach would necessitate modifications in the I/O processing in FLASH, and in all of the post-processing tools. A workable solution comes from a cost–benefit analysis.</p>
<p>A simple method to eliminate the memory bottleneck is, during the early simulation stages, to force blocks above a certain density threshold to be more refined than blocks with lower density regions. The same effect could also be achieved by using particle count per block as a refinement criterion. This approach clearly produces unnecessary extra work in the blocks that did not need to be refined. At first glance, this solution seems excessive, since each refinement step in a three-dimensional domain causes eight children to be created per block. However, automatic block redistribution for load balancing is an inherent part of FLASH and PARAMESH’s refinement process. Typically, the number of leaf blocks is not identical on all processors. Because there are very few blocks that undergo forced refinement, in most GCD situations the redistribution of blocks ensures that the only effect of creating extra blocks is to make the number of leaf blocks per processor more uniform across processors. A minor amount of computational increase eliminates the memory bottleneck caused by the particles.</p>
<p>An example of <italic>P </italic>processors having <italic>B </italic>global count of leaf blocks will illustrate the above point. If the block density b on each processor is b = <inline-formula id="inline-formula2-1094342012464404">
<mml:math id="mml-inline2-1094342012464404">
<mml:mo fence="false" stretchy="false">⌈</mml:mo>
</mml:math>
</inline-formula>
<italic>B/P~</italic>
<inline-formula id="inline-formula3-1094342012464404">
<mml:math id="mml-inline3-1094342012464404">
<mml:mo fence="false" stretchy="false">⌉</mml:mo>
</mml:math>
</inline-formula>, then some processors will have b – 1 leaf blocks while others will have <italic>b </italic>leaf blocks. If b<italic>
<sub>f</sub>
</italic> blocks are forced to refine, then as long as 8*<italic>b<sub>f</sub>
</italic> &lt; <italic>p</italic>, where <italic>p </italic>is the number of processors with b – 1 leaf blocks, the maximum count of leaf blocks on any processor will still be <italic>b</italic>. Recall that synchronization in the form of a guard cell fill occurs for every physics operator. Therefore the computational time required for one timestep is dictated by the processors with <italic>b</italic> blocks. Hence, if 8*<italic>b<sub>f</sub>
</italic> &lt; <italic>p</italic>, there will be no change in the computational time. In the worst case of excessive initial refinement, the maximum leaf block count increases by at most one on any processor, and only for a few timesteps. Thus the incurred cost is negligible. Similar to the reduced accuracy in the computation of the gravitational potential that was described in Section 6.1, this code modification is an example of pragmatic cost-benefit balancing in real production simulations.</p>
</sec>
<sec id="section18-1094342012464404">
<title>6.4. Refinement criteria</title>
<p>When a simulation uses AMR, perhaps the most important factor in optimizing the available computational resources is the selection of refinement criteria that best match the length scales arising in the computation. The smaller the percentage of the computational domain that requires the finest resolution, the finer that resolution can be. Recognizing this rule, we very carefully research what the optimal refinement criteria are during our preparation for production runs. Pre-production preparation, however, is never enough. Because the simulations often take us into unknown territory in terms of both the physical regime and the exercise of computational resources, the evaluation and modification of refinement criteria is an ongoing process during the entire simulation.</p>
<p>In the GCD simulation, the early stages operate with the default criteria provided by PARAMESH, which monitor the second spatial derivatives of specified mesh variables. Until the bubble breaks through the surface of the star, a very small percentage of blocks need to be at the finest spatial resolution, the fill factor is low and uniform criteria inside the stellar material work well. After the bubble breaks through the surface, the number of blocks that need to be at the finest or nearly the finest spatial resolution grows rapidly because the ash in the bubble flows over the stellar surface, pushing unburnt matter in front of it. During the large-scale simulations, it rapidly became apparent that the actual fill factor was going to be significantly higher than the estimate derived from test runs. Therefore, the highest level of refinement would have to be reduced for the simulation to fit into the available memory resources.</p>
<p>However, while the default criteria caused much of the computational domain to be refined at the finest level as the hot ash from the bubble flowed over the stellar surface, the matter lying far from the center of the star is of little scientific interest. Verification simulations show that the fidelity of the simulation would not be compromised if the computational domain lying outside a certain radius was not refined. The customizability of FLASH, which allowed us to include geometry-based refinement criteria in addition to the default criterion that uses the second spatial derivatives of physical variables, was crucial in this instance. The geometry-based refinement criterion leveled off the growth of the block count to manageable levels, as shown in <xref ref-type="fig" rid="fig3-1094342012464404">Figure 3</xref>.</p>
<fig id="fig3-1094342012464404" position="float">
<label>Figure 3.</label>
<caption>
<p>Growth of total leaf and parent blocks over the time progression of a GCD simulation. The first relatively flat region occurs before the bubble breakout from the surface of the star, and the last flat region occurs when refinement above a given radius is turned off. In the region in between, the block count grows based on default criterion before the geometry-based refinement turnoff.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig3.tif"/>
</fig>
<p>Further refinement criteria must be implemented later in the simulation, when the hot ash begins to converge at the opposite point on the stellar surface to where bubble breakout occurred. The converging hot ash produces inwardly and outwardly directed jets. The part of the domain where the jets occur needs to be refined well before the converging ash collides, in order to resolve what happens when it does. None of FLASH’s existing physics-based criteria for refinement could handle this situation. The need for refinement in this part of the computational domain became clear only during analysis of the simulation as it was in progress, which showed that an unphysical artifact had formed ahead of the front. In this case, the problem was solved by implementing an additional geometry-based criterion that refines a cone-shaped region (see <xref ref-type="fig" rid="fig4-1094342012464404">Figure 4</xref>) where jets are expected to form before the flow arrives.</p>
<fig id="fig4-1094342012464404" position="float">
<label>Figure 4.</label>
<caption>
<p>After bubble breakout, hot ash spreads across the surface of the star and collides at a point opposite that where breakout occurred. Full mesh refinement was imposed within the gray cone during the GCD simulation but before the flow arrived in the region, based on the expectation of jet formation.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig4.tif"/>
</fig>
</sec>
<sec id="section19-1094342012464404">
<title>6.5. Mesh regridding</title>
<p>As mentioned in section 3, the blocks in PARAMESH, and therefore in FLASH, are organized in an octree data structure, and the solution advances only on blocks corresponding to the leaf nodes. The blocks are created by the refinement and destroyed by the derefinement processes as the regions of high resolution change during the course of the simulation. For every refine/derefine (<italic>regridding</italic>) step, the blocks are redistributed among processors in order to maintain load balance. The process of regridding changes the local identities of the blocks, thereby changing their neighborhood metadata. Efficient implementation of guard cell filling requires local knowledge about the location and identity of off-processor blocks that share a face, edge, or vertex with any of the local blocks. The data structure that maintains this, and other similar meta-information about such blocks, is called the <italic>local tree</italic>. Building the local tree is a cost associated only with the regridding; guard cell fills in evolution steps do not incur this cost.</p>
<p>PARAMESH includes efficient methods to maintain the neighborhood meta-information about all blocks that are face neighbors of any of the local blocks, but not for edge or vertex neighbors. To complete the local tree view that includes edge and vertex neighbors required for FLASH, each processor sends metadata about all local blocks to its adjacent processor, which in turn forwards it to its adjacent processor and so on until <italic>all</italic> processes have been visited. After each hop, there is a check to see whether the received metadata describes any blocks that are non-face-neighbors of the local blocks. This algorithm is equivalent to traversing all processors in a ring topology, which is not scalable with processor count. The local tree formation did not pose noticeable performance problems until the advent of near-petascale machines such as BG/P Intrepid with up to 163,840 processors available for simulations. With the GCD simulations operating in a physical regime where there is a regridding event every two to three timesteps, a non-scalable algorithm became a serious performance bottleneck for this class of machines.</p>
<p>Looking for ways to eliminate this performance bottleneck, we found it possible to augment PARAMESH’s original face-neighbor metadata-generating mechanism to include edge and vertex neighbors. In the pre-existing algorithm, whenever blocks refine, new blocks establish links to their face neighbors and face neighbors establish links to the new blocks. Since child blocks are created on the same processor as the parent, it is possible for the new child blocks to establish links to all surrounding blocks, and not just the face neighbors. The cost for the augmented algorithm was a few extra local communications. This change eliminated the regridding bottleneck, enabling the GCD simulations to scale to the full BG/P Intrepid machine. The drastic improvement in performance is shown in <xref ref-type="fig" rid="fig5-1094342012464404">Figure 5</xref>.</p>
<fig id="fig5-1094342012464404" position="float">
<label>Figure 5.</label>
<caption>
<p>Result of optimization in the regridding step.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="section20-1094342012464404">
<title>7. Preparing for the next generation</title>
<p>The previous sections detail ways in which we have adapted FLASH to the latest high-performance computing platforms, through a combination of pre-production analyses and mid-simulation changes. We are also preparing for future challenges. The next generation petascale machines are expected to have increased parallelism due to two factors. Firstly, increased fine-grained parallelism is implied by the trend towards multicore nodes with increasing core count, and with possibly heterogeneous architectures. Secondly, to attain production-grade petascale performance, it is also necessary to proportionately increase the node count. Thus applications will need to enhance both their micro- and macro-parallelism in order to be able to fully exploit future platforms. In addition, efficient I/O will be critical for simulations like those of the GCD model of SNe Ia, where near-complete snapshots of nearly all of the physical variables in the simulation are needed for scientific data analysis.</p>
<p>The Flash Center is actively working to prepare FLASH for future platforms. The reduction in memory per core is one of the biggest challenges facing the current parallel model of one MPI process per core. We can exploit FLASH’s block structure to manage intra-node parallelism, because multiple blocks are assigned to each MPI process. Hence one MPI process can be mapped to one node, with intra-node parallelism achieved by processing each block in a separate thread. In addition, we have implemented the more traditional loop-level OpenMP directives for finer-grained parallelism as another option. At the time of this writing we are evaluating the relative performances of these approaches on Mira, the BG/Q machine, as a part of the acceptance suite of applications for this platform. <xref ref-type="fig" rid="fig6-1094342012464404">Figure 6</xref> shows the performance of the block-level threading implementations of this hybrid approach on the IBM BG/P for an application where the computation cost of the hydrodynamics solver and the mesh dominate over every other component. The application is configured to run with one MPI process per node. The block count per node does not change with thread count, though it changes within the node during the course of the evolution because of refinement. PARAMESH is not threaded; it therefore introduces a sequential component to the overall execution time. The two bottom curves show the speedup when the sequential cost of PARAMESH is included in the evolution time, and the two top curves show the speedup when this cost is excluded. <xref ref-type="fig" rid="fig7-1094342012464404">Figure 7</xref> shows the relative performances of the two threading approaches described above on Vesta, the BG/Q early access hardware platform at ANL. The plots show that both approaches perform quite well, with loop-level threading within blocks giving slightly better results. <xref ref-type="fig" rid="fig8-1094342012464404">Figure 8</xref> shows plots with different combinations of MPI ranks and threads per node on the BG/Q. In general these experiments indicate that a judicious choice of number of MPI processes per node with a modest number of threads per process give good results on the BG/Q.</p>
<fig id="fig6-1094342012464404" position="float">
<label>Figure 6.</label>
<caption>
<p>Result of threading at block level for intra-node parallelism.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig6.tif"/>
</fig>
<fig id="fig7-1094342012464404" position="float">
<label>Figure 7.</label>
<caption>
<p>Speed up with block level versus loop level threading on BG/Q.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig7.tif"/>
</fig>
<fig id="fig8-1094342012464404" position="float">
<label>Figure 8.</label>
<caption>
<p>Performance with various combinations of MPI ranks and count of threads per node on BG/Q.</p>
</caption>
<graphic xlink:href="10.1177_1094342012464404-fig8.tif"/>
</fig>
<p>I/O optimization has been an ongoing process during the entire history of the development of FLASH. Each new generation of HPC platforms has exposed performance limitations of either the I/O implementation in FLASH, the parallel file system, or the I/O software stack. Our response to I/O challenges has been to work in close collaboration with researchers in the field and keep the code updated with advances and optimizations. Specifics of I/O optimizations for the Blue Gene series of machines, including our plans for forthcoming platforms, are described in <xref ref-type="bibr" rid="bibr20-1094342012464404">Latham et al. (2012</xref>).</p>
</sec>
<sec id="section21-1094342012464404">
<title>8. Experience gained</title>
<p>The Flash Center has had vast experience in successfully running simulations on several generations of leading-edge HPC platforms. As a part of an ASC initiative, the Flash Center has had access to unclassified versions or partitions of the platforms installed at the three Defense Program laboratories: Los Alamos National Laboratory, Lawrence Livermore National Laboratory, and Sandia National Laboratories. The early ASCI machines included ASCI Blue Mountain built by SGI, Blue Pacific built by IBM, and Red built by Intel. The FLASH code was among the first codes ported to these machines as they became available (<xref ref-type="bibr" rid="bibr28-1094342012464404">Rosner et al., 2000</xref>). Early science from these machines included a study of the cellular structure of astrophysical detonations (<xref ref-type="bibr" rid="bibr31-1094342012464404">Timmes et al., 2000</xref>) and a study of helium detonations on the surfaces of neutron stars, a model for X-ray bursts (<xref ref-type="bibr" rid="bibr38-1094342012464404">Zingale et al., 2001</xref>). In 2000 the FLASH code won the Gordon Bell Prize in a Special Category for reactive fluid flow simulations using AMR that achieved 238 GFlops on 6420 processors of ASCI Red (<xref ref-type="bibr" rid="bibr3-1094342012464404">Calder et al., 2000</xref>). In December 2005, the IBM BG/L machine to be commissioned at LLNL became briefly available for unclassified use, with preparation time of less than one month. We used this opportunity to run a weakly compressible stirred-turbulence simulation at an unprecedented resolution (<xref ref-type="bibr" rid="bibr1-1094342012464404">Antypas et al., 2006</xref>; <xref ref-type="bibr" rid="bibr11-1094342012464404">Fisher et al., 2008</xref>). The GCD simulations described in this paper have been run on Unclassified Purple, an IBM Power 5 platform at LLNL; Bassi, an IBM Power 5 platform, and Seaborg, an IBM Power 3+ platform, at Lawrence Berkeley National Laboratory; and Intrepid, an IBM Bluegene/P platform at ANL.</p>
<p>We believe that the single most important reason for our success is the presence of an extremely strong interdisciplinary team, which not only spans the expertise needed, but equally importantly is tightly coupled. We practiced co-design between the code team and domain scientists long before the term entered common usage. As a consequence, team members not only bring varied perspectives to problem solving, but can also effectively communicate with each other and work collaboratively to find solutions. The second important conclusion we reached is that no amount of pre-production preparation can be counted on to be adequate. This statement is not to deny the importance of planning and pre-production preparation; without it, simulations would have no chance of success. Rather, scientifically significant simulations almost always enter unexplored territory in physical regimes and in pushing the available computational resources. Therefore, the team carrying out the simulations needs to continually evaluate the application code against performance and scientific metrics, be prepared for the unexpected, and adapt the code as necessary. It is equally important for the supercomputing facilities to make provision for adequate resources for simulation planning and testing at scale.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure" id="fn11-1094342012464404">
<label>Funding</label>
<p>This work was supported by the ASC/Alliance Program NNSA, US Department of Energy (grant number B523820) to the Flash Center at the University of Chicago.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342012464404">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Antypas</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Calder</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Dubey</surname>
<given-names>A</given-names>
</name>
<etal/>
</person-group> (<year>2006</year>) <article-title>Scientific applications on the massively parallel BG/L machine</article-title>. In: <source>The international conference on parallel and distributed processing techniques and applications</source>, pp. <fpage>292</fpage>–<lpage>298</lpage>.</citation>
</ref>
<ref id="bibr2-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berger</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>1989</year>) <article-title>Local adaptive mesh refinement for shock hydrodynamics</article-title>. <source>Journal of Computational Physics</source> <volume>82</volume>: <fpage>64</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr3-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Calder</surname>
<given-names>AC</given-names>
</name>
<name>
<surname>Curtis</surname>
<given-names>BC</given-names>
</name>
<name>
<surname>Dursi</surname>
<given-names>LJ</given-names>
</name>
<etal/>
</person-group> (<year>2000</year>) <article-title>High-performance reactive fluid flow simulations using adaptive mesh refinement on thousands of processors</article-title>. In: <source>Proceedings of Supercomputing 2000</source>.</citation>
</ref>
<ref id="bibr4-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Calder</surname>
<given-names>AC</given-names>
</name>
<name>
<surname>Townsley</surname>
<given-names>DM</given-names>
</name>
<name>
<surname>Seitenzahl</surname>
<given-names>IR</given-names>
</name>
<etal/>
</person-group> (<year>2007</year>) <article-title>Capturing the fire: Flame energetics and neutronization for Type Ia supernova simulations</article-title>. <source>The Astrophysical Journal</source> <volume>656</volume>: <fpage>313</fpage>–<lpage>332</lpage>.</citation>
</ref>
<ref id="bibr5-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Glaz</surname>
<given-names>H</given-names>
</name>
</person-group> (<year>1985</year>) <article-title>Efficient solution algorithms for the Riemann problem for real gases</article-title>. <source>Journal of Computational Physics</source> <volume>59</volume>: <fpage>264</fpage>–<lpage>289</lpage>.</citation>
</ref>
<ref id="bibr6-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Woodward</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>1984</year>) <article-title>The piecewise parabolic method (PPM) for gas-dynamical simulations</article-title>. <source>Journal of Computational Physics</source> <volume>54</volume>: <fpage>174</fpage>–<lpage>201</lpage>.</citation>
</ref>
<ref id="bibr7-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dubey</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Antypas</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Daley</surname>
<given-names>C</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Parallel algorithms for moving Lagrangian data on block structured Eulerian meshes</article-title>. <source>Parallel Computing</source> <volume>37</volume>: <fpage>101</fpage>–<lpage>113</lpage>.</citation>
</ref>
<ref id="bibr8-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dubey</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Antypas</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Ganapathy</surname>
<given-names>M</given-names>
</name>
<etal/>
</person-group> (<year>2009</year>) <article-title>Extensible component-based architecture for FLASH, a massively parallel, multiphysics simulation code</article-title>. <source>Parallel Computing</source> <volume>35</volume>: <fpage>512</fpage>–<lpage>522</lpage>.</citation>
</ref>
<ref id="bibr9-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dubey</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Daley</surname>
<given-names>C</given-names>
</name>
<name>
<surname>ZuHone</surname>
<given-names>J</given-names>
</name>
<etal/>
</person-group> (<year>2012</year>) <article-title>Imposing a Lagrangian particle framework on an Eulerian hydrodynamics infrastructure in FLASH</article-title>. <source>The Astrophysical Journal Supplement Series</source> <volume>201</volume>: <fpage>27</fpage>.</citation>
</ref>
<ref id="bibr10-1094342012464404">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Dursi</surname>
<given-names>LJ</given-names>
</name>
<name>
<surname>Zingale</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>Efficiency gains from time refinement on AMR meshes and explicit timestepping</article-title>. In: <person-group person-group-type="editor">
<name>
<surname>Plewa</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Linde</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Weirs</surname>
<given-names>V</given-names>
</name>
</person-group> (eds) <source>Adaptive Mesh Refinement-Theory and Applications</source>, Chicago, pp. <fpage>103</fpage>–<lpage>113</lpage>. Springer.</citation>
</ref>
<ref id="bibr11-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fisher</surname>
<given-names>RT</given-names>
</name>
<name>
<surname>Kadanoff</surname>
<given-names>LP</given-names>
</name>
<name>
<surname>Lamb</surname>
<given-names>DQ</given-names>
</name>
<etal/>
</person-group> (<year>2008</year>) <article-title>Terascale turbulence computation using the FLASH3 application framework on the IBM Blue Gene/L system</article-title>. <source>IBM Journal of Research and Development</source> <volume>52</volume>: <fpage>127</fpage>–<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr12-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fryxell</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Olson</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Ricker</surname>
<given-names>P</given-names>
</name>
<etal/>
</person-group> (<year>2000</year>) <article-title>FLASH: Adaptive mesh hydrodynamics code</article-title>. <source>The Astrophysical Journal Supplement Series</source> <volume>131</volume>: <fpage>273</fpage>–<lpage>334</lpage>.</citation>
</ref>
<ref id="bibr13-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gamezo</surname>
<given-names>VN</given-names>
</name>
<name>
<surname>Khokhlov</surname>
<given-names>AM</given-names>
</name>
<name>
<surname>Oran</surname>
<given-names>ES</given-names>
</name>
</person-group> (<year>2004</year>) <article-title>Deflagrations and detonations in thermonuclear supernovae</article-title>. <source>Physical Review Letters</source> <volume>92</volume>: <fpage>211102</fpage>.</citation>
</ref>
<ref id="bibr14-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Godunov</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>1959</year>) <article-title>A difference method for numerical calculation of discontinuous solutions of the equations of hydrodynamics</article-title>. <source>Matematicheskii Sbornik</source> <volume>89</volume>: <fpage>271</fpage>–<lpage>306</lpage>.</citation>
</ref>
<ref id="bibr15-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Höflich</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Gerardy</surname>
<given-names>CL</given-names>
</name>
<name>
<surname>Fesen</surname>
<given-names>RA</given-names>
</name>
<etal/>
</person-group> (<year>2002</year>) <article-title>Infrared spectra of the subluminous Type Ia supernova SN 1999by</article-title>. <source>The Astrophysical Journal</source> <volume>568</volume>: <fpage>791</fpage>–<lpage>806</lpage>.</citation>
</ref>
<ref id="bibr16-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jordan</surname>
<given-names>GI</given-names>
</name>
<name>
<surname>Fisher</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Townsley</surname>
<given-names>D</given-names>
</name>
<etal/>
</person-group> (<year>2008</year>) <article-title>Three-dimensional simulations of the deflagration phase of the gravitationally confined detonation model of Type Ia supernovae</article-title>. <source>The Astrophysical Journal</source> <volume>681</volume>: <fpage>1448</fpage>–<lpage>1457</lpage>.</citation>
</ref>
<ref id="bibr17-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Khokhlov</surname>
<given-names>AM</given-names>
</name>
</person-group> (<year>1991</year>) <article-title>Nucleosynthesis in delayed detonation models of Type Ia supernovae</article-title>. <source>Astronomy and Astrophysics</source> <volume>245</volume>: <fpage>L25</fpage>–<lpage>L28</lpage>.</citation>
</ref>
<ref id="bibr18-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Khokhlov</surname>
<given-names>AM</given-names>
</name>
</person-group> (<year>1995</year>) <article-title>Propagation of turbulent flames in supernovae</article-title>. <source>The Astrophysical Journal</source> <volume>449</volume>: <fpage>695</fpage>.</citation>
</ref>
<ref id="bibr19-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kuhlen</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Woosley</surname>
<given-names>SE</given-names>
</name>
<name>
<surname>Glatzmaier</surname>
<given-names>GA</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Carbon ignition in Type Ia supernovae. II. A three-dimensional numerical model</article-title>. <source>The Astrophysical Journal</source> <volume>640</volume>: <fpage>407</fpage>–<lpage>416</lpage>.</citation>
</ref>
<ref id="bibr20-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Latham</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Daley</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Liao</surname>
<given-names>W</given-names>
</name>
<etal/>
</person-group> (<year>2012</year>) <article-title>A case study for scientific I/O: Improving the FLASH astrophysics code</article-title>. <source>Computational Science and Discovery</source> <volume>5</volume>: <fpage>015001</fpage>.</citation>
</ref>
<ref id="bibr21-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Livne</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Asida</surname>
<given-names>SM</given-names>
</name>
<name>
<surname>Höflich</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>On the sensitivity of deflagrations in a Chandrasekhar mass white dwarf to initial conditions</article-title>. <source>The Astrophysical Journal</source> <volume>632</volume>: <fpage>443</fpage>–<lpage>449</lpage>.</citation>
</ref>
<ref id="bibr22-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>MacNeice</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Olson</surname>
<given-names>KM</given-names>
</name>
<name>
<surname>Mobarry</surname>
<given-names>C</given-names>
</name>
<etal/>
</person-group> (<year>2000</year>) <article-title>PARAMESH: A parallel adaptive mesh refinement community toolkit</article-title>. <source>Computer Physics Communications</source> <volume>126</volume>: <fpage>330</fpage>–<lpage>354</lpage>.</citation>
</ref>
<ref id="bibr23-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Meakin</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Seitenzahl</surname>
<given-names>I</given-names>
</name>
<name>
<surname>Townsley</surname>
<given-names>D</given-names>
</name>
<etal/>
</person-group> (<year>2009</year>) <article-title>Study of the detonation phase in the gravitationally confined detonation model of Type Ia supernovae</article-title>. <source>The Astrophysical Journal</source> <volume>693</volume>: <fpage>1188</fpage>.</citation>
</ref>
<ref id="bibr24-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Plewa</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Detonating failed deflagration model of thermonuclear supernovae. I. Explosion dynamics</article-title>. <source>The Astrophysical Journal</source> <volume>657</volume>: <fpage>942</fpage>–<lpage>960</lpage>.</citation>
</ref>
<ref id="bibr25-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Plewa</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Calder</surname>
<given-names>AC</given-names>
</name>
<name>
<surname>Lamb</surname>
<given-names>DQ</given-names>
</name>
</person-group> (<year>2004</year>) <article-title>Type Ia supernova explosion: Gravitationally confined detonation</article-title>. <source>The Astrophysical Journal</source> <volume>612</volume>: <fpage>L37</fpage>–<lpage>L40</lpage>.</citation>
</ref>
<ref id="bibr26-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Röpke</surname>
<given-names>FK</given-names>
</name>
<name>
<surname>Hillebrandt</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Schmidt</surname>
<given-names>W</given-names>
</name>
<etal/>
</person-group> (<year>2007</year>) <article-title>A three-dimensional deflagration model for Type Ia supernovae confronted with observations</article-title>. <source>The Astrophysical Journal</source> <volume>668</volume>: <fpage>1132</fpage>–<lpage>1139</lpage>.</citation>
</ref>
<ref id="bibr27-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Röpke</surname>
<given-names>FK</given-names> </name>
<name>
<surname>Niemeyer</surname>
<given-names>JC</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Delayed detonations in full-star models of type Ia supernova explosions</article-title>. <source>Astronomy and Astrophysics</source> <volume>464</volume>: <fpage>683</fpage>–<lpage>686</lpage>.</citation>
</ref>
<ref id="bibr28-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rosner</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Calder</surname>
<given-names>AC</given-names>
</name>
<name>
<surname>Dursi</surname>
<given-names>LJ</given-names>
</name>
<etal/>
</person-group> (<year>2000</year>) <article-title>Flash code: Studying astrophysical thermonuclear flashes</article-title>. <source>Computing in Science and Engineering</source> <volume>2</volume>: <fpage>33</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr29-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Strang</surname>
<given-names>G</given-names>
</name>
</person-group> (<year>1968</year>) <article-title>On the construction and comparison of difference schemes</article-title>. <source>SIAM Journal on Numerical Analysis</source> <volume>5</volume>: <fpage>506</fpage>–<lpage>517</lpage>.</citation>
</ref>
<ref id="bibr30-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Timmes</surname>
<given-names>FX</given-names>
</name>
<name>
<surname>Swesty</surname>
<given-names>FD</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>The accuracy, consistency, and speed of an electron-positron equation of state based on table interpolation of the Helmholtz free energy</article-title>. <source>The Astrophysical Journal Supplement Series</source> <volume>126</volume>: <fpage>501</fpage>–<lpage>516</lpage>.</citation>
</ref>
<ref id="bibr31-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Timmes</surname>
<given-names>FX</given-names>
</name>
<name>
<surname>Zingale</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Olson</surname>
<given-names>K</given-names>
</name>
<etal/>
</person-group> (<year>2000</year>) <article-title>On the cellular structure of carbon detonations</article-title>. <source>The Astrophysical Journal</source> <volume>543</volume>: <fpage>938</fpage>–<lpage>954</lpage>.</citation>
</ref>
<ref id="bibr32-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Townsley</surname>
<given-names>DM</given-names>
</name>
<name>
<surname>Calder</surname>
<given-names>AC</given-names>
</name>
<name>
<surname>Asida</surname>
<given-names>SM</given-names>
</name>
<etal/>
</person-group> (<year>2007</year>) <article-title>Flame evolution during Type Ia supernovae and the deflagration phase in the gravitationally confined detonation scenario</article-title>. <source>The Astrophysical Journal</source> <volume>668</volume>: <fpage>1118</fpage>–<lpage>1131</lpage>.</citation>
</ref>
<ref id="bibr33-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vladimirova</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Weirs</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Ryzhik</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>Flame capturing with an advection-reaction-diffusion model</article-title>. <source>Combustion Theory and Modeling</source> <volume>10</volume>: <fpage>727</fpage>–<lpage>747</lpage>.</citation>
</ref>
<ref id="bibr34-1094342012464404">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Warren</surname>
<given-names>MS</given-names>
</name>
<name>
<surname>Salmon</surname>
<given-names>JK</given-names>
</name>
</person-group> (<year>1993</year>) <article-title>A parallel hashed Oct-Tree N-body algorithm</article-title>. In: <source>Supercomputing ’93: Proceedings of the 1993 ACM/IEEE conference on supercomputing</source>, <publisher-loc>New York</publisher-loc>, <publisher-name>USA</publisher-name>, pp. <fpage>12</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr35-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Woodward</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>1984</year>) <article-title>The numerical simulation of two-dimensional fluid flow with strong shocks</article-title>. <source>Journal of Computational Physics</source> <volume>54</volume>: <fpage>115</fpage>–<lpage>173</lpage>.</citation>
</ref>
<ref id="bibr36-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Woosley</surname>
<given-names>SE</given-names>
</name>
<name>
<surname>Wunsch</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Kuhlen</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2004</year>) <article-title>Carbon ignition in Type Ia supernovae: An analytic model</article-title>. <source>The Astrophysical Journal</source> <volume>607</volume>: <fpage>921</fpage>–<lpage>930</lpage>.</citation>
</ref>
<ref id="bibr37-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zingale</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Almgren</surname>
<given-names>AS</given-names>
</name>
<name>
<surname>Bell</surname>
<given-names>JB</given-names>
</name>
<etal/>
</person-group> (<year>2009</year>) <article-title>Low Mach number modeling of Type IA supernovae. IV. White dwarf convection</article-title>. <source>The Astrophysical Journal</source> <volume>704</volume>: <fpage>196</fpage>–<lpage>210</lpage>.</citation>
</ref>
<ref id="bibr38-1094342012464404">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zingale</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Timmes</surname>
<given-names>FX</given-names>
</name>
<name>
<surname>Fryxell</surname>
<given-names>B</given-names>
</name>
<etal/>
</person-group> (<year>2001</year>) <article-title>Helium detonations on neutron stars</article-title>. <source>The Astrophysical Journal Supplement Series</source> <volume>133</volume>: <fpage>195</fpage>–<lpage>220</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>