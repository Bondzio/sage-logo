<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">CON</journal-id>
<journal-id journal-id-type="hwp">spcon</journal-id>
<journal-title>Convergence</journal-title>
<issn pub-type="ppub">1354-8565</issn>
<issn pub-type="epub">1748-7382</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1354856511429641</article-id>
<article-id pub-id-type="publisher-id">10.1177_1354856511429641</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Web media and the quantitative content analysis: Methodological challenges in measuring online news content</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Sjøvaag</surname>
<given-names>Helle</given-names>
</name> 
<xref ref-type="aff" rid="aff1-1354856511429641"/>
<xref ref-type="corresp" rid="corresp1-1354856511429641"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Stavelin</surname>
<given-names>Eirik</given-names>
</name>
<xref ref-type="aff" rid="aff1-1354856511429641"/>
</contrib>
<aff id="aff1-1354856511429641">The University of Bergen, Norway</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-1354856511429641">Helle Sjøvaag, Department of Information Science and Media Studies, The University of Bergen, PO Box 7802, 5020 Bergen, Norway Email: <email>helle.sjovaag@infomedia.uib.no</email>
</corresp>
<fn fn-type="other" id="fn23-1354856511429641">
<p>Helle Sjøvaag is a researcher and Eirik Stavelin is a PhD candidate at the Department of Information Science and Media Studies at the University of Bergen, Norway.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>18</volume>
<issue>2</issue>
<fpage>215</fpage>
<lpage>229</lpage>
<permissions>
<copyright-statement>© SAGE Publications 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This article presents a method for quantitative content analysis of news online. The research design is based on a triangulation approach, using qualitative and quantitative measures combined with automated computer-assisted analysis. Used to perform a content analysis of the online news output of the Norwegian Broadcasting Corporation [NRK] from 2009, this approach revealed that methodologies designed for measuring broadcasting news content do not suffice in the online news environment. Online research methods need to be redesigned to account for the medium-specific news features on the internet. Computer-assisted coding methods can contribute depth and scale to such an analysis, as it can extract and assemble detailed data on large quantities of articles. Using a combination of automatic coding methods with established content analysis for television news, this article presents a new design for quantitative content analysis of news online.</p>
</abstract>
<kwd-group>
<kwd>information science</kwd>
<kwd>journalism</kwd>
<kwd>online news</kwd>
<kwd>quantitative content analysis</kwd>
<kwd>research methods</kwd>
<kwd>web analysis</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1354856511429641">
<title>Introduction</title>
<p>This article presents a method for quantitative content analysis of news online. The research design is based on a triangulation approach, using qualitative and quantitative measures combined with automated computer-assisted analysis. Used to perform a content analysis of the online news output of the Norwegian Broadcasting Corporation (NRK) from 2009, this approach revealed that methodologies designed for measuring broadcasting news content do not suffice in the online news environment. Online research needs to be redesigned to account for the medium specific news features on the internet. This is particularly relevant for the design of variables which register latent content and which require human judgment, as well as for establishing methods for registering manifest content that is quantifiable by simple computing methods in online news dissemination. Computer-assisted coding methods can contribute depth and scale to such an analysis, by extracting and assembling detailed data on large quantities of articles. Using a combination of automatic coding methods and established content analysis for television news, this article presents a new method for quantitative content analysis of news online.</p>
</sec>
<sec id="section2-1354856511429641">
<title>Case study</title>
<p>The case study analyses the Norwegian Broadcasting Corporation’s (NRK) website nrk.no for 2009. NRK is the Norwegian publicly funded public service broadcaster. It is the largest media outlet for television and radio news and the third most frequently visited online site in Norway (TNS <xref ref-type="bibr" rid="bibr25-1354856511429641">Gallup, 2010</xref>). The project analyzed consists of four sets of sample data: dataset (1) comprises n = 74,430 news articles<sup>
<xref ref-type="fn" rid="fn1-1354856511429641">1</xref>
</sup> published by NRK on its website during 2009 (quantitative and computer coded); dataset (2) consists of n = 2162 news articles published on 10 preselected dates in 2009 (quantitative and manually coded); dataset (3) consists of n = 1192 top-10 front page stories published on the same dates and thus comparable (quantitative and manually coded); and dataset (4) comprises two in-depth qualitative case study analyses of NRK’s national election and Copenhagen climate summit coverage in 2009. Items analyzed are limited to the text-based news articles published on nrk.no, and do not include redistribution of audiovisual content from broadcasting to the internet. The aims of the project were to assess (a) the degree to which nrk.no presented a continuous and updated news agenda online during 2009; (b) the geographical and thematic distribution of its news content; (c) front page priorities; (d) the depth and perspective of the news content; and (e) the degree to which nrk.no used interactive or other internet specific tools in its news dissemination.</p>
<p>Overall, the findings confirm that nrk.no presented a continuous and updated news service during 2009. The analysis also shows that the news content on nrk.no was characterized by a high level of local news. However, its front page bore the hallmarks of a national online news site and was dominated by international news, politics, crime stories, and popular culture. However, nrk.no was limited in its use of interactive elements. The website had limited use of external links, embedded video and audio, few quizzes, tests and little commentary. It was typical of the online news genre, with short news stories averaging 260 words, ‘breaking news’ and a high publication frequency with over 200 published texts per day. We triangulated our methods to produce the findings. We used an inductive research method on latent content – that is, content requiring judgment to assign coding values to news items, such as sports, politics, business and crime. Then we employed a deductive method on manifest content – that is, content which is observable and countable, such as hyperlinks, pictures, publication date and author (<xref ref-type="bibr" rid="bibr18-1354856511429641">Krippendorff, 2004</xref>: 20; <xref ref-type="bibr" rid="bibr20-1354856511429641">Neuendorf, 2002</xref>: 23). These two methods pointed out the problems in transferring methods of content analysis from audiovisual media to the online environment, which we will discuss in detail later in the article.</p>
</sec>
<sec id="section3-1354856511429641">
<title>The quantitative content analysis of news</title>
<p>Measures of content variables in media messages have been conducted for over 50 years, and quantitative methods are growing increasingly popular in the communication sciences (<xref ref-type="bibr" rid="bibr20-1354856511429641">Neuendorf, 2002</xref>: 27). Quantitative content analysis methods in fact grew out of quantitative newspaper analysis in the USA in the 1920s. Many such early studies were concerned with the quality of the news presented (<xref ref-type="bibr" rid="bibr20-1354856511429641">Neuendorf, 2002</xref>: 5). This focus has remained central, such as in the study of foreign news (<xref ref-type="bibr" rid="bibr9-1354856511429641">Galtung and Ruge, 1965</xref>) and Glasgow Media Group’s <italic>Bad News</italic> studies (<xref ref-type="bibr" rid="bibr11-1354856511429641">Group, 1976</xref>).</p>
<p>In Scandinavia this method has become common in studies of news. In Sweden, Asp, as well as Djerf-Pierre and Weibull, and Jönsson and Strömbäck have employed quantitative analysis techniques on the study of the news (<xref ref-type="bibr" rid="bibr2-1354856511429641">Asp, 1995</xref>; <xref ref-type="bibr" rid="bibr6-1354856511429641">Djerf-Pierre and Weibull, 2001</xref>; <xref ref-type="bibr" rid="bibr14-1354856511429641">Jönsson, 2004</xref>; <xref ref-type="bibr" rid="bibr15-1354856511429641">Jönsson and Strömbäck, 2007</xref>). In Denmark, <xref ref-type="bibr" rid="bibr13-1354856511429641">Hjarvard (1999</xref>) and in Norway, Sand and Helland, and Waldahl et al.’s work is in this field (<xref ref-type="bibr" rid="bibr12-1354856511429641">Helland, 1993</xref>; <xref ref-type="bibr" rid="bibr24-1354856511429641">Sand and Helland, 1998</xref>; <xref ref-type="bibr" rid="bibr26-1354856511429641">Waldahl et al., 2002</xref>, <xref ref-type="bibr" rid="bibr27-1354856511429641">2009</xref>). This project falls within the same tradition of quantitative content analyses of news, but attempts to appropriate methods to the study of the online medium.</p>
</sec>
<sec id="section4-1354856511429641">
<title>Web analysis</title>
<p>The particular features of the web need to be accounted for both in the research design and in the analysis of coded content. Lev Manovich points out the underlying structure – the database – as an important form of cultural expression. He explains its basic elements: ‘Where the database form really flourished ... is the Internet. As defined by original HTML, a Web page is a sequential list of separate elements – text blocks, images, digital video clips, and links to other pages’ (<xref ref-type="bibr" rid="bibr19-1354856511429641">Manovich, 2001</xref>: 220). The web however is structured less strictly than a true database,<sup>
<xref ref-type="fn" rid="fn2-1354856511429641">2</xref>
</sup> but the structure of an online newspaper webpage is sufficiently rigid for researchers to perform an automated quantitative analysis of its media content with database-like queries.</p>
<p>The automated or computer-assisted content analysis is automated in the sense that a script registers the properties of the articles in a consecutive loop. However, as Neuendorf observes ‘typical computer coding analysis is limited to text only’ (<xref ref-type="bibr" rid="bibr20-1354856511429641">Neuendorf, 2002</xref>: 126). Our method is limited to words only, but it is adapted for the web. It does not try to categorize content through the words of the stories, but through the structured markup<sup>
<xref ref-type="fn" rid="fn3-1354856511429641">3</xref>
</sup> and URL structure of the website. This makes the method different from earlier automated methods in media studies. Also it makes the approach media specific as it is designed for web content. Although its reach is limited to textual data, it can also detect and quantify various media elements such as images or pictures, video, audio and interactive elements. This is because these web-specific elements can be detected within the semi-structured nature of HTML.</p>
<p>As IBM programmer George Fuechsel is alleged to have said ‘Garbage in, garbage out’, in our terms computers cannot cope with unexpected input. The quality of the analysis will improve in closer proximity to the original material. The computer-assisted analysis we propose utilizes and attempts to recreate the schema of the underlying database. The site’s markup is a distorted representation of the database. Although the markup helps to identify metadata (e.g., publication date and author), it is mostly a distraction as the desired matter is news articles. All other content – visual representation, navigation, advertisement – is in this context considered ‘garbage’ and needs to be filtered out. The ideal dataset for automated content analysis of an online news site is its underlying database.</p>
<p>Prior knowledge of the web structure can be drawn on to aid the research design. A key issue here is the structure of web documents as produced by CMSs.<sup>
<xref ref-type="fn" rid="fn4-1354856511429641">4</xref>
</sup> ‘Boiler plate’ header, footer and global menus run across all or most of the documents. Boilerplate text is usually generated automatically, and is often removed in linguistic, statistic or other analytic inquiries (<xref ref-type="bibr" rid="bibr8-1354856511429641">Fairon and Naets, 2007</xref>). The typical boilerplate for a news site is relatively easy to identify (<xref ref-type="bibr" rid="bibr17-1354856511429641">Kohlschütter et al., 2010</xref>). Our dataset operated with only one boilerplate, something that simplified its identification and thus facilitated the further steps in the process significantly.</p>
<p>NRK is a broadcaster, however; its webpage resembles an online edition of a newspaper. A large quantity of online news is what Boczkowski calls ‘shovelware’. This is a term that refers to ‘the common practice of taking information generated originally for the paper’s print edition and deploying it virtually unchanged onto the website (<xref ref-type="bibr" rid="bibr4-1354856511429641">Boczkowski, 2005</xref>). Although the content is not ‘shoveled’ from a printed newspaper, it is ported from TV or radio content. A premise for ‘shoveling’ or porting content is the capability of the CMS to manage the presentation and publication online. Knowledge of how a CMS works leads to the assumption that this material is forced into a predefined pattern. Most attributes will remain constant, although ‘shoveled’ from one platform to another. Attributes that news media traditionally separate from the body text in publication, such as headlines, publication date, author and so on, can be extracted automatically after being forced through the CMS. Knowledge of exactly which elements the CMS allows is not possible prior to enquiry. The codebook should include all elements necessary to answer the research questions, however the selectors need to be formulated and tested before the list of elements in the codebook can be finalized. Both the capacity of the CMS and the consistency of the output markup can limit the list of elements feasible for extraction.</p>
</sec>
<sec id="section5-1354856511429641">
<title>Assumptions, problems and solutions</title>
<p>Few model research designs existed that we could use for the purpose of this analysis (<xref ref-type="bibr" rid="bibr16-1354856511429641">Karlsson and Strömbäck, 2010</xref>: 5–6). Our design outline was based on a previous quantitative content analysis of Norwegian television news (including NRK news) conducted by Professor Ragnar Waldahl and colleagues at the University of Oslo (<xref ref-type="bibr" rid="bibr26-1354856511429641">Waldahl et al., 2002</xref>, <xref ref-type="bibr" rid="bibr27-1354856511429641">2009</xref>). Our assumption was that Waldahl et al.’s content categories and variables used to analyze NRK’s traditional broadcast news could be used in our study of its online news output. This proved partially true. The basic latent content categories used – business and economy; politics; crime; accidents; entertainment and features, social issues; and weather – are comprehensive enough to apply in most studies of news content. However, in this study, assumptions regarding content profile and the latent and manifest variables created problems in the data collection process, in the coding process, in the analysis and in the reliability testing. The solutions to these methodological problems brought new insights into NRK as a news producer and the web as a news medium, which would have gone unnoticed without this type of method triangulation. In the following we present our assumptions in the design of the method, the problems these assumptions created for the design, and how these problems were eventually resolved.</p>
</sec>
<sec id="section6-1354856511429641">
<title>Computer-assisted data analysis</title>
<p>Some prior research conducted on the classification of latent content through computation exists that is relevant to this study. Computer-assisted approaches to content analysis can be found (<xref ref-type="bibr" rid="bibr20-1354856511429641">Neuendorf, 2002</xref>), as can relevant examples within sentiment analysis (<xref ref-type="bibr" rid="bibr21-1354856511429641">Pang and Lee, 2008</xref>) and in the tradition of information retrieval and search engine research (<xref ref-type="bibr" rid="bibr7-1354856511429641">Dumais and Chen, 2000</xref>). However, we did not have access to prior research that could aid in the research design of an automated quantitative computer-assisted analysis of news content online. The codebook, selector code and analysis method had to be designed from scratch. In this article we propose a computer-assisted method for the coding of manifest content, while human labor is still considered superior for the coding of latent content.</p>
<p>On the assumption that the CMS-produced markup is accessible across all the material, and that its content (minus boilerplate) is universally printed and thus possible to automatically select and quantify, our goal was to reverse engineer the underlying database (<xref ref-type="bibr" rid="bibr5-1354856511429641">Chikofsky and Cross, 1990</xref>). This list of attributes contains metadata such as publication and updated date stamp, geographical location (through district office designation in the URL); news agency (syndication services), and author. Other collected attributes were more web-specific, such as media players (audio/video), flash games, polls, questioners, hyperlinks, and commentary sections. These web-specific elements were counted for each article in the sample. The resulting structured data were then statistically analyzed.</p>
<p>When planning computer-assisted analysis of an online news site the first action is to select what data to base the analysis on, and determine how to obtain this data. We tried to arrange a database dump to be performed by NRK, but did not succeed for various reasons. Instead we obtained an index of the nrk.no website. The alternative would have been web crawling to gather information and material located under the domain name. The index contained URLs date stamped with the latest update in XML<sup>
<xref ref-type="fn" rid="fn5-1354856511429641">5</xref>
</sup> format. This data set allowed us to narrow down the sample from over half a million URLs to merely the items updated in 2009 – about 75,000 URLs.</p>
<p>
<xref ref-type="fig" rid="fig1-1354856511429641">Figure 1</xref> shows what an entry in the index looks like.</p>
<p>This practice of date-stamping items with the update date rather than original publication date meant that we did not have the full set of published articles for 2009. About 4 per cent of the articles in the final sample were not published in 2009, but were updated in 2009. Likewise, some 2009 publications were lost because they were updated in 2010, and hence had a 2010 date stamp. Once the data selection was completed, we downloaded<sup>
<xref ref-type="fn" rid="fn6-1354856511429641">6</xref>
</sup> the documents to a local machine and performed a boilerplate removal process to get rid of the elements surrounding the articles, such as header, menu system, footer and advertisements.</p>
<fig id="fig1-1354856511429641" position="float">
<label>Figure 1.</label>
<graphic alternate-form-of="fig1-1354856511429641" xlink:href="10.1177_1354856511429641-fig1.tif"/>
</fig>
<p>The remaining parts of each item now consisted of markup relevant to the analysis. We wrote snippets of code to select and count the elements specified in the codebook.<sup>
<xref ref-type="fn" rid="fn7-1354856511429641">7</xref> </sup>We refer to these snippets as selectors, as they select elements in the remaining markup. The language of choice was <italic>python</italic> with the extending parser <italic>BeautifulSoup</italic>.<sup>
<xref ref-type="fn" rid="fn8-1354856511429641">8</xref>
</sup> The selector code snippets where tested one by one as they were written, and assembled as a single script in the end.</p>
<p>
<xref ref-type="fig" rid="fig2-1354856511429641">Figure 2</xref> shows an example of a selector counting polls.</p>
<p>Elements of the articles are held by the markup, and these can be registered and counted as in the example in <xref ref-type="fig" rid="fig2-1354856511429641">Figure 2</xref>. The same procedure was used for most of the attributes specified in the codebook. The result of this automatic analysis was a .csv file containing one row of attributes: present; lacking; count numbers; and selected values for each article in the sample. The .csv is compatible with most commercial software for statistical analysis, as well as spreadsheets and databases. This flexibility made the further treatment of data possible in using various tools for statistical analysis, including SPSS, MS Excel and mySQL.</p>
<fig id="fig2-1354856511429641" position="float">
<label>Figure 2.</label>
<graphic alternate-form-of="fig2-1354856511429641" xlink:href="10.1177_1354856511429641-fig2.tif"/>
</fig>
<p>This design had both strengths and weaknesses. The optimal dataset was not likely to be available, selection through an index proved inaccurate and latent content was excluded. Despite these issues the approach could handle large quantities of data and the amount of noise from inaccurate selection through the index was limited to 4 per cent of the content. This could easily be excluded later in the analysis.</p>
</sec>
<sec id="section7-1354856511429641">
<title>Attaining the content profile</title>
<p>As NRK is a license-fee-funded public service broadcaster, part of its remit is to reflect and disseminate news from around the country. NRK has 13 local news offices that contribute around 80 per cent of the daily news content published on nrk.no. This local online news production quota is not reflected on NRK’s television channels,<sup>
<xref ref-type="fn" rid="fn9-1354856511429641">9</xref>
</sup> where news dissemination is dominated by the production of NRK’s central office in Oslo. nrk.no’s front page and its national news agenda reflects this, with only 2 per cent local news content, as opposed to 45 per cent local news content on the website’s ‘inside’. This production- and institution-driven difference in news content priorities between centre and periphery is reflected also in the amount of international news found on the front page (23%), versus the amount of international news found on the website’s ‘inside’ (2%). The news agenda of the local offices are clearly different from the national agenda reflected on nrk.no’s front page. The substantial local news profile in the overall content sample thus entailed problems for the design of the method and the coding process. In particular, the study showed that content analysis designed for news content aimed at a national audience is not well suited for the study of local news.</p>
<p>Some of the online news attributes we wished to register were not found in the markup, but in the URL structure. This included the main news categories, local office and relevant attachment to TV or radio programs. This implied a problem of non-exclusive categories. In a library a book is either found on the shelf for drama or for romance. Digital categories do not demand such a rigid allocation. Items can be in several categories at once. For online news this makes perfect sense – an article can be relevant for publication both in the economy section and in the domestic affairs section<sup>
<xref ref-type="fn" rid="fn10-1354856511429641">10</xref>
</sup> so the article can be accessed via multiple portals,<sup>
<xref ref-type="fn" rid="fn11-1354856511429641">11</xref>
</sup> and thus potentially reach a wider public. While this is the norm for online publishing, it is problematic for a content analysis designed on the principle of mutual exclusivity.</p>
<p>If the news site operates with a strict policy on URL management collecting data elements from the URL structure can be reliable. However, in the case of nrk.no’s news publications from 2009, this turned out to be problematic. Of the URLs in the sample, 3.5 per cent<sup>
<xref ref-type="fn" rid="fn12-1354856511429641">12</xref>
</sup> were effectively double publications, in other words, the same article was published under two different URL addresses. Given the relatively low number of units here, we eliminated these by a distinct query in the database. This operation introduced a systemic bias to the selection, as these URL addresses were excluded based on alphabetical order. Given a higher number of such double-URL units in a similar sample, we would recommend a fully relational reconstruction of the database. As the number of attributes we could collect through the URLs was limited in this case, and given that the attributes turned out as expected – we considered the data reliable for analysis. We realized that the URL structure of a news site is another constructed representation of the material, and therefore is best kept out of the analysis when possible.</p>
<p>Categorization is a key task in quantitative analysis. Using the news categories described in the website’s GUI<sup>
<xref ref-type="fn" rid="fn13-1354856511429641">13</xref>
</sup> we found that no more than 9.5 per cent of the sample could be categorized, while 90.5 per cent of the material fell outside these categories. In addition four out of seven global header categories contained less than 1 per cent of the actual material. By altering the categories in this variable to account for the most common categories found in the URLs, the NA<sup>
<xref ref-type="fn" rid="fn14-1354856511429641">14</xref>
</sup> value was reduced to 20.9 per cent. This gave us a good understanding of how NRK manages and categorizes its own content, and further insight into NRK as a news organization. Of the content on nrk.no in 2009, 71.2 per cent was categorized as local news – a label not included in the global menu. This difference between the GUI categories and the URL structure indicates the ‘front page’ of a website does not necessarily reflect what is published inside.</p>
<p>The latent content variables contained up to eight sub-variables further specifying the news content including an ‘also’ category. The nature of the local news content presented some problems for this design, resulting in high ‘other’ percentages for the ‘social issues’, ‘crime’ and ‘entertainment/feature’ variables. For instance, ‘other social issues’ first registered at 30 per cent of the cases. News stories about power outages, roadblocks and hunting licenses in local communities were so plentiful in a local news-heavy sample that it was deemed necessary to establish a new sub-category ‘transport’. This reduced ‘other social issues’ to 20 per cent, which was still rather high. In the case of the ‘entertainment and feature’ variable, we found it necessary to add a ‘media’ category to the codebook to account for high ‘other’ values on nrk.no’s front page. As the front page is presented as a national news provider competing with other national online newspapers, its culture and entertainment profile was high at 23 per cent of the content. NRK is a national broadcaster that uses its front page to promote its own programming. Under the heading of entertainment and feature stories, the added ‘media’ category finally accounted for 10 per cent of the entertainment content on the front page, comprising mainly self-promotion. Hence, had the coding scheme not been adjusted to register ‘media-stories’, this medium-specific feature could have gone unnoticed. The same applies to the ‘crime’ variable, where ‘petty crime’ was separated from a large ‘other crime’ category. On analysis 13 per cent petty crime (such as driving violations and disruptive behavior) was published by NRK’s district offices, more evidence of the bias towards local news in nrk.no. Such methodological problems would not be an issue for a national newspaper where local stories are few and would belong in the category ‘other’.</p>
<p>In computer-assisted analysis of large quantities of data you must be able to select the elements you want and therefore you need to be able to test the selectors on a sample. Of course not all attributes in the codebook are indeed present in the material and thus feasible to extract. In the case of nrk.no 2009 we initially wrote selectors for elements tested on examples taken from nrk.no from 2010 which proved not to present in the analyzed material.<sup>
<xref ref-type="fn" rid="fn15-1354856511429641">15</xref>
</sup> It was reasonable to assume that the same formatting would be used. This is the best way of testing elements not easily spotted, and also an acceptable way of confirming the absence of an element. This methodological issue points out a weakness in the methodology itself: ad-hoc content and markup will fall outside the scope of the selectors, as they rely on the predictability of the CMS formatted markup.</p>
<p>Nil returns need to be interpreted in different ways. If a test for whether or not an article contains hyperlinks results in a nil return, this is considered a true valid result<sup>
<xref ref-type="fn" rid="fn16-1354856511429641">16</xref>
</sup> – the article does not contain a hyperlink. However when a test for whether an article contains a reference to a news agency results in a nil return, this is more problematic. It tells us nothing more than the fact that the article is not credited in a predictable structured manner. It does not confirm whether or not the article truly is syndicated. There could be other reasons: internal procedures within the news organization may be different across the various offices, or it may be sloppiness or poor CMS skills. It does not mean that the article is originally produced by NRK, only that the reference to a news agency is not found.</p>
<p>The codebook was designed to register its data in the SPSS statistical analysis tool. Whereas the SPSS system built for this analysis required NA categories for all variables, computational Boolean questions<sup>
<xref ref-type="fn" rid="fn17-1354856511429641">17</xref>
</sup> do not. NA categories for some of the Boolean questions in fact led to some confusion. When true or false options fail to cover all cases, a count number or a selected value was used. This was particularly effective in analyzing the usage of media players and other interactive elements, commentary sections and hyperlinks. In answering these questions the computer-assisted approach proved very effective.</p>
</sec>
<sec id="section8-1354856511429641">
<title>Coding design</title>
<p>The research design and coding scheme for this study were not only based on the demands of the research question, but also on the methods available, the nature of the medium under scrutiny and its content. Such adjustments are necessary to enable conclusions to be drawn (<xref ref-type="bibr" rid="bibr20-1354856511429641">Neuendorf, 2002</xref>: 74). Because we aimed to analyze a new medium, the coding scheme needed to remain open to adjustments before, during and after the coding process itself. A short pilot study enabled the mapping of internet-specific elements that could be automatically coded by computer, and equally identified online news markers in need of manual coding. Technical news-relevant variables coded automatically were designed from scratch, such as the interactive elements embedded in news articles and frequency of publication. The coding was also helped by frequent contact with NRK personnel, who provided information and answered questions. It was important also to establish the scope and limits of the material in both the manual and automatic analyses, and the basis on which conclusions could be drawn.</p>
<p>Waldahl et al.’s latent content variables were also found in this study. This includes separating items according to the domestic/international dimension, as well as the primary separation of content into business, crime, social issues, politics, accidents, feature, and sports (<xref ref-type="bibr" rid="bibr26-1354856511429641">Waldahl et al., 2002</xref>). Although adjusted for online news analysis, our findings suggest they continue to be relevant and appropriate content measures regardless of which news medium is under observation there by showing the durability of news as a particular type of institutionally produced content. News has certain enduring qualities across media platforms, and the separation of content characteristics is one of them. However, reducing this data into a finite set of variables comes with its own set of problems (<xref ref-type="bibr" rid="bibr28-1354856511429641">Weber, 1990</xref>: 15). More than challenging traditional journalistic content types, this study rather affirms the ubiquitous difficulties concerning inter-subjectivity for latent variables in the coding process, which introduced new categories that needed to be coded retroactively, for instance rarely used multimedia features, and the local news content category ‘transport’. In the following we suggest a method for the quantitative content analysis of online news, and discuss the reliability of results.</p>
</sec>
<sec id="section9-1354856511429641">
<title>Recommendations for quantitative content analysis of online news</title>
<p>We make nine recommendations to researchers embarking on the design of quantitative analysis of online news.<sup>
<xref ref-type="fn" rid="fn18-1354856511429641">18</xref>
</sup>
</p>
<sec id="section10-1354856511429641">
<title>1. Operationalize research questions or hypotheses</title>
<p>Any content analysis process necessarily starts with finding the appropriate methods to answer the research question or hypotheses validly. Some research questions cannot be answered using quantitative measures only, and should be supplied with qualitative content analysis of chosen cases within the sample. Once it has been established that the assembled data and chosen methodology can actually answer research questions or test hypotheses, the coding scheme can be designed.</p>
</sec>
<sec id="section11-1354856511429641">
<title>2. Design the coding scheme</title>
<p>A coding scheme for latent content should first of all be developed deductively based on available codebooks from previous quantitative content studies. This is necessary because previous coding schemes have already been validated through use, and they enable comparisons with previous studies. As coding of latent values entails judgment, a thorough codebook is important in the coding process to clarify gray areas (<xref ref-type="bibr" rid="bibr1-1354856511429641">Allern, 2001</xref>: 78). When creating a codebook for online news features, however, an inductive design is necessary.</p>
</sec>
<sec id="section12-1354856511429641">
<title>3. Define the sample</title>
<p>The codebook for automated analysis should be formulated according to the research questions, however the list of elements feasible for automated quantifications needs verification by functioning selectors. The sample needs to be clearly defined, and direct contact with the data holder is recommended to pursue a database dump, API access or an index-file. If neither is obtained, web crawling is necessary. When the sample is defined and downloaded, selectors can be written.</p>
</sec>
<sec id="section13-1354856511429641">
<title>4. Write the selectors</title>
<p>The programming language needs to have sufficient expressive power to select the desired elements in the markup. Writing the selectors one by one as snippets, with testing as a part of the process, is recommended. In the end all the snippets need assembling into one script capable of running though the sample. This will involve some additional programming to handle input/output and errors. We recommend using .csv as output format if no other specific formats are required by further analysis. Logging of the automated process is advised in all stages, to facilitate subsequent problem solving. Comment the code thoroughly. Plan ahead for suitable times to run the script, as this task can be time consuming if the sample is large.<sup>
<xref ref-type="fn" rid="fn19-1354856511429641">19</xref>
</sup> Test on smaller samples and involve team members in testing for errors early in the process.</p>
</sec>
<sec id="section14-1354856511429641">
<title>5. Run a pilot study</title>
<p>Once a schematic basis has been deemed suitable for the research questions, a pilot study should be carried out to map the extent to which the scheme is appropriate to the medium in question. This process allows for a development of new variables designed to capture medium-specific characteristics not already accounted for in the design. It also enables the removal or adjustment of inappropriate variables in the schematic basis that are either too time consuming to measure, or that are designed to measure things that are not available in the sample. The combination of computer-assisted coding with manual coding in this part of the process saves time, enhances the sample and increases validity. Markers that need only be quantified – such as the number of internal or external links within a text, the use of commentary and other interactive elements, and the update frequencies – can be collected automatically via the computer-assisted coding. Measures that require judgment according to codebook guidelines, such as content categorization and sources, must be coded manually.</p>
</sec>
<sec id="section15-1354856511429641">
<title>6. Start the coding process</title>
<p>Once manual and automatic coding schemes have been developed and tested, coding can commence. Multiple coders in the manual process is recommended, however single-person coding for projects with limited resources is valid provided reliability tests are preformed by a separate coder. The appropriateness of the codebook should be reviewed continuously during the coding process. Sometimes issues arise that have not been mapped in the pilot study. A question that arose during this project was how to code stories revolving around the activities of the military. It was decided that the appropriate coding for military matters was under the variable ‘politics’ and the sub-category ‘national politics’, as the military represents state administration. In cases such as these, precedents are established that should be noted and followed in the continuing coding process.</p>
</sec>
<sec id="section16-1354856511429641">
<title>7. Review the codebook continuously</title>
<p>In the event that a large number of similar cases can be identified that cannot be adequately placed under one of the established variables, a new sub-category should be considered to prevent the ‘other’ category from becoming disproportionately high. Large ‘other’ categories reveal weaknesses in the method as they indicate that the codebook has not been able to assign established values to a large number of cases. However, caution should be exercised in establishing new categories or variables, as the original intention of the measure could be compromised. Large collapsible categories are preferred to smaller specific ones in these types of studies in order to make inferences about the characteristics of the content being analyzed.</p>
</sec>
<sec id="section17-1354856511429641">
<title>8. Perform coding spell-check</title>
<p>After manual and computer-assisted coding is completed, the arduous but necessary process of combing through the cases to look for errors and anomalies begins. A good statistical analysis program is handy in this part of the process to facilitate this quantitative version of a spell check. By examining the cases coded under each of the variables, problems can more easily be detected and corrected. This includes punching-errors and systematic miscoding, and can help identify types of cases that were difficult to log. Such errors can be corrected manually provided the sample is manageable, or automatically where problems are identified in the computer-assisted coding. A survey of cases can result in the establishment of new sub-categories, as was the case with this project. Here, the sub-categories ‘transport’, ‘the media’ and ‘petty crime’ were established after the coding was completed to reduce large ‘other’ categories. Provided clear rules are established for this procedure and the codebook is updated, the validity of this measure should not be difficult to obtain. During this stage it can also be useful to collect examples of cases typical to each variable and category, as well as cases that are atypical, that is, cases that were difficult to log and hence inflated the ‘other’ categories.</p>
</sec>
<sec id="section18-1354856511429641">
<title>9. Establish inter-coder agreement</title>
<p>Any quantitative content analysis should measure and report the validity of the coding design through testing inter-coder agreement. Not only is reliability reporting required for the validity of the analysis, but also contributes to the further improvement of the quantitative content analysis as research method within media and communication studies. This analysis used one primary coder, while inter-coder reliability was measured in three stages by secondary coders using Cohen’s kappa (k). Kappa measures the agreement between two coders who independently assign values to items applying codebook variables that are mutually exclusive. Kappa calculates inter-coder agreement by dividing actual agreement by chance or random agreement, arriving then at observed agreement (<xref ref-type="bibr" rid="bibr18-1354856511429641">Krippendorff, 2004</xref>: 245–247). For the content analysis n = 2162, reliability was tested on 100 units in all three stages. Also, 100 units were tested on the front-page analysis, n = 1192, which proved acceptable reliability in the initial test and was therefore not repeated.</p>
</sec>
</sec>
<sec id="section19-1354856511429641">
<title>Reliability</title>
<p>As Cohen’s kappa has been criticized for being somewhat conservative, and often results in low variance in studies such as these, we set validity measures at above .70 for kappa and above 70 per cent for raw agreement. Reliability testing failed to establish validity for the n = 2162 coding scheme in the first measure. Kappa was achieved for none of the 14 variables tested in the first reliability test, with 9 variables measuring below .50 (kappa ranged from .20 to .67, and from 60% to 94% agreement). As raw agreement here was acceptable in most cases (with overall content and social issues the only variables scoring below 70%), we assumed low variance to be a contributing factor in addition to inadequate coder training. The second reliability test was, due to restricted resources, performed by one of the authors, and resulted in acceptable kappa (between .43 and .86) and raw agreement (between 67% and 97%) for all variables except social issues, which still measured low at .61/67 per cent. Therefore, a third test measuring only the primary content variables was performed by two of the senior researchers to secure inter-subjectivity within the project, and reliability was attained at .75/79 per cent.</p>
<p>The reliability of the computer-assisted coding was due to restricted resources not tested in this study. It is however recommended to do so. The main advantage of computer-assisted analysis is scale. In traditional content analyses the size of the sample correlates with the certainty of the numbers. This favors automation as we can perform analyses on more units in less time. This also makes it harder to find and pinpoint errors post run time,<sup>
<xref ref-type="fn" rid="fn20-1354856511429641">20</xref>
</sup> as the dataset becomes very large, very quickly. A test of the accuracy of the selections and quantification of attributes could be given as a percentage. This can be done by testing the algorithm on a dataset of known manually categorized material, and comparing the manual with the automated result. This is similar to a recall-value in information retrieval (<xref ref-type="bibr" rid="bibr3-1354856511429641">Baeza-Yates and Ribeiro-Neto, 2009</xref>). Such a statistical classification is a meaningful way of measuring the accuracy of the algorithm and quality of the compiled data. Particularly if selectors aim for latent content, where the results are expected to be poor, a measure for the algorithm is needed. In such cases a variation of a precision-value would be appropriate.</p>
<p>In the process of writing the selectors, testing will involve some adjustments to achieve both sufficient flexibility and precision. All selectors in use should be tested. Manual examination of the results produced by the algorithm is required. Iterative testing – from single units to smaller samples and random samples on the entire material – will ensure consistency throughout the project. Multiple testers should participate by looking for errors in a processed sample. The aim is to ensure that the algorithm is precise to a degree overcoming human labor.<sup>
<xref ref-type="fn" rid="fn21-1354856511429641">21</xref>
</sup> For manifest content the aim should be 100 per cent agreement – a feasible goal if the CMS prints content in a consistent way. Testing results will vary from news site to news site, and among different software. A test to formally validate the precision of the algorithm and accompany the results will strengthen the credibility of the research findings.<sup>
<xref ref-type="fn" rid="fn22-1354856511429641">22</xref>
</sup> Errors will be quantified if allowed.</p>
<p>The reliability tests should be performed early in the coding process, before one-third of the material is coded. Once data has been gathered, coded and tested for reliability, it is time to interpret the results, or as Robert Weber says, to translate from one system of meaning to another (<xref ref-type="bibr" rid="bibr28-1354856511429641">Weber, 1990</xref>: 77). Understanding large quantities of text translated into collapsible categories requires theory, and there are several appropriate theoretical perspectives one might use to interpret these results (p.79). In this study, the analytical parameters are bound by the research questions designed to provide data to assess NRK’s news dissemination online according to its public service remit.</p>
</sec>
<sec id="section20-1354856511429641">
<title>Conclusion</title>
<p>The main methodological findings emerging from the quantitative content analysis of the news content of nrk.no in 2009 relate to the need to adjust the design of the methodology to the medium. Analyzing the content of an online news publication quantitatively is not the same as analyzing the news output of a traditional news broadcaster. Traditional news media – whether a print newspaper, nightly news programming on television or a radio program – have limited publication space but the online environment does not. As a result, online news tends to be more varied thematically than traditional news that undergoes stricter editorial procedures due to space and time limitations. We found that news output of a national public service broadcaster reflected its remit for local news production to a larger degree on an online publication platform without the editorial limitations of a national news agenda.</p>
<p>In the process of designing a codebook for quantitative content analyses, both the consideration of the object of analysis and the medium of dissemination are important for the final design. The presentational format of the online news affects the research design to the extent that (1) manifest content had to be measured using an inductive approach to account for medium-specific aspects of news dissemination, and (2) latent content measures had to be adjusted according to the heavy local news content specific to nrk.no. The question is not only what specifically characterizes news dissemination online and how to capture this quantitatively, but also how to research the transition of the enduring features of news and journalism from one medium to another. The various news media do not only possess different characteristics, but are also similar in many ways, particularly in terms of news as genre, as a communication form, and in terms of journalistic standards and institutional practices. These recommendations can help researchers with these questions, and with further studies of online news.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The authors wish to thank Dag Elgesem, Hallvard Moe, Maren Agdestein, Joachim Laberg, Linn Lorgen and Gyri S Losnegaard.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure" id="fn24-1354856511429641">
<p>This article is the result of a project funded by the Norwegian Media Authority.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-1354856511429641">
<label>1.</label>
<p>Based on the available data, we estimate that 74,430 articles is a figure close to representing the whole population for 2009; only short of about 5000–10,000 published articles. There is no reason to assume any thematic patterns in the content that is lacking from the survey.</p>
</fn>
<fn fn-type="other" id="fn2-1354856511429641">
<label>2.</label>
<p>Database in this context refers to any software that passes for a database. This excludes concepts and systems merely based on the loose idea of structured information, or that exemplifies a database-like form.</p>
</fn>
<fn fn-type="other" id="fn3-1354856511429641">
<label>3.</label>
<p>A markup language is a system for annotating a text in a way that is syntactically distinguishable from that text.</p>
</fn>
<fn fn-type="other" id="fn4-1354856511429641">
<label>4.</label>
<p>CMS – content management system. The software that is used to publish the content on the website.</p>
</fn>
<fn fn-type="other" id="fn5-1354856511429641">
<label>5.</label>
<p>Extensible Markup Language – a flexible text format for creating structured computer documents. See the sample entry from the index in <xref ref-type="fig" rid="fig1-1354856511429641">Figure 1</xref>.</p>
</fn>
<fn fn-type="other" id="fn6-1354856511429641">
<label>6.</label>
<p>Using the open source software <italic>wget</italic> (see <xref ref-type="bibr" rid="bibr10-1354856511429641">Gnu Operating System, 2011</xref> for details). This process took close to five days. This was partly because this is a time-consuming task, but also because we added intentional delays in the algorithm to limit the strain on NRK’s servers.</p>
</fn>
<fn fn-type="other" id="fn7-1354856511429641">
<label>7.</label>
<p>A snippet is a small region of reusable source code, machine code or text.</p>
</fn>
<fn fn-type="other" id="fn8-1354856511429641">
<label>8.</label>
<p>For more information on <italic>Python</italic> and <italic>BeautifulSoup</italic>, see the Python official website <ext-link ext-link-type="uri" xlink:href="http://www.python.org">http://www.python.org</ext-link> (<xref ref-type="bibr" rid="bibr22-1354856511429641">Python, 1990–2011)</xref> and <xref ref-type="bibr" rid="bibr23-1354856511429641">Richardson (1996–2011</xref>) <ext-link ext-link-type="uri" xlink:href="http://www.crummy.com/software/BeautifulSoup/">http://www.crummy.com/software/BeautifulSoup/</ext-link></p>
</fn>
<fn fn-type="other" id="fn9-1354856511429641">
<label>9.</label>
<p>NRK comprises three television channels: NRK 1 (main channel); NRK 2 (news and current affairs); and NRK 3/NRK Super (children’s programming, series and movies).</p>
</fn>
<fn fn-type="other" id="fn10-1354856511429641">
<label>10.</label>
<p>This is an example of different URLs to the same text: http://www.nrk.no/nyheter/okonomi/1.6923373; http://www.nrk.no/nyheter/norge/1.6923373</p>
</fn>
<fn fn-type="other" id="fn11-1354856511429641">
<label>11.</label>
<p>Examples would be www.nrk.no/economy or www.nrk.no/domestic-affairs</p>
</fn>
<fn fn-type="other" id="fn12-1354856511429641">
<label>12.</label>
<p>This figure, 3.5 per cent, comprises 2757 units in the sample.</p>
</fn>
<fn fn-type="other" id="fn13-1354856511429641">
<label>13.</label>
<p>Graphical User Interface. We used the categories listed in the global header of the site in this test.</p>
</fn>
<fn fn-type="other" id="fn14-1354856511429641">
<label>14.</label>
<p>NA = not applicable.</p>
</fn>
<fn fn-type="other" id="fn15-1354856511429641">
<label>15.</label>
<p>Selectors for games, as in mini-games, editorial games or other (flash) games.</p>
</fn>
<fn fn-type="other" id="fn16-1354856511429641">
<label>16.</label>
<p>Given a functional correct selector.</p>
</fn>
<fn fn-type="other" id="fn17-1354856511429641">
<label>17.</label>
<p>Questions with answers either true or false.</p>
</fn>
<fn fn-type="other" id="fn18-1354856511429641">
<label>18.</label>
<p>The procedural elements of this research design are based on the basic guidelines or workflows provided by <xref ref-type="bibr" rid="bibr20-1354856511429641">Neuendorf (2002</xref>: 50–51) and <xref ref-type="bibr" rid="bibr28-1354856511429641">Weber (1990</xref>: 22 ff).</p>
</fn>
<fn fn-type="other" id="fn19-1354856511429641">
<label>19.</label>
<p>Runtime on our sample (74,430 units) was 6.5 hours, and we ran it over again several times as we detected minor mistakes and errors.</p>
</fn>
<fn fn-type="other" id="fn20-1354856511429641">
<label>20.</label>
<p>The time during which a program is executing.</p>
</fn>
<fn fn-type="other" id="fn21-1354856511429641">
<label>21.</label>
<p>In all aspects that do not require human judgment. Make sure all selectors aim for manifest content. Latent content selectors will need a more solid documentation of precision.</p>
</fn>
<fn fn-type="other" id="fn22-1354856511429641">
<label>22.</label>
<p>To better reproduce the results, the selector code could be added to the report, or even published as an open source project. This will further strengthen the credibility of the findings.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Allern</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2001</year>) <source>Nyhetsverdier: om markedsorientering og jounalistikk i ti norske aviser</source> <source>[News Values: Journalism and Market Orientation in 10 Norwegian Newspapers]</source>. <publisher-loc>Kristiansand</publisher-loc>: <publisher-name>IJ-forlaget</publisher-name>.</citation>
</ref>
<ref id="bibr2-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Asp</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>1995</year>) <source>Kommersialiserade TV-hyheter på gott och ont: en jämförande undersökning av Rapport TV2 och Nyherterna TV4 [Commercial TV News: A Comparative Study of Rapport TV2 and Nyheterna TV4]</source>. <publisher-loc>Gothenburg</publisher-loc>: <publisher-name>Institutionen för Journalistik och Masskommunikation</publisher-name>.</citation>
</ref>
<ref id="bibr3-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Baeza-Yates</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Ribeiro-Neto</surname>
<given-names>B</given-names>
</name>
</person-group> (<year>2009</year>) <source>Modern Information Retrieval</source>. <publisher-loc>Harlow</publisher-loc>: <publisher-name>Pearson Addison-Wesley</publisher-name>.</citation>
</ref>
<ref id="bibr4-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Boczkowski</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2005</year>) <source>Digitizing the News: Innovation in Online Newspapers</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>The MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr5-1354856511429641">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chikofsky</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Cross</surname>
<given-names>J</given-names>
</name>
</person-group> (<year>1990</year>) <article-title>Reverse engineering and design recovery: A taxonomy</article-title>. <source>IEEE Software</source> <volume>7</volume>(<issue>1</issue>): <fpage>13</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr6-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Djerf-Pierre</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Weibull</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>2001</year>) <source>Spegla, granska, tolka: aktualitetsjournalistik i svensk radio och TV under 1900-talet</source> <source>[Current Affairs Journalism in Swedish Radio and TV in the 1990s]</source>. <publisher-loc>Stockholm</publisher-loc>: <publisher-name>Prisma</publisher-name>.</citation>
</ref>
<ref id="bibr7-1354856511429641">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Dumais</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>H</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>Hierarchical classification of web content</article-title>. In: <source>Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</source>. <publisher-loc>Athens, Greece</publisher-loc>: <publisher-name>ACM</publisher-name>, pp. <fpage>256</fpage>–<lpage>263</lpage>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=345508.345593">http://portal.acm.org/citation.cfm?id=345508.345593</ext-link> <comment>(accessed 22 June 2010)</comment>.</citation>
</ref>
<ref id="bibr8-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Fairon</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Naets</surname>
<given-names>H</given-names>
</name>
</person-group> (<year>2007</year>) <source>Building and Exploring Web Corpora</source>. <publisher-loc>Louvain</publisher-loc>: <publisher-name>Presses université de Louvain</publisher-name>.</citation>
</ref>
<ref id="bibr9-1354856511429641">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Galtung</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Ruge</surname>
<given-names>MH</given-names>
</name>
</person-group> (<year>1965</year>) <article-title>The structure of foreign news: The presentation of the Congo, Cuba and Cyprus crises in four Norwegian newspapers</article-title>. <source>Journal of Peace Research</source> <volume>2</volume>(<issue>1</issue>): <fpage>64</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr10-1354856511429641">
<citation citation-type="web">
<collab collab-type="author">Gnu Operating System</collab> (<year>2011</year>) <source>Gnu wget Manual</source>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.gnu.org/software/wget/manual/">http://www.gnu.org/software/wget/manual/</ext-link> <comment>(accessed 17 November 2011)</comment>.</citation>
</ref>
<ref id="bibr11-1354856511429641">
<citation citation-type="book">
<collab collab-type="author">Group GUM</collab> (<year>1976</year>) <source>Bad News</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge &amp; Kegan Paul</publisher-name>.</citation>
</ref>
<ref id="bibr12-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Helland</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>1993</year>) <source>Public service and commercial news: Contexts of production, genre conventions and textual claims in television</source>. <publisher-loc>Leicester</publisher-loc>: <publisher-name>University of Leicester</publisher-name>.</citation>
</ref>
<ref id="bibr13-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Hjarvard</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>1999</year>) <source>Tv-nyheder i konkurrence [TV News in Competition]</source>. <publisher-loc>Fredriksberg, Denmark</publisher-loc>: <publisher-name>Samfundslitteratur</publisher-name>.</citation>
</ref>
<ref id="bibr14-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Jönsson</surname>
<given-names>AM</given-names>
</name>
</person-group> (<year>2004</year>) <source>Samma nyheter eller likadana?: studier av mångfald i svenska TV-nyheter</source> <source>[Same News or Different? Diversity in Swedish TV News]</source>. <publisher-loc>Gothenburg</publisher-loc>: <publisher-name>Institutionen för journalistik och masskommunikation</publisher-name>.</citation>
</ref>
<ref id="bibr15-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Jönsson</surname>
<given-names>AM</given-names>
</name>
<name>
<surname>Strömbäck</surname>
<given-names>J</given-names>
</name>
</person-group> (<year>2007</year>) <source>TV-journalistik i konkurrensens tid: nyhets- och samhällsprogram i svensk TV 1990–2004</source> <source>[TV Journalism in Competition: News and Current Affairs on Swedish TV 1990–2004]</source>. <publisher-loc>Stockholm</publisher-loc>: <publisher-name>Ekerlid</publisher-name>.</citation>
</ref>
<ref id="bibr16-1354856511429641">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Karlsson</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Strömbäck</surname>
<given-names>J</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Freezing the flow of online news – Exploring approaches to the study of the liquidity of online news</article-title>. <source>Journalism Studies</source> <volume>11</volume>(<issue>1</issue>): <fpage>2</fpage>–<lpage>19</lpage>.</citation>
</ref>
<ref id="bibr17-1354856511429641">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Kohlschütter</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Fankhauser</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Nejdl</surname>
<given-names>W</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Boilerplate detection using shallow text features</article-title>. In: <source>Proceedings of the third ACM international conference on Web search and data mining</source>. <publisher-loc>New York, New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>, pp. <fpage>441</fpage>–<lpage>450</lpage>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1718542">http://portal.acm.org/citation.cfm?id=1718542</ext-link> <comment>(accessed 24 May 2010)</comment>.</citation>
</ref>
<ref id="bibr18-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Krippendorff</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>2004</year>) <source>Content Analysis: An Introduction to its Methodology</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr19-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Manovich</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>2001</year>) <source>The Language of New Media</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Neuendorf</surname>
<given-names>KA</given-names>
</name>
</person-group> (<year>2002</year>) <source>The Content Analysis Guidebook</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr21-1354856511429641">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pang</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Opinion mining and sentiment analysis</article-title>. <source>Foundations and Trends in Information Retrieval</source> <volume>2</volume>
<issue>(1–2)</issue>: <fpage>1</fpage>–<lpage>135</lpage>.</citation>
</ref>
<ref id="bibr22-1354856511429641">
<citation citation-type="web">
<collab collab-type="author">Python</collab> (<year>1990–2011</year>) <comment>Official website. Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.python.org">http://www.python.org</ext-link> <comment>(accessed 17 November 2011)</comment>.</citation>
</ref>
<ref id="bibr23-1354856511429641">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Richardson</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>1996–2011</year>) <source>Beautiful Soup</source>. <comment>Crummy website. Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.crummy.com/software/BeautifulSoup/">http://www.crummy.com/software/BeautifulSoup/</ext-link> <comment>(accessed 17 November 2011)</comment>.</citation>
</ref>
<ref id="bibr24-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Sand</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Helland</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>1998</year>) <source>Bak TV-nyhetene: produksjon og presentasjon i NRK og TV2</source> <source>[TV News: Production and Presentation in NRK and TV 2]</source>. <publisher-loc>Bergen</publisher-loc>: <publisher-name>Fagbokforlaget</publisher-name>.</citation>
</ref>
<ref id="bibr25-1354856511429641">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Gallup</surname>
<given-names>TNS</given-names>
</name>
</person-group> (<year>2010</year>) <source>Rapport – Topplisten – TNS Gallup</source>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://rapp.tns-gallup.no/?aid=9072261">http://rapp.tns-gallup.no/?aid=9072261</ext-link> <comment>(accessed 28 June 2010)</comment>.</citation>
</ref>
<ref id="bibr26-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Waldahl</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Andersen</surname>
<given-names>MB</given-names>
</name>
<name>
<surname>Rønning</surname>
<given-names>H</given-names>
</name>
</person-group> (<year>2002</year>) <source>Nyheter først og fremst: norske tv-nyheter, myter og realiteter</source> <source>[Norwegian TV News: Myths and Realities]</source>. <publisher-loc>Oslo</publisher-loc>: <publisher-name>Universitetsforlaget</publisher-name>.</citation>
</ref>
<ref id="bibr27-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Waldahl</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Andersen</surname>
<given-names>MB</given-names>
</name>
<name>
<surname>Rønning</surname>
<given-names>H</given-names>
</name>
</person-group> (<year>2009</year>) <source>Tv-nyhetenes verden 1st edn [The World of TV News]</source>. <publisher-loc>Oslo</publisher-loc>: <publisher-name>Universitetsforlaget</publisher-name>.</citation>
</ref>
<ref id="bibr28-1354856511429641">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Weber</surname>
<given-names>RP</given-names>
</name>
</person-group> (<year>1990</year>) <source>Basic content analysis</source>. <publisher-loc>Newbury Park, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>