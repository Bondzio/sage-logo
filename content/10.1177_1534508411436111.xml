<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AEI</journal-id>
<journal-id journal-id-type="hwp">spaei</journal-id>
<journal-title>Assessment for Effective Intervention</journal-title>
<issn pub-type="ppub">1534-5084</issn>
<issn pub-type="epub">1938-7458</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1534508411436111</article-id>
<article-id pub-id-type="publisher-id">10.1177_1534508411436111</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Testing the Relation Between Fidelity of Implementation and Student Outcomes in Math</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Crawford</surname><given-names>Lindy</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff1-1534508411436111">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Carpenter</surname><given-names>Dick M.</given-names><suffix>II</suffix></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff2-1534508411436111">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Wilson</surname><given-names>Mary T.</given-names></name>
<degrees>PhD</degrees>
<xref ref-type="aff" rid="aff2-1534508411436111">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Schmeister</surname><given-names>Megan</given-names></name>
<degrees>MEd</degrees>
<xref ref-type="aff" rid="aff3-1534508411436111">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>McDonald</surname><given-names>Marilee</given-names></name>
<degrees>BS</degrees>
<xref ref-type="aff" rid="aff2-1534508411436111">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-1534508411436111"><label>1</label>Texas Christian University, Fort Worth, TX, USA</aff>
<aff id="aff2-1534508411436111"><label>2</label>University of Colorado Colorado Springs, Colorado Springs, CO, USA</aff>
<aff id="aff3-1534508411436111"><label>3</label>Digital Directions International, Carbondale, CO, USA</aff>
<author-notes>
<corresp id="corresp1-1534508411436111">Lindy Crawford, Ann Jones Endowed Chair in Special Education, College of Education Box 297900, Texas Christian University, Fort Worth, TX 76129, USA Email: <email>lindy.crawford@tcu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>9</month>
<year>2012</year>
</pub-date>
<volume>37</volume>
<issue>4</issue>
<fpage>224</fpage>
<lpage>235</lpage>
<permissions>
<copyright-statement>© 2012 Hammill Institute on Disabilities</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Hammill Institute on Disabilities</copyright-holder>
</permissions>
<abstract>
<p>The relation between fidelity of implementation and student outcomes in a computer-based middle school mathematics curriculum was measured empirically. Participants included 485 students and 23 teachers from 11 public middle schools across seven states. Implementation fidelity was defined using two constructs: fidelity to structure and fidelity to process. Because of the nested nature of the data, we used a two-level hierarchical linear model for analysis. Four variables, all categorized as fidelity to structure variables, proved significant—total time in intervention (<italic>p</italic> &lt;.001), concentration of time in intervention (<italic>p</italic> = .03), direct observation of intervention fidelity (<italic>p</italic> = .04), and pretest score (<italic>p</italic> &lt;.001). Fidelity to process was found to be nonsignificant. The importance of measuring the relation between implementation fidelity and student outcomes is discussed as well as implications for researchers and teachers.</p>
</abstract>
<kwd-group>
<kwd>math</kwd>
<kwd>technology</kwd>
<kwd>middle school</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>In the field of education, intervention studies explore the efficacy and effectiveness of instructional practices and by doing so, further our knowledge of what works. These studies and their results are fundamental to advancing best practices for teaching students. And although educational researchers strive to design conceptually and methodologically sound studies that meet the principles put forth by the field (<xref ref-type="bibr" rid="bibr1-1534508411436111">American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education [AERA, APA, &amp; NCME], 1999</xref>; <xref ref-type="bibr" rid="bibr38-1534508411436111">Shadish, Cook, &amp; Campbell, 2002</xref>), it is the rare study that examines the effect of actual principles on outcomes achieved. Researchers strive to meet standards for internal and external validity without questioning the influence of different standards within the context of unique studies with diverse populations. Fidelity of implementation, one measure of internal validity, is a “multilevel, multivariate phenomenon affected by personal, programmatic, and contextual factors” (<xref ref-type="bibr" rid="bibr45-1534508411436111">Zvoch, 2009</xref>, p. 46). Understanding the contribution of implementation fidelity to research outcomes increases our confidence in the validity of reported findings. In this article, therefore, we use data from a larger study of the effects of a middle school computer-based mathematics intervention to analyze the relation among multiple measures of implementation fidelity and student outcomes.</p>
<sec id="section1-1534508411436111">
<title>Defining Fidelity</title>
<p>In the field of education, one broadly accepted definition of implementation fidelity does not exist, and often distinctions are made when defining fidelity within efficacy or effectiveness studies (<xref ref-type="bibr" rid="bibr33-1534508411436111">O’Donnell, 2008</xref>). However, dozens of fidelity indices have been proposed and investigated in the fields of public and mental health. Professionals in these fields often operationalize the construct of implementation fidelity into two components: (a) fidelity to structure and (b) fidelity to process (<xref ref-type="bibr" rid="bibr30-1534508411436111">Mowbray, Holter, Teague, &amp; Bybee, 2003</xref>). Fidelity to structure includes observable behaviors and extant data such as frequency and intensity of contacts and evidence of procedural guidelines. In the field of education, one definition that characterizes fidelity to structure is “the extent to which the treatment conditions, as implemented, conform to the researcher’s specifications for the treatment” (<xref ref-type="bibr" rid="bibr13-1534508411436111">Gall, Gall, &amp; Borg, 2007</xref>, p. 395). Fidelity to process includes more subjective measures, such as emotional climate and quality of professional interactions (<xref ref-type="bibr" rid="bibr30-1534508411436111">Mowbray et al., 2003</xref>). Both of these constructs are necessary when measuring implementation fidelity and tend to complement one another (<xref ref-type="bibr" rid="bibr30-1534508411436111">Mowbray et al., 2003</xref>).</p>
</sec>
<sec id="section2-1534508411436111">
<title>Fidelity Research in Education</title>
<p><xref ref-type="bibr" rid="bibr29-1534508411436111">Mislevy (2007)</xref> states, “validity emerges from design activities” (p. 467). The stronger the research design the more valid our interpretation of results. <xref ref-type="bibr" rid="bibr6-1534508411436111">Christ (2007)</xref> defines threats to internal validity as “those factors that have the potential to provide alternate explanations for the observed effects,” citing the seminal work by <xref ref-type="bibr" rid="bibr4-1534508411436111">Campbell and Stanley (1963)</xref>. Weak implementation fidelity is one of these factors; thus, measuring fidelity is one way to increase the internal validity of a research study (<xref ref-type="bibr" rid="bibr22-1534508411436111">Hohmann &amp; Shear, 2002</xref>; <xref ref-type="bibr" rid="bibr33-1534508411436111">O’Donnell, 2008</xref>).</p>
<p>Although it is important to maximize and measure implementation fidelity, it is rarely accomplished in the field of education (<xref ref-type="bibr" rid="bibr13-1534508411436111">Gall et al., 2007</xref>; <xref ref-type="bibr" rid="bibr33-1534508411436111">O’Donnell, 2008</xref>). Well-established educational researchers acknowledge the challenge of creating and implementing sound research studies within school settings (<xref ref-type="bibr" rid="bibr16-1534508411436111">Gersten et al., 2005</xref>; <xref ref-type="bibr" rid="bibr24-1534508411436111">Hulleman &amp; Cordray, 2009</xref>). In fact, “within a study, the intended intervention may only marginally resemble what is actually implemented” (<xref ref-type="bibr" rid="bibr14-1534508411436111">Gersten, Baker, &amp; Lloyd, 2000</xref>, p. 4). A balancing act is required when attempting to implement a rigorous research design within the realities posed by our school systems. Well-planned research methods can easily become distorted when moved into the reality of classroom implementation. In light of the challenges faced by educational researchers, and in particular those researchers conducting efficacy studies in classroom environments, the importance of documenting implementation fidelity cannot be underestimated. <xref ref-type="bibr" rid="bibr33-1534508411436111">O’Donnell (2008)</xref> states the importance of measuring fidelity of implementation and studying its relation to outcomes achieved; nevertheless, her review found a paucity of studies in education that measured fidelity as related to educational outcomes. Others in the field of education have reported a similar dearth of research (<xref ref-type="bibr" rid="bibr31-1534508411436111">National Research Council of the National Academies, 2004</xref>). Some studies measuring the relation between fidelity and achievement outcomes, however, have been conducted in the field of education. And in a review of this research, O’Donnell reports that increased fidelity of implementation led to statistically significantly higher outcomes.</p>
<p>Researchers have consistently found that students whose teachers implement curriculum with high fidelity made greater gains than their peers in low-fidelity classrooms (<xref ref-type="bibr" rid="bibr32-1534508411436111">Noell, Gresham, &amp; Gansle, 2002</xref>; Songer &amp; Gotwals; 2005; <xref ref-type="bibr" rid="bibr43-1534508411436111">Ysseldyke &amp; Bolt, 2007</xref>; <xref ref-type="bibr" rid="bibr44-1534508411436111">Ysseldyke et al., 2003</xref>). Specifically, in the <xref ref-type="bibr" rid="bibr40-1534508411436111">Songer and Gotwals (2005)</xref> study, students of teachers who had high fidelity of implementation of a science curriculum not only learned the basic scientific principles being taught but also learned “how to reason with these concepts in complex scientific situations” (p. 18). Similarly, in <xref ref-type="bibr" rid="bibr44-1534508411436111">Ysseldyke et al. (2003)</xref>, math students in classes of high-implementers gained an average of 18 percentile points more than students in the control group.</p>
<p>Important, however, is the fact that much of the research surrounding implementation fidelity in educational settings has involved teacher-led instruction. And although a computer-based intervention may require similar behaviors on the part of teachers and students, it also requires unique behaviors (<xref ref-type="bibr" rid="bibr28-1534508411436111">Mills &amp; Ragan, 2000</xref>). Different teachers’ use of the same educational technologies will likely vary (<xref ref-type="bibr" rid="bibr2-1534508411436111">Baker, 2001</xref>), and “conditions for successful implementation depend partly on the beliefs, motivations, and practices of teachers” (<xref ref-type="bibr" rid="bibr42-1534508411436111">Weston, 2004</xref>, p. 57). In developing a model to measure implementation fidelity of an integrated learning system (ILS) (i.e., computer-based instructional program), <xref ref-type="bibr" rid="bibr28-1534508411436111">Mills and Ragan (2000)</xref> empirically validated five critical constructs representing 15 separate behaviors: “(a) integrating computer instruction with classroom instruction, (b) facilitating ILS instruction while students are using the ILS in the classroom or lab, (c) using reinforcement and motivational strategies to sustain learner interest in ILS instruction, (d) participating in training in the use of ILS technology, and (e) receiving ongoing instruction and technical support in ILS use” (p. 37). In light of these five components identified and empirically validated by Mills and Ragan it would be incorrect to assume that computer-based instruction can be viewed as a stand-alone intervention with little relation to the behaviors of the teacher. Thus, in the same way that it is important to quantify the effect of fidelity of implementation on student outcomes during teacher-led instruction, so it is important to study this relation within a computer-based intervention.</p>
</sec>
<sec id="section3-1534508411436111">
<title>Fidelity Constructs in the Context of This Study</title>
<p>As introduced above, fidelity of implementation can be categorized into two broad constructs: fidelity to structure and fidelity to process. In this study, we categorize the following variables as fidelity to structure: (a) total time in intervention, (b) concentration of time in the intervention, and (c) teacher adherence to and student engagement with the program (as measured through direct observations). We operationalize fidelity to process through use of a rating scale combining indicators such as teacher communication, classroom management, and problem-solving skills—behaviors that <xref ref-type="bibr" rid="bibr30-1534508411436111">Mowbray et al. (2003)</xref> identified as process variables and <xref ref-type="bibr" rid="bibr28-1534508411436111">Mills and Ragan (2000)</xref> validated as essential in delivery of computer-based instruction. Thus, we ended up with four fidelity variables categorized under the two constructs of structure and process.</p>
<sec id="section4-1534508411436111">
<title>Total Time in Intervention</title>
<p>The length of time students are engaged in an intervention is an important measure of fidelity to structure (Gall et al., 1996). A weak experimental treatment is one that does not allow enough time for a change to occur (Gall et al., 1996). General consensus exists regarding how much time is enough time for change to occur. In his article on evidence-based research in education, <xref ref-type="bibr" rid="bibr39-1534508411436111">Slavin (2008)</xref> shares that the <italic>Best Evidence Encyclopedia</italic> uses a 12-week expectation. <xref ref-type="bibr" rid="bibr15-1534508411436111">Gersten and Edyburn (2007)</xref> suggest that an intervention should be implemented for no less than one quarter of the school year, or 9 weeks. They report that ideally the length should be much longer—for example, interventions that last a full semester or an entire school year. Similarly, <xref ref-type="bibr" rid="bibr5-1534508411436111">Cavanaugh, Kim, Wanzek, and Vaughn (2004)</xref> reported moderate to high effect sizes for studies of reading interventions employed for a duration of 8 to 10 weeks.</p>
<p>Terms such as <italic>duration</italic> and <italic>dosage</italic> are also used to describe the length of an intervention. Dosage is defined as duration × sessions per week × length of session (<xref ref-type="bibr" rid="bibr37-1534508411436111">Rohrbeck, Ginsburg-Block, Fantuzzo, &amp; Miller, 2003</xref>) but still represents total time. Researchers tend to investigate frequency, intensity, and/or duration (how often, how much time per day, and how long) as separate variables. For example, in their review of Kindergarten reading intervention studies, <xref ref-type="bibr" rid="bibr5-1534508411436111">Cavanaugh et al. (2004)</xref> found that the more often an intervention was employed every week (frequency) the more likely the intervention was effective; time spent per day on effective interventions ranged between 15 and 30 minutes; and those interventions with the greatest effect sizes were used over 8 to 10 weeks.</p>
</sec>
<sec id="section5-1534508411436111">
<title>Concentration of Time in Intervention</title>
<p>We chose to measure concentration of time in this study because of our observations that some school sites implemented the intervention for 10 to 15 minutes per day over a course of many weeks as opposed to other sites that implemented the intervention for 30 to 45 minutes every day for a fewer number of weeks. We were concerned that measuring only total time would fail to capture the more nuanced indicator of concentration of time. <xref ref-type="bibr" rid="bibr37-1534508411436111">Rohrbeck et al. (2003)</xref> echoed our concern, “It is conceivable that an examination of the number of weeks may pit intensive, tightly controlled short-term interventions against less intensive long-term interventions” (p. 251).</p>
<p>Meta-analyses in the field of educational interventions support this more nuanced investigation of fidelity as they have reported conflicting findings on the effect of duration alone, with some syntheses finding an association between duration and intervention effect (<xref ref-type="bibr" rid="bibr5-1534508411436111">Cavanaugh et al., 2004</xref>; <xref ref-type="bibr" rid="bibr7-1534508411436111">Cohen, Kulik, &amp; Kulik, 1982</xref>) and other studies reporting no effect for duration (<xref ref-type="bibr" rid="bibr8-1534508411436111">Cook, Scruggs, Mastropieri, &amp; Casto, 1985</xref>; <xref ref-type="bibr" rid="bibr11-1534508411436111">Elbaum &amp; Vaughn, 2001</xref>; <xref ref-type="bibr" rid="bibr37-1534508411436111">Rohrbeck et al., 2003</xref>).</p>
</sec>
<sec id="section6-1534508411436111">
<title>Direct Observations of Implementation Fidelity</title>
<p>Direct observations allow researchers to investigate the level of adherence teachers and students demonstrate to the program. Adherence is generally measured through checklists (<xref ref-type="bibr" rid="bibr36-1534508411436111">Power, Blom-Hoffman, Clarke, Riley-Tillman, &amp; Kelleher, 2005</xref>) and often requires direct observations of program implementation. An important first step is to operationally define components of an intervention (<xref ref-type="bibr" rid="bibr19-1534508411436111">Gresham, MacMillan, Beebe-Frankenberger, &amp; Bocian, 2000</xref>; <xref ref-type="bibr" rid="bibr33-1534508411436111">O’Donnell, 2008</xref>). An operational definition of critical components in the intervention will reduce inferences and thus increase the reliability of direct observation data (<xref ref-type="bibr" rid="bibr19-1534508411436111">Gresham et al., 2000</xref>). Once each component is operationally defined, a checklist is developed and then completed through direct observations of each component. A final step is to calculate a total score for implementation fidelity. One score method is to calculate a percentage integrity score by adding the number of components implemented correctly and dividing by total number of components (<xref ref-type="bibr" rid="bibr17-1534508411436111">Gresham, 1989</xref>).</p>
</sec>
<sec id="section7-1534508411436111">
<title>Fidelity to Process</title>
<p>Providers of teacher-led interventions (e.g., teachers; social workers; behavioral consultants) have been found to implement interventions differently both within and across sites (<xref ref-type="bibr" rid="bibr40-1534508411436111">Songer &amp; Gotwals, 2005</xref>; <xref ref-type="bibr" rid="bibr45-1534508411436111">Zvoch, 2009</xref>), as have providers of computer-based interventions (<xref ref-type="bibr" rid="bibr3-1534508411436111">Becker, 1994</xref>; <xref ref-type="bibr" rid="bibr25-1534508411436111">Maddux, Johnson, &amp; Harlow, 1993</xref>). Differences in how providers implement interventions can be partially attributed to fidelity to process indicators. Traditional indicators of fidelity to process include teacher motivation, preparation and experience (<xref ref-type="bibr" rid="bibr45-1534508411436111">Zvoch, 2009</xref>), as well as time and classroom management (<xref ref-type="bibr" rid="bibr26-1534508411436111">Melde, Esbensen, &amp; Tusinski, 2006</xref>), all of which may contribute to differences in implementation. In a computer-based environment process variables include integration, facilitation, and management of computer-based instruction within computer lab environments as well as engaging in necessary training and ongoing troubleshooting (<xref ref-type="bibr" rid="bibr28-1534508411436111">Mills &amp; Ragan, 2000</xref>). Measuring these types of indicators often requires the use of indirect measures that can be used to supplement data derived from direct observations (<xref ref-type="bibr" rid="bibr18-1534508411436111">Gresham, Gansle, &amp; Noell, 1993</xref>; <xref ref-type="bibr" rid="bibr30-1534508411436111">Mowbray et al., 2003</xref>), as direct observations such as fidelity checklists do not always capture the more subtle indicators of implementation fidelity (<xref ref-type="bibr" rid="bibr14-1534508411436111">Gersten et al., 2000</xref>). Or as <xref ref-type="bibr" rid="bibr30-1534508411436111">Mowbray et al. (2003)</xref> notes, “A focus on structural criteria may produce high reliability and validity at the cost of overly simplistic conceptions of program operations, while omitting key ingredients which are complex, reflecting values and principles, and which are, perhaps, more important” (p. 333). Important to note, however, is the subjectivity associated with indirect measures of implementation fidelity; a review of the literature reported low correlations between direct and indirect assessments of fidelity (<xref ref-type="bibr" rid="bibr19-1534508411436111">Gresham et al., 2000</xref>).</p>
<p>Relying on the previous research and recommendations of professionals in the field, we hypothesize that the following implementation variables—total time in intervention, concentration of time in intervention, direct observations of teacher adherence to and student engagement with the program, and fidelity to process (as measured through an indirect rating scale)—contribute significantly to student outcomes. As a follow-up to an earlier intervention study (<xref ref-type="bibr" rid="bibr9-1534508411436111">Crawford, 2008</xref>), we now turn our attention to the relation between fidelity of implementation of a computer-based intervention and student outcomes in math.</p>
</sec>
</sec>
<sec id="section8-1534508411436111" sec-type="methods">
<title>Method</title>
<sec id="section9-1534508411436111">
<title>Research Question and Design</title>
<p>Our analysis was guided by one primary research question: Is there a significant relation between indicators of implementation fidelity of a computer-based intervention and students’ math performance? To answer this question, we used a two-level hierarchical linear model (HLM) to examine the relation between the four fidelity variables and students’ math performance. The sample for this study included the treatment group from a larger randomized control study (conducted by the lead author, acting as an independent evaluator), of an intervention called <italic>HELP Math</italic><sup>©</sup> (<xref ref-type="bibr" rid="bibr20-1534508411436111">n.d.</xref>).</p>
</sec>
<sec id="section10-1534508411436111">
<title>Intervention</title>
<p><italic>HELP Math</italic> is a web-based supplemental math curriculum designed for English language learners (ELLs) in Grades 6 through 8 although it has also been found to be effective with students who are fluent English speakers (<xref ref-type="bibr" rid="bibr10-1534508411436111">Digital Directions International, n.d.</xref>). It uses sheltered instruction to deliver math lessons with interactive language support: Students can hear, see, and manipulate math content. At the time of the study, <italic>HELP Math</italic> consisted of 44 lessons embedded within four modules (Numbers Make Sense, Geometry, Algebra, and Data Analysis), aligned with grade-level standards including those of the National Council of Teachers of Mathematics. <italic>HELP Math</italic> emphasizes the language of mathematics, conceptual understanding and problem solving—skills that all students need to succeed in mathematics.</p>
</sec>
<sec id="section11-1534508411436111">
<title>Participants</title>
<p>In this study, we included only those students who participated in the treatment group: seventh- and eighth-grade students in 11 public middle schools in seven states. A total of 654 seventh- and eighth-grade students in the treatment group took the pretest in fall 2007, and 485 students took the posttest in spring 2008. Attrition factors included the following: students who withdrew from school, transferred to other classes within the school, were absent during testing, were not given a posttest because of teacher oversight, or did not label the posttest accurately. Our final analysis included the 485 participants who completed both the pre- and posttests. Of these participants, 374 (77%) were in seventh grade and 111 (23%) were in eighth grade; 249 (51%) were male and 236 (49%) were female. In addition, 337 (69%) of the students were ELLs, with Spanish speakers comprising 96% of that population. Although 31% of the participants were not designated as ELLs, the program has been shown to improve the math performance of middle school students at varying levels of English fluency (<xref ref-type="bibr" rid="bibr9-1534508411436111">Crawford, 2008</xref>). Students receiving special education services comprised 10% of the sample.</p>
<p>Twenty-three teachers participated, 10 males and 13 females. Their experience ranged from less than 1 year to 20 years (<italic>M</italic> = 7.3, <italic>SD</italic> = 6.4). Teachers had been at their schools an average of 3.0 years (<italic>SD</italic> = 2.6). Twelve of the teachers majored in education (K-6; secondary; and/or bilingual) and the remaining teachers majored in a subject area such as math (<italic>n</italic> = 4) or one of six other fields (<italic>n</italic> = 7). Twenty of the 23 teachers held state teaching licenses appropriate for their subject area (e.g., a K-8 license or a mathematics license).</p>
</sec>
<sec id="section12-1534508411436111">
<title>Procedures</title>
<sec id="section13-1534508411436111">
<title>Training</title>
<p>Before beginning the study, all 23 teachers attended a half-day training on use of <italic>HELP Math</italic> and a half-day professional development on how to use sheltered instruction while teaching mathematics. Study participants were given a login and password for the <italic>HELP Math</italic> program. When classes and teachers were ready to begin implementation, students took the pretest. Following pretests, teachers introduced the intervention and guided student activities; the expectation was that teachers facilitated the computer-based instruction. Researchers conducted direct observations of the level of teacher adherence to and students’ engagement with the program and helped troubleshoot any problems through emails, in person, and on the phone. At the conclusion of the study, teachers administered the posttest.</p>
</sec>
<sec id="section14-1534508411436111">
<title>Implementation schedule</title>
<p>Schools and teachers agreed to implement the program for 25 hours per student. Exact implementation schedules depended on each school’s access to laptop computers or computer labs and teacher schedules. Most schools scheduled the intervention two to four times a week for 30 to 45 minutes per session. Scheduling of the intervention versus actual implementation of the intervention varied by classrooms and some teachers did not fully enact their schedules. We were able to collect data on actual implementation time because the <italic>HELP Math</italic> program is designed to keep track of each student’s time spent working within a particular lesson. From the total 485 participants, 138 students (28%) actually logged 25 hours or more at completion of the study. On average, students spent 20 hours on the program (SD = 8) across an average of 20 weeks. Dividing the average number of hours by the average number of weeks results in two 30-minute sessions per week on average, with some schools logging less time and other schools logging considerably more time per week. It is this variable of time that is explored further in our analyses as we observed large differences in implementation time across sites. Moreover, little research has been conducted related to the amount of time necessary to affect student achievement on this intervention or computer-based programs in general.</p>
</sec>
</sec>
<sec id="section15-1534508411436111">
<title>Data</title>
<sec id="section16-1534508411436111">
<title>Independent variables</title>
<p>Our first independent variable, total time in intervention, represents the number of minutes each student spent on the intervention. The program tallied the time students spent on each lesson and module. The second independent variable, concentration of time in intervention, aimed to capture the use of the intervention with consistency. We defined concentration of time in intervention as the ratio of minutes each student used the intervention over time (measured in days) elapsed from pretest to posttest.</p>
<p>The third variable, teacher adherence to and students’ engagement with the program as measured through direct observations, resulted in a total score. Direct observation items and mean scores on these items across all teachers can be found in <xref ref-type="table" rid="table1-1534508411436111">Table 1</xref>. Each teacher was observed twice using this measure. Items were operationally defined under four major constructs. The first construct in the direct observation, adherence to the program by the teacher, included five “logistical” behaviors (Items 1–5). The second construct was defined as “instructional quality” and was measured through Items 6–8. “Student engagement,” the third construct, was measured through five items (9, 11, 14, 15, 20). The fourth construct was defined as “facility of use” (Items 10, 12, 13, 16, 17, 18, 19). Items 1 through 8 represented direct teacher behaviors and Items 9 through 20 measured student behaviors as influenced by teacher behaviors. Ideally, we would have analyzed data from this observation instrument through a factor analysis to determine the validity of our four constructs developed a priori, but because of the truncated scale (0–2) used in the measure we did not have enough variance across data to conduct a meaningful factor analysis. Instead, we relied on the combined scores across the four constructs averaged over total number of observations to draw conclusions about teachers’ adherence to and students’ engagement with the program. We had used this measure in prior years and had made revisions to some of the items to better capture teacher and student interactions with the program.</p>
<table-wrap id="table1-1534508411436111" position="float">
<label>Table 1.</label>
<caption>
<p>Direct Observation of Fidelity Items and Mean Scores</p>
</caption>
<graphic alternate-form-of="table1-1534508411436111" xlink:href="10.1177_1534508411436111-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Item</th>
<th align="center">Fidelity Question</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Teacher administers and returns pretest/posttest.</td>
<td>2.00</td>
<td>0.00</td>
</tr>
<tr>
<td>2</td>
<td>Teacher schedules 20 minutes or more per <italic>HELP</italic> session.</td>
<td>1.91</td>
<td>0.25</td>
</tr>
<tr>
<td>3</td>
<td>Teacher schedules two or more <italic>HELP</italic> sessions per week.</td>
<td>1.67</td>
<td>0.44</td>
</tr>
<tr>
<td>4</td>
<td>Computer lab is available to students providing an adequate number of workstations.</td>
<td>1.72</td>
<td>0.54</td>
</tr>
<tr>
<td>5</td>
<td>Computer lab is ready for students with easy access to the <italic>HELP</italic> program (do they have a desktop link, password, etc.).</td>
<td>1.83</td>
<td>0.36</td>
</tr>
<tr>
<td>6</td>
<td>Teacher introduces <italic>HELP</italic> session and provides direction to students.</td>
<td>1.59</td>
<td>0.51</td>
</tr>
<tr>
<td>7</td>
<td>Teacher redirects or instructs individual students as needed.</td>
<td>1.70</td>
<td>0.39</td>
</tr>
<tr>
<td>8</td>
<td>Teacher continuously monitors students’ use of <italic>HELP</italic> program.</td>
<td>1.63</td>
<td>0.48</td>
</tr>
<tr>
<td>9</td>
<td>Students access <italic>HELP</italic> program and successfully log on.</td>
<td>1.89</td>
<td>0.26</td>
</tr>
<tr>
<td>10</td>
<td>Students remain on-task throughout the session (note student behaviors below).</td>
<td>1.54</td>
<td>0.45</td>
</tr>
<tr>
<td>11</td>
<td>Students use navigational buttons such as <italic>next</italic> and <italic>back</italic> to manipulate program.</td>
<td>1.91</td>
<td>0.25</td>
</tr>
<tr>
<td>12</td>
<td>Students use key terms tab and/or click on vocabulary word link.</td>
<td>0.85</td>
<td>0.59</td>
</tr>
<tr>
<td>13</td>
<td>Students purposefully interact with program and answer questions.</td>
<td>1.63</td>
<td>0.41</td>
</tr>
<tr>
<td>14</td>
<td>Students navigate from section to section within the <italic>HELP</italic> lesson using menu.</td>
<td>1.89</td>
<td>0.26</td>
</tr>
<tr>
<td>15</td>
<td>Students use the mouse effectively to manipulate the <italic>HELP</italic> program.</td>
<td>1.91</td>
<td>0.25</td>
</tr>
<tr>
<td>16</td>
<td>Students listen to <italic>HELP</italic> audio and adjust volume as needed.</td>
<td>1.63</td>
<td>0.41</td>
</tr>
<tr>
<td>17</td>
<td>Students use the calculator function in the <italic>HELP</italic> program as needed.</td>
<td>0.91</td>
<td>0.56</td>
</tr>
<tr>
<td>18</td>
<td>Students purposefully select answers in lessons and/or final quiz.</td>
<td>1.63</td>
<td>0.38</td>
</tr>
<tr>
<td>19</td>
<td>Students complete the final quiz before beginning the next <italic>HELP</italic> lesson.</td>
<td>1.65</td>
<td>0.63</td>
</tr>
<tr>
<td>20</td>
<td>Students use <italic>log off</italic> button to end <italic>HELP</italic> session.</td>
<td>1.83</td>
<td>0.36</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1534508411436111">
<p><italic>Note</italic>. Scale for each item was 0–2.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The fourth variable, fidelity to process, was operationalized through use of a rating scale designed by the researchers. Data from a minimum of three formal and informal observations were used to rate teachers; data also were drawn from implementation checklists, field notes, and teacher interviews. Data from these measures allowed researchers to look deeper into quality or process variables related to implementation. Relying on previous research about process variables important to teacher-led instruction (<xref ref-type="bibr" rid="bibr45-1534508411436111">Zvoch, 2009</xref>) as well as research on those implementation variables essential for computer-based instruction (<xref ref-type="bibr" rid="bibr28-1534508411436111">Mills &amp; Ragan, 2000</xref>), we considered seven teacher factors in rating fidelity of process that were not captured in our direct observations of fidelity to structure. Following each of these seven variables is an operational definition: (a) student attrition attributed to teacher behaviors (e.g., teachers neglected to give the posttest and therefore student data was not usable for this analyses, students did not log on using the correct password, posttests labeled incorrectly), (b) scheduling of intervention (e.g., amount of time appropriated per week, realistic scheduling within the context of computer availability, teacher compliance with schedule), (c) decision making (placing students in appropriate level of curriculum, integration of computer-based lessons with content of classroom instruction, using computer-generated reports for instructional decision making), (d) communication with research team regarding correct implementation of the computer-based program (asking clarification questions, calling for troubleshooting assistance, asking for help with interpreting computer-generated reports), (e) problem-solving skills (related to use of technology or loss of scheduled computer lab time), (f) adherence to research commitment (allowing all students to log on to the program and not using access to the computer as a behavioral reward or punishment, beginning and ending the computer lab sessions on time, running reports of student progress on schedule), and (g) management of classroom and student behavior (including monitoring students while they were working on computers, facilitating instruction as opposed to distancing self from the computer-based intervention, promoting a positive class culture). Using these definitions, we rated the quality of each teacher’s implementation of the intervention (fidelity to process) on a 1 to 5 scale, with 5 being the highest score. The two raters demonstrated exact agreement on individual item scores for individual teachers 85% of the time and agreement was off-by-one 12% of the time. They reached consensus on those items that did not have exact agreement and this consensus score is included in the analysis (see <xref ref-type="table" rid="table2-1534508411436111">Table 2</xref>).</p>
<table-wrap id="table2-1534508411436111" position="float">
<label>Table 2.</label>
<caption>
<p>Teachers’ Fidelity to Process Scores</p>
</caption>
<graphic alternate-form-of="table2-1534508411436111" xlink:href="10.1177_1534508411436111-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Teacher</th>
<th align="center">Teacher-Related Student Attrition</th>
<th align="center">Scheduling</th>
<th align="center">Adherence to Research Commitment</th>
<th align="center">Communication</th>
<th align="center">Problem Solving</th>
<th align="center">Teacher Decision Making</th>
<th align="center">Classroom Management</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>5</td>
<td>32</td>
</tr>
<tr>
<td>2</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>25</td>
</tr>
<tr>
<td>3</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>34</td>
</tr>
<tr>
<td>4</td>
<td>5</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>4</td>
<td>4</td>
<td>5</td>
<td>29</td>
</tr>
<tr>
<td>5</td>
<td>3</td>
<td>3</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>25</td>
</tr>
<tr>
<td>6</td>
<td>3</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>4</td>
<td>26</td>
</tr>
<tr>
<td>7</td>
<td>5</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>3</td>
<td>21</td>
</tr>
<tr>
<td>8</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>3</td>
<td>5</td>
<td>5</td>
<td>33</td>
</tr>
<tr>
<td>9</td>
<td>4</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>32</td>
</tr>
<tr>
<td>10</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>5</td>
<td>5</td>
<td>29</td>
</tr>
<tr>
<td>11</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>15</td>
</tr>
<tr>
<td>12</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>26</td>
</tr>
<tr>
<td>13</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>15</td>
</tr>
<tr>
<td>14</td>
<td>5</td>
<td>3</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>28</td>
</tr>
<tr>
<td>15</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>35</td>
</tr>
<tr>
<td>16</td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>20</td>
</tr>
<tr>
<td>17</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>35</td>
</tr>
<tr>
<td>18</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>26</td>
</tr>
<tr>
<td>19</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>27</td>
</tr>
<tr>
<td>20</td>
<td>4</td>
<td>4</td>
<td>3</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>27</td>
</tr>
<tr>
<td>21</td>
<td>3</td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>25</td>
</tr>
<tr>
<td>22</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>14</td>
</tr>
<tr>
<td>23</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>5</td>
<td>34</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1534508411436111">
<p><italic>Note</italic>. Scale for each item was 1–5. Data sources for score assignment included direct observations, implementation checklists, field notes and teacher interviews.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>

<sec id="section17-1534508411436111">
<title>Dependent measure</title>
<p>We created and validated the pre- and posttest used in this study. The same set of items was included on the pre- and posttest. We created questions by modifying those found on various state math tests and writing questions that resembled those posed in <italic>HELP Math</italic> lessons. The test consisted of questions from five mathematical strands: Basic Skills, Number Sense, Algebra, Geometry, and Data Analysis. A balance of items similar to state math test items and <italic>HELP Math</italic> items was a purposeful attempt to reduce bias across control and treatment groups.</p>
<p>One year before the study described here, we created a pilot test with 20 Basic Skills items and 32 items across the four mathematical strands (Number Sense, Algebra, Geometry, and Data Analysis), for a total of 52 items. We analyzed items for language load, clarity of wording, and cultural bias. Final items were piloted with 42 middle school students recruited through summer school programs in the local community. Results were organized into lowest and highest scoring items within each strand. For the lowest scoring items, we set 2 standard deviations from the mean as the cut point for making changes or deleting the item. Two items were modified using that criterion. The highest scoring items on the pilot test were from the Basic Skills strand. Within Basic Skills, 1 standard deviation above or below the mean was used as a cut point for inclusion in the final version of the test. Of the 20 basic skills items, 11 scored within 1 standard deviation. This process resulted in a final test containing 40 items, 8 items from each of the five strands. Next, we investigated the technical adequacy of the test. Scores of 586 seventh- and eighth-grade students were included in the analyses. Test reliability was moderate (pretest α = .79, posttest α = .86), with a discrimination index score of .66 at pretest and .62 at posttest (0- to 1-point scale). The distribution of scores was normal as were distributions of scores from subtests.</p>
</sec>
</sec>
<sec id="section18-1534508411436111">
<title>Analysis</title>
<p>Because of the nested nature of the data, we used a two-level HLM for analysis. Level 1 represented student data, specifically pretest results, total time in intervention, and concentration of time in intervention. Level 2 included teacher data—direct observations and fidelity to process (rating scale) scores. Although the direct observations included measurement of both teacher behaviors and student engagement in the program, we have classified it as a teacher variable because student response to an intervention is largely dependent on teacher behavior (<xref ref-type="bibr" rid="bibr24-1534508411436111">Hulleman &amp; Cordray, 2009</xref>). Because of the colinearity between total time in intervention and concentration of time, we ran two separate models, one for each variable, taking the form:</p>
<p><disp-formula id="disp-formula1-1534508411436111">
<mml:math display="block" id="math1-1534508411436111">
<mml:mrow>
<mml:mtext>Math posttest</mml:mtext>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>pretest</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>total minutes or concentration</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:mtext>r</mml:mtext>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-1534508411436111" xlink:href="10.1177_1534508411436111-eq1.tif"/>
</disp-formula></p>
<p><disp-formula id="disp-formula2-1534508411436111">
<mml:math display="block" id="math2-1534508411436111">
<mml:mrow>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>01</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mtext>fidelity to process</mml:mtext>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>02</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>direct observation</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:mtext>u</mml:mtext>
<mml:mn>0</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-1534508411436111" xlink:href="10.1177_1534508411436111-eq2.tif"/>
</disp-formula></p>
<p><disp-formula id="disp-formula3-1534508411436111">
<mml:math display="block" id="math3-1534508411436111">
<mml:mrow>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>10</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-1534508411436111" xlink:href="10.1177_1534508411436111-eq3.tif"/>
</disp-formula></p>
<p><disp-formula id="disp-formula4-1534508411436111">
<mml:math display="block" id="math4-1534508411436111">
<mml:mrow>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>20</mml:mn>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-1534508411436111" xlink:href="10.1177_1534508411436111-eq4.tif"/>
</disp-formula></p>
<p>Because we had no a priori theory or empirical direction concerning random coefficients for the pretest and total minutes/concentration slopes, those remained fixed. Likewise, lacking any theoretical or empirical direction on interactions, we did not include any cross-level interactions with direct observations, fidelity to process, total time, and concentration of time. Two of the measures, the direct observation total score and the score from the fidelity to process scale were grand-mean centered in the analysis. Pretest score, total time, and concentration of time were entered uncentered. Level 2 variables were centered to make 0 a meaningful value and aid in the interpretation of the intercept. As <xref ref-type="bibr" rid="bibr21-1534508411436111">Hofmann and Gavin (1998)</xref> describe, sometimes 0 has no real meaning, as it falls outside of the range of the real data or is not included in a variable’s scale of measurement (as in an ordinal scale with no 0). Grand mean centering gives the 0 a real value. With the Level 2 variables here, direct observations and fidelity to process, 0 was not a value in the real range of data and were thus centered. Level 1 variables were not mean centered because 0 had real meaning—a test score of “0” or zero minutes on the program.</p>
<p>It is important to note that our goal in this article was not model building. Rather, we sought to test the relation between a specific index of fidelity measures and an educational outcome. Thus, the results below do not include a “full” model and a parsimonious “final” model. Results do, however, include four different models—an empty model and three additive models—to measure the contributions of the addition of variables. The empty model includes no predictors. Model 1 includes only the pretest at Level 1. Model 2 includes all Level 1 variables—pretest, and either total time in intervention or concentration of time in intervention. Model 3 includes all Level 1 variables and both Level 2 variables—direct observation of implementation fidelity and the fidelity to process score. Intraclass correlation coefficients (ICCs) are calculated for each model, and Level 1 percentage of variance accounted for (PVAF; <xref ref-type="bibr" rid="bibr23-1534508411436111">Hox, 2002</xref>) is determined after the addition of pretest to the empty model and after the addition of either total minutes or concentration is added to Model 1 (the pretest-only model). We also re-ran Model 3 with all predictors standardized, facilitating a comparison of the effects across all independent variables.</p>
</sec>
</sec>
<sec id="section19-1534508411436111" sec-type="results">
<title>Results</title>
<p>Beginning with descriptive statistics, results indicate an increase of 1.71 points (or 11%) in mean math performance from the pretest to the posttest (see <xref ref-type="table" rid="table3-1534508411436111">Table 3</xref>). On average, each student spent a little more than 1,184 total minutes (almost 20 hours) on the intervention. Teachers averaged 26.65 points out of a possible score of 35 on the fidelity to process scale (76.14%). Mean scores on the direct observation of implementation fidelity averaged 33 out of a possible 40 (82.5%). EL students achieved a mean of 17.46 on the posttest, whereas non-EL students achieved slightly less with a mean of 16.85; differences were not statistically significant. This latter difference is important to note because the intervention was designed to meet the needs of ELLs, but 31% of this sample represented fluent English speakers.</p>
<table-wrap id="table3-1534508411436111" position="float">
<label>Table 3.</label>
<caption>
<p>Descriptive Statistics for Dependent and Independent Variables</p>
</caption>
<graphic alternate-form-of="table3-1534508411436111" xlink:href="10.1177_1534508411436111-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Variable</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center">Minimum</th>
<th align="center">Maximum</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pretest</td>
<td>15.56</td>
<td>6.18</td>
<td>0.0</td>
<td>33.0</td>
</tr>
<tr>
<td>Posttest</td>
<td>17.27</td>
<td>7.16</td>
<td>1.0</td>
<td>38.0</td>
</tr>
<tr>
<td>ELLs</td>
<td>17.46</td>
<td>6.99</td>
<td>1.0</td>
<td>38.0</td>
</tr>
<tr>
<td>Non-ELLs</td>
<td>16.85</td>
<td>7.55</td>
<td>1.0</td>
<td>36.0</td>
</tr>
<tr>
<td>Total minutes per student</td>
<td>1184.14</td>
<td>488.48</td>
<td>24.9</td>
<td>2475.1</td>
</tr>
<tr>
<td>Time concentration: Minutes per session</td>
<td>11.09</td>
<td>5.64</td>
<td align="center">—</td>
<td align="center">—</td>
</tr>
<tr>
<td>Fidelity to process score</td>
<td>26.65</td>
<td>6.31</td>
<td>14.0</td>
<td>35.0</td>
</tr>
<tr>
<td>Direct observation score</td>
<td>32.89</td>
<td>6.31</td>
<td>22.0</td>
<td>38.0</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Turning to HLM results, <xref ref-type="table" rid="table4-1534508411436111">Table 4</xref> presents the findings for the models with total time. In Model 3, three variables prove significant—total minutes in intervention (<italic>p</italic> &lt;.001), direct observation (<italic>p</italic> =.04), and pretest (<italic>p</italic> &lt;. 001). For total minutes, the relation is positive—more minutes equals greater performance—but the effect is small: Each 1-minute increase on the intervention results in a .001-point increase in math performance. For direct observation, the relation is likewise positive; each one-unit increase on direct observation measure results in a .42-point increase in performance. As is often the case, the relationship between pretest and posttest sores is positive. In this model, fidelity to process is not significant. The “standard” column in <xref ref-type="table" rid="table4-1534508411436111">Table 4</xref> presents the results with the variables on a standard scale. The variable with the greatest effect is pretest, followed by direct observation and total minutes.</p>
<table-wrap id="table4-1534508411436111" position="float">
<label>Table 4.</label>
<caption>
<p>HLM Results for Total Minutes Spent in Intervention</p>
</caption>
<graphic alternate-form-of="table4-1534508411436111" xlink:href="10.1177_1534508411436111-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Effects</th>
<th align="center">Empty Coeff. (<italic>SE</italic>)</th>
<th align="center">Model 1 Coeff. (<italic>SE</italic>)</th>
<th align="center">Model 2 Coeff. (<italic>SE</italic>)</th>
<th align="center">Model 3 Coeff. (<italic>SE</italic>)</th>
<th align="center">Standard Coeff. (<italic>SE</italic>)</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Fixed</td>
</tr>
<tr>
<td>Intercept</td>
<td>17.25 (1.05)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>8.43 (1.00)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>8.47 (0.96)</td>
<td>8.32 (0.97)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>17.03 (0.57)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
</tr>
<tr>
<td>Total minutes</td>
<td/>
<td/>
<td>0.002 (0.00)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>0.001 (0.000)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>0.80 (0.23)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
</tr>
<tr>
<td>Pretest</td>
<td/>
<td>0.577 (0.04)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>0.56 (0.04)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>0.57 (0.06)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>3.54 (0.34)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
</tr>
<tr>
<td>Fidelity to process</td>
<td/>
<td/>
<td/>
<td>0.03 (0.15)</td>
<td>0.15 (0.82)</td>
</tr>
<tr>
<td>Direct observation</td>
<td/>
<td/>
<td/>
<td>0.42 (0.19)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>1.58 (0.70)<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
</tr>
<tr>
<td colspan="6">Random</td>
</tr>
<tr>
<td><italic>u</italic><sub>00</sub></td>
<td>23.05</td>
<td>10.89</td>
<td>9.34</td>
<td>7.15</td>
<td>7.15</td>
</tr>
<tr>
<td>σ<sup>2</sup></td>
<td>30.67</td>
<td>23.25</td>
<td>22.99</td>
<td>22.99</td>
<td>22.99</td>
</tr>
<tr>
<td>χ<sup>2</sup></td>
<td>346.57<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>239.04<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>200.08<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>134.19<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
<td>134.19<xref ref-type="table-fn" rid="table-fn4-1534508411436111">*</xref></td>
</tr>
<tr>
<td>ICC</td>
<td>.43</td>
<td>.31</td>
<td>.28</td>
<td>.23</td>
<td/>
</tr>
<tr>
<td>Level 1 PVAF</td>
<td/>
<td>.24</td>
<td>.01</td>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-1534508411436111">
<p><italic>Note</italic>. Standard = Model 3 with predictor variables converted to standard scores; HLM = hierarchical linear model; ICC = intraclass correlation coefficient; PVAF = percentage of variance accounted for.</p>
</fn>
<fn id="table-fn4-1534508411436111">
<label>*</label>
<p><italic>p</italic> &lt; .05.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>With concentration of time as the variable included in Level 1, rather than total minutes, the same pattern of significant and nonsignificant variables is evident (see <xref ref-type="table" rid="table5-1534508411436111">Table 5</xref>). As indicated in Model 3, concentration of time in intervention (<italic>p</italic> = .03), direct observation (<italic>p</italic> = .04), and pretest (<italic>p</italic> &lt; .001) are significant, whereas fidelity to process is not (<italic>p</italic> = .77). For concentration, each additional minute per session on the intervention yields a .10-point increase in math performance. Likewise, as the direct observation score increases 1 point, math performance increases by .43 points. The latter is quite similar across models. As above, the standardized coefficients indicate pretest is the strongest predictor, followed by direct observation and total minutes. To reiterate, direct observation of implementation fidelity was the strongest predictor of math performance, after the pretest score.</p>
<table-wrap id="table5-1534508411436111" position="float">
<label>Table 5.</label>
<caption>
<p>HLM Results for Concentration of Time in Intervention</p>
</caption>
<graphic alternate-form-of="table5-1534508411436111" xlink:href="10.1177_1534508411436111-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Effects</th>
<th align="center">Empty Coeff. (<italic>SE</italic>)</th>
<th align="center">Model 1 Coeff. (<italic>SE</italic>)</th>
<th align="center">Model 2 Coeff. (<italic>SE</italic>)</th>
<th align="center">Model 3 Coeff. (<italic>SE</italic>)</th>
<th align="center">Standard Coeff. (<italic>SE</italic>)</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Fixed</td>
</tr>
<tr>
<td>Intercept</td>
<td>17.25 (1.05)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>8.43 (1.00)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>8.42 (1.00)</td>
<td>8.28 (0.96)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>17.07</td>
</tr>
<tr>
<td>Total minutes</td>
<td/>
<td/>
<td>0.16 (.07)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>0.10 (0.05)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>0.57 (0.26)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
</tr>
<tr>
<td>Pretest</td>
<td/>
<td>0.577 (0.04)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>.57 (.04)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>0.58 (0.05)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>3.58 (0.33)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
</tr>
<tr>
<td>Fidelity to process</td>
<td/>
<td/>
<td/>
<td>0.05 (0.16)</td>
<td>0.25 (0.76)</td>
</tr>
<tr>
<td>Direct observation</td>
<td/>
<td/>
<td/>
<td>0.43 (0.19)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>1.60 (0.70)<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
</tr>
<tr>
<td colspan="6">Random</td>
</tr>
<tr>
<td><italic>u</italic><sub>00</sub></td>
<td>23.05</td>
<td>10.89</td>
<td>10.75</td>
<td>7.94</td>
<td>7.94</td>
</tr>
<tr>
<td>σ<sup>2</sup></td>
<td>30.67</td>
<td>23.25</td>
<td>23.06</td>
<td>23.09</td>
<td>23.09</td>
</tr>
<tr>
<td>χ<sup>2</sup></td>
<td>346.57<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>239.04<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>232.75<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>149.48<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
<td>149.48<xref ref-type="table-fn" rid="table-fn6-1534508411436111">*</xref></td>
</tr>
<tr>
<td>ICC</td>
<td>.43</td>
<td>.31</td>
<td>.31</td>
<td>.25</td>
<td/>
</tr>
<tr>
<td>Level 1 PVAF</td>
<td/>
<td>.24</td>
<td>.01</td>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-1534508411436111">
<p><italic>Note</italic>. Standard = Model 3 with predictor variables converted to standard scores. HLM = hierarchical linear model; ICC = Intraclass correlation coefficient; PVAF = Percentage of variance accounted for.</p>
</fn>
<fn id="table-fn6-1534508411436111">
<label>*</label>
<p><italic>p</italic> &lt; .05.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Turning to variance components (bottom panels of <xref ref-type="table" rid="table4-1534508411436111">Tables 4</xref> and <xref ref-type="table" rid="table5-1534508411436111">5</xref>), the empty model ICC was .43, meaning 43% of total variance was between classes. This between-class variance was reduced similarly from one model to the next with the use of both measures of time. In Model 3, the ICC was 23% for total minutes and 25% for concentration, although significant between-class variance remains in both cases. At Level 1, the addition of pretest accounts for 24% of the variance, when compared to the empty model. The addition of total time, or concentration of time, accounts for only 1%, as compared to the pretest only model. Finally, results of separate HLM analyses across both total time and concentration of time revealed no significant differences in math performance between EL and non-EL students (when EL status was included as an independent variable). Thus, fidelity does not appear to be jeopardized when the program is used outside of its originally intended population.</p>
<p>It is also possible to calculate optimal times for students to participate in the program in order to achieve desirable outcomes. The latter we define as an “A” grade on the posttest, which is a score ranging from 36 to 40. To determine optimal amounts of time, we used the HLM equation with the coefficients reported in <xref ref-type="table" rid="table4-1534508411436111">Tables 4</xref> and <xref ref-type="table" rid="table5-1534508411436111">5</xref>, and the mean values reported in <xref ref-type="table" rid="table3-1534508411436111">Table 3</xref> for the pretest and fidelity to process variables. For fidelity to structure, we set the variable to 40, which was the highest possible score on the direct observation measure. So, for a student with an average pretest score, a teacher with average scores related to fidelity to process who institutes the program as intended (score of 40 on direct observation), the optimal number of total minutes in the program is 3,500 (or approximately 58 hours). It is also important to consider concentration, or optimal amount of time. Using coefficients from <xref ref-type="table" rid="table5-1534508411436111">Table 5</xref> and the same variable settings used for the total time calculation, optimal time per session is approximately 15 minutes.</p>
<p>Continuing to explore the relation between total time and concentration of time, the data reveal that, except for two outliers, the fewer the overall intervention days spent on the computer, the greater the minutes spent per session and vice versa. As an example, the fewest number of days overall (elapsed time) was 54 days, and students in this classroom spent 24 minutes per day interacting with the intervention, whereas the class with the greatest amount of days, 157, averaged only 5.74 minutes per day on the intervention.</p>
</sec>
<sec id="section20-1534508411436111" sec-type="discussion">
<title>Discussion</title>
<sec id="section21-1534508411436111">
<title>Summary of Findings</title>
<p>Four variables were significant in their ability to predict student outcomes: (a) teacher adherence to and student engagement with the program as measured through direct observation, (b) concentration of time in intervention, (c) total time in intervention, and (d) pretest scores. The first three of these variables were classified as fidelity to structure variables and are listed in order of importance as indicated by the results. Specifically, when measured as total time, math performance increased by one point for every 1,000 minutes, and when measured as concentration, an increase of 10 minutes yielded a 1-point increase in math performance. When the independent variables were standardized, by converting them to <italic>z</italic> scores, results revealed that the “adherence” variable, as measured through direct observation, was approximately 2 to 3 times greater than that of the variable of time in the program. More specifically, increasing the direct observation score by 1 <italic>SD</italic> resulted in a 1.59-point increase in math posttest (40 points possible), whereas a 1 <italic>SD</italic> increase in total minutes yielded a .80 increase in posttest score and a 1 <italic>SD</italic> increase in concentration produced a .56 increase in math posttest. The effect of adherence to the program is so strong that modest decreases on the direct observation score require substantial increases of time spent in the program to maintain a desirable achievement outcome.</p>
<p>In summary, fidelity of implementation, and specifically those variables we grouped as fidelity to structure, of a computer- based middle school math program is significantly and positively related to increased student performance. Results showed that an increased fidelity to structure relates significantly to higher outcomes in student posttests, whereas fidelity to process—including classroom management, teacher communication, and problem solving—demonstrated no significant increase in outcome measures. Calculations of the effect of time on student outcomes revealed that the optimal concentration of time during any individual session was 15 minutes and the optimal amount of time for students to engage with the program was 58 hours.</p>
</sec>
<sec id="section22-1534508411436111">
<title>Implications for Research and Practice</title>
<p>Guidelines exist for how much time should be spent on a teacher-led intervention. Many of these guidelines are associated with the Response to Intervention model and its emphasis on Tier 2 interventions designed for students who need supplemental instruction to support the core curriculum. Guidelines for these interventions are set at 20 to 40 minutes per session, 5 days per week (<xref ref-type="bibr" rid="bibr12-1534508411436111">Fletcher &amp; Vaughn, 2009</xref>) across 9 to 12 weeks (<xref ref-type="bibr" rid="bibr27-1534508411436111">Mellard &amp; Johnson, 2008</xref>; <xref ref-type="bibr" rid="bibr35-1534508411436111">Pierangelo &amp; Giuliani, 2008</xref>). Guidelines are based on research into the impact of teacher-led interventions. Similar guidelines have not been fully established, however, for computer-based interventions. Results of this study contribute empirically to this discussion, and support the positive influence of time (both length and concentration) on student outcomes as related to a computer-based math intervention. And, a resulting implication for practice dictates the need for teachers to ensure students are spending that allotted time in a productive manner.</p>
<p>Statistically, we found that the optimal concentration of time during any individual session was 15 minutes, and the optimal amount of total time was 58 hours. But to calculate optimal time we had to set parameters around the variables included in the statistical model—for example, we set high standards for students’ posttest scores assuming that positive outcomes implies an A grade at posttest and true adherence to program implementation implies a perfect score on the direct observation measure. Obviously, any changes to these parameters results in a change to the reported optimal time. A statistical analysis, however, is a place to start and because the variables and their parameters have been fully explained in this article, other researchers can continue to explore the construct of “optimal time” as related to computer-based interventions. Interestingly, the time recommended for students to engage in SuccessMaker Math, a computer-based mathematics intervention for students in Grades K–8 (<xref ref-type="bibr" rid="bibr34-1534508411436111">Pearson, 2008-2009</xref>) is 15 minutes per session. And although 15 minutes is set as the default time for any one session of SuccessMaker Math, we were unable to find any empirical data directly related to the program that supported this recommendation. Finally, what was learned from direct observations of <italic>HELP Math</italic> in classroom settings was that the program held students’ attention for at least 30 minutes in every setting, and the complexity of the program’s content (problem solving as opposed to drill and practice) seemed to necessitate this amount of time for students to fully engage (<xref ref-type="bibr" rid="bibr9-1534508411436111">Crawford, 2008</xref>). These observations challenge the findings from the statistical model of 15 minutes per session, implying the need for more research into what is a statistically optimal amount of time per session as opposed to what is realistically optimal in classroom settings.</p>
<p>Teacher adherence to and students’ engagement with the program was found to be even more significant than time. Researchers emphasize the need to conduct direct observations of implementation fidelity but few educational research studies explore the effect of observation scores on student outcomes. We found that an increase in adherence to program elements by teachers and students was positively associated with an increase in student outcomes. Specifically, we found that as the direct observation score increased by 1 point (on a scale of 40), math performance on posttest increased by almost .5 points (also on a scale of 40). Although little research has been conducted into the relation between fidelity of computer-based interventions and student outcomes, these findings imply that fidelity is just as important in this context as it is in teacher-led instruction. These results demonstrate that helping students log on to a computer and then walking away does not constitute appropriate implementation and that to foster positive student outcomes, teachers must implement computer-based programs with the same level of rigor that they implement teacher-led interventions.</p>
<p>Finally, the variable of fidelity to process did not contribute significantly to student outcomes. As we discussed previously, however, subjective measures of process variables are less reliable than more direct observational measures and our findings should not suggest that process variables are unimportant in the field of educational research. As stressed by <xref ref-type="bibr" rid="bibr41-1534508411436111">Tucker and Blythe (2008)</xref>, we should examine two components of implementation fidelity: (a) treatment adherence and (b) practitioner competence. They note that adherence is qualitatively different from competence and we must attempt to measure both. They also call for more research into the impact of these constructs on desired outcomes. Similarly, <xref ref-type="bibr" rid="bibr28-1534508411436111">Mills and Ragan (2000)</xref> identify teacher skills needed for fidelity of computer-based instruction such as integration of computer-based curriculum with classroom instruction, trouble-shooting technology problems as they arise, and seeking out ongoing training—skills that are less observable in a time-constrained direct observation. It is clear that continued research into how best to measure both constructs, and their effect on student outcomes in a computer-based instructional environment, is needed. For teachers, understanding the components and structure of a computer-based curriculum and being able to help students expeditiously with any problems they encounter (both process variables) enables students to increase their time on task, which in turn contributes to positive outcomes.</p>
</sec>
<sec id="section23-1534508411436111">
<title>Limitations</title>
<p>One possible limitation of this study is that our sample included fluent English speakers as well as students who were ELLs—the population for whom the program was intended. However, as previously documented, the program has been found to be successful with both populations of students. Moreover, in this study we found no differences across the two groups of students at posttest and no difference in results of the HLM analyses when the variable of language fluency was included. These findings suggest that this is less of a limitation than it may first appear to be. A second limitation may be the use of an indirect measure of fidelity—the fidelity to process scale used in this study. Specifically, and as discussed in our introduction, indirect measures of fidelity are weakly correlated with direct measures of fidelity (<xref ref-type="bibr" rid="bibr19-1534508411436111">Gresham et al., 2000</xref>), and process variables tend to be more subjective and harder to define than more objective and observable behaviors. And because fidelity to process was the only variable in this study found not to relate to the outcome variable, one must question the validity of the construct. Yet computer-based instruction requires subtle changes to classroom culture and teacher problem-solving and facilitation skills that may not be as easily observable as behaviors associated with teacher-led instruction. Therefore, the challenge is to measure these somewhat elusive variables systematically with as little subjectivity as possible.</p>
</sec>
</sec>
<sec id="section24-1534508411436111" sec-type="conclusions">
<title>Conclusions</title>
<p>Educational research poses challenges that differ greatly from those challenges associated with basic laboratory research. In the lab, research integrity is largely dependent on the researcher who has control over the environment. In the field of education, research integrity is often dependent on the fidelity to which teachers implement the intervention. Although a lack of fidelity may not be intentional, it still challenges the integrity of one’s research and the validity of the findings. We can (and must) improve the implementation fidelity of our research studies and also increase our attention to the impact of this fidelity on outcomes achieved. In this study, we measured the relation between implementation fidelity and student outcomes and found that when an online supplemental math intervention was delivered as intended, with fidelity, student outcomes improved. But, as fidelity of intervention decreased so did student scores, reinforcing the belief that measuring students’ pre–post gains is not enough; we must also interpret these scores in the context of implementation fidelity.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article:</p>
<p>This research was funded in part by the U.S. Department of Education (Grant CFDA84.286B).</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1534508411436111">
<citation citation-type="book">
<collab>American Educational Research Association, American Psychological Association, &amp; National Council on Measurement in Education</collab>. (<year>1999</year>). <source>Standards for educational and psychological testing</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr2-1534508411436111">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Baker</surname><given-names>E.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Technology: How do we know it works?</article-title> In <person-group person-group-type="editor">
<name><surname>Heineke</surname><given-names>W.</given-names></name>
<name><surname>Blasi</surname><given-names>L.</given-names></name>
</person-group> (Eds.), <source>Methods of evaluating educational technology</source> (pp. <fpage>77</fpage>-<lpage>84</lpage>). <publisher-loc>Greenwich, CT</publisher-loc>: <publisher-name>Information Age</publisher-name>.</citation>
</ref>
<ref id="bibr3-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Becker</surname><given-names>H. J.</given-names></name>
</person-group> (<year>1994</year>). <article-title>How exemplary computer-using teachers differ from other teachers: Implications for realizing the potential of computers in schools</article-title>. <source>Journal of Research on Computing in Education</source>, <volume>26</volume>, <fpage>291</fpage>–<lpage>321</lpage>.</citation>
</ref>
<ref id="bibr4-1534508411436111">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Campbell</surname><given-names>D. T.</given-names></name>
<name><surname>Stanley</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1963</year>). <source>Experimental and quasi-experimental designs for research</source>. <publisher-loc>Dallas, TX</publisher-loc>: <publisher-name>Houghton Mifflin</publisher-name>.</citation>
</ref>
<ref id="bibr5-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cavanaugh</surname><given-names>C. L.</given-names></name>
<name><surname>Kim</surname><given-names>A.</given-names></name>
<name><surname>Wanzek</surname><given-names>J.</given-names></name>
<name><surname>Vaughn</surname><given-names>S.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Kindergarten reading interventions for at-risk students: Twenty years of research</article-title>. <source>Learning Disabilities: A Contemporary Journal</source>, <volume>2</volume>, <fpage>9</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr6-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christ</surname><given-names>T. J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Experimental control and threats to internal validity of concurrent and nonconcurrent multiple baseline designs</article-title>. <source>Psychology in the Schools</source>, <volume>44</volume>, <fpage>451</fpage>–<lpage>459</lpage>.</citation>
</ref>
<ref id="bibr7-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>P. A.</given-names></name>
<name><surname>Kulik</surname><given-names>J. A.</given-names></name>
<name><surname>Kulik</surname><given-names>C. C.</given-names></name>
</person-group> (<year>1982</year>). <article-title>Educational outcomes of tutoring: A meta-analysis of findings</article-title>. <source>American Educational Research Journal</source>, <volume>19</volume>, <fpage>237</fpage>–<lpage>248</lpage>.</citation>
</ref>
<ref id="bibr8-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cook</surname><given-names>S. B.</given-names></name>
<name><surname>Scruggs</surname><given-names>T. E.</given-names></name>
<name><surname>Mastropieri</surname><given-names>M. A.</given-names></name>
<name><surname>Casto</surname><given-names>G. C.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Handicapped students as tutors</article-title>. <source>Journal of Special Education</source>, <volume>19</volume>, <fpage>483</fpage>–<lpage>492</lpage>.</citation>
</ref>
<ref id="bibr9-1534508411436111">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Crawford</surname><given-names>L.</given-names></name>
</person-group> (<year>2008</year>, <month>March</month>). <source>Effects of an online supplementary mathematics curriculum for English language learners</source>. <conf-name>Paper presented at the annual meeting of the American Educational Research Association</conf-name>, <conf-loc>New York, NY</conf-loc>.</citation>
</ref>
<ref id="bibr10-1534508411436111">
<citation citation-type="web">
<collab>Digital Directions International</collab>. (<comment>n.d.</comment>). <source>HELP Math: Summary of research findings</source> (<comment>Technical report</comment>). <publisher-loc>Carbondale, CO</publisher-loc>: <publisher-name>Author</publisher-name>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.helpprogram.net">http://www.helpprogram.net</ext-link></comment>.</citation>
</ref>
<ref id="bibr11-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Elbaum</surname><given-names>B.</given-names></name>
<name><surname>Vaughn</surname><given-names>S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>School-based interventions to enhance the self-concept of student with learning disabilities: A meta-analysis</article-title>. <source>Elementary School Journal</source>, <volume>101</volume>, <fpage>303</fpage>–<lpage>329</lpage>.</citation>
</ref>
<ref id="bibr12-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fletcher</surname><given-names>J. M.</given-names></name>
<name><surname>Vaughn</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Response to intervention: Preventing and remediating academic difficulties</article-title>. <source>Child Development Perspectives</source>, <volume>3</volume>, <fpage>30</fpage>–<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr13-1534508411436111">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gall</surname><given-names>M. D.</given-names></name>
<name><surname>Gall</surname><given-names>J. P.</given-names></name>
<name><surname>Borg</surname><given-names>W. R.</given-names></name>
</person-group> (<year>2007</year>). <source>Educational research: An introduction</source> (<edition>8th ed.</edition>). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Allyn &amp; Bacon</publisher-name>.</citation>
</ref>
<ref id="bibr14-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gersten</surname><given-names>R.</given-names></name>
<name><surname>Baker</surname><given-names>S.</given-names></name>
<name><surname>Lloyd</surname><given-names>J. W.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Designing high-quality research in special education: Group experimental design</article-title>. <source>Journal of Special Education</source>, <volume>34</volume>, <fpage>2</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr15-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gersten</surname><given-names>R.</given-names></name>
<name><surname>Edyburn</surname><given-names>D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Defining quality indicators for group design in special education technology research</article-title>. <source>Journal of Special Education Technology</source>, <volume>22</volume>, <fpage>3</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr16-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gersten</surname><given-names>R.</given-names></name>
<name><surname>Fuchs</surname><given-names>L. S.</given-names></name>
<name><surname>Compton</surname><given-names>D.</given-names></name>
<name><surname>Coyne</surname><given-names>M.</given-names></name>
<name><surname>Greenwood</surname><given-names>C.</given-names></name>
<name><surname>Innocenti</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Quality indicators for group experimental and quasi-experimental research in special education</article-title>. <source>Exceptional Children</source>, <volume>71</volume>, <fpage>149</fpage>–<lpage>164</lpage>.</citation>
</ref>
<ref id="bibr17-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gresham</surname><given-names>F. M.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Assessment of treatment integrity in school consultation and prereferral intervention</article-title>. <source>School Psychology Review</source>, <volume>18</volume>, <fpage>37</fpage>–<lpage>50</lpage>.</citation>
</ref>
<ref id="bibr18-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gresham</surname><given-names>F. M.</given-names></name>
<name><surname>Gansle</surname><given-names>K.</given-names></name>
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Treatment integrity in applied behavior analysis with children</article-title>. <source>Journal of Applied Behavior Analysis</source>, <volume>26</volume>, <fpage>257</fpage>–<lpage>263</lpage>.</citation>
</ref>
<ref id="bibr19-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gresham</surname><given-names>F. M.</given-names></name>
<name><surname>MacMillan</surname><given-names>D. L.</given-names></name>
<name><surname>Beebe-Frankenberger</surname><given-names>M. E.</given-names></name>
<name><surname>Bocian</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Treatment integrity in learning disabilities intervention research: Do we really know how treatments are implemented?</article-title> <source>Learning Disabilities Research &amp; Practice</source>, <volume>15</volume>, <fpage>198</fpage>–<lpage>205</lpage>.</citation>
</ref>
<ref id="bibr20-1534508411436111">
<citation citation-type="web">
<collab>HELP Math</collab> (<comment>n.d.</comment>). <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.helpprogram.net">http://www.helpprogram.net</ext-link></comment></citation>
</ref>
<ref id="bibr21-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hofmann</surname><given-names>D. A.</given-names></name>
<name><surname>Gavin</surname><given-names>M. B.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Centering decisions in hierarchical linear models: Implications for research in organizations</article-title>. <source>Journal of Management</source>, <volume>24</volume>, <fpage>623</fpage>–<lpage>641</lpage>.</citation>
</ref>
<ref id="bibr22-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hohmann</surname><given-names>A. A.</given-names></name>
<name><surname>Shear</surname><given-names>M. K.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Community-based intervention research: Coping with the “noise” of real life in study design</article-title>. <source>American Journal of Psychiatry</source>, <volume>159</volume>, <fpage>201</fpage>–<lpage>207</lpage>.</citation>
</ref>
<ref id="bibr23-1534508411436111">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hox</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <source>Multilevel analysis: Techniques and applications</source>. <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr24-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hulleman</surname><given-names>C. S.</given-names></name>
<name><surname>Cordray</surname><given-names>D. S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Moving from the lab to the field: The role of fidelity and achieved relative intervention strength</article-title>. <source>Journal of Research on Educational Effectiveness</source>, <volume>2</volume>, <fpage>88</fpage>–<lpage>110</lpage>.</citation>
</ref>
<ref id="bibr25-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Maddux</surname><given-names>C. D.</given-names></name>
<name><surname>Johnson</surname><given-names>L.</given-names></name>
<name><surname>Harlow</surname><given-names>S.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The state of the art in computer education: Issues for discussion with teachers-in-training</article-title>. <source>Journal of Technology and Teacher Education</source>, <volume>1</volume>, <fpage>219</fpage>–<lpage>228</lpage>.</citation>
</ref>
<ref id="bibr26-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Melde</surname><given-names>C.</given-names></name>
<name><surname>Esbensen</surname><given-names>F.</given-names></name>
<name><surname>Tusinski</surname><given-names>K.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Addressing program fidelity using onsite observations and program provider descriptions of program delivery</article-title>. <source>Evaluation Review</source>, <volume>30</volume>, <fpage>714</fpage>–<lpage>740</lpage>.</citation>
</ref>
<ref id="bibr27-1534508411436111">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mellard</surname><given-names>D. F.</given-names></name>
<name><surname>Johnson</surname><given-names>E.</given-names></name>
</person-group> (<year>2008</year>). <source>RTI: A practitioner’s guide to implementing response to intervention</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin Press</publisher-name>.</citation>
</ref>
<ref id="bibr28-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mills</surname><given-names>S.</given-names></name>
<name><surname>Ragan</surname><given-names>T.</given-names></name>
</person-group> (<year>2000</year>). <article-title>A tool for analyzing implementation fidelity of an integrated learning system</article-title>. <source>Educational Technology Research and Development</source>, <volume>48</volume>, <fpage>21</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr29-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mislevy</surname><given-names>R. J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Validity by design</article-title>. <source>Educational Researcher</source>, <volume>36</volume>, <fpage>463</fpage>–<lpage>469</lpage>.</citation>
</ref>
<ref id="bibr30-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mowbray</surname><given-names>C. T.</given-names></name>
<name><surname>Holter</surname><given-names>M. C.</given-names></name>
<name><surname>Teague</surname><given-names>G. B.</given-names></name>
<name><surname>Bybee</surname><given-names>D.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Fidelity criteria: Development, measurement, and validation</article-title>. <source>American Journal of Evaluation</source>, <volume>24</volume>, <fpage>315</fpage>–<lpage>340</lpage>.</citation>
</ref>
<ref id="bibr31-1534508411436111">
<citation citation-type="book">
<collab>National Research Council of the National Academies: Committee for a Review of the Evaluation Data on the Effectiveness of NSF-Supported and Commercially Generated Mathematics Curriculum Materials</collab>. (<year>2004</year>). <source>On evaluating curricular effectiveness: Judging the quality of K–12 mathematics evaluations</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr32-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Noell</surname><given-names>G. H.</given-names></name>
<name><surname>Gresham</surname><given-names>F. M.</given-names></name>
<name><surname>Gansle</surname><given-names>K. A.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Does treatment integrity matter? A preliminary investigation of instructional implementation and mathematics performance</article-title>. <source>Journal of Behavioral Education</source>, <volume>11</volume>, <fpage>51</fpage>–<lpage>67</lpage>.</citation>
</ref>
<ref id="bibr33-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>O’Donnell</surname><given-names>C. L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Defining, conceptualizing, and measuring fidelity of implementation and its relation to outcomes in K–12 curriculum intervention research</article-title>. <source>Review of Educational Research</source>, <volume>78</volume>, <fpage>33</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr34-1534508411436111">
<citation citation-type="book">
<collab>Pearson Education</collab>. (<year>2008–2009</year>). <source>SuccessMaker 2.0 math reference guide</source>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr35-1534508411436111">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pierangelo</surname><given-names>R.</given-names></name>
<name><surname>Giuliani</surname><given-names>G.</given-names></name>
</person-group> (<year>2008</year>). <source>Frequently asked questions about response to intervention</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin Press</publisher-name>.</citation>
</ref>
<ref id="bibr36-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Power</surname><given-names>T. J.</given-names></name>
<name><surname>Blom-Hoffman</surname><given-names>J.</given-names></name>
<name><surname>Clarke</surname><given-names>A. T.</given-names></name>
<name><surname>Riley-Tillman</surname><given-names>T. C.</given-names></name>
<name><surname>Kelleher</surname><given-names>C.</given-names></name>
<name><surname>Manz</surname><given-names>P. H.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Reconceptualizing intervention integrity: A partnership-based framework for linking research with practice</article-title>. <source>Psychology in the Schools</source>, <volume>42</volume>, <fpage>495</fpage>–<lpage>507</lpage>.</citation>
</ref>
<ref id="bibr37-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rohrbeck</surname><given-names>C. A.</given-names></name>
<name><surname>Ginsburg-Block</surname><given-names>M. D.</given-names></name>
<name><surname>Fantuzzo</surname><given-names>J. W.</given-names></name>
<name><surname>Miller</surname><given-names>T. R.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Peer-assisted learning interventions with elementary school students: A meta-analytic review</article-title>. <source>Journal of Educational Psychology</source>, <volume>95</volume>, <fpage>240</fpage>–<lpage>257</lpage>.</citation>
</ref>
<ref id="bibr38-1534508411436111">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shadish</surname><given-names>W. R.</given-names></name>
<name><surname>Cook</surname><given-names>T. D.</given-names></name>
<name><surname>Campbell</surname><given-names>D. T.</given-names></name>
</person-group> (<year>2002</year>). <source>Experimental and quasi-experimental designs for generalized causal inference</source>. <publisher-loc>Belmont, CA</publisher-loc>: <publisher-name>Wadsworth</publisher-name>.</citation>
</ref>
<ref id="bibr39-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Perspectives on evidence-based research in education: What works? Issues in synthesizing educational program evaluations</article-title>. <source>Educational Researcher</source>, <volume>37</volume>(<issue>5</issue>), <fpage>5</fpage>–<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr40-1534508411436111">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Songer</surname><given-names>N. B.</given-names></name>
<name><surname>Gotwals</surname><given-names>A. W.</given-names></name>
</person-group> (<year>2005</year>, <month>April</month>). <article-title>Fidelity of implementation in three sequential curricular units</article-title>. In <person-group person-group-type="author">
<name><surname>Lynch</surname><given-names>S.</given-names></name>
</person-group> (Chair) &amp; <person-group person-group-type="author">
<name><surname>O’Donnell</surname><given-names>C. L.</given-names></name></person-group>, <source>“Fidelity of implementation” in implementation and scale-up research designs: Applications from four studies of innovative science curriculum materials and diverse populations</source>. <conf-name>Symposium conducted at the annual meeting of the American Educational Research Association</conf-name>, <conf-loc>Montreal, Canada</conf-loc>.</citation>
</ref>
<ref id="bibr41-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tucker</surname><given-names>A. R.</given-names></name>
<name><surname>Blythe</surname><given-names>B.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Attention to treatment fidelity in social work outcomes: A review of the literature from the 1990s</article-title>. <source>Social Work Research</source>, <volume>32</volume>, <fpage>185</fpage>–<lpage>190</lpage>.</citation>
</ref>
<ref id="bibr42-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weston</surname><given-names>T.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Formative evaluation for implementation: Evaluating educational technology applications and lessons</article-title>. <source>American Journal of Evaluation</source>, <volume>25</volume>, <fpage>51</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr43-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ysseldyke</surname><given-names>J.</given-names></name>
<name><surname>Bolt</surname><given-names>D. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Effect of technology-enhanced continuous progress monitoring on math achievement</article-title>. <source>School Psychology Review</source>, <volume>36</volume>, <fpage>453</fpage>–<lpage>467</lpage>.</citation>
</ref>
<ref id="bibr44-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ysseldyke</surname><given-names>J.</given-names></name>
<name><surname>Spicuzza</surname><given-names>R.</given-names></name>
<name><surname>Kosciolek</surname><given-names>S.</given-names></name>
<name><surname>Teelucksingh</surname><given-names>E.</given-names></name>
<name><surname>Boys</surname><given-names>C.</given-names></name>
<name><surname>Lemkuil</surname><given-names>A.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Using curriculum-based instructional management system to enhance math achievement in urban schools</article-title>. <source>Journal of Education for Students Placed at Risk</source>, <volume>8</volume>, <fpage>247</fpage>–<lpage>265</lpage>.</citation>
</ref>
<ref id="bibr45-1534508411436111">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zvoch</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Treatment fidelity in multisite evaluation</article-title>. <source>American Journal of Evaluation</source>, <volume>30</volume>, <fpage>44</fpage>–<lpage>61</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>