<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TSO</journal-id>
<journal-id journal-id-type="hwp">sptso</journal-id>
<journal-id journal-id-type="nlm-ta">Teach Sociol</journal-id>
<journal-title>Teaching Sociology</journal-title>
<issn pub-type="ppub">0092-055X</issn>
<issn pub-type="epub">1939-862X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0092055X13479949</article-id>
<article-id pub-id-type="publisher-id">10.1177_0092055X13479949</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Chalkboard Versus the Avatar</article-title>
<subtitle>Comparing the Effectiveness of Online and In-class Courses</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Bergstrand</surname><given-names>Kelly</given-names></name>
<xref ref-type="aff" rid="aff1-0092055X13479949">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Savage</surname><given-names>Scott V.</given-names></name>
<xref ref-type="aff" rid="aff2-0092055X13479949">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0092055X13479949">
<label>1</label>University of Arizona, Tucson, AZ, USA</aff>
<aff id="aff2-0092055X13479949">
<label>2</label>University of California, Riverside, CA, USA</aff>
<author-notes>
<corresp id="corresp1-0092055X13479949">Kelly Bergstrand, Department of Sociology, University of Arizona, Social Sciences Building Room 400, 1145 E. South Campus Dr., Tucson, AZ 85721, USA. Email: <email>kellyjb@email.arizona.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>41</volume>
<issue>3</issue>
<fpage>294</fpage>
<lpage>306</lpage>
<permissions>
<copyright-statement>© American Sociological Association 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">American Sociological Association</copyright-holder>
</permissions>
<abstract>
<p>Increasingly, colleges and universities are relying on fully online classes to teach students. This article investigates how students evaluate online courses in comparison to more traditional face-to-face courses. Data come from undergraduate student evaluations of 118 sociology courses, and results of a series of hierarchical linear models indicate that students feel they have learned less in online courses, believe they are treated with more respect in in-class courses, and rate online courses less highly than in-class courses. Findings also suggest that the negative effects of teaching online are not universal for instructors, as the switch to online classes actually results in better evaluations for teachers who typically perform poorly in the classroom. These findings caution against the broad use of online sociology classes as a strategy for coping with increasing enrollments and shrinking budgets and suggest educators should select the course format that best complements their teaching strengths and skills.</p>
</abstract>
<kwd-group>
<kwd>online teaching</kwd>
<kwd>student evaluations of instructors</kwd>
<kwd>learning outcomes</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Increasingly, instructors in higher education are teaching classes that are entirely Web-based as colleges and universities use online courses to deal with larger enrollments and smaller budgets (<xref ref-type="bibr" rid="bibr17-0092055X13479949">Finkelstein and Scholz 2000</xref>). A survey of 2,500 U.S. colleges and universities found that 5.6 million students were enrolled in at least one online class (<xref ref-type="bibr" rid="bibr4-0092055X13479949">Allen and Seaman 2010</xref>). Despite the proliferation of online courses, universities and researchers have only begun to investigate systematically what they can and cannot teach effectively online (<xref ref-type="bibr" rid="bibr10-0092055X13479949">Coughan 2012</xref>; <xref ref-type="bibr" rid="bibr20-0092055X13479949">Jaffee 1997</xref>; <xref ref-type="bibr" rid="bibr45-0092055X13479949">Van Gundy et al. 2006</xref>).</p>
<p>This study adds to the existing research on the effectiveness of online classes by comparing students’ evaluations of online sociology classes to more traditional, face-to-face classes. Relying on a unique data set of aggregate student assessments for 118 college sociology courses at a large public university in the Southwest, we find that students typically evaluate online teachers as less effective than teachers in face-to-face classes, view online teachers as less respectful, indicate that they learn less in online classes, and rate online classes more negatively than face-to-face classes. We also find that these effects are not uniform across instructors, indicating that some instructors are better at teaching Web-delivered courses while others excel in the traditional classroom environment. These findings inform discussion about how mode of course delivery matters for teaching sociology and is a topic warranting both further scholarly investigation and scrutiny in implementation.</p>
<sec id="section1-0092055X13479949">
<title>Literature Review</title>
<sec id="section2-0092055X13479949">
<title>Student Evaluations: Advantages and Shortcomings</title>
<p>Student evaluations of teachers and courses are a fixture of higher education in the United States, with studies reporting that more than 90 percent of schools use them (e.g., <xref ref-type="bibr" rid="bibr38-0092055X13479949">Simpson and Siguaw 2000</xref>). As such, they provide unique insight into students’ perceptions of the educational experience and teacher effectiveness. Further, student evaluations are an institutionalized component of judging college instructors and often weigh heavily in tenure, promotion, and pay decisions (<xref ref-type="bibr" rid="bibr29-0092055X13479949">Murray 1984</xref>).</p>
<p>Nevertheless, student evaluations have been criticized for being biased by factors unrelated to learning; for example, expectations of good grades can increase evaluations of an instructor (e.g., <xref ref-type="bibr" rid="bibr14-0092055X13479949">Ewing 2012</xref>; <xref ref-type="bibr" rid="bibr21-0092055X13479949">Johnson 2003</xref>), and instructors rated as likable receive a boost in ratings of teaching ability, even when associated with a decrease in student perceptions of learning (<xref ref-type="bibr" rid="bibr12-0092055X13479949">Delucchi 2000</xref>). Researchers have also studied the effects of instructors’ traits on student evaluations. Generally, they have not found substantial evaluation differences by gender, although when differences exist they tend to favor male instructors (<xref ref-type="bibr" rid="bibr48-0092055X13479949">Wachtel 1998</xref>). The effects of instructors’ race or ethnicity on evaluations has been surprisingly understudied (<xref ref-type="bibr" rid="bibr48-0092055X13479949">Wachtel 1998</xref>), although <xref ref-type="bibr" rid="bibr37-0092055X13479949">Shapiro (1990)</xref> found that any differences between white and nonwhite instructors disappeared after controlling for other variables. Additionally, attributes of the course, such as class size, may affect evaluations, with larger classes receiving lower evaluations (<xref ref-type="bibr" rid="bibr15-0092055X13479949">Feldman 1984</xref>). Similarly, student perceptions of course difficulty can affect evaluations, with courses perceived as very easy or very difficult receiving lower evaluations (<xref ref-type="bibr" rid="bibr8-0092055X13479949">Centra 2003</xref>). Finally, low response rates for course evaluations have the potential to introduce bias by not adequately representing students’ sentiments. This might be particularly relevant for online courses, as one study found that online evaluations had lower response rates than did in-class evaluations, although the course evaluations still had comparable results (<xref ref-type="bibr" rid="bibr42-0092055X13479949">Stowell, Addison, and Smith 2012</xref>).</p>
<p>Despite these concerns, numerous reviews have demonstrated that student evaluations are generally reliable and valid instruments for measuring learning outcomes (<xref ref-type="bibr" rid="bibr11-0092055X13479949">d’Apollonia and Abrami 1997</xref>; <xref ref-type="bibr" rid="bibr25-0092055X13479949">Marsh 1987</xref>; <xref ref-type="bibr" rid="bibr27-0092055X13479949">Marsh and Roche 1997</xref>; <xref ref-type="bibr" rid="bibr35-0092055X13479949">Seldin 1993</xref>). For example, <xref ref-type="bibr" rid="bibr11-0092055X13479949">d’Apollonia and Abrami (1997)</xref> found that almost half of the variation in student learning can be explained by student perceptions of teaching effectiveness. Scholars also defend student ratings by noting that other methods of evaluating teaching, such as classroom visitations and self-ratings, are beset with their own problems (<xref ref-type="bibr" rid="bibr3-0092055X13479949">Aleamoni 1981</xref>). In fact, research has shown that students’ ratings have greater validity coefficients than ratings from colleagues, trained observers, or self-reports (<xref ref-type="bibr" rid="bibr19-0092055X13479949">Howard, Conway, and Maxwell 1985</xref>). For these reasons, many instructors and administrators support using student ratings to evaluate instruction (<xref ref-type="bibr" rid="bibr35-0092055X13479949">Seldin 1993</xref>).</p>
<p>Despite the potential shortcomings of student evaluations and the fact that they only speak to actual teaching effectiveness to the extent that student perceptions reflect reality, they do provide insight into instruction quality and are important indicators of students’ satisfaction with their education. Whether students believe they are receiving a quality education can affect the success of sociology departments, such as their ability to attract and retain students. Scholars recognize the importance of public opinion for the survival of any profession (e.g., <xref ref-type="bibr" rid="bibr1-0092055X13479949">Abbott 1988</xref>). For a profession like sociology, one of the primary ways to influence public opinion is through teaching. If switching to online classes hurts our ability to do this, we hurt the profession.</p>
</sec>
<sec id="section3-0092055X13479949">
<title>Comparing Web-delivered and Traditional Courses</title>
<p>Web courses, defined as “asynchronous computer-mediated courses” (<xref ref-type="bibr" rid="bibr34-0092055X13479949">Schulte 2004</xref>:6), lack direct, face-to-face interaction between instructors and students, making the fully online class qualitatively different from the traditional classroom setting. Proponents of online classes contend that fully online courses cater to the unique experiences of many young adults living in an increasingly global economy and thus might be an effective medium for teaching. <xref ref-type="bibr" rid="bibr23-0092055X13479949">Little, Titarenko, and Bergelson (2005)</xref>, for instance, described how online classes can foster cross-cultural learning by connecting students all over the globe. Online classes also provide platforms for students to engage course material on their own time, which might encourage participation and learning (<xref ref-type="bibr" rid="bibr34-0092055X13479949">Schulte 2004</xref>). This might be particularly true for students who have difficulties attending a traditionally scheduled class because of work responsibilities, family obligations, or chronic illness. In this way, online classes make higher education accessible for students who might otherwise be unable to attend college.<sup><xref ref-type="fn" rid="fn1-0092055X13479949">1</xref></sup> Thus, as <xref ref-type="bibr" rid="bibr9-0092055X13479949">Clark-Ibanez and Scott (2008)</xref> have noted, many students like online classes for the flexibility and anonymity they afford them and even the savings they experience from not having to pay for parking and gasoline. An affinity for the online format might in turn lead students to rate such classes more positively than traditional classroom-based courses.</p>
<p>Opponents of online classes disagree, worrying that online classes place too much of the burden on students and may adversely affect education quality. Students have more discretion over how they manage their time. While such flexibility is typically viewed as a benefit, it could be a drawback if students fall behind on course material. Cramming for tests is a notoriously bad method for learning (<xref ref-type="bibr" rid="bibr6-0092055X13479949">Bloom and Shuell 1981</xref>), and if online courses breed this type of preparatory behavior, students may view them as less effective. The lack of face-to-face interaction with the instructor also may negatively affect how students think about their online instructors and the education they receive. <xref ref-type="bibr" rid="bibr30-0092055X13479949">Onwuegbuzie and his colleagues (2007</xref>:146) noted that college students often identify “the interpersonal context as the most important indicator of effective instruction.” This is a problem for online classes, as emotions are more difficult to express electronically. Students may feel that written feedback or recorded lectures lack the enthusiasm or substance of verbal feedback or real-time lectures. These potential shortcomings could result in lower student evaluations of courses and teachers.</p>
<p>The disagreement between proponents and opponents of online education has spawned research on the effects of online classes. Results are decidedly mixed. A number of studies have found no significant difference between traditional classes and Web-based courses in student achievement (e.g., <xref ref-type="bibr" rid="bibr46-0092055X13479949">van Schaik, Barker, and Beckstrand 2003</xref>; <xref ref-type="bibr" rid="bibr47-0092055X13479949">Waschull 2001</xref>). Still, others have indicated that they do differ, with some studies finding students in online classes perform better (e.g., <xref ref-type="bibr" rid="bibr43-0092055X13479949">Tucker 2001</xref>) and some finding the opposite (e.g., <xref ref-type="bibr" rid="bibr44-0092055X13479949">Urtel 2008</xref>). Meta-analyses have confirmed these mixed results, consistently finding that quasi-experimental studies on the whole show no significant difference between the two formats (<xref ref-type="bibr" rid="bibr28-0092055X13479949">Means et al. 2009</xref>; <xref ref-type="bibr" rid="bibr39-0092055X13479949">Sitzmann et al. 2006</xref>), but that this is in part due to the great variability in findings across studies (<xref ref-type="bibr" rid="bibr5-0092055X13479949">Bernard et al. 2004</xref>).</p>
<p>Closer inspection of these studies helps explain the discrepant results. A majority of studies comparing online to more traditional classes were quasi-experimental where one course was taught in both an online and in-class format (<xref ref-type="bibr" rid="bibr24-0092055X13479949">Logan, Augustyniak, and Rees 2002</xref>; <xref ref-type="bibr" rid="bibr43-0092055X13479949">Tucker 2001</xref>; <xref ref-type="bibr" rid="bibr44-0092055X13479949">Urtel 2008</xref>; <xref ref-type="bibr" rid="bibr46-0092055X13479949">van Schaik et al. 2003</xref>; <xref ref-type="bibr" rid="bibr47-0092055X13479949">Waschull 2001</xref>). Several of these studies had small sample sizes, with classes as small as 11 students (<xref ref-type="bibr" rid="bibr24-0092055X13479949">Logan et al. 2002</xref>). Additionally, the author of the study and the instructor were frequently the same person, begging the question of whether instructors introduced bias into the studies by altering their teaching strategies, consciously or not. For example, by artificially making online and in-class courses equivalent, these studies could be missing out on key differences that emerge between the two forms in practice, such as if instructors design syllabi differently depending on the mode of course delivery. Lastly, by limiting the study to one instructor, these studies were unable to evaluate other relevant factors and traits that vary across instructors and could affect course evaluations. On those occasions when researchers have relied on other methodological approaches to study the effects of online classes, new problems arise. For instance, <xref ref-type="bibr" rid="bibr22-0092055X13479949">Kelly, Ponton, and Rovai’s (2007)</xref> study compared Web courses to traditional courses without controlling for other variables that may affect student evaluations, like course difficulty and course size. Similarly, <xref ref-type="bibr" rid="bibr7-0092055X13479949">Carle’s (2009)</xref> study comparing online and in-class courses also did not control for these two variables. Because student perceptions of course difficulty and course size can affect teaching evaluations (<xref ref-type="bibr" rid="bibr8-0092055X13479949">Centra 2003</xref>; <xref ref-type="bibr" rid="bibr15-0092055X13479949">Feldman 1984</xref>), failing to control for these variables may conceal the true effect of course format.</p>
<p>Thus, our research fills a unique niche by examining multiple courses and instructors, none of whom taught under the assumption they would be studied. Moreover, this research incorporates a number of important contextual variables to the analysis, such as class size, student perceptions of course difficulty, and instructor traits. Finally, our study provides a broad portrait of the effects of online courses and traditional courses. Although a number of studies have relied on the quasi-experimental design to compare traditional and online sociology courses (e.g., <xref ref-type="bibr" rid="bibr13-0092055X13479949">Driscoll et al. 2012</xref>), we are unaware of any study that compares the broad implementation of online courses across an entire sociology department. This is particularly important given the mixed results of previous studies, leaving an open question as to whether online or traditional courses are perceived as more effective. As such, our study addresses the practical issue of what the wide-scale use of online teaching means for students in sociology.</p>
<p>Ongoing debates over the utility of online courses also tend to focus on the course format at the expense of the professionals who teach them. This omission suggests a uniform effect of online classes on teaching effectiveness, but such a view may be inaccurate. Perhaps whether the shift to an online format has a positive or negative effect on teacher effectiveness depends on the teacher. Casual observation tells us that not all teachers are the same. While some thrive in front of the classroom, others struggle with presenting material and engaging in discussion. Online teaching, therefore, may have different effects for different teachers, with some instructors having communication and performance abilities that allow them to excel in classroom settings and others having skill sets better suited for Web-delivered courses, such as strong writing skills.</p>
<p>In addition to assessing perceived teaching effectiveness, we also address whether students felt that they were treated with respect in the course. While studies have focused on the effects of mode of course delivery on evaluations of teaching effectiveness or learning outcomes, rarely do they assess whether an online format might negatively affect interpersonal interactions between students and instructors. Because online courses do not include face-to-face interaction and primarily rely on electronic, written communications that lack facial and vocal cues that can convey sympathy or emotion, it is possible that students will perceive online instructors as being less respectful toward, or concerned about, students. Thus, in our analyses we examine whether online classes affect whether students believe they were treated with respect, a measure that has been found to be part of a valid construct assessing how students perceive interactions with instructors (<xref ref-type="bibr" rid="bibr33-0092055X13479949">Roberts and Clifton 1992</xref>). This is supported by factor analyses and undergraduate perceptions that have identified a “respect for student” construct emerging out of whether instructors are polite, sympathetic, helpful, accessible, and hold generally favorable attitudes toward students (<xref ref-type="bibr" rid="bibr36-0092055X13479949">Shank, Walker, and Hayes 1996</xref>). Consequently, a course evaluation question on respect can reveal whether students perceive their interactions with instructors to be negative and may tap into whether students view instructors as unfair, discriminatory, sarcastic, disdainful, or patronizing toward students. Examining this is important because favorable student evaluations of instructors’ respect or concern for students are positively correlated with student achievement and learning in courses (<xref ref-type="bibr" rid="bibr16-0092055X13479949">Feldman 1989</xref>).</p>
<p>This review of the literature on Web courses reveals a number of shortcomings that indicate how little we know about what their implementation means for student evaluations in sociology. By comparing student evaluations of 118 undergraduate sociology courses taught at a large public research university, we examine whether students deem online classes to be as effective as more traditional in-person classes. We also examine whether the shift to online classes affects some instructors more than others. Our results are particularly noteworthy because previous studies have linked student evaluations to actual learning outcomes (<xref ref-type="bibr" rid="bibr11-0092055X13479949">d’Apollonia and Abrami 1997</xref>).</p>
</sec>
</sec>
<sec id="section4-0092055X13479949" sec-type="methods">
<title>Data and Methods</title>
<p>To compare online to in-class courses, we rely on teacher-course evaluation data collected and published online by a large public research university in the Southwest, with more than 400 sociology majors. Because course evaluation data are publicly available to the larger university community, we downloaded it to create a data set, with the approval of the university’s Institutional Review Board. Our data set consists of aggregate undergraduate student course evaluations for sociology courses taught between the fall of 2009 and the summer of 2011. The sample only includes evaluations of courses taught by graduate student instructors, as no faculty members taught online courses. The data do not provide information about individual undergraduate students, so we do not control for individual student characteristics or for the possibility that certain types of students are self-selecting into online courses, which could affect evaluations. Our analytic sample includes aggregate evaluations for 118 sociology courses taught by 21 instructors, with an average of 5.6 course evaluations per instructor.<sup><xref ref-type="fn" rid="fn2-0092055X13479949">2</xref></sup></p>
<p>In our sample, the majority of instructors were white (<italic>N</italic> = 18), and there were 8 male instructors and 13 female instructors. On average, the instructors had been enrolled in the graduate program for approximately 5.5 years. Graduate students must complete both a master’s degree and a course on teaching before they can teach classes. The course on teaching provides a general overview of undergraduate instruction and a section on Web-delivered instruction; however, this class is not specifically designed for training in online teaching methods. There are also university resources available to help with the technical aspects of designing and implementing online courses, such as online instructional support liaisons.</p>
<p>Graduate instructors teach undergraduate courses at all levels (100–400) and can choose the format of courses (online or in-class). They are assigned to teach courses based on the needs of the department as well as the expertise and interests of the instructor. Graduate instructors have freedom in designing and teaching their courses; however, all new syllabi for courses must be approved by the department, and faculty members periodically observe classes to evaluate graduate student instruction. Regardless of whether a course is taught completely online or in the classroom, graduate student instructors typically rely on the Desire to Learn (D2L) Web platform. D2L has a number of important functionalities, including a course homepage, discussion pages, chat rooms, online quizzes, drop boxes, and content pages where instructors can post video lectures, PowerPoint slides, and written memos. We do not have survey data on the online teaching practices of graduate students; however, common practices include posting lecture memos or PowerPoint slides to the course Web site and utilizing discussion boards. It is rare for an instructor to provide video lectures or live lectures through videoconferencing, although some instructors do provide audio lectures.</p>
<sec id="section5-0092055X13479949">
<title>Dependent Variables</title>
<p>The four dependent variables for this study are average undergraduate perceptions of the amount learned in the course, the teaching effectiveness of the instructor, the overall rating of the course, and whether the student felt he or she was treated with respect. These come from questions on course evaluation forms administered by the university. The unit of analysis is the course, and each of these four variables is measured at the aggregate course level. To do so, for each dependent variable we calculate a class average by multiplying the number of students who selected an answer choice by the relevant value assigned to the category (e.g., 1–5) and then dividing that sum by the total number of survey respondents.</p>
<p>The dependent variables were measured using students’ responses to the following questions and prompts: “How much do you feel you have learned in this course?” “What is your overall rating of this course?” “What is your overall rating of this instructor’s teaching effectiveness?” and “I was treated with respect in this class.” The answer choices for each variable ranged from 1 (most negative evaluation) to 5 (most positive evaluation) and were phrased as follows: The amount learned ranged from “almost nothing” to “an exceptional amount,” overall course rating varied from “one of the worst” to “one of the best,” teaching effectiveness varied from “almost never effective” to “almost always effective,” and whether students felt like they were treated with respect in the class ranged from “strongly disagree” to “strongly agree.”</p>
</sec>
<sec id="section6-0092055X13479949">
<title>Course-level and Instructor-level Independent Variables</title>
<p>The primary independent variable of interest was whether the course was an in-class course or a Web course (in-class = 0, online = 1). Also of interest was whether some instructors benefited by teaching online classes and whether some instructors were hurt by teaching online classes. To evaluate this, we included two variables. First, we include a measure of each instructor’s average effectiveness for in-class courses only. This measure serves as a proxy for instructors’ in-class skills, such as public speaking or the ability to foster class discussion. Second, we interacted this with the online variable. This allowed us to evaluate whether strong classroom instructors are disproportionately hurt by teaching in a completely online environment and whether weak classroom instructors are helped by teaching in the online environment.</p>
<p>We also controlled for other variables that might affect course evaluations. These independent variables were the gender of the instructor (0 = female, 1 = male), the number of students enrolled in the course, and the response rate of how many enrolled students returned the course evaluation form. We also included a measure of the perceived difficulty of each course, which came from a question on the course evaluation forms asking, “The difficulty level of the course is,” with answers ranging from 1 = “extremely easy” to 5 = “extremely difficult.” We then created a course average by summing together the products of the number of students who selected an answer choice by its assigned value and dividing this summation by the total number of respondents.<sup><xref ref-type="fn" rid="fn3-0092055X13479949">3</xref></sup></p>
</sec>
<sec id="section7-0092055X13479949">
<title>Hierarchical Linear Models</title>
<p>Because it is likely that the teaching abilities of individual instructors are affecting course evaluations, it is necessary to control for the fact that often multiple course evaluations cluster within the same instructor. Consequently, we employ hierarchical linear modeling (HLM) to analyze the data (<xref ref-type="bibr" rid="bibr32-0092055X13479949">Raudenbush and Bryk 2002</xref>; <xref ref-type="bibr" rid="bibr41-0092055X13479949">Snijders and Bosker 1999</xref>). HLM allows researchers to evaluate data that are nested in larger units; for example, students can be nested in schools, employees can be nested in organizations, and families can be nested in neighborhoods. When such clustering occurs, it is likely that observations are no longer independent from one another, violating an assumption in traditional regression methods. HLM accounts for such relations among observations. It also has the ability to incorporate independent variables at both the individual level (e.g., students, employees) as well as at the cluster level (e.g., school, company) as well as include interactions across levels. Specifically, it allows for a multilevel model in which there are level 1 units (in our data, the course evaluations) and level 2 units (in our data, the instructors) and the slope and the intercept from level 1 data are allowed to vary across level 2 units (<xref ref-type="bibr" rid="bibr32-0092055X13479949">Raudenbush and Bryk 2002</xref>). When an intercept or slope does not vary across units, this is termed a <italic>fixed effect</italic>. Fixed effects represent average values across units and are interpreted as regression coefficients (<xref ref-type="bibr" rid="bibr40-0092055X13479949">Snijders 2005</xref>).</p>
<p>For this study, we use HLM to assess the effects of course-level variables on course evaluations while also accounting for any potential effects from instructor-level variables. We use a full maximum likelihood estimation and conduct the analyses using Stata software. The level 1 variables include the predictors relevant to course evaluations, such as whether it is online or in-class, the course evaluation response rate, the number of students enrolled in the class, and the perceived difficulty of the course by students. The level 2 variables pertain to characteristics of the instructors, such as the gender of the instructor.<sup><xref ref-type="fn" rid="fn4-0092055X13479949">4</xref></sup> Of the level 1 variables, we expect the effect of course format and course difficulty to vary depending on the instructor. While instructors have less control over the number of students enrolled in the class, the evaluation response rate, or instructors’ gender, instructors can choose to teach online or in-class or to make their courses more or less difficult. Thus, we include course mode of delivery and course difficulty as random effects in the model.</p>
<p>An intraclass correlation coefficient (ICC) measures the proportion of variance in the outcome that is between units (<xref ref-type="bibr" rid="bibr32-0092055X13479949">Raudenbush and Bryk 2002</xref>). The ICC values for each dependent variable when grouped by instructor and with no independent variables included in the model are as follows: overall course rating, .204; amount learned, .159; teaching effectiveness, .345; and respect, .173. These ICCs indicate that clustering is evident and that it is appropriate to use HLM. Additionally, a conservative likelihood ratio test comparing the multilevel model to linear regression was significant below the .05 level for most models, indicating HLM is the preferred analytical method.</p>
</sec>
<sec id="section8-0092055X13479949">
<title>Differences in Online and Traditional Course Evaluations</title>
<p>Our first set of models assesses the effects of course mode of delivery, the gender of the instructor, the number of enrolled students, the response rate, and the perceived difficulty of the course on each of the four dependent variables (teaching effectiveness, overall course evaluation, amount learned, and treated with respect), grouped by instructor. We initially included course format and perceived difficulty as random effect variables, but indicators of model fit and between-instructor variance revealed that only course format should be estimated as a random effect.</p>
<p>Our second set of models tests the hypothesis that the perceived effectiveness of online courses depends on the skill sets of individual instructors. For these models we add the variable that assesses instructors’ average in-class teaching effectiveness and an interaction term between course format and this variable. To facilitate the interpretation of interaction effects, the in-class effectiveness variable is grand mean centered, such that each instructor’s in-class effectiveness score is subtracted from the mean of all instructors’ in-class effectiveness scores. Because instructors might vary in terms of their preferences for online courses, course difficulty, and the effectiveness of in-class teaching methods, we allow these effects to have a random component. However, as in the first set of models, only the course format variable proved to have a significant random component and was the only random effect included in final models.</p>
</sec>
</sec>
<sec id="section9-0092055X13479949" sec-type="results">
<title>Results</title>
<sec id="section10-0092055X13479949">
<title>Comparing Online Courses to Traditional Courses</title>
<p>Undergraduate students tend to evaluate online courses more negatively than in-class courses (see <xref ref-type="table" rid="table1-0092055X13479949">Table 1</xref>). When providing overall evaluations of a course, students gave online courses significantly lower ratings than traditional courses (β = –.238, <italic>p</italic> = .032). They also indicated that they learned significantly less in online courses (β = –.232, <italic>p</italic> = .022) and felt that they were treated with less respect in online courses (β = –.244, <italic>p</italic> = .002). Results further show that online courses have a marginally significant negative effect on undergraduate perceptions of teaching effectiveness (β = –.191, <italic>p</italic> = .076). Thus, moving from an in-class format to an online format results in a change of approximately –.20 for course evaluation outcome variables that scale from 1 to 5. These coefficients are not large; however, the standard deviations of the means of the outcome variables are fairly small, between .30 and .50, indicating that many instructors have similar scores on the dependent variables. Therefore, the negative effect of online course delivery may in fact be a relatively substantial effect.</p>
<table-wrap id="table1-0092055X13479949" position="float">
<label>Table 1.</label>
<caption>
<p>Hierarchical Linear Model on Students’ Course Evaluations.</p>
</caption>
<graphic alternate-form-of="table1-0092055X13479949" xlink:href="10.1177_0092055X13479949-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Overall course rating<hr/></th>
<th align="center" colspan="2">Amount learned<hr/></th>
<th align="center" colspan="2">Teaching effectiveness<hr/></th>
<th align="center" colspan="2">Respect<hr/></th>
</tr>
<tr>
<th/>
<th align="center">β</th>
<th align="center">SE</th>
<th align="center">β</th>
<th align="center">SE</th>
<th align="center">β</th>
<th align="center">SE</th>
<th align="center">β</th>
<th align="center">SE</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="9">Fixed effects</td>
</tr>
<tr>
<td colspan="9"><italic>Course evaluation level</italic></td>
</tr>
<tr>
<td>  Online</td>
<td>−.238<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">*</xref></td>
<td>.111</td>
<td>−.232<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">*</xref></td>
<td>−.101</td>
<td>−.191<sup><xref ref-type="table-fn" rid="table-fn2-0092055X13479949">^</xref></sup></td>
<td>.108</td>
<td>−.244<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">**</xref></td>
<td>.079</td>
</tr>
<tr>
<td>  Class size</td>
<td>−.003<sup><xref ref-type="table-fn" rid="table-fn2-0092055X13479949">^</xref></sup></td>
<td>.002</td>
<td>−.002</td>
<td>.002</td>
<td>−.004<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">*</xref></td>
<td>.002</td>
<td>−.001</td>
<td>.001</td>
</tr>
<tr>
<td>  Response rate</td>
<td>.462<sup><xref ref-type="table-fn" rid="table-fn2-0092055X13479949">^</xref></sup></td>
<td>.236</td>
<td>.107</td>
<td>.226</td>
<td>.457<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">*</xref></td>
<td>.225</td>
<td>.156</td>
<td>.138</td>
</tr>
<tr>
<td>  Course difficulty</td>
<td>−.377<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">***</xref></td>
<td>.109</td>
<td>−.108</td>
<td>.104</td>
<td>−.260<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">*</xref></td>
<td>.107</td>
<td>−.244<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">***</xref></td>
<td>.063</td>
</tr>
<tr>
<td colspan="9"><italic>Instructor level</italic></td>
</tr>
<tr>
<td>  Gender</td>
<td>.078</td>
<td>.114</td>
<td>.059</td>
<td>.108</td>
<td>.097</td>
<td>.128</td>
<td>.097</td>
<td>.068</td>
</tr>
<tr>
<td>Constant</td>
<td>4.99<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">***</xref></td>
<td>.408</td>
<td>4.32<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">***</xref></td>
<td>.391</td>
<td>5.00<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">***</xref></td>
<td>.397</td>
<td>5.48<xref ref-type="table-fn" rid="table-fn2-0092055X13479949">***</xref></td>
<td>.238</td>
</tr>
<tr>
<td colspan="9">Random effects</td>
</tr>
<tr>
<td>  Online</td>
<td>.043</td>
<td>.036</td>
<td>.015</td>
<td>.024</td>
<td>.050</td>
<td>.040</td>
<td>.060</td>
<td>.026</td>
</tr>
<tr>
<td>  Constant</td>
<td>.016</td>
<td>.021</td>
<td>.022</td>
<td>.018</td>
<td>.037</td>
<td>.028</td>
<td>.000</td>
<td>.000</td>
</tr>
<tr>
<td>  Residual</td>
<td>.142</td>
<td>.023</td>
<td>.132</td>
<td>.020</td>
<td>.120</td>
<td>.021</td>
<td>.048</td>
<td>.007</td>
</tr>
<tr>
<td>Sample size</td>
<td>118</td>
<td/>
<td>118</td>
<td/>
<td>118</td>
<td/>
<td>118</td>
<td/>
</tr>
<tr>
<td>−2 log likelihood</td>
<td>126.44</td>
<td/>
<td>114.12</td>
<td/>
<td>116.93</td>
<td/>
<td>11.24</td>
<td/>
</tr>
<tr>
<td>Bayesian Information Criterion</td>
<td>169.38</td>
<td/>
<td>157.05</td>
<td/>
<td>159.87</td>
<td/>
<td>49.40</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0092055X13479949">
<p>Notes: All regression coefficients are unstandardized. Random effects estimates are variances. SE = standard errors.</p>
</fn>
<fn id="table-fn2-0092055X13479949">
<label>^</label>
<p><italic>p</italic> ≤ .10. *<italic>p</italic> ≤ .05. **<italic>p</italic> ≤ .01. ***<italic>p</italic> ≤ .001 (two-tailed tests).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>In addition, we found several other factors affected course evaluations. Larger class sizes and increased perceptions of course difficulty were associated with more negative course evaluations, and higher response rates were associated with more positive course evaluations. Perceptions of course difficulty had significant negative effects for three of the dependent variables: teaching effectiveness (β = –.260, <italic>p</italic> = .015), the overall course rating (β = –.377, <italic>p</italic> = .001), and the respect variable (β = –.244, <italic>p</italic> = .000). Class size negatively affected both teaching effectiveness (β = –.004, <italic>p</italic> = .011) and the overall rating of a course (β = –.003, <italic>p</italic> = .092). The coefficients for class size are close to zero, but given that online and in-class courses average 42 students, a one unit increase is likely to have a small effect on the dependent variable. Conversely, the response rate had positive effects on teaching effectiveness (β = .457, <italic>p</italic> = .043) and the overall course rating (β = .462, <italic>p</italic> = .051). The coefficients for response rate are larger, but this variable ranges between zero and one. Thus, moving from a hypothetical response rate of zero, where no one submits a course evaluation form, to one in which all enrolled students fill out a course evaluation form increases evaluations of teaching effectiveness and the overall course rating by approximately .45 on a 1 to 5 scale.</p>
</sec>
<sec id="section11-0092055X13479949">
<title>Instructor In-class Effectiveness and Differences in Course Evaluations</title>
<p>Our second set of models tests the hypothesis that the perceived effectiveness of online courses depends on the skill sets of individual instructors. These results are in <xref ref-type="table" rid="table2-0092055X13479949">Table 2</xref>. We find a significant interaction between the instructors’ average in-class teaching effectiveness and course format for two outcome variables: (1) overall course rating and (2) the amount learned in the class. Likelihood ratio tests for these two variables were significant (<italic>p</italic> = .000 for both), indicating improved model fit when adding the in-class teaching effectiveness variable and the interaction term to the first set of models. In regard to the overall course rating, for an instructor with an average in-class teaching effectiveness score, online courses are rated more negatively than in-class courses (β = –.322, <italic>p</italic> = .002). To evaluate the interaction effect, we entered values that would typify instructors with a high in-class effectiveness score, an average effectiveness score, and a low effectiveness score.<sup><xref ref-type="fn" rid="fn5-0092055X13479949">5</xref></sup> We find that for teachers with the highest in-class effectiveness score, the effect of online courses is the most strongly negative, while for instructors with the lowest in-class effectiveness score, online courses actually have a positive effect on overall course evaluations.</p>
<table-wrap id="table2-0092055X13479949" position="float">
<label>Table 2.</label>
<caption>
<p>Hierarchical Linear Modeling with Interaction Effects on Students’ Course Evaluations.</p>
</caption>
<graphic alternate-form-of="table2-0092055X13479949" xlink:href="10.1177_0092055X13479949-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Course rating<hr/></th>
<th align="center" colspan="2">Amount learned<hr/></th>
<th align="center" colspan="2">Respect<hr/></th>
</tr>
<tr>
<th/>
<th align="center">β</th>
<th align="center">SE</th>
<th align="center">β</th>
<th align="center">SE</th>
<th align="center">β</th>
<th align="center">SE</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Fixed effects</td>
</tr>
<tr>
<td colspan="7"><italic>Course evaluation level</italic></td>
</tr>
<tr>
<td> Online</td>
<td>−.322<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">**</xref></td>
<td>.104</td>
<td>−.303<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.094</td>
<td>−.260<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.080</td>
</tr>
<tr>
<td> Class size</td>
<td>−.002</td>
<td>.002</td>
<td>−.002</td>
<td>.002</td>
<td>−.001</td>
<td>.001</td>
</tr>
<tr>
<td> Response rate</td>
<td>.118</td>
<td>.210</td>
<td>−.179</td>
<td>.204</td>
<td>.086</td>
<td>.143</td>
</tr>
<tr>
<td> Course difficulty</td>
<td>−.296<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.091</td>
<td>−.027</td>
<td>.088</td>
<td>−.234<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.063</td>
</tr>
<tr>
<td colspan="7"><italic>Instructor level</italic></td>
</tr>
<tr>
<td> Gender</td>
<td>.038</td>
<td>.093</td>
<td>.024</td>
<td>.085</td>
<td>.095</td>
<td>.068</td>
</tr>
<tr>
<td> In-class effectiveness</td>
<td>.802<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.126</td>
<td>.770<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.124</td>
<td>.128</td>
<td>.084</td>
</tr>
<tr>
<td colspan="7"><italic>Cross-level interaction</italic></td>
</tr>
<tr>
<td> Online × In-class effectiveness</td>
<td>−.632<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.183</td>
<td>−.624<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.165</td>
<td>−.107</td>
<td>.145</td>
</tr>
<tr>
<td>Constant</td>
<td>4.943<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.342</td>
<td>4.238<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.330</td>
<td>5.49<xref ref-type="table-fn" rid="table-fn4-0092055X13479949">***</xref></td>
<td>.235</td>
</tr>
<tr>
<td colspan="7">Random effects</td>
</tr>
<tr>
<td> Online</td>
<td>.069</td>
<td>.036</td>
<td>.037</td>
<td>.023</td>
<td>.063</td>
<td>.027</td>
</tr>
<tr>
<td> Constant</td>
<td>.000</td>
<td>.000</td>
<td>.000</td>
<td>.000</td>
<td>.000</td>
<td>.000</td>
</tr>
<tr>
<td> Residual</td>
<td>.104</td>
<td>.015</td>
<td>.101</td>
<td>.015</td>
<td>.046</td>
<td>.007</td>
</tr>
<tr>
<td>Sample size</td>
<td colspan="2">118</td>
<td colspan="2">118</td>
<td colspan="2">118</td>
</tr>
<tr>
<td>−2 log likelihood</td>
<td colspan="2">92.54</td>
<td colspan="2">82.24</td>
<td colspan="2">8.95</td>
</tr>
<tr>
<td>Bayesian Information Criterion</td>
<td colspan="2">140.25</td>
<td colspan="2">129.95</td>
<td colspan="2">56.66</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0092055X13479949">
<p>Notes: Regression coefficients are unstandardized. Random effects estimates are variances. SE = standard errors.</p>
</fn>
<fn id="table-fn4-0092055X13479949">
<label>^</label>
<p><italic>p</italic> ≤ .10. *<italic>p</italic> ≤ .05. **<italic>p</italic> ≤ .01. ***<italic>p</italic> ≤ .001 (two-tailed tests).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Moreover, for instructors with an average in-class teaching effectiveness score, students feel that they have learned less in online classes when compared to in-class courses (β = –.303, <italic>p</italic> = .001). We find that in regard to the amount learned in online courses, the instructors with the highest rates of in-class effectiveness experience the most negative decrease in evaluations, while instructors with low rates of in-class effectiveness experience an increase in positive evaluations. These results suggest that the relative effectiveness of online and in-class courses is not uniform across instructors and is contingent on the individual skill sets of instructors.</p>
</sec>
</sec>
<sec id="section12-0092055X13479949" sec-type="discussion">
<title>Discussion</title>
<p>The use of Web-delivered courses in higher education is becoming more common, with almost 30 percent of all college and university students having enrolled in at least one online course (<xref ref-type="bibr" rid="bibr4-0092055X13479949">Allen and Seaman 2010</xref>). Some have even speculated that online courses will revolutionize higher education, with a shrinking market for residential colleges and low-cost online programs offered by elite institutions outcompeting other types of educational experiences (<xref ref-type="bibr" rid="bibr31-0092055X13479949">Perez-Pena 2012</xref>). But what does this shift mean for teaching sociology? Research relying on quasi-experimental methods shows that a well-designed Web-delivered sociology course can be as effective as its more traditional counterpart (e.g., <xref ref-type="bibr" rid="bibr13-0092055X13479949">Driscoll et al. 2012</xref>). This research, however, does not examine the consequences of large-scale implementation of Web-based sociology courses across a department. To our knowledge, no other study has looked at more than 100 courses and 20 instructors in the field of sociology to provide a broad portrait of the effect of online courses for sociology. Thus, our study contributes to debate on an important topic in teaching sociology by examining the consequences of the mass implementation of completely online sociology courses.</p>
<p>Our research indicates that undergraduates rate online sociology classes more negatively than in-class courses. Online courses receive lower overall course ratings, and students state that they learn less in online courses. This is worrisome because research reveals a high correlation between student ratings and actual achievement (<xref ref-type="bibr" rid="bibr2-0092055X13479949">Abrami, d’Apollonia, and Cohen 1990</xref>; <xref ref-type="bibr" rid="bibr11-0092055X13479949">d’Apollonia and Abrami 1997</xref>). Thus, contrary to previous research (e.g., <xref ref-type="bibr" rid="bibr22-0092055X13479949">Kelly et al. 2007</xref>; <xref ref-type="bibr" rid="bibr23-0092055X13479949">Little et al. 2005</xref>), the general implementation of online sociology courses at this university detrimentally affected student perceptions about the quality of education.</p>
<p>Our results also reveal that undergraduates rate online sociology instructors as treating students with less respect. Respect is an important dimension of effective teaching (<xref ref-type="bibr" rid="bibr11-0092055X13479949">d’Apollonia and Abrami 1997</xref>). Students who feel that their teachers respect them tend to like and perform better in school (<xref ref-type="bibr" rid="bibr18-0092055X13479949">Hallinan 2008</xref>). While the data preclude our ability to state with certainty why this is the case, a number of possibilities exist. For instance, by separating students from teachers in space, online classes prevent the face-to-face interactions critical to the student-teacher relationship. The written electronic format may also make it difficult for instructors to deliver negative criticism or feedback in a supportive manner, as instructors are unable to soften criticism with facial or vocal expressions. Similarly, it is possible that electronic forms of correspondence may lead to greater confusion if students misinterpret sarcasm or humor or misunderstand messages. Future research should investigate these possibilities.</p>
<p>Another important contribution of this article is that it identifies a potential factor affecting whether online courses are received positively or negatively by students. It is possible that one of the reasons why previous studies on this topic have mixed findings is that the effectiveness of online courses depends on the instructor teaching the course. It makes sense that the amount of time and energy put into an online course by an instructor, as well as the types of resources utilized by an instructor, will impact the effectiveness of Web-delivered courses. Our research also suggests that the skill sets and traits of instructors could make some individuals better suited to teach online courses. Results show that instructors with above-average levels of in-class effectiveness experience the greatest drop in perceptions regarding overall course ratings and the amount learned in the course. Instructors who excel in methods associated with in-class course presentation, such as public speaking or generating class discussion, may not have been able to recreate effectively these skills in an online environment. Conversely, instructors with the lowest ratings of in-class effectiveness actually experienced a positive boost in course evaluations from the online setting. Thus, the finding that online courses have a negative effect on course evaluations must be conditioned by an understanding that different instructors may be better suited for different forums and methods of teaching.</p>
<p>Before instructors agree to teach online courses, they should think carefully about what their strengths and weaknesses are and whether the shift to the online format will enhance or detract from their strongest instructional abilities. In-class courses reward public speaking skills, an ability to respond quickly and orally to student questions, and a confidence in social settings. Online courses remove the pressures of public speaking and allow for more time to formulate responses to student questions. Additionally, online courses often require more writing, such as written class notes, blogs, or lectures, as well as responses to discussion boards and to students through e-mail. This could create a distinct advantage to instructors with strong and clear writing skills. For these reasons, we caution against concluding that Web courses universally result in more negative student evaluations. For some instructors, they offer a viable alternative.</p>
<p>The aforementioned findings hold even when controlling for factors other than the mode of delivery thought to affect course evaluations. Consideration of these control variables points to several interesting results. For example, courses that students deem more difficult tended to receive lower evaluations, indicating a negative reaction to more challenging courses. The first set of models also suggests that increased class size may have a small negative effect on course evaluations; this could reflect the increased difficulty instructors experience in trying to give quality feedback and individual attention to students in large classes. Lastly, there is some evidence that an increased response rate increases positive course evaluations, perhaps reflecting a trend in which students who like a course are more willing to take the time to fill out a course evaluation form than students who are neutral toward or dislike the course.</p>
<p>Despite the merits of this research, we recognize its limitations. This research was performed at a large public research university, so it is possible that other types of institutions, such as liberal arts colleges or community colleges, will have different experiences with Web-delivered courses. We rely on students’ interpretations of the course evaluation questions, which could vary. Also, we do not have data on the teaching practices of instructors, students’ characteristics, or learning outcomes, all of which would be useful for comparing online and in-class courses and suggest avenues for future research. Moreover, our sample only includes graduate student instructors. Research suggests, however, that teaching evaluations for an instructor do not change significantly over time, indicating that new instructors do not differ dramatically from more advanced instructors (<xref ref-type="bibr" rid="bibr26-0092055X13479949">Marsh 2007</xref>). Thus, while we acknowledge the possibility that the results of this study might differ for faculty members—and indeed could even be more pronounced if faculty members have developed instructional skills better suited for in-class courses than Web-delivered courses—it is also reasonable to expect that the results of this study are applicable across both types of instructors.</p>
</sec>
<sec id="section13-0092055X13479949" sec-type="conclusions">
<title>Conclusion</title>
<p>While some have argued that undergraduate students prefer the self-paced style of Internet courses and the ability to use novel forms of technology, our study indicates that at least in terms of evaluations of the course overall and the amount learned in the course, online sociology classes are being rated by undergraduates as less effective when compared to in-class courses. Additionally, this negative evaluation extends to ratings of the instructor, with undergraduates rating online instructors as being less effective teachers and as treating them with less respect than do their in-class counterparts. Our study, then, cautions against the broad use of online sociology classes as a strategy for coping with increasing student enrollment or budget constraints. Web-based courses do not have to come at the expense of student evaluations, but this requires technological infrastructure as well as motivated instructors who use Web-based courses to provide unique learning opportunities unavailable in more traditional course settings (<xref ref-type="bibr" rid="bibr23-0092055X13479949">Little et al. 2005</xref>). Furthermore, as our findings suggest, some instructors may even benefit from a move to online courses. Consequently, it is likely that online courses are not universally “superior” or “inferior” to traditional courses but depend on the training and resources provided by institutions, as well as on the decisions and teaching strategies of instructors. Thus, administrators and instructors at institutions of higher education should continue to strive to ensure that undergraduate students enrolled in online courses receive the same quality of education as their peers in traditional classroom environments.</p>
</sec>
</body>
<back>
<ack>
<p>We would like to thank August Woerner for his assistance with data collection.</p>
</ack>
<notes>
<fn-group>
<fn fn-type="other">
<p>Reviewers for this manuscript were, in alphabetical order, Melinda Messineo and Daphne Pedersen.</p>
<p>Contributions of the authors are equal; authors are listed in alphabetical order.</p>
</fn>
<fn fn-type="other" id="fn1-0092055X13479949">
<label>1.</label>
<p>The caveat here is that online classes require access to a computer and the Internet, which may be a barrier for students of low socioeconomic status.</p>
</fn>
<fn fn-type="other" id="fn2-0092055X13479949">
<label>2.</label>
<p>Two cases had incorrect data on one of the variables and were dropped from the analysis, resulting in the total number of observations changing from 120 to 118. We remove from the sample required theory, methods, and statistics courses because they are not taught online. We also limit the sample to instructors who have taught both in-class and online.</p>
</fn>
<fn fn-type="other" id="fn3-0092055X13479949">
<label>3.</label>
<p>Sensitivity analyses reveal that on average students were only slightly more likely to view upper-level courses as more difficult than introductory-level courses (3.4 to 3.2), suggesting that variations in student perceptions of course difficulty have more to do with the instructor than with course level.</p>
</fn>
<fn fn-type="other" id="fn4-0092055X13479949">
<label>4.</label>
<p>Initial models included instructors’ race, but inclusion of this variable did not improve model fit nor was it significant. It is also possible that instructors are themselves also grouped into course topical areas; however, our analyses did not show evidence that a third level of clustering by course type was having a significant effect on the analysis.</p>
</fn>
<fn fn-type="other" id="fn5-0092055X13479949">
<label>5.</label>
<p>For these calculations, we entered 0 or 1 for online, and for in-class effectiveness, which was mean centered, the minimum of our data to represent a low score (–1.77), the average (0), and the maximum to represent a high score (0.53). We then entered these values into an equation that retained the coefficients of relevant variables pertaining to online format, in-class effectiveness, and the interaction term, as well as any other relevant significant variables (for these we entered the variable’s mean) and the constant. For both overall course rating and amount learned, the highest in-class effectiveness instructors’ scores dropped ≈ –0.65, the average in-class effectiveness instructors’ scores dropped ≈ –0.30, and the lowest in-class effectiveness instructors’ scores increased 0.8 when changing from an in-class to online format.</p>
</fn>
</fn-group>
</notes>
<bio>
<title>Author Biographies</title>
<p><bold>Kelly Bergstrand</bold> is a PhD candidate at the University of Arizona and specializes in social movements, environmental sociology, and social psychology. Her current research projects include examining the role of grievances in mobilization, investigating activism on the U.S.-Mexico border, and mapping the community impact of environmental disasters.</p>
<p><bold>Scott V. Savage</bold> is an assistant professor at the University of California, Riverside. He is a social psychologist and organizational theorist interested in issues of trust, commitment, and influence. His current research investigates job retention and occupational mobility as well as extensions of status characteristics theory.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Abbott</surname><given-names>Andrew</given-names></name>
</person-group>. <year>1988</year>. <source>The System of Professions: An Essay on the Division of Expert Labor</source>. <publisher-loc>Chicago</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abrami</surname><given-names>Philip C.</given-names></name>
<name><surname>d’Apollonia</surname><given-names>Sylvia</given-names></name>
<name><surname>Cohen</surname><given-names>Peter A.</given-names></name>
</person-group> <year>1990</year>. “<article-title>Validity of Student Ratings of Instruction: What We Know and What We Do Not.</article-title>” <source>Journal of Educational Psychology</source> <volume>82</volume>(<issue>2</issue>):<fpage>219</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr3-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Aleamoni</surname><given-names>Lawrence M.</given-names></name>
</person-group> <year>1981</year>. “<article-title>Student Ratings of Instruction.</article-title>” Pp. <fpage>110</fpage>–<lpage>45</lpage> in <source>Handbook of Teacher Evaluation</source>, edited by <person-group person-group-type="editor">
<name><surname>Millman</surname><given-names>J.</given-names></name>
</person-group> <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr4-0092055X13479949">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Allen</surname><given-names>I. Elaine</given-names></name>
<name><surname>Seaman</surname><given-names>Jeff</given-names></name>
</person-group>. <year>2010</year>. <source>Class Differences: Online Education in the United States, 2010</source>. <access-date>Retrieved July 25, 2012</access-date> (<ext-link ext-link-type="uri" xlink:href="http://sloanconsortium.org/publications/survey/class_differences">http://sloanconsortium.org/publications/survey/class_differences</ext-link>).</citation>
</ref>
<ref id="bibr5-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bernard</surname><given-names>Robert M.</given-names></name>
<name><surname>Abrami</surname><given-names>Philip C.</given-names></name>
<name><surname>Lou</surname><given-names>Yiping</given-names></name>
<name><surname>Borokhovski</surname><given-names>Evgueni</given-names></name>
<name><surname>Wade</surname><given-names>Anne</given-names></name>
<name><surname>Wozney</surname><given-names>Lori</given-names></name>
<name><surname>Wallet</surname><given-names>Peter Andrew</given-names></name>
<name><surname>Fiset</surname><given-names>Manon</given-names></name>
<name><surname>Huang</surname><given-names>Binru</given-names></name>
</person-group>. <year>2004</year>. “<article-title>How Does Distance Education Compare with Classroom Instruction? A Meta-analysis of the Empirical Literature.</article-title>” <source>Review of Educational Research</source> <volume>74</volume>(<issue>3</issue>):<fpage>379</fpage>–<lpage>439</lpage>.</citation>
</ref>
<ref id="bibr6-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bloom</surname><given-names>Kristine C.</given-names></name>
<name><surname>Shuell</surname><given-names>Thomas J.</given-names></name>
</person-group> <year>1981</year>. “<article-title>Effects of Massed and Distributed Practice on the Learning and Retention of Second-language Vocabulary.</article-title>” <source>Journal of Educational Research</source> <volume>74</volume>(<issue>4</issue>):<fpage>245</fpage>–<lpage>48</lpage>.</citation>
</ref>
<ref id="bibr7-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carle</surname><given-names>Adam C.</given-names></name>
</person-group> <year>2009</year>. “<article-title>Evaluating College Students’ Evaluations of a Professor’s Teaching Effectiveness across Time and Instruction Mode (Online vs. Face-to-face) Using a Multilevel Growth Modeling Approach.</article-title>” <source>Computers and Education</source> <volume>53</volume>(<issue>2</issue>):<fpage>429</fpage>–<lpage>35</lpage>.</citation>
</ref>
<ref id="bibr8-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Centra</surname><given-names>John A.</given-names></name>
</person-group> <year>2003</year>. “<article-title>Will Teachers Receive Higher Student Evaluations by Giving Higher Grades and Less Course Work?</article-title>” <source>Research in Higher Education</source> <volume>44</volume>(<issue>5</issue>):<fpage>495</fpage>–<lpage>518</lpage>.</citation>
</ref>
<ref id="bibr9-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clark-Ibanez</surname><given-names>Marisol</given-names></name>
<name><surname>Scott</surname><given-names>Linda</given-names></name>
</person-group>. <year>2008</year>. “<article-title>Learning to Teach Online.</article-title>” <source>Teaching Sociology</source> <volume>36</volume>(<issue>1</issue>):<fpage>34</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr10-0092055X13479949">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Coughan</surname><given-names>Sean</given-names></name>
</person-group>. <year>2012</year>. “<article-title>MIT Launches Free Online ‘Fully Automated’ Course.</article-title>” <source>BBC News</source>. <access-date>Retrieved March 5, 2012</access-date> (<ext-link ext-link-type="uri" xlink:href="http://www.bbc.co.uk/news/education-17012968">http://www.bbc.co.uk/news/education-17012968</ext-link>).</citation>
</ref>
<ref id="bibr11-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>d’Apollonia</surname><given-names>Sylvia</given-names></name>
<name><surname>Abrami</surname><given-names>Philip C.</given-names></name>
</person-group> <year>1997</year>. “<article-title>Navigating Student Ratings of Instruction.</article-title>” <source>American Psychologist</source> <volume>52</volume>(<issue>11</issue>):<fpage>1198</fpage>–<lpage>208</lpage>.</citation>
</ref>
<ref id="bibr12-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Delucchi</surname><given-names>Michael</given-names></name>
</person-group>. <year>2000</year>. “<article-title>Don’t Worry, Be Happy: Instructor Likability, Student Perceptions of Learning, and Teacher Ratings in Upper-level Sociology Courses.</article-title>” <source>Teaching Sociology</source> <volume>28</volume>(<issue>3</issue>):<fpage>220</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr13-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Driscoll</surname><given-names>Adam</given-names></name>
<name><surname>Jicha</surname><given-names>Karla</given-names></name>
<name><surname>Hunt</surname><given-names>Andrea N.</given-names></name>
<name><surname>Tichavsky</surname><given-names>Lisa</given-names></name>
<name><surname>Thompson</surname><given-names>Gretchen</given-names></name>
</person-group>. <year>2012</year>. “<article-title>Can Online Courses Deliver In-class Results? A Comparison of Student Performance and Satisfaction in an Online Versus a Face-to-face Introductory Sociology Course.</article-title>” <source>Teaching Sociology</source> <volume>40</volume>(<issue>4</issue>):<fpage>312</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr14-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ewing</surname><given-names>Andrew M.</given-names></name>
</person-group> <year>2012</year>. “<article-title>Estimating the Impact of Relative Expected Grade on Student Evaluations of Teachers.</article-title>” <source>Economics of Education Review</source> <volume>31</volume>(<issue>1</issue>):<fpage>141</fpage>–<lpage>54</lpage>.</citation>
</ref>
<ref id="bibr15-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Feldman</surname><given-names>Kenneth</given-names></name>
</person-group>. <year>1984</year>. “<article-title>Class Size and College Students’ Evaluations of Teachers and Courses: A Closer Look.</article-title>” <source>Research in Higher Education</source> <volume>21</volume>(<issue>1</issue>):<fpage>45</fpage>–<lpage>116</lpage>.</citation>
</ref>
<ref id="bibr16-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Feldman</surname><given-names>Kenneth A.</given-names></name>
</person-group> <year>1989</year>. “<article-title>The Association between Student Ratings of Specific Instructional Dimensions and Student Achievement: Refining and Extending the Synthesis of Data from Multisection Validity Studies.</article-title>” <source>Research in Higher Education</source> <volume>30</volume>(<issue>6</issue>):<fpage>583</fpage>–<lpage>645</lpage>.</citation>
</ref>
<ref id="bibr17-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Finkelstein</surname><given-names>Martin J.</given-names></name>
<name><surname>Scholz</surname><given-names>Bernhard W.</given-names></name>
</person-group> <year>2000</year>. “<article-title>What Do We Know about Information Technology and the Cost of Collegiate Teaching and Learning?</article-title>” Pp. <fpage>3</fpage>–<lpage>34</lpage> in <source>Dollars, Distance, and Online Education: The New Economics of College Teaching and Learning</source>, edited by <person-group person-group-type="editor">
<name><surname>Finkelstein</surname><given-names>M. J.</given-names></name>
<name><surname>Frances</surname><given-names>C.</given-names></name>
<name><surname>Jewett</surname><given-names>F. I.</given-names></name>
<name><surname>Scholz</surname><given-names>B. W.</given-names></name>
</person-group> <publisher-loc>Phoenix, AZ</publisher-loc>: <publisher-name>The Oryx Press</publisher-name>.</citation>
</ref>
<ref id="bibr18-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hallinan</surname><given-names>Maureen T.</given-names></name>
</person-group> <year>2008</year>. “<article-title>Teacher Influences on Students’ Attachment to School.</article-title>” <source>Sociology of Education</source> <volume>81</volume>(<issue>3</issue>):<fpage>271</fpage>–<lpage>83</lpage>.</citation>
</ref>
<ref id="bibr19-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Howard</surname><given-names>George S.</given-names></name>
<name><surname>Conway</surname><given-names>Christine G.</given-names></name>
<name><surname>Maxwell</surname><given-names>Scott E.</given-names></name>
</person-group> <year>1985</year>. “<article-title>Construct Validity of Measures of College Teaching Effectiveness.</article-title>” <source>Journal of Educational Psychology</source> <volume>77</volume>(<issue>2</issue>):<fpage>187</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr20-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jaffee</surname><given-names>David</given-names></name>
</person-group>. <year>1997</year>. “<article-title>Asynchronous Learning: Technology and Pedagogical Strategy in a Distance Learning Course.</article-title>” <source>Teaching Sociology</source> <volume>25</volume>(<issue>4</issue>):<fpage>262</fpage>–<lpage>77</lpage>.</citation>
</ref>
<ref id="bibr21-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>Valen E.</given-names></name>
</person-group> <year>2003</year>. <source>Grade Inflation: A Crisis in College Education</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr22-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kelly</surname><given-names>Henry F.</given-names></name>
<name><surname>Ponton</surname><given-names>Michael</given-names></name>
<name><surname>Rovai</surname><given-names>Alfred P.</given-names></name>
</person-group> <year>2007</year>. “<article-title>A Comparison of Student Evaluations of Teaching between Online and Face to Face Courses.</article-title>” <source>Internet and Higher Education</source> <volume>10</volume>(<issue>2</issue>):<fpage>89</fpage>–<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr23-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Little</surname><given-names>Craig B.</given-names></name>
<name><surname>Titarenko</surname><given-names>Larissa</given-names></name>
<name><surname>Bergelson</surname><given-names>Mira</given-names></name>
</person-group>. <year>2005</year>. “<article-title>Creating a Successful International Distance-learning Classroom.</article-title>” <source>Teaching Sociology</source> <volume>33</volume>(<issue>4</issue>):<fpage>355</fpage>–<lpage>70</lpage>.</citation>
</ref>
<ref id="bibr24-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Logan</surname><given-names>Elisabeth</given-names></name>
<name><surname>Augustyniak</surname><given-names>Rebecca</given-names></name>
<name><surname>Rees</surname><given-names>Alison</given-names></name>
</person-group>. <year>2002</year>. “<article-title>Distance Education as Different Education: A Student-centered Investigation of Distance Learning Experience.</article-title>” <source>Journal of Education for Library and Information Science</source> <volume>43</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr25-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marsh</surname><given-names>Herbert W.</given-names></name>
</person-group> <year>1987</year>. “<article-title>Students’ Evaluations of University Teaching: Research Findings, Methodological Issues, and Directions for Future Research.</article-title>” <source>International Journal of Educational Research</source> <volume>11</volume>(<issue>3</issue>):<fpage>253</fpage>–<lpage>388</lpage>.</citation>
</ref>
<ref id="bibr26-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marsh</surname><given-names>Herbert W.</given-names></name>
</person-group> <year>2007</year>. “<article-title>Do University Teachers Become More Effective with Experience? A Multilevel Growth Model of Students’ Evaluations of Teaching over 13 Years.</article-title>” <source>Journal of Educational Psychology</source> <volume>99</volume>:<fpage>775</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr27-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Marsh</surname><given-names>Herbert W.</given-names></name>
<name><surname>Roche</surname><given-names>Lawrence A.</given-names></name>
</person-group> <year>1997</year>. “<article-title>Making Students’ Evaluations of Teaching Effectiveness Effective.</article-title>” <source>American Psychologist</source> <volume>52</volume>(<issue>11</issue>):<fpage>1187</fpage>–<lpage>97</lpage>.</citation>
</ref>
<ref id="bibr28-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Means</surname><given-names>Barbara</given-names></name>
<name><surname>Toyama</surname><given-names>Yukie</given-names></name>
<name><surname>Murphy</surname><given-names>Robert</given-names></name>
<name><surname>Bakia</surname><given-names>Marianne</given-names></name>
<name><surname>Jones</surname><given-names>Karla</given-names></name>
</person-group>. <year>2009</year>. <source>Evaluation of Evidence-based Practices in Online Learning: A Meta-analysis and Review of Online Learning Studies</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Office of Planning, Evaluation, and Policy Development, U.S. Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr29-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Murray</surname><given-names>Harry G.</given-names></name>
</person-group> <year>1984</year>. “<article-title>The Impact of Formative and Summative Evaluation of Teaching in North American Universities.</article-title>” <source>Assessment and Evaluation in Higher Education</source> <volume>9</volume>(<issue>2</issue>):<fpage>117</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr30-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Onwuegbuzie</surname><given-names>Anthony J.</given-names></name>
<name><surname>Witcher</surname><given-names>Ann E.</given-names></name>
<name><surname>Collins</surname><given-names>Kathleen M. T.</given-names></name>
<name><surname>Filer</surname><given-names>Janet D.</given-names></name>
<name><surname>Wiedmaier</surname><given-names>Cheryl D.</given-names></name>
<name><surname>Moore</surname><given-names>Chris W.</given-names></name>
</person-group> <year>2007</year>. “<article-title>Students’ Perceptions of Characteristics of Effective College Teachers: A Validity Study of a Teaching Evaluation Form Using a Mixed-methods Analysis.</article-title>” <source>American Educational Research Journal</source> <volume>44</volume>(<issue>1</issue>):<fpage>113</fpage>–<lpage>60</lpage>.</citation>
</ref>
<ref id="bibr31-0092055X13479949">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Perez-Pena</surname><given-names>Richard</given-names></name>
</person-group>. <year>2012</year>. “<article-title>Top Universities Test the Online Appeal of Free.</article-title>” <source>The New York Times</source>, <month>July</month> <day>17</day>. <access-date>Retrieved August 10, 2012</access-date> (<ext-link ext-link-type="uri" xlink:href="http://www.nytimes.com/2012/07/18/education/top-universities-test-the-online-appeal-of-free.html">http://www.nytimes.com/2012/07/18/education/top-universities-test-the-online-appeal-of-free.html</ext-link>).</citation>
</ref>
<ref id="bibr32-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Raudenbush</surname><given-names>Stephen W.</given-names></name>
<name><surname>Bryk</surname><given-names>Anthony S.</given-names></name>
</person-group> <year>2002</year>. <source>Hierarchical Linear Models: Applications and Data Analysis Methods</source>. <edition>2nd ed.</edition> <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr33-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Roberts</surname><given-names>Lance W.</given-names></name>
<name><surname>Clifton</surname><given-names>Rodney A.</given-names></name>
</person-group> <year>1992</year>. “<article-title>Measuring the Affective Quality of Life of University Students: The Validation of an Instrument.</article-title>” <source>Social Indicators Research</source> <volume>27</volume>(<issue>2</issue>):<fpage>113</fpage>–<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr34-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schulte</surname><given-names>Aileen</given-names></name>
</person-group>. <year>2004</year>. “<article-title>The Development of an Asynchronous Computer-mediated Course: Observations on How to Promote Interactivity.</article-title>” <source>College Teaching</source> <volume>52</volume>(<issue>1</issue>):<fpage>6</fpage>–<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr35-0092055X13479949">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Seldin</surname><given-names>Peter</given-names></name>
</person-group>. <year>1993</year>. “<article-title>When Students Rate Professors.</article-title>” <source>The Chronicle of Higher Education</source>, <month>July</month> <day>21</day>. <access-date>Retrieved August 2, 2012</access-date> (<ext-link ext-link-type="uri" xlink:href="http://chronicle.com/article/When-Students-Rate-Professors/73120/">http://chronicle.com/article/When-Students-Rate-Professors/73120/</ext-link>).</citation>
</ref>
<ref id="bibr36-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shank</surname><given-names>Matthew D.</given-names></name>
<name><surname>Walker</surname><given-names>Mary</given-names></name>
<name><surname>Hayes</surname><given-names>Thomas</given-names></name>
</person-group>. <year>1996</year>. “<article-title>Understanding Professional Service Expectations: Do We Know What Our Students Expect in a Quality Education?</article-title>” <source>Journal of Professional Services Marketing</source> <volume>13</volume>(<issue>1</issue>):<fpage>71</fpage>–<lpage>89</lpage>.</citation>
</ref>
<ref id="bibr37-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shapiro</surname><given-names>Gary</given-names></name>
</person-group>. <year>1990</year>. “<article-title>Effect of Instructor and Class Characteristics on Students’ Class Evaluations.</article-title>” <source>Research in Higher Education</source> <volume>31</volume>(<issue>2</issue>):<fpage>135</fpage>–<lpage>48</lpage>.</citation>
</ref>
<ref id="bibr38-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Simpson</surname><given-names>Penny</given-names></name>
<name><surname>Siguaw</surname><given-names>Judy</given-names></name>
</person-group>. <year>2000</year>. “<article-title>Student Evaluations of Teaching: An Exploratory Study of the Faculty Response.</article-title>” <source>Journal of Marketing Education</source> <volume>22</volume>(<issue>3</issue>):<fpage>199</fpage>–<lpage>213</lpage>.</citation>
</ref>
<ref id="bibr39-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sitzmann</surname><given-names>Traci</given-names></name>
<name><surname>Kraiger</surname><given-names>Kurt</given-names></name>
<name><surname>Steward</surname><given-names>David</given-names></name>
<name><surname>Wisher</surname><given-names>Robert</given-names></name>
</person-group>. <year>2006</year>. “<article-title>The Comparative Effectiveness of Web-based and Classroom Instruction: A Meta-analysis.</article-title>” <source>Personnel Psychology</source> <volume>59</volume>(<issue>3</issue>):<fpage>623</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr40-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Snijders</surname><given-names>Tom A. B.</given-names></name>
</person-group> <year>2005</year>. “<article-title>Fixed and Random Effects.</article-title>” Pp. <fpage>664</fpage>–<lpage>65</lpage> in <source>Encyclopedia of Statistics in Behavioral Science</source>, edited by <person-group person-group-type="editor">
<name><surname>Everitt</surname><given-names>B. S.</given-names></name>
<name><surname>Howell</surname><given-names>D. C.</given-names></name>
</person-group> <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr41-0092055X13479949">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Snijders</surname><given-names>Tom A. B.</given-names></name>
<name><surname>Bosker</surname><given-names>Roel J.</given-names></name>
</person-group> <year>1999</year>. <source>Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr42-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stowell</surname><given-names>Jeffrey R.</given-names></name>
<name><surname>Addison</surname><given-names>William E.</given-names></name>
<name><surname>Smith</surname><given-names>Jennifer L.</given-names></name>
</person-group> <year>2012</year>. “<article-title>Comparison of Online and Classroom-based Student Evaluations of Instruction.</article-title>” <source>Assessment and Evaluation in Higher Education</source> <volume>37</volume>(<issue>4</issue>):<fpage>465</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr43-0092055X13479949">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Tucker</surname><given-names>Sheila</given-names></name>
</person-group>. <year>2001</year>. “<article-title>Distance Education: Better, Worse, or as Good as Traditional Education?</article-title>” <source>Journal of Distance Learning Administration</source> <volume>4</volume>(<issue>4</issue>). <access-date>Retrieved July 15, 2012</access-date> (<ext-link ext-link-type="uri" xlink:href="http://www.westga.edu/~distance/ojdla/winter44/tucker44.html">http://www.westga.edu/~distance/ojdla/winter44/tucker44.html</ext-link>).</citation>
</ref>
<ref id="bibr44-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Urtel</surname><given-names>Mark G.</given-names></name>
</person-group> <year>2008</year>. “<article-title>Assessing Academic Performance between Traditional and Distance Education Course Formats.</article-title>” <source>Technology and Society</source> <volume>11</volume>(<issue>1</issue>):<fpage>322</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr45-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van Gundy</surname><given-names>Karen</given-names></name>
<name><surname>Morton</surname><given-names>Beth A.</given-names></name>
<name><surname>Liu</surname><given-names>Hope Q.</given-names></name>
<name><surname>Kline</surname><given-names>Jennifer</given-names></name>
</person-group>. <year>2006</year>. “<article-title>Effects of Web Based Instruction on Math Anxiety, the Sense of Mastery, and Global Self-esteem: A Quasi-experimental Study of Undergraduate Statistics Students.</article-title>” <source>Teaching Sociology</source> <volume>34</volume>(<issue>4</issue>):<fpage>370</fpage>–<lpage>88</lpage>.</citation>
</ref>
<ref id="bibr46-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>van Schaik</surname><given-names>Paul</given-names></name>
<name><surname>Barker</surname><given-names>Philip</given-names></name>
<name><surname>Beckstrand</surname><given-names>Scott</given-names></name>
</person-group>. <year>2003</year>. “<article-title>A Comparison of On-campus and Online Course Delivery Methods in Southern Nevada.</article-title>” <source>Innovations in Education and Teaching International</source> <volume>40</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr47-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Waschull</surname><given-names>Stefanie</given-names></name>
</person-group>. <year>2001</year>. “<article-title>The Online Delivery of Psychology Courses: Attrition, Performance, and Evaluation.</article-title>” <source>Teaching of Psychology</source> <volume>28</volume>(<issue>2</issue>):<fpage>143</fpage>–<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr48-0092055X13479949">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wachtel</surname><given-names>Howard K.</given-names></name>
</person-group> <year>1998</year>. “<article-title>Student Evaluation of College Teaching Effectiveness: A Brief Review.</article-title>” <source>Assessment and Evaluation in Higher Education</source> <volume>23</volume>(<issue>2</issue>):<fpage>191</fpage>–<lpage>212</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>