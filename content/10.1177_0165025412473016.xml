<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JBD</journal-id>
<journal-id journal-id-type="hwp">spjbd</journal-id>
<journal-title>International Journal of Behavioral Development</journal-title>
<issn pub-type="ppub">0165-0254</issn>
<issn pub-type="epub">1464-0651</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0165025412473016</article-id>
<article-id pub-id-type="publisher-id">10.1177_0165025412473016</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Face-scanning behavior to silently-talking faces in 12-month-old infants: The impact of pre-exposed auditory speech</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Kubicek</surname>
<given-names>Claudia</given-names>
</name>
<xref ref-type="aff" rid="aff1-0165025412473016">1</xref>
<xref ref-type="corresp" rid="corresp1-0165025412473016"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>de Boisferon</surname>
<given-names>Anne Hillairet</given-names>
</name>
<xref ref-type="aff" rid="aff2-0165025412473016">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dupierrix</surname>
<given-names>Eve</given-names>
</name>
<xref ref-type="aff" rid="aff2-0165025412473016">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lœvenbruck</surname>
<given-names>Hélène</given-names>
</name>
<xref ref-type="aff" rid="aff2-0165025412473016">2</xref>
<xref ref-type="aff" rid="aff3-0165025412473016">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gervain</surname>
<given-names>Judit</given-names>
</name>
<xref ref-type="aff" rid="aff4-0165025412473016">4</xref>
<xref ref-type="aff" rid="aff5-0165025412473016">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schwarzer</surname>
<given-names>Gudrun</given-names>
</name>
<xref ref-type="aff" rid="aff1-0165025412473016">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0165025412473016">
<label>1</label>Justus-Liebig-University Giessen, Germany</aff>
<aff id="aff2-0165025412473016">
<label>2</label>Université Pierre Mendès, France</aff>
<aff id="aff3-0165025412473016">
<label>3</label>GIPSA-lab, UMR CNRS, France</aff>
<aff id="aff4-0165025412473016">
<label>4</label>Université Paris Descartes, France</aff>
<aff id="aff5-0165025412473016">
<label>5</label>CNRS, France</aff>
<author-notes>
<corresp id="corresp1-0165025412473016">Claudia Kubicek, Abteilung für Entwicklungspsychologie, Justus-Liebig-Universität Giessen, Otto-Behaghel-Str. 10 F1, 35394 Giessen, Germany. Email: <email>claudia.kubicek@psychol.uni-giessen.de</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>37</volume>
<issue>2</issue>
<issue-title>Special issue: Development of Face Processing: New Evidence on Multi-Modal Contributions, Scanning, and Recognition</issue-title>
<fpage>106</fpage>
<lpage>110</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">International Society for the Study of Behavioural Development</copyright-holder>
</permissions>
<abstract>
<p>The present eye-tracking study aimed to investigate the impact of auditory speech information on 12-month-olds’ gaze behavior to silently-talking faces. We examined German infants’ face-scanning behavior to side-by-side presentation of a bilingual speaker’s face silently speaking German utterances on one side and French on the other side, before and after auditory familiarization with one of the two languages. The results showed that 12-month-old infants showed no general visual preference for either of the visual speeches, neither before nor after auditory input. But, infants who heard native speech decreased their looking time to the mouth area and focused longer on the eyes compared to their scanning behavior without auditory language input, whereas infants who heard non-native speech increased their visual attention on the mouth region and focused less on the eyes. Thus, it can be assumed that 12-month-olds quickly identified their native language based on auditory speech and guided their visual attention more to the eye region than infants who have listened to non-native speech.</p>
</abstract>
<kwd-group>
<kwd>audio-visual speech perception</kwd>
<kwd>face scanning</kwd>
<kwd>infants</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>From birth onwards, infants encounter a socially-rich environment, hearing and seeing talking faces on a daily basis, especially when communicating with their caregivers. Such early face-to-face communication should lead to a close binding between face and speech. It provides infants with redundant or complementary auditory-visual speech information, which could enhance their perception, understanding and learning of spoken language.</p>
<p>Although there is a substantial body of literature on infants’ auditory processing of speech (e.g., <xref ref-type="bibr" rid="bibr14-0165025412473016">Werker &amp; Tees, 1999</xref>), far fewer researchers have focused on the visual information that infants extract from talking faces. Weikum and colleagues (2007) addressed the question whether visual speech cues alone are sufficient for infants’ language discrimination. The authors used silent video clips of English-French bilingual speakers who were reciting different sentences in either French or English in a habituation paradigm. Monolingual 4- and 6-month-old infants were able to visually discriminate between the two languages, whereas infants at 8 months of age were not. The decline in the ability to visually discriminate their native language from an unfamiliar language observed in 8-month-olds is concordant with the perceptual narrowing view (<xref ref-type="bibr" rid="bibr10-0165025412473016">Scott, Pascalis, &amp; Nelson, 2007</xref>), that is, a tendency for infants to still develop perceptual abilities for native attributes, while declining for non-native attributes that infants have little experience with. This view has been described, for example, in studies of infants’ phonetic development (<xref ref-type="bibr" rid="bibr4-0165025412473016">Kuhl, Tsao, &amp; Liu, 2003</xref>; <xref ref-type="bibr" rid="bibr13-0165025412473016">Werker &amp; Tees, 1984</xref>), face discrimination (<xref ref-type="bibr" rid="bibr3-0165025412473016">Kelly et al., 2007</xref>; <xref ref-type="bibr" rid="bibr8-0165025412473016">Pascalis, de Haan, &amp; Nelson, 2002</xref>), and scanning of own- and other-race faces (<xref ref-type="bibr" rid="bibr7-0165025412473016">Liu et al., 2011</xref>; <xref ref-type="bibr" rid="bibr15-0165025412473016">Wheeler et al., 2011</xref>).</p>
<p>A recent study by <xref ref-type="bibr" rid="bibr9-0165025412473016">Pons, Lewkowicz, Soto-Faraco and Sebastián-Gallés (2009</xref>) investigated infants’ intersensory speech perception. The authors compared 6- and 11-month-old English and Spanish infants’ ability to orient toward silent videos of a Spanish woman pronouncing the syllables “ba” or “va” before and after auditory-only familiarization with one of the two syllables. The /ba/ vs. /va/ phonological contrast is known to be perceived by adult English speakers but not by Spanish ones. As expected, no visual preference for either of the silent videos was observed prior to auditory familiarization for any of the four groups of children. After the auditory-only familiarization with either the /ba/ or the /va/ syllable, 6-month-old English and Spanish infants looked longer at the matching visible syllables, but at 11 months of age only the English infants did so. The fact that 6-month-olds looked at the corresponding visual stimulus more than at the non-corresponding stimulus suggests that these infants have perceived the relationship between the auditory and the visual stimulus, and thus that they performed intersensory matching. The fact that older infants did not match audible and visible attributes of non-native speech is interpreted by Pons and colleagues as suggesting that infants’ sensitivity to intersensory speech narrows during the first year of life and that perceptual narrowing is a domain general developmental process.</p>
<p>By tracking eye movements, it was demonstrated that the way in which infants process audible-talking faces in their native language changes over the course of the first year: <xref ref-type="bibr" rid="bibr6-0165025412473016">Lewkowicz and Hansen-Tift (2012</xref>) investigated infants’ visual attention to facial regions in 4- to 12-month-old English-learners while they watched and listened to a female speaking their native (English) or a non-native (Spanish) language. The authors were particularly interested in potential differences regarding the face-scanning behavior at the end of the first year. Independently of language familiarity, 4-month-olds looked longer at the eye region, 6-month-olds gazed equally long at the eyes and the mouth, and 8-month-old infants looked longer at the mouth area. Interestingly, 12-month-olds gazed equally long at the eyes and the mouth when native speech was spoken, whereas they maintained their looking toward the mouth area for the non-native language. The authors concluded that perceptual narrowing and growing native language expertise explained this behavior. Whereas the processing of non-native speech required infants to access complementary audio-visual cues from the mouth region, the processing of native speech was less depending on such cues.</p>
<p>To sum up, on the one hand, monolingual 8-month-old infants seem to no longer be able to visually discriminate their native language from an unfamiliar language, an ability infants have at 4 and 6 months of age (<xref ref-type="bibr" rid="bibr12-0165025412473016">Weikum et al., 2007</xref>). On the other hand, when auditory speech information is presented, 12-month-old infants show a differential scanning pattern depending on whether the faces talked in their native or non-native language (<xref ref-type="bibr" rid="bibr6-0165025412473016">Lewkowicz &amp; Hansen-Tift, 2012</xref>), and 11-month-olds seem to remain sensitive to visual cues to native speech contrasts after audible-only speech input (<xref ref-type="bibr" rid="bibr9-0165025412473016">Pons et al., 2009</xref>). These findings lead to the general question of the extent to which auditory speech has an impact on the visual processing of silently-talking faces at the end of the first year.</p>
<p>To address this question, we used a variant of the intersensory matching procedure (<xref ref-type="bibr" rid="bibr9-0165025412473016">Pons et al., 2009</xref>) and compared 12-month-old German infants’ gaze behavior to facial regions of faces silently-speaking German (native) and French (non-native) fluent speech before (baseline trials) and after (post-test trials) auditory-only familiarization with one of the two languages. In accordance with the study of Weikum and colleagues (2007), who did not observe any visual language discrimination abilities in 8-month-olds, 12-month-old infants also should not show a visual preference for either of the languages. Moreover, we predicted no difference in scanning patterns between the German- or the French-speaking face in the baseline trials. By contrast, in the post-test trials, corresponding with the results of <xref ref-type="bibr" rid="bibr6-0165025412473016">Lewkowicz and Hansen-Tift (2012</xref>), we expected infants’ looking times to the <italic>mouth </italic>region to increase when French auditory language is presented, in comparison to baseline, and infants’ looking times to the <italic>eyes</italic> to increase when German auditory language is presented.</p>
<sec id="section1-0165025412473016" sec-type="methods">
<title>Method</title>
<sec id="section2-0165025412473016">
<title>Participants</title>
<p>Forty monolingual German-learning 12-month-old infants (<italic>M</italic> = 377 days, <italic>SD</italic> = 8 days; age range: 365 days – 394 days, 21 males) were tested. All infants were full-term with no visual or auditory deficits, as reported by parents. The data from seven additional infants were discarded from the final sample due to equipment failure (two), failure to complete the calibration procedure (four) or extreme fussiness (one).</p>
</sec>
<sec id="section3-0165025412473016">
<title>Stimuli</title>
<p>Visual stimuli were silent video clips of four female bilingual German-French speakers. Recording took place in Germany (two speakers) and in France (two speakers). The speakers were recorded against a blue background looking directly into the camera while they were reciting a German and a French version of a children’s short story in a child-directed manner. Each of the 30-second video clips showed a full-face image of the speaker and measured 20.6 cm × 18 cm when displayed side-by-side on the monitor, separated by an 11-cm gap. Both the French and German videos were edited to make sure that they started on a closed mouth and that the first mouth-opening was synchronized.</p>
<p>Auditory stimuli consisted of the same story and were presented at conversational sound pressure level (65dB ± 5dB).</p>
</sec>
<sec id="section4-0165025412473016">
<title>Apparatus and procedure</title>
<p>Caregivers were informed of the purpose of the study and gave written consent for their infant to participate. Each infant was tested individually in a baby lab, the caregiver sitting on a chair with the infant on her lap. To prevent parents from influencing the looking behavior of their infants, they were told to keep their eyes closed and to refrain from talking for the duration of the experiment. The infants were seated on the caregiver’s lap at a distance of 60 cm in front of a 22-inch monitor (resolution: 1280 × 1024 pixels). Stimuli were presented by using E-Prime 2.0 software (Psychology Software Tools, Sharpsburg, PA).</p>
<p>Eye-tracking data were captured by a Tobii X120 eye tracker with a sampling rate of 60 Hz. Infants completed an infant-adapted 5-point calibration sequence, which was run using Tobii Studio software (Tobii Technology, Sweden). Calibration was checked for accuracy and repeated up to three times if necessary.</p>
<p>After showing an attention-getter, we used a procedure adapted from <xref ref-type="bibr" rid="bibr9-0165025412473016">Pons et al. (2009</xref>), which is a variant of the intersensory matching procedure. Importantly, in this procedure, the sound is not presented at the same time as the visual stimuli to ensure that audiovisual synchrony is not mediating intersensory matching.</p>
<p>There were six 30-second trials (see <xref ref-type="fig" rid="fig1-0165025412473016">Figure 1</xref>): in the first and second trials (baseline condition), infants were presented with two side-by-side silent video clips, displaying one bilingual speaker uttering the same story in French on one side and in German on the other side. The left-right position of French and German videos was counterbalanced across infants in the first trial and reversed in the second one. In the third trial (auditory familiarization trial), infants were presented with a voice speaking either French or German while they were watching an attention-getter. Infants were randomly assigned to one of two auditory groups (German or French) and the specific speaker the infants listened to was different from the speaker they had seen in the baseline trials. In the fourth trial (post-test trial), we presented the two initial silent videos again. The fifth and sixth trials were a repetition of the auditory familiarization and post-test trial but the left-right presentation of the silent videos was reversed in the sixth trial. The voices and silent videos of the four female bilingual German-French speakers were counterbalanced across infants.</p>
<fig id="fig1-0165025412473016" position="float">
<label>Figure 1.</label>
<caption>
<p>Schematic representation of the procedure used in the current study. Only the German auditory condition is shown.</p>
</caption>
<graphic xlink:href="10.1177_0165025412473016-fig1.tif"/>
</fig>
</sec>
<sec id="section5-0165025412473016">
<title>Data analysis</title>
<p>Data were analysed for the total duration of fixations within an area of interest. Fixations were defined as having a minimum radius of 35 pixels and a minimum duration of 100 ms. Three Areas of Interest (AOIs) were created: whole face, mouth and eyes. The AOI of the whole face was defined by outlining the face with the hair excluded. A small buffer zone of approximately 1 cm allowed for head movements during the recording (see <xref ref-type="bibr" rid="bibr7-0165025412473016">Liu et al., 2011</xref>; <xref ref-type="bibr" rid="bibr15-0165025412473016">Wheeler et al., 2011</xref>). The mouth AOI and the eyes AOI were created in the same way as in the study of <xref ref-type="bibr" rid="bibr6-0165025412473016">Lewkowicz and Hansen-Tift (2012</xref>) by defining rectangular AOIs that covered both eyes and the mouth, respectively, for each stimulus face (see <xref ref-type="fig" rid="fig2-0165025412473016">Figure 2</xref>).</p>
<fig id="fig2-0165025412473016" position="float">
<label>Figure 2.</label>
<caption>
<p>Example of eyes and mouth AOI plots, 81 × 95mm (72 × 72 dpi).</p>
</caption>
<graphic xlink:href="10.1177_0165025412473016-fig2.tif"/>
</fig>
<p>For the three AOIs (face, mouth and eyes) of each French- or German-speaking face, gaze data were aggregated across the baseline and post-test trials, respectively. Thus, data analysis was conducted with the AOIs (face, mouth and eyes) of the bilingual person speaking French (resp. German) during the baseline trials (first and second trials), and the AOIs of the bilingual person speaking French (resp. German) during the post-test trials (fourth and sixth trials).</p>
</sec>
</sec>
<sec id="section6-0165025412473016">
<title>Results</title>
<p>Preliminary analyses revealed no significant effects of infants’ gender or of the bilingual speakers’ identity on infants’ gaze behavior to AOIs. Thus, the data for these two factors were collapsed for subsequent analyses.</p>
<sec id="section7-0165025412473016">
<title>Face AOI</title>
<p>As a dependent variable, we computed a preferential looking-score by dividing the fixation time on one face (e.g., face AOI of the face speaking German) by the amount of total looking time (sum of looking times at the face AOI of the German and French faces) across the baseline and post-test trials, respectively. To determine whether the infants showed a preference for one of the visual speeches during the baseline condition, and whether such a preference could be affected by the auditory speech the infants were familiarized with, we submitted the preferential looking scores to an ANOVA with “visual speech” (German, French) and “test” (baseline, post-test) as within-subjects factors, and “auditory group” (French, German) as a between-subjects factor.</p>
<p>This analysis did not show any significant effect (all <italic>p</italic>s &gt; .6), indicating that infants did not show a preference for either of the faces during baseline (<italic>M<sub>German</sub></italic> = 50%, <italic>SD<sub>German</sub></italic> = 11%; <italic>M<sub>French</sub></italic> = 50%, <italic>SD<sub>French</sub></italic> = 11%) and that this pattern was not influenced by auditory familiarization, neither after infants were exposed to German auditory speech (<italic>M<sub>German</sub></italic> = 48%, <italic>SD<sub>German</sub></italic> = 10%; <italic>M<sub>French</sub></italic> = 52%, <italic>SD<sub>French</sub></italic> = 11%) nor after the infants heard French (<italic>M<sub>German</sub></italic> = 50%, <italic>SD<sub>German</sub></italic> = 11%; <italic>M<sub>French</sub></italic> = 50%, <italic>SD<sub>French</sub></italic> = 11%).</p>
</sec>
<sec id="section8-0165025412473016">
<title>Proportional fixation on mouth and eyes AOIs</title>
<p>The fixation data were converted into proportional fixation times within the mouth and eyes AOI relative to the total on-face fixation times. To analyse whether the infants showed different looking patterns within the faces after auditory familiarization, we computed a four-way ANOVA with “AOI” (mouth, eyes), “visual speech” (German, French) and “test” (baseline, post-test) as within-subjects factors and “auditory group” (French, German) as a between-subjects factor.</p>
<p>This analysis failed to show any effect of visual speech, <italic>F</italic>(1, 38) = .04, <italic>p =</italic> .83, η<sup>2</sup> = .00, nor any interaction with this factor. This seems to indicate that infants scanned the German- and French-speaking faces in a similar way, even after auditory familiarization. However, the ANOVA revealed a significant main effect of AOI, <italic>F</italic>(1, 38) = 126.23, <italic>p &lt;</italic> .001, η<sup>2</sup> = .77. Infants looked longer at the mouth AOI (<italic>M<sub>mouth</sub></italic> = 48%, <italic>SD<sub>mouth</sub></italic> = 20%) than at the eyes AOI (<italic>M<sub>eyes</sub></italic> = 14%, <italic>SD<sub>eyes</sub></italic> <italic>= </italic>12%). We also found a significant AOI × auditory group interaction, <italic>F</italic>(1, 38) = 9.78, <italic>p </italic>&lt; .01, η<sup>2</sup> = .21 and a significant AOI × test × auditory group interaction, <italic>F</italic>(1, 38) = 24.87, <italic>p </italic>&lt; .001, η<sup>2</sup> = .40, indicating that during the post-test trials, the looking patterns toward the mouth and the eye region of both the German- and French-speaking faces were influenced by the auditory input. However, the looking time was affected similarly for German and French faces, as there was no difference in looking times <italic>between</italic> both faces (see above). As illustrated in <xref ref-type="fig" rid="fig3-0165025412473016">Figure 3</xref>, this latter interaction is explained by the fact that the auditory familiarization modulated looking behavior in an opposite manner for French and German.</p>
<fig id="fig3-0165025412473016" position="float">
<label>Figure 3.</label>
<caption>
<p>Mean and standard errors of proportional looking times (%) to mouth and eyes AOI within baseline and post-test trials of both auditory groups.</p>
</caption>
<graphic xlink:href="10.1177_0165025412473016-fig3.tif"/>
</fig>
<p>Whereas infants auditorily-familiarized with French looked longer at the mouth region of both faces during the post-test trials (<italic>M </italic>= 57%, <italic>SD</italic> = 15%) than during baseline (<italic>M</italic> = 48%, <italic>SD</italic> = 11%; <italic>t</italic>(19) = −2.18, <italic>p</italic> &lt; .05), infants who had heard German looked shorter at the mouth region of both faces during the post-test trials (<italic>M </italic>= 35%, <italic>SD</italic> = 14%) than during baseline (<italic>M</italic> = 48%, <italic>SD</italic> = 17%; <italic>t</italic>(19) = 5.09, <italic>p</italic> &lt; .001). Conversely, whereas infants auditorily-familiarized with French looked shorter at the eyes region of both faces during the post-test trials (<italic>M </italic>= 9%, <italic>SD</italic> = 5%) than during baseline (<italic>M</italic> = 14%, <italic>SD</italic> = 8%; <italic>t</italic>(19) = 2.23, <italic>p</italic> &lt; .05), infants who heard German looked longer at the eyes region of both faces during the post-test trials (<italic>M</italic> = 23%, <italic>SD</italic> = 9%) than during baseline (<italic>M</italic> = 14%, <italic>SD</italic> = 10%; <italic>t</italic>(19) = −3.42, <italic>p</italic> &lt; .01).</p>
</sec>
</sec>
<sec id="section9-0165025412473016">
<title>Discussion</title>
<p>The main goal of the present study was to investigate whether 12-month-olds’ visual exploration of faces speaking in their native language (German) or in a non-native (French) language was influenced by auditory speech. The results showed that listening to auditory speech did not lead to different visual preferences for the German- and French-speaking faces. Auditory speech rather affected infants’ visual scanning of the faces but independently of whether they were silently speaking French or German. After auditory exposure to their native language, infants looked less at the mouth of <italic>both</italic> faces, whereas looking times at the eyes increased. Conversely, after auditory exposure to their non-native language, infants looked more at the mouth of <italic>both</italic> faces, whereas looking times at the eyes decreased.</p>
<p>These results are consistent with the study of <xref ref-type="bibr" rid="bibr6-0165025412473016">Lewkowicz and Hansen-Tift (2012</xref>) who showed that 12-month-old infants looked longer at the mouth when they heard and saw a person speaking in a foreign language (Spanish), in contrast to infants who attended equally-long on the mouth and the eyes when the talker spoke their native language (English). However, <xref ref-type="bibr" rid="bibr6-0165025412473016">Lewkowicz and Hansen-Tift (2012</xref>) presented the infants with visual and auditory speech simultaneously, whereas our design allowed us to isolate the impact of separately presented auditory speech on infants’ looking patterns to silently-talking faces.</p>
<p>According to <xref ref-type="bibr" rid="bibr6-0165025412473016">Lewkowicz and Hansen-Tift (2012</xref>), our results can be considered in the context of multisensory perceptual narrowing. As infants at the end of the first year have specialized speech perception skills of their native language (<xref ref-type="bibr" rid="bibr14-0165025412473016">Werker &amp; Tees, 1999</xref>), it can be assumed that the processing of their native speech does not necessarily rely on language-specific visual inputs like the movements of the mouth. On the contrary, the processing of unfamiliar speech relies more on the processing on the mouth. In line with this interpretation, our results showed that after hearing their native language, infants focused less on the mouth and increased looking times toward the eyes of the silently-talking faces, probably to gain access to socially relevant cues (<xref ref-type="bibr" rid="bibr5-0165025412473016">Langton, Watt, &amp; Bruce, 2000</xref>). In contrast, infants who were presented with French auditory speech (their non-native language) focused less on the eyes and showed increased visual attention to the mouth, possibly to profit from complementary audio-visual information that is available in this area.</p>
<p>With regard to infants’ visual processing of silently-talking faces without previous listening to auditory speech (baseline trials), our results have demonstrated that the infants did not show any preference for one of the two visual speeches, suggesting that they were not able to discriminate their native language from a non-native language on the basis of visual cues alone. These results confirm and extend the findings of Weikum and colleagues (2007) by showing similar results among infants older than 8 months. Unexpectedly, our findings showed that infants focused longer on the mouth than on the eyes on both the German- and the French-speaking faces in the baseline (see <xref ref-type="fig" rid="fig1-0165025412473016">Figure 1</xref>).Thus, without the opportunity to listen to speech, infants at 12 months of age looked mainly at the mouth compared to the eyes of silently-talking faces. Our results are in line with studies indicating that older infants focus longer on the mouth of in-sound presented talking faces compared to smiling faces (<xref ref-type="bibr" rid="bibr11-0165025412473016">Tenenbaum, Shah, Sobel, Malle, &amp; Morgan, 2012</xref>), or when they are exposed to silently-talking faces of young children (<xref ref-type="bibr" rid="bibr1-0165025412473016">Frank, Vul, &amp; Saxe, 2011</xref>) or of their mother (<xref ref-type="bibr" rid="bibr2-0165025412473016">Hunnius &amp; Geuze, 2004</xref>). It is reasonable to assume that in our study, infants’ attention was mainly attracted by the movement of the mouth when no auditory speech was presented before.</p>
<p>Importantly, the auditory input did not enable the 12-month-old infants to discriminate the visually presented French and German languages neither in terms of a general visual preference nor in terms of differential scanning patterns between the faces. Thus, even for their native language, infants did not perform cross-modal matching of heard and seen speech. This result seems to be in contrast to Pons and colleagues’ result (2009) indicating that 11-month-old English infants successfully discriminated between the visually presented syllables “ba” and “va” after they had heard one of these syllables acoustically. Thus, it seems that, at the syllable level, discrimination of visual speech or cross-modal matching of visual-auditory information can be enabled by prior auditory speech in infants at the end of the first year. At the utterance level, however, auditory speech does not enable visual language discrimination. It can be assumed that cross-modal processing of fluent speech, especially when amodal cues (e.g., synchrony) are eliminated due to sequential presentation of auditory and visual speech, is too difficult for infants at this age. Another interpretation could be that infants at the end of the first year do not notice the differences between the visual cues carried by two different languages at the utterance level.</p>
<p>A limitation of the present study is that only 12-month-olds were tested and future research will have to be conducted using our design to examine younger infants. This will allow us to determine the extent to which higher interest in visual speech combined with less auditory language understanding, which can be assumed in younger infants, affect their visual processing of silently-talking faces.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure" id="fn2-0165025412473016"><label>Funding</label>
<p>This study was supported through a grant from the German Research Foundation (Deutsche Forschungsgemeinschaft) for GS (SCHW 665/11-1) and ANR--10-FRAL-017 for HL &amp; JG.</p></fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Frank</surname>
<given-names>M. C.</given-names>
</name>
<name>
<surname>Vul</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Saxe</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Measuring the development of social attention using free-viewing</article-title>. <source>Infancy</source>, <volume>16</volume>, <fpage>1</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr2-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hunnius</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Geuze</surname>
<given-names>R. H.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Developmental changes in visual scanning of dynamic faces and abstract stimuli in infants: A longitudinal study</article-title>. <source>Infancy</source>, <volume>6</volume>(<issue>2</issue>), <fpage>231</fpage>–<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr3-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kelly</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Quinn</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Slater</surname>
<given-names>A. M.</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Ge</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Pascalis</surname>
<given-names>O.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>The other-race effect develops during infancy: Evidence of perceptual narrowing</article-title>. <source>Psychological Science</source>, <volume>18</volume>(<issue>12</issue>), <fpage>1084</fpage>–<lpage>1089</lpage>.</citation>
</ref>
<ref id="bibr4-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kuhl</surname>
<given-names>P. K.</given-names>
</name>
<name>
<surname>Tsao</surname>
<given-names>F. M.</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>H. M.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Foreign-language experience in infancy: Effects of short-term exposure and social interaction on phonetic learning</article-title>. <source>Proceedings of the National Academy of Sciences USA</source>, <volume>100</volume>, <fpage>9096</fpage>–<lpage>9101</lpage>.</citation>
</ref>
<ref id="bibr5-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Langton</surname>
<given-names>S. R. H.</given-names>
</name>
<name>
<surname>Watt</surname>
<given-names>R. J.</given-names>
</name>
<name>
<surname>Bruce</surname>
<given-names>V.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Do the eyes have it? Cues to the direction of social attention</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>4</volume>(<issue>2</issue>), <fpage>50</fpage>–<lpage>59</lpage>.</citation>
</ref>
<ref id="bibr6-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lewkowicz</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Hansen-Tift</surname>
<given-names>A. M.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Infants deploy selective attention to the mouth of a talking face when learning speech</article-title>. <source>Proceedings of the National Academy of Sciences USA</source>. <comment>Advance online publication. doi:10.1073/pnas.1114783109</comment>
</citation>
</ref>
<ref id="bibr7-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Quinn</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Wheeler</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Xiao</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Ge</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Similarity and difference in the processing of same- and other-race faces as revealed by eye tracking in 4- to 9-month-olds</article-title>. <source>Journal of Experimental Child Psychology</source>, <volume>108</volume>, <fpage>180</fpage>–<lpage>189</lpage>.</citation>
</ref>
<ref id="bibr8-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pascalis</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>de Haan</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Nelson</surname>
<given-names>C. A.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Is face processing species-specific during the first year of life?</article-title> <source>Science</source>, <volume>296</volume>, <fpage>1321</fpage>–<lpage>1323</lpage>.</citation>
</ref>
<ref id="bibr9-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pons</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Lewkowicz</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Soto-Faraco</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Sebastián-Gallés</surname>
<given-names>N.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Narrowing of intersensory speech perception in infancy</article-title>. <source>Proceedings of the National Academy of Sciences USA</source>, <volume>106</volume>, <fpage>10598</fpage>–<lpage>10602</lpage>.</citation>
</ref>
<ref id="bibr10-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Scott</surname>
<given-names>L. S.</given-names>
</name>
<name>
<surname>Pascalis</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Nelson</surname>
<given-names>C. A.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A domain general theory of the development of perceptual discrimination</article-title>. <source>Current Directions in Psychological Science</source>, <volume>16</volume>(<issue>4</issue>), <fpage>197</fpage>–<lpage>201</lpage>.</citation>
</ref>
<ref id="bibr11-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tenenbaum</surname>
<given-names>E. J.</given-names>
</name>
<name>
<surname>Shah</surname>
<given-names>R. J.</given-names>
</name>
<name>
<surname>Sobel</surname>
<given-names>D. M.</given-names>
</name>
<name>
<surname>Malle</surname>
<given-names>B. F.</given-names>
</name>
<name>
<surname>Morgan</surname>
<given-names>J. L.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Increased focus on the mouth among infants in the first year of life: A longitudinal eye-tracking study</article-title>. <source>Infancy</source>, <volume>17</volume>, <fpage>1</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr12-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Weikum</surname>
<given-names>W. M.</given-names>
</name>
<name>
<surname>Vouloumanos</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Navarra</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Soto-Faraco</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Sebastián-Gallés</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Werker</surname>
<given-names>J. F.</given-names>
</name>
</person-group> (<year>2007</year>).<article-title>Visual language discrimination in infancy</article-title>. <source>Science</source>, <volume>316</volume>, <fpage>1159</fpage>.</citation>
</ref>
<ref id="bibr13-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Werker</surname>
<given-names>J. F.</given-names>
</name>
<name>
<surname>Tees</surname>
<given-names>R. C.</given-names>
</name>
</person-group> (<year>1984</year>). <article-title>Cross-language speech perception: Evidence for perceptual reorganization during the first year of life</article-title>. <source>Infant Behavior and Development</source>, <volume>7</volume>, <fpage>49</fpage>–<lpage>63</lpage>.</citation>
</ref>
<ref id="bibr14-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Werker</surname>
<given-names>J. F.</given-names>
</name>
<name>
<surname>Tees</surname>
<given-names>R. C.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Influences in infants speech processing: Toward a new synthesis</article-title>. <source>Annual Review of Psychology</source>, <volume>50</volume>, <fpage>509</fpage>–<lpage>535</lpage>.</citation>
</ref>
<ref id="bibr15-0165025412473016">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wheeler</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Anzures</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Quinn</surname>
<given-names>P. C.</given-names>
</name>
<name>
<surname>Pascalis</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Omrin</surname>
<given-names>D. S.</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Caucasian infants scan own- and other-race faces differently</article-title>. <source>PLoS ONE</source>, <volume>6</volume>, <fpage>e18621</fpage>. <comment>doi:10.137/journal.pone.0018621</comment>
</citation>
</ref>
</ref-list>
</back>
</article>