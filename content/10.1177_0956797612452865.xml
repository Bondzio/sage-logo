<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612452865</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612452865</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Articles</subject></subj-group></article-categories>
<title-group>
<article-title>The Capacity of Audiovisual Integration Is Limited to One Item</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Van der Burg</surname><given-names>Erik</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612452865">1</xref>
<xref ref-type="aff" rid="aff2-0956797612452865">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Awh</surname><given-names>Edward</given-names></name>
<xref ref-type="aff" rid="aff3-0956797612452865">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Olivers</surname><given-names>Christian N. L.</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612452865">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0956797612452865">
<label>1</label>Vrije Universiteit Amsterdam</aff>
<aff id="aff2-0956797612452865">
<label>2</label>University of Sydney</aff>
<aff id="aff3-0956797612452865">
<label>3</label>University of Oregon</aff>
<author-notes>
<corresp id="corresp1-0956797612452865">Erik Van der Burg, School of Psychology, 506 Griffith Taylor Building, University of Sydney, Sydney, New South Wales 2006, Australia E-mail: <email>erik.vanderburg@sydney.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>24</volume>
<issue>3</issue>
<fpage>345</fpage>
<lpage>351</lpage>
<history><date date-type="received"><day>10</day>
<month>1</month>
<year>2012</year></date>
<date date-type="accepted"><day>29</day>
<month>5</month>
<year>2012</year></date></history>
<permissions>
<copyright-statement>Â© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>The human visual attention system is geared toward detecting the most salient and relevant events in an overwhelming stream of information. There has been great interest in measuring how many visual events can be processed at a time, and most of the work has suggested that the limit is three to four. However, attention to a visual stimulus can also be driven by a synchronous auditory event. The present work indicates that a fundamentally different limit applies to audiovisual processing, such that at most only a single audiovisual event can be processed at a time. This limited capacity is not due to a limitation in visual selection; participants were able to process about four visual objects simultaneously. Instead, we propose that audiovisual orienting is subject to a fundamentally different capacity limit than pure visual selection is.</p>
</abstract>
<kwd-group>
<kwd>multisensory processing</kwd>
<kwd>attention</kwd>
<kwd>capacity</kwd>
<kwd>vision</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Visual selective attention is crucial for the ability to select salient or relevant visual events. An important question is how many such events can be detected and processed at a time. Previous work has arrived at a visual-capacity estimate of about 3 to 4 individual units of processing. For example, <xref ref-type="bibr" rid="bibr32-0956797612452865">Yantis and Johnson (1990)</xref> and <xref ref-type="bibr" rid="bibr31-0956797612452865">Wright (1994)</xref> found that about four flashed locations can be prioritized in search, and other researchers have found that similar numbers of items can be easily enumerated and tracked across space (for reviews, see <xref ref-type="bibr" rid="bibr4-0956797612452865">Cavanagh &amp; Alvarez, 2005</xref>, and <xref ref-type="bibr" rid="bibr23-0956797612452865">Trick &amp; Pylyshyn, 1994</xref>). Ultimately, this limit of 4 appears to reflect the capacity to individuate, or index, visual events in a visual short-term or working memory system (<xref ref-type="bibr" rid="bibr2-0956797612452865">Awh, Barton, &amp; Vogel, 2007</xref>; <xref ref-type="bibr" rid="bibr8-0956797612452865">Cowan, 2001</xref>; <xref ref-type="bibr" rid="bibr14-0956797612452865">Luck &amp; Vogel, 1997</xref>; <xref ref-type="bibr" rid="bibr17-0956797612452865">Pashler, 1988</xref>; <xref ref-type="bibr" rid="bibr18-0956797612452865">Pylyshyn, 2001</xref>; <xref ref-type="bibr" rid="bibr21-0956797612452865">Sperling, 1960</xref>).</p>
<p>However, visual stimulation is not the only way to make a visual event stand out. Synchronizing a visual event with an auditory or tactile signal makes it salient among a multitude of nonsynchronized visual stimuli (<xref ref-type="bibr" rid="bibr16-0956797612452865">Ngo &amp; Spence, 2010</xref>; <xref ref-type="bibr" rid="bibr24-0956797612452865">Van der Burg, Cass, Olivers, Theeuwes, &amp; Alais, 2010</xref>; <xref ref-type="bibr" rid="bibr26-0956797612452865">Van der Burg, Olivers, Bronkhorst, &amp; Theeuwes, 2008b</xref>; <xref ref-type="bibr" rid="bibr27-0956797612452865">Van der Burg, Olivers, Bronkhorst, &amp; Theeuwes, 2009</xref>), even when the auditory or tactile signal is irrelevant to the task (<xref ref-type="bibr" rid="bibr15-0956797612452865">Matusz &amp; Eimer, 2011</xref>; <xref ref-type="bibr" rid="bibr25-0956797612452865">Van der Burg, Olivers, Bronkhorst, &amp; Theeuwes, 2008a</xref>). A recent electroencephalogram study (<xref ref-type="bibr" rid="bibr29-0956797612452865">Van der Burg, Talsma, Olivers, Hickey, &amp; Theeuwes, 2011</xref>) revealed that such audiovisual events elicit very early (~50 ms) activity, followed by components indicative of attentional capture (N2pc; <xref ref-type="bibr" rid="bibr13-0956797612452865">Luck &amp; Hillyard, 1994</xref>) and visual memory (contralateral delay activity, CDA; contralateral negative slow wave, CNSW; <xref ref-type="bibr" rid="bibr12-0956797612452865">Klaver, Talsma, Wijers, Heinze, &amp; Mulder, 1999</xref>; <xref ref-type="bibr" rid="bibr30-0956797612452865">Vogel &amp; Machizawa, 2004</xref>).</p>
<p>These findings raise the question as to how many visual events can be prioritized when synchronized to a sound. To date, the majority of studies investigating multisensory integration have used a single combination of two sensory signals (e.g., one sound with one visual event; see, e.g., <xref ref-type="bibr" rid="bibr1-0956797612452865">Alais &amp; Burr, 2004</xref>; <xref ref-type="bibr" rid="bibr5-0956797612452865">Chen &amp; Spence, 2011</xref>; <xref ref-type="bibr" rid="bibr10-0956797612452865">Jack &amp; Thurlow, 1973</xref>; <xref ref-type="bibr" rid="bibr22-0956797612452865">Thomas, 1941</xref>). But in principle, multiple visual events might bind to an auditory signal as long as they appear within the temporal window of integration (i.e., the boundary interval during which multisensory integration occurs; <xref ref-type="bibr" rid="bibr6-0956797612452865">Colonius &amp; Diederich, 2004</xref>; <xref ref-type="bibr" rid="bibr19-0956797612452865">Slutsky &amp; Recanzone, 2001</xref>). In that case, the capacity for registering sound-driven visual events would ultimately be restricted by the visual processing limitations discussed earlier. Thus, the measured capacity of audiovisual integration would be capped by the three- to four-item limit on visual working memory.</p>
<p>However, there are reasons to hypothesize a more restricted capacity. From an ecological point of view, it would make sense to bind only one visual event to a specific sound. In natural scenes, individual, object-related sounds (unlike the sound of the wind or a babbling brook) come from a single source (<xref ref-type="bibr" rid="bibr1-0956797612452865">Alais &amp; Burr, 2004</xref>; <xref ref-type="bibr" rid="bibr3-0956797612452865">Beierholm, Kording, Shams, &amp; Ma, 2009</xref>; <xref ref-type="bibr" rid="bibr20-0956797612452865">Soto-Faraco, Kingstone, &amp; Spence, 2003</xref>). Thus, in the current work, we tested whether audiovisual orienting may be subject to a fundamentally different limit than other measures of on-line visual capacity are.</p>
<sec id="section1-0956797612452865">
<title>Experiment 1</title>
<p>We adopted the <italic>pip-and-pop</italic> paradigm (<xref ref-type="bibr" rid="bibr26-0956797612452865">Van der Burg et al., 2008b</xref>) to determine the capacity for detecting audiovisual events. In Experiment 1a, participants saw 24 black and white discs (see <xref ref-type="fig" rid="fig1-0956797612452865">Fig. 1</xref>). Every 150 ms, a randomly selected subset of 1 to 8 discs changed polarity from black to white or the reverse. The penultimate change was synchronized with a spatially uninformative auditory signal. The discs that changed at this time were the targets. Participants were asked to remember which discs were the targets and to determine whether or not a probe presented at the end of the trial fell on one of them. To investigate whether the capacity estimated on the basis of this procedure would generalize across different display conditions, in Experiment 1b, we manipulated the display density (16 or 24 discs), and in Experiment 1c, we manipulated the stimulus onset asynchrony (SOA) from one change to the next (150 or 200 ms). (Audiovisual binding is less ambiguous under 200-ms separations of visual events than under 150-ms separations; <xref ref-type="bibr" rid="bibr24-0956797612452865">Van der Burg et al., 2010</xref>.) Experiment 1d controlled for the possibility that the sound, rather than specifically binding to the visual signal, simply acted as a general temporal marker or warning signal that could improve detection of a concurrent visual event. This experiment compared the effect of a concurrent sound with that of a concurrent nonspecific visual signal (i.e., a ring drawing attention to the entire array of discs; cf. <xref ref-type="bibr" rid="bibr26-0956797612452865">Van der Burg et al., 2008b</xref>).</p>
<fig id="fig1-0956797612452865" position="float">
<label>Fig. 1.</label>
<caption>
<p>Illustration of the events in a trial in Experiment 1a. Every 150 ms, a random subset of 24 discs (only 8 are drawn here) changed in luminance. The penultimate change was synchronized with a sound (here, 3 items are marked for illustrative purposes). At the end of the trial, participants were asked whether or not a probed disc was one of the target (i.e., auditorily cued) items.</p>
</caption>
<graphic xlink:href="10.1177_0956797612452865-fig1.tif"/></fig>
<p>For each participant, capacity was estimated by adopting a simple model that is equivalent to <xref ref-type="bibr" rid="bibr8-0956797612452865">Cowanâs (2001)</xref> <italic>K</italic>. In our study, we manipulated the number of visual events <italic>n</italic> (i.e., the number of discs that changed polarity in synchrony with the auditory signal). If <italic>n</italic> is smaller than the capacity, then the proportion correct (<italic>p</italic>) is expected to be optimal; that is,</p>
<p>
<disp-formula id="disp-formula1-0956797612452865">
<mml:math display="block" id="math1-0956797612452865">
<mml:mtext>if</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>n</mml:mi>
<mml:mo>â¤</mml:mo>
<mml:mi>K</mml:mi>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>then</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>p</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>.</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula1-0956797612452865" xlink:href="10.1177_0956797612452865-eq1.tif"/>
</disp-formula>
</p>
<p>If the number of visual events exceeds the capacity, the expected proportion correct is given by the probability that the event fell within the observerâs capacity plus the probability of a correct guess when an event falls outside capacity (here, .5):</p>
<p>
<disp-formula id="disp-formula2-0956797612452865">
<mml:math display="block" id="math2-0956797612452865">
<mml:mtext>if</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>n</mml:mi>
<mml:mo>&gt;</mml:mo>
<mml:mi>K</mml:mi>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>then</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>p</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>K</mml:mi>
<mml:mo>/</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi>n</mml:mi>
<mml:mo>+</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>5</mml:mn>
</mml:math>
<graphic alternate-form-of="disp-formula2-0956797612452865" xlink:href="10.1177_0956797612452865-eq2.tif"/>
</disp-formula>
</p>
<p><xref ref-type="fig" rid="fig2-0956797612452865">Figure 2</xref> shows the predictions of the model for different numbers of visual events (<italic>n</italic> = 1â8) and different capacity levels (<italic>K</italic> = 0â3).</p>
<fig id="fig2-0956797612452865" position="float">
<label>Fig. 2.</label>
<caption>
<p>Predicted proportion correct as a function of the number of visual events, for four different capacities (<italic>K</italic>).</p>
</caption>
<graphic xlink:href="10.1177_0956797612452865-fig2.tif"/></fig>
<sec id="section2-0956797612452865">
<title>Method</title>
<sec id="section3-0956797612452865">
<title>Participants</title>
<p>Nine students (7 females, 2 males; mean age = 19.6 years, range = 18â23 years), 8 students (6 females, 2 males; mean age = 21.0 years, range = 19â25 years), 8 students (6 females, 2 males; mean age = 20.0 years, range = 18â23 years), and 10 students (5 females, 5 males; mean age = 22.8 years, range = 20â26 years) participated in Experiments 1a through 1d, respectively. They received course credits or money for their participation. All participants were naive as to the purpose of the experiment. In each of Experiments 1a, 1b, and 1d, the data of 1 participant were excluded from analyses because the participant had an overall proportion of correct responses near chance level (.50, .53, and .51, respectively).</p>
</sec>
<sec id="section4-0956797612452865">
<title>Design and procedure</title>
<p>Experiments were run in a dimly lit room. Participants were seated 80 cm from the monitor and wore headphones. Each trial began with a white fixation dot (0.08Â°, 95.36 cd/m<sup>2</sup>) presented for 1,000 ms at the center of the screen. Participants then viewed a changing visual display consisting of black (&lt; 0.5 cd/m<sup>2</sup>) and white (95.36 cd/m<sup>2</sup>) discs (radius = 0.65Â°) on a gray (10.10 cd/m<sup>2</sup>) background. In Experiments 1a, 1c, and 1d, there were 24 discs, and in Experiment 1b, the set size was 16 or 24 discs (50% of the trials each, in random order). Polarity was randomly determined for each disc at the start of a trial. All discs were randomly placed on an imaginary circle (radius = 6.5Â°) around the fixation dot. In Experiments 1a, 1b, and 1d, the displays changed every 150 ms. In Experiment 1c, the interval was either 150 ms or 200 ms (50% of the trials each, in random order). The total number of display changes on a given trial was randomly determined (9â17), and each display change consisted of a randomly determined number of discs (1â8) changing polarity (from black to white or the reverse).</p>
<p>In Experiments 1a, 1b, and 1c, the penultimate display change was always accompanied by a synchronous sound, a 60-ms, 500-Hz tone. In Experiment 1d, it could instead be accompanied by a displaywide visual cue. The visual cue was a set of two green concentric rings (0.1Â° wide; inner ring: radius = 5.72Â°; outer ring: radius = 7.28Â°) that were presented for 60 ms and surrounded the array of discs. The onset of the auditory and visual cues was synchronized with the polarity change of the discs. In Experiments 1a and 1d, the number of discs that changed polarity in synchrony with the cue (i.e., the number of visual events) was varied from 1 to 8. In Experiments 1b and 1c, the number of visual events was 1, 2, 3, 4, or 8.</p>
<p>After all polarity changes, the display became static, and a probe was presented (a red circle; radius = 0.5Â°; 16.89 cd/m<sup>2</sup>), either on one of the target discs (valid probe) or on a nontarget disc (invalid probe). Probe validity was 50%. Participants were asked to press the âjâ key on the computer keyboard when the probe was valid and the ânâ key when the probe was invalid. The dependent variable was the proportion correct.</p>
<p>In Experiment 1a, number of visual events and probe validity were randomly mixed within 10 blocks of 48 trials. In Experiment 1b, set size, number of visual events, and probe validity were randomly mixed within 10 blocks of 80 trials. In Experiment 1c, SOA, number of visual events, and probe validity were randomly mixed within 10 blocks of 80 trials. In Experiment 1d, number of visual events and probe validity were randomly mixed within 14 blocks of 48 trials. Cue modality (auditory vs. visual) was manipulated between blocks, in counterbalanced, alternating order, and participants were informed about the cue modality prior to each block.</p>
<p>Prior to the experimental blocks, participants received five practice blocks of 16 trials each (number of visual events was fixed to 1). After each practice or experimental block, participants received feedback about their overall proportion correct.</p>
</sec>
<sec id="section5-0956797612452865">
<title>Model fitting</title>
<p>The proportion correct, <italic>p</italic>, was derived from <xref ref-type="disp-formula" rid="disp-formula1-0956797612452865">Equations 1</xref> and <xref ref-type="disp-formula" rid="disp-formula2-0956797612452865">2</xref> with capacity <italic>K</italic> as the free parameter, which was optimized by minimizing the root-mean-square error (RMSE, the root of the averaged squared differences) between the model and the data. The model had one variable, <italic>n</italic>, which followed the number of visual events (1â8). Fitting was done within Microsoft Excel Solver and was initiated from several starting values of <italic>K</italic>. The outcome with the smallest RMSE was selected. The model was fitted to each individualâs data.</p>
</sec></sec>
<sec id="section6-0956797612452865">
<title>Results</title>
<p>The results of Experiment 1 are presented in <xref ref-type="fig" rid="fig3-0956797612452865">Figure 3</xref>. Results from practice blocks were excluded from analyses. The figure suggests good model fit, as was confirmed by low RMSEs (0.036â0.060). Overall <italic>R</italic><sup>2</sup> was high, at .86 to .92.</p>
<fig id="fig3-0956797612452865" position="float">
<label>Fig. 3.</label>
<caption>
<p>Results of Experiment 1: mean proportion correct (circles) and model fit (solid lines) as a function of number of visual events in (a) Experiment 1a, (b) Experiment 1b, (c) Experiment 1c, and (d) Experiment 1d. Results are shown separately for the two set sizes in Experiment 1b, the two stimulus onset asynchronies (SOAs) in Experiment 1c, and the two cue modalities in Experiment 1d. The error bars represent the overall standard errors of individualsâ mean proportion correct (some error bars are too small to be visible here).</p>
</caption>
<graphic xlink:href="10.1177_0956797612452865-fig3.tif"/></fig>
<p>In Experiment 1a, the overall proportion correct was .61 (false alarm rate = .13). An analysis of variance (ANOVA) revealed a reliable main effect of the number of visual events, <italic>F</italic>(7, 49) = 45.9, <italic>p</italic> &lt; .001, as the proportion correct decreased with increasing number of visual events. On average, the estimated capacity was smaller than 1 (0.69; range = 0.48â0.95).</p>
<p>In Experiment 1b, the overall proportion correct was .67 (false alarm rate = .09). An ANOVA with set size and number of visual events as within-subjects variables yielded a reliable main effect of the number of visual events, <italic>F</italic>(4, 24) = 53.5, <italic>p</italic> &lt; .001. The set-size effect and the two-way interaction failed to reach significance, <italic>p</italic>s â¥ .19. On average, the capacity was 0.84 when the set size was 16 (range = 0.47â1.22) and 0.71 when the set size was 24 (range = 0.54 to 0.95); set size did not have a reliable effect on estimated capacity, <italic>p</italic> &gt; 0.1.</p>
<p>In Experiment 1c, the overall proportion correct was .69 (false alarm rate = .09). An ANOVA with SOA and number of visual events as within-subjects variables revealed reliable main effects of the number of visual events, <italic>F</italic>(4, 28) = 99.0, <italic>p</italic> &lt; .001, and SOA, <italic>F</italic>(1, 7) = 37.0, <italic>p</italic> &lt; .001. Overall proportion correct was better when the SOA was 200 ms (.71) than when the SOA was 150 ms (.61). The two-way interaction was not reliable (<italic>F</italic> &lt; 1). On average, the capacity was 0.79 when the SOA was 150 ms (range = 0.44â1.13) and 1.05 when the SOA was 200 ms (range = 0.70â1.56). This difference in capacity was reliable, <italic>t</italic>(7) = 5.2, <italic>p</italic> = .001.</p>
<p>In Experiment 1d, the overall proportion correct was .56 (false alarm rate = .25). An ANOVA with cue modality and number of visual events as within-subjects variables yielded a reliable main effect of the number of visual events, <italic>F</italic>(7, 56) = 7.0, <italic>p</italic> &lt; .001. The main effect of cue modality and the two-way interaction were also reliable, <italic>F</italic>(1, 8) = 14.3, <italic>p</italic> = .005, and <italic>F</italic>(7, 56) = 13.3, <italic>p</italic> &lt; .001, respectively. Participants were able to use the nonspecific information in the visual cues, as performance was better than chance level in this condition, <italic>t</italic>(8) = 2.5, <italic>p</italic> &lt; .05. However, the estimated capacity was better when auditory signals accompanied the targets (0.58; range = 0.24â0.85) than when visual signals accompanied the targets (0.13; range = 0â0.36), <italic>t</italic>(8) = 6.5, <italic>p</italic> &lt; .001. Thus, the benefits of audiovisual cues go beyond temporal warning alone.</p>
</sec>
<sec id="section7-0956797612452865">
<title>Discussion</title>
<p>The results show that even though participants could reliably detect a single visual event presented in synchrony with an auditory signal (confirming <xref ref-type="bibr" rid="bibr26-0956797612452865">Van der Burg et al., 2008b</xref>), overall performance declined dramatically when more than one visual event was synchronized with the sound. More specifically, the model fits lead to the conclusion that at most one visual event can be linked to a sound at a time. The set-size manipulation had no effect on audiovisual capacity, whereas a slower rate of display change did reliably improve capacity. This latter effect was likely due to the reduced likelihood of misbindings (i.e., binding the sound with a distractor disc instead of with a target disc; see <xref ref-type="bibr" rid="bibr24-0956797612452865">Van der Burg et al., 2010</xref>, for a detailed discussion). However, even under these conditions, the capacity of audiovisual orienting did not exceed 1.</p>
<p>The results are unlikely due to general temporal cuing or warning effects. As we have shown before, such effects have a different time course than audiovisual-integration effects (<xref ref-type="bibr" rid="bibr26-0956797612452865">Van der Burg et al., 2008b</xref>). Moreover, in Experiment 1d, a general cue of a visual nature (i.e., a cue that, like the sound cue, was not specific to any of the items) yielded little to no benefit in performance (i.e., the proportion correct and capacity were both low). Thus, simple temporal knowledge about when to expect the target events was insufficient for detecting the synchronized targets. Instead, we suggest that the capacity limit in the audiovisual condition reflects the capacity limit of binding an auditory signal to a visual event (<xref ref-type="bibr" rid="bibr29-0956797612452865">Van der Burg et al., 2011</xref>).</p>
</sec></sec>
<sec id="section8-0956797612452865">
<title>Experiment 2</title>
<p>Although the results of Experiment 1 point toward a dramatically lower capacity for audiovisual orienting compared with standard visual orienting, we considered the possibility that the capacity of visual orienting itself might be low with the dynamic displays we used. Thus, our goal in Experiment 2 was to directly compare audiovisual and visual capacity for each participant. Audiovisual capacity was determined as in Experiment 1a. Visual capacity was determined by measuring performance in almost precisely the same task, except that target discs were indicated by a brief change in color instead of a synchronous sound. Thus, unlike in Experiment 1d, the visual signal was specific to the target items.</p>
<p>In addition to comparing absolute capacity estimates in the audiovisual and visual tasks, we were able to examine individual differences in capacity. As has been found in past studies of on-line visual capacity, we observed considerable variation across observers in the number of positions that captured attention in the audiovisual and visual conditions (see, e.g., <xref ref-type="bibr" rid="bibr2-0956797612452865">Awh et al., 2007</xref>; <xref ref-type="bibr" rid="bibr7-0956797612452865">Cowan, 1995</xref>; <xref ref-type="bibr" rid="bibr11-0956797612452865">Kane, Bleckley, Conway, &amp; Engle, 2001</xref>). Our findings regarding individual differences provide insight into the question of whether the audiovisual limitations we observed were in fact caused by visual limitations. If this were the case, then one might expect a positive correlation between audiovisual capacity and visual capacity.</p>
<sec id="section9-0956797612452865">
<title>Method</title>
<p>Sixteen students (10 females, 6 males; mean age = 22.5 years, range = 19â29 years) participated in Experiment 2. The experiment was identical to Experiment 1a, except that we included a visual condition to determine visual capacity. In this condition, in the penultimate display change, the target discs became temporarily (i.e., for 150 ms) green, until the next display change. The task in this visual condition was identical to the task in the audiovisual condition. Number of visual events and probe validity were randomly mixed within 14 blocks of 48 trials. Cue modality (audiovisual vs. visual) was manipulated between blocks, in counterbalanced, alternating order. Participants were informed about the modality prior to each block.</p>
</sec>
<sec id="section10-0956797612452865">
<title>Results and discussion</title>
<p>The results of Experiment 2 are presented in <xref ref-type="fig" rid="fig4-0956797612452865">Figure 4</xref>. Overall proportion correct was .75 (false alarm rate = .14). Data were subjected to an ANOVA with cue modality and number of visual events as within-subjects variables. The ANOVA yielded a reliable main effect of the number of visual events, <italic>F</italic>(7, 105) = 80.2, <italic>p</italic> &lt; .001. The main effect of cue modality and the two-way interaction were also reliable, <italic>F</italic>(1, 15) = 475.6, <italic>p</italic> &lt; .001, and <italic>F</italic>(7, 105) = 7.1, <italic>p</italic> &lt; .001, respectively. On average, audiovisual capacity was 0.78 (range = 0.30â1.36), whereas visual capacity was 3.34 (range = 2.64â4.34); the difference between these two estimates was reliable, <italic>t</italic>(15) = 16.5, <italic>p</italic> &lt; .001. The overall RMSE (0.056) was low. <xref ref-type="fig" rid="fig4-0956797612452865">Figure 4b</xref> presents audiovisual capacity as a function of visual capacity for each participant. The Pearson correlation between audiovisual and visual capacity was low (.13, <italic>p</italic> &gt; .6).</p>
<fig id="fig4-0956797612452865" position="float">
<label>Fig. 4.</label>
<caption>
<p>Results of Experiment 2: (a) mean proportion correct (circles) and model fit (solid lines) as a function of number of visual events for each modality (visual vs. audiovisual) and (b) audiovisual capacity as a function of visual capacity (circles represent results for individual participants, and the solid line represents the trend line corresponding to the correlation between audiovisual capacity and visual capacity). The error bars represent the overall standard errors of individualsâ mean proportion correct (some error bars are too small to be visible here).</p>
</caption>
<graphic xlink:href="10.1177_0956797612452865-fig4.tif"/></fig>
<p>This experiment corroborates the idea that audiovisual capacity is limited to 1 and that the severe audiovisual limit observed in Experiment 1 was not a by-product of limitations in visual orienting capacity. Our results are consistent with past estimates of on-line visual capacity, as we observed a visual capacity greater than 3 (see also <xref ref-type="bibr" rid="bibr8-0956797612452865">Cowan, 2001</xref>; <xref ref-type="bibr" rid="bibr14-0956797612452865">Luck &amp; Vogel, 1997</xref>; <xref ref-type="bibr" rid="bibr17-0956797612452865">Pashler, 1988</xref>); in addition, there was no noteworthy correlation between visual capacity and the capacity to bind visual stimuli to a sound.</p>
<p>One might argue that the low capacity estimate in the audiovisual condition was due to the fact that the audiovisual events were less salient than the cues that were used in the visual task, rather than to qualitatively different capacities for audiovisual and visual orienting. One argument against cue salience as the determining factor for capacity is illustrated in <xref ref-type="fig" rid="fig5-0956797612452865">Figure 5</xref>, which shows performance for 8 participants (from different experiments) who, during their entire experimental session, made at most one error in the audiovisual condition on trials with a single target. Thus, these observers found it very easy to detect one audiovisual event. Yet even for these observers, performance dropped steeply when there was more than one such event, such that their average capacity across all trials was estimated at 1.05.</p>
<fig id="fig5-0956797612452865" position="float">
<label>Fig. 5.</label>
<caption>
<p>Overall performance (and model fit) for 8 participants (Experiments 1 and 2) who performed at ceiling (â¤ 1 error) when a single visual event was synchronized with the tone.</p>
</caption>
<graphic xlink:href="10.1177_0956797612452865-fig5.tif"/></fig>
</sec></sec>
<sec id="section11-0956797612452865" sec-type="discussion">
<title>General Discussion</title>
<p>Even though participants could reliably detect a single visual event that was in synchrony with a sound (as in <xref ref-type="bibr" rid="bibr26-0956797612452865">Van der Burg et al., 2008b</xref>), overall performance declined substantially when more than one visual event was synchronized with the auditory signal. As Experiment 2 showed, this decline was not due to a limitation in visual orienting capacity per se.</p>
<p>Our results for visual orienting capacity fall in line with the three- to four-item limit that has previously been observed using a wide array of experimental approaches (<xref ref-type="bibr" rid="bibr8-0956797612452865">Cowan, 2001</xref>). Nevertheless, the estimated capacity for detecting audiovisual events did not exceed 1 (<italic>M</italic> = 0.75 across all experiments). Note that the capacity estimation model we used explicitly assumes that there are no encoding limits or attentional lapses affecting performance, and thus likely leads to a slight underestimation of the real visual and audiovisual capacities. However, even when such lapses or difficulties were assumed to be negligible (for the data underlying <xref ref-type="fig" rid="fig5-0956797612452865">Fig. 5</xref>), audiovisual capacity still did not exceed 1. It is clear that audiovisual integration capacity is severely limited compared with visual orienting capacity.</p>
<p>The question remains why the capacity to bind visual events to a sound is limited to at most one visual event. Rather than this merely being an information processing limitation, there may be a more adaptive, functional reason for it: A capacity of one object is consistent with the visual system being tuned to the fact that in natural environments, a sound in principle does not originate from more than one visual source. It therefore makes sense to bind a sound to only one object. Such binding is also consistent with the finding that participants perceive temporally aligned audiovisual events as coming from the same source object even when they are presented from discordant locations (i.e., the ventriloquism illusion; <xref ref-type="bibr" rid="bibr1-0956797612452865">Alais &amp; Burr, 2004</xref>; <xref ref-type="bibr" rid="bibr22-0956797612452865">Thomas, 1941</xref>).</p>
<p>If only one of multiple visual candidates is going to be associated with a sound, which one is integrated with it? One possibility is that the auditory signal is integrated with what happens to be the most dominant or salient synchronized visual event at that moment. This possibility could be tested by systematically manipulating the relative salience of these events. Another possibility is that one object happens to be more attended than others at the time the sound is presented, and the sound then attaches to that object. For example, research has indicated that audiovisual integration in multiple-object displays occurs largely automatically (<xref ref-type="bibr" rid="bibr15-0956797612452865">Matusz &amp; Eimer, 2011</xref>; <xref ref-type="bibr" rid="bibr25-0956797612452865">Van der Burg et al., 2008a</xref>, <xref ref-type="bibr" rid="bibr26-0956797612452865">2008b</xref>; <xref ref-type="bibr" rid="bibr29-0956797612452865">Van der Burg et al., 2011</xref>), but that this automaticity is nevertheless partly dependent on the size of the attentional window (<xref ref-type="bibr" rid="bibr28-0956797612452865">Van der Burg, Olivers, &amp; Theeuwes, 2012</xref>). The more observers distribute their attention across the display, the more likely they are to detect the single visual event that was synchronized with the auditory signal, whereas focusing on a central location appears to be more detrimental. Therefore, it may be the case that in the current experiments, integration occurred for the single most attended object in the display, at the expense of other synchronized objects.</p>
<p>We conclude that the visual system not only has an intrasensory capacity limitation (a maximum of three to four objects), but also has a separate, and even stricter, intersensory limitation such that attention is captured by only one audiovisual event at a time. We propose that this limitation occurs early in the processing chain (see, e.g., <xref ref-type="bibr" rid="bibr29-0956797612452865">Van der Burg et al., 2011</xref>; <xref ref-type="bibr" rid="bibr9-0956797612452865">Giard &amp; PeronnÃ©t, 1999</xref>), independently of limits in purely visual selection.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p></fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Alais</surname><given-names>D.</given-names></name>
<name><surname>Burr</surname><given-names>D.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The ventriloquism effect results from near-optimal bimodal integration</article-title>. <source>Current Biology</source>, <volume>14</volume>, <fpage>257</fpage>â<lpage>262</lpage>.</citation>
</ref>
<ref id="bibr2-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Awh</surname><given-names>E.</given-names></name>
<name><surname>Barton</surname><given-names>B.</given-names></name>
<name><surname>Vogel</surname><given-names>E. K.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Visual working memory represents a fixed number of items, regardless of complexity</article-title>. <source>Psychological Science</source>, <volume>18</volume>, <fpage>622</fpage>â<lpage>628</lpage>.</citation>
</ref>
<ref id="bibr3-0956797612452865">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Beierholm</surname><given-names>U.</given-names></name>
<name><surname>Kording</surname><given-names>K. P.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
<name><surname>Ma</surname><given-names>W. J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Comparing Bayesian models of multisensory cue combination without mandatory integration</article-title>. In <source>Advances in neural information processing systems</source> <volume>20</volume> (pp. <fpage>81</fpage>â<lpage>88</lpage>). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cavanagh</surname><given-names>P.</given-names></name>
<name><surname>Alvarez</surname><given-names>G. A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Tracking multiple targets with multifocal attention</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>9</volume>, <fpage>349</fpage>â<lpage>354</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>Y. C.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The crossmodal facilitation of visual object representations by sound: Evidence from the backward masking paradigm</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>37</volume>, <fpage>1784</fpage>â<lpage>1802</lpage>.</citation>
</ref>
<ref id="bibr6-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Colonius</surname><given-names>H.</given-names></name>
<name><surname>Diederich</surname><given-names>A.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Multisensory interaction in saccadic reaction time: A time-window-of-integration model</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>16</volume>, <fpage>1000</fpage>â<lpage>1009</lpage>.</citation>
</ref>
<ref id="bibr7-0956797612452865">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cowan</surname><given-names>N.</given-names></name>
</person-group> (<year>1995</year>). <source>Attention and memory: An integrated framework</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr8-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cowan</surname><given-names>N.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The magical number 4 in short-term memory: A reconsideration of mental storage capacity [Target article and commentaries]</article-title>. <source>Behavioral &amp; Brain Sciences</source>, <volume>24</volume>, <fpage>87</fpage>â<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Giard</surname><given-names>M. H.</given-names></name>
<name><surname>PeronnÃ©t</surname><given-names>F.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Auditory-visual integration during multimodal object recognition in humans: A behavioral and electrophysical study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>11</volume>, <fpage>473</fpage>â<lpage>490</lpage>.</citation>
</ref>
<ref id="bibr10-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jack</surname><given-names>C. E.</given-names></name>
<name><surname>Thurlow</surname><given-names>W. R.</given-names></name>
</person-group> (<year>1973</year>). <article-title>Effects of degree of visual association and angle of displacement on the âventriloquismâ effect</article-title>. <source>Perceptual &amp; Motor Skills</source>, <volume>37</volume>, <fpage>967</fpage>â<lpage>979</lpage>.</citation>
</ref>
<ref id="bibr11-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kane</surname><given-names>M. J.</given-names></name>
<name><surname>Bleckley</surname><given-names>M. K.</given-names></name>
<name><surname>Conway</surname><given-names>A. R. A.</given-names></name>
<name><surname>Engle</surname><given-names>R. W.</given-names></name>
</person-group> (<year>2001</year>). <article-title>A controlled-attention view of working-memory capacity</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>130</volume>, <fpage>169</fpage>â<lpage>183</lpage>.</citation>
</ref>
<ref id="bibr12-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Klaver</surname><given-names>P.</given-names></name>
<name><surname>Talsma</surname><given-names>D.</given-names></name>
<name><surname>Wijers</surname><given-names>A. A.</given-names></name>
<name><surname>Heinze</surname><given-names>H. J.</given-names></name>
<name><surname>Mulder</surname><given-names>G.</given-names></name>
</person-group> (<year>1999</year>). <article-title>An event-related brain potential correlate of visual short-term memory</article-title>. <source>NeuroReport</source>, <volume>10</volume>, <fpage>2001</fpage>â<lpage>2005</lpage>.</citation>
</ref>
<ref id="bibr13-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Luck</surname><given-names>S. J.</given-names></name>
<name><surname>Hillyard</surname><given-names>S. A.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Spatial filtering during visual search: Evidence from human electrophysiology</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>20</volume>, <fpage>1000</fpage>â<lpage>1014</lpage>.</citation>
</ref>
<ref id="bibr14-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Luck</surname><given-names>S. J.</given-names></name>
<name><surname>Vogel</surname><given-names>E. K.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The capacity of visual working memory for features and conjunctions</article-title>. <source>Nature</source>, <volume>390</volume>, <fpage>279</fpage>â<lpage>281</lpage>.</citation>
</ref>
<ref id="bibr15-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Matusz</surname><given-names>P. J.</given-names></name>
<name><surname>Eimer</surname><given-names>M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Multisensory enhancement of attentional capture in visual search</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>18</volume>, <fpage>904</fpage>â<lpage>909</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ngo</surname><given-names>M. K.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Auditory, tactile, and multisensory cues facilitate search for dynamic visual stimuli</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>72</volume>, <fpage>1654</fpage>â<lpage>1665</lpage>.</citation>
</ref>
<ref id="bibr17-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pashler</surname><given-names>H.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Familiarity and visual change detection</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>44</volume>, <fpage>369</fpage>â<lpage>378</lpage>.</citation>
</ref>
<ref id="bibr18-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pylyshyn</surname><given-names>Z.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Visual indexes, preconceptual objects, and situated vision</article-title>. <source>Cognition</source>, <volume>80</volume>, <fpage>127</fpage>â<lpage>158</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slutsky</surname><given-names>D. A.</given-names></name>
<name><surname>Recanzone</surname><given-names>G. H.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Temporal and spatial dependency of the ventriloquism effect</article-title>. <source>NeuroReport</source>, <volume>12</volume>, <fpage>7</fpage>â<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr20-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Soto-Faraco</surname><given-names>S.</given-names></name>
<name><surname>Kingstone</surname><given-names>A.</given-names></name>
<name><surname>Spence</surname><given-names>C.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Multisensory contributions to the perception of motion</article-title>. <source>Neuropsychologia</source>, <volume>41</volume>, <fpage>1847</fpage>â<lpage>1862</lpage>.</citation>
</ref>
<ref id="bibr21-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sperling</surname><given-names>G.</given-names></name>
</person-group> (<year>1960</year>). <article-title>The information available in brief visual presentations</article-title>. <source>Psychological Monographs: General and Applied</source>, <volume>74</volume>(<issue>11</issue>), <fpage>1</fpage>â<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr22-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thomas</surname><given-names>G. J.</given-names></name>
</person-group> (<year>1941</year>). <article-title>Experimental study of the influence of vision on sound localization</article-title>. <source>Journal of Experimental Psychology</source>, <volume>28</volume>, <fpage>167</fpage>â<lpage>177</lpage>.</citation>
</ref>
<ref id="bibr23-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Trick</surname><given-names>L. M.</given-names></name>
<name><surname>Pylyshyn</surname><given-names>Z. W.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Why are small and large numbers enumerated differently? A limited-capacity preattentive stage in vision</article-title>. <source>Psychological Review</source>, <volume>101</volume>, <fpage>80</fpage>â<lpage>102</lpage>.</citation>
</ref>
<ref id="bibr24-0956797612452865">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Cass</surname><given-names>J.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
<name><surname>Alais</surname><given-names>D.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Efficient visual search from synchronized auditory signals requires transient audiovisual events</article-title>. <source>PLoS ONE</source>, <volume>5</volume>(<issue>5</issue>), <fpage>e10664</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0010664">http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0010664</ext-link></citation>
</ref>
<ref id="bibr25-0956797612452865">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Bronkhorst</surname><given-names>A. W.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2008a</year>). <article-title>Audiovisual events capture attention: Evidence from temporal order judgments</article-title>. <source>Journal of Vision</source>, <volume>8</volume>(<issue>5</issue>), Article 2. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/8/5/2.full">http://www.journalofvision.org/content/8/5/2.full</ext-link></citation>
</ref>
<ref id="bibr26-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Bronkhorst</surname><given-names>A. W.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2008b</year>). <article-title>Pip and pop: Non-spatial auditory signals improve spatial visual search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>34</volume>, <fpage>1053</fpage>â<lpage>1065</lpage>.</citation>
</ref>
<ref id="bibr27-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Bronkhorst</surname><given-names>A. W.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Poke and pop: Tactile-visual synchrony increases visual saliency</article-title>. <source>Neuroscience Letters</source>, <volume>450</volume>, <fpage>60</fpage>â<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr28-0956797612452865">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2012</year>). <article-title>The attentional window modulates capture by audiovisual events</article-title>. <source>PLoS ONE</source>, <volume>7</volume>(<issue>7</issue>), <fpage>e39137</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0039137">http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0039137</ext-link></citation>
</ref>
<ref id="bibr29-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Van der Burg</surname><given-names>E.</given-names></name>
<name><surname>Talsma</surname><given-names>D.</given-names></name>
<name><surname>Olivers</surname><given-names>C. N. L.</given-names></name>
<name><surname>Hickey</surname><given-names>C.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Early multisensory interactions affect the competition among multiple visual objects</article-title>. <source>NeuroImage</source>, <volume>55</volume>, <fpage>1208</fpage>â<lpage>1218</lpage>.</citation>
</ref>
<ref id="bibr30-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vogel</surname><given-names>E. K.</given-names></name>
<name><surname>Machizawa</surname><given-names>M. G.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Neural activity predicts individual differences in visual working memory capacity</article-title>. <source>Nature</source>, <volume>428</volume>, <fpage>748</fpage>â<lpage>751</lpage>.</citation>
</ref>
<ref id="bibr31-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wright</surname><given-names>R. D.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Shifts of visual attention to multiple simultaneous location cues</article-title>. <source>Canadian Journal of Experimental Psychology</source>, <volume>48</volume>, <fpage>205</fpage>â<lpage>217</lpage>.</citation>
</ref>
<ref id="bibr32-0956797612452865">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yantis</surname><given-names>S.</given-names></name>
<name><surname>Johnson</surname><given-names>D. N.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Mechanisms of attentional priority</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>16</volume>, <fpage>812</fpage>â<lpage>825</lpage>.</citation>
</ref></ref-list>
</back>
</article>