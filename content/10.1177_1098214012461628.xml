<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AJE</journal-id>
<journal-id journal-id-type="hwp">spaje</journal-id>
<journal-title>American Journal of Evaluation</journal-title>
<issn pub-type="ppub">1098-2140</issn>
<issn pub-type="epub">1557-0878</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1098214012461628</article-id>
<article-id pub-id-type="publisher-id">10.1177_1098214012461628</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Dialogues</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Good Evaluation Measures</article-title>
<subtitle>More Than Their Psychometric Properties</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Weitzman</surname>
<given-names>Beth C.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1098214012461628">1</xref>
<xref ref-type="corresp" rid="corresp1-1098214012461628"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Silver</surname>
<given-names>Diana</given-names>
</name>
<xref ref-type="aff" rid="aff1-1098214012461628">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1098214012461628">
<label>1</label>Steinhardt School of Culture, Education, and Human Development, New York University, New York, NY, USA</aff>
<author-notes>
<corresp id="corresp1-1098214012461628">Beth C. Weitzman, Pless Hall, 4th floor, 82 Washington Square East, New York, NY 10003, USA. Email: <email>beth.weitzman@nyu.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>34</volume>
<issue>1</issue>
<fpage>115</fpage>
<lpage>119</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Evaluation Association</copyright-holder>
</permissions>
<abstract>
<p>In this commentary, we examine Braverman's insights into the trade-offs between feasibility and rigor in evaluation measures and reject his assessment of the trade-off as a zero-sum game. We, argue that feasibility and policy salience are, like reliability and validity, intrinsic to the definition of a good measure. To reduce the tension between feasibility and measurement rigor, we argue that evaluators should make greater use of existing data, identify ways in which improved measurement will result in improved program management, and “thickly” invest measurement resources in areas where questions are most important and evaluation is most needed.</p>
</abstract>
<kwd-group>
<kwd>measurement</kwd>
<kwd>administrative data</kwd>
<kwd>management</kwd>
<kwd>evaluation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Marc Braverman has written a thoughtful and carefully considered article (<xref ref-type="bibr" rid="bibr2-1098214012461628">Braverman, 2013</xref>, this issue) reminding us of the importance of measures and measurement in establishing credible evidence regarding programs and their impacts. While the field of program evaluation has for many years debated the characteristics of “useful” and persuasive evaluations, particularly in regard to stakeholder engagement (<xref ref-type="bibr" rid="bibr9-1098214012461628">Bryson, Patton, &amp; Bowman, 2011</xref>; <xref ref-type="bibr" rid="bibr4-1098214012461628">Fleischer &amp; Christie, 2009</xref>; <xref ref-type="bibr" rid="bibr6-1098214012461628">House, 1980</xref>; <xref ref-type="bibr" rid="bibr7-1098214012461628">Johnson et al., 2009</xref>; <xref ref-type="bibr" rid="bibr8-1098214012461628">Weiss, 1998</xref>), this discussion has focused primarily on issues of research design rather than on measurement. Braverman begins to fill the gap with this article, in which he lays out the principles and approaches that should guide the selection of measures for the evaluation of local programs of a limited scale. Decisions regarding measures, like evaluation design, must be guided by the size and scope of the project in question, the resources available, and the specific audiences for the evaluation results.</p>
<p>For Braverman, there is a “sweet spot” in negotiating the choice of appropriate measures that is found somewhere between considerations of the rigor of measures and measurement strategies and the feasibility of executing them. Rigor refers especially to the underlying psychometric properties of a measure and its ability to fully and meaningfully capture the relevant construct. Braverman defines feasibility primarily as the financial and time costs associated with gathering data. Without rigor, Braverman argues, the evaluation and its findings will not be persuasive. He argues that evaluators should engage stakeholders in a discussion of the trade-offs between feasibility and rigor during evaluation planning, and that the local program context is an important determinant for locating the measurement sweet spot he describes. He contends that the level of rigor must reflect the audience for the evaluation and its sophistication.</p>
<p>Overall, we find Braverman’s insights into these trade-offs to be evocative of many of our own evaluation experiences, particularly those where we elected to use measures aimed for “goodness,” rather than “perfection.” The field has traditionally prized measures with the highest levels of reliability and validity. However, we argue that feasibility and policy salience are intrinsic to the definition of a good measure. Measures that are feasible and salient may be less precise (i.e., somewhat less perfect in regard to reliability and validity) without losing their goodness or value from an evaluation perspective. This emphasis on “goodness” has been applicable even in regard to large-scale evaluations of importance to the field and policy makers. We agree with Braverman that rigor and feasibility must both be carefully considered in identifying good measures for any evaluation. At the same time, we question the emphasis Braverman places on conceptualizing the relationship among these elements as opposite poles on a continuum, as he notes in figure 2 in his article.</p>
<p>Program evaluation benefits from its multidisciplinary perspective, and evaluators bring to the table the perspectives and tools of their own discipline. Measurement, and most especially its psychometric properties, has long been the domain of psychologists. This is not to say that others are unconcerned with trying to operationalize key constructs in a meaningful and reliable way, but psychologists are especially adept and interested in the underlying structures and meanings of each measure used in a given study. We, however, come to the world of program evaluation through the lens of policy analysis. It may surprise many evaluators trained in psychology to learn how little time is spent on measurement in departments of economics, public administration, or public policy. In these fields, measures that are timely, standardized, and easy to obtain are given great attention, even as they may only be a poor man’s proxy for the constructs in question. Routinely gathered data may include everything from unemployment claims, to hospital utilization, to numbers of shelter requests, to reading test scores. Such measures may allow evaluations to be more salient to important managerial and policy questions. The fact that these data have been collected in essentially the same manner, across time, program, and jurisdictions, gives the evaluator the opportunity to add a dimension of methodological rigor using the data to establish other comparison groups or to employ time-series analyses. Further, as these measures have been previously created and used by decision makers, the results from the evaluation may be more readily embraced and understood by both the immediate and the wider evaluation audience. In imagining a program for at-risk youth, one might wish to measure impact with validated instruments that aim to measure carefully all the changes experienced by the participants. But regularly gathered data on crime, wages, and school enrollment may have greater power to persuade, even as they may be subject to lower levels of reliability and validity. For these reasons, their qualities of “goodness” may be considered as important as the technical rigor held dear by the field of psychology and measurement experts.</p>
<p>We have three main critiques of the article, each of which speaks to the zero-sum assumption embedded in Braverman’s presentation. First, much of the discussion in Braverman’s article centers on issues raised in primary data collection. From our perspective, considering a wider variety of data sources and taking a broader view of data collection strategies can lessen the tension between cost and credibility. Second, the tension between feasibility and measurement rigor may be exaggerated when other benefits of improved measurement, such as enhanced program management, are not considered. Third, we think it critical that evaluators help stakeholders identify aspects of the evaluation that would most benefit from investments in rigorous, or what we have labeled “thick,” measurement. This requires that the evaluator provides stakeholders with a deeper understanding and appreciation of prior studies and what is already known in the literature about the policy or type of program under investigation. This should be seen as an essential element of what an evaluator brings to the table in the early stages of planning. Further, knowledge of existing relevant data sets, as well as previously validated instruments, enable the evaluator to strengthen the goodness of the measures to be used.</p>
<p>Braverman gives little attention to the use of administrative and managerial data, and thus, by omission, seems to extend his argument across types of measures. However, for us, to the degree that primary data may be complemented or replaced by existing information relevant to the program and its participants, issues of time and cost and burden are greatly ameliorated. Such data may not allow one to fully explore all aspects of the underlying logic of a program. Indeed, existing data present many such constraints. Still, close substitutes for key indicators may often be found or created through gentle massaging of existing information. In this way, locating Braverman’s sweet spot may be less of a zero-sum exercise, but could be achieved through the use of multiple measures from multiple data sources.</p>
<p>Consider, for example, an evaluation of an afterschool program in a particular community. It may be important to understand the degree to which children fully engage in the activities that are offered. Many of the most technically rigorous measures of engagement require expert on-site observations; this is the kind of cost that Braverman warns may be prohibitive in many settings and evaluations. Afterschool attendance may, however, serve as a reasonable proxy for engagement in some circumstances: programs with the highest levels of attendance might be presumed to have more engaged participants than those at the lowest levels, especially in programs that are voluntary and where children and families can exercise substantial choice. In this example, we assume that kids are voting their feet, even as we lack specific information on how or why these choices are being made. Attendance data are often routinely collected for purposes other than evaluation, and are readily available, making this a low-cost option. With any luck, it may also meet standards of technical rigor in regard to reliability. There are many other examples of data routinely collected for billing, management, or to meet government mandates; these data should, in our estimation, be the first place that evaluators look to find possible proxy measures of key constructs in the program’s logic model. Obviously, many evaluators have used standardized test scores (for two recent examples, see <xref ref-type="bibr" rid="bibr3-1098214012461628">Cho, Glewwe, &amp; Whitler, 2012</xref>; <xref ref-type="bibr" rid="bibr5-1098214012461628">Hinrichs, 2011</xref>) in evaluation of academic programs. Similarly, programs for homeless individuals and families have been evaluated using information routinely gathered on housing placements or case management visits (e.g., see <xref ref-type="bibr" rid="bibr1-1098214012461628">Althaus et al., 2011</xref>).</p>
<p>We recognize that existing data may not provide close enough approximations of needed measures. Often, the information is of insufficient clarity, consistency, or reliability to be used for evaluation purposes. In such situations, the addition of a new variable to the administrative record or the adjustment of an existing variable in that record can benefit the program’s evaluation and its management. To the degree that more rigorous measures provide management with information that is more accurate, more reliable, and more clear, the evaluation is not the only beneficiary. The ratio of costs to benefits of data collection has now shifted. Imagine a program serving a homeless population that gathered information on housing histories in a manner that lacked consistency or clarity; an evaluator would be unable to use that information in evaluating the program’s impact on housing stability. The program’s management would also be less able to use that information to effectively target services. In this situation, the evaluator has the opportunity to work with key stakeholders to consider the importance of good information to key managerial and clinical decisions. Ensuring reliable measurement in this case has a value beyond the evaluation and its credibility. To the degree that programmatic needs can be aligned with evaluation needs in their chosen measures, one can make the case that a greater investment can be justified.</p>
<p>In other situations, existing data may not allow for the direct measurement of key concepts involving desired behavioral changes but may allow one to measure outcomes closely tied to those behaviors. For example, emergency room visits may be imperfect but useful for measuring compliance with changes in bike helmet or food safety laws. Trying to measure changes in helmet use through surveys, observations, and other means of primary data collection may allow for better measurement of changed behavior but would require substantial resources. Existing data on head injuries, supplemented by some process indicators, may give one a “good enough” indicator of program impact. The strength of these measures depends substantially on how well the relationship between behavior and outcome has been established. In health care, even relatively rare sentinel events have proven useful for evaluating programs aimed at improving patient safety and the overall quality of care. For example, an evaluation of a program aimed at increasing hand washing among hospital employees may not be able to engage in expensive direct observation of hand washing behavior or surveys of physicians and nurses; rather, they could instead opt only to routinely gather information on rates of patient infection.</p>
<p>Making greater use of administrative data for program evaluation may, however, also require greater explanation. Braverman’s reminder that local program context is an essential component of a measure’s usefulness is especially warranted here. Because administrative data may not directly measure the program element as conceived by those developing the program, its persuasiveness may be less immediately obvious. Evaluators can help stakeholders to see the value of existing data in a new and different light during evaluation planning.</p>
<p>Overall, in thinking about where to place measurement resources, we found it useful to imagine “thickness” and “thinness” of measures. Braverman reminds us that single-item measures are rarely as precise as those that require multiple items and that single methods or sources of data are typically strengthened when complemented by data from other sources. We think of this kind of “layering” as thickness of measures and, like Braverman, we recognize the benefits and costs that thick measures entail. Providing some guidance about when and where to use the thickest and most expensive measures increases the chance that the rigor of the evaluation need not be compromised despite the limits on costs. From our perspective, evaluation resources should be focused on program areas of greatest uncertainty and innovation.</p>
<p>In our experience, program stakeholders need guidance from evaluators to determine what about their program is truly innovative. This requires that the evaluator possess knowledge of the field and the critical lessons of prior relevant research and evaluation. Braverman’s emphasis on an explicit discussion with stakeholders resonates for us and we believe that a frank discussion of innovation should be part of this discussion. Many programs use evaluation resources ineffectively, measuring “thinly” each step of a program model that has been developed from a solid base in the literature. We suggest that thickly measuring the truly innovative or uncertain components of a program is a better use of evaluation resources.</p>
<p>Persuasive, credible, and useful evidence can also be gained from focusing primary data collection efforts thickly, particularly in complex initiatives. One can engage in primary data collection for a small subset of participants, or for a subset of their activities. In a multistrategy effort to improve early-grade reading scores in an urban school district, we recommended that the local coalition avoid using substantial resources collecting evaluation data on an already well-regarded and tested program. Rather, we encouraged them to limit their evaluation activities in this regard to the hiring of reading experts to observe a handful of classrooms for just 3 days; these activities aimed to provide feedback about the program’s implementation. Such strategies provided valuable programmatic information, and allowed evaluators greater resources for more richly measuring other aspects of program implementation that had less of a solid evidence base.</p>
<p>Braverman has initiated a thoughtful discussion that merits further attention from our field. He affirms the role of the evaluator and the importance of expertise in regard to measurement; he calls on evaluators to extend their expertise beyond technical rigor to involving stakeholders in weighing the costs and benefits of measurement strategies more generally. Our challenge as a field is to bring creativity and flexibility to the identification of evaluation measures, while affirming the central importance of producing credible and rigorous evaluations for stakeholders and wider audiences alike.</p>
</body>
<back>
<fn-group>
<fn fn-type="conflict" id="fn1-1098214012461628">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn2-1098214012461628">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Althaus</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Paroz</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Hugli</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Ghali</surname>
<given-names>W. A.</given-names>
</name>
<name>
<surname>Daeppen</surname>
<given-names>J. B.</given-names>
</name>
<name>
<surname>Peytremann-Bridevaux</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Bodenmann</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Effectiveness of interventions targeting frequent users of emergency departments: A systematic review</article-title>. <source>Annals of Emergency Medicine</source>, <volume>58</volume>, <fpage>41</fpage>–<lpage>52</lpage>.</citation>
</ref>
<ref id="bibr2-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author"><name><surname>Braverman</surname><given-names>M. T.</given-names></name></person-group> (<year>2013</year>) <article-title>Negotiating Measurement: Methodological and interpersonal considerations in the choice and interpretation of instruments</article-title>. <source>American Journal of Evaluation</source>, <volume>34</volume>, <fpage>99</fpage>–<lpage>114</lpage>.</citation>
</ref>
<ref id="bibr9-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bryson</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Patton</surname>
<given-names>M. Q.</given-names>
</name>
<name>
<surname>Bowman</surname>
<given-names>R. A.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Working with evaluation stakeholders. A rationale, step-wise approach and toolkit</article-title>. <source>Evaluation and Program Planning</source>. <volume>34</volume>, <fpage>1</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr3-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cho</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Glewwe</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Whitler</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Do reductions in class size raise students’ test scores? Evidence from population variation in Minnesota’s elementary schools</article-title>. <source>Economics of Education Review</source>, <volume>31</volume>, <fpage>77</fpage>–<lpage>95</lpage>.</citation>
</ref>
<ref id="bibr4-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fleischer</surname>
<given-names>D. N.</given-names>
</name>
<name>
<surname>Christie</surname>
<given-names>C. A.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Evaluation use: Results from a survey of U.S. American Evaluation Association members</article-title>. <source>American Journal of Evaluation</source>, <volume>30</volume>, <fpage>158</fpage>–<lpage>175</lpage>.</citation>
</ref>
<ref id="bibr5-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hinrichs</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>When the bell tolls: The effects of school starting times on academic achievement</article-title>. <source>Education Finance and Policy</source>, <volume>6</volume>, <fpage>486</fpage>–<lpage>507</lpage>.</citation>
</ref>
<ref id="bibr6-1098214012461628">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>House</surname>
<given-names>E. R.</given-names>
</name>
</person-group> (<year>1980</year>). <source>Evaluating with validity</source>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr7-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Johnson</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Greenseid</surname>
<given-names>L. O.</given-names>
</name>
<name>
<surname>Toal</surname>
<given-names>S. A.</given-names>
</name>
<name>
<surname>King</surname>
<given-names>J. A.</given-names>
</name>
<name>
<surname>Lawrenz</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Volkov</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Research on evaluation use: A review of the empirical literature from 1986 to 2005</article-title>. <source>American Journal of Evaluation</source>, <volume>30</volume>, <fpage>377</fpage>–<lpage>410</lpage>.</citation>
</ref>
<ref id="bibr8-1098214012461628">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Weiss</surname>
<given-names>C. H.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Have we learned anything new about the use of evaluation?</article-title> <source>American Journal of Evaluation</source>, <volume>19</volume>, <fpage>21</fpage>–<lpage>33</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>