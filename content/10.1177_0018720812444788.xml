<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HFS</journal-id>
<journal-id journal-id-type="hwp">sphfs</journal-id>
<journal-title>Human Factors: The Journal of Human Factors and Ergonomics Society</journal-title>
<issn pub-type="ppub">0018-7208</issn>
<issn pub-type="epub">1547-8181</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0018720812444788</article-id>
<article-id pub-id-type="publisher-id">10.1177_0018720812444788</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Cognitive Processes</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Target Finding With a Spatially Aware Handheld Chart Display</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Jacobs</surname><given-names>Karen</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Soares</surname><given-names>Marcelo</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Ware</surname><given-names>Colin</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Arsenault</surname><given-names>Roland</given-names></name>
</contrib>
<aff id="aff1-0018720812444788">University of New Hampshire, Durham, New Hampshire</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0018720812444788">Colin Ware, Center for coastal and Ocean Mapping, University of New Hampshire, Durham, NH 03824, USA; <email>cware@ccom.unh.edu</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>54</volume>
<issue>6</issue>
<issue-title>Special Section: Keynote Addresses From the 18th Triennial Congress of the International Ergonomics Association</issue-title>
<fpage>1040</fpage>
<lpage>1052</lpage>
<history>
<date date-type="received">
<day>28</day>
<month>4</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>9</day>
<month>3</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>Â© 2012, Human Factors and Ergonomics Society</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<sec id="section1-0018720812444788">
<title>Objective:</title>
<p>The objective was to evaluate the use of a spatially aware handheld chart display in a comparison with a track-up fixed display configuration and to investigate how cognitive strategies vary when performing the task of matching chart symbols with environmental features under different display geometries and task constraints.</p>
</sec>
<sec id="section2-0018720812444788">
<title>Background:</title>
<p>Small-screen devices containing both accelerometers and magnetometers support the development of spatially aware handheld maps. These can be designed so that symbols representing targets in the external world appear in a perspective view determined by the orientation of the device.</p>
</sec>
<sec id="section3-0018720812444788">
<title>Method:</title>
<p>A panoramic display was used to simulate a marine environment. The task involved matching targets in the scene to symbols on simulated chart displays. In Experiment 1, a spatially aware handheld chart display was compared to a fixed track-up chart display. In Experiment 2, a gaze monitoring system was added and the distance between the chart display and the scene viewpoint was varied.</p>
</sec>
<sec id="section4-0018720812444788">
<title>Results:</title>
<p>All respondents were faster with the handheld device. Novices were much more accurate with the handheld device. People allocated their gaze very differently if they had to move between a map display and a view of the environment.</p>
</sec>
<sec id="section5-0018720812444788">
<title>Conclusion:</title>
<p>There may be important benefits to spatially aware handheld displays in reducing errors relating to common navigation tasks.</p>
</sec>
<sec id="section6-0018720812444788">
<title>Application:</title>
<p>Both the difficulty of spatial transformations and the allocation of attention should be considered in the design of chart displays.</p>
</sec>
</abstract>
<kwd-group>
<kwd>handheld chart</kwd>
<kwd>spatially aware display</kwd>
<kwd>attention</kwd>
<kwd>map reading</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section7-0018720812444788" sec-type="intro">
<title>Introduction</title>
<p>The widespread adoption of small-screen devices that contain both accelerometers and magnetometers (such as the iPhone and Android-based devices) has enabled the development of spatially aware navigation aids (<xref ref-type="bibr" rid="bibr6-0018720812444788">Hermann, Bieber, &amp; Duesterhoeft, 2003</xref>; <xref ref-type="bibr" rid="bibr11-0018720812444788">Schall et al., 2009</xref>; <xref ref-type="bibr" rid="bibr14-0018720812444788">Suomela, Roimela, &amp; Lehikoinene, 2002</xref>). The accelerometers provide the device with information about the gravity vector, and the magnetometers provide a digital compass. When integrated with a digital chart and other georeferenced data, this combination makes it possible for the device to provide a perspective view showing the location of landmarks and other georeferenced features present in the environment in the direction the user is facing. <xref ref-type="fig" rid="fig1-0018720812444788">Figure 1</xref> illustrates the concept.</p>
<fig id="fig1-0018720812444788" position="float">
<label>Figure 1.</label>
<caption>
<p>The handheld chart concept. Showing chart information in perspective depending on the orientation of the display should make it easier to match external targets with chart symbols.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig1.tif"/></fig>
<p>Despite the rapid emergence of applications taking advantage of geospatially aware devices, there has been relatively little work evaluating this kind of display as compared to more conventional fixed electronic map displays that appear on car dashboards and the bridges of ships. We are particularly interested in the value of handheld georeferenced charts that can be used to support ship operations. One of the more common and difficult tasks facing the mariner is that of noticing a navigation mark, such as a channel buoy, and matching this to the same feature on the chart (called the <italic>find-on-chart</italic> task). Alternatively, a particular landmark may be first identified on the chart, and the mariner is then faced with the problem of finding the same feature on the water or shore (called the <italic>find-in-scene</italic> task). These tasks can be challenging for a number of reasons: Navigation marks may be visible at a distance of a few kilometers, although at this distance they are very small in terms of visual angle and close to the resolution limit of the human eye. Visibility is often poor because of low light, rain, and fog, with the motion and height of waves compounding the difficulty. Also, some correspondence must be made between the 2D space of the chart and the perspective view of the world. The study reported here is an investigation of the value of handheld, spatially aware devices compared to more conventional chart displays with an emphasis on the problems of establishing this correspondence. Note that in the following we use the terms <italic>map</italic> and <italic>chart</italic> interchangeably, depending on which is most appropriate, with the assumption that the perceptual and cognitive tasks are the same. Most of the prior research into this topic has dealt with fixed displays on a vehicle, either a car, a ship, or a plane.</p>
<p>When navigating a vessel, the mariner typically has some form of electronic chart display fixed on the bridge, analogous to the GPS map displays used in motor vehicles. Three views are commonly available:</p>
<list id="list1-0018720812444788" list-type="simple">
<list-item><p><italic>North-up plan view</italic>: This is the classic orthographic map view, with north up, usually with the ship placed in the middle, oriented appropriately.</p></list-item>
<list-item><p><italic>Track-up plan view</italic>: This is also an orthographic map view but is oriented so that the heading of the ship is in the upward direction on the map. The map rotates as the vehicle rotates.</p></list-item>
<list-item><p><italic>Track-up perspective view</italic>: In this case, the map is given in a forward-looking perspective view on the screen. The viewpoint is often above and behind the vehicle.</p></list-item></list>
<p>In visual cognitive terms, the task involves relating symbols on a display to objects in the scene. This can be conceptualized in terms of creating a cognitive binding between objects visible in two different spatial reference frames. One way of solving the problem is through mental rotations. In his work on displays for pilots, <xref ref-type="bibr" rid="bibr1-0018720812444788">Aretz (1991)</xref> identified two different mental rotations necessary for successful map use: The first is azimuthal rotation, used to cognitively align a map with the direction of travel. The track-up display executes this rotation in the display computer, eliminating the need for the task to be performed mentally. The second is vertical tilt; a map can be horizontal, in which case it directly matches the plane of the displayed information, or it can be oriented vertically, as is typical of the map displays used on car dashboards.</p>
<p>When an observer is looking forward in a vehicle, a fixed map with a track-up view has no azimuthal mismatch between the map and the view of the environment, whereas a north-up view can have a mismatch of up to 180Â°. A number of studies have compared north-up views to track-up views, and these suggest that the track-up view is preferable, in that it is easier to use and results in fewer errors (<xref ref-type="bibr" rid="bibr1-0018720812444788">Aretz, 1991</xref>; <xref ref-type="bibr" rid="bibr8-0018720812444788">Levine, Marchon, &amp; Hanley, 1984</xref>; <xref ref-type="bibr" rid="bibr13-0018720812444788">Shepard &amp; Hurwitz, 1984</xref>). The amount of angular mismatch is important: People take much longer and are less accurate when a map is aligned more than 90Â° from their direction of travel (<xref ref-type="bibr" rid="bibr3-0018720812444788">Gugerty &amp; Brooks, 2001</xref>; <xref ref-type="bibr" rid="bibr16-0018720812444788">Wickens, 1999</xref>). Nevertheless, it is important to note that experienced navigators may prefer the north-up over the track-up view because it gives them a consistent frame of reference for geographic data. This is especially important if two individuals are discussing a map or chart over a telephone or radio link (<xref ref-type="bibr" rid="bibr16-0018720812444788">Wickens, 1999</xref>).</p>
<p>Tilting a display from horizontal to vertical is less disruptive of performance, especially if the view in the display is an oblique perspective (<xref ref-type="bibr" rid="bibr7-0018720812444788">Hickox &amp; Wickens, 1999</xref>). This may be at least partially the result of the tilt misalignment being limited to 90Â° (and often much less). Track-up perspective views have also been studied for application in the field of aviation human factors (<xref ref-type="bibr" rid="bibr7-0018720812444788">Hickox &amp; Wickens, 1999</xref>; <xref ref-type="bibr" rid="bibr12-0018720812444788">Schreiber, Wickens, Alton, Renner, &amp; Hickox, 1998</xref>; <xref ref-type="bibr" rid="bibr16-0018720812444788">Wickens, 1999</xref>). These studies suggest that the relative tilt angle between the perspective display and the scene perspective has a significant effect only if the angles are relatively large; mismatches of 20Â° or less result in minimal or no disruption of performance.</p>
<p>For a display mounted vertically, a perspective view reduces the tilt mismatch between the display and the environment and can entirely eliminate it if the view exactly matches the world view of the user. It might be thought that the ideal display would be one in which the tilt angle, the azimuth angle, and the perspective geometry were calculated in such a way that the display image closely matched the observerâs view of the scene. This is sometimes called an egocentric view, and it can be easily obtained with handheld devices.</p>
<p>With handheld devices, both track-up and perspective views can take into account the device orientation. It might be supposed that an egocentric perspective view would be better than a track-up plan view, but for an urban navigation task using a handheld device <xref ref-type="bibr" rid="bibr14-0018720812444788">Suomela et al. (2002)</xref> found that responses were faster for the track-up view. Also, participants gave subjective reports consistent with this; they reported that the perspective view was more difficult than the track-up view, especially for the purpose of locating targets. One of the problems they found with the perspective view was that some of the targets were hidden because of occlusion by buildings in the environment. In the marine environment, since there are no buildings and navigation marks are relatively small, occlusion is less likely to be a problem. The occlusion problem reported by Suomela et al. is sometimes mitigated by rendering an oblique perspective view using a viewpoint well above the viewerâs actual viewpoint. This is the approach we used in the present study.</p>
<p>Instead of carrying out the task by means of mental rotation, the task of matching symbols on a map with objects in a scene might be carried out by a process of visual spatial reasoning. For example, if an observer wishes to identify a set of navigation markers shown in a track-up display, as shown in <xref ref-type="fig" rid="fig2-0018720812444788">Figure 2a</xref>, and relate them to environment objects, as shown in <xref ref-type="fig" rid="fig2-0018720812444788">Figure 2b</xref>, a form of reasoning something like the following may occur: Transform the angles subtended between the buoy symbols, relative to the shipâs location on the chart display, to lateral spacing of buoys along the horizon. Compare relative distances between the targets and the ship icon on the chart with declination relative to the horizon in the environment.</p>
<fig id="fig2-0018720812444788" position="float">
<label>Figure 2.</label>
<caption>
<p>In a conventional track-up chart display the user must cognitively translate between the scene view (a) and the chart view (b). (c) With a spatially aware handheld device, the arrangement of the symbols on the display can bear a much closer relationship to the arrangement of features of the scene as they appear from the userâs perspective.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig2.tif"/></fig>
<p>Recent work has applied cognitive modeling to both the find-on-map task (<xref ref-type="bibr" rid="bibr5-0018720812444788">Gunzelmann &amp; Anderson, 2006</xref>) and the find-in-scene task (<xref ref-type="bibr" rid="bibr4-0018720812444788">Gunzelmann, 2008</xref>). They characterize the cognitive process as a series of operations that can be summarized as follows for the find-on-map task: <italic>Locate target in the scene, encode position of target within local cluster of targets, encode cluster with respect to forward direction. Transfer attention to map and locate viewer position, locate the cluster using information about heading, and finally locate the target within the cluster.</italic></p>
<p>The map display they used was simplified with a limited view showing only a region corresponding to what was visible in the scene view. The results showed a large effect of the orientation of the target with respect to the viewer as well as a map orientation effect. These were accurately predicted by the Adaptive Control of Thought-Rational (ACT-R) model.</p>
</sec>
<sec id="section8-0018720812444788">
<title>Attention Transfer Costs</title>
<p>Besides the cognitive effort involved in spatial transformations, a second cognitive cost involves working memory. To make a comparison between a display and the environment, the user must encode a set of relationships between features in the display, either in the form of mental imagery in visual working memory or in some kind of nonvisual predicate form involving verbal working memory. This encoding is held in visual or verbal working memory while attention is transferred to the scene using an eye movement. If the map display and environment target area are far apart in egocentric space, head and body movements may also be required to make this transfer of attention. It may take a second or more to turn the head around with a target that is behind the ship in the case of a forward facing track-up display on a bridge. In many larger ships, the situation is worse; an officer must walk between the display and one of the bridge wings to match up a chart feature with an environment feature. In some cases the two-step process proposed by <xref ref-type="bibr" rid="bibr5-0018720812444788">Gunzelmann and Anderson (2006)</xref> may be insufficient, and it will be necessary to transfer attention back and forth between the scene and the display several times. The process can be characterized as a visual-cognitive loop (see <xref ref-type="fig" rid="fig3-0018720812444788">Figure 3</xref>). This loop consists of the mariner encoding visuospatial information from the display, storing it in some form of working memory while attention is transferred to the scene, then attempting to find a partial correspondence between the information held in working memory and the information in the scene. Subsequent iterations are made until a correspondence between the chart symbol and scene object is established.</p>
<fig id="fig3-0018720812444788" position="float">
<label>Figure 3.</label>
<caption>
<p>The visual cognitive loop.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig3.tif"/></fig>
<p>The key temporal costs in this loop are the time to execute one cycle and the number of cycles that are needed. One of the main factors determining the number of cycles is the capacity of visual working memory (<xref ref-type="bibr" rid="bibr9-0018720812444788">Plumlee &amp; Ware, 2006</xref>). Many studies have shown that visual working memory can store only a small amount of information, at most four simple shapes (<xref ref-type="bibr" rid="bibr15-0018720812444788">Vogel, Woodman, &amp; Luck, 2001</xref>). If insufficient information has been stored for a reliable match, attention may be transferred back and forth between the fixed display and the scene several times before arriving at a confident decision (<xref ref-type="bibr" rid="bibr9-0018720812444788">Plumlee &amp; Ware, 2006</xref>). Alternatively, a guess may be made with a corresponding rise in errors.</p>
<p>A spatially aware handheld device can potentially provide a considerable advantage by reducing both the difficulty of the task in terms of mental transformations and the costs associated with the transfer of attention between the scene and the display, the key difference being that the viewpoint is tied to the orientation of the user and not to the heading of the vehicle. The users rotate the device as they turn their body, and the perspective view is computed with track-up corresponding to the direction they are facing. The device can be held in a position such that the important part of the scene and the map are close together in the visual field, enabling rapid eye movements taking 200 ms or less to transfer attention from one to the other (<xref ref-type="bibr" rid="bibr10-0018720812444788">Rayner, 1988</xref>). In the case of a fixed bridge display, the user may take several seconds to walk to the bridge window, and during this time working memory information will decay or be lost through interference with other incoming imagery.</p>
</sec>
<sec id="section9-0018720812444788">
<title>Study Goals</title>
<p>Our goal in the two experiments we report here was to first determine if the predicted advantages would be found for a <italic>user-orientated handheld chart</italic> showing an orientation-coupled perspective view in comparison with a more conventional <italic>ship-heading-coupled track-up chart</italic> display. Our second goal was to investigate how the visual cognitive loop was executed for this task. In Experiment 1 we looked at the efficiency with which the task could be performed with a handheld device displaying a perspective view compared to a track-up view with a fixed screen. In Experiment 2 we carried out manipulations designed to change the working memory load to assess its role in task performance.</p>
<p>A track-up chart display on the bridge of a ship is always in a fixed position somewhere near the center of the bridge. <italic>Up</italic> on the display corresponds to <italic>ahead</italic> of the bow of the ship. In the case of the handheld display, when observers turn toward a target in the world while holding the display in front of them, the appropriate part of the scene will come into view. Moreover, it will be roughly in the same perspective view, although from an elevated viewpoint.</p>
</sec>
<sec id="section10-0018720812444788">
<title>Experiment 1</title>
<sec id="section11-0018720812444788">
<title>Method</title>
<p>To carry out this study we constructed a 180Â° panoramic virtual environment (VE), as illustrated in <xref ref-type="fig" rid="fig4-0018720812444788">Figures 4</xref> and <xref ref-type="fig" rid="fig5-0018720812444788">5</xref>. This showed a model of a marine environment that was populated with buoys that acted as targets. We studied two basic tasks common in ship navigation:</p>
<list id="list2-0018720812444788" list-type="simple">
<list-item><p><italic>Find-on-chart task</italic>: In the find-on-chart task, a target identified in the scene must be identified on the electronic chart display.</p></list-item>
<list-item><p><italic>Find-in-scene task</italic>: In the find-in-scene task, a target identified on the chart must be located in the scene.</p></list-item>
<list-item><p><italic>Prediction</italic>: Mariners familiar with the fixed type of display will perform better than inexperienced participants.</p></list-item>
<list-item><p><italic>Prediction</italic>: The handheld display will result in better performance compared to the fixed display, especially benefitting inexperienced users.</p></list-item></list>
<fig id="fig4-0018720812444788" position="float">
<label>Figure 4.</label>
<caption>
<p>A 180Â° panoramic display was used to simulate a scene as perceived from the deck of a vessel.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig4.tif"/></fig>
<fig id="fig5-0018720812444788" position="float">
<label>Figure 5.</label>
<caption>
<p>The panoramic display showing part of San Francisco Bay. The participant is wearing a gaze monitoring camera.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig5.tif"/></fig>
</sec>
<sec id="section12-0018720812444788">
<title>Participants</title>
<p>There were 16 participants, 9 of whom were undergraduate students who were paid for participating; 7 were employees of the Center for Coastal and Ocean Mapping, selected because they had significant experience working on the bridges of ships with various electronic chart displays. Two of these were the captains of launches operated by the center, one had previously been a Coast Guard master helmsman, and the others had less, but still substantial, experience with navigation displays on ships. Unfortunately, there was a perfect correlation between expertise and age. All of the novice participants were younger than 30, and all of the experienced participants were older than 30. Two of the participants were female, and both were in the experienced category.</p>
</sec>
<sec id="section13-0018720812444788">
<title>The Virtual Environment Display</title>
<p>The VE was displayed on a 180Â° custom-made semicircular screen having a radius of 2.15 m. Imagery was produced using a Dell Precision Workstation 690 with twin Nvidia Quadro FX4500 graphics cards. This was projected using four Optoma 1,280 Ã 800 projectors, each providing 45Â° of the visual field.</p>
<p>The computer graphics were generated by means of FlightGear flight simulator software using a model of the San Francisco Bay area. Incorporated into this was a 3D model of the Coastal Surveyor, a research vessel operated by the Center for Coastal and Ocean Mapping. However, only the bow and stern portions were visible in the VE. The bridge structures where eliminated using the near clipping plane. To add realism, the boat was made to pitch 5Â° with a period of 3 s and roll by 2.5Â° with a period of 6.3 s.</p>
</sec>
<sec id="section14-0018720812444788">
<title>Handheld Perspective Display</title>
<p>Our handheld perspective view was displayed using a 5 Ã 7 in., 800 Ã 480 resolution screen. A Polhemus 3Space Fastrak sensor was attached to monitor the screen orientation. The view direction was determined by how the user oriented the device as follows: The perspective view was constructed by raising the virtual viewpoint 10 m above the actual viewpoint of the observer. The width of the viewing frustum was set to 54Â° and the height proportionately at 34Â°. The azimuth, tilt, and roll angles of the view direction were determined by the orientation of a vector orthogonal to the handheld screen. This controlled the attitude of the viewing frustum. The view was updated in real time and depended on how the user was holding the device. Because of its limited field of view, only a subset of the targets was present on the screen. The observers would rotate their body, holding the screen in front of them, to match up parts of the scene with the display.</p>
</sec>
<sec id="section15-0018720812444788">
<title>Fixed Track-Up Display</title>
<p>The track-up display was shown using a 19 in. monitor, 1,280 Ã 1,024, placed on a desk in a location that corresponded to a position in the center of the virtual bridge. The view was defined by the heading of the ship in the VE.</p>
</sec>
<sec id="section16-0018720812444788">
<title>Targets</title>
<p>There were 30 buoys randomly placed at distances between 25 m and 1 km from the virtual vessel, separated from each other by at least 100 m. These consisted of randomly chosen cones, spheres, and cubes, which were randomly colored red, yellow, green, orange, or white. The spheres were 2 m in diameter, and the other shapes were of comparable size. This meant that the most distant targets subtended only about 7 min of arc, and could be difficult, although never impossible, to find on the textured rendering of the sea surface.</p>
<p>The targets used in the task were selected from buoys on the starboard side of the vessel in the field of view of the panoramic virtual display. In addition, no target was closer than 10Â° from either edge of the panoramic display (ahead and astern with respect to the ship).</p>
<p>The corresponding symbols on the chart displays were not colored and were all circular points. This was done so that the task could be performed only using cognitive spatial transformations and not using color or shape cues.</p>
</sec>
<sec id="section17-0018720812444788">
<title>Procedure</title>
<p>The participant stood roughly in the center of the space and used either the track-up fixed chart display or the handheld chart display for a given trial block. The procedure for the two tasks was as follows.</p>
<sec id="section18-0018720812444788">
<title>Find-in-scene</title>
<p>A particular buoy was highlighted on the display (it turned blue). With the handheld display the participants would normally start a trial roughly facing the center of the scene. With the fixed display they would start facing the display (to the right hand side of the scene display). Participants located the buoy in the scene and indicated this to the experimenter, by pointing and naming, for example, âThe red cone.â At this point the experimenter clicked on either a âcorrectâ or an âincorrectâ button. The computer recorded timing and accuracy information.</p>
</sec>
<sec id="section19-0018720812444788">
<title>Find-on-chart</title>
<p>The experimenter pointed to and named a particular target buoy in the scene, and the participants confirmed that they could see it. At this point, the experimenter pushed a button to start the trial. The participants located the corresponding mark on the display and registered it using the touch-screen interface.</p>
</sec>
</sec>
<sec id="section20-0018720812444788">
<title>Design</title>
<p>There were four conditions, consisting of the product of the display condition (either <italic>handheld</italic> or <italic>fixed track-up</italic> display) with the two tasks (<italic>find-in-scene</italic> or <italic>find-on-chart</italic> task). These were given in randomized groups of four, with five trials in each. For each block of five trials, a new random set of targets was generated. The first group of four conditions was considered practice trials, with the experimenter giving feedback and explaining the task as necessary. Following this were three replications, resulting in 30 trials per participant per condition.</p>
</sec>
<sec id="section21-0018720812444788">
<title>Results</title>
<p>The results from Experiment 1 are summarized in <xref ref-type="fig" rid="fig6-0018720812444788">Figures 6</xref> and <xref ref-type="fig" rid="fig7-0018720812444788">7</xref>. Participants responded faster in the <italic>find-on-chart</italic> condition than in the <italic>find-in-scene</italic> condition (9.2 s vs. 12.5 s, respectively), <italic>F</italic>(1, 15) = 20.3, <italic>p</italic> &lt; .001. They also responded faster with the handheld display compared to the fixed display, 9.4 s versus 12.4 s, <italic>F</italic>(1, 15) = 19.9, <italic>p</italic> &lt; .001. There was no effect of expertise on the time to respond. There were no significant interactions.</p>
<fig id="fig6-0018720812444788" position="float">
<label>Figure 6.</label>
<caption>
<p>Average time to respond for Experiment 1. Error bars represent 95% confidence intervals.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig6.tif"/></fig>
<fig id="fig7-0018720812444788" position="float">
<label>Figure 7.</label>
<caption>
<p>Mean errors for Experiment 1.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig7.tif"/></fig>
<p>The error rate obtained with the handheld display (13.0%) was only half that obtained with the fixed display (26.5%). A two-tailed Ï<sup>2</sup> test showed this effect to be significant (<italic>p</italic> &lt; .01). Also, the <italic>find-in-scene</italic> condition yielded fewer errors than the <italic>find-on-chart</italic> condition, 16.2% versus 23.1%, respectively (<italic>p</italic> &lt; .05). A two-way Ï<sup>2</sup> test failed to find a significant interaction.</p>
<p>There was an effect of expertise on errors (<xref ref-type="fig" rid="fig8-0018720812444788">Figure 8</xref>). Novices made more than twice as many errors with the fixed display than did the experts (34.6% vs. 12.7%). But with the handheld display there was no difference between groups (mean error = 12.5%). A two-variable Ï<sup>2</sup> showed this interaction effect to be significant (<italic>p</italic> &lt; .01).</p>
<fig id="fig8-0018720812444788" position="float">
<label>Figure 8.</label>
<caption>
<p>Mean error rates comparing experts and novices.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig8.tif"/></fig>
<p>There was no effect of the angle subtended between the target and the bow direction on the ship, for either the fixed display or the handheld display. The mean time for angles 0Â° to 60Â° was 13.7 s versus 13.8 s for angles 120Â° to 180Â°. There was an effect for the distance of the target: Participants took about 26% longer for the furthest quartile of targets than the closest quartile (<italic>p</italic> &lt; .01). They also made 69% more errors in the more distant quartile, and this difference was significant according to a Ï<sup>2</sup> test (<italic>p</italic> &lt; .05).</p>
<p>Although this information was not explicitly gathered, participants using the handheld display were often observed turning toward the screen, facing the part of the display containing the target, and holding the display in such a way that they could look over the top to see the corresponding part of the scene.</p>
</sec>
<sec id="section22-0018720812444788">
<title>Discussion</title>
<p>As predicted, the results showed that both novices and experts performed the task faster with the handheld display. Also as predicted, experts performed better than novices with the fixed track-up display, but only in terms of errors made. Nonexperts made many fewer errors when using the handheld display, whereas the experts made approximately the same number of errors with both displays. Presumably this can be attributed to the extensive prior experience of the experts with the cognitive operations associated with the fixed display. Comparing these results to prior work by Gunzelmann and colleagues (<xref ref-type="bibr" rid="bibr4-0018720812444788">Gunzelmann, 2008</xref>; <xref ref-type="bibr" rid="bibr5-0018720812444788">Gunzelmann &amp; Anderson, 2006</xref>), it is notable that our average response times are much longer. Although they reported response times in the 2 s to 5 s range, ours were all greater than 10 s. In addition, our error rates were higher. A possible reason for this is that our task may have been more challenging. In their display, a smaller set of targets was used and the map display was highly simplified to closely match the scene display in content. The targets in our scene were much smaller and more difficult to see; more distant targets subtended only about 7 min of arc, and they were viewed against a textured background.</p>
<p>We are unable to explain why the results failed to reveal longer response times for targets more to the aft of the ship for the track-up map. Certainly participants had to turn through a larger angle to look from the display to the scene, yet this failed to influence the results.</p>
</sec>
</sec>
<sec id="section23-0018720812444788">
<title>Experiment 2</title>
<p>We designed our second experiment to better understand how the cognitive process is executed with the fixed and handheld displays under conditions that more accurately simulated bridge operations. In larger vessels there is typically a single display, somewhere near the center of the bridge, and when looking for targets off to the side, bridge officers often move to the bridge wings to get a clearer view, which may be many steps (and seconds) away. If information is held in visual working memory, some of it will necessarily be lost as the result of the additional visual demands from visually guided walking. If information is held in verbal working memory, conversations taking place on the bridge will necessarily cause verbally coded spatial information to be lost.</p>
<p>To better understand the impact of distance between chart display and a view of the environment, we added a condition that simulated the situation where the mariner would have to walk between the chart display in the center of the bridge and the bridge wing to make an observation. Previous research into working memory has used a verbal blocking method to tease out the relative importance of visual and verbal working memory in performing a task (<xref ref-type="bibr" rid="bibr9-0018720812444788">Plumlee &amp; Ware, 2006</xref>; <xref ref-type="bibr" rid="bibr15-0018720812444788">Vogel et al., 2001</xref>). If performance worsens when participants are given a task that occupies verbal working memory, this suggests verbal working memory involvement (participants may be essentially reciting under their breath, the relative locations of targets). If there is no decline in performance, then this suggests that the task relevant information is held in visual working memory, with little verbal working memory involvement.</p>
<p>To better understand the allocation of attention during task performance, we added a camera-based gaze monitoring system that enabled us to determine when participants were looking at the chart display and when they were looking at the environment display.</p>
<sec id="section24-0018720812444788">
<title>Method</title>
<p>This experiment used the <italic>find-in-scene</italic> task only. The procedure was mostly identical to that used in the first experiment, except that in the remote display condition the test participant was required to walk several steps between the fixed chart display and the scene display.</p>
</sec>
<sec id="section25-0018720812444788">
<title>Participants</title>
<p>There were 12 participants in the experiment. Eight were undergraduate students, paid for participating, with no experience with track-up chart displays, at least as used on a ship. Four of the participants were mariners experienced with heads-up chart displays. Two of the experts and two of the novices were female. As with Experiment 1, there was a confounding of expertise and age. All of the experienced participants were older than 30, whereas none of the novice participants were older than 30.</p>
</sec>
<sec id="section26-0018720812444788">
<title>Auditory Monitoring Task</title>
<p>In half of the conditions, participants were required to monitor an audio stream for an artificially generated call sign. This was accomplished using speech synthesis software that generated artificial call codes consisting of a letter from the ICAO phonetic alphabet followed by two digits, for example, âalpha nine threeâ or âdelta one five.â These occurred regularly with a pause varying from 2.5 to 5.0 s in between. The pitch and speed of the calls were randomly varied to simulate different speakers. When participants heard their designated call sign âalpha,â they were required to repeat âalphaâ as an acknowledgment.</p>
</sec>
<sec id="section27-0018720812444788">
<title>Conditions</title>
<p>There were three display conditions: The handheld display and the fixed-near conditions were the same as those used in Experiment 1. To this we added a fixed-far condition as illustrated in <xref ref-type="fig" rid="fig9-0018720812444788">Figure 9</xref>. Barrier screens were added to ensure that it was not possible to simultaneously view the environment display and the chart display.</p>
<fig id="fig9-0018720812444788" position="float">
<label>Figure 9.</label>
<caption>
<p>This diagram shows the relative positions in plan view of the fixed-near and fixed-far chart displays. The barriers blocked the view of the scene display from positions where the fixed-far display could be seen.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig9.tif"/></fig>
<p>There were a total of six experimental conditionsâthe three display conditions carried out each with and without the auditory monitoring task. The six conditions were structured in randomly ordered groups of six, with five trials given in each condition before moving on to the next. As with Experiment 1, the first set of six was for training and was followed by three replications, yielding 15 trials per condition.</p>
</sec>
<sec id="section28-0018720812444788">
<title>Gaze Monitoring System</title>
<p>Our gaze monitoring system consisted of two Logitech HD 7209 web cameras, one mounted in the center of the participantâs forehead, pointing forward. The other was mounted below and to the right of the participantâs right eye, pointing at the eye. This camera was modified by moving its lens outward to bring the eye into sharp focus. The video imagery from the two cameras was captured using a small notebook computer that the participant wore on the small of his or her back, attached by means of Velcro to a heavy belt of a type used by construction workers. Software running on this computer combined frames into a single side-by-side image for recording. Recordings were automatically begun when a trial started and terminated when a trial ended. Separate video files were created for each trial.</p>
<p>This camera system made it possible to determine if a participant was looking at the chart display, at the scene display, or elsewhere. It was not, however, possible to use it to determine which of the individual targets a participant was fixating on at a specific moment.</p>
</sec>
<sec id="section29-0018720812444788">
<title>Results</title>
<p>The results from the second experiment are summarized in <xref ref-type="fig" rid="fig10-0018720812444788">Figures 10</xref> and <xref ref-type="fig" rid="fig11-0018720812444788">11</xref>. Participants responded fastest with the handheld display (7.13 s), followed by the fixed-near display (11.97 s), and slowest with the fixed-far display (18.18 s), <italic>F</italic>(2, 22) = 67.5, <italic>p</italic> &lt; .001. A Tukey honestly significant difference (HSD) test showed significant differences between each of the conditions. There was no significant effect of auditory monitoring and no interaction.</p>
<fig id="fig10-0018720812444788" position="float">
<label>Figure 10.</label>
<caption>
<p>Task performance times from Experiment 2. Error bars represent 95% confidence intervals.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig10.tif"/></fig>
<fig id="fig11-0018720812444788" position="float">
<label>Figure 11.</label>
<caption>
<p>Percentage errors from Experiment 2.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig11.tif"/></fig>
<p>As with Experiment 1, errors were substantially reduced with the handheld device (fixed far = 23%, fixed near = 19%, hand held = 4.6%; <italic>p</italic> &lt; .001, Ï<sup>2</sup>). <xref ref-type="fig" rid="fig11-0018720812444788">Figure 11</xref> suggests a similar pattern of result to Experiment 1, namely, that experts did better in terms of errors with the track-up displays, but in this case the interaction failed to reach significance.</p>
<p>The camera recordings were analyzed by two undergraduate research assistants working independently. Each processed the entire set, to allow for an evaluation of consistency. They were provided with a custom interface that allowed them to view the two video images (from the camera looking out and the camera looking at the participantâs eye) side by side. Their task was to code intervals into three classes: looking at the chart display, looking at the environment display, or looking elsewhere. For the purpose of this analysis we defined a âlookâ to be a continuous interval of time when a participant was examining the chart display, examining the environment display, or looking elsewhere.</p>
<p>The head-mounted cameras sometimes shifted such that the participantâs eye was no longer in the frame. Because of this, 7.7% of the trials were eliminated (the assistants agreed on all but one of these 81 instances). The remaining trials were used for analysis. The <italic>r</italic><sup>2</sup> correlations between the times measured by the assistants for looks at <italic>display, scene</italic>, or <italic>elsewhere</italic> were .98, .98, and .85, respectively. Also, there was exact agreement on the number of looks per trial for <italic>display, scene</italic>, or <italic>elsewhere</italic> in 89%, 94%, and 93% of the cases, respectively. The results we report are averages for the two analyses.</p>
<p>The results from the monitoring of gaze are summarized in <xref ref-type="fig" rid="fig12-0018720812444788">Figure 12</xref>. As <xref ref-type="fig" rid="fig12-0018720812444788">Figure 12a</xref> shows, average look time was almost 3 times longer with the remote fixed display compared to the other two. For each of the display conditions the time spent looking at the scene was roughly the same as the time looking at the chart. An ANOVA on the combined chart and scene results showed this difference to be significant, <italic>F</italic>(2, 22) = 76.9, <italic>p</italic> &lt; .001. A Tukey HSD test showed the fixed-far display to have the longest times (5.23 s), with the other two conditions not significantly different on this measure (fixed near = 1.89 s, hand held = 1.51 s).</p>
<fig id="fig12-0018720812444788" position="float">
<label>Figure 12.</label>
<caption>
<p>Results from view monitoring. (a) The mean time per look. (b) The number of looks per trial. (c) The breakdown of the total time per trial.</p>
</caption>
<graphic xlink:href="10.1177_0018720812444788-fig12.tif"/></fig>
<p>To evaluate if there was an overall difference between the different conditions based on cognitive time (as opposed to walking or head-turning time), we used the view monitoring data to subtract looking <italic>elsewhere</italic> time from the task performance time data. The results still showed a significant, although somewhat smaller, effect of the viewing condition, <italic>F</italic>(2, 22) = 51.7, <italic>p</italic> &lt; .001 (fixed far <italic>M</italic> = 15.7, fixed near <italic>M</italic> = 11.7, hand held <italic>M</italic> = 7.1). A Tukey HSD showed the conditions to be significantly different from one another.</p>
<p>The average number of looks under the different conditions is shown in <xref ref-type="fig" rid="fig12-0018720812444788">Figure 12b</xref>. Averaging looks at the scene and looks at the chart (1.5 for fixed far, 3.1 fixed near, and 2.38 hand held), a Tukey HSD test showed the three conditions to be each distinct from the other.</p>
<p><xref ref-type="fig" rid="fig12-0018720812444788">Figure 12c</xref> shows the breakdown of the average time per trial into three components, the time spent looking at the scene, at the map, and âelsewhere.â In the case of the far condition, the elsewhere category mostly consisted of the time taken walking between the chart display and the scene vantage. It was much smaller in the case of the fixed-near condition. This time would be the time the participant took to turn his or her head from the display to the scene. In the case of the handheld display, the time was often too short to be measured using our methodology.</p>
</sec>
</sec>
<sec id="section30-0018720812444788" sec-type="discussion">
<title>Discussion</title>
<p>Judging where oneâs ship is with reference to charted features of the marine environment, especially navigation buoys, is an essential part of situational awareness and is critical to safe navigation. The results from this study suggest significant benefits in the use of spatially aware handheld chart displays, in terms of both time to make a judgment and accuracy. The benefits were especially marked for novices; those experienced with track-up chart displays did not show a reduction in errors in the first experiment, although they did in the second.</p>
<p>The advantage of handheld displays over fixed displays was found to increase with the physical distance between the fixed chart display and the viewpoint for observing the environment display. It is worth noting that this benefit was more than just the additional time needed to transfer attention (by walking) between the chart display and the environment and can be attributed to cognitive factors.</p>
<p>Experiment 2 revealed that very different cognitive strategies were employed in the different conditions. This was most marked in the case of the fixed-near and fixed-far conditions. With the far condition, participants studied the display, and the scene, for almost 3 times as long, but mostly only switched gaze once, looking once at the chart and once at the scene. In contrast, there were, on average, five gaze transitions (between chart and scene) in the fixed-near condition and three with the handheld devices. These differences suggest adaptations in cognitive strategy when using with the remote display.</p>
<p>Prior work has shown that when people can make rapid eye movements to look back and forth in a comparison task, they adopt a strategy of storing only a single simple visual object in working memory (<xref ref-type="bibr" rid="bibr9-0018720812444788">Plumlee &amp; Ware, 2006</xref>), rather than the four simple objects or patterns that working memory is capable of holding (<xref ref-type="bibr" rid="bibr15-0018720812444788">Vogel et al., 2001</xref>). The results of the present study suggest that this change in strategy also applies to larger temporal scales. Because of the time and effort needed to transfer gaze from the chart to the scene, participants attempted to store as much information as possible in working memory before moving to the scene.</p>
<p>A theoretical explanation for the different strategies can be found in the <italic>soft constraints</italic> hypothesis (<xref ref-type="bibr" rid="bibr2-0018720812444788">Gray, Simms, Fu, &amp; Schoelles, 2006</xref>). According to soft constraints, the allocation of cognitive, perceptual, and motor resources is optimized to minimize the time to perform a task. To minimize time, participants in the fixed-far condition reduced the cost of performance by minimizing the number of trips back and forth between the remote display and the scene. Presumably, to accomplish this they put a much greater burden on working memory, attempting to store everything that is needed before walking to the other display. The result was longer response times and higher error rates.</p>
<p>The differences in errors between the results from the fixed-near and the handheld conditions are more likely because of differences in task difficulty. Prior research (discussed in the introduction) suggests the geometric reasoning needed with a fixed track-up display, when looking to the side, rather than ahead, is more complex and error prone than is the geometric reasoning needed to perform the task with the user-oriented track-up perspective view provided by the handheld display. The interaction effect that occurred between user expertise and display type supports this interpretation; experts, having become skilled at the particular mental operations needed with a fixed track-up display, made fewer errors.</p>
<p>Our participants did not always follow the two stage strategy modeled by <xref ref-type="bibr" rid="bibr5-0018720812444788">Gunzelmann and Anderson (2006)</xref>: first encode target in the scene context, transfer attention to the map, and find corresponding symbol. Instead, they transferred attention back and forth multiple times in two of the three conditions of the second experiment. It seems likely that this difference has largely to do with task difficulty. Our scene was much more visually complex than the one they used, and it had smaller targets (in terms of visual angle), against a background textured to represent water.</p>
<p>Contrary to our expectations, the auditory monitoring task did not have a significant impact on task performance. This suggests that information is mainly held in visual and not verbal working memory during the transitioning of attention between the display and the environment. It is also possible, however, that the monitoring task was less disruptive of information stored in verbal working memory than we had anticipated.</p>
<p>Our overall conclusion is that handheld displays have the potential to reduce errors and overall cognitive load for ship navigators. On a more theoretical level, our results suggest that both geometric reasoning and cognitive strategies should be considered as factors in chart- and map-reading tasks.</p>
</sec>
<sec id="section31-0018720812444788">
<title>Key Points</title>
<list id="list3-0018720812444788" list-type="bullet">
<list-item><p>A handheld chart can be made spatially aware, providing a perspective view that is tailored to the direction in which the user is looking.</p></list-item>
<list-item><p>Handheld charts that provide a perspective view of the environment can make it easier to find points of correspondence between symbols on the chart and features such as buoys in the environment.</p></list-item>
<list-item><p>People adopt very different looking strategies if they have to physically relocate between a map display and a view of the environment, compared to the case where eye movements can be made between the environment and the display. We attribute this to adaptations in cognitive strategies.</p></list-item></list>
</sec>
</body>
<back>
<ack><p>This work was supported by National Oceanic and Atmospheric Administration Grant NA05NOS4001153. We are grateful for advice on the statistical analysis from Ernst Linder and to Kurt Schwehr for helpful design suggestions relating to the apparatus. The Panoramic screen and projector mounting system was designed by Andy McLeod.</p></ack>
<bio>
<p>Colin Ware is director of the Data Visualization Research Lab, which is part of the Center for Coastal and Ocean Mapping at the University of New Hampshire. He is a professor of computer science and cross appointed with ocean engineering. He holds a PhD in experimental psychology (Toronto) and an MMath in computer science (Waterloo). Most of his research involves applying perception to problems in information visualization. He has also has been the lead designer of a number of visualization software products including Fledermaus, GraphVisualizer3D, and GeoZui3D. He has published more than 110 articles and two books on topics relating to visualization, human factors, and interactive techniques.</p>
<p>Roland Arsenault is an information technologist in the Center for Coastal and Ocean Mapping at the University of New Hampshire. He has an undergraduate degree in computer science from the University of New Brunswick. His research interests include the design of interactive displays and data visualization.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Aretz</surname><given-names>A. J.</given-names></name>
</person-group> (<year>1991</year>). <article-title>The design of electronic map displays</article-title>. <source>Human Factors</source>, <volume>33</volume>, <fpage>85</fpage>â<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr2-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gray</surname><given-names>W. D.</given-names></name>
<name><surname>Simms</surname><given-names>C. R.</given-names></name>
<name><surname>Fu</surname><given-names>W. T.</given-names></name>
<name><surname>Schoelles</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The soft constraints hypothesis: A rational analysis approach to resource allocation for interactive behavior</article-title>. <source>Psychological Review</source>, <volume>113</volume>, <fpage>461</fpage>â<lpage>482</lpage>.</citation>
</ref>
<ref id="bibr3-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gugerty</surname><given-names>L.</given-names></name>
<name><surname>Brooks</surname><given-names>J.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Seeing where you are heading: Integrating environmental and egocentric reference frames in cardinal direction judgments</article-title>. <source>Journal of Experimental Psychology: Applied</source>, <volume>7</volume>, <fpage>251</fpage>â<lpage>266</lpage>.</citation>
</ref>
<ref id="bibr4-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gunzelmann</surname><given-names>G.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Strategy generalization across orientation tasks: Testing a computational cognitive model</article-title>. <source>Cognitive Science</source>, <volume>32</volume>, <fpage>835</fpage>â<lpage>861</lpage>.</citation>
</ref>
<ref id="bibr5-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gunzelmann</surname><given-names>G.</given-names></name>
<name><surname>Anderson</surname><given-names>J. R.</given-names></name>
</person-group> (<year>2006</year>). <article-title>location matters: Why target location impacts performance in orientation tasks</article-title>. <source>Memory and Cognition</source>, <volume>34</volume>, <fpage>41</fpage>â<lpage>59</lpage>.</citation>
</ref>
<ref id="bibr6-0018720812444788">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Hermann</surname><given-names>F.</given-names></name>
<name><surname>Bieber</surname><given-names>G.</given-names></name>
<name><surname>Duesterhoeft</surname><given-names>A.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Egocentric maps on mobile devices</article-title>. In <conf-name>Proceedings of the 4th International Workshop on Mobile Computing</conf-name> (pp. <fpage>32</fpage>â<lpage>37</lpage>) <publisher-loc>Stuttgart, Germany</publisher-loc>: <publisher-name>IRB Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr7-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hickox</surname><given-names>J. C.</given-names></name>
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Effect of elevation angle disparity, complexity and feature type on relating out-of-cockpit field of view to an electronic cartographic map</article-title>. <source>Journal of Experimental Psychology: Applied</source>, <volume>5</volume>, <fpage>284</fpage>â<lpage>301</lpage>.</citation>
</ref>
<ref id="bibr8-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Levine</surname><given-names>M.</given-names></name>
<name><surname>Marchon</surname><given-names>I.</given-names></name>
<name><surname>Hanley</surname><given-names>G.</given-names></name>
</person-group> (<year>1984</year>). <article-title>The placement and misplacement of you are here maps</article-title>. <source>Environment and Behavior</source>, <volume>16</volume>, <fpage>139</fpage>â<lpage>157</lpage>.</citation>
</ref>
<ref id="bibr9-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Plumlee</surname><given-names>M.</given-names></name>
<name><surname>Ware</surname><given-names>C.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Cognitive costs of zooming versus using multiple windows</article-title>. <source>ACM Transactions on Applied Perception</source>, <volume>13</volume>(<issue>2</issue>), <fpage>1</fpage>â<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr10-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rayner</surname><given-names>K.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Eye movements in reading and information processing: 20 years of research</article-title>. <source>Psychological Bulletin</source>, <volume>124</volume>, <fpage>372</fpage>â<lpage>422</lpage>.</citation>
</ref>
<ref id="bibr11-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schall</surname><given-names>G.</given-names></name>
<name><surname>Mendez</surname><given-names>E.</given-names></name>
<name><surname>Kruijff</surname><given-names>E.</given-names></name>
<name><surname>Veas</surname><given-names>E.</given-names></name>
<name><surname>Junghanns</surname><given-names>S.</given-names></name>
<name><surname>Reitinger</surname><given-names>B.</given-names></name>
<name><surname>Schmalstieg</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Handheld augmented reality for underground infrastructure visualization</article-title>. <source>Personal and Ubiquitous Computing</source>, <volume>13</volume>, <fpage>281</fpage>â<lpage>291</lpage>.</citation>
</ref>
<ref id="bibr12-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schreiber</surname><given-names>B.</given-names></name>
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
<name><surname>Alton</surname><given-names>J.</given-names></name>
<name><surname>Renner</surname><given-names>G.</given-names></name>
<name><surname>Hickox</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Navigational checking using 3D maps: The influence of elevation angle, azimuth and foreshortening</article-title>. <source>Human Factors</source>, <volume>40</volume>, <fpage>209</fpage>â<lpage>223</lpage>.</citation>
</ref>
<ref id="bibr13-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shepard</surname><given-names>R. N.</given-names></name>
<name><surname>Hurwitz</surname><given-names>S.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Upward direction mental rotation and discrimination of left and right</article-title>. <source>Cognition</source>, <volume>18</volume>, <fpage>161</fpage>â<lpage>193</lpage>.</citation>
</ref>
<ref id="bibr14-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Suomela</surname><given-names>R.</given-names></name>
<name><surname>Roimela</surname><given-names>K.</given-names></name>
<name><surname>Lehikoinene</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>The evolution of perspective view in WalkMap</article-title>. <source>Personal and Ubiquitous Computing</source>, <volume>7</volume>, <fpage>249</fpage>â<lpage>262</lpage>.</citation>
</ref>
<ref id="bibr15-0018720812444788">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vogel</surname><given-names>E. K.</given-names></name>
<name><surname>Woodman</surname><given-names>G. F.</given-names></name>
<name><surname>Luck</surname><given-names>S. J.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Storage of features, conjunctions and objects in visual working memory</article-title>. <source>Journal of Experimental Psychology, Human Perception and Performance</source>, <volume>27</volume>, <fpage>92</fpage>â<lpage>194</lpage>.</citation>
</ref>
<ref id="bibr16-0018720812444788">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Frames of reference for navigation</article-title>. In <person-group person-group-type="editor">
<name><surname>Gopher</surname><given-names>D.</given-names></name>
<name><surname>Koriat</surname><given-names>A.</given-names></name>
</person-group> (Eds.), <source>Attention and performance</source> (<volume>Vol. 16</volume>, pp. <fpage>113</fpage>â<lpage>144</lpage>). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>