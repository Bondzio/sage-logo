<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JIS</journal-id>
<journal-id journal-id-type="hwp">spjis</journal-id>
<journal-title>Journal of Information Science</journal-title>
<issn pub-type="ppub">0165-5515</issn>
<issn pub-type="epub">1741-6485</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0165551512439173</article-id>
<article-id pub-id-type="publisher-id">10.1177_0165551512439173</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Content-based analysis to detect Arabic web spam</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Al-Kabi</surname><given-names>Mohammed</given-names></name>
<aff id="aff1-0165551512439173">Yarmouk University, Jordan</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Wahsheh</surname><given-names>Heider</given-names></name>
<aff id="aff2-0165551512439173">Yarmouk University, Jordan</aff>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Alsmadi</surname><given-names>Izzat</given-names></name>
<aff id="aff3-0165551512439173">Yarmouk University, Jordan</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Al-Shawakfa</surname><given-names>Emad</given-names></name>
<aff id="aff4-0165551512439173">Yarmouk University, Jordan</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Wahbeh</surname><given-names>Abdullah</given-names></name>
<aff id="aff5-0165551512439173">Dakota State University, USA</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Al-Hmoud</surname><given-names>Ahmed</given-names></name>
<aff id="aff6-0165551512439173">Yarmouk University, Jordan</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0165551512439173">Izzat Alsmadi, CIS Department, Yarmouk University, P.O. Box 566, 21163 Irbid, Jordan. Email: <email>ialsmadi@yu.edu.jo</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2012</year>
</pub-date>
<volume>38</volume>
<issue>3</issue>
<fpage>284</fpage>
<lpage>296</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">Chartered Institute of Library and Information Professionals</copyright-holder>
</permissions>
<abstract>
<p>Search engines are important outlets for information query and retrieval. They have to deal with the continual increase of information available on the web, and provide users with convenient access to such huge amounts of information. Furthermore, with this huge amount of information, a more complex challenge that continuously gets more and more difficult to illuminate is the spam in web pages. For several reasons, web spammers try to intrude in the search results and inject artificially biased results in favour of their websites or pages. Spam pages are added to the internet on a daily basis, thus making it difficult for search engines to keep up with the fast-growing and dynamic nature of the web, especially since spammers tend to add more keywords to their websites to deceive the search engines and increase the rank of their pages. In this research, we have investigated four different classification algorithms (naïve Bayes, decision tree, SVM and K-NN) to detect Arabic web spam pages, based on content. The three groups of datasets used, with 1%, 15% and 50% spam contents, were collected using a crawler that was customized for this study. Spam pages were classified manually. Different tests and comparisons have revealed that the Decision Tree was the best classifier for this purpose.</p>
</abstract>
<kwd-group>
<kwd>Arabic content features</kwd>
<kwd>Arabic web spam</kwd>
<kwd>Arabic web spam detection</kwd>
<kwd>content features</kwd>
<kwd>web spam</kwd>
<kwd>web spam detection</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0165551512439173" sec-type="intro">
<title>1. Introduction</title>
<p>The web contains vast amount of information, which makes it the largest reservoir of information around the globe. This reservoir is characterized by its dynamic nature and expandability. Each day a huge number of new web pages are added to this reservoir in different natural languages, and in addition to these text-based web pages, other types of file (images, audio, video, etc.) are also added to this pool.</p>
<p>The Arabic language is the official language of 22 countries, and the fifth most spoken language. It is one of the official languages of the United Nations. It is one of the Semitic languages, so it is written from right to left. It is based on 28 letters, and these letters are adopted by other languages, such as Urdu, Persian, Malay, Pashto and Swahili. The Arabic language uses the diacritical marks (<italic>harakaat, tashkiil</italic>) in holy books and poetry, in addition to the 28 alphabetic characters. These diacritical marks are considered as vowels (<italic>harakaat</italic>, ‘حَركات’) to change the voice from one consonant to another [<xref ref-type="bibr" rid="bibr1-0165551512439173">1</xref>].</p>
<p>Arab internet users are found mainly in the Middle East and North Africa (MENA); they constitute around 5% of the world’s population, and around 3.3% of the world’s internet users. Arabic web materials do not exceed 1%, and 35% of the Arabic content is published within blogs and forums compared with no more than 10% in other natural languages, so the number of web pages in Arabic with valuable information is small, relative to a low Arab contribution in general, but there is a high percentage of contribution within blogs and forums, where the percentage of Arab contribution exceeds its counterparts in other natural languages by a factor of 3.5 [<xref ref-type="bibr" rid="bibr2-0165551512439173">2</xref>]. This means that more than a third of Arabic web content is unstructured, and lacks quality.</p>
<p>One of the main problems facing web search engines in MENA is the lack of a large number of Arabic web pages with valuable information, and this is clear within the free encyclopaedia (Wikipedia) that enables internet users to create and edit different articles, where the Arab contribution does not exceed 1% at best.</p>
<p>Gyongyi et al. [<xref ref-type="bibr" rid="bibr3-0165551512439173">3</xref>] defined web spam as hyperlinked web pages placed on the world wide web (WWW) with the intention of deceiving the various search engines to assign these pages higher ranks within their results pages. There are two main types of web spam: content spam and link spam. Link spam web pages have irrelevant and misleading hyperlinks, and content spam web pages have irrelevant and misleading terms. Web spamming usually annoys search engine users, and harm the credibility of web search engines. Therefore the search engine may lose a substantial proportion of its users, and search engine users may fail to get the relevant answers (web pages), since these receive a low rank while the spam web pages get a high rank [<xref ref-type="bibr" rid="bibr4-0165551512439173">4</xref><xref ref-type="bibr" rid="bibr5-0165551512439173"/><xref ref-type="bibr" rid="bibr6-0165551512439173"/><xref ref-type="bibr" rid="bibr7-0165551512439173"/>–<xref ref-type="bibr" rid="bibr8-0165551512439173">8</xref>], and most search engine users usually do not view more than the top 10 results.</p>
<p>The internet is characterized by its dynamic nature, by its ease of use, and by its allowing users to express themselves via blogs, forums, personal websites and the like. These characteristics make the task of differentiating between spam web pages and non-spam web pages more difficult. A study by Ntoulas et al. [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>] shows that 13.8% of English web pages were classified as web spam pages, and that the accuracy of their automatic classifier in detecting spammed web pages ranged between 82% and 86%. This percentage has to be higher when we consider Arabic web spam pages, since 35% of the Arabic content lies inside blogs and forums, which are characterized by their low quality, and many of these adopt different spamming techniques to mislead the various search engines. Web users who are searching for a specific topic using Arabic terms may face much more of a problem of web spam, relative to the use of English terms.</p>
<p>Web spam can be based on techniques such as link stuffing, keyword stuffing, cloaking, or web farming [<xref ref-type="bibr" rid="bibr4-0165551512439173">4</xref>]. It can be beneficial to commercial websites, since an increase in search engine referrals will lead to an increase in sales and revenues. Commercial websites aim to improve their rank within web search engine result pages: therefore they have to use either web spamming, which is considered unethical, or search engine optimization (SEO), which is based on various techniques aiming to improve the contents of web pages to satisfy the different navigators to these pages. SEO is considered an ethical way to improve the ranking of web pages [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>].</p>
<p>Web spam, or spamdexing, represents one of the main topics of adversarial information retrieval, which takes various forms that challenge web search engines: content manipulation, link creation and manipulation, cloaking, click fraud, and tag spam [<xref ref-type="bibr" rid="bibr10-0165551512439173">10</xref>].</p>
<p>This study aims to detect Arabic spammed web pages using content-based analysis. The available corpus of Arabic web spam pages is small, so we aimed to collect around 15,000 Arabic spammed web pages, and identify their attributes. These attributes are then analysed to detect spammed web pages.</p>
</sec>
<sec id="section2-0165551512439173">
<title>2. Related work</title>
<p>Many studies have been conducted to explore different techniques to discover web spam. The related work is categorized into three sections: non-Arabic web spam studies, web spam detection surveys, and earlier Arabic web spam research papers.</p>
<sec id="section3-0165551512439173">
<title>2.1. Non-Arabic web spam detection</title>
<p>There are many studies that could be presented within this section, but since our study is not a literature survey, we have selected only some of these studies. This section is divided into four subsections according to the approach adopted by each of the presented papers in this section.</p>
<sec id="section4-0165551512439173">
<title>2.1.1. Non-Arabic, content-based web spam detection</title>
<p>Five studies related to content-based web spam detection are presented in this subsection.</p>
<p>Svore et al. [<xref ref-type="bibr" rid="bibr11-0165551512439173">11</xref>] performed a study that was divided into two parts. Dataset construction is studied in the first, since it is crucial to the accuracy of the web spam classifier when there is no overlapping between the test and training sets. The second part of their paper covers a novel approach called rank-time features, which improves the performance of the web spam classifier over content-based web spam classifiers. According to the authors, their study was the first to refer to rank-time features, especially query-dependent rank-time features.</p>
<p>Another novel technique for spam detection was presented by Pera et al. [<xref ref-type="bibr" rid="bibr12-0165551512439173">12</xref>]. This technique was based on phrases’ enhanced similarity measures in the title and body of web pages, as well as on the fractional presence of hidden text. Evaluation tests have showed that the accuracy of this technique is around 94%, and it has performed better than existing anti-spam methods, by an average of 10%, in terms of <italic>F</italic>-measure. The researchers have also noticed that the use of bigrams has significantly upgraded the performance of their spam-detection techniques.</p>
<p>Ntoulas et al. [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>] have conducted a study to discover artificially created spam web pages that affect the ranks of different URLs displayed within search engine results pages, using content-based analysis. A real dataset of web pages was used, and a combination of heuristic methods for detecting content-based spam was also used to create a highly accurate C4.5 classifier with an accuracy of 86.2% for detecting spam web pages.</p>
<p>A study by Martinez-Romo et al. [<xref ref-type="bibr" rid="bibr13-0165551512439173">13</xref>] was based on two datasets (Webspam-Uk2006 and Webspam-Uk2007), and applied a language model approach to the extracted information from web pages to differentiate clearly between spam and non-spam web pages. The topicality relation between two linked web pages was also tested through Kullback–Leibler divergence. Various new features, based on language models (statistical language model, SLM), were proposed and adopted to enhance the effectiveness of the spam-detection process. The adoption of such a model has revealed an enhancement range from 2% to 6%.</p>
<p>An analysis of current anti-spam methods (detection and prevention) in Web 2.0 through a comprehensive framework was presented by Hayati et al. [<xref ref-type="bibr" rid="bibr14-0165551512439173">14</xref>]. The detection strategy was based on analysing the content of the web page, and the prevention strategy was based on methods that stop spammers from uploading spam web pages to the web server. The proposed framework evaluated anti-spam methods from different viewpoints. The proposed framework in [<xref ref-type="bibr" rid="bibr14-0165551512439173">14</xref>] has revealed the need for additional, robust methods that are prevention based, are unsupervised, and do not increase user–system interaction complexity. The study showed the drawbacks of current literature methods.</p>
</sec>
<sec id="section5-0165551512439173">
<title>2.1.2. Non-Arabic link-based web spam detection</title>
<p>This subsection presents two studies related to link-based web spam detection. Abernethy et al. [<xref ref-type="bibr" rid="bibr15-0165551512439173">15</xref>] have presented a learning algorithm called Witch, to identify spam web pages. Their algorithm exploits links, web graph, and content. The authors claim that benchmarking of their algorithm has revealed state-of-the-art accuracy to classify different web pages. Also, their algorithm has performed well, even with little training. Using SVM with graph regularization has yielded highly accurate results to detect spam web pages, thus outperforming all other state-of-the-art methods implemented by the researchers of [<xref ref-type="bibr" rid="bibr15-0165551512439173">15</xref>].</p>
<p>Liang et al. [<xref ref-type="bibr" rid="bibr16-0165551512439173">16</xref>] have presented a new algorithm called R-SpamRank, to identify potential spam web pages, especially those existing in a link farm. First, the researchers manually selected a small number of spam web pages as a seed for the proposed algorithm. The evaluation test of R-SpamRank was based on a dataset of 5 million web pages, where the test was performed on the top 10,000 web pages with the highest R-SpamRank values. It was found that 91.1% of the 10,000 web pages tested were spammed web pages. The accuracy of this novel web spam-detection algorithm was found to be 91.1%.</p>
</sec>
<sec id="section6-0165551512439173">
<title>2.1.3. Non-Arabic hybrid approach web spam detection</title>
<p>The hybrid approach, which mixes content and link attributes to identify spam web pages, has been adopted by several studies. In this subsection only two studies are presented.</p>
<p>Araujo et al. [<xref ref-type="bibr" rid="bibr17-0165551512439173">17</xref>] have presented a new web spam detection system based on building two suspicious groups according to the method used to detect them. Detection of the first group was based on qualified link (QL) analysis. Detection of the second group was based on content, using an extended language model. A classifier that uses the features was used to detect web spam pages among the two suspicious groups, and this led to an average improvement of 10% in the results.</p>
<p>Castillo et al. [<xref ref-type="bibr" rid="bibr18-0165551512439173">18</xref>] have presented a pioneering study that merged link-based analysis, content-based analysis, and the topology of the web graph to detect a web spam. The researchers have exploited the idea that spammers link to each other, and they use the same features found in other studies, such as [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>]. The authors believe that their study is the first to combine link-based and content-based features. They have found that linked hosts tend to belong to the same class (spam or non-spam). The best classifier presented in the study has yielded an accuracy of 88.4%, with 6.3% false positives.</p>
</sec>
<sec id="section7-0165551512439173">
<title>2.1.4. Non-Arabic web spam detection using machine learning</title>
<p>Machine learning represents an emerging discipline used in many fields, and one of these is the detection of spam web pages. This subsection presents three studies that have adopted machine learning to identify such pages.</p>
<p>In their study, Geng et al. [<xref ref-type="bibr" rid="bibr19-0165551512439173">19</xref>] have viewed web spam detection as a binary classification problem, in which an ensemble classification strategy is proposed. Spam websites and pages are harder to find than reputable websites or pages. In addition, the percentage of spam websites and pages within the web is much lower than that of their counterpart. Geng et al.’s study aimed to exploit fully the information contained within reputable websites and pages. Three learning algorithms (C4.5, Bagging, and Adaboost) were applied within the proposed ensemble classification strategy. Experiments have shown that the proposed strategy has improved web spam detection effectively.</p>
<p>Dai et al. [<xref ref-type="bibr" rid="bibr20-0165551512439173">20</xref>] used a variety of historical content features to detect spam web pages. The researchers used archived copies of web pages to extract a variety of features. Supervised learning techniques, used in machine learning to produce an inferred function (classifier), were used in their study to combine current page content classifiers with temporal feature classifiers. Tests showed that <italic>F</italic>-measure performance improved by 30% relative to the baseline classifier.</p>
<p>A study by Niu et al. [<xref ref-type="bibr" rid="bibr21-0165551512439173">21</xref>] has presented a discriminating function with the ability to keep on learning by using genetic programming to detect web spam. The use of such an evolving discriminating function will enable the web spam classifier to adapt itself to evolving new web spam techniques. The proposed function uses a number of small-scale individual populations, and takes the best individuals in each population to obtain the best possible discriminating function. Tests on the proposed function using the well-known WEBSPAM-UK2006 collection show improvements in recall, <italic>F</italic>-measure and accuracy of 26%, 11% and 4% respectively. The research was restricted to link-based features; content-based features were discarded.</p>
</sec>
</sec>
<sec id="section8-0165551512439173">
<title>2.2. Surveys of web spam detection</title>
<p>Wang et al. [<xref ref-type="bibr" rid="bibr6-0165551512439173">6</xref>] have presented a novel content trust model for spam detection of textual web pages. A series of experiments were conducted on two real datasets of web pages (English and Chinese). The conducted tests proved the effectiveness of their proposed model. Wang and Zeng’s [<xref ref-type="bibr" rid="bibr7-0165551512439173">7</xref>] study overlaps with the studies conducted by Wang et al. [<xref ref-type="bibr" rid="bibr6-0165551512439173">6</xref>, <xref ref-type="bibr" rid="bibr8-0165551512439173">8</xref>], which are also related to the content trust model for spam detection. Evaluation of the proposed model has yielded good results on the crawled datasets used. Tests have shown that this model can be easily fooled by spammers, and thus needs to be enhanced for better results.</p>
<p>A study by Fetterly et al. [<xref ref-type="bibr" rid="bibr22-0165551512439173">22</xref>] was concerned with the evolution of different web pages over time; the first three authors decided to develop their study after they found within their datasets a large number of machine-generated spam web pages. Fetterly et al. [<xref ref-type="bibr" rid="bibr23-0165551512439173">23</xref>] therefore conducted a study to discover machine-generated spam web pages that contained well-formed German sentences extracted from a large collection of German sentences. Their study has developed techniques that can be used to discover spam web pages based on the replication of phrases.</p>
<p>Statistical analysis was used within Fetterly et al.’s [<xref ref-type="bibr" rid="bibr24-0165551512439173">24</xref>] study to identify machine-generated spam web pages. Their study was based on two large datasets, of 150 million and 429 million HTML web pages respectively, in addition to 38 million HTTP redirects. The analysis was based on various properties that have helped to differentiate spam from non-spam web pages.</p>
<p>Benczúr et al. [<xref ref-type="bibr" rid="bibr25-0165551512439173">25</xref>] reported the importance of web archive quality against three types of web spam: content spam, link spam and cloaking spam. Their study provided an architecture to facilitate cooperation between web archives in different domains and countries to detect spam effectively. This method has shown good results over 100,000 pages of the WEBSPAM-UK2007 dataset.</p>
<p>Spirin et al. [<xref ref-type="bibr" rid="bibr26-0165551512439173">26</xref>] surveyed various spam forms, techniques and algorithms created to detect web spam. Their study categorized existing algorithms into three categories, based on the type of information they use: content-based methods, link-based methods, and methods based on non-traditional data such as user behaviour and HTTP sessions. The anti-spam algorithms yielded a competitive improvement, approximating 90% in web spam detection.</p>
</sec>
<sec id="section9-0165551512439173">
<title>2.3. Arabic web spam researches</title>
<p>In their study, Wahsheh et al. [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>] manually built an Arabic web spam dataset that consists of 400 spam Arabic web pages. Three classifiers were tested: decision tree, naïve Bayes, and K-nearest neighbour (K-NN). Their study showed that K-NN performed better than the other two classifiers in detecting Arabic web spam pages.</p>
<p>The study of Jaramh et al. [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>] followed that of Wahsheh et al. [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>], but it presented new features to enhance the effectiveness of the web spam classifiers. The study also used three classifiers (decision tree, naïve Bayes, and LogitBoost), and the results showed that the decision tree classifier was the best.</p>
<p>Al-Kabi et al. [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>] conducted a study on an extended content-based spam dataset, with new detection features additional to those of [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>] and [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>].Three classification techniques – decision tree, LogitBoost, and support vector machine (SVM) – were used to detect Arabic web spam. Test results showed that the decision tree classifier was better than the other two content-based web spam detection classifiers; it yielded good results, with an accuracy of 99.3462% and an error rate of 0.6538%.</p>
</sec>
</sec>
<sec id="section10-0165551512439173">
<title>3. Proposed framework</title>
<p><xref ref-type="fig" rid="fig1-0165551512439173">Figure 1</xref> presents the proposed framework for the approach adopted in this study to detect Arabic web spam. It comprises the following main procedures:</p>
<fig id="fig1-0165551512439173" position="float">
<label>Figure 1.</label>
<caption><p>General framework of the adopted approach.</p></caption>
<graphic xlink:href="10.1177_0165551512439173-fig1.tif"/>
</fig>
<list id="list1-0165551512439173" list-type="order">
<list-item><p>Building the dataset – this was performed through the use of a crawler built by Alsmadi [<xref ref-type="bibr" rid="bibr30-0165551512439173">30</xref>], and customized for the purposes of this study. Arabic web pages were collected, and were then manually classified as either spam or non-spam web pages.</p></list-item>
<list-item><p>Extracting the features from Arabic web spam dataset using the developed web analyser.</p></list-item>
<list-item><p>Using the four classifying algorithms (K-NN, SVM, decision tree, and naïve Bayes, which are supported by KNIME software) on our dataset, and then comparing the results to identify the best technique for detecting Arabic spammed web pages.</p></list-item>
</list>
<sec id="section11-0165551512439173">
<title>3.1. Arabic spam dataset</title>
<p>The study of Arabic web spam detection is still at its early stages, owing to the lack of studies and suitable datasets. The datasets used by [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>], [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>] and [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>] were not large enough, since they were created manually. Therefore the authors of this study have used a web crawler developed by Alsmadi to collect the dataset [<xref ref-type="bibr" rid="bibr30-0165551512439173">30</xref>]. The crawler was also extended to include spam detection algorithms and processing methods.</p>
<p>The crawler used an open source web test automation library (<ext-link ext-link-type="uri" xlink:href="http://WatiN.org">WatiN.org</ext-link>) to automatically parse through websites and collect all their web page elements. In some cases, the inner content of web pages was needed, because several web applications (including search engines as well as spammers) can use inner text to hide their text. It can be hard to detect such text, particularly for automatic information retrieval tools.</p>
<p>In this research the authors have collected a dataset consisting of 15,000 Arabic web pages. This was split into three groups with spam percentages of 1%, 15%, and 50% relative to the total number of web pages in the dataset. The authors have also built a web page analyser to extract the new proposed features from the collected web pages based on their contents.</p>
<p>During the collection of web pages, the authors have relied on Google top searches in the Arab world presented by Al-Eroud et al. [<xref ref-type="bibr" rid="bibr31-0165551512439173">31</xref>] Examples of such words include ‘Facebook’ (فيس بوك), words of ‘songs’ (أغاني), ‘chat’ (دردشه, شات), ‘YouTube’ (يوتيوب), and ‘games’ (العاب). The researchers of [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>] used the ‘Google Insights for Search’ web page to obtain the top searches (keywords) in the Arab world; these were used in this study as seeds to collect the spam web pages. After collecting the web pages, the content of each one was manually checked to identify spam from non-spam web pages. This identification was based mainly on the existence of web page content that conflicted with the page’s title, as well as the existence of meaningless English words, the letters of which lie on the same keyboard keys as the letters of the Arabic top used words. Therefore the lengths of these meaningless English words are always equal to the lengths of Arabic top used words. Arabic spammers aim to make their web pages rank higher by adding such meaningless English words, but from our point of view these lead to deterioration of the quality of the Arabic content, and therefore such web pages are treated as spam web pages.</p>
<p><xref ref-type="fig" rid="fig2-0165551512439173">Figure 2</xref> shows a manually checked Arabic spam web page. It is an example of content-based web spam in Arabic; the content contains Arabic words beside some meaningless English words generated by pressing the same keys holding the Latin letters of the keyboard used to display the Arabic word. <xref ref-type="table" rid="table1-0165551512439173">Table 1</xref> shows the number of retrieved pages for some of the Arabic queries [<xref ref-type="bibr" rid="bibr31-0165551512439173">31</xref>] that were used to build the Arabic web spam dataset.</p>
<fig id="fig2-0165551512439173" position="float">
<label>Figure 2.</label>
<caption><p>Keyword stuffing within Arabic–English characters: web spam example.</p></caption>
<graphic xlink:href="10.1177_0165551512439173-fig2.tif"/>
</fig>
<table-wrap id="table1-0165551512439173" position="float">
<label>Table 1.</label>
<caption><p>Number of retrieved pages for Arabic queries [<xref ref-type="bibr" rid="bibr31-0165551512439173">31</xref>]</p></caption>
<graphic alternate-form-of="table1-0165551512439173" xlink:href="10.1177_0165551512439173-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Arabic query</th>
<th align="left">Translated query</th>
<th align="left">Google results</th>
</tr>
</thead>
<tbody>
<tr>
<td>العاب</td>
<td>Games</td>
<td>99,700,000</td>
</tr>
<tr>
<td>العاب بنات</td>
<td>Girls’ games</td>
<td>17,800,000</td>
</tr>
<tr>
<td>العاب فلاش</td>
<td>Flash games</td>
<td>12,700,000</td>
</tr>
<tr>
<td>اغاني</td>
<td>Songs</td>
<td>47,800,000</td>
</tr>
<tr>
<td>اليوتيوب</td>
<td>YouTube</td>
<td>4,330,000</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section12-0165551512439173">
<title>3.2. Features of web pages</title>
<p><xref ref-type="fig" rid="fig3-0165551512439173">Figure 3</xref> shows the algorithm that was developed in this study to extract features from web pages depending on their content. The analyser was capable of extracting the features mentioned in [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>] and [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>], plus a few new ones.</p>
<fig id="fig3-0165551512439173" position="float">
<label>Figure 3.</label>
<caption><p>The developed web analyser algorithm.</p></caption>
<graphic xlink:href="10.1177_0165551512439173-fig3.tif"/>
</fig>
<p>The proposed features used to detect Arabic web spam are as follows:</p>
<list id="list2-0165551512439173" list-type="bullet">
<list-item><p>The number of characters in the &lt;title&gt; elements of web pages, since spammers use many unrelated characters in the &lt;title&gt; element to affect the weight of the title in the process of ranking web pages within web search engine results pages.</p></list-item>
<list-item><p>The number of words in the &lt;meta&gt; element of each web page. Some search engines, such as Google, neglect &lt;meta&gt; elements completely, but others include them in their ranking process. Therefore spammers still use keyword-stuffing methods to increase the possibility of increasing the ranking of their spammed web pages.</p></list-item>
<list-item><p>The number of &lt;meta&gt; elements within each web page. Spammers may use many &lt;meta&gt; elements to describe their spam information about web pages.</p></list-item>
<list-item><p>The number of popular words in web pages. It is known that the repetition of popular words in different locations of the web pages affects their ranking, and it is a good method for deceiving both search engines and users.</p></list-item>
<list-item><p>Identification of the longest word and the shortest word is helpful in detecting Arabic web spam. Spammers try to use long words to raise their web page rankings.</p></list-item>
<list-item><p>The number of characters in web page URLs. Spammers add keywords to the URLs of their web pages to increase their rankings within different search engines, and therefore the URLs of these web pages will be long.</p></list-item>
<list-item><p>The number of URLs in the web page. Spammers try to insert numerous URLs in their web pages, to lure users to other spam web pages.</p></list-item>
</list>
<p>Investigating Arabic spam web pages has led to the discovery of various features that are particularly related to Arabic web spam. The weights of the following four content-based features are higher than their counterparts’ weights for purely English spam web pages:</p>
<list id="list3-0165551512439173" list-type="order">
<list-item><p>the number of characters, or words, in the &lt;title&gt; element;</p></list-item>
<list-item><p>the number of characters, or words, in the &lt;meta&gt; element;</p></list-item>
<list-item><p>the number of popular words in the pages; and</p></list-item>
<list-item><p>the number of characters, or words, in the &lt;body&gt; element.</p></list-item>
</list>
<p>The above features are based on keyword-stuffing, a method used to increase the ranking of spammed web pages. The keyword-stuffing method used in Arabic web spam is slightly different from that for Latin-based languages: Arabic keywords, as well as words created using the matching Latin letters on the keyboard to produce some meaningless words, are inserted into the &lt;meta&gt;, &lt;title&gt; and &lt;body&gt; elements of the page. For instance, the Arabic keyword ‘أغاني’ and its matching, meaningless word ‘Hyhkd’ are inserted into the document.</p>
<p><xref ref-type="fig" rid="fig4-0165551512439173">Figure 4</xref> shows examples of Arabic web spam pages, based on one of the top keywords, ‘chat’ (دردشه, شات), where the spammers have used keyword stuffing.</p>
<fig id="fig4-0165551512439173" position="float">
<label>Figure 4.</label>
<caption><p>Keyword stuffing: web spam example.</p></caption>
<graphic xlink:href="10.1177_0165551512439173-fig4.tif"/>
</fig>
</sec>
<sec id="section13-0165551512439173">
<title>3.3. Algorithms for detecting Arabic web spam</title>
<p>KNIME has produced many classification algorithms that can be used to predict whether a given web page is spam or non-spam. We used the following four classification algorithms to detect Arabic web spam: naïve Bayes, decision tree, SVM and K-NN.</p>
<sec id="section14-0165551512439173">
<title>3.3.1. Naïve Bayes</title>
<p>The naïve Bayes algorithm is a probabilistic classifier based on applying Bayes’ theorem. It requires a small amount of training data to estimate the parameters (means and variances of the variables) needed for the classification process. In this study, the accuracy of the results obtained for the three dataset groups (as shown in <xref ref-type="table" rid="table2-0165551512439173">Table 2</xref>) was 99.92%, 99.10% and 76.26% respectively.</p>
<table-wrap id="table2-0165551512439173" position="float">
<label>Table 2.</label>
<caption><p>Naïve Bayes results</p></caption>
<graphic alternate-form-of="table2-0165551512439173" xlink:href="10.1177_0165551512439173-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset group</th>
<th align="left">Percentage of spam web pages</th>
<th align="left">Correct classification</th>
<th align="left">Incorrect classification</th>
<th align="left">Accuracy</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1%</td>
<td>2,640</td>
<td>2</td>
<td>99.9%</td>
<td>0.08%</td>
</tr>
<tr>
<td>2</td>
<td>15%</td>
<td>2,643</td>
<td>24</td>
<td>99.1%</td>
<td>0.9%</td>
</tr>
<tr>
<td>3</td>
<td>50%</td>
<td>4,616</td>
<td>437</td>
<td>76.3%</td>
<td>23.7%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The results shown in <xref ref-type="table" rid="table2-0165551512439173">Table 2</xref> indicate that the different percentages of spam in the three datasets have a significant impact on the accuracy of the naïve Bayes classifier results.</p>
</sec>
<sec id="section15-0165551512439173">
<title>3.3.2. Decision tree</title>
<p>The decision tree algorithm given in KNIME was used. The web pages within our dataset were divided into two groups: a test set and a training set. The training dataset was used to build a tree of rules which were then used to classify web pages in the test set as either spam or non-spam, depending on their features. The results obtained for the three dataset groups (as shown in <xref ref-type="table" rid="table3-0165551512439173">Table 3</xref>) were 99.9621%, 99.9625% and 99.521% respectively.</p>
<table-wrap id="table3-0165551512439173" position="float">
<label>Table 3.</label>
<caption><p>Decision tree results</p></caption>
<graphic alternate-form-of="table3-0165551512439173" xlink:href="10.1177_0165551512439173-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset group</th>
<th align="left">Percentage of spam web pages</th>
<th align="left">Correct classification</th>
<th align="left">Incorrect classification</th>
<th align="left">Accuracy</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1%</td>
<td>2,641</td>
<td>1</td>
<td>99.96%</td>
<td>0.04%</td>
</tr>
<tr>
<td>2</td>
<td>15%</td>
<td>2,666</td>
<td>1</td>
<td>99.96%</td>
<td>0.04%</td>
</tr>
<tr>
<td>3</td>
<td>50%</td>
<td>6,024</td>
<td>29</td>
<td>99.5%</td>
<td>0.48%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The results show that the decision tree classifier results were slightly affected by the different percentages of spam web pages in the three datasets.</p>
</sec>
<sec id="section16-0165551512439173">
<title>3.3.3. Support vector machine (SVM)</title>
<p>SVM is a method used for classification and regression analysis. It is based on the concept of decision planes, which define decision boundaries, and separate a set of objects with different class memberships. In this study, the results obtained for the three dataset groups (as shown in <xref ref-type="table" rid="table4-0165551512439173">Table 4</xref>) were 97.8425%, 97.3003% and 81.865% respectively.</p>
<table-wrap id="table4-0165551512439173" position="float">
<label>Table 4.</label>
<caption><p>SVM results</p></caption>
<graphic alternate-form-of="table4-0165551512439173" xlink:href="10.1177_0165551512439173-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset group</th>
<th align="left">Percentage of spam web pages</th>
<th align="left">Correct classification</th>
<th align="left">Incorrect classification</th>
<th align="left">Accuracy</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1%</td>
<td>2,585</td>
<td>57</td>
<td>97.8%</td>
<td>2.2%</td>
</tr>
<tr>
<td>2</td>
<td>15%</td>
<td>2,595</td>
<td>72</td>
<td>97.3%</td>
<td>2.7%</td>
</tr>
<tr>
<td>3</td>
<td>50%</td>
<td>4,212</td>
<td>933</td>
<td>81.9%</td>
<td>18.1%</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section17-0165551512439173">
<title>3.3.4. K-nearest neighbour (K-NN)</title>
<p>K-NN was used with different cases, depending on the value of <italic>K</italic>. The most accurate result (98.908% accuracy) was achieved when <italic>K</italic> = 1. When <italic>K</italic> = 3 the accuracy was 98.492%, and when <italic>K</italic> = 7 the accuracy was 98.344%. <xref ref-type="table" rid="table5-0165551512439173">Tables 5</xref>–<xref ref-type="table" rid="table7-0165551512439173">7</xref> list the results of this algorithm, based on these three values of <italic>K</italic>. They show that the different percentages of spam web pages within the three datasets have little effect on the effectiveness of the K-NN classifier in detecting spam.</p>
<table-wrap id="table5-0165551512439173" position="float">
<label>Table 5.</label>
<caption><p>K-NN results (<italic>K</italic> = 1)</p></caption>
<graphic alternate-form-of="table5-0165551512439173" xlink:href="10.1177_0165551512439173-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset group</th>
<th align="left">Percentage of spam web pages</th>
<th align="left">Correct classification</th>
<th align="left">Incorrect classification</th>
<th align="left">Accuracy</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1%</td>
<td>3,081</td>
<td>15</td>
<td>99.5%</td>
<td>0.48%</td>
</tr>
<tr>
<td>2</td>
<td>15%</td>
<td>3,104</td>
<td>18</td>
<td>99.4%</td>
<td>0.58%</td>
</tr>
<tr>
<td>3</td>
<td>50%</td>
<td>5,976</td>
<td>66</td>
<td>98.9%</td>
<td>1.1%</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table6-0165551512439173" position="float">
<label>Table 6.</label>
<caption><p>K-NN results (<italic>K</italic> = 3)</p></caption>
<graphic alternate-form-of="table6-0165551512439173" xlink:href="10.1177_0165551512439173-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset group</th>
<th align="left">Percentage of spam web pages</th>
<th align="left">Correct classification</th>
<th align="left">Incorrect classification</th>
<th align="left">Accuracy</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1%</td>
<td>3,079</td>
<td>17</td>
<td>99.3%</td>
<td>0.55%</td>
</tr>
<tr>
<td>2</td>
<td>15%</td>
<td>3,101</td>
<td>21</td>
<td>99.3%</td>
<td>0.67%</td>
</tr>
<tr>
<td>3</td>
<td>50%</td>
<td>5,945</td>
<td>91</td>
<td>98.5%</td>
<td>1.5%</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table7-0165551512439173" position="float">
<label>Table 7.</label>
<caption><p>K-NN results (<italic>K</italic> = 7)</p></caption>
<graphic alternate-form-of="table7-0165551512439173" xlink:href="10.1177_0165551512439173-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset group</th>
<th align="left">Percentage of spam web pages</th>
<th align="left">Correct classification</th>
<th align="left">Incorrect classification</th>
<th align="left">Accuracy</th>
<th align="left">Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1%</td>
<td>3,077</td>
<td>19</td>
<td>99.4%</td>
<td>0.6%</td>
</tr>
<tr>
<td>2</td>
<td>15%</td>
<td>3,100</td>
<td>22</td>
<td>99.3%</td>
<td>0.7%</td>
</tr>
<tr>
<td>3</td>
<td>50%</td>
<td>5,938</td>
<td>100</td>
<td>98.3%</td>
<td>1.66%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The results of the decision tree and K-NN algorithms have shown that they are the best, in terms of accuracy and ability, at detecting Arabic spam web pages effectively.</p>
</sec>
</sec>
</sec>
<sec id="section18-0165551512439173">
<title>4. A comparison study</title>
<p>In the work described in this paper, an extended Arabic dataset was used in addition to the use of more automatically extracted features. In addition, two programs (Web Crawler and Arabic Content Web Spam Analyser) were designed and implemented.</p>
<p>This research is based on the KNIME tool for detecting Arabic spam, and on the WEKA tool for evaluating the results harvested from this study against those of other, related studies [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>, <xref ref-type="bibr" rid="bibr13-0165551512439173">13</xref>, <xref ref-type="bibr" rid="bibr18-0165551512439173">18</xref>, <xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref><xref ref-type="bibr" rid="bibr28-0165551512439173"/>–<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>]. The WEKA tool was used for its ability to provide many different accuracy-related metrics: kappa statistic (KS), mean absolute error (MAE), root mean squared error (RMSE), relative absolute error (RAE), root relative squared error (RRSE), true positive (TP), false positive (FP), precision (P), recall (R), <italic>F</italic>-measure (F-M), and receiver operating characteristic (ROC).</p>
<p>The accuracy or performance metrics or measures reported in this section are:</p>
<list id="list4-0165551512439173" list-type="bullet">
<list-item>
<p><italic>KS (kappa statistic)</italic>. The kappa coefficient is the proportionate reduction in errors compared with the errors of a completely random classification.</p></list-item>
<list-item><p><italic>MAE (mean absolute error)</italic>. The mean absolute error measures how far the estimates are from the actual values.</p></list-item>
<list-item><p><italic>RMSE (root mean squared error)</italic>. This is related to the error variance or standard deviation. It is a statistical parameter that encapsulates the deviation from the mean inherent in the standard deviation while preserving a measure of the deviation due to systematic error. The closer RMSE is to zero, the better is the prediction.</p></list-item>
<list-item><p><italic>RAE (root absolute error)</italic>. This is the forecast error expressed as a percentage of the error from a simple forecasting model.</p></list-item>
<list-item><p><italic>RRSE (root relative squared error)</italic>. This is the error relative to what it would have been if a simple predictor had been used. It is obtained by taking the square root of the relative squared error.</p></list-item>
</list>
<p>The results of this study show that the decision tree classifier is the best algorithm for identifying Arabic web spam; this result matches the results of both Jaramh et al. [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>] and Al-Kabi [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>]. Wahsheh et al.’s results [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>], on the other hand, showed that K-NN was the best classifier when <italic>K</italic> = 1. This indicates that an increase in the size of the used dataset, and in the number of adopted features, has a positive effect on the accuracy of the classifier used.</p>
<p>The results of this study are compared here with those of Wahsheh et al. [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>] and Al-Kabi [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>], owing to the availability of the dataset used and the similar features. The results of Jaramh et al. [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>] are omitted, owing to the lack of the used dataset and the features used by the researcher.</p>
<p><xref ref-type="table" rid="table8-0165551512439173">Table 8</xref> shows a comparison of the performance measurements using the different algorithms conducted by this study and the two previous studies [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>, <xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>]. Only two of the spam percentage dataset groups were used in the comparison shown in <xref ref-type="table" rid="table8-0165551512439173">Table 8</xref>: the dataset with 1% spam for K-NN, naïve Bayes and SVM, since this percentage yielded the best results for these classifiers; and the dataset with 15% spam for decision tree, since this percentage yielded the best results for this classifier. The results shown in <xref ref-type="table" rid="table8-0165551512439173">Table 8</xref>, particularly for the 1% and 15% spam contents, are better than those presented in any previous comparable work.</p>
<table-wrap id="table8-0165551512439173" position="float">
<label>Table 8.</label>
<caption><p>IR metrics for three Arabic spam datasets</p></caption>
<graphic alternate-form-of="table8-0165551512439173" xlink:href="10.1177_0165551512439173-table8.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Algorithm</th>
<th align="left">KS</th>
<th align="left">MAE</th>
<th align="left">RMSE</th>
<th align="left">RAE</th>
<th align="left">RRSE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Naïve Bayes (1%) in our research</td>
<td>0.9818</td>
<td>0.0008</td>
<td>0.0275</td>
<td>1.9%</td>
<td>18.9%</td>
</tr>
<tr>
<td>Decision tree (15%) in our research</td>
<td>0.8526</td>
<td>0.0096</td>
<td>0.0955</td>
<td>17.3%</td>
<td>58.9%</td>
</tr>
<tr>
<td>K-NN, <italic>K</italic> = 1 (1%) in our research</td>
<td>0.9556</td>
<td>0.0021</td>
<td>0.0435</td>
<td>5.3%</td>
<td>29.9%</td>
</tr>
<tr>
<td>K-NN, <italic>K</italic> = 3 (1%) in our research</td>
<td>0.948</td>
<td>0.0024</td>
<td>0.0476</td>
<td>6.2%</td>
<td>32.8%</td>
</tr>
<tr>
<td>K-NN. <italic>K</italic> = 7 (1%) in our research</td>
<td>0.9714</td>
<td>0.0013</td>
<td>0.0337</td>
<td>3.4%</td>
<td>24.2%</td>
</tr>
<tr>
<td>SVM (1%) in our research</td>
<td>0</td>
<td>0.0216</td>
<td>0.1469</td>
<td>54.9%</td>
<td>101%</td>
</tr>
<tr>
<td>Detecting Arabic web spam (decision tree) [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>]</td>
<td>0.927</td>
<td>0.0365</td>
<td>0.191</td>
<td>7.3%</td>
<td>38.2%</td>
</tr>
<tr>
<td>Detecting Arabic web spam (naïve Bayes) [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>]</td>
<td>0.8391</td>
<td>0.0882</td>
<td>0.2446</td>
<td>17.6%</td>
<td>48.9%</td>
</tr>
<tr>
<td>Detecting Arabic web spam (K-NN) [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>]</td>
<td>0.9706</td>
<td>0.0182</td>
<td>0.1204</td>
<td>3.6%</td>
<td>24%</td>
</tr>
<tr>
<td>Combating Arabic web spam using content analysis (decision tree) [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>]</td>
<td>0.9869</td>
<td>0.0078</td>
<td>0.0787</td>
<td>1.6%</td>
<td>15.7%</td>
</tr>
<tr>
<td>Combating Arabic web spam using content analysis (SVM) [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>]</td>
<td>0.63</td>
<td>0.185</td>
<td>0.4301</td>
<td>37%</td>
<td>86%</td>
</tr>
<tr>
<td>Combating Arabic web spam using content analysis (LogitBoost) [<xref ref-type="bibr" rid="bibr29-0165551512439173">29</xref>]</td>
<td>0.9055</td>
<td>0.0992</td>
<td>0.1957</td>
<td>19.8%</td>
<td>39.1%</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The study by Ntoulas et al. [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>] was based on an English dataset collected by an MSN search crawler, and the Castillo et al. study [<xref ref-type="bibr" rid="bibr18-0165551512439173">18</xref>]was based on the WEBSPAM-UK2006 dataset. These two studies used the C4.5 decision tree algorithm to identify spammed web pages, with accuracy rates of 86.2% and 88.4% respectively.</p>
<p>A comparison of the results of this study with those of previous studies shows the effectiveness of the method used in this study to identify, with high accuracy, spammed Arabic web pages: TP = 99.8%, FP = 0.3%, and F-M = 99.7%. By comparison, the results presented by Martinez-Romo et al. [<xref ref-type="bibr" rid="bibr13-0165551512439173">13</xref>] for TP, FP and F-M were 33%, 4% and 30% respectively.</p>
<p>This study concludes that the naïve Bayes algorithm was ineffective in detecting Arabic spammed web pages; which contradicts the results presented by the authors in [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>]. This difference may be caused by the small number of spammed web pages in the dataset used in [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>].</p>
<p><xref ref-type="table" rid="table9-0165551512439173">Table 9</xref> shows accuracy comparisons between this study and those presented in [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>], [<xref ref-type="bibr" rid="bibr13-0165551512439173">13</xref>], [<xref ref-type="bibr" rid="bibr18-0165551512439173">18</xref>], [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>] and [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>]. As for <xref ref-type="table" rid="table8-0165551512439173">Table 8</xref>, only two of the spam datasets were used in the comparisons shown in <xref ref-type="table" rid="table9-0165551512439173">Table 9</xref>: the 1% dataset for K-NN, naïve Bayes and SVM, and the 15% dataset for decision tree. The results in this table show that the use of these two datasets helped to achieve results that were more accurate than those presented in previous studies. The results obtained with the decision tree classifier approach demonstrated optimal accuracy.</p>
<table-wrap id="table9-0165551512439173" position="float">
<label>Table 9.</label>
<caption><p>Comparison of accuracy values presented in five studies</p></caption>
<graphic alternate-form-of="table9-0165551512439173" xlink:href="10.1177_0165551512439173-table9.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset</th>
<th align="left">True positive</th>
<th align="left">False positive</th>
<th align="left">Precision</th>
<th align="left">Recall</th>
<th align="left">
<italic>F</italic>-measure</th>
<th align="left">ROC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Decision tree (15%) in this study</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>Naïve Bayes (1%) in this study</td>
<td>0.99</td>
<td>0.03</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>SVM (1%) in this study</td>
<td>0.98</td>
<td>0.97</td>
<td>0.96</td>
<td>0.97</td>
<td>0.96</td>
<td>0.5</td>
</tr>
<tr>
<td>K-NN, <italic>K</italic> = 1 (1%) in this study</td>
<td>0.99</td>
<td>0.03</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>0.98</td>
</tr>
<tr>
<td>K-NN, <italic>K</italic> = 3 (1%) in this study</td>
<td>0.99</td>
<td>0.02</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>K-NN, <italic>K</italic> = 7 (1%) in this study</td>
<td>0.99</td>
<td>0</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
<td>0.99</td>
</tr>
<tr>
<td>Detecting spam web pages through content analysis [<xref ref-type="bibr" rid="bibr9-0165551512439173">9</xref>]</td>
<td>–</td>
<td>–</td>
<td>0.86</td>
<td>0.09</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>Detecting Arabic web spam (decision tree) [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>]</td>
<td>0.98</td>
<td>0.06</td>
<td>0.94</td>
<td>0.98</td>
<td>0.96</td>
<td>0.96</td>
</tr>
<tr>
<td>Detecting Arabic web spam (naïve Bayes) [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>]</td>
<td>0.92</td>
<td>0.08</td>
<td>0.92</td>
<td>0.92</td>
<td>0.92</td>
<td>0.97</td>
</tr>
<tr>
<td>Detecting Arabic web spam (K-NN) [<xref ref-type="bibr" rid="bibr27-0165551512439173">27</xref>]</td>
<td>0.98</td>
<td>0.02</td>
<td>0.98</td>
<td>0.98</td>
<td>0.98</td>
<td>0.98</td>
</tr>
<tr>
<td>Detecting Arabic spam web pages using content analysis [<xref ref-type="bibr" rid="bibr28-0165551512439173">28</xref>]</td>
<td>0.91</td>
<td>0.01</td>
<td>–</td>
<td>–</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>Know your neighbours: web spam detection using the web topology [<xref ref-type="bibr" rid="bibr18-0165551512439173">18</xref>]</td>
<td>0.94</td>
<td>0.06</td>
<td>–</td>
<td>–</td>
<td>0.68</td>
<td>–</td>
</tr>
<tr>
<td>Web spam identification through language model analysis [<xref ref-type="bibr" rid="bibr13-0165551512439173">13</xref>]</td>
<td>0.33</td>
<td>0.04</td>
<td>–</td>
<td>–</td>
<td>0.30</td>
<td>–</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section19-0165551512439173" sec-type="conclusions">
<title>5. Conclusions and future work</title>
<p>Web spam detection represents one of the main challenges of adversarial information retrieval aspects of search engines. Spammers are continually changing the spamming techniques they use to get their web pages ranked higher than they really deserve in order to show advertisements about their products, and thus earn more money. They try their best to avoid the detection and elimination of links to their pages from search engines index. Against this, email service providers and search engine owners try their best to avoid presenting unsolicited content to their users.</p>
<p>In the work described in this paper, we built a large Arabic web spam dataset using a crawler customized for the purposes of this research, thus producing a dataset containing 15,000 Arabic spammed web pages. Many of the content-based features were extracted by a web analyser algorithm, and then the pages were manually classified as either spam or non-spam. The dataset used in this research was built to be used both in this work and by others.</p>
<p>This research was conducted at a web page level, not on a website level, because spammers on any one website may use spamming methods in some of its pages, while not using these methods in the rest of the site.</p>
<p>Several data-mining techniques were used to evaluate the accuracy of Arabic spam content prediction. This study was based on the use of four algorithms: decision tree, K-NN, SVM and NB. The analysis included automatic methods for collecting data and applying algorithms, and manual verification for spam content. The results show that the decision tree algorithm was the best, with an accuracy value of 99.521%.</p>
<p>It is necessary to fight web spam effectively, and it is a shared responsibility between website owners and search engines. There are two main techniques for web spamming: content based and link based. Arabic spammed web pages adopt either or both of these two techniques.</p>
<p>As future research, we plan to study in more detail the effect on the accuracy of different spam classifiers of using a larger number of datasets with different percentages of spam pages. We also plan to conduct a number of studies on features of web page contents and links, to help identify spammed web pages more accurately. In addition, Arab users may be used in the future to provide us with their feedback, which will help in providing more relevant information to them from web search engines.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="bibr1-0165551512439173">
<label>[1]</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ryding</surname><given-names>KC</given-names></name>
</person-group>. <source>A reference grammar of modern standard Arabic</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>, <year>2005</year>.</citation>
</ref>
<ref id="bibr2-0165551512439173">
<label>[2]</label>
<citation citation-type="web">
<collab>MENA Online Advertising Industry slideshare</collab>. <comment><ext-link ext-link-type="uri" xlink:href="http://www.slideshare.net/aitmit/mena-online-advertising-industry">http://www.slideshare.net/aitmit/mena-online-advertising-industry</ext-link></comment> (<access-date>accessed 10 October 2011</access-date>).</citation>
</ref>
<ref id="bibr3-0165551512439173">
<label>[3]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Gyongyi</surname><given-names>Z</given-names></name>
<name><surname>Garcia-Molina</surname><given-names>H</given-names></name>
<name><surname>Pedersen</surname><given-names>J</given-names></name>
</person-group>. <article-title>Combating web spam with TrustRank</article-title>. In: <conf-name>Proceedings of the 30th International Conference on Very Large Databases (VLDB)</conf-name>, <conf-loc>Toronto, Canada</conf-loc>, <year>2004</year>, pp. <fpage>576</fpage>–<lpage>587</lpage>.</citation>
</ref>
<ref id="bibr4-0165551512439173">
<label>[4]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Gyongyi</surname><given-names>Z</given-names></name>
<name><surname>Garcia-Molina</surname><given-names>H</given-names></name>
</person-group>. <article-title>Web spam taxonomy</article-title>. In: <conf-name>Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web</conf-name>, <conf-loc>Chiba, Japan</conf-loc>, <year>2005</year>, pp. <fpage>1</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr5-0165551512439173">
<label>[5]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Jindal</surname><given-names>N</given-names></name>
<name><surname>Liu</surname><given-names>B</given-names></name>
</person-group>. <article-title>Analyzing and detecting review spam</article-title>. <conf-name>Proceedings of the 7th IEEE International Conference on Data Mining (ICDM 2007)</conf-name>, <conf-loc>Omaha, NE</conf-loc>, USA, <year>2007</year>, pp. <fpage>547</fpage>–<lpage>552</lpage>.</citation>
</ref>
<ref id="bibr6-0165551512439173">
<label>[6]</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W</given-names></name>
<name><surname>Zeng</surname><given-names>G</given-names></name>
<name><surname>Sun</surname><given-names>M</given-names></name>
<name><surname>Gu</surname><given-names>H</given-names></name>
<name><surname>Zhang</surname><given-names>Q</given-names></name>
</person-group>. <article-title>EviRank: an evidence based content trust model for web spam detection</article-title>. In: <person-group person-group-type="editor">
<name><surname>Chang</surname><given-names>KC</given-names></name>
<etal/>
</person-group>. (eds) <source>APWeb/WAIM 2007 Ws</source>, <comment>LNCS 4537</comment>, <year>2007</year>, pp. <fpage>299</fpage>–<lpage>307</lpage>.</citation>
</ref>
<ref id="bibr7-0165551512439173">
<label>[7]</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W</given-names></name>
<name><surname>Zeng</surname><given-names>G</given-names></name>
</person-group>. <article-title>Content trust model for detecting web spam</article-title>. In: <person-group person-group-type="editor">
<name><surname>Etalle</surname><given-names>S</given-names></name>
<name><surname>Marsh</surname><given-names>S</given-names></name>
</person-group> (eds) <source>Trust management</source>. <comment>International Federation for Information Processing</comment>, <volume>Vol. 238</volume>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2007</year>, pp. <fpage>139</fpage>–<lpage>152</lpage>.</citation>
</ref>
<ref id="bibr8-0165551512439173">
<label>[8]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wang</surname><given-names>W</given-names></name>
<name><surname>Zeng</surname><given-names>G</given-names></name>
<name><surname>Tang</surname><given-names>D</given-names></name>
</person-group>. <article-title>Using evidence based content trust model for spam detection</article-title>. <source>Expert Systems with Applications</source> <year>2010</year>; <volume>37</volume>(<issue>8</issue>): <fpage>5599</fpage>–<lpage>5606</lpage>.</citation>
</ref>
<ref id="bibr9-0165551512439173">
<label>[9]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Ntoulas</surname><given-names>A</given-names></name>
<name><surname>Najork</surname><given-names>M</given-names></name>
<name><surname>Manasse</surname><given-names>M</given-names></name>
<name><surname>Fetterly</surname><given-names>D</given-names></name>
</person-group>. <article-title>Detecting spam web pages through content analysis</article-title>. In: <conf-name>Proceedings of the 15th International World Wide Web Conference</conf-name>, <conf-loc>Edinburgh, Scotland</conf-loc>, <year>2006</year>, pp. <fpage>83</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr10-0165551512439173">
<label>[10]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fetterly</surname><given-names>D</given-names></name>
</person-group>. <article-title>Adversarial information retrieval: the manipulation of web content</article-title>. <source>ACM Computing Reviews</source>, <month>July</month> <year>2007</year>.</citation>
</ref>
<ref id="bibr11-0165551512439173">
<label>[11]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Svore</surname><given-names>KM</given-names></name>
<name><surname>Wu</surname><given-names>Q</given-names></name>
<name><surname>Burges</surname><given-names>CJC</given-names></name>
<name><surname>Raman</surname><given-names>A</given-names></name>
</person-group>. <article-title>Improving web spam classification using rank-time features</article-title>. In: <conf-name>Proceedings of AIRWeb ’07</conf-name>, <conf-loc>Banff, Alberta, Canada</conf-loc>, <year>2007</year>, pp. <fpage>9</fpage>–<lpage>16</lpage>.</citation>
</ref>
<ref id="bibr12-0165551512439173">
<label>[12]</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pera</surname><given-names>MS</given-names></name>
<name><surname>Ng</surname><given-names>Y-K</given-names></name>
</person-group>. <article-title>Identifying spam web pages based on content similarity</article-title>. In: <person-group person-group-type="editor">
<name><surname>Gervasi</surname><given-names>O</given-names></name>
<etal/>
</person-group>. (eds) <source>ICCSA 2008</source>, <comment>Part II, LNCS 5073</comment>, <year>2008</year>, pp. <fpage>204</fpage>–<lpage>219</lpage>.</citation>
</ref>
<ref id="bibr13-0165551512439173">
<label>[13]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Martinez-Romo</surname><given-names>J</given-names></name>
<name><surname>Araujo</surname><given-names>L</given-names></name>
</person-group>. <article-title>Web spam identification through language model analysis</article-title>. In: <conf-name>Proceedings of AIRWeb ’09</conf-name>, <conf-loc>Madrid, Spain</conf-loc>, <year>2009</year>, pp. <fpage>21</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr14-0165551512439173">
<label>[14]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Hayati</surname><given-names>P</given-names></name>
<name><surname>Potdar</surname><given-names>V</given-names></name>
</person-group>. <article-title>Toward Spam 2.0: an evaluation of Web 2.0 anti-spam methods</article-title>. In: <conf-name>Proceedings of the 7th IEEE International Conference on Industrial Informatics (INDIN 2009)</conf-name>, <conf-loc>Cardiff, UK</conf-loc>, <year>2009</year>, pp. <fpage>875</fpage>–<lpage>880</lpage>.</citation>
</ref>
<ref id="bibr15-0165551512439173">
<label>[15]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Abernethy</surname><given-names>J</given-names></name>
<name><surname>Chapelle</surname><given-names>O</given-names></name>
<name><surname>Castillo</surname><given-names>C</given-names></name>
</person-group>. <article-title>Web spam identification through content and hyperlinks</article-title>. In: <conf-name>Proceedings of AIRWeb ’08</conf-name>, <conf-loc>Beijing, China</conf-loc>, <year>2008</year>, pp. <fpage>41</fpage>–<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr16-0165551512439173">
<label>[16]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Liang</surname><given-names>C</given-names></name>
<name><surname>Ru</surname><given-names>L</given-names></name>
<name><surname>Zhu</surname><given-names>X</given-names></name>
</person-group>. <article-title>R-SpamRank: a spam detection algorithm based on link analysis</article-title>. <source>Journal of Computational Information Systems</source> <year>2007</year>; <volume>3</volume>(<issue>4</issue>): <fpage>1705</fpage>–<lpage>1712</lpage>.</citation>
</ref>
<ref id="bibr17-0165551512439173">
<label>[17]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Araujo</surname><given-names>L</given-names></name>
<name><surname>Martinez-Romo</surname><given-names>J</given-names></name>
</person-group>. <article-title>Web spam detection: new classification features based on qualified link analysis and language models</article-title>. <source>IEEE Transactions on Information Forensics and Security</source> <year>2010</year>; <volume>5</volume>(<issue>3</issue>): <fpage>581</fpage>–<lpage>590</lpage>.</citation>
</ref>
<ref id="bibr18-0165551512439173">
<label>[18]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Castillo</surname><given-names>C</given-names></name>
<name><surname>Donato</surname><given-names>D</given-names></name>
<name><surname>Gionis</surname><given-names>A</given-names></name>
<name><surname>Murdock</surname><given-names>V</given-names></name>
<name><surname>Silvestri</surname><given-names>F</given-names></name>
</person-group>. <article-title>Know your neighbours: web spam detection using the web topology</article-title>. In: <conf-name>Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</conf-name>, <conf-loc>New York, NY, USA</conf-loc>, <year>2007</year>, pp. <fpage>423</fpage>–<lpage>430</lpage>.</citation>
</ref>
<ref id="bibr19-0165551512439173">
<label>[19]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Geng</surname><given-names>G</given-names></name>
<name><surname>Wang</surname><given-names>C</given-names></name>
<name><surname>Li</surname><given-names>Q</given-names></name>
<name><surname>Xu</surname><given-names>L</given-names></name>
<name><surname>Jin</surname><given-names>X</given-names></name>
</person-group>. <article-title>Boosting the performance of web spam detection with ensemble under-sampling classification</article-title>. In: <conf-name>Proceedings of the 4th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)</conf-name>, <conf-loc>Hainan, China</conf-loc>, <year>2007</year>, pp. <fpage>583</fpage>–<lpage>587</lpage>.</citation>
</ref>
<ref id="bibr20-0165551512439173">
<label>[20]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Dai</surname><given-names>N</given-names></name>
<name><surname>Davison</surname><given-names>BD</given-names></name>
<name><surname>Qi</surname><given-names>X</given-names></name>
</person-group>. <article-title>Looking into the past to better classify web spam</article-title>. In: <conf-name>Proceedings of AIRWeb ’09</conf-name>, <conf-loc>Madrid, Spain</conf-loc>, <year>2009</year>, pp. <fpage>1</fpage>–<lpage>8</lpage>.</citation>
</ref>
<ref id="bibr21-0165551512439173">
<label>[21]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Niu</surname><given-names>X</given-names></name>
<name><surname>Ma</surname><given-names>J</given-names></name>
<name><surname>He</surname><given-names>Q</given-names></name>
<name><surname>Wang</surname><given-names>S</given-names></name>
<name><surname>Zhang</surname><given-names>D</given-names></name>
</person-group>. <article-title>Learning to detect web spam by genetic programming</article-title>. In: <conf-name>Proceedings of the 11th International Conference on Web-age Information Management</conf-name>, <comment>LNCS 6184</comment>, <year>2010</year>, pp. <fpage>18</fpage>–<lpage>27</lpage>.</citation>
</ref>
<ref id="bibr22-0165551512439173">
<label>[22]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Fetterly</surname><given-names>D</given-names></name>
<name><surname>Manasse</surname><given-names>M</given-names></name>
<name><surname>Najork</surname><given-names>M</given-names></name>
<name><surname>Wiener</surname><given-names>J</given-names></name>
</person-group>. <article-title>A large-scale study of the evolution of web pages</article-title>. In: <conf-name>Proceedings of the 12th International World Wide Web Conference</conf-name>, <conf-loc>Budapest, Hungary</conf-loc>, <month>May</month> <year>2003</year>, pp. <fpage>669</fpage>–<lpage>678</lpage>.</citation>
</ref>
<ref id="bibr23-0165551512439173">
<label>[23]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Fetterly</surname><given-names>D</given-names></name>
<name><surname>Manasse</surname><given-names>M</given-names></name>
<name><surname>Najork</surname><given-names>M</given-names></name>
</person-group>. <article-title>Detecting phrase-level duplication on the world wide web</article-title>. In: <conf-name>Proceedings of SIGIR ’05</conf-name>, <conf-loc>New York, NY, USA</conf-loc>, <year>2005</year>, pp. <fpage>170</fpage>–<lpage>177</lpage>.</citation>
</ref>
<ref id="bibr24-0165551512439173">
<label>[24]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Fetterly</surname><given-names>D</given-names></name>
<name><surname>Manasse</surname><given-names>M</given-names></name>
<name><surname>Najork</surname><given-names>M</given-names></name>
</person-group>. <article-title>Spam, damn spam, and statistics: using statistical analysis to locate spam web pages</article-title>. In: <conf-name>Proceedings of WebDB ’04</conf-name>, <conf-loc>New York, NY, USA</conf-loc>, <year>2004</year>, pp. <fpage>1</fpage>–<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr25-0165551512439173">
<label>[25]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Benczúr</surname><given-names>AA</given-names></name>
<name><surname>Siklósi</surname><given-names>D</given-names></name>
<name><surname>Szabó</surname><given-names>J</given-names></name>
<name><surname>Bíró</surname><given-names>I</given-names></name>
<name><surname>Fekete</surname><given-names>Z</given-names></name>
<name><surname>Kurucz</surname><given-names>M</given-names></name>
<name><surname>Pereszlényi</surname><given-names>A</given-names></name>
<name><surname>Rácz</surname><given-names>S</given-names></name>
<name><surname>Szabó</surname><given-names>A</given-names></name>
</person-group>. <article-title>Web spam: a survey with vision for the archivist</article-title>. In: <conf-name>Proceedings of the 8th International Web Archiving Workshop (IWAW ’08)</conf-name>, <conf-loc>Aarhus, Denmark</conf-loc>, <month>September</month> <year>2008</year>, pp. <fpage>1</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr26-0165551512439173">
<label>[26]</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Spirin</surname><given-names>N</given-names></name>
<name><surname>Han</surname><given-names>J</given-names></name>
</person-group>. <article-title>Survey on web spam detection: principles and algorithms</article-title>. <comment><ext-link ext-link-type="uri" xlink:href="https://wiki.engr.illinois.edu/download/attachments/188588798/WebSpamSurvey.pdf">https://wiki.engr.illinois.edu/download/attachments/188588798/WebSpamSurvey.pdf</ext-link></comment> (<access-date>accessed 4 December 2011</access-date>).</citation>
</ref>
<ref id="bibr27-0165551512439173">
<label>[27]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Wahsheh</surname><given-names>HA</given-names></name>
<name><surname>Al-Kabi</surname><given-names>MN</given-names></name>
</person-group>. <article-title>Detecting Arabic web spam</article-title>. In: <conf-name>Proceedings of the 5th International Conference on Information Technology (ICIT 2011)</conf-name>, <conf-loc>Amman, Jordan</conf-loc>, <day>11–13</day> <month>May</month> <year>2011</year>, <comment>Paper ID 631</comment>, <fpage>8</fpage> pp.</citation>
</ref>
<ref id="bibr28-0165551512439173">
<label>[28]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jaramh</surname><given-names>R</given-names></name>
<name><surname>Saleh</surname><given-names>T</given-names></name>
<name><surname>Khattab</surname><given-names>S</given-names></name>
<name><surname>Farag</surname><given-names>I</given-names></name>
</person-group>. <article-title>Detecting Arabic spam web pages using content analysis</article-title>. <source>International Journal of Reviews in Computing</source> <year>2011</year>; <volume>6</volume>: <fpage>1</fpage>–<lpage>8</lpage>.</citation>
</ref>
<ref id="bibr29-0165551512439173">
<label>[29]</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Al-Kabi</surname><given-names>MN</given-names></name>
<name><surname>Wahsheh</surname><given-names>HA</given-names></name>
<name><surname>Al-Eroud</surname><given-names>AF</given-names></name>
<name><surname>Alsmadi</surname><given-names>IM</given-names></name>
</person-group>. <article-title>Combating Arabic web spam using content analysis</article-title>. In: <conf-name>Proceedings of the 2011 IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT)</conf-name>, <conf-loc>Amman Jordan</conf-loc>, <month>December</month> <year>2011</year>, pp. <fpage>1</fpage>–<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr30-0165551512439173">
<label>[30]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Alsmadi</surname><given-names>I</given-names></name>
</person-group>. <article-title>The automatic evaluation of website metrics and state</article-title>. <source>International Journal of Web-Based Learning and Teaching Technologies</source> <year>2010</year>; <volume>5</volume>(<issue>4</issue>): <fpage>1</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr31-0165551512439173">
<label>[31]</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Al-Eroud</surname><given-names>AF</given-names></name>
<name><surname>Al-Ramahi</surname><given-names>MA</given-names></name>
<name><surname>Al-Kabi</surname><given-names>MN</given-names></name>
<name><surname>Alsmadi</surname><given-names>IM</given-names></name>
<name><surname>Al-Shawakfa</surname><given-names>EM</given-names></name>
</person-group>. <article-title>Evaluating Google queries based on language preferences</article-title>. <source>Journal of Information Science</source> <year>2011</year>; <volume>37</volume>(<issue>3</issue>): <fpage>282</fpage>–<lpage>292</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>