<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPM</journal-id>
<journal-id journal-id-type="hwp">spepm</journal-id>
<journal-title>Educational and Psychological Measurement</journal-title>
<issn pub-type="ppub">0013-1644</issn>
<issn pub-type="epub">1552-3888</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0013164411404410</article-id>
<article-id pub-id-type="publisher-id">10.1177_0013164411404410</article-id>
<title-group>
<article-title>Modeling the Predictive Validity of SAT Mathematics Items Using Item Characteristics</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Kobrin</surname><given-names>Jennifer L.</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411404410">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kim</surname><given-names>YoungKoung</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411404410">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Sackett</surname><given-names>Paul R.</given-names></name>
<xref ref-type="aff" rid="aff2-0013164411404410">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0013164411404410"><label>1</label>The College Board, New York, NY, USA</aff>
<aff id="aff2-0013164411404410"><label>2</label>University of Minnesota, Minneapolis, USA</aff>
<author-notes>
<corresp id="corresp1-0013164411404410">Jennifer L. Kobrin, The College Board, 45 Columbus Avenue, New York, NY 10023-6992, USA. Email: <email>jkobrin@collegeboard.org</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>72</volume>
<issue>1</issue>
<fpage>99</fpage>
<lpage>119</lpage>
<permissions>
<copyright-statement>© 2012 SAGE Publications</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>There is much debate on the merits and pitfalls of standardized tests for college admission, with questions regarding the format (multiple-choice vs. constructed response), cognitive complexity, and content of these assessments (achievement vs. aptitude) at the forefront of the discussion. This study addressed these questions by investigating the relationship between SAT Mathematics (SAT-M) item characteristics and the item’s ability to predict college outcomes. Using multiple regression, SAT-M item characteristics (content area, format, cognitive complexity, and abstract/concrete classification) were used to predict three outcome measures: the correlation of item score with first-year college grade point average, the correlation of item score with mathematics course grades, and the percentage of students who answered the item correctly and chose to major in a mathematics or science field. Separate models were run including and excluding item difficulty and discrimination as covariates. The results revealed that many of the item characteristics were related to the outcome measures and that item difficulty and discrimination had a mediating effect on several of the predictor variables, particularly on the effects of nonroutine/insightful items and multiple-choice items.</p>
</abstract>
<kwd-group>
<kwd>college outcomes</kwd>
<kwd>mathematics items</kwd>
<kwd>test validity</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Several themes commonly emerge in discussions of high-stakes testing. One is skepticism about multiple-choice testing, with the common contention that “life isn’t multiple choice” (<xref ref-type="bibr" rid="bibr29-0013164411404410">Ryan &amp; Greguras, 1998</xref>) and that constructed response items or performance assessments would be more valid indicators of student achievement and better predictors of subsequent performance. Another is a call for tests that measure higher-order thinking rather than rote memory or recall. A third is an assertion that achievement tests are more appropriate than aptitude tests for assessing college readiness.</p>
<sec id="section1-0013164411404410">
<title>Multiple-Choice Versus Constructed Response</title>
<p>Critics of standardized tests commonly assert that multiple-choice questions on college entrance examinations are artificial and do not represent the types of tasks that college students undertake in their coursework. There is widespread belief that these tests place too much emphasis on factual knowledge and the application of procedures and fail to assess higher-order thinking. Quite the opposite, performance assessments and constructed response items are often praised for their greater emphasis on problem solving, comprehension, critical thinking, reasoning, and metacognitive processes (<xref ref-type="bibr" rid="bibr19-0013164411404410">Linn, Baker, &amp; Dunbar, 1991</xref>). In this sense, tests comprising constructed response items are thought to be better predictors of college success than tests of multiple-choice items.</p>
<p>Whereas there is little debate that constructed response items provide a more realistic context for assessment than multiple-choice items, <xref ref-type="bibr" rid="bibr29-0013164411404410">Ryan and Greguras (1998)</xref> aptly note that the appearance of realism, or the face validity of an assessment, should “not be assumed to be important until researchers provide more concrete demonstrations of its relation to applicant <italic>behavior</italic> and other outcomes” (p. 198). This statement underscores the importance of research on the validity of assessments for predicting relevant outcomes.</p>
<p>The most common procedure for examining the validity of college admission tests involves correlating the scores on the tests with various measures of college performance. Whereas constructed response items are typically thought to have greater face validity and more desirable effects on education, the trade-off is usually less predictive power than multiple-choice tests. Because it is generally possible to administer more multiple-choice items than constructed response items in a fixed period of time allotted for testing, tests using a multiple-choice format are usually more reliable and show larger correlations with criterion measures.</p>
<p><xref ref-type="bibr" rid="bibr2-0013164411404410">Bennett (1993)</xref> suggested that examining the incremental validity of constructed response items over multiple-choice tests in predicting college outcomes would provide a useful measure of the relevance of constructed response tests. He cited two studies finding that essay tests added very little to the prediction of first-year college grade point average (FYGPA) (<xref ref-type="bibr" rid="bibr4-0013164411404410">Bridgeman, 1991</xref>; <xref ref-type="bibr" rid="bibr6-0013164411404410">Bridgeman &amp; Lewis, 1991</xref>) and one study finding that essay tests generally provided a slightly larger increment in predicting individual course grades (<xref ref-type="bibr" rid="bibr3-0013164411404410">Breland, Camp, Jones, Morris, &amp; Rock, 1987</xref>). <xref ref-type="bibr" rid="bibr2-0013164411404410">Bennett (1993)</xref> concluded that essay tasks appeared to add little predictive value over multiple-choice tests in college admissions. However, there are some instances where constructed response items may perform as well or better than multiple-choice items. For example, <xref ref-type="bibr" rid="bibr5-0013164411404410">Bridgeman (1992)</xref> compared multiple-choice and open-ended versions of the same quantitative items on the GRE and found that both formats had similar relationships with GRE scores and undergraduate GPA.</p>
</sec>
<sec id="section2-0013164411404410">
<title>Cognitive Complexity</title>
<p>Critics who denounce multiple-choice tests often claim that such tests cannot assess higher-level cognitive skills or critical thinking. Yet multiple-choice items can be constructed to tap higher-order thinking skills, just as constructed response items can sometimes tap lower-level skills. Judgments regarding the cognitive complexity of a test item need to take into account student familiarity with the problems and the ways in which students attempt to solve them (<xref ref-type="bibr" rid="bibr19-0013164411404410">Linn et al., 1991</xref>). Even if an item appears to be at a high cognitive level, if the students are familiar with the problem, the question is a recall item (<xref ref-type="bibr" rid="bibr20-0013164411404410">McDonald, 2002</xref>). Items assessing higher cognitive processes are thought to be better predictors of college readiness and college success than items that primarily assess routine mental processes, because these higher mental processes are valued in college.</p>
</sec>
<sec id="section3-0013164411404410">
<title>Achievement Versus Aptitude Tests</title>
<p>Another popular belief is that achievement tests are superior to aptitude tests and that the former are more valid indicators of college readiness (<xref ref-type="bibr" rid="bibr1-0013164411404410">Atkinson &amp; Geiser, 2009</xref>; <xref ref-type="bibr" rid="bibr21-0013164411404410">National Association for College Admissions Counseling, 2008</xref>). The SAT is sometimes referred to as an aptitude test, even though it is aligned with high school and college curricula (<xref ref-type="bibr" rid="bibr31-0013164411404410">Vasavada, Carman, Hart, &amp; Luisier, 2010</xref>). A few studies have directly compared the predictive validity of the SAT and SAT Subject Tests, which are 1-hour tests designed to measure knowledge in specific subject areas and the students’ ability to apply that knowledge. These studies generally find slightly larger correlations with college grades for the subject tests, although both tests provide a similar increment in the prediction over the other when high school GPA is also included in the prediction model (<xref ref-type="bibr" rid="bibr11-0013164411404410">Geiser &amp; Studley, 2002</xref>; <xref ref-type="bibr" rid="bibr15-0013164411404410">Kobrin, Camara, &amp; Milewski, 2002</xref>). The College Board offers two separate SAT subject tests in mathematics—Level 1 and Level 2. Level 1 covers content learned in 2 years of algebra and 1 year of geometry, whereas Level 2 also covers precalculus (elementary functions) and trigonometry. The Level 2 test has been found to have a larger correlation with first-year college grades than Level 1 (<xref ref-type="bibr" rid="bibr15-0013164411404410">Kobrin et al., 2002</xref>; <xref ref-type="bibr" rid="bibr27-0013164411404410">Ramist, Lewis, &amp; McCamley-Jenkins, 2001</xref>).This finding supports the widely held notion that students mastering higher-level mathematics content do in fact perform better in college.</p>
</sec>
<sec id="section4-0013164411404410">
<title>Abstract Versus Concrete Assessment</title>
<p>Although this issue does not receive the same amount of attention and scrutiny as those described above, there is precedent for examining potential differences in the validity of abstract and concrete test items. Prior research has found mixed results with regard to performance differences on abstract and concrete mathematics problems. Concrete items are those that include a setting or a real-world instantiation of the concept, whereas abstract items present symbolic language instead of concrete examples. Although some studies have found better performance on concrete items (<xref ref-type="bibr" rid="bibr18-0013164411404410">Koedinger &amp; Nathan, 2004</xref>; <xref ref-type="bibr" rid="bibr24-0013164411404410">Nunes, Schliemann, &amp; Carraher, 1993</xref>), other studies have found better performance on abstract items (<xref ref-type="bibr" rid="bibr14-0013164411404410">Kaminski, Sloutsky, &amp; Heckler, 2008</xref>). <xref ref-type="bibr" rid="bibr17-0013164411404410">Koedinger, Alibali, and Nathan (2008)</xref> offered an explanation for the inconsistent results in prior studies. They found that college students performed better on concrete problems when the problems were simple, but the students performed better on abstract items when the problems were more complex.</p>
</sec>
<sec id="section5-0013164411404410">
<title>Purpose of the Present Study</title>
<p>The present study used the SAT mathematics test to shed light on the questions raised above by examining the relationship between item characteristics and the items’ relationship with various college outcomes. This study afforded the opportunity to separate the content area, format, cognitive complexity, and abstract/concrete designation of the items to determine the unique contribution of each factor in an item’s prediction of college outcomes. Because the difficulty and discrimination of an item varies by each item characteristic, and is also related to an item’s validity for predicting college outcomes, it was also of interest to assess whether the item characteristics have an effect on college outcomes after controlling for item difficulty and discrimination. This study addressed the following two research questions:</p>
<list id="list1-0013164411404410" list-type="simple">
<list-item><p><italic>Research Question 1</italic>: How do SAT mathematics items’ content area, format, cognitive complexity, and abstract/concrete designation affect the item’s prediction of college outcomes?</p>
</list-item>
<list-item><p><italic>Research Question 2</italic>: Is the relationship between item characteristics and college outcomes mediated by item difficulty and discrimination?</p></list-item>
</list>
<p>Most studies on the validity of the SAT and other large-scale tests examine the relationship between total test scores and various outcomes of interest. Yet <xref ref-type="bibr" rid="bibr13-0013164411404410">Haladyna (2004)</xref> noted that since the test item is the basic unit of observation in a test, it is important to collect validity evidence for both item responses and test scores. The validity evidence gathered to support interpretation of an item response is also part of the validity evidence for the test scores as a whole. The collection of validity evidence at the item level is akin to the use of coefficient alpha to evaluate whether a set of test items works together as composite to create a test score. If an individual test item is shown to have a low item–total correlation with the total test score, or coefficient alpha increases if the item is deleted, that item is scrutinized and possibly removed from the test. Similarly, information on the validity of each item on a test and how item characteristics are related to item validity can help test developers identify the item characteristics that are most strongly related to criteria of interest.</p>
<p>Many researchers have examined the relationship between characteristics or types of test items and the difficulty of the item, also called item difficulty modeling (see, e.g., <xref ref-type="bibr" rid="bibr9-0013164411404410">Enright, Morley, &amp; Sheehan, 2002</xref>; <xref ref-type="bibr" rid="bibr30-0013164411404410">Sebrechts, Enright, Bennett, &amp; Martin, 1996</xref>), but very few have directly examined whether item characteristics are related to validity. There are many facets to validity and all are important, but this study focused on validity for predicting college performance, since this is the primary use of the SAT. Studying the validity of an assessment by item type can provide information on whether all types of items contribute equally to its validity or whether certain types of items may detract from the overall validity of the test. Other testing programs have conducted research on the validity of specific types of items. For example, <xref ref-type="bibr" rid="bibr28-0013164411404410">Roussos and Norton (1998)</xref> examined the validity of three different item types on the Law School Admission Test for predicting first-year average in law school. The three item types were analytical reasoning, logical reasoning, and reading comprehension. Each was found to have a substantial correlation with first-year average, and the authors concluded that all should remain as part of the Law School Admission Test to maintain its overall level of predictive validity.</p>
</sec>
<sec id="section6-0013164411404410" sec-type="methods">
<title>Method</title>
<sec id="section7-0013164411404410">
<title>The SAT Mathematics Test</title>
<p>The SAT Reasoning Test (hereafter referred to as the SAT) is a standardized test of critical reading, mathematics, and writing ability that is used to assist colleges and universities to make admission decisions. The SAT consists of three separately timed sections: critical reading, mathematics, and writing. The SAT mathematics section (SAT-M) consists of 54 items, administered in three separately timed sections, for a total of 70 minutes. The majority of the SAT-M consists of multiple-choice questions, but 10 items are student-produced responses (SPR), which have no answer choices provided and require students to solve the problem and enter their response on a special grid. SAT-M items are also categorized by content area. Approximately 20% to 25% of the items are numbers and operations, 35% to 40% algebra and functions, 25% to 30% geometry and measurement, and 10% to 15% data analysis, statistics, and probability.</p>
<p>During the test development process, each SAT-M item is coded according to cognitive complexity and whether the item is abstract or concrete. Cognitive complexity is coded into three categories. Routine (RTNE) items are those that require examinees to recall factual knowledge and/or perform mathematical manipulation, comprehension (COMP) items are those that require examinees to solve problems that demonstrate comprehension of mathematical ideas and/or concepts, and nonroutine/insightful items (NONR) are those that require examinees to solve nonroutine problems or problems that require insight, ingenuity, or higher mental processes. As stated earlier, concrete items are those that include a setting or a real-world instantiation of the concept, whereas abstract items present symbolic language instead of concrete examples. For example, a concrete item might ask the student to solve a probability problem involving randomly choosing a red marble from a bag containing red and blue marbles or involving rolling a six-sided die, whereas an abstract item might present probability as choosing <italic>n</italic> things from a larger set of <italic>m</italic> things (<xref ref-type="bibr" rid="bibr14-0013164411404410">Kaminski et al., 2008</xref>). <xref ref-type="app" rid="app1-0013164411404410">Appendix A</xref> includes examples of some disclosed SAT items in these categories.</p>
<p>Although SAT-M subscores by item type are not reported, there have been efforts to explore the reliability of cluster scores based on content specifications or item type (<xref ref-type="bibr" rid="bibr10-0013164411404410">Ewing, Huff, Andrews, &amp; King, 2005</xref>). Several studies on the dimensionality of the SAT-M have found the test to be essentially unidimensional, with some indication that there is a small factor related to geometry items (<xref ref-type="bibr" rid="bibr7-0013164411404410">Dorans &amp; Lawrence, 1987</xref>; <xref ref-type="bibr" rid="bibr8-0013164411404410">Doub &amp; Lawrence, 1996</xref>; <xref ref-type="bibr" rid="bibr25-0013164411404410">Oh &amp; Sathy, 2006</xref>). However, <xref ref-type="bibr" rid="bibr12-0013164411404410">Gierl, Tan, and Wang (2005)</xref> found more substantial evidence for two dimensions on the SAT-M but noted that the dimensions were difficult to identify and were not strongly replicable across samples.</p>
</sec>
<sec id="section8-0013164411404410">
<title>Data Source</title>
<p>The data were obtained from 110 colleges and universities that participated in the College Board’s National SAT Validity Study (<xref ref-type="bibr" rid="bibr16-0013164411404410">Kobrin, Patterson, Shaw, Mattern, &amp; Barbuti, 2008</xref>). FYGPA, course grades, and major were available for 161,584 students who finished their first year of college in spring 2007. Official SAT Reasoning Test scores were obtained from the 2006 College-Bound Senior Cohort database that consists of data of the students who participated in the SAT program and reported plans to graduate from high school in 2006.</p>
<p><xref ref-type="table" rid="table1-0013164411404410">Table 1</xref> displays characteristics of the items used in the current study. These items came from 12 SAT forms that were administered between March 2005 and April 2006, which were the first operational forms of the SAT after its revision in March 2005. Each SAT form includes a total of 54 mathematics items, so a total of 648 items were available. On these 12 particular forms, each mathematics item appeared on only one form. The exception was two forms that were administered a first time, then reprinted and administered again during a separate test administration date. For the purposes of this study, the data were combined on these two identical forms. Four items were excluded from the study due to missing data on one or more of the predictor or outcome variables, so a total of 644 items were studied.</p>
<table-wrap id="table1-0013164411404410" position="float">
<label>Table 1.</label>
<caption>
<p>SAT-M Items by Content Area, Format, Cognitive Category, and Abstract/Concrete</p>
</caption>
<graphic alternate-form-of="table1-0013164411404410" xlink:href="10.1177_0013164411404410-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center"><italic>N</italic>
</th>
<th align="center">Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Content area</td>
</tr>
<tr>
<td> Number &amp; operations (NO)</td>
<td>131</td>
<td>20.3</td>
</tr>
<tr>
<td> Algebra &amp; functions (AF)</td>
<td>254</td>
<td>39.4</td>
</tr>
<tr>
<td> Geometry &amp; measurement (GM)</td>
<td>183</td>
<td>28.4</td>
</tr>
<tr>
<td> Data Analysis/stat/probability (DA)</td>
<td>76</td>
<td>11.8</td>
</tr>
<tr>
<td colspan="3">Format</td>
</tr>
<tr>
<td> Multiple-choice (MC)</td>
<td>526</td>
<td>81.7</td>
</tr>
<tr>
<td> Student-produced response (SPR)</td>
<td>118</td>
<td>18.3</td>
</tr>
<tr>
<td colspan="3">Cognitive category</td>
</tr>
<tr>
<td> Routine (RTNE)</td>
<td>78</td>
<td>12.1</td>
</tr>
<tr>
<td> Comprehension (COMP)</td>
<td>160</td>
<td>24.8</td>
</tr>
<tr>
<td> Nonroutine/insightful (NONR)</td>
<td>160</td>
<td>24.8</td>
</tr>
<tr>
<td colspan="3">Abstract/concrete</td>
</tr>
<tr>
<td> Abstract (ABS)</td>
<td>468</td>
<td>72.7</td>
</tr>
<tr>
<td> Concrete (CONC)</td>
<td>176</td>
<td>27.3</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013164411404410">
<p>Note: These items came from 12 SAT forms that were administered between March 2005 and April 2006.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section9-0013164411404410">
<title>Outcome Variables</title>
<p>Three outcome variables were computed for this study. The first two outcome variables were point–biserial correlations of the item score (1, 0) with FYGPA and mathematics course grades (henceforth referred to, respectively, as the FYGPA and mathematics course grade validity coefficients). In calculation of the item score for each student, omitted items (where students did not respond to that item but did respond to at least one item placed later in the test) were scored incorrect, and items that were not reached (where students did not respond to that item or any item placed later in the test) were designated as missing. In calculation of the mathematics course grade validity coefficients, if more than one mathematics course was taken, the mathematics GPA was the average of the grades earned in each course. If only one course was taken, the mathematics GPA was the mathematics grade for the single course.</p>
<p>The third outcome variable was the percentage of students majoring in a science, technology, engineering, or math (STEM) field among those answering the item correctly (hereafter referred to as the STEM percentage). STEM fields include mathematics, natural sciences (including physical sciences and biological/agricultural sciences), engineering/engineering technologies, and computer/information sciences. Given the national interest in increasing the number of students pursuing an STEM career (e.g., <xref ref-type="bibr" rid="bibr23-0013164411404410">National Science Board, 2010</xref>), this outcome variable was included to determine whether students answering particular types of items correctly are more or less likely to major in an STEM field. This information can potentially help identify students with an interest in STEM. According to the National Postsecondary Student Aid Study of 2003-2004, 14% of all undergraduates enrolled in U.S. postsecondary institutions in 2003-2004 were enrolled in an STEM field (<xref ref-type="bibr" rid="bibr22-0013164411404410">National Center for Education Statistics, 2009</xref>). In the sample used in the current study, approximately 10% of students majored in an STEM field. The National Center for Education Statistics percentage is higher because that figure also includes students whose secondary major field was STEM, whereas in the current study only the primary majors were available.</p>
</sec>
<sec id="section10-0013164411404410">
<title>Predictor Variables</title>
<p>A set of effect code variables was created to designate the 12 separate cohorts of students taking each SAT form. As distribution of ability among students taking the test varies from administration to administration, controlling for cohort captured these range restriction effects. Effect codes were also created for each of the four item characteristics (content area, format, cognitive category, and abstract/concrete). Additional predictor variables included item difficulty (measured by equated delta, an inverse translation of proportion correct into a scale with a mean of 13 and an <italic>SD</italic> of 4, based on the curve for a normal distribution and equated over tests and samples) and item discrimination (the biserial correlation of the item score with SAT-M total score).</p>
</sec>
<sec id="section11-0013164411404410">
<title>Statistical Analyses</title>
<p>First, descriptive statistics and correlations were calculated for the predictor and outcome variables by each item characteristic. Then, two sets of multiple regression analyses were performed, regressing each of the three outcome variables (FYGPA validity coefficient, mathematics course grade validity coefficient, and STEM percentage) on the predictor variables (or sets of predictor variables). A significance level of <italic>p</italic> &lt; .01 was used as the criterion to retain predictor variables in the models. The first set of regression analyses entered only the item characteristics (effect code variables for content area, format, cognitive complexity, and abstract/concrete), while the second set included item difficulty and discrimination as covariates. In all models the effect code variables for cohort were entered first, to account for differences in the mean ability of the population of students taking the different test forms. The effect code variables for the item characteristics were entered into the models in the order in which they accounted for the most residual variance of the outcome variables. The final models entered the two- and three-way interactions among the effect codes for the item characteristics. The regression coefficients for the final models were inspected to determine whether particular item types were associated with significantly higher or lower FYGPA validity coefficients, mathematics course grade validity coefficients, and STEM percentage.</p>
</sec>
</sec>
<sec id="section12-0013164411404410" sec-type="results">
<title>Results</title>
<sec id="section13-0013164411404410">
<title>Descriptive Statistics</title>
<p><xref ref-type="table" rid="table2-0013164411404410">Table 2</xref> shows the descriptive statistics for the predictor and outcome variables. There was great variability in the outcome variables across items, and in fact some items had very small negative coefficients. As shown in <xref ref-type="table" rid="table3-0013164411404410">Table 3</xref>, item difficulty and item discrimination was generally the same across content area, which is consistent with the content and statistical specifications for the SAT mathematics test. As expected, on average the SPR items were more difficult than the multiple-choice items. Also as expected, the nonroutine/insightful items were the most difficult, and the routine items were easiest, but items in all cognitive categories were equally discriminating on average. The abstract and concrete items had similar mean difficulty and discrimination.</p>
<table-wrap id="table2-0013164411404410" position="float">
<label>Table 2.</label>
<caption>
<p>Descriptive Statistics for Continuous Variables</p>
</caption>
<graphic alternate-form-of="table2-0013164411404410" xlink:href="10.1177_0013164411404410-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Variables</th>
<th align="center">Min</th>
<th align="center">Max</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="5">Predictors</td>
</tr>
<tr>
<td> Item difficulty</td>
<td>4.6</td>
<td>20.5</td>
<td>12.5</td>
<td>3.2</td>
</tr>
<tr>
<td> Item discrimination</td>
<td>0.3</td>
<td>0.8</td>
<td>0.6</td>
<td>0.1</td>
</tr>
<tr>
<td colspan="5">Outcomes</td>
</tr>
<tr>
<td> FYGPA validity coefficients</td>
<td>−0.06</td>
<td>0.29</td>
<td>0.13</td>
<td>0.05</td>
</tr>
<tr>
<td> Mathematics course grade validity coefficients</td>
<td>−0.07</td>
<td>0.28</td>
<td>0.12</td>
<td>0.05</td>
</tr>
<tr>
<td> STEM percentage</td>
<td>0.05</td>
<td>0.24</td>
<td>0.13</td>
<td>0.02</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013164411404410">
<p>Note: FYGPA = first-year college grade point average; STEM = science, technology, engineering, or math. For all variables, <italic>N</italic> = 644.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table3-0013164411404410" position="float">
<label>Table 3.</label>
<caption>
<p>Descriptive Statistics for Item-Level Variables and Outcome Variables (Mean, <italic>SD</italic>) by Item Characteristics</p>
</caption>
<graphic alternate-form-of="table3-0013164411404410" xlink:href="10.1177_0013164411404410-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center"><italic>N</italic> Items</th>
<th align="center">Item Difficulty</th>
<th align="center">Item Discrimination</th>
<th align="center">FYGPA Validity Coefficients</th>
<th align="center">Mathematics Course Grade Validity Coefficients</th>
<th align="center">STEM Percentage</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="7">Content area</td>
</tr>
<tr>
<td> Number &amp; operations</td>
<td>131</td>
<td>12.1 (3.3)</td>
<td>.6 (0.1)</td>
<td>.12 (.05)</td>
<td>.10 (.04)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td> Algebra &amp; functions</td>
<td>254</td>
<td>12.6 (3.2)</td>
<td>.6 (0.1)</td>
<td>.14 (.05)</td>
<td>.12 (.05)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td> Geometry &amp; measurement</td>
<td>183</td>
<td>12.6 (3.1)</td>
<td>.6 (0.1)</td>
<td>.14 (.05)</td>
<td>.13 (.05)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td> Data Analysis/stat/probability</td>
<td>76</td>
<td>12.3 (3.3)</td>
<td>.5 (0.1)</td>
<td>.11 (.05)</td>
<td>.10 (.05)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td colspan="7">Format</td>
</tr>
<tr>
<td> Multiple-choice</td>
<td>526</td>
<td>12.2 (3.2)</td>
<td>.5 (0.1)</td>
<td>.13 (.05)</td>
<td>.12 (.05)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td> Student-produced response</td>
<td>118</td>
<td>14.0 (3.0)</td>
<td>.6 (0.1)</td>
<td>.14 (.05)</td>
<td>.13 (.04)</td>
<td>.13 (.03)</td>
</tr>
<tr>
<td colspan="7">Cognitive category</td>
</tr>
<tr>
<td> Routine</td>
<td>78</td>
<td>9.2 (2.5)</td>
<td>.6 (0.1)</td>
<td>.11 (.05)</td>
<td>.10 (.05)</td>
<td>.12 (.02)</td>
</tr>
<tr>
<td> Comprehension</td>
<td>406</td>
<td>12.3 (3.0)</td>
<td>.6 (0.1)</td>
<td>.13 (.05)</td>
<td>.12 (.05)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td> Nonroutine/insightful</td>
<td>160</td>
<td>14.5 (2.5)</td>
<td>.6 (0.1)</td>
<td>.13 (.04)</td>
<td>.12 (.05)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td colspan="7">Abstract/concrete</td>
</tr>
<tr>
<td> Abstract</td>
<td>468</td>
<td>12.7 (3.1)</td>
<td>.6 (0.1)</td>
<td>.14 (.05)</td>
<td>.13 (.05)</td>
<td>.13 (.02)</td>
</tr>
<tr>
<td> Concrete</td>
<td>176</td>
<td>12.1 (3.5)</td>
<td>.5 (0.1)</td>
<td>.11 (.05)</td>
<td>.10 (.05)</td>
<td>.12 (.02)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0013164411404410">
<p>Note: FYGPA = first-year college grade point average; STEM = science, technology, engineering, or math.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Item difficulty and discrimination were both moderately related to the outcome variables, warranting their inclusion in the models as covariates. Item difficulty correlated .54 with the FYPGA validity coefficients, .48 with the mathematics course grade validity coefficients, and .45 with the STEM percentage. Item discrimination correlated .44 and .47 with the FYGPA and mathematics course grade validity coefficients, respectively, but had a much smaller correlation with the STEM percentage (<italic>r</italic> = .16). All of these correlations were statistically significant at <italic>p</italic> &lt; .01.</p>
<p>The mean FYGPA and mathematics course grade validity coefficients were generally similar across content area, format, cognitive complexity, and abstract/concrete items. The validity coefficients were slightly higher for algebra and functions and geometry and measurement items, compared with number and operations, and data analysis, statistics, or probability items; were slightly higher for SPR items compared with multiple-choice items; were higher for comprehension and nonroutine/insightful items compared with routine items; and were higher for abstract items compared with concrete items (see <xref ref-type="table" rid="table3-0013164411404410">Table 3</xref>). The STEM percentage was similar for all item types. The multiple regression analyses that followed tested whether these item characteristics were significantly related to the magnitude of the outcome variables and the extent to which that relationship is moderated by item difficulty and discrimination.</p>
</sec>
<sec id="section14-0013164411404410">
<title>Multiple Regression Analyses Excluding Item Difficulty and Discrimination</title>
<p><xref ref-type="table" rid="table4-0013164411404410">Table 4</xref> shows the multiple regression results for predicting the outcome variables, with and without controlling for item difficulty and discrimination. In Model 1, the effect codes for cohort were entered into each regression model. For all three outcome variables, the <italic>R</italic><sup>2</sup> for cohort was statistically significant at <italic>p</italic> &lt; .01, indicating significant variability in the extent to which SAT-M items predicted the outcome variables depending on the particular cohort. This finding was attributed to differences in the ability level of SAT test takers at different SAT administrations. Cohort accounted for approximately 14% of the variance of the FYGPA validity coefficients, 8% of the variance of the mathematics course grade coefficients, and 35% of the variance of the STEM percentage. This indicates that the effects of cohort were much more pronounced in the prediction of the percentage of students majoring in STEM, compared with the prediction of the FYGPA and mathematics course grade validity coefficients. Because of the significant effects of cohort, the effect codes for cohort were retained in all subsequent models.</p>
<table-wrap id="table4-0013164411404410" position="float">
<label>Table 4.</label>
<caption>
<p>Multiple Regression of Item Validity Coefficients on SAT-M Item Characteristics With and Without Covariates: Change in <italic>R</italic><sup>2</sup> Associated With Each Additional Predictor</p>
</caption>
<graphic alternate-form-of="table4-0013164411404410" xlink:href="10.1177_0013164411404410-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="2">FYGPA Validity Coefficients</th>
<th align="center" colspan="2">Mathematics Course Grade Validity Coefficients</th>
<th align="center" colspan="2">STEM Percentage</th>
</tr>
<tr>
<th align="left">Model</th>
<th align="center">Predictors</th>
<th align="center">(a)</th>
<th align="center">(b)</th>
<th align="center">(c)</th>
<th align="center">(d)</th>
<th align="center">(e)</th>
<th align="center">(f)</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Cohort effect codes</td>
<td>.139<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.139<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.084<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.084<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.350<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.350<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
</tr>
<tr>
<td>2</td>
<td>Add item difficulty and discrimination</td>
<td align="center">—</td>
<td>.443<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td align="center">—</td>
<td>.375<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td align="center">—</td>
<td>.221<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
</tr>
<tr>
<td>3</td>
<td>Add abstract effect code</td>
<td>.057<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.023<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.067<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.032<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.005</td>
<td>.001</td>
</tr>
<tr>
<td>4</td>
<td>Add cognitive effect codes</td>
<td>.041<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.019<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.033<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.021<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.049<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.001</td>
</tr>
<tr>
<td>5</td>
<td>Add content effect codes</td>
<td>.023<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.006</td>
<td>.025<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.010<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.006</td>
<td>.000</td>
</tr>
<tr>
<td>6</td>
<td>Add format effect code (m-c)</td>
<td>.016<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.008<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.025<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.001</td>
<td>.022<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.001</td>
</tr>
<tr>
<td>7</td>
<td>Add 2-way interactions</td>
<td>.014</td>
<td>.020<xref ref-type="table-fn" rid="table-fn5-0013164411404410">*</xref>
</td>
<td>.012</td>
<td>.019</td>
<td>.016</td>
<td>.018</td>
</tr>
<tr>
<td>8</td>
<td>Add 3-way interactions</td>
<td>.017</td>
<td>.015</td>
<td>.012</td>
<td>.012</td>
<td>.017</td>
<td>.014</td>
</tr>
<tr>
<td/>
<td>Total <italic>R</italic><sup>2</sup> (<italic>SE</italic> est.)</td>
<td>.307 (.045)</td>
<td>.672 (.031)</td>
<td>.258 (.044)</td>
<td>.555 (.035)</td>
<td>.465 (.016)</td>
<td>.607 (.013)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0013164411404410">
<p>Note: FYGPA = first-year college grade point average; STEM = science, technology, engineering, or math. Models (a), (c), and (e) are the models without covariates; Models (b), (d), and (f) are the models including item difficulty and discrimination as covariates.</p>
</fn>
<fn id="table-fn5-0013164411404410">
<label>*</label>
<p>Change in <italic>R</italic><sup>2</sup> was statistically significant at <italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>In the models excluding item difficulty and discrimination, the effect codes for the four main item characteristics considered in this study (content area, format, abstract/concrete, and cognitive complexity) were added separately to Model 1, to determine which set of effect codes provided the most incremental validity to the prediction of the outcome variables. The effect code designating whether the item was abstract or concrete had the largest partial correlation with both the FYGPA and mathematics course grade validity coefficients, and was entered next, bringing the total <italic>R</italic><sup>2</sup> to .196 and .151, respectively. Next, the effect codes for cognitive complexity were added, which significantly increased <italic>R</italic><sup>2</sup> to .237 for the FYGPA model and .184 for the mathematics course grade model. The effect codes for content area and format also each provided a significant increment to <italic>R</italic><sup>2</sup> in both these models. In the model predicting the STEM percentage, the cognitive and format effect codes each significantly increased <italic>R</italic><sup>2</sup>. However, the effect code for abstract items and the content effect codes did not add to the prediction of the percentage of students majoring in STEM. The two-way and three-way interaction effects were not statistically significant at <italic>p</italic> &lt; .01 in any of the models.</p>
<p><xref ref-type="table" rid="table5-0013164411404410">Table 5</xref> shows the unstandardized and standardized regression coefficients for the final models. <xref ref-type="app" rid="app2-0013164411404410">Appendix B</xref> provides the full regression equations for these models. For the models excluding item difficulty and discrimination, the final model is Model 6, which includes the effect codes for cohort, abstract/concrete, cognitive complexity, content, and format and excludes the interaction effects. In the models for all three outcome variables, the regression coefficients for abstract items and nonroutine/insightful items were statistically significant at <italic>p</italic> &lt; .01, indicating that these items had larger correlations with FYGPA and mathematics course grades compared with the grand mean and that students answering these items correctly were more likely to major in an STEM field compared with the grand mean. The effect of abstract items was largest in the prediction of mathematics course grade validity coefficients (i.e., had the largest standardized regression coefficient) compared with the other two outcome variables, and the effect of nonroutine/insightful items was largest in the prediction of the STEM percentage compared with the other two outcome variables.</p>
<table-wrap id="table5-0013164411404410" position="float">
<label>Table 5.</label>
<caption>
<p>Unstandardized and Standardized Regression Coefficients for Final Models With and Without Covariates</p>
</caption>
<graphic alternate-form-of="table5-0013164411404410" xlink:href="10.1177_0013164411404410-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">FYGPA Validity Coefficients</th>
<th align="center" colspan="2">Mathematics Course Grade Validity Coefficients</th>
<th align="center" colspan="2">STEM Percentage</th>
</tr>
<tr>
<th align="left">Variable</th>
<th align="center">(a)</th>
<th align="center">(b)</th>
<th align="center">(c)</th>
<th align="center">(d)</th>
<th align="center">(e)</th>
<th align="center">(f)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Item difficulty</td>
<td align="center">—</td>
<td>.009 (.572)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td align="center">—</td>
<td>.008 (.488)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td align="center">—</td>
<td>.003 (.446)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
</tr>
<tr>
<td>Item discrimination</td>
<td align="center">—</td>
<td>.197 (.393)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td align="center">—</td>
<td>.171 (.353)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td align="center">—</td>
<td>.012 (.057)<xref ref-type="table-fn" rid="table-fn7-0013164411404410">*</xref>
</td>
</tr>
<tr>
<td>Abstract item</td>
<td>.013 (.222)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.007 (.118)<xref ref-type="table-fn" rid="table-fn7-0013164411404410">*</xref>
</td>
<td>.014 (.244)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.009 (.164)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.002 (.066)<xref ref-type="table-fn" rid="table-fn7-0013164411404410">*</xref>
</td>
<td>&lt;.0001 (.020)</td>
</tr>
<tr>
<td>Nonroutine/insightful item</td>
<td>.012 (.135)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>−.021 (−.244)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.008 (.096)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>−.011 (−.128)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.008 (.236)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.001 (.021)</td>
</tr>
<tr>
<td>Comprehension item</td>
<td>.011 (.144)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.004 (.054)</td>
<td>.010 (.142)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.006 (.090)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>&lt;−.0001 (−.003)</td>
<td>−.001 (−.038)</td>
</tr>
<tr>
<td>Algebra &amp; functions item</td>
<td>.011 (.158)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>−.002 (−.035)</td>
<td>.008 (.124)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.003 (.050)</td>
<td>.002 (.090)<xref ref-type="table-fn" rid="table-fn7-0013164411404410">*</xref>
</td>
<td>.001 (.028)</td>
</tr>
<tr>
<td>Data/analysis item</td>
<td>−.008 (−.088)</td>
<td>.001 (.012)</td>
<td>−.005 (−.056)</td>
<td>&lt;.0001 (−.005)</td>
<td>−.001 (−.037)</td>
<td>−.001 (−.019)</td>
</tr>
<tr>
<td>Geometry &amp; measurement item</td>
<td>.006 (.081)</td>
<td>.004 (.051)</td>
<td>.009 (.119)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.005 (.073)<xref ref-type="table-fn" rid="table-fn7-0013164411404410">*</xref>
</td>
<td>&lt;.0001 (.012)</td>
<td>&lt;.0001 (.006)</td>
</tr>
<tr>
<td>Multiple-choice item</td>
<td>−0.008 (−.128)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.007 (.110)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>−.010 (−.159)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>.003 (.040)</td>
<td>−.004 (−.147)<xref ref-type="table-fn" rid="table-fn8-0013164411404410">**</xref>
</td>
<td>−.001 (−.031)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0013164411404410">
<p>Note: FYGPA = first-year college grade point average; STEM = science, technology, engineering, or math. Models (a), (c), and (e) are the models without covariates; Models (b), (d), and (f) are the models including item difficulty and discrimination as covariates. The standardized regression coefficients are shown in parentheses.</p>
</fn>
<fn id="table-fn7-0013164411404410">
<label>*</label>
<p>The <italic>t</italic> value for the coefficient was statistically significant at <italic>p</italic> &lt; .05.</p>
</fn>
<fn id="table-fn8-0013164411404410">
<label>**</label>
<p>The <italic>t</italic> value for the coefficient was statistically significant at <italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The regression coefficient for comprehension items was statistically significant for both the FYGPA and mathematics course grade models but was not significant for the STEM percentage model. With regard to the content effect codes, algebra and functions items had significantly larger FYGPA and mathematics course grade validity coefficients and a larger percentage of students answering these items correctly majored in STEM compared with the grand mean. Geometry and measurement items had higher mathematics course grade validity coefficients compared with the grand mean but did not have a significant effect in predicting either of the other two outcome variables. The regression coefficient for multiple-choice items was negative and statistically significant in all three models, indicating that the multiple-choice items had smaller FYGPA and mathematics course grade validity coefficients than the SPR items, and students answering these items correctly were <italic>less</italic> likely to major in STEM, holding all other variables in the model constant.</p>
</sec>
<sec id="section15-0013164411404410">
<title>Multiple Regression Analyses Controlling for Item Difficulty and Discrimination</title>
<p>The second set of regression models shown in <xref ref-type="table" rid="table4-0013164411404410">Table 4</xref> included item difficulty and discrimination. These models assessed whether the effects of the item characteristics were mediated by item difficulty and discrimination. Entering item difficulty and discrimination significantly increased the <italic>R</italic><sup>2</sup> for each of the three outcome variables, adding most to the prediction of the FYGPA validity coefficients and least to the prediction of the STEM percentage.</p>
<p>After accounting for item difficulty and discrimination, the effect code for abstract items, the cognitive effect codes, and the effect code for multiple-choice items each significantly increased <italic>R</italic><sup>2</sup> in the FYGPA model. The set of effect codes representing all possible two-way interactions also significantly increased <italic>R</italic><sup>2</sup> in this model. Two of the interaction effect codes had regression coefficients significantly greater than zero (<italic>p</italic> &lt; .01); these were the interaction of nonroutine/insightful and multiple-choice items and the interaction of abstract and algebra and functions items. In the mathematics course grade model, all item characteristics significantly increased <italic>R</italic><sup>2</sup> with the exception of the multiple-choice effect code. In the STEM percentage model, none of the item characteristics provided an increment to <italic>R</italic><sup>2</sup> after item difficulty and discrimination were entered into the model.</p>
<p><xref ref-type="table" rid="table5-0013164411404410">Table 5</xref> displays the unstandardized and standardized regression coefficients for the final regression models including item difficulty and discrimination. The final models were Model 7 for the FYGPA validity coefficients and Model 6 for both the mathematics course grade validity coefficients and the STEM percentage. As expected, an item’s difficulty and discrimination were strong predictors in all three models, with higher values associated with higher FYGPA and mathematics course grade validity coefficients, as well as a larger percentage of students answering the item correctly and majoring in STEM.</p>
<p>After controlling for item difficulty and discrimination, the regression coefficient for abstract items was still positive and statistically significant, but the magnitude of the effect was reduced with the inclusion of these covariates in the models. The nonroutine/insightful items had a large negative effect in both the FYGPA and mathematics course grade models. Note, of course, that these findings reflect net effects after controlling for other variables in the model; recall that in the models excluding difficulty and discrimination the nonroutine/insightful items had positive regression coefficients. Comprehension items and geometry and measurement items both had statistically significant regression coefficients only in the mathematics course grade model, and the magnitude of their effects was smaller with the inclusion of item difficulty and discrimination. The positive effect of algebra and function items disappeared when item difficulty and discrimination were included in the model.</p>
<p>Interestingly, the multiple-choice effect code had a significant and positive regression coefficient in both the FYGPA and mathematics course grade models after controlling for item difficulty and discrimination, reversing the negative effect that was found in the model excluding these covariates. In the STEM percentage model, none of the regression coefficients for the item characteristic effect codes were significant after accounting for item difficulty and discrimination.</p>
</sec>
</sec>
<sec id="section16-0013164411404410" sec-type="discussion">
<title>Discussion</title>
<p>The purpose of this study was to determine whether particular characteristics of SAT mathematics items (content area, format, cognitive complexity, and abstract/concrete) were associated with the item’s ability to predict FYGPA, mathematics course grades, and the percentage of students majoring in an STEM field and whether item difficulty and discrimination mediate these effects.</p>
<p>Item difficulty and discrimination had a mediating effect on several of the predictor variables, particularly on the effects of nonroutine/insightful items and multiple-choice items. Categorization of the item as abstract or concrete was a significant predictor of an item’s validity for predicting FYGPA and mathematics course grades, whether or not item difficulty and discrimination are controlled. The abstract items were significantly better predictors of these two outcomes than the concrete items. Yet although the abstract and concrete items in this study had very similar mean difficulty and discrimination, once item difficulty and discrimination were controlled, the beneficial effect of abstract items on the validity coefficients was reduced.</p>
<p>There is research indicating that students may benefit more from learning mathematics through a single abstract, symbolic representation rather than from multiple concrete examples (<xref ref-type="bibr" rid="bibr14-0013164411404410">Kaminski et al., 2008</xref>). If it is true that students learn mathematics concepts better via abstract examples, abstract test items may be better than concrete items to assess their knowledge. There is evidence that certain groups of students (females, minorities) perform better on abstract items because these items are more similar to textbook problems than are concrete items (<xref ref-type="bibr" rid="bibr26-0013164411404410">O’Neill &amp; McPeek, 1993</xref>). Similar to the current study, <xref ref-type="bibr" rid="bibr17-0013164411404410">Koedinger et al. (2008)</xref> found an interaction between item difficulty and abstract/concrete items in the performance of college students. These findings suggest that the benefits of abstract items may be mediated by item difficulty and discrimination, and additional research is needed to tease out these effects and determine whether or not they persist for different student subgroups.</p>
<p>Although the items coded at the highest level of cognitive complexity (nonroutine/insightful) in this study were positively related to FYGPA and mathematics course grades, these items actually had significantly lower validity coefficients after controlling for their difficulty and discrimination. Although we expected that difficulty and discrimination would largely account for the validity of the items, we did not expect negative validity coefficients for the nonroutine/insightful items. This finding warrants further investigation, including a review of these particular items. One possible explanation is that, on the one hand, these items are more difficult, contributing positively to their validity, but on the other hand, their complexity or an unfamiliar setting makes them prone to misinterpretation on the part of examinees, which decreases validity after the positive effects of difficulty and discrimination are controlled.</p>
<p>Another possible confounding factor involves students’ prior knowledge and problem-solving strategies. An item is coded as nonroutine/insightful if it can be answered using multiple approaches with at least one approach involving insight, although other more direct solution paths can also be used to solve the problem (I. Cane, personal communication, September 17, 2010). The assumption is made that students use the insightful strategy, but this is not always the case. If a student happens to be familiar with the concept presented in the item, for that student, the item may actually be a routine item involving simple recall. The variability in students’ prior knowledge and problem-solving approaches may have introduced error into the coding of the nonroutine/insightful items, contributing to the unexpected results.</p>
<p>In the model not accounting for item difficulty and discrimination, multiple-choice items had significantly lower FYGPA and mathematics course grade validity coefficients compared with SPR items, and they had a negative effect on the percentage of students majoring in STEM. However, when item difficulty and discrimination were controlled, multiple-choice items had larger correlations with FYGPA and mathematics course grades and did not affect the STEM percentage. These findings suggest that since the SPR (constructed-response) items are more difficult than multiple-choice items, the larger validity coefficients for the SPR items are mostly due to their difficulty. Once difficulty is controlled, multiple-choice items may be better predictors of FYGPA and mathematics course grades than SPR items. These results suggest that multiple-choice items may perform just as well as constructed response items in predicting college outcomes, if the two-item formats are matched in difficulty and discrimination. An important caveat to note is that the constructed response items that appear on the SAT are pretested and meet specific statistical specifications. These items have a predefined and limited number of correct answers, and thus represent only one type of constructed response assessment. The item types examined here do not include forms of assessment where examinees are asked to explain their response processes or where there are multiple possible responses that can earn full credit (<xref ref-type="bibr" rid="bibr19-0013164411404410">Linn et al., 1991</xref>). Further work with other types of constructed response items is needed.</p>
<p>The content area of an item provided a significant increment to the prediction of the item’s validity coefficient for predicting FYGPA and mathematics course grades when item difficulty and discrimination were not considered. This effect disappeared in the model predicting FYGPA validity coefficients once item difficulty and discrimination were controlled but remained in the model predicting mathematics course grade validity coefficients. The items’ content area is expected to be more closely related to grades in mathematics courses than to FYGPA, since the latter is based on grades in a variety of subject areas that require different cognitive skills than those required specifically in mathematics courses. The positive effect of algebra and functions items seemed to be entirely mediated by the item’s difficulty and discrimination. On the other hand, geometry and measurement items significantly contributed to an item’s ability to predict mathematics course grade irrespective of the item’s difficulty and discrimination. This finding may be due to the fact that geometry and measurement items tap spatial and visual perception skills that are not assessed by items in the other content areas. Perhaps it is this unique component that enables these items to maintain their relationship with mathematics course grades even after their difficulty and discrimination are controlled, since the ability to understand spatial relationships is important for success in all mathematics courses (R. O’Callaghan and I. Cane, personal communication, February 9, 2011).</p>
<p>Although the results are specific to the SAT and are confined to the items on this test, they are useful to understand how test item characteristics are associated with an item’s validity for predicting college outcomes. This information may be used to help test developers increase the predictive validity of the test, although it is always necessary to maintain the test’s content and statistical specifications. The item characteristics examined in this study, including cohort, accounted for approximately 30% of the variance of the FYGPA validity coefficients, 26% of the variance of mathematics course grade validity coefficients, and 46% of the variance of the STEM percentage. When item difficulty and item discrimination were also included, the models accounted for 67% of the variance in the FYGPA validity coefficients, 56% of the variance of the mathematics course grade validity coefficients, and 61% of the variance of the STEM percentage. Research is currently underway to model the predictive validity of other types of tests (e.g., reading, writing) and attempt to identify other item characteristics that are associated with item validity. It will also be important to examine whether item characteristics are associated with differential validity for certain subgroups of test takers.</p>
</sec>
</body>
<back>
<app-group>
<app id="app1-0013164411404410">
<title>Appendix A</title>
<graphic id="img1-0013164411404410" position="anchor" xlink:href="10.1177_0013164411404410-img1.tif"/>
</app>
<app id="app2-0013164411404410">
<title>Appendix B</title>
<graphic id="img2-0013164411404410" position="anchor" xlink:href="10.1177_0013164411404410-img2.tif"/>
</app>
</app-group>
<ack>
<p>The authors would like to thank Robin O’Callaghan and Ilirjan Cane of the College Board for their assistance with this study and George Engelhard for his comments and suggestions on an earlier version of this article.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interests with respect to the authorship and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) received no financial support for the research and/or authorship of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Atkinson</surname><given-names>R. C.</given-names></name>
<name><surname>Geiser</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Reflections on a century of college admissions tests</article-title>. <source>Educational Researcher</source>, <volume>38</volume>, <fpage>665</fpage>-<lpage>676</lpage>.</citation>
</ref>
<ref id="bibr2-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bennett</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1993</year>). <article-title>On the meanings of constructed response</article-title>. In <person-group person-group-type="editor">
<name><surname>Bennett</surname><given-names>R. E.</given-names></name>
<name><surname>Ward</surname><given-names>W. C.</given-names></name>
</person-group> (Eds.), <source>Construction versus choice in cognitive measurement</source>. (pp. <fpage>1</fpage>-<lpage>27</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr3-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Breland</surname><given-names>H. M.</given-names></name>
<name><surname>Camp</surname><given-names>R.</given-names></name>
<name><surname>Jones</surname><given-names>R. J.</given-names></name>
<name><surname>Morris</surname><given-names>M. M.</given-names></name>
<name><surname>Rock</surname><given-names>D. A.</given-names></name>
</person-group> (<year>1987</year>). <source>Assessing writing skill</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr4-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bridgeman</surname><given-names>B.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Essays and multiple choice tests as predictors of college freshman GPA</article-title>. <source>Research in Higher Education</source>, <volume>32</volume>, <fpage>319</fpage>-<lpage>332</lpage>.</citation>
</ref>
<ref id="bibr5-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bridgeman</surname><given-names>B.</given-names></name>
</person-group> (<year>1992</year>). <article-title>A comparison of quantitative questions in open-ended and multiple-choice formats</article-title>. <source>Journal of Educational Measurement</source>, <volume>29</volume>, <fpage>253</fpage>-<lpage>271</lpage>.</citation>
</ref>
<ref id="bibr6-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bridgeman</surname><given-names>B.</given-names></name>
<name><surname>Lewis</surname><given-names>C.</given-names></name>
</person-group> (<year>1991</year>). <source>Sex differences in the relationship of Advanced Placement essay and multiple-choice scores to grades in college courses</source> (<comment>RR-91-48</comment>). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr7-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dorans</surname><given-names>N. J.</given-names></name>
<name><surname>Lawrence</surname><given-names>I. M.</given-names></name>
</person-group> (<year>1987</year>). <source>The internal construct validity of the SAT</source> (<comment>ETS Research Rep. RR 87-35</comment>). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr8-0013164411404410">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Doub</surname><given-names>T. W.</given-names></name>
<name><surname>Lawrence</surname><given-names>I. M.</given-names></name>
</person-group> (<conf-date>1996, April</conf-date>). <article-title>Item-level dimensionality of SAT I verbal and mathematical reasoning tests</article-title>. <conf-name>Paper presented at the meeting of the American Educational Research Association</conf-name>, <conf-loc>New York, NY</conf-loc>.</citation>
</ref>
<ref id="bibr9-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Enright</surname><given-names>M. K.</given-names></name>
<name><surname>Morley</surname><given-names>M.</given-names></name>
<name><surname>Sheehan</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Items by design: The impact of systematic feature variation on item statistical characteristics</article-title>. <source>Applied Measurement in Education</source>, <volume>15</volume>, <fpage>49</fpage>-<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr10-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ewing</surname><given-names>M.</given-names></name>
<name><surname>Huff</surname><given-names>K.</given-names></name>
<name><surname>Andrews</surname><given-names>M.</given-names></name>
<name><surname>King</surname><given-names>K.</given-names></name>
</person-group> (<year>2005</year>). <source>Assessing the reliability of skills measured by the SAT</source> (<comment>College Board Research Note RN-24</comment>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr11-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Geiser</surname><given-names>S.</given-names></name>
<name><surname>Studley</surname><given-names>R.</given-names></name>
</person-group> (<year>2002</year>). <article-title>UC and the SAT: Predictive validity and differential impact of the SAT I and SAT II at the University of California</article-title>. <source>Educational Assessment</source>, <volume>8</volume>, <fpage>1</fpage>-<lpage>26</lpage>.</citation>
</ref>
<ref id="bibr12-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gierl</surname><given-names>M. J.</given-names></name>
<name><surname>Tan</surname><given-names>X.</given-names></name>
<name><surname>Wang</surname><given-names>C.</given-names></name>
</person-group> (<year>2005</year>). <source>Identifying content and cognitive dimensions on the SAT</source> (<comment>College Board Research Rep. No. 2005-11</comment>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr13-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Haladyna</surname><given-names>T. M.</given-names></name>
</person-group> (<year>2004</year>). <source>Developing and validating multiple-choice test items</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Psychology Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kaminski</surname><given-names>J. A.</given-names></name>
<name><surname>Sloutsky</surname><given-names>V. M.</given-names></name>
<name><surname>Heckler</surname><given-names>A. F.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The advantage of abstract examples in learning math</article-title>. <source>Science</source>, <volume>320</volume>, <fpage>454</fpage>-<lpage>455</lpage>.</citation>
</ref>
<ref id="bibr15-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kobrin</surname><given-names>J. L.</given-names></name>
<name><surname>Camara</surname><given-names>W. J.</given-names></name>
<name><surname>Milewski</surname><given-names>G. B.</given-names></name>
</person-group> (<year>2002</year>). <source>The utility of the SAT I and SAT II for admissions decisions in California and the nation</source> (<comment>College Board Research Rep. No. 2002-6</comment>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr16-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kobrin</surname><given-names>J. L.</given-names></name>
<name><surname>Patterson</surname><given-names>B. F.</given-names></name>
<name><surname>Shaw</surname><given-names>E. J.</given-names></name>
<name><surname>Mattern</surname><given-names>K. D.</given-names></name>
<name><surname>Barbuti</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2008</year>). <source>Validity of the SAT for predicting first-year college grade point average</source> (<comment>College Board Research Rep. No. 2008-5</comment>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr17-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Koedinger</surname><given-names>K. R.</given-names></name>
<name><surname>Alibali</surname><given-names>M. W.</given-names></name>
<name><surname>Nathan</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Trade-offs between grounded and abstract representations: Evidence from algebra problem solving</article-title>. <source>Cognitive Science</source>, <volume>32</volume>, <fpage>366</fpage>-<lpage>397</lpage>.</citation>
</ref>
<ref id="bibr18-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Koedinger</surname><given-names>K. R.</given-names></name>
<name><surname>Nathan</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The real story behind story problems: Effects of representation on quantitative reasoning</article-title>. <source>Journal of the Learning Sciences</source>, <volume>13</volume>, <fpage>129</fpage>-<lpage>164</lpage>.</citation>
</ref>
<ref id="bibr19-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Linn</surname><given-names>R.</given-names></name>
<name><surname>Baker</surname><given-names>E. L.</given-names></name>
<name><surname>Dunbar</surname><given-names>S. B.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Complex, performance-based assessment: Expectations and validation criteria</article-title>. <source>Educational Researcher</source>, <volume>20</volume>(<issue>8</issue>), <fpage>15</fpage>-<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr20-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McDonald</surname><given-names>M.</given-names></name>
</person-group> (<year>2002</year>). <source>Systematic assessment of learning outcomes: Developing multiple-choice exams</source>. <publisher-loc>Sudbury, MA</publisher-loc>: <publisher-name>Jones &amp; Bartlett</publisher-name>.</citation>
</ref>
<ref id="bibr21-0013164411404410">
<citation citation-type="book">
<collab>National Association for College Admissions Counseling</collab>. (<year>2008</year>). <source>Report of the Commission on the Use of Standardized Tests in Undergraduate Admissions</source>. <publisher-loc>Arlington, VA</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr22-0013164411404410">
<citation citation-type="book">
<collab>National Center for Education Statistics</collab>. (<year>2009</year>). <source>Students who study science, technology, engineering, and mathematics (STEM) in postsecondary education</source> (<comment>Stats in Brief, NCES 2009-161</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr23-0013164411404410">
<citation citation-type="gov">
<collab>National Science Board</collab>. (<year>2010</year>). <source>Preparing the next generation of STEM innovators: Identifying and developing our nation’s human capital</source> (<comment>NSB-10-33</comment>). <publisher-loc>Arlington, VA</publisher-loc>: <publisher-name>Author</publisher-name>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.nsf.gov/nsb/publications/2010/nsb1033.pdf">http://www.nsf.gov/nsb/publications/2010/nsb1033.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr24-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Nunes</surname><given-names>T.</given-names></name>
<name><surname>Schliemann</surname><given-names>A. D.</given-names></name>
<name><surname>Carraher</surname><given-names>D. W.</given-names></name>
</person-group> (<year>1993</year>). <source>Street mathematics and school mathematics</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr25-0013164411404410">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Oh</surname><given-names>H.</given-names></name>
<name><surname>Sathy</surname><given-names>V.</given-names></name>
</person-group> (<year>2006</year>). <source>Construct comparability and continuity in the SAT</source> (<comment>Unpublished research report</comment>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr26-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>O’Neill</surname><given-names>K. A.</given-names></name>
<name><surname>McPeek</surname><given-names>W. M.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Item and test characteristics that are associated with differential item functioning</article-title>. In <person-group person-group-type="editor">
<name><surname>Holland</surname><given-names>P.</given-names></name>
<name><surname>Wainer</surname><given-names>H.</given-names></name>
</person-group> (Eds.), <source>Differential item functioning</source> (pp. <fpage>255</fpage>-<lpage>336</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr27-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ramist</surname><given-names>L.</given-names></name>
<name><surname>Lewis</surname><given-names>C.</given-names></name>
<name><surname>McCamley-Jenkins</surname><given-names>L.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Using achievement tests/SAT II Subject Tests to demonstrate achievement and predict college grades: Sex, language, ethnic, and parental education groups</article-title>. (<comment>College Board Research Report No. 2001-5</comment>). <publisher-loc>New York</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
<ref id="bibr28-0013164411404410">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Roussos</surname><given-names>L. A.</given-names></name>
<name><surname>Norton</surname><given-names>L. L.</given-names></name>
</person-group> (<conf-date>1998, March</conf-date>). <source>LSAT item-type validity study</source> (<comment>Tech. Rep. 98-01</comment>). <publisher-loc>Newtown, PA</publisher-loc>: <publisher-name>Law School Admission Council</publisher-name>.</citation>
</ref>
<ref id="bibr29-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ryan</surname><given-names>A. M.</given-names></name>
<name><surname>Greguras</surname><given-names>G. J.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Life is not multiple choice: Reaction to the alternatives</article-title>. In <person-group person-group-type="editor">
<name><surname>Hakel</surname><given-names>M. D.</given-names></name>
</person-group> (Ed.), <source>Beyond multiple choice: Evaluating alternatives to traditional testing for selection</source> (pp. <fpage>183</fpage>-<lpage>202</lpage>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence A. Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr30-0013164411404410">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sebrechts</surname><given-names>M. M.</given-names></name>
<name><surname>Enright</surname><given-names>M.</given-names></name>
<name><surname>Bennett</surname><given-names>R. E.</given-names></name>
<name><surname>Martin</surname><given-names>K.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Using algebra word problems to assess quantitative ability: Attributes, strategies, and errors</article-title>. <source>Cognition and Instruction</source>, <volume>14</volume>, <fpage>285</fpage>-<lpage>343</lpage>.</citation>
</ref>
<ref id="bibr31-0013164411404410">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vasavada</surname><given-names>N.</given-names></name>
<name><surname>Carman</surname><given-names>E.</given-names></name>
<name><surname>Hart</surname><given-names>B.</given-names></name>
<name><surname>Luisier</surname><given-names>D.</given-names></name>
</person-group> (<year>2010</year>). <source>Common core state standards alignment: ReadiStep, PSAT/NMSQT and SAT</source> (<comment>College Board Research Rep. No. 2010-5A</comment>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Board</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>