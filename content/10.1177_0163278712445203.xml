<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EHP</journal-id>
<journal-id journal-id-type="hwp">spehp</journal-id>
<journal-id journal-id-type="nlm-ta">Eval Health Prof</journal-id>
<journal-title>Evaluation &amp; the Health Professions</journal-title>
<issn pub-type="ppub">0163-2787</issn>
<issn pub-type="epub">1552-3918</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0163278712445203</article-id>
<article-id pub-id-type="publisher-id">10.1177_0163278712445203</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Professional Issue: “Occupation Evaluation”</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Score Comparability Study for the NBDHE</article-title>
<subtitle>Paper–Pencil Versus Computer Versions</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Tsai</surname>
<given-names>Tsung-Hsun</given-names>
</name>
<xref ref-type="aff" rid="aff1-0163278712445203">1</xref>
<xref ref-type="corresp" rid="corresp1-0163278712445203"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shin</surname>
<given-names>Chingwei David</given-names>
</name>
<xref ref-type="aff" rid="aff2-0163278712445203">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0163278712445203">
<label>1</label>Department of Testing Services, American Dental Association, Chicago, IL, USA</aff>
<aff id="aff2-0163278712445203">
<label>2</label>Psychometrics &amp; Research Services, Assessment &amp; Information Group, Pearson,, Iowa City, IA, USA</aff>
<author-notes>
<corresp id="corresp1-0163278712445203">Tsung-Hsun Tsai, Department of Testing Services, American Dental Association, 211 E. Chicago Avenue, Suite 600, Chicago, IL 60611, USA. Email: <email>tsait@ada.org</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2013</year>
</pub-date>
<volume>36</volume>
<issue>2</issue>
<fpage>228</fpage>
<lpage>239</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This study evaluated the comparability of a paper–pencil (PP) version and two computer-based (CB) versions of the National Board Dental Hygiene Examination. Comparability was evaluated by validity and psychometric criteria. Data were collected from the following resources: (1) 4,560 candidates enrolled in accredited dental hygiene programs who took the PP version in the Spring 2009, (2) 973 and 1,033 candidates enrolled in accredited dental hygiene programs who took two separate CB versions in 2009, and (3) the survey data from 2,486 candidates who took the CB versions in 2009. The results from the PP and CB versions were found to be comparable on several criteria.</p>
</abstract>
<kwd-group>
<kwd>comparability</kwd>
<kwd>validity</kwd>
<kwd>NBDHE</kwd>
<kwd>factor analytic approach</kwd>
<kwd>Rasch IRT model</kwd>
<kwd>beta-binomial model</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>In recent years, many testing programs transited their tests from paper–pencil (PP) tests to computer-based (CB) tests due to a number of advantages that the CB test offers such as shorter testing time, flexible scheduling, faster score reporting, opportunity to include innovative items, and reduced costs of test production. Professional testing standards (American Psychological Association Committee on Professional Standards and Committee on Psychological Tests and Assessments <xref ref-type="bibr" rid="bibr3-0163278712445203">[APA], 1986</xref>; American Educational Research Association [AERA], APA, &amp; National Council on Measurement in Education <xref ref-type="bibr" rid="bibr2-0163278712445203">[NCME], 1999</xref>) and the <italic>Standards and Assessments Peer Review Guidance</italic> (<xref ref-type="bibr" rid="bibr22-0163278712445203">U.S. Department of Education, 2007</xref>) stress the need to study score comparability across test administration media. During the last two decades, numerous comparability studies have been conducted on a variety of educational and psychological tests to examine whether test presentation mode affects examinee performance. These studies revealed mixed conclusions concerning the comparability of CB and PP tests. Some studies indicate that a CB test is more difficult than a PP test or vice versa (e.g., <xref ref-type="bibr" rid="bibr6-0163278712445203">Choi &amp; Tinkler, 2002</xref>) while some studies show that CB tests and PP tests are comparable (e.g., <xref ref-type="bibr" rid="bibr9-0163278712445203">Kim &amp; Hyunh, 2007</xref>). Given this reason, for any testing program that offers both computer and paper versions of a test, a comparability study must be conducted to determine whether scores from the computer and paper versions of the test are equivalent.</p>
<p>This study evaluated the comparability of a PP and two computer versions of the National Board Dental Hygiene Examination (NBDHE). Comparability was evaluated by validity and psychometric criteria. The following testing parameters were satisfied for the NBDHE program: (1) all PP and CB versions of the NBDHE were built from the same content and statistical specifications; as a result, all versions were parallel with respect to content and difficulty (<xref ref-type="bibr" rid="bibr1-0163278712445203">Allen &amp; Yen, 1979</xref>) and (2) scores for all versions were placed on the same scale using the common-item equivalent groups design (<xref ref-type="bibr" rid="bibr11-0163278712445203">Kolen &amp; Brennan, 1995</xref>) under the Rasch model (<xref ref-type="bibr" rid="bibr19-0163278712445203">Rasch, 1960/1980</xref>).</p>
<sec id="section1-0163278712445203">
<title>Perspective/Framework</title>
<p>Due to an increase in the use of computer technology in testing programs, a computer version is often offered to examinees in addition to a traditional PP version. The literature on administration mode effects in a variety of testing situations was reviewed. While a majority of studies indicate that scores are comparable with no statistically significant mode effects on test performance (<xref ref-type="bibr" rid="bibr9-0163278712445203">Kim &amp; Hyunh, 2007</xref>; <xref ref-type="bibr" rid="bibr10-0163278712445203">Kingston, 2009</xref>; <xref ref-type="bibr" rid="bibr15-0163278712445203">Paek, 2005</xref>; <xref ref-type="bibr" rid="bibr24-0163278712445203">Wang, Jiao, Young, Brooks, &amp; Olson, 2007</xref>, <xref ref-type="bibr" rid="bibr25-0163278712445203">2008</xref>), some studies reported mode effects (<xref ref-type="bibr" rid="bibr4-0163278712445203">Bennett et al., 2008</xref>; <xref ref-type="bibr" rid="bibr6-0163278712445203">Choi &amp; Tinkler, 2002</xref>; <xref ref-type="bibr" rid="bibr13-0163278712445203">Mazzeo &amp; Harvey, 1988</xref>; <xref ref-type="bibr" rid="bibr14-0163278712445203">Mead &amp; Drasgow, 1993</xref>; <xref ref-type="bibr" rid="bibr21-0163278712445203">Texas Education Agency, 2008</xref>). The inconsistency in these findings may be attributed to the fact that these comparability studies involved a wide range of variations in test characteristics—speeded tests versus nonspeeded tests, content areas, examinees’ characteristics, data collection designs, and test formats. In this study, any significant difference in candidates’ performance on the PP and CB versions can be attributed to the administration mode effects (<xref ref-type="bibr" rid="bibr23-0163278712445203">Wang &amp; Kolen, 2001</xref>).</p>
<p>
<xref ref-type="bibr" rid="bibr12-0163278712445203">Leeson (2006)</xref> provided a review on some potential factors contributing to observed mode effects—classified broadly as examinee and technological factors. The examinee factors consist of the examinees’ demographic characteristics—gender and ethnicity, cognitive processing, ability levels, familiarity with computers, and anxiety when interacting with computers. The technological factors involve computer interface legibility—screen size and resolution, font characteristics, line length, and user interface such as scrolling to locate reading, the ability to review and change completed items, and how to present the items (one at a time or several at a time). The factor that this study is specifically interested in is the candidates’ experience with computer technology and the ramification on the test performance. To accomplish this objective, the post–examination survey data were used to evaluate the candidates’ experience with the computer technology. This was based on the assumption that the comparability of the PP and CB versions might be impacted by the candidates’ familiarity with computers.</p>
<p>There are numerous delivery methods for an examination that is administered on computer. The Joint Commission on National Dental Examinations (Joint Commission), which is responsible for conducting the NBDHE, uses linear computer-based testing (LCBT), that is, items on a CB version are delivered in random order within an intact linear format. In this study, all the PP and CB versions were assembled from the same content and statistical specifications. Assuming that candidates’ underlying abilities are similar whichever version they take, any difference in candidates’ performance would be attributed to the mode in which the examination is administered. While LCBT is the commonly used delivery method, two major questions are:</p>
<list list-type="order">
<list-item>
<p>Are scores on the computer–administered NBDHE exams comparable to scores on PP exams that assess parallel content?</p>
</list-item>
<list-item>
<p>If not, what might be the potential factors?</p>
</list-item>
</list>
</sec>
<sec id="section2-0163278712445203" sec-type="methods">
<title>Method</title>
<sec id="section3-0163278712445203">
<title>NBDHE</title>
<p>The NBDHE is designed to assist state boards in assessing the qualifications of individuals who seek licenses to practice dental hygiene. The examination is typically taken by student candidates during the last year of a dental hygiene program. The NBDHE assesses the candidate’s ability to understand important information from basic biomedical, dental, and dental hygiene sciences and the ability to apply such information in a problem-solving context. This comprehensive examination consists of 350 multiple-choice items. Items are balanced within multiple disciplines from which the items are sampled. Items are presented with a stem pairing a question or statement with a list of four or five possible responses. The examination includes 200 discipline-based items and 150 items based on 12–15 dental hygiene patient cases. Each case presented in the examination consists of: (1) patient histories, (2) dental charts, (3) diagnostic radiographs, and (4) clinical photographs.</p>
</sec>
<sec id="section4-0163278712445203">
<title>A Post–examination Survey</title>
<p>For the CB versions, candidates are offered a post–examination survey. The survey consists of questions relating to the content and format of the examination as well as the candidates’ experience with computer technology. Candidates also get an opportunity to make comments on their perceptions of their testing experiences.</p>
</sec>
<sec id="section5-0163278712445203">
<title>Data</title>
<p>The data used in this study were collected from the following samples: (1) the item responses from 4,690 candidates enrolled in accredited dental hygiene programs who took the PP version in the Spring 2009, (2) the item responses from 973 and 1,033 candidates enrolled in accredited dental hygiene programs who took two separate CB versions in 2009, and (3) the survey data from 2,486 candidates who took the CB versions in 2009 and responded to the survey.</p>
</sec>
<sec id="section6-0163278712445203">
<title>Analyses</title>
<p>In light of the recommendations from <xref ref-type="bibr" rid="bibr23-0163278712445203">Wang and Kolen (2001)</xref>, analyses based on validity and psychometric criteria were conducted to evaluate the comparability of the PP and CB versions of the NBDHE. In addition to the validity and psychometric criteria analyses, the survey data from a total of 2,486 candidates who took the CB versions in 2009 and responded to the survey were analyzed to understand candidates’ testing experiences with computer technology and a potential impact on their test performance.</p>
<p>With regard to the validity criterion, the construct-related validity was examined to ensure that the underlying construct measured by the PP and CB versions is the same. While several approaches can be used to evaluate the construct-related validity, this study adopted the factor analytic approach through the use of a principal component analysis (PCA; <xref ref-type="bibr" rid="bibr16-0163278712445203">Pearson, 1901</xref>). The statistical package for the social sciences (<xref ref-type="bibr" rid="bibr20-0163278712445203">SPSS Inc., 2009</xref>) was used to conduct PCA.</p>
<p>Regarding the psychometric criterion, the reliability of the pass/fail score, the candidates’ performance, and the assumption of the Rasch item response theory (IRT) model were examined. The Rasch IRT model is used to assemble, equate, and score the examination and report the examination results. One major IRT assumption is unidimensionality. Again, the factor analytic approach was used to determine the dimensionality of the PP and CB versions of the examination. As a high-stake licensure examination, it is critical to ensure that the pass/fail score is reliable and valid when the licensing decision is made on the basis of the examination results. Therefore, an analysis, which used the <italic>Beta-Binomial</italic> model derived by <xref ref-type="bibr" rid="bibr8-0163278712445203">Hanson and Brennan (1990)</xref>, was conducted to compute the reliability of the pass/fail score for the PP and CB versions of the NBDHE in this study. The candidates’ performance on the NBDHE was reported in terms of one overall standard score as the examination is comprehensive in structure. Score conversions were developed to convert the raw scores, that is, total number of correct responses, to standard scores for all versions of the examination through the equating process. The score scale ranges from 49 to 99, and 75 is the minimum passing score. The difference in the candidates’ performance, which is quantified by the standard score mean difference, was examined in determining any significant difference in the PP and CB versions. To visualize the difference in the candidates’ performance, the standard score distributions for the PP and CB versions are presented in a graphical format.</p>
</sec>
</sec>
<sec id="section7-0163278712445203">
<title>Results</title>
<p>Results from the analyses on various criteria are reported below.</p>
<sec id="section8-0163278712445203">
<title>Descriptive Statistics</title>
<p>A summary of descriptive statistics for the PP and CB versions is presented in <xref ref-type="table" rid="table1-0163278712445203">Table 1</xref>. The following statistics are reported: (1) the number of items, (2) the sample size, that is, the number of candidates enrolled in accredited dental hygiene programs and took the examination, (3) the score reliability coefficient, KR<sub>20</sub>, and (4) the difficulty levels.</p>
<table-wrap id="table1-0163278712445203" position="float">
<label>Table 1.</label>
<caption>
<p>Descriptive Statistics.</p>
</caption>
<graphic alternate-form-of="table1-0163278712445203" xlink:href="10.1177_0163278712445203-table1.tif"/>
<table>
<thead>
<tr>
<th>Version</th>
<th align="center">Paper–pencil</th>
<th align="center" colspan="2">Computer</th>
</tr>
<tr>
<th>Form</th>
<th>1</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of items</td>
<td>350</td>
<td>350</td>
<td>350</td>
</tr>
<tr>
<td>Sample size</td>
<td>4,560</td>
<td>973</td>
<td>1,033</td>
</tr>
<tr>
<td>KR<sub>20</sub>
</td>
<td>0.89</td>
<td>0.91</td>
<td>0.91</td>
</tr>
<tr>
<td>Difficulty (%)</td>
<td>69.8</td>
<td>67.9</td>
<td>64.3</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>A significantly greater number of candidates taking the traditional PP version in Spring 2009 was found because this was the last opportunity for candidates to take the PP version as the examination was completely converted to CB versions after the Spring 2009 administration. The score reliability coefficient, KR<sub>20</sub>, measuring internal consistency, is reported as a standard indicator of the score reliability since all items on the NBDHE are dichotomously scored. As shown, comparable score reliabilities of PP and CB versions were found. The difficulty level of the examination is measured by the average percentage of the examination items answered correctly by the candidates. While the difference in the difficulty levels of the PP and CB versions was found, it was not statistically significant at the .05 level. All the difficulty levels fall between 60% and 70%.</p>
</sec>
<sec id="section9-0163278712445203">
<title>The Survey Results</title>
<p>Immediately following the administration of a CB version, candidates are afforded an opportunity to complete a survey related to the content and format of the examination as well as their experience with computer technology. The responses of candidates to the survey provided this study with a reasonably clear indication of the quality of the CB versions of the examination and the candidates’ experience with computer technology. A representative sample of the responses from 2,486 survey respondents in 2009 is summarized below:</p>
<list list-type="bullet">
<list-item>
<p>At least 93% of candidates responded to each of the survey questions.</p>
</list-item>
<list-item>
<p>Approximately 94% of candidates were either comfortable or very comfortable with computer technology.</p>
</list-item>
<list-item>
<p>About 92% of candidates were either satisfied or very satisfied with the time allotted to take a CB version of the examination.</p>
</list-item>
<list-item>
<p>At least 94% of candidates were either satisfied or very satisfied with the speed in which the computer responded to user action.</p>
</list-item>
</list>
<p>Overall, the survey results showed that a majority of candidates taking the CB versions were comfortable with using the computer technology to take the examination and were satisfied with the speed in which the computer responds to user action and time allotted to take the computer version of the examination.</p>
</sec>
<sec id="section10-0163278712445203">
<title>Factor Analytic Approach</title>
<p>The factor analytic approach was used to evaluate the comparability of the PP and CB versions for this study with regard to validity and statistical assumption criteria. The results of the factor analysis, including the percentage of total score variance explained by the first component and the number of components extracted, are presented in <xref ref-type="table" rid="table2-0163278712445203">Table 2</xref>. As shown, the total percentage of score variance that can be explained by the first principal component for the PP and CB versions was high, that is, &gt;90%. There was one principal component extracted through PCA. Results supported the conclusion that the PP and CB versions of the examination measure the same underlying construct and are unidimensional in nature.</p>
<table-wrap id="table2-0163278712445203" position="float">
<label>Table 2.</label>
<caption>
<p>Results From Factor Analysis.</p>
</caption>
<graphic alternate-form-of="table2-0163278712445203" xlink:href="10.1177_0163278712445203-table2.tif"/>
<table>
<thead>
<tr>
<th>Version</th>
<th>Paper–pencil</th>
<th colspan="2">Computer</th>
</tr>
<tr>
<th>Form</th>
<th>1</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr>
<td>% of variance explained</td>
<td>90.16</td>
<td>90.68</td>
<td>91.71</td>
</tr>
<tr>
<td>Number of components extracted</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section11-0163278712445203">
<title>Analyses Utilizing the Psychometric Criterion</title>
<p>
<xref ref-type="table" rid="table3-0163278712445203">Table 3</xref> presents a summary of the results from the analyses of the pass/fail score reliability and candidates’ performance. Measures of candidates’ test performance are the standard score means and standard deviations.</p>
<table-wrap id="table3-0163278712445203" position="float">
<label>Table 3.</label>
<caption>
<p>Results From Analyses Utilizing the Psychometric Criterion.</p>
</caption>
<graphic alternate-form-of="table3-0163278712445203" xlink:href="10.1177_0163278712445203-table3.tif"/>
<table>
<thead>
<tr>
<td>Version</td>
<td align="center">Paper–pencil</td>
<td align="center" colspan="2">Computer</td>
</tr>
<tr>
<td>Form</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">2</td>
</tr>
</thead>
<tbody>
<tr>
<td>Pass/fail score reliability </td>
<td>0.97</td>
<td>0.98</td>
<td>0.97</td>
</tr>
<tr>
<td>Standard score mean</td>
<td>84.44</td>
<td>83.42</td>
<td>82.80</td>
</tr>
<tr>
<td>Standard score <italic>SD</italic>
</td>
<td>5.19</td>
<td>5.17</td>
<td>4.92</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>As shown, a high reliability of the pass/fail score for the PP and CB versions was found. The differences of the standard score means and standard deviations in the PP and CB versions were statistically insignificant at the .05 level. A lower standard score mean achieved by the candidates who took the other CB version was found, however.</p>
<p>The candidates’ performance, which were also displayed by the standard score distributions, on the PP and CB versions are presented in <xref ref-type="fig" rid="fig1-0163278712445203">Figure 1</xref>. The <italic>x</italic>-axis represents a full range of the standard score, 49 to 99, where a standard score of 75 represents the minimum passing standard score. The <italic>y</italic>-axis is the percentage of candidates receiving each score. As shown, the score distributions for the PP and CB versions were similar.</p>
<fig id="fig1-0163278712445203" position="float">
<label>Figure 1.</label>
<caption>
<p>Score distributions.</p>
</caption>
<graphic alternate-form-of="fig1-0163278712445203" xlink:href="10.1177_0163278712445203-fig1.tif"/>
</fig>
</sec>
</sec>
<sec id="section12-0163278712445203">
<title>Discussion</title>
<p>This study compared candidates’ performance on the PP and CB versions of the NBDHE. The differences in candidates’ performance on the PP and CB versions were insignificant at the .05 level. The results were found to be comparable on several criteria. The study concluded that the administration mode did not affect candidates’ performance.</p>
<p>Some observations were noted during the conversion process. First, certain items were not easily converted or presented on a computer screen. Second, the procedures for achieving score comparability were different. For the PP version, the postequating procedure was used after administering the examination while the preequating procedure was used for the CB versions before administering the examination. As a result, differential equating procedures might be another source contributing to score discrepancies in the PP and CB versions of the NBDHE. Third, candidates performed poorly on the CB versions relative to the PP version. This might be due to candidates’ lack of experience in computer testing. One way to reduce this effect is to offer computer-based practice tests to the candidates. Fourth, the difference in item order on the PP and CB versions was found. As a result, differential item-order effects might occur during the conversion process. One may suspect that the performance on the CB and PP versions might be different because the CB version usually does not provide opportunity for candidates to review and change answers like the PP version does. This is not the case, however, in NBDHE. When candidates take the NBDHE (PP and CB versions), candidates are allowed to review and change their answers at the morning session during the morning, and the afternoon session during the afternoon (only). Candidates are not allowed to review and changes answers from the morning session in the afternoon. Once candidates have handed in their paper booklets, or ended the computer examination, they are done.</p>
<p>This article provides a framework and several criteria to evaluate comparability of the PP and CB versions of the NBDHE. Observations during the conversion process were also noted and documented. For those practitioners in licensure testing programs wishing to develop a plan or conduct analyses to evaluate comparability of the PP and CB versions, this article provides a framework, evaluation criteria, and practical considerations.</p>
<p>The next study will extend the scope of this study by addressing the above–mentioned observations. Specifically, an ad hoc content review process will be introduced to enhance the content validity of the CB versions. That is, the content in the computer versions of the examination will be balanced with the content in the PP versions. Additionally, the effects of post- versus preequating procedures on the test performance will be further assessed. The performance for the candidates taking and not taking the practice tests on the CB versions will be examined; the results will also be compared to the candidates taking the PP versions. Moreover, the test performance will be further examined to understand if differential item-order effects occur during the conversion process.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors thank Hong Wang’s contribution to the review of literature and Jason L. Meyers’ comments on the early version of this article, which was accepted by the American Educational Research Association Annual Meeting in 2010. The authors also thank three anonymous reviewers and the editor for their comments on this article.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn1-0163278712445203">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn2-0163278712445203">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0163278712445203">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Allen</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Yen</surname>
<given-names>W. M.</given-names>
</name>
</person-group> (<year>1979</year>). <source>Introduction to measurement theory</source>. <publisher-loc>Belmont, CA</publisher-loc>: <publisher-name>Wadsworth</publisher-name>.</citation>
</ref>
<ref id="bibr2-0163278712445203">
<citation citation-type="book">
<collab collab-type="author">American Educational Research Association (AERA), American Psychological Association (APA), and the National Council on Measurement in Education (NCME).</collab> (<year>1999</year>). <source>Standards for educational and psychological testing</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr3-0163278712445203">
<citation citation-type="book">
<collab collab-type="author">American Psychological Association Committee on Professional Standards and Committee on Psychological Tests and Assessments (APA).</collab> (<year>1986</year>). <source>Guidelines for computer-based tests and interpretations</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr4-0163278712445203">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Bennett</surname>
<given-names>R. E.</given-names>
</name>
<name>
<surname>Braswell</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Oranje</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Sandene</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Kaplan</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Yan</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Does it matter if I take my mathematics test on computer? A second empirical study of mode effects in NAEP</article-title>. <source>Journal of Technology, Learning, and Assessment</source>, <volume>6</volume>(<issue>9</issue>). <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.jtla.org">http://www.jtla.org</ext-link>.</citation>
</ref>
<ref id="bibr6-0163278712445203">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Choi</surname>
<given-names>S. W.</given-names>
</name>
<name>
<surname>Tinkler</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>2002</year>). <source>Evaluating comparability of paper and computer based assessment in a K-12 setting</source>. <conf-name>Paper presented at the Annual Meeting of the National Council on Measurement in Education</conf-name>, <conf-loc>New Orleans, LA</conf-loc>.</citation>
</ref>
<ref id="bibr8-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hanson</surname>
<given-names>B. A.</given-names>
</name>
<name>
<surname>Brennan</surname>
<given-names>R. L.</given-names>
</name>
</person-group> (<year>1990</year>). <article-title>An investigation of classification consistency indexes estimated under alternative strong true score models</article-title>. <source>Journal of Educational Measurement</source>, <volume>27</volume>, <fpage>345</fpage>–<lpage>359</lpage>.</citation>
</ref>
<ref id="bibr9-0163278712445203">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Kim</surname>
<given-names>D.-H.</given-names>
</name>
<name>
<surname>Huynh</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Comparability of computer and paper-and-pencil versions of algebra and biology assessments</article-title>. <source>Journal of Technology, Learning, and Assessment</source>, <volume>6</volume>(<issue>4</issue>). <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.jtla.org">http://www.jtla.org</ext-link>.</citation>
</ref>
<ref id="bibr10-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kingston</surname>
<given-names>N. M.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Comparability of computer- and paper-administered multiple-choice tests for K-12 populations: A synthesis</article-title>. <source>Applied Measurement in Education</source>, <volume>22</volume>, <fpage>22</fpage>–<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr11-0163278712445203">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kolen</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Brennan</surname>
<given-names>R. L.</given-names>
</name>
</person-group> (<year>1995</year>). <source>Test equating: Methods and practices</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr12-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leeson</surname>
<given-names>H. V.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>The mode effect: A literature review of human and technological issues in computerized testing</article-title>. <source>International Journal of Testing</source>, <volume>6</volume>, <fpage>1</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr13-0163278712445203">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Mazzeo</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Harvey</surname>
<given-names>A. L.</given-names>
</name>
</person-group> (<year>1988</year>). <source>The equivalence of scores from automated and conventional educational and psychological tests: A review of the literature</source> <comment>(College Board Report 88-8)</comment>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>College Entrance Examination Board</publisher-name>.</citation>
</ref>
<ref id="bibr14-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mead</surname>
<given-names>A. D.</given-names>
</name>
<name>
<surname>Drasgow</surname>
<given-names>F.</given-names>
</name>
</person-group> (<year>1993</year>). <article-title>Equivalence of computerized and paper-and-pencil cognitive ability tests: A meta-analysis</article-title>. <source>Psychological Bulletin</source>, <volume>114</volume>, <fpage>449</fpage>–<lpage>458</lpage>.</citation>
</ref>
<ref id="bibr15-0163278712445203">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Paek</surname>
<given-names>P</given-names>
</name>
</person-group>. (<year>2005</year>). <source>Recent trends in comparability studies</source> <comment>(Pearson Research Report 08-2005)</comment>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.pearsonassessments.com/comparabilitystudies">http://www.pearsonassessments.com/comparabilitystudies</ext-link>.</citation>
</ref>
<ref id="bibr16-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author"><name><surname>Pearson</surname><given-names>K</given-names></name></person-group>. (<year>1901</year>). <article-title>On lines and planes of closest fit to systems of points in space</article-title>. <source>Philosophical Magazine</source>, <volume>2</volume>, <fpage>559</fpage>–<lpage>572</lpage>.</citation>
</ref>
<ref id="bibr18-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pomplun</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Ritchie</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Custer</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Factors in paper-and-pencil and computer reading score differences at the primary grades</article-title>. <source>Educational Assessment</source>, <volume>11</volume>, <fpage>127</fpage>–<lpage>143</lpage>.</citation>
</ref>
<ref id="bibr19-0163278712445203">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Rasch</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>1980</year>). <source>Probabilistic models for some intelligence and attainment tests</source> <comment>(Copenhagen, Danish Institute for Educational Research), expanded edition (1980) with foreword and afterword by</comment> <person-group person-group-type="editor">
<name>
<surname>Wright</surname>
<given-names>B.D.</given-names>
</name>
</person-group> <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name> <comment>(Original work published 1960)</comment>.</citation>
</ref>
<ref id="bibr20-0163278712445203">
<citation citation-type="book">
<collab collab-type="author">SPSS Inc.</collab> (<year>2009</year>). <source>SPSS<sup>®</sup> user’s guide</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr21-0163278712445203">
<citation citation-type="web">
<collab collab-type="author">Texas Education Agency</collab>. (<year>2008</year>). <source>A review of literature on the comparability of scores obtained from examinees on computer-based and paper-based tests</source>. <comment>Retrieved</comment> <comment>from</comment> <ext-link ext-link-type="uri" xlink:href="http://ritter.tea.state.tx.us/student.assessment/resources/techdigest/Technical_Reports/2008_literature_review_of_comparability_report.pdf">http://ritter.tea.state.tx.us/student.assessment/resources/techdigest/Technical_Reports/2008_literature_review_of_comparability_report.pdf</ext-link>
</citation>
</ref>
<ref id="bibr22-0163278712445203">
<citation citation-type="web">
<collab collab-type="author">U.S. Department of Education</collab>. (<year>2007</year>). <source>Standards and assessments peer review guidance: Information and examples for meeting requirements of the no child left behind act of 2001</source>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www2.ed.gov/policy/elsec/guid/saaprguidance.doc">http://www2.ed.gov/policy/elsec/guid/saaprguidance.doc</ext-link>.</citation>
</ref>
<ref id="bibr23-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Kolen</surname>
<given-names>M. J.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Evaluating comparability in computerized adaptive testing: Issues, criteria and an example</article-title>. <source>Journal of Educational Measurement</source>, <volume>38</volume>, <fpage>19</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr24-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Jiao</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Young</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Brooks</surname>
<given-names>T. E.</given-names>
</name>
<name>
<surname>Olson</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A meta-analysis of testing mode effects in Grade K–12 mathematics tests</article-title>. <source>Educational and Psychological Measurement</source>, <volume>67</volume>, <fpage>219</fpage>–<lpage>238</lpage>.</citation>
</ref>
<ref id="bibr25-0163278712445203">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Jiao</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Young</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Brooks</surname>
<given-names>T. E.</given-names>
</name>
<name>
<surname>Olson</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Comparability of computer-based and paper-and-pencil testing in K-12 assessment: A meta-analysis of testing mode effects</article-title>. <source>Educational and Psychological Measurement</source>, <volume>68</volume>, <fpage>5</fpage>–<lpage>24</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>