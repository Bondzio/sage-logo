<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JTE</journal-id>
<journal-id journal-id-type="hwp">spjte</journal-id>
<journal-title>Journal of Teacher Education</journal-title>
<issn pub-type="ppub">0022-4871</issn>
<issn pub-type="epub">1552-7816</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022487111421175</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022487111421175</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Predicting Performance</article-title>
<subtitle>A Comparison of University Supervisors’ Predictions and Teacher Candidates’ Scores on a Teaching Performance Assessment</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Sandholtz</surname><given-names>Judith Haymore</given-names></name>
<xref ref-type="aff" rid="aff1-0022487111421175">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Shea</surname><given-names>Lauren M.</given-names></name>
<xref ref-type="aff" rid="aff1-0022487111421175">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022487111421175"><label>1</label>University of California, Irvine, USA</aff>
<author-notes>
<corresp id="corresp1-0022487111421175">Judith Haymore Sandholtz, Department of Education, University of California, Irvine, 3200 Education Building, CA 92697-5500, USA Email: <email>judith.sandholtz@uci.edu</email></corresp>
<fn fn-type="other" id="bio1-0022487111421175">
<p>Judith Haymore Sandholtz is an associate professor in the Department of Education at the University of California, Irvine. Her research focuses on teacher professional development, teacher education, school–university partnerships, and technology in education.</p>
</fn>
<fn fn-type="other" id="bio2-0022487111421175">
<p>Lauren M. Shea is a doctoral candidate in the Department of Education at the University of California, Irvine. Her research interests include professional development for teachers of English Language Learners and online blended methodologies in teacher education.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2012</year>
</pub-date>
<volume>63</volume>
<issue>1</issue>
<fpage>39</fpage>
<lpage>50</lpage>
<permissions>
<copyright-statement>© 2012 American Association of Colleges for Teacher Education</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Association of Colleges for Teacher Education</copyright-holder>
</permissions>
<abstract>
<p>The implementation of teaching performance assessments has prompted a range of concerns. Some educators question whether these assessments provide information beyond what university supervisors gain through their formative evaluations and classroom observations of candidates. This research examines the relationship between supervisors’ predictions and candidates’ performance on a summative assessment based on a capstone teaching event, the Performance Assessment for California Teachers. The study, based on records for 337 teacher candidates over a 2-year period, specifically addresses the following questions: To what extent do university supervisors predict candidates’ total scores? On which questions and categories of the assessment do supervisors most accurately predict their candidates’ scores? Do supervisors predict scores more accurately for high- and low-performing candidates? The findings indicate that university supervisors’ perspectives did not always correspond with outcomes on the performance assessment, particularly for high and low performers.</p>
</abstract>
<kwd-group>
<kwd>preservice education</kwd>
<kwd>assessment</kwd>
<kwd>supervision</kwd>
<kwd>teaching performance assessment</kwd>
</kwd-group>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>January/February 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>The assessment of teaching practice continues to be a significant issue for teacher education programs. Federal legislation requires that graduates’ performance on licensing tests be included in evaluations of schools of education (<xref ref-type="bibr" rid="bibr8-0022487111421175">Darling-Hammond, 2006</xref>), and legislation in California requires that teacher certification programs implement a performance assessment to evaluate candidates’ mastery of specified teaching performance expectations (<xref ref-type="bibr" rid="bibr5-0022487111421175">California Commission on Teacher Credentialing [CCTC], 2006</xref>). The implementation of performance assessments has prompted a range of concerns. Some concerns center on the difficulty of defining teaching and the reliability of performance assessments, but teacher educators also argue that the assessments limit the richness of their programs and harm the nature of relationships essential for learning (<xref ref-type="bibr" rid="bibr36-0022487111421175">Snyder, 2009</xref>). A key concern across programs is the cost, which, combined with a lack of funding (<xref ref-type="bibr" rid="bibr14-0022487111421175">Guaglianone, Payne, Kinsey, &amp; Chiero, 2009</xref>; <xref ref-type="bibr" rid="bibr31-0022487111421175">Porter, Youngs, &amp; Odden, 2001</xref>), leads teacher educators to question whether resources could be better spent in other ways (<xref ref-type="bibr" rid="bibr36-0022487111421175">Snyder, 2009</xref>). Some question whether performance assessments provide information beyond what university supervisors gain through their formative evaluations and classroom observations of candidates. Our aim in this research is to explore the extent to which supervisors’ perspectives about candidates’ performance correspond with outcomes from summative performance assessments. The study specifically examines the relationship between university supervisors’ predictions and teacher candidates’ performance on a summative assessment based on a capstone teaching event, part of the Performance Assessment for California Teachers (PACT). The study addresses the following questions: (a) To what extent do university supervisors accurately predict candidates’ total scores on a performance-based teaching assessment? (b) On which questions and categories of the assessment do university supervisors most accurately predict their candidates’ scores? and (c) Do university supervisors predict scores more accurately for high- and low-performing candidates?</p>
<sec id="section1-0022487111421175">
<title>Theoretical Framework</title>
<p>The theoretical framework for this study draws from research establishing the complex nature of teaching and, consequently, the challenges of assessing teaching practices. In contrast to process–product research in which effective teaching could be attributed to discrete, observable teaching performances operating independent of time and place (<xref ref-type="bibr" rid="bibr34-0022487111421175">Shulman, 1986</xref>), conceptions of effective teaching now recognize the complex, changing situations and often competing demands that teachers face (<xref ref-type="bibr" rid="bibr10-0022487111421175">Darling-Hammond &amp; Sykes, 1999</xref>; <xref ref-type="bibr" rid="bibr23-0022487111421175">National Board for Professional Teaching Standards [NBPTS], 1999</xref>; <xref ref-type="bibr" rid="bibr33-0022487111421175">Richardson &amp; Placier, 2001</xref>). The core activities of teaching occur in real time, involve social and intellectual interactions, and are shaped by the students in the environment, thus increasing the complexity of the task (<xref ref-type="bibr" rid="bibr20-0022487111421175">Leinhardt, 2001</xref>). The racial, cultural, social, and linguistic backgrounds of students shape the context in ways that require teachers to adopt a more expansive view of pedagogy. For students to experience equal educational opportunities, teachers need to conceptualize learning as a cultural process (<xref ref-type="bibr" rid="bibr19-0022487111421175">Lee, 2007</xref>). Rather than a singular focus on student achievement, culturally relevant teachers take a more holistic approach that considers issues of moral, ethical, and personal development (<xref ref-type="bibr" rid="bibr16-0022487111421175">Ladson-Billings, 1994</xref>, <xref ref-type="bibr" rid="bibr17-0022487111421175">1995</xref>). In these complex contexts, teachers must exercise professional judgment in making decisions, and their decisions are inextricably linked to the specific content and the particular students being taught. The unique, often problematic, situations that arise preclude formulaic solutions (<xref ref-type="bibr" rid="bibr23-0022487111421175">NBPTS, 1999</xref>).</p>
<p>Teachers draw on specialized expertise in making decisions about their work. Expertise, considered to be applied formal knowledge (<xref ref-type="bibr" rid="bibr3-0022487111421175">Brint, 1994</xref>), is a defining characteristic of professions and a foundation for professional judgment. The knowledge base for teachers extends beyond subject matter knowledge to include, for example, knowledge of educational aims, learners, curriculum, general pedagogy, and subject-specific pedagogy (<xref ref-type="bibr" rid="bibr22-0022487111421175">Munby, Russell, &amp; Martin, 2001</xref>; <xref ref-type="bibr" rid="bibr35-0022487111421175">Shulman, 1987</xref>). Teachers apply their professional knowledge to decide what and how to teach to promote student learning. When teaching is viewed as more than the simple transmission of facts and ideas, the need for professional judgment and autonomy becomes clear. Across professions, autonomy and freedom of action are necessary conditions for professionals to adapt their service to particular client needs and circumstances (<xref ref-type="bibr" rid="bibr3-0022487111421175">Brint, 1994</xref>; <xref ref-type="bibr" rid="bibr13-0022487111421175">Friedson, 2001</xref>). In school settings, teachers must adapt their teaching to meet the diverse and changing needs of students in their classrooms. Variability of context, combined with the complexity of teaching, has shifted the view of the teacher to “a thinking, decision-making, reflective, and autonomous professional” (<xref ref-type="bibr" rid="bibr33-0022487111421175">Richardson &amp; Placier, 2001</xref>).</p>
<p>These changes in conceptions of effective teaching prompted dissatisfaction with traditional measures and led to increased attention to teacher assessments that acknowledge progressive, professional practices (<xref ref-type="bibr" rid="bibr31-0022487111421175">Porter et al., 2001</xref>; <xref ref-type="bibr" rid="bibr37-0022487111421175">Tellez, 1996</xref>). In contrast to bureaucratic forms of evaluation that suggest teachers’ work is highly prescribed and rule-governed, a professional view recognizes that teachers analyze and adapt their practices (<xref ref-type="bibr" rid="bibr6-0022487111421175">Darling-Hammond, 1986</xref>, <xref ref-type="bibr" rid="bibr7-0022487111421175">2001</xref>; <xref ref-type="bibr" rid="bibr23-0022487111421175">NBPTS, 1999</xref>; <xref ref-type="bibr" rid="bibr33-0022487111421175">Richardson &amp; Placier, 2001</xref>). The expanded views of effective teaching coincided with increased calls for accountability for teacher preparation programs and the performance of candidates. Professional organizations developed standards based not only on what teachers needed to know but also on what they needed to be able to do. The standards became the basis for designing assessments that would determine whether a candidate achieved the criteria contained in the standards.</p>
<p>Systems of teacher assessment developed by professional organizations such as the Educational Testing Service (ETS), the Interstate New Teacher Assessment and Support Consortium (INTASC), and the NBPTS all feature performance-based assessments stemming from established standards. The aim is to replicate what candidates encounter in a real work situation and determine competence by judging their performance in the actual tasks and activities.</p>
<p>Researchers report advantages and disadvantages of using performance-based teaching assessments to determine the competence of preservice candidates. One advantage is that assessments are tied to professional teaching standards that reflect a high degree of consensus about what constitutes effective teaching (<xref ref-type="bibr" rid="bibr1-0022487111421175">Arends, 2006a</xref>). In addition, depending on the validity, reliability, and fairness of assessment systems, claims about the quality of candidates have the potential to be based on credible data rather than on subjective impressions (<xref ref-type="bibr" rid="bibr2-0022487111421175">Arends, 2006b</xref>). Another key benefit is that performance assessments, in contrast to traditional forms of evaluation, include evidence from teaching practice. Rather than a system that relies on completion of coursework and pencil-and-paper licensure examinations, performance assessments provide more direct evaluation of teaching ability (<xref ref-type="bibr" rid="bibr21-0022487111421175">Mitchell, Robinson, Plake, &amp; Knowles, 2001</xref>; <xref ref-type="bibr" rid="bibr26-0022487111421175">Pecheone &amp; Chung, 2006</xref>; <xref ref-type="bibr" rid="bibr31-0022487111421175">Porter et al., 2001</xref>). Direct methods of assessment are better predictors of success in work settings than are indirect tests (<xref ref-type="bibr" rid="bibr38-0022487111421175">Uhlenbeck, Verloop, &amp; Beijaard, 2002</xref>). In teaching performance assessments, candidates perform tasks that stem directly from what teachers do in their classrooms. The focus shifts from determining a candidate’s possession of knowledge and skills to determining the way in which a candidate uses his or her knowledge, skills, and dispositions in teaching and learning contexts (<xref ref-type="bibr" rid="bibr9-0022487111421175">Darling-Hammond &amp; Snyder, 2000</xref>).</p>
<p>In addition to serving an evaluative function, performance assessments can offer a professional learning opportunity for teacher candidates (<xref ref-type="bibr" rid="bibr4-0022487111421175">Bunch, Aguirre, &amp; Tellez, 2009</xref>; <xref ref-type="bibr" rid="bibr9-0022487111421175">Darling-Hammond &amp; Snyder, 2000</xref>). The ability to learn from one’s own practice is considered an important component of effective teaching (NBPTS, 1999). After completing performance assessment tasks, candidates report gaining greater awareness of their own actions in the classroom as well as their students’ behavior, which allows them to better plan their instructional strategies (<xref ref-type="bibr" rid="bibr25-0022487111421175">Okhremtchouk et al., 2009</xref>). Performance assessments also have the potential to inform teacher preparation programs about areas of strength and weakness in preparing candidates, possibly leading to program improvement (<xref ref-type="bibr" rid="bibr8-0022487111421175">Darling-Hammond, 2006</xref>; <xref ref-type="bibr" rid="bibr26-0022487111421175">Pecheone &amp; Chung, 2006</xref>). In addition to providing information for formative evaluations of teacher education programs, researchers propose that performance assessments also offer a means of evaluating the quality of teacher preparation programs for accreditation and accountability purposes (<xref ref-type="bibr" rid="bibr26-0022487111421175">Pecheone &amp; Chung, 2006</xref>).</p>
<p>The potential benefits of performance assessments are accompanied by a range of related concerns. When performance assessments are used to make summative or high-stakes decisions, the overarching challenge is ensuring validity, reliability, and fairness of the measures. Assessments of teaching performance must be tailored to particular disciplines or levels, and substantial training is needed to achieve and maintain interrater agreement (<xref ref-type="bibr" rid="bibr1-0022487111421175">Arends, 2006a</xref>). When performance assessments are used for multiple functions such as credentialing decisions, accreditation, program improvement, and candidate learning, additional measurement challenges arise. The key issues are balancing competing demands and ensuring that one function does not dominate and lessen the value of the measure for the other functions (<xref ref-type="bibr" rid="bibr36-0022487111421175">Snyder, 2009</xref>).</p>
<p>Given the demands of developing and implementing performance assessments, a key concern is the significant amount of financial and human resources required. In the long run, these costs may become increasingly burdensome for both programs and candidates. Candidates express concern that their university coursework and student teaching practices, as well as their personal lives, suffer due to the extensive time devoted to completing the performance assessment (<xref ref-type="bibr" rid="bibr25-0022487111421175">Okhremtchouk et al., 2009</xref>). The labor-intensive programs may take limited resources away from other important functions in teacher education programs or may lead to superficial implementation (<xref ref-type="bibr" rid="bibr39-0022487111421175">Zeichner, 2003</xref>). When accreditation of programs is at stake, there is a danger of “turning performance-based teacher education into a purely mechanical implementation activity that has lost sight of any moral purpose and of the need . . . to ask the hard questions about what is being accomplished and for whose benefit” (<xref ref-type="bibr" rid="bibr39-0022487111421175">Zeichner, 2003</xref>, p. 502).</p>
<p>A related concern is that the emphasis in teacher education programs is shifting to alignment and compliance, thus limiting the way teaching is represented in the curriculum, inhibiting consideration of other perspectives, and avoiding issues related to values and philosophical choices (<xref ref-type="bibr" rid="bibr11-0022487111421175">Delandshere &amp; Arens, 2001</xref>; <xref ref-type="bibr" rid="bibr15-0022487111421175">Kornfeld, Grady, Marker, &amp; Ruddell, 2007</xref>). A study of the hidden curriculum of one performance-based teacher education program concluded that superficial demonstrations of compliance with external mandates became more important than authentic intellectual engagement (<xref ref-type="bibr" rid="bibr32-0022487111421175">Rennert-Ariev, 2008</xref>). Although not an intended consequence, performance assessments have the potential to lead to curriculum reduction in teacher education programs (<xref ref-type="bibr" rid="bibr1-0022487111421175">Arends, 2006a</xref>). The close link between standards and assessment systems creates a situation in which ideas not addressed in the standards are not included in evaluations of preservice candidates (<xref ref-type="bibr" rid="bibr11-0022487111421175">Delandshere &amp; Arens, 2001</xref>). For example, some researchers note that teaching standards and performance assessments do not adequately incorporate attributes and strategies associated with culturally relevant teaching (<xref ref-type="bibr" rid="bibr18-0022487111421175">Ladson-Billings, 2000</xref>; <xref ref-type="bibr" rid="bibr39-0022487111421175">Zeichner, 2003</xref>). In addition, performance assessments may exclude aspects of teaching that are important but not easily measured (<xref ref-type="bibr" rid="bibr1-0022487111421175">Arends, 2006a</xref>).</p>
</sec>
<sec id="section2-0022487111421175">
<title>PACT</title>
<p>Following legislation in 1998 that required teacher preparation programs to use standardized performance assessments in evaluating credential candidates, the CCTC contracted with the ETS to develop an instrument, known as the California Teacher Performance Assessment (CalTPA). Institutions could either adopt the state-developed model or develop alternate models and submit them for approval (<xref ref-type="bibr" rid="bibr5-0022487111421175">CCTC, 2006</xref>). A key purpose of the performance assessments was to determine whether credential candidates had mastered the state’s teaching performance expectations. A consortium, which was initially composed of 12 universities and has expanded to more than 30 institutions, opted to design an alternative performance assessment. The consortium wanted to develop “an integrated, authentic, and subject-specific assessment” that was “consistent with the core values of member institutions” (<xref ref-type="bibr" rid="bibr26-0022487111421175">Pecheone &amp; Chung, 2006</xref>, p. 22). The consortium’s model, the PACT, was pilot tested over 5 years beginning in 2002. In 2007, the consortium published a technical report summarizing validity and reliability studies of the model (<xref ref-type="bibr" rid="bibr27-0022487111421175">Pecheone &amp; Chung, 2007</xref>), and the PACT was approved by the CCTC.</p>
<p>The PACT assessment is modeled after the portfolio assessments of the Connecticut State Department of Education, the INTASC, and the NBPTS. The assessment includes the use of artifacts from teaching and written commentaries in which the candidates describe their teaching context, analyze their classroom work, and explain the rationale for their actions. The PACT assessments focus on candidates’ use of subject-specific pedagogy to promote student learning.</p>
<p>The PACT program includes two key components: (a) a formative assessment based on embedded signature assessments that are developed by local teacher education programs and (b) a summative assessment based on a capstone teaching event. Embedded signature assessments tend to reflect local program values and be embedded into one or more courses. According to the PACT website, examples of embedded signature assessments include a community study, an observation of classroom management, a child case study, or a curriculum unit. Programs use embedded signature assessments as additional requirements or as a course assignment, but they are not yet an approved form of assessment. In contrast to many classroom assignments, embedded signature assessments have formalized scoring criteria that are used by multiple instructors. The capstone teaching event is standardized across programs and involves subject-specific assessments of a candidate’s competency in five areas or categories: planning, instruction, assessment, reflection, and academic language. Candidates plan and teach an instructional unit, or part of a unit, that is videotaped. Using the video, student work samples, and related artifacts for documentation, the candidates analyze their teaching and their students’ learning. Following analytic prompts, the candidates describe and justify their decisions by explaining their reasoning and providing evidence to support their conclusions. The prompts help candidates consider how student learning is developed through instruction and how analysis of student learning informs teaching decisions during the act of teaching and upon reflection. The capstone teaching event is designed not only to measure but also to promote candidates’ abilities to integrate their knowledge of content, students, and instructional context in making instructional decisions and to stimulate teacher reflection on practice (<xref ref-type="bibr" rid="bibr26-0022487111421175">Pecheone &amp; Chung, 2006</xref>).</p>
<p>The teaching events and the scoring rubrics align with the state’s teaching standards for preservice teachers. The content-specific rubrics are organized according to two or three guiding questions under the five categories identified above. For example, the guiding questions for planning in elementary mathematics include the following: How do the plans support students’ development of conceptual understanding, computational/procedural fluency, and mathematical reasoning skills? How do the plans make the curriculum accessible to the students in the class? and What opportunities do students have to demonstrate their understanding of the standards/objectives? For each guiding question, the rubric includes descriptions of performance for each of four levels. According to the implementation handbook (<xref ref-type="bibr" rid="bibr28-0022487111421175">PACT Consortium, 2009</xref>), Level 1, the lowest level, is defined as not meeting performance standards. These candidates have some skill but need additional student teaching before they would be ready to be in charge of a classroom. Level 2 is considered an acceptable level of performance on the standards. These candidates are judged to have adequate knowledge and skills with the expectation that they will improve with more support and experience. Level 3 is defined as an advanced level of performance on the standards relative to most beginners. Candidates at this level are judged to have a solid foundation of knowledge and skills. Level 4 is considered to be an outstanding and rare level of performance for a beginning teacher and is reserved for stellar candidates. This level offers candidates a sense of what they should be aiming for as they continue to develop as teachers.</p>
<p>To prepare to assess the teaching events, scorers complete a 2-day training in which they learn how to apply the scoring rubrics. These sessions are conducted by lead trainers. Teacher education programs send an individual to be trained by PACT as a lead trainer, or institutions might collaborate to develop a number of lead trainers. The training emphasizes what is used as sources of evidence, how to match evidence to the rubric level descriptors, and the distinctions between the four levels. Scorers are instructed to assign a score based on a preponderance of evidence at a particular level. In addition to the rubric descriptions, the consortium developed a document that assists trainers and scorers in understanding the distinctions between levels. The document provides an expanded description for scoring levels for each guiding question and describes differences between adjacent score levels and the related evidence.</p>
</sec>
<sec id="section3-0022487111421175" sec-type="methods">
<title>Method</title>
<sec id="section4-0022487111421175">
<title>Context</title>
<p>The data for this study were drawn from records for candidates in a public university’s teacher education program over a 2-year period, 2007-2009. In this program, all of the university supervisors also acted as scorers for the performance assessment, although typically not for their own advisees. In keeping with the training outlined by the PACT Consortium, the supervisors at this university participated in 2 days of training each year. A lead trainer, who works in the teacher education program and had been trained by PACT for the role, conducted the sessions. During the training, the supervisors scored two or three benchmark teaching events, which were provided by the PACT Consortium. Each person read a specific section of the event, assigned a score, and compared their scores with the benchmark scores. The group then discussed any variations. Following the training, and before being allowed to score teaching events, the supervisors had to pass a calibration standard set by the PACT Consortium. Each supervisor’s scores on the calibration teaching event (provided each year by PACT) had to meet three criteria: (a) resulted in the same pass/fail decision, (b) included at least six exact matches out of the 11 rubric scores, and (c) did not include any scores that were two away from the predetermined score. After completing training for PACT scoring and passing the calibration standards, university supervisors predicted scores for their own candidates and then received their assigned assessments to score. The training, calibrating, predicting, and scoring took place within a 2-week period.</p>
<p>In this program, the university supervisors did not teach courses or seminars for student teachers, and they were not directly involved in preparing candidates for the performance assessment. The supervisors’ role was to provide support and guidance for student teachers in their assigned classrooms. In keeping with this role, supervisors have substantial teaching backgrounds in a specific subject or at a particular level. Over the academic year, the supervisors made ongoing, periodic classroom visits to observe student teachers in the field. After each classroom observation, the supervisors talked with the student teacher and completed a written evaluation form that documented and followed up on the content of their discussion. By the time supervisors made predictions about their candidates’ PACT scores, they had completed three classroom visits for each student teacher. Although university supervisors evaluate candidates’ classroom teaching, their role focuses on formative assessment. They do not assign grades for the field experience component. The program coordinator (elementary or secondary) assigns the grades for student teaching based on supervisor assessments, mentor teacher evaluations, lesson plans, and other assignments.</p>
</sec>
<sec id="section5-0022487111421175">
<title>Data Source</title>
<p>The pool included a total of 363 teacher candidates (156 multiple-subject/elementary education and 207 single-subject/secondary education). In 2007-2008, there were 152 candidates and 27 supervisors. In 2008-2009, there were 211 candidates and 32 supervisors. The records included scores on the PACT teaching event and predictive scores assigned by the university supervisor. We eliminated 26 records, 14 due to missing predictive scores and 12 because both the teaching event scores and predictive scores were assigned by the same individual. The analysis included data from 337 candidates.</p>
<p>The predictions and the teaching event scores included a ranking from one to four on each of 11 guiding questions that are grouped within the five categories. Therefore, the possible total score ranged from 11 to 44. <xref ref-type="table" rid="table1-0022487111421175">Table 1</xref> summarizes the focus of the guiding questions within each category at the time of data collection. As described above, the rankings are defined as follows—Level 1: not meeting performance standards, Level 2: acceptable level of performance, Level 3: advanced level of performance relative to most beginners, Level 4: outstanding and rare level of performance for a beginning teacher (PACT Consortium, 2009).</p>
<table-wrap id="table1-0022487111421175" position="float">
<label>Table 1.</label>
<caption>
<p>Focus of Guiding Questions in PACT Rubrics</p>
</caption>
<graphic alternate-form-of="table1-0022487111421175" xlink:href="10.1177_0022487111421175-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Category</th>
<th align="center">Focus of guiding questions</th>
</tr>
</thead>
<tbody>
<tr>
<td>Planning</td>
<td>Establishing a balanced instructional focus<break/>Making content accessible<break/>Designing assessments</td>
</tr>
<tr>
<td>Instruction</td>
<td>Engaging students in learning<break/>Monitoring student learning during instruction</td>
</tr>
<tr>
<td>Assessment</td>
<td>Analyzing student work from an assessment<break/>Using assessment to inform teaching</td>
</tr>
<tr>
<td>Reflection</td>
<td>Monitoring student progress<break/>Reflecting on learning</td>
</tr>
<tr>
<td>Academic language</td>
<td>Understanding language demands<break/>Supporting academic language</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0022487111421175">
<p>Note: PACT = Performance Assessment for California Teachers. An additional question on assessment was added in 2009-2010.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section6-0022487111421175">
<title>Data Analysis</title>
<p>We used aggregate data of predictions and scores from the 337 candidates. Paired-samples correlations and measurements of frequency of distribution of difference were utilized through the statistical software program, SPSS. To assess the association between predictions and scores, a paired-samples correlation was estimated for the total score, each of the five categories, and each of the 11 guiding questions. A correlation of 1.0 would indicate that supervisors predicted their candidate’s performance exactly on the PACT assessment. To determine the percentage of supervisors who did not predict their candidate’s performance, we used a frequency of distribution of difference. This analysis was completed by determining the difference between the prediction and the teaching event score for each candidate’s total score, each of the five categories, and each of the 11 questions. A difference of zero would indicate that the supervisor exactly predicted the candidate’s score.</p>
<p>To compare the total score differences using a standard that does not expect an exact match, we used the PACT training calibration standard. In this analysis, we disaggregated the data and examined the predictions and scores for each candidate to determine accuracy according to the three conditions of the calibration standard: (a) the same pass/fail designation, (b) at least 6 (of 11) exact matches, and (c) all nonmatches must be within 1 point. We determined pass/fail designation by the number of Level 1 scores for individual questions, which according to the established PACT passing standard must be no more than two. We then calculated the number of exact matches and matches that differed by two or more. If the prediction met all three conditions of the calibration standard, it was considered accurate for our analyses. <xref ref-type="table" rid="table2-0022487111421175">Table 2</xref> provides examples of cases that differ from each condition yet fall within the zero-to-five accuracy range from our first level of analysis.</p>
<table-wrap id="table2-0022487111421175" position="float">
<label>Table 2.</label>
<caption>
<p>Examples of Cases That Differ from the Standard</p>
</caption>
<graphic alternate-form-of="table2-0022487111421175" xlink:href="10.1177_0022487111421175-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="left" colspan="13">Example: Prediction and score for each question and total</th>
</tr>
<tr>
<th align="left">PACT calibration criteria</th>
<th align="center">Reason</th>
<th/>
<th align="center">Q1</th>
<th align="center">Q2</th>
<th align="center">Q3</th>
<th align="center">Q4</th>
<th align="center">Q5</th>
<th align="center">Q6</th>
<th align="center">Q7</th>
<th align="center">Q8</th>
<th align="center">Q9</th>
<th align="center">Q10</th>
<th align="center">Q11</th>
<th align="center">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Must have same pass/fail designation</td>
<td>Prediction of passing but candidate failed</td>
<td>Prediction</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>22</td>
</tr>
<tr>
<td/>
<td/>
<td>Score</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2</td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2</td>
<td>17</td>
</tr>
<tr>
<td/>
<td>Prediction of failing but candidate passed</td>
<td>Prediction</td>
<td>2</td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2</td>
<td>2</td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2</td>
<td>19</td>
</tr>
<tr>
<td/>
<td/>
<td>Score</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>24</td>
</tr>
<tr>
<td>Must have six (of 11) exact matches</td>
<td>Fewer than six exact matches</td>
<td>Prediction</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>3</td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2</td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>3</td>
<td>24</td>
</tr>
<tr>
<td/>
<td/>
<td>Score</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>1</td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>2</td>
<td>24</td>
</tr>
<tr>
<td>All nonmatches must be within 1 point</td>
<td>Nonmatch greater than 1 point</td>
<td>Prediction</td>
<td>3</td>
<td>4</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>3</td>
<td>2</td>
<td>3</td>
<td>3</td>
<td>30</td>
</tr>
<tr>
<td/>
<td/>
<td>Score</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>3</td>
<td>3<sup><xref ref-type="table-fn" rid="table-fn3-0022487111421175">a</xref></sup></td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>2</td>
<td>30</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0022487111421175">
<p>Note: PACT = Performance Assessment for California Teachers. A fail designation results from more than two scores of Level 1.</p>
</fn>
<fn id="table-fn3-0022487111421175">
<label>a</label>
<p>Denotes the reason the criterion was not met.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>To examine predictions for high-performing and low-performing candidates, we placed candidates who scored 37 points and above (out of a total of 44 points) into a high-performer category (<italic>n</italic> = 22) and candidates who scored 20 points or below into a low-performer category (<italic>n</italic> = 21). We chose scores of 37 and 20 as cutoff points for two reasons. First, both scores, 37 and 20, fell at the end of the second standard deviation (<italic>M</italic> = 27.75, <italic>SD</italic> = 5.521) of the total scores. Second, the score meant that the candidate received a ranking on at least one question that was at the end of the rubric scale. That is, the high performers received one or more rankings of four, and the low performers received one or more rankings of one. We completed the same analyses on these subsamples to determine whether supervisors more accurately predicted their performance.</p>
</sec>
</sec>
<sec id="section7-0022487111421175">
<title>Findings</title>
<p>In the following sections, we present the results for each research question and summarize the ranges, spreads, and accuracies. We examine predictions for total scores, predictions on individual questions and categories, and predictions for high- and low-performing candidates. We then discuss the findings and consider implications for teacher education programs.</p>
<sec id="section8-0022487111421175">
<title>Research Question 1: Total Score Predictions</title>
<p>The correlation between predictions and total scores for candidates over the 2-year period was .289. <xref ref-type="table" rid="table3-0022487111421175">Table 3</xref> shows the distribution of difference for scores in 5-point spreads and the percentage of predictions in those spreads that met the training calibration standard for accuracy.</p>
<table-wrap id="table3-0022487111421175" position="float">
<label>Table 3.</label>
<caption>
<p>Distribution of Difference for Total Score</p>
</caption>
<graphic alternate-form-of="table3-0022487111421175" xlink:href="10.1177_0022487111421175-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Difference between predictions and scores</th>
<th align="center">Frequency</th>
<th align="center">Percentage</th>
<th align="center">Percentage that met calibration standard</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>22</td>
<td>6.5</td>
<td>5.0</td>
</tr>
<tr>
<td>1-5</td>
<td>194</td>
<td>57.0</td>
<td>38.2</td>
</tr>
<tr>
<td>6-10</td>
<td>95</td>
<td>28.1</td>
<td>0</td>
</tr>
<tr>
<td>11-15</td>
<td>20</td>
<td>5.9</td>
<td>0</td>
</tr>
<tr>
<td>16-20</td>
<td>5</td>
<td>1.8</td>
<td>0</td>
</tr>
<tr>
<td>21 and above</td>
<td>1</td>
<td>0.3</td>
<td>0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0022487111421175">
<p>Note: Total analyses completed with 337 candidates.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Of the 337 predictions, 22 (6.5%) matched the total scores, and 194 of the predictions (57%) were within 1 to 5 points of the total score. However, not all of these predictions would be considered accurate according to the PACT calibration standard. Of the 22 cases in which the predicted and total scores matched, all had the same pass/fail designation. But 4 cases did not include at least six exact matches (Condition b) and 1 case had nonmatches greater than 1 point as well as fewer than six exact matches (Conditions c and b). Of the 194 cases in which the prediction and the total scores differed from 1 to 5 points, 65 cases did not meet the calibration standard. Of these 65, 6 did not have the same pass/fail designation, 52 did not include at least six exact matches, and 19 had nonmatches greater than 1 point. Twelve cases failed to meet two of the three conditions. Consequently, of the 216 cases in which predictions and total scores were within 0 to 5 points of each other, 146 (67.6%) met all three conditions of the calibration standard and 70 (32.4%) did not. There were no cases in which the prediction and the score matched exactly for each of the 11 questions. However, there were 8 cases in which the predictions and scores matched for 10 of the 11 questions.</p>
<p>When we use the calibration standard as a measure of accuracy, 43.2% of the total 337 predictions are within the accurate range. Predictions that did not meet the calibration standard for accuracy (<italic>n</italic> = 191) were not overwhelming in one direction but instead split between over- and underpredictions. Of the 191 cases, 57.6% were underpredictions and 39.8% were overpredictions. In 2.5% of these cases, the prediction and total score matched, but, as described above, other conditions were not met.</p>
<p>Results for tested subgroups (by year and program type) followed a similar trend of low correlations between predictions and scores. For the 2007-2008 and the 2008-2009 subgroups, the total score correlations were .244 and .320, respectively. The total score correlations by program type were .252 for the elementary education group and .347 for the secondary education group. All of the correlations were significant at the .01 level.</p>
</sec>
<sec id="section9-0022487111421175">
<title>Research Question 2: Individual Question and Category Predictions</title>
<p><xref ref-type="table" rid="table4-0022487111421175">Table 4</xref> shows the correlations, percentage of accurate predictions, and the range of inaccurate predictions for each of the 11 guiding questions. Correlations between predictions and scores on the questions ranged from .110 to .242. The accurate predictions ranged from 41.5% on Question 6 (analyzing student work from an assessment) to 54.3% on Question 10 (understanding language demands). Questions 1, 2, 3, and 6 had one prediction spanning the full range of the rubric, meaning a difference of 3 points between the prediction and the score. In all cases, these 3-point differences were overpredictions, suggesting the university supervisor predicted exemplary performance, but the scorer evaluated the assessment on that question as not meeting standards.</p>
<table-wrap id="table4-0022487111421175" position="float">
<label>Table 4.</label>
<caption>
<p>Correlations, Percentage Accurate Predictions, and Range for Predictions and Scores</p>
</caption>
<graphic alternate-form-of="table4-0022487111421175" xlink:href="10.1177_0022487111421175-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Question</th>
<th align="center">Correlation between predictions and scores</th>
<th align="center">Percentage of accurate predictions</th>
<th align="center">Range of difference: Predictions and scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q1 Planning: Establishing a balanced instructional focus</td>
<td>.149<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>51.0</td>
<td>3</td>
</tr>
<tr>
<td>Q2 Planning: Making content accessible</td>
<td>.201<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>46.9</td>
<td>3</td>
</tr>
<tr>
<td>Q3 Planning: Designing assessments</td>
<td>.136<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>45.1</td>
<td>3</td>
</tr>
<tr>
<td>Q4 Instruction: Engaging students in learning</td>
<td>.242<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>50.1</td>
<td>2</td>
</tr>
<tr>
<td>Q5 Instruction: Monitoring student learning during instruction</td>
<td>.225<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>49.0</td>
<td>2</td>
</tr>
<tr>
<td>Q6 Assessment: Analyzing student work from an assessment</td>
<td>.090<xref ref-type="table-fn" rid="table-fn5-0022487111421175">*</xref></td>
<td>41.5</td>
<td>3</td>
</tr>
<tr>
<td>Q7 Assessment: Using assessment to inform teaching</td>
<td>.110<xref ref-type="table-fn" rid="table-fn5-0022487111421175">**</xref></td>
<td>52.5</td>
<td>2</td>
</tr>
<tr>
<td>Q8 Reflection: Monitoring student progress</td>
<td>.143<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>48.4</td>
<td>2</td>
</tr>
<tr>
<td>Q9 Reflection: Reflecting on learning</td>
<td>.228<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>47.8</td>
<td>2</td>
</tr>
<tr>
<td>Q10 Academic language: Understanding language demands</td>
<td>.205<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>54.3</td>
<td>2</td>
</tr>
<tr>
<td>Q11 Academic language: Supporting academic language development</td>
<td>.240<xref ref-type="table-fn" rid="table-fn5-0022487111421175">***</xref></td>
<td>54.0</td>
<td>2</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0022487111421175">
<label>*</label>
<p><italic>p</italic> &lt; .10. **<italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p><xref ref-type="table" rid="table5-0022487111421175">Table 5</xref> shows the correlations, percentage of accurate predictions, and the range of inaccurate predictions for each of the five categories. As summarized earlier in <xref ref-type="table" rid="table1-0022487111421175">Table 1</xref>, the 11 guiding questions are grouped into five categories: planning, instruction, assessment, reflection, and academic language. Correlations between predictions and scores on the categories ranged from .113 to .269. The predictions for the categories had lower accuracy than predictions for individual questions. The accurate predictions ranged from 17.5% (planning) to 38% (academic language).</p>
<table-wrap id="table5-0022487111421175" position="float">
<label>Table 5.</label>
<caption>
<p>Correlations and Percentage Accurate Predictions</p>
</caption>
<graphic alternate-form-of="table5-0022487111421175" xlink:href="10.1177_0022487111421175-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Category</th>
<th align="center">Correlation between predictions and scores</th>
<th align="center">Percentage of accurate predictions</th>
</tr>
</thead>
<tbody>
<tr>
<td>Category P: Planning</td>
<td>.219<xref ref-type="table-fn" rid="table-fn6-0022487111421175">***</xref></td>
<td>17.5</td>
</tr>
<tr>
<td>Category I: Instruction</td>
<td>.269<xref ref-type="table-fn" rid="table-fn6-0022487111421175">***</xref></td>
<td>32.6</td>
</tr>
<tr>
<td>Category A: Assessment</td>
<td>.113<xref ref-type="table-fn" rid="table-fn6-0022487111421175">**</xref></td>
<td>30.3</td>
</tr>
<tr>
<td>Category R: Reflection</td>
<td>.250<xref ref-type="table-fn" rid="table-fn6-0022487111421175">***</xref></td>
<td>28.5</td>
</tr>
<tr>
<td>Category D: Academic language</td>
<td>.268<xref ref-type="table-fn" rid="table-fn6-0022487111421175">***</xref></td>
<td>38.0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0022487111421175">
<label>*</label>
<p><italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section10-0022487111421175">
<title>Research Question 3: Predictions for High-Performing and Low-Performing Candidates</title>
<p>As with the total group of candidates, the analysis of data for the high- and low-performing groups focused on the accuracy of predictions for total scores, individual questions, and categories. For the high-performing candidates (total PACT score 37 or above) and the low-performing candidates (total PACT score 20 or below), the university supervisors were no more likely to accurately predict their scores than the other candidates’ scores. The percentage of accurate predictions and the correlations between predictions and scores were low.</p>
<sec id="section11-0022487111421175">
<title>High-performing candidates</title>
<p>For the 22 high performers, correlations between predictions and scores ranged from .056 to .276 for the categories and from –.364 to .404 for the individual questions (see <xref ref-type="table" rid="table6-0022487111421175">Table 6</xref>). None of the correlations were statistically significant at the .05 level. The frequency of accurate predictions for the individual questions ranged from one accurate prediction (4.5%) for Question 6 (analyzing student work from an assessment) to nine accurate predictions (40.9%) for Question 4 (engaging students in learning). The frequency of accurate predictions for the categories, consisting of two or three questions, ranged from one accurate prediction (4.5%) for Assessment to six accurate predictions (27.3%) for Reflection.</p>
<table-wrap id="table6-0022487111421175" position="float">
<label>Table 6.</label>
<caption>
<p>Correlations, Percentage Accurate Predictions, and Range for Predictions and Scores for High-Performing Candidates (<italic>n</italic> = 22)</p>
</caption>
<graphic alternate-form-of="table6-0022487111421175" xlink:href="10.1177_0022487111421175-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Question or category</th>
<th align="center">Correlation between predictions and scores</th>
<th align="center">Frequency/percentage of accurate predictions</th>
<th align="center">Range of difference: Predictions and scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q1 Planning: Establishing a balanced instructional focus</td>
<td>.279</td>
<td>8/36.4</td>
<td>2</td>
</tr>
<tr>
<td>Q2 Planning: Making content accessible</td>
<td>−.364<xref ref-type="table-fn" rid="table-fn7-0022487111421175">*</xref></td>
<td>5/22.7</td>
<td>2</td>
</tr>
<tr>
<td>Q3 Planning: Designing assessments</td>
<td>.368<xref ref-type="table-fn" rid="table-fn7-0022487111421175">*</xref></td>
<td>7/31.8</td>
<td>2</td>
</tr>
<tr>
<td>Q4 Instruction: Engaging students in learning</td>
<td>.212</td>
<td>9/40.9</td>
<td>2</td>
</tr>
<tr>
<td>Q5 Instruction: Monitoring student learning during instruction</td>
<td>−.089</td>
<td>6/27.3</td>
<td>2</td>
</tr>
<tr>
<td>Q6 Assessment: Analyzing student work from an assessment</td>
<td>.404<xref ref-type="table-fn" rid="table-fn7-0022487111421175">*</xref></td>
<td>1/4.5</td>
<td>2</td>
</tr>
<tr>
<td>Q7 Assessment: Using assessment to inform teaching</td>
<td>−.162</td>
<td>6/27.3</td>
<td>2</td>
</tr>
<tr>
<td>Q8 Reflection: Monitoring student progress</td>
<td>.058</td>
<td>7/31.8</td>
<td>2</td>
</tr>
<tr>
<td>Q9 Reflection: Reflecting on learning</td>
<td>.176</td>
<td>11/50.0</td>
<td>2</td>
</tr>
<tr>
<td>Q10 Academic language: Understanding language demands</td>
<td>.240</td>
<td>7/31.8</td>
<td>2</td>
</tr>
<tr>
<td>Q11 Academic language: Supporting academic language development</td>
<td>.209</td>
<td>8/36.4</td>
<td>2</td>
</tr>
<tr>
<td>Category P: Planning</td>
<td>.188</td>
<td>3/13.6</td>
<td/>
</tr>
<tr>
<td>Category I: Instruction</td>
<td>.133</td>
<td>2/9.1</td>
<td/>
</tr>
<tr>
<td>Category A: Assessment</td>
<td>.056</td>
<td>1/4.5</td>
<td/>
</tr>
<tr>
<td>Category R: Reflection</td>
<td>.085</td>
<td>6/27.3</td>
<td/>
</tr>
<tr>
<td>Category D: Academic language</td>
<td>.276</td>
<td>5/22.7</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-0022487111421175">
<label>*</label>
<p><italic>p</italic> &lt; .10.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Predictions for only 5 of the 22 high-performing candidates would be considered accurate according to the training calibration standard. In 1 of those 5 cases, the prediction and the total score matched with nine of the predictions and scores out of the 11 individual questions matching. For all 22 high performers, the supervisors accurately predicted that the candidate would pass the assessment. However, in the 17 cases that did not meet the calibration standard, 13 had fewer than six exact matches on the individual questions and also had nonmatches greater than 1 point. Four predictions did not meet the calibration standard due to a mismatch on one of the three conditions. In all 17 cases, the university supervisors underpredicted total scores for the high performers. In 2 cases, the supervisors underpredicted the total score by almost half of the possible score of 44. More specifically, one underpredicted the total score by 19 points and the other by 21 points. A total of 22 candidates received scores that identified them as high performers, according to our cutoff point of 37 or above, yet, in only 2 of those cases did the supervisors similarly predict this level of high performance.</p>
</sec>
<sec id="section12-0022487111421175">
<title>Low-performing candidates</title>
<p>For the 21 low performers, correlations between predictions and scores ranged from –.311 to .718 for the 11 questions; only one correlation was statistically significant (Question 10: understanding language demands). <xref ref-type="table" rid="table7-0022487111421175">Table 7</xref> shows these results. Correlations for the five categories ranged from –.329 to .504 with one statistically significant correlation (Reflection). The frequency of accurate predictions for individual questions ranged from 5 accurate predictions (23.8%) for Question 10 (understanding language demands) to 10 accurate predictions (47.6%) for Question 5 (monitoring student learning during instruction). The frequency of accurate predictions for the categories ranged from 2 accurate predictions (9.5%) for Assessment to 6 accurate predictions (28.6%) for Instruction.</p>
<table-wrap id="table7-0022487111421175" position="float">
<label>Table 7.</label>
<caption>
<p>Correlations, Percentage Accurate Predictions, and Range for Predictions and Scores for Low-Performing Candidates (<italic>n</italic> = 21)</p>
</caption>
<graphic alternate-form-of="table7-0022487111421175" xlink:href="10.1177_0022487111421175-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Question or category</th>
<th align="center">Correlation between predictions and scores</th>
<th align="center">Frequency/percentage of accurate predictions</th>
<th align="center">Range of difference: Predictions and scores</th>
</tr>
</thead>
<tbody>
<tr>
<td>Q1 Planning: Establishing a balanced instructional focus</td>
<td>−.238</td>
<td>9/42.9</td>
<td>3</td>
</tr>
<tr>
<td>Q2 Planning: Making content accessible</td>
<td>−.104</td>
<td>7/33.3</td>
<td>3</td>
</tr>
<tr>
<td>Q3 Planning: Designing assessments</td>
<td>−.274</td>
<td>9/42.9</td>
<td>3</td>
</tr>
<tr>
<td>Q4 Instruction: Engaging students in learning</td>
<td>.229</td>
<td>9/42.9</td>
<td>2</td>
</tr>
<tr>
<td>Q5 Instruction: Monitoring student learning during instruction</td>
<td>.085</td>
<td>10/47.6</td>
<td>2</td>
</tr>
<tr>
<td>Q6 Assessment: Analyzing student work from an assessment</td>
<td>−.311</td>
<td>6/28.6</td>
<td>2</td>
</tr>
<tr>
<td>Q7 Assessment: Using assessment to inform teaching</td>
<td>.171</td>
<td>6/28.6</td>
<td>2</td>
</tr>
<tr>
<td>Q8 Reflection: Monitoring student progress</td>
<td>.067</td>
<td>7/33.3</td>
<td>2</td>
</tr>
<tr>
<td>Q9 Reflection: Reflecting on learning</td>
<td>.494</td>
<td>11/52.4</td>
<td>2</td>
</tr>
<tr>
<td>Q10 Academic language: Understanding language demands</td>
<td>.718<xref ref-type="table-fn" rid="table-fn8-0022487111421175">***</xref></td>
<td>5/23.8</td>
<td>1</td>
</tr>
<tr>
<td>Q11 Academic language: Supporting academic language development</td>
<td>.077</td>
<td>11/52.4</td>
<td>2</td>
</tr>
<tr>
<td>Category P: Planning</td>
<td>−.329</td>
<td>4/19</td>
<td/>
</tr>
<tr>
<td>Category I: Instruction</td>
<td>.314</td>
<td>6/28.6</td>
<td/>
</tr>
<tr>
<td>Category A: Assessment</td>
<td>−.113</td>
<td>2/9.5</td>
<td/>
</tr>
<tr>
<td>Category R: Reflection</td>
<td>.504<xref ref-type="table-fn" rid="table-fn8-0022487111421175">**</xref></td>
<td>4/19</td>
<td/>
</tr>
<tr>
<td>Category D: Academic language</td>
<td>.476</td>
<td>4/19</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn8-0022487111421175">
<label>**</label>
<p><italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>For most of the low-performing candidates (17/21 or 81%), the supervisors did not predict low performance according to our cutoff point of 20. Using the calibration standard, only 4 of the 21 cases would be considered accurate. In 13 of the 21 cases, the pass/fail designations differed. Of the 17 inaccurate predictions, 7 did not meet all three conditions of the standard, 5 did not meet two conditions, and 5 did not meet one condition. In 16 of these 17 cases, the university supervisors overpredicted their candidates’ total score, and in 1 case, the supervisor underpredicted the total score. In the most extreme case, the supervisor overpredicted the low-performing candidate’s total score by 19 out of a possible 44 points.</p>
</sec>
</sec>
</sec>
<sec id="section13-0022487111421175" sec-type="discussion">
<title>Discussion</title>
<p>Our findings indicate that in the majority of cases (63.5%), supervisors’ predictions were within 5 points of the candidate’s total score on the PACT teaching event. However, in some cases, predictions and scores were within 5 points but differed on other dimensions such as pass/fail designation. When we use the calibration standard as a measure of accuracy, 43.2% of the total 337 predictions are within the accurate range. The predictions that did not meet the calibration standard for accuracy were split between over- and underpredictions. For the total candidate group, supervisors did not have more accurate predictions on any individual questions or categories. No one question or category stood out as more or less accurate in prediction-score matching, and supervisors did not consistently over- or underpredict for a particular question or category. Approximately half of the supervisors accurately predicted performance on each individual question.</p>
<p>The most surprising differences occurred in prediction-score matching for high and low performers. Given the commonly held view that it is easiest to identify students on either end of the continuum, we anticipated more agreement for high and low performers. We thought that the supervisors, who observe and evaluate candidates in the classroom, would be in a prime position to predict which preservice teachers would perform particularly well or poorly on a teaching performance assessment. But supervisors did not provide closer predictions for those candidates. In the high-performing group, most supervisors underpredicted their candidates’ performance, and, in 2 cases, the difference was nearly half of the possible total score. The supervisors correctly predicted that candidates would pass the assessment, but, in only 2 of the 22 cases did supervisors predict a total score that identified candidates as high performers in relationship to our cutoff point of 37. In the low-performing group, the majority of supervisors overpredicted their candidates’ total scores, with one supervisor overpredicting by 19 points. In only 4 of the 21 cases did supervisors predict a total score that identified candidates as low performers in relationship to our cutoff point of 20. For the majority of the high and low performers, a group of 43 out of 337 candidates, their supervisors did not identify them as the exceptional candidates who would either excel or fail.</p>
<p>In addition to differences in total scores, the range of differences between predictions and scores on individual questions, particularly for high and low performers, was surprising. Differences of 1 point on a question are not striking. But differences of 2 and 3 points reflect highly contrasting perspectives about a candidate’s skills and performance in a particular area. A 2-point range on a question is the difference between “not passing” and an “advanced level of performance” or between “adequate,” which is the lowest passing ranking, and an “outstanding performance.” A 3-point overprediction means that the supervisor predicted a score reserved for stellar candidates, yet the candidate received a failing score. It is difficult to comprehend that a supervisor would view a candidate as outstanding in an area in which the candidate fails on the performance assessment. Because the supervisors train and serve as scorers, they are familiar with the format, requirements, and standards of the performance assessment. The differences would not arise from a lack of knowledge about the performance assessment itself. The differences also do not appear to be because of tendencies of a particular supervisor or scorer. Among the high performers with the greatest differences between predictions and total scores, none had the same university supervisor or the same scorer for their teaching event. Similarly, none of the low performers with the greatest differences had the same supervisor or scorer.</p>
<p>We propose that the discrepancies between the predictions and scores in our study stem from three differences in the tasks of the supervisors and scorers. First, supervisors and scorers draw on different data sources. Whereas supervisors make predictions based on formative evaluations and classroom observations of candidates, scorers make judgments based on teaching artifacts and written commentaries. Supervisors in this program are not directly involved in preparing candidates for the performance assessment and do not review drafts of written commentaries. Their predictions stem from their observations of classroom teaching and discussions with candidates about their plans and instructional practice but not from candidates’ written analyses of their teaching. Candidates who may be effective classroom teachers may not be as skilled in writing about their instructional practice. Moreover, some candidates may aim to achieve a high score on the performance assessment whereas others may make student teaching the priority. Second, supervisors observe and gauge progress over time, whereas scorers make a single judgment at 1 point in time. Supervisors focus on formative evaluations and feedback to help candidates improve whereas scorers make a summative assessment. Multiple scorers may consistently agree on the scores for a performance assessment, but supervisors may have differing perspectives from observing the ups and downs in a candidate’s overall progression. Third, supervisors assess candidates’ teaching in active classrooms with changing situations whereas scorers view a bounded, preselected segment of a class. What supervisors view in observations may not correspond with what scorers view in the performance assessment. For example, supervisors may observe ongoing classroom management problems that interfere with student learning whereas scorers see only minor issues in a video clip. For the performance assessment, candidates may choose among multiple teaching segments to submit, but candidates are unable to select what the supervisor observes during classroom visits. Consequently, supervisors tend to have more opportunity to observe how candidates respond to immediate situations that arise and how they adapt instruction to the particular context.</p>
</sec>
<sec id="section14-0022487111421175" sec-type="conclusions">
<title>Conclusion and Implications</title>
<p>In this study, university supervisors’ perspectives about their candidates did not always correspond with outcomes on the PACT teaching event, a summative performance assessment. Most of the candidates with the highest and lowest scores on the assessment were not those for whom the supervisors anticipated outstanding or poor performance. As described above, we posit that the primary reason that predictions did not match performance is not lack of knowledge about the assessment or the state’s teaching performance expectations but rather differences in the roles of supervisors and scorers.</p>
<p>This study highlights issues that hold implications for practice, policy, and research on assessing preservice teachers’ qualifications. The increasing emphasis on performance assessments is changing the role of the university supervisor in evaluating candidates’ qualifications. Concerns about the validity and reliability of student teaching observations suggest that reliance on supervisor’s evaluations of candidates for making summative judgments is problematic. Observations may be conducted too infrequently, training of supervisors may be insufficient to achieve interrater agreement, and observation forms may not be tailored to specific disciplines or levels (<xref ref-type="bibr" rid="bibr2-0022487111421175">Arends, 2006b</xref>). Researchers report that summative judgments made from student teaching observation forms are unable to differentiate among various levels of effectiveness and that 95% of candidates receive a grade of “A” in student teaching (<xref ref-type="bibr" rid="bibr2-0022487111421175">Arends, 2006b</xref>). However, in developing and implementing more discriminating forms of assessment, we may be eliminating, rather than lessening, the use of supervisors’ observations in evaluations. Performance assessments and supervisor perspectives may provide different, yet equally valuable, information for overall assessments of candidates. In making classroom visits, supervisors gain a firsthand and in-depth view of the specific school context in which preservice candidates are teaching, and they observe student teachers’ improvement and progress over time in this context. Supervisors, for example, may have more opportunities to observe how candidates interact with students from varying cultural and linguistic backgrounds and be in a stronger position to determine how candidates identify the language demands of learning tasks relative to students’ current levels of academic language proficiency. Supervisors also may be better positioned to observe qualities related to the caring aspect of teacher–student relationships (<xref ref-type="bibr" rid="bibr24-0022487111421175">Noddings, 1991</xref>) and the psychological and moral dimensions of teaching (<xref ref-type="bibr" rid="bibr12-0022487111421175">Fenstermacher &amp; Richardson, 2005</xref>). <xref ref-type="bibr" rid="bibr18-0022487111421175">Ladson-Billings (2000)</xref> questions how a sense of caring and cultural solidarity can be exhibited in an assessment and what pieces of evidence would demonstrate the connection between a teacher and his or her students. During follow-up discussions with supervisors after observations, candidates can discuss specific challenges, receive prompt feedback from supervisors, and engage in immediate reflection on their teaching. During later observations, supervisors can gauge if candidates make adaptations based on their reflections. The PACT assessment also incorporates contextual factors but with different forms of documentation. In the PACT teaching event, candidates describe their context and explain their selected instructional segment in relationship to the context. The written commentary offers evidence about how candidates analyze their own teaching after more in-depth reflection and how they use their analyses to plan for future instruction. But, due to the nature of the assessment, there is no mechanism for determining whether candidates actually implement the adaptations they propose. Given the financial and human resources required for both performance assessments and university supervisors, teacher education programs may be faced with difficult choices. Before we adopt practices or policies that either intentionally, or unintentionally, eliminate supervisors’ perspectives about candidates’ effectiveness, we need to carefully consider and closely examine the trade-offs.</p>
<p>Our findings also underscore the value of using multiple methods in assessing the qualifications and competence of preservice candidates. Performance assessments offer numerous benefits in comparison with traditional evaluations, but the credibility of performance assessments for licensing decisions is an ongoing concern (<xref ref-type="bibr" rid="bibr2-0022487111421175">Arends, 2006b</xref>). In addition, unintended consequences are emerging as performance assessments are being mandated and implemented (<xref ref-type="bibr" rid="bibr11-0022487111421175">Delandshere &amp; Arens, 2001</xref>; <xref ref-type="bibr" rid="bibr15-0022487111421175">Kornfeld et al., 2007</xref>; <xref ref-type="bibr" rid="bibr32-0022487111421175">Rennert-Ariev, 2008</xref>). Multiple sources of information about a candidate stand to contribute to a more thorough assessment of effectiveness. The limitations of particular assessment strategies can be overcome by using broad-based assessment systems that include multiple sources of evidence from multiple evaluators. For a comprehensive assessment of candidates’ progress, strategies that “appreciate the complexity of teaching and learning and that provide a variety of lenses on the process of learning to teach” are needed (<xref ref-type="bibr" rid="bibr8-0022487111421175">Darling-Hammond, 2006</xref>, p. 120). Candidates may appear more, or less, effective during classroom observations than in their videotaped segment and accompanying commentary in a performance assessment. In addition, candidates with strong writing skills may have an advantage in analyzing and describing their teaching practice. In the face of limited time and varied personal responsibilities, some candidates may be inclined to devote more time and attention to their classroom teaching or alternately to the summative assessment.</p>
<p>Research on evaluation of practicing teachers similarly highlights the complexity of teaching and the need for analyzing and documenting effective teaching through a range of strategies. <xref ref-type="bibr" rid="bibr29-0022487111421175">Peterson (1987</xref>, <xref ref-type="bibr" rid="bibr30-0022487111421175">2000)</xref> reports that multiple measures tap different aspects of teacher quality. In addition, multiple evaluators contribute a range of perspectives. “For some questions and situations, the perspectives of people in different roles are needed to recognize satisfactory or outstanding work” (<xref ref-type="bibr" rid="bibr30-0022487111421175">Peterson, 2000</xref>, p. 5). If performance assessments become a single, high-stakes measure of preservice teacher qualifications and teacher education outcomes, multiple perspectives will be lost. In that case, supervisors’ insights about a candidate’s effectiveness would have little bearing on decisions about progress or possible remediation.</p>
<p>This study holds implications for future research about assessment of preservice candidates’ qualifications. First, the lack of agreement about high and low performers in this study contradicts common wisdom about identifying stellar and struggling candidates and is an area that warrants further study. Why did these differences occur? What is the correlation between outstanding or failing scores on a performance assessment and other measures of candidates’ abilities? Second, to make overall judgments about preservice teachers’ abilities, we need to know more about the relationship between specific measures and particular aspects of teaching effectiveness in the student teaching phase. Some strategies may offer valuable evidence on one component but not another. In addition, strategies that may be informative in assessing practicing teachers may not be as relevant for evaluating preservice teachers. Continuing research on the contributions and limits of performance assessments and other strategies will be important in determining how to evaluate and foster candidates’ professional growth during teacher preparation programs.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Arends</surname><given-names>R. I.</given-names></name>
</person-group> (<year>2006a</year>). <article-title>Performance assessment in perspective: History, opportunities, and challenges</article-title>. In <person-group person-group-type="editor">
<name><surname>Castle</surname><given-names>S.</given-names></name>
<name><surname>Shaklee</surname><given-names>B. S.</given-names></name>
</person-group> (Eds.), <source>Assessing teacher performance: Performance-based assessment in teacher education</source> (pp. <fpage>3</fpage>-<lpage>22</lpage>). <publisher-loc>Lanham, MD</publisher-loc>: <publisher-name>Rowman &amp; Littlefield Education</publisher-name>.</citation>
</ref>
<ref id="bibr2-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Arends</surname><given-names>R. I.</given-names></name>
</person-group> (<year>2006b</year>). <article-title>Summative performance assessments</article-title>. In <person-group person-group-type="editor">
<name><surname>Castle</surname><given-names>S.</given-names></name>
<name><surname>Shaklee</surname><given-names>B. S.</given-names></name>
</person-group> (Eds.), <source>Assessing teacher performance: Performance-based assessment in teacher education</source> (pp. <fpage>93</fpage>-<lpage>123</lpage>). <publisher-loc>Lanham, MD</publisher-loc>: <publisher-name>Rowman &amp; Littlefield Education</publisher-name>.</citation>
</ref>
<ref id="bibr3-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brint</surname><given-names>S.</given-names></name>
</person-group> (<year>1994</year>). <source>In an age of experts: The changing role of professionals in politics and public life</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bunch</surname><given-names>G. C.</given-names></name>
<name><surname>Aguirre</surname><given-names>J. M.</given-names></name>
<name><surname>Tellez</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Beyond the scores: Using candidate responses on high stakes performance assessment to inform teacher preparation for English learners</article-title>. <source>Issues in Teacher Education</source>, <volume>18</volume>(<issue>1</issue>), <fpage>103</fpage>-<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr5-0022487111421175">
<citation citation-type="gov">
<collab>California Commission on Teacher Credentialing</collab>. (<year>2006</year>). <source>Summary of commission responsibilities for major provisions of SB 1209</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.ctc.ca.gov/educator-prep/SB1209/default.html">http://www.ctc.ca.gov/educator-prep/SB1209/default.html</ext-link></comment></citation>
</ref>
<ref id="bibr6-0022487111421175">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
</person-group> (<year>1986</year>). <article-title>A proposal for evaluation in the teaching profession</article-title>. <source>Elementary School Journal</source>, <volume>86</volume>(<issue>4</issue>), <fpage>531</fpage>-<lpage>551</lpage>.</citation>
</ref>
<ref id="bibr7-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Standard setting in teaching: Changes in licensing, certification, and assessment</article-title>. In <person-group person-group-type="editor">
<name><surname>Richardson</surname><given-names>V.</given-names></name>
</person-group> (Ed.), <source>Fourth handbook of research on teaching</source> (pp. <fpage>751</fpage>-<lpage>776</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr8-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Assessing teacher education: The usefulness of multiple measures for assessing program outcomes</article-title>. <source>Journal of Teacher Education</source>, <volume>57</volume>(<issue>2</issue>), <fpage>120</fpage>-<lpage>138</lpage>.</citation>
</ref>
<ref id="bibr9-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
<name><surname>Snyder</surname><given-names>J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Authentic assessment of teaching in context</article-title>. <source>Teaching and Teacher Education</source>, <volume>16</volume>(<issue>5-6</issue>), <fpage>523</fpage>-<lpage>545</lpage>.</citation>
</ref>
<ref id="bibr10-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Darling-Hammond</surname><given-names>L.</given-names></name>
<name><surname>Sykes</surname><given-names>G.</given-names></name>
</person-group> (<year>1999</year>). <source>Teaching as the learning profession: Handbook of policy and practice</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr11-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Delandshere</surname><given-names>G.</given-names></name>
<name><surname>Arens</surname><given-names>S. A.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Representations of teaching and standards-based reform: Are we closing the debate about teacher education?</article-title> <source>Teaching and Teacher Education</source>, <volume>17</volume>, <fpage>547</fpage>-<lpage>566</lpage>.</citation>
</ref>
<ref id="bibr12-0022487111421175">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fenstermacher</surname><given-names>G. D.</given-names></name>
<name><surname>Richardson</surname><given-names>V.</given-names></name>
</person-group> (<year>2005</year>). <article-title>On making determinations of quality in teaching</article-title>. <source>Teachers College Record</source>, <volume>107</volume>(<issue>1</issue>), <fpage>186</fpage>-<lpage>215</lpage>.</citation>
</ref>
<ref id="bibr13-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Friedson</surname><given-names>E.</given-names></name>
</person-group> (<year>2001</year>). <source>Professionalism, the third logic: On the practice of knowledge</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Guaglianone</surname><given-names>C. L.</given-names></name>
<name><surname>Payne</surname><given-names>M.</given-names></name>
<name><surname>Kinsey</surname><given-names>G. W.</given-names></name>
<name><surname>Chiero</surname><given-names>R.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Teaching performance assessment: A comparative study of implementation and impact among California State University campuses</article-title>. <source>Issues in Teacher Education</source>, <volume>18</volume>(<issue>1</issue>), <fpage>129</fpage>-<lpage>148</lpage>.</citation>
</ref>
<ref id="bibr15-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kornfeld</surname><given-names>J.</given-names></name>
<name><surname>Grady</surname><given-names>K.</given-names></name>
<name><surname>Marker</surname><given-names>P. M.</given-names></name>
<name><surname>Ruddell</surname><given-names>M. R.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Caught in the current: A self-study of state-mandated compliance in a teacher education program</article-title>. <source>Teachers College Record</source>, <volume>109</volume>(<issue>2</issue>), <fpage>1902</fpage>-<lpage>1930</lpage>.</citation>
</ref>
<ref id="bibr16-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ladson-Billings</surname><given-names>G.</given-names></name>
</person-group> (<year>1994</year>). <source>The dreamkeepers: Successful teachers of African American students</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr17-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ladson-Billings</surname><given-names>G.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Toward a theory of culturally relevant pedagogy</article-title>. <source>American Educational Research Journal</source>, <volume>32</volume>(<issue>3</issue>), <fpage>465</fpage>-<lpage>491</lpage>.</citation>
</ref>
<ref id="bibr18-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ladson-Billings</surname><given-names>G.</given-names></name>
</person-group> (<year>2000</year>). <source>The validity of National Board for Professional Teaching Standards (NBPTS)/Interstate New Teacher Assessment and Support Consortium (INTASC) assessments for effective urban teachers</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education</publisher-name>.</citation>
</ref>
<ref id="bibr19-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>C.</given-names></name>
</person-group> (<year>2007</year>). <source>Culture, literacy, and learning: Taking bloom in the midst of the whirlwind</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Teachers College Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Leinhardt</surname><given-names>G.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Instructional explanations: A commonplace for teaching and location for contrast</article-title>. In <person-group person-group-type="editor">
<name><surname>Richardson</surname><given-names>V.</given-names></name>
</person-group> (Ed.), <source>Fourth handbook of research on teaching</source> (pp. <fpage>333</fpage>-<lpage>357</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr21-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mitchell</surname><given-names>K. J.</given-names></name>
<name><surname>Robinson</surname><given-names>D. Z.</given-names></name>
<name><surname>Plake</surname><given-names>B. S.</given-names></name>
<name><surname>Knowles</surname><given-names>K. T.</given-names></name>
</person-group> (<year>2001</year>). <source>Testing teacher candidates: The role of licensure tests in improving teacher quality</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academy Press</publisher-name>.</citation>
</ref>
<ref id="bibr22-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Munby</surname><given-names>H.</given-names></name>
<name><surname>Russell</surname><given-names>T.</given-names></name>
<name><surname>Martin</surname><given-names>A. K.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Teachers knowledge and how it develops</article-title>. In <person-group person-group-type="editor">
<name><surname>Richardson</surname><given-names>V.</given-names></name>
</person-group> (Ed.), <source>Fourth handbook of research on teaching</source> (pp. <fpage>877</fpage>-<lpage>904</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr23-0022487111421175">
<citation citation-type="book">
<collab>National Board for Professional Teaching Standards</collab>. (<year>1999</year>). <source>What teachers should know and be able to do</source>. <publisher-loc>Arlington, VA</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr24-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Noddings</surname><given-names>N.</given-names></name>
</person-group> (<year>1991</year>). <source>The challenge to care in schools: An alternative approach to education</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Teachers College Press</publisher-name>.</citation>
</ref>
<ref id="bibr25-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Okhremtchouk</surname><given-names>I.</given-names></name>
<name><surname>Seiki</surname><given-names>S.</given-names></name>
<name><surname>Gilliland</surname><given-names>B.</given-names></name>
<name><surname>Atch</surname><given-names>C.</given-names></name>
<name><surname>Wallace</surname><given-names>M.</given-names></name>
<name><surname>Kato</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Voices of pre-service teachers: Perspectives on the Performance Assessment for California Teachers (PACT)</article-title>. <source>Issues in Teacher Education</source>, <volume>18</volume>(<issue>1</issue>), <fpage>39</fpage>-<lpage>62</lpage>.</citation>
</ref>
<ref id="bibr26-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pecheone</surname><given-names>R. L.</given-names></name>
<name><surname>Chung</surname><given-names>R. R.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Evidence in teacher education: The Performance Assessment for California Teachers (PACT)</article-title>. <source>Journal of Teacher Education</source>, <volume>57</volume>(<issue>1</issue>), <fpage>22</fpage>-<lpage>36</lpage>.</citation>
</ref>
<ref id="bibr27-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pecheone</surname><given-names>R. L.</given-names></name>
<name><surname>Chung</surname><given-names>R. R.</given-names></name>
</person-group> (<year>2007</year>). <source>PACT technical report</source>. <publisher-loc>Stanford, CA</publisher-loc>: <publisher-name>PACT Consortium</publisher-name>.</citation>
</ref>
<ref id="bibr28-0022487111421175">
<citation citation-type="web">
<collab>Performance Assessment for California Teachers Consortium</collab>. (<year>2009</year>). <source>Implementation handbook</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.pacttpa.org/_main/hub.php?pageName=Implementation_Handbook">http://www.pacttpa.org/_main/hub.php?pageName=Implementation_Handbook</ext-link></comment></citation>
</ref>
<ref id="bibr29-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Peterson</surname><given-names>K.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Teacher evaluation with multiple and variable lines of evidence</article-title>. <source>American Educational Research Journal</source>, <volume>24</volume>(<issue>2</issue>), <fpage>311</fpage>-<lpage>317</lpage>.</citation>
</ref>
<ref id="bibr30-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Peterson</surname><given-names>K.</given-names></name>
</person-group> (<year>2000</year>). <source>Teacher evaluation: A comprehensive guide to new directions and practices</source> (<edition>2nd ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin Press</publisher-name>.</citation>
</ref>
<ref id="bibr31-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Porter</surname><given-names>A.</given-names></name>
<name><surname>Youngs</surname><given-names>P.</given-names></name>
<name><surname>Odden</surname><given-names>A.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Advances in teacher assessments and their use</article-title>. In <person-group person-group-type="editor">
<name><surname>Richardson</surname><given-names>V.</given-names></name>
</person-group> (Ed.), <source>Handbook of research on teaching</source> (<edition>4th ed.</edition>, pp. <fpage>259</fpage>-<lpage>297</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr32-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rennert-Ariev</surname><given-names>P.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The hidden curriculum of performance-based teacher education</article-title>. <source>Teachers College Record</source>, <volume>110</volume>(<issue>1</issue>), <fpage>105</fpage>-<lpage>138</lpage>.</citation>
</ref>
<ref id="bibr33-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Richardson</surname><given-names>V.</given-names></name>
<name><surname>Placier</surname><given-names>P.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Teacher change</article-title>. In <person-group person-group-type="editor">
<name><surname>Richardson</surname><given-names>V.</given-names></name>
</person-group> (Ed.), <source>Fourth handbook of research on teaching</source> (pp. <fpage>905</fpage>-<lpage>947</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Educational Research Association</publisher-name>.</citation>
</ref>
<ref id="bibr34-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shulman</surname><given-names>L. S.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Paradigms and research programs in the study of teaching: A contemporary perspective</article-title>. In <person-group person-group-type="editor">
<name><surname>Wittrock</surname><given-names>M.</given-names></name>
</person-group> (Ed.), <source>Handbook of research on teaching</source> (<edition>3rd ed.</edition>, pp. <fpage>3</fpage>-<lpage>36</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Macmillan</publisher-name>.</citation>
</ref>
<ref id="bibr35-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shulman</surname><given-names>L. S.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Knowledge and teaching: Foundations of the new reform</article-title>. <source>Harvard Educational Review</source>, <volume>57</volume>, <fpage>1</fpage>-<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr36-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Snyder</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Taking stock of performance assessments in teaching</article-title>. <source>Issues in Teacher Education</source>, <volume>18</volume>(<issue>1</issue>), <fpage>7</fpage>-<lpage>11</lpage>.</citation>
</ref>
<ref id="bibr37-0022487111421175">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tellez</surname><given-names>K.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Authentic assessment</article-title>. In <person-group person-group-type="editor">
<name><surname>Sikula</surname><given-names>J.</given-names></name>
</person-group> (Ed.), <source>The handbook of research in teacher education</source> (<edition>2nd ed.</edition>, pp. <fpage>704</fpage>-<lpage>721</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Macmillan</publisher-name>.</citation>
</ref>
<ref id="bibr38-0022487111421175">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Uhlenbeck</surname><given-names>A. M.</given-names></name>
<name><surname>Verloop</surname><given-names>N.</given-names></name>
<name><surname>Beijaard</surname><given-names>D.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Requirements for an assessment procedure for beginning teachers: Implications from recent theories on teaching and assessment</article-title>. <source>Teachers College Record</source>, <volume>104</volume>(<issue>2</issue>), <fpage>242</fpage>-<lpage>272</lpage>.</citation>
</ref>
<ref id="bibr39-0022487111421175">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zeichner</surname><given-names>K. M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The adequacies and inadequacies of three current strategies to recruit, prepare, and retain the best teachers for all students</article-title>. <source>Teachers College Record</source>, <volume>105</volume>(<issue>3</issue>), <fpage>490</fpage>-<lpage>519</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>