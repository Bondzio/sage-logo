<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JRM</journal-id>
<journal-id journal-id-type="hwp">spjrm</journal-id>
<journal-title>Journal of Research in Music Education</journal-title>
<issn pub-type="ppub">0022-4294</issn>
<issn pub-type="epub">1945-0095</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0022429411434932</article-id>
<article-id pub-id-type="publisher-id">10.1177_0022429411434932</article-id>
<title-group>
<article-title>An Analysis of the Ratings and Interrater Reliability of High School Band Contests</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Hash</surname><given-names>Phillip M.</given-names></name>
<xref ref-type="aff" rid="aff1-0022429411434932">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0022429411434932"><label>1</label>Calvin College, Grand Rapids, MI, USA</aff>
<author-notes>
<corresp id="corresp1-0022429411434932">Phillip M. Hash, Calvin College, 3201 Burton SE, Grand Rapids, MI 49546 Email: <email>pmh3@calvin.edu</email></corresp>
<fn fn-type="other" id="bio1-0022429411434932">
<p>Phillip M. Hash is associate professor of music education at Calvin College. His research interests include music education history and instrumental music education.</p></fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2012</year>
</pub-date>
<volume>60</volume>
<issue>1</issue>
<fpage>81</fpage>
<lpage>100</lpage>
<history>
<date date-type="received">
<day>25</day>
<month>6</month>
<year>2010</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>5</month>
<year>2011</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012 National Association for Music Education</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">MENC: The National Association for Music Education</copyright-holder>
</permissions>
<abstract>
<p>The purpose of this study was to examine procedures for analyzing ratings of large-group festivals and provide data with which to compare results from similar events. Data consisted of ratings from senior division concert band contests sponsored by the South Carolina Band Directors Association from 2008 to 2010. Three concert-performance and two sight-reading judges evaluated each band to determine a final rating. Research questions examined (a) frequency distributions of ratings; (b) interrater reliability as measured by Spearman correlation of individual judges’ ratings, internal consistency (α), and two forms of interrater agreement (IRA); and (c) differences in mean ratings among individual adjudicators, contest sites, years, and classifications. The average final rating for all bands (<italic>N</italic> = 353) was 1.73, with 86.7% (<italic>n</italic> = 306) earning a I/Superior or II/Excellent. Interrater correlation, IRA, and internal consistency were higher for sight-reading versus concert performance. Each of these measures rose above the .80 benchmark for good reliability, except interrater correlation and average pairwise IRA in the concert portion of the contest. Data indicated significant differences in 8 out of 18 judging panels, in contest sites in 2010, and among ensemble classifications. In this study, the author demonstrated an effective procedure for analyzing ratings of large-group festivals and identified implications for improving these events.</p>
</abstract>
<kwd-group>
<kwd>assessment</kwd>
<kwd>band</kwd>
<kwd>contests</kwd>
<kwd>festivals</kwd>
<kwd>interrater reliability</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Participating in an annual adjudicated festival or contest is an important part of the curriculum for many school instrumental and vocal ensembles (<xref ref-type="bibr" rid="bibr31-0022429411434932">Garman, Boyle, &amp; DeCarbo, 1991</xref>). These events have the potential to raise performance standards, build morale and esprit de corps within the group, increase interest, and provide both students and directors with recommendations for improving ensemble quality (<xref ref-type="bibr" rid="bibr56-0022429411434932">Rohrer, 2002</xref>). Contest ratings might negatively affect directors and programs, as well. Teachers in 74% of states that sponsored orchestra contests (<italic>N</italic> = 39) reported feeling a medium-high to high level of intensity connected with participation (<xref ref-type="bibr" rid="bibr4-0022429411434932">Barnes &amp; McCashin, 2005</xref>), perhaps because many administrators, parents, students, and directors believe that ratings serve as an indicator of teacher and program quality (<xref ref-type="bibr" rid="bibr12-0022429411434932">Boyle, 1992</xref>; <xref ref-type="bibr" rid="bibr15-0022429411434932">Burnsed, Hinkle, &amp; King, 1985</xref>; <xref ref-type="bibr" rid="bibr21-0022429411434932">Conrad, 2003</xref>). Just as high ratings can boost morale, attitude, and interest, low scores may result in reduced student retention or negative attitudes toward the program or director (<xref ref-type="bibr" rid="bibr5-0022429411434932">Batey, 2002</xref>).</p>
<p>Contest ratings also sometimes influence music educator evaluations and may affect job retention or loss (<xref ref-type="bibr" rid="bibr3-0022429411434932">Baker, 2004</xref>; <xref ref-type="bibr" rid="bibr4-0022429411434932">Barnes &amp; McCashin, 2005</xref>; <xref ref-type="bibr" rid="bibr5-0022429411434932">Batey, 2002</xref>; <xref ref-type="bibr" rid="bibr15-0022429411434932">Burnsed et al., 1985</xref>). Furthermore, this trend might increase over the next several years as states continue to pass legislation requiring school districts to tie teacher evaluations to student achievement (<xref ref-type="bibr" rid="bibr32-0022429411434932">Hassel &amp; Hassel, 2009</xref>; <xref ref-type="bibr" rid="bibr42-0022429411434932">Lee, 2009</xref>; <xref ref-type="bibr" rid="bibr67-0022429411434932">Williams, 2009</xref>). Although standardized tests will likely play a large part in determining teacher effectiveness in “core” disciplines (<xref ref-type="bibr" rid="bibr2-0022429411434932">American Recovery and Reinvestment Act, 2009</xref>), it is unclear how administrators will evaluate educators in nontested subjects. Contest ratings may serve this purpose for music teachers (<xref ref-type="bibr" rid="bibr48-0022429411434932">National Association for Music Education [NAfME], 2011</xref>) because these ratings—like standardized tests—provide a third-party evaluation consisting of numerical scores that can be used to compare the achievement of one ensemble or director to another.</p>
<p>Because of the increasing importance placed on contest ratings and their potential to affect the success of music teachers and programs, organizations sponsoring these events should work to establish statistical reliability for the assessments they provide. Only a handful of researchers have analyzed reliability within the context of actual large-group festivals (<xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel, 2006</xref>; <xref ref-type="bibr" rid="bibr15-0022429411434932">Burnsed et al., 1985</xref>; <xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al., 1991</xref>; <xref ref-type="bibr" rid="bibr38-0022429411434932">King &amp; Burnsed, 2009</xref>; <xref ref-type="bibr" rid="bibr41-0022429411434932">Latimer, Bergee, &amp; Cohen, 2010</xref>). Many studies, however, have examined performance evaluation using recorded examples (e.g., <xref ref-type="bibr" rid="bibr39-0022429411434932">Kinney, 2009</xref>; <xref ref-type="bibr" rid="bibr49-0022429411434932">Norris &amp; Borst, 2007</xref>), music majors as adjudicators (e.g., <xref ref-type="bibr" rid="bibr53-0022429411434932">Price &amp; Chang, 2005</xref>), solo and ensemble contest data (e.g., <xref ref-type="bibr" rid="bibr7-0022429411434932">Bergee, 2007</xref>; <xref ref-type="bibr" rid="bibr8-0022429411434932">Bergee &amp; McWhirter, 2005</xref>), and college performance juries (e.g., <xref ref-type="bibr" rid="bibr6-0022429411434932">Bergee, 2003</xref>; <xref ref-type="bibr" rid="bibr20-0022429411434932">Ciorba &amp; Smith, 2009</xref>). Several authors also have considered music evaluation in relation to adjudicator experience (e.g., <xref ref-type="bibr" rid="bibr25-0022429411434932">Fiske, 1975</xref>) and bias (e.g., <xref ref-type="bibr" rid="bibr17-0022429411434932">Cassidy &amp; Sims, 1991</xref>), contest procedures (e.g., <xref ref-type="bibr" rid="bibr4-0022429411434932">Barnes &amp; McCashin, 2005</xref>), repertoire selection (e.g., <xref ref-type="bibr" rid="bibr3-0022429411434932">Baker, 2004</xref>), ensemble size (e.g., <xref ref-type="bibr" rid="bibr35-0022429411434932">Killian, 1998</xref>, <xref ref-type="bibr" rid="bibr36-0022429411434932">1999</xref>, <xref ref-type="bibr" rid="bibr37-0022429411434932">2000</xref>), and grade inflation (<xref ref-type="bibr" rid="bibr11-0022429411434932">Boeckman, 2002</xref>). All of these studies examined factors that may affect the reliability of large-group contest ratings (<xref ref-type="bibr" rid="bibr43-0022429411434932">McPherson &amp; Thompson, 1998</xref>).</p>
<sec id="section1-0022429411434932">
<title>Adjudicator Experience and Contest Procedures</title>
<p>Adjudicator experience is one of the many variables that might affect a group’s final rating. <xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al. (1991)</xref> determined that interrater reliability was lower among inexperienced orchestra judges with backgrounds in composition and private instruction rather than in music education. Additional research by <xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel (2006)</xref> indicated that orchestras received higher scores from wind versus string specialists, suggesting that the wind specialists were less comfortable with string adjudication due to a lack of knowledge about these instruments. <xref ref-type="bibr" rid="bibr4-0022429411434932">Barnes and McCashin (2005)</xref> reported that a number of state organizations had difficulty finding qualified orchestra judges, meaning that individuals unfamiliar with string technique or repertoire commonly evaluated these ensembles. This condition may affect the reliability of final ratings because higher levels of expertise and familiarity with the material evaluated have been associated with higher internal consistency among adjudicators (<xref ref-type="bibr" rid="bibr39-0022429411434932">Kinney, 2009</xref>). Although a number of organizations that sponsor contests provide judges’ training, these efforts may (<xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel, 2006</xref>; <xref ref-type="bibr" rid="bibr33-0022429411434932">Hunter &amp; Russ, 1996</xref>; <xref ref-type="bibr" rid="bibr68-0022429411434932">Winter, 1993</xref>) or may not (<xref ref-type="bibr" rid="bibr11-0022429411434932">Boeckman, 2002</xref>; <xref ref-type="bibr" rid="bibr26-0022429411434932">Fiske, 1978</xref>) improve the consistency of final scores.</p>
<p>The adjudication process and contest schedule also may affect judges’ decisions. Interrater reliability was stronger for individual categories (e.g., tone, intonation, interpretation) and final ratings when judges evaluated choirs using a rubric with detailed descriptors for each possible score (1–5) in all categories versus a traditional form with no category descriptors (<xref ref-type="bibr" rid="bibr49-0022429411434932">Norris &amp; Borst, 2007</xref>). Other factors may include the number of hours adjudicators work in a given day (<xref ref-type="bibr" rid="bibr4-0022429411434932">Barnes &amp; McCashin, 2005</xref>), performance times (<xref ref-type="bibr" rid="bibr8-0022429411434932">Bergee &amp; McWhirter, 2005</xref>; <xref ref-type="bibr" rid="bibr9-0022429411434932">Bergee &amp; Platt, 2003</xref>; <xref ref-type="bibr" rid="bibr28-0022429411434932">Flores &amp; Ginsburgh, 1996</xref>), and the size of the judging panel. <xref ref-type="bibr" rid="bibr6-0022429411434932">Bergee (2003</xref>, <xref ref-type="bibr" rid="bibr7-0022429411434932">2007</xref>) and <xref ref-type="bibr" rid="bibr27-0022429411434932">Fiske (1983)</xref> concluded that a minimum of five judges was needed to attain an acceptable level of interrater reliability in solo performance. <xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel (2006)</xref> determined that reliability was better for three- versus two-member judging panels. However, no significant differences in the reliability of marching band ratings were identified by <xref ref-type="bibr" rid="bibr38-0022429411434932">King and Burnsed (2009)</xref> when high and low scores were eliminated, a procedure that essentially reduced adjudication panels from five to three members.</p>
</sec>
<sec id="section2-0022429411434932">
<title>Repertoire and Ensemble Size</title>
<p>Difficulty of repertoire and ensemble size also might influence adjudication. <xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel (2006)</xref> noted better reliability for Class 1 versus Class 3 high school bands and orchestras in Indiana, indicating a higher level of agreement for advanced groups. In addition, Texas choirs performing repertoires at an advanced level of difficulty for their classifications earned significantly higher ratings than choirs performing music at the minimally required level (<xref ref-type="bibr" rid="bibr3-0022429411434932">Baker, 2004</xref>). The author suggested that teachers who set higher goals for their ensembles in terms of literature might also set higher standards for overall performance. Ratings, therefore, could be an indication of a director’s expectations for the group.</p>
<p><xref ref-type="bibr" rid="bibr38-0022429411434932">King and Burnsed (2009)</xref> and <xref ref-type="bibr" rid="bibr55-0022429411434932">Rickels (2009)</xref> found that larger marching bands received significantly higher ratings than did smaller marching bands in contests held in Virginia and Arizona. Similarly, <xref ref-type="bibr" rid="bibr35-0022429411434932">Killian (1998</xref>, <xref ref-type="bibr" rid="bibr36-0022429411434932">1999</xref>, <xref ref-type="bibr" rid="bibr37-0022429411434932">2000</xref>) determined that larger choirs earned a significantly higher number of superior ratings compared to smaller groups at University Interscholastic League festivals in Texas. Although these findings may indicate a bias of some judges against smaller or less experienced ensembles (<xref ref-type="bibr" rid="bibr64-0022429411434932">Sullivan, 2003</xref>), it also is possible that errors or inconsistencies in tone, intonation, balance, and other areas were more noticeable in these groups.</p>
</sec>
<sec id="section3-0022429411434932">
<title>Adjudicator Bias and Grade Inflation</title>
<p>Other factors also might lead to adjudicator bias, including the level of conductor expressivity (<xref ref-type="bibr" rid="bibr46-0022429411434932">Morrison, Price, Geiger, &amp; Cornacchio, 2009</xref>), ensemble labels such as “concert band” versus “wind ensemble” (<xref ref-type="bibr" rid="bibr59-0022429411434932">Silvey, 2009</xref>) or “high school” versus “beginning” (<xref ref-type="bibr" rid="bibr18-0022429411434932">Cavitt, 1997</xref>), race of performer (<xref ref-type="bibr" rid="bibr22-0022429411434932">Elliot, 1995/1996</xref>) or conductor (<xref ref-type="bibr" rid="bibr66-0022429411434932">VanWeelden &amp; McGee, 2007</xref>), and adjudicators’ knowledge of special circumstances related to the performers (<xref ref-type="bibr" rid="bibr17-0022429411434932">Cassidy &amp; Sims, 1991</xref>; <xref ref-type="bibr" rid="bibr19-0022429411434932">Cavitt, 2002</xref>; <xref ref-type="bibr" rid="bibr54-0022429411434932">Radocy, 1976</xref>). <xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al. (1991)</xref>, for example, discovered that adjudicators informed that orchestras were in the early stages of development might have been more lenient toward these groups. The authors, therefore, recommended that festival managers avoid comments that might bias judges in order to ensure that ratings are as accurate and objective as possible (also see <xref ref-type="bibr" rid="bibr21-0022429411434932">Conrad, 2003</xref>; <xref ref-type="bibr" rid="bibr29-0022429411434932">Forbes, 1994</xref>).</p>
<p>Grade inflation may be an issue in music performance evaluations in general. <xref ref-type="bibr" rid="bibr11-0022429411434932">Boeckman (2002)</xref> found a large gain in band contest ratings in Ohio beginning in the 1970s, coinciding with an educational shift toward self-esteem and relativity in student assessment. Data, furthermore, showed a gradual reluctance on the part of adjudicators to give ratings lower than a Division II, even though five ratings were possible. Between 1971 and 2000, almost all bands in Ohio earned a Division I (45.8%) or II (49.0%) rating. Similar findings have been reported for concert band sight-reading (<xref ref-type="bibr" rid="bibr50-0022429411434932">Orman, Yarbrough, Neill, &amp; Whitaker, 2007</xref>), choral sight-singing (<xref ref-type="bibr" rid="bibr69-0022429411434932">Yarbrough, Orman, &amp; Neill, 2007</xref>), marching band festivals (<xref ref-type="bibr" rid="bibr38-0022429411434932">King &amp; Burnsed, 2009</xref>), and solo and ensemble contests (<xref ref-type="bibr" rid="bibr8-0022429411434932">Bergee &amp; McWhirter, 2005</xref>; <xref ref-type="bibr" rid="bibr9-0022429411434932">Bergee &amp; Platt, 2003</xref>; <xref ref-type="bibr" rid="bibr10-0022429411434932">Bergee &amp; Westfall, 2005</xref>).</p>
</sec>
<sec id="section4-0022429411434932">
<title>Interrater Reliability in Large-Group Contest Settings</title>
<p>Only a few studies have examined interrater reliability among judges working in actual large-group contest settings. <xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel (2006)</xref> analyzed the reliability of three-member panels (<italic>N</italic> = 43) judging high school bands and orchestras (<italic>N</italic> = 840) in festivals sponsored by the Indiana State School Music Association in 2002 and 2003. Data indicated that internal consistency within individual panels ranged from an alpha of .44 to .94 with a mean of .82 in 2002 and from .76 to .94 with a mean of .87 in 2003. Interrater correlation (IRC) as measured by Pearson’s <italic>r</italic> generally was acceptable for ensemble classification in 2002 but ranged from –.12 to 1.0 in 2003. Average reliability by ensemble type also varied widely in 2002 (bands, <italic>r</italic> = .82; string orchestras, <italic>r</italic> = –.23; full orchestras, <italic>r</italic> = .79) but improved in 2003 (bands, <italic>r</italic> = .87; string orchestras, <italic>r</italic> = .58; full orchestras, <italic>r</italic> = .83), perhaps due to an effort in the fall of 2002 to train nonstring specialists to adjudicate orchestras.</p>
<p>Concert band adjudicators at four selected contest sites in Virginia (<italic>n</italic> = 3) and North Carolina (<italic>n</italic> = 1) achieved a high level of interrater reliability (α = .93; <xref ref-type="bibr" rid="bibr15-0022429411434932">Burnsed et al., 1985</xref>). Although some individual captions (tone, intonation, balance, and musical effect) differed significantly, the authors determined that caption scores and final ratings were correlated so highly as to represent a single global performance rating. Similar results have been attained in solo trumpet (<xref ref-type="bibr" rid="bibr25-0022429411434932">Fiske, 1975</xref>), marching band (<xref ref-type="bibr" rid="bibr38-0022429411434932">King &amp; Burnsed, 2009</xref>), and other large-group evaluations (<xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al., 1991</xref>; <xref ref-type="bibr" rid="bibr41-0022429411434932">Latimer et al., 2010</xref>). These findings call into question the validity of individual caption scores and suggest that judges may grade captions to fit the overall rating they intend to issue.</p>
<p>Analysis of the interrater reliability of orchestra festivals in Dade County, Florida, in 1983, 1986, 1987, 1989, and 1990 indicated a wide range of reliability coefficients for final ratings (<italic>r</italic> = .54-.89) and individual captions (<italic>r</italic> = .27-.84), depending on year and category (<xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al., 1991</xref>). Correlations among individual caption scores and final ratings were generally high, with technique and interpretation serving as the best predictors of final ratings. Similar results were attained at high school large-group festivals in Kansas (<xref ref-type="bibr" rid="bibr41-0022429411434932">Latimer et al., 2010</xref>). Reliability as measured by Kendall’s coefficient of concordance (<italic>W</italic>) ranged from .47 to .77 for individual dimension scores and equaled .72 for final ratings and .80 for total score. The categories most highly correlated with the final rating were tone, intonation, and expression.</p>
<p><xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al. (1991)</xref> recommended several ways of improving interrater reliability, including a revision of category descriptions that were well defined and had a common meaning for all adjudicators. <xref ref-type="bibr" rid="bibr38-0022429411434932">King and Burnsed (2009)</xref>, likewise, suggested revising traditional large-group evaluation forms to include criteria-specific rating scales such as those proposed by <xref ref-type="bibr" rid="bibr57-0022429411434932">Saunders and Holahan (1997)</xref> for solo wind performance. Other authors have made similar recommendations after finding that these types of assessment tools attained a high level of internal consistency and interrater reliability (<xref ref-type="bibr" rid="bibr20-0022429411434932">Ciorba &amp; Smith, 2009</xref>; <xref ref-type="bibr" rid="bibr41-0022429411434932">Latimer et al., 2010</xref>; <xref ref-type="bibr" rid="bibr49-0022429411434932">Norris &amp; Borst, 2007</xref>).</p>
</sec>
<sec id="section5-0022429411434932">
<title>Purpose of the Study</title>
<p>The purpose of this study was to examine procedures for analyzing ratings of large-group festivals and provide data with which to compare results from similar events. Data consisted of ratings from senior division (high school) concert band contests sponsored by the South Carolina Band Directors Association (SCBDA) from 2008 to 2010. I examined the following research questions: (1) What was the distribution of ratings among the bands? (2) What was the reliability of individual judging panels? and (3) Did average final ratings differ among judges, contest sites, years, or classifications?</p>
<p>It is not surprising that festival ratings often are called into question due to their importance to various stakeholders in education, the numerous variables that may affect adjudication, the subjective nature of performance evaluation (<xref ref-type="bibr" rid="bibr21-0022429411434932">Conrad, 2003</xref>; <xref ref-type="bibr" rid="bibr29-0022429411434932">Forbes, 1994</xref>; <xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al., 1991</xref>; <xref ref-type="bibr" rid="bibr43-0022429411434932">McPherson &amp; Thompson, 1998</xref>), and the lack of statistical data supporting the results of these assessments (<xref ref-type="bibr" rid="bibr38-0022429411434932">King &amp; Burnsed, 2009</xref>). This study and others are needed to lend credibility to contest evaluations, detect and correct inconsistencies in the judging process, identify models of excellent adjudication for others to emulate, and examine the most effective methods for measuring interrater reliability.</p>
</sec>
<sec id="section6-0022429411434932" sec-type="methods">
<title>Method</title>
<sec id="section7-0022429411434932">
<title>Participants</title>
<p>For this study, I analyzed ratings for 353 senior division bands participating in SCBDA concert festivals held from 2008 to 2010. Although generally composed of high school students, these ensembles could include performers of any age through grade 12 in order to accommodate bands with both middle school and high school players. The director selected the classification in which a band entered the contest based on the difficulty of the band’s repertoire (see <xref ref-type="table" rid="table1-0022429411434932">Table 1</xref>). Each band performed a warm-up piece of the director’s choosing and two adjudicated selections from the SCBDA Concert Music List.</p>
<table-wrap id="table1-0022429411434932" position="float">
<label>Table 1.</label>
<caption>
<p>Classifications of Senior Division Bands in the South Carolina Band Directors Association Concert Festival</p>
</caption>
<graphic alternate-form-of="table1-0022429411434932" xlink:href="10.1177_0022429411434932-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Classification</th>
<th align="center">Repertoire grade level</th>
</tr>
</thead>
<tbody>
<tr>
<td>II</td>
<td>2 &amp; 2</td>
</tr>
<tr>
<td>III</td>
<td>2 &amp; 3 or 3 &amp; 3</td>
</tr>
<tr>
<td>IV</td>
<td>3 &amp; 4 or 4 &amp; 4</td>
</tr>
<tr>
<td>V</td>
<td>4 &amp; 5 or 5 &amp; 5</td>
</tr>
<tr>
<td>VI</td>
<td>5 &amp; 6 or 6 &amp; 6</td>
</tr>
<tr>
<td>VI M<sup><xref ref-type="table-fn" rid="table-fn1-0022429411434932">a</xref></sup></td>
<td>6</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0022429411434932">
<label>a</label>
<p>Designates classification VI Masterworks. In addition to a warm-up selection, only one piece listed on the Grade 6 Masterworks List is required, allowing bands to perform longer works within the allotted time.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>After playing prepared repertoire in the concert-performance segment, bands moved to a different room where they participated in the sight-reading portion. During this event, each group had 6 min to study and then perform an unfamiliar piece of music, the difficulty of which was determined by each band’s classification.</p>
<p>Three concert-performance and two sight-reading judges selected by the SCBDA adjudication committee evaluated each ensemble using forms developed by <xref ref-type="bibr" rid="bibr44-0022429411434932">MENC: The National Association for Music Education (n.d.)</xref>. Judges graded each category from I (superior) to V (poor) and determined a rating based on the distribution of grades. Categories for the concert performance included tone, intonation, technique, balance, interpretation, musical effect, and other factors. Sight-reading categories included technical accuracy, flexibility, interpretation, musical effect, and general comments. Each judge’s rating was converted to points (I = 5, II = 4, III = 3, IV = 2, V = 1), which were added together to determine a single overall rating (I = 25–23, II = 22–18, III = 17–13, IV = 12–8, V = 7–5). With this formula, the concert performance accounted for 60% and the sight-reading 40% of the final rating.</p>
<p>Rating I/Superior represented the best conceivable performance for the event and classification. Rating II/Excellent signified an exceptional performance in many respects but not worthy of the highest rating due to minor defects in performance or ineffective interpretation. Rating III/Good represented a good performance but not outstanding, showing accomplishments and marked promise but lacking in one or more essential qualities. Rating IV/Average designated an average performance but not worthy of a III rating. Rating V/Poor indicated much room for improvement. Contest rules required judges to sit in separate locations in the performance venue and render decisions without conferring with one another (<xref ref-type="bibr" rid="bibr60-0022429411434932">SCBDA, 2009</xref>).</p>
</sec>
<sec id="section8-0022429411434932">
<title>Data Analysis</title>
<p>Data for this study included individual and final ratings assigned by 45 adjudicators (27 concert performance, 18 sight-reading) from 18 judging panels (nine concert performance, nine sight-reading) at nine contest locations over a three-year period. I downloaded ratings from the SCBDA website, transferred them to a Microsoft Office 2007 Excel database, transposed the classifications and ratings from Roman to Arabic numerals, and entered them into SPSS 15.0 for statistical analysis. A contest host helped fill in incomplete classification data for one festival site in 2009. Analysis involved nonparametric statistics because contest ratings represent ordinal data with no absolute value or distance between ranks (<xref ref-type="bibr" rid="bibr10-0022429411434932">Bergee &amp; Westfall, 2005</xref>; <xref ref-type="bibr" rid="bibr51-0022429411434932">Phillips, 2008</xref>). The institutional review board at Calvin College reviewed and approved these procedures.</p>
<p>Descriptive and frequency data were compiled for final ratings awarded in each classification and the three years combined, as well as the number of each possible rating (I–V) issued by individual adjudicators. I measured interrater reliability through a number of statistical procedures, each of which described a different aspect of this construct. IRC as expressed by Spearman’s rank order coefficient (<italic>r<sub>s</sub></italic>) measured the extent to which individual judges’ ratings moved in the same direction. For the concert performance, this statistic equaled the average pairwise correlation among the three adjudicators. Internal consistency for both concert-performance and sight-reading panels was determined using Cronbach’s alpha (α) with the ratings of individual judges treated as “items.” This calculation indicated the degree to which judges’ ratings corresponded with one another (<xref ref-type="bibr" rid="bibr1-0022429411434932">Adler &amp; Clark, 2008</xref>). I also calculated interrater agreement (IRA) because neither <italic>r<sub>s</sub></italic> nor alpha measured agreement among judges’ ratings. In the concert performance, IRA included two calculations—the average percentage of pairwise agreement (IRA<sub>pw</sub>) between individual judges (<xref ref-type="bibr" rid="bibr30-0022429411434932">Freelon, 2009</xref>) and the percentage of agreements for all ratings combined (IRA<sub>co</sub>). Calculating IRA<sub>co</sub> involved dividing the total number of agreements within each performance (0, 2, or 3) by the total number of ratings issued (<italic>N</italic> = 1,059). IRA for sight-reading simply equaled the percentage of agreements between the two adjudicators.</p>
<p>All calculations of mean IRC involved Fischer’s <italic>z</italic> transformation whereby coefficients (<italic>r<sub>s</sub></italic>) were transformed to <italic>z</italic>, averaged, and back transformed to <italic>r<sub>s</sub></italic> in order to control for underestimation caused by averaging raw correlations (<xref ref-type="bibr" rid="bibr58-0022429411434932">Silver &amp; Dunlap, 1987</xref>). Combined IRA<sub>pw</sub> and alpha simply equaled the average of these statistics from each contest site. Total IRA<sub>co</sub> for the concert performance and IRA in sight-reading were calculated using the entire data set since these measures could be considered separately from specific adjudication panels. A benchmark of .80, as proposed by <xref ref-type="bibr" rid="bibr16-0022429411434932">Carmines and Zeller (1979)</xref> and <xref ref-type="bibr" rid="bibr40-0022429411434932">Krippendorff (2004)</xref>, represented the minimum level for good reliability in this study. Additional analysis using Friedman and Kruskal-Wallis ANOVAs and post hoc Mann-Whitney <italic>U</italic> tests examined differences in mean ratings among individual judges, sites, years, and classifications to determine the level of consistency between these variables.</p>
<p>All of these procedures were necessary to assess reliability and diagnose potential inconsistencies in adjudication. Individual judges’ ratings, for example, might move in the same direction (<italic>r<sub>s</sub></italic>) but not agree (<xref ref-type="bibr" rid="bibr7-0022429411434932">Bergee, 2007</xref>). This result would likely indicate that adjudicators evaluated bands in a similar manner but at different levels of severity. Data also may indicate a high level of internal consistency (α), even if ratings do not agree. This finding also could imply varying levels of severity or that judges scored groups in a contrary manner due to the application of differing criterion. Likewise, ratings with a low level of internal consistency (α) may suggest that individual evaluators applied an inconsistent standard from one group to the next (<xref ref-type="bibr" rid="bibr26-0022429411434932">Fiske, 1978</xref>). Finally, analyzing the percentage of IRA alone might illustrate the simplest form of reliability but would not provide enough information to diagnose possible causes for a low result.</p>
<p>Data needed to calculate the reliability of individual captions (e.g., sound quality, balance) were unavailable for this study. Other research (<xref ref-type="bibr" rid="bibr15-0022429411434932">Burnsed et al., 1985</xref>; <xref ref-type="bibr" rid="bibr25-0022429411434932">Fiske, 1975</xref>; <xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al., 1991</xref>; <xref ref-type="bibr" rid="bibr38-0022429411434932">King &amp; Burnsed, 2009</xref>), however, has suggested that these scores probably had little effect on results because judges appear to mark individual categories to add up to a predetermined final rating.</p>
</sec>
</sec>
<sec id="section9-0022429411434932" sec-type="results">
<title>Results</title>
<sec id="section10-0022429411434932">
<title>Distribution of Ratings</title>
<p>Mean final ratings by site varied from 1.87 (<italic>SD</italic> = 0.72) to 1.51 (<italic>SD</italic> = 0.54) for an average of 1.73 (<italic>SD</italic> = 0.70) for all bands (<italic>N</italic> = 353) over the three-year period (see <xref ref-type="table" rid="table2-0022429411434932">Table 2</xref>). Most bands (86.7%, <italic>n</italic> = 306) earned a final rating of I/Superior (40.8%, <italic>n</italic> = 144) or II/Excellent (45.9%, <italic>n</italic> = 162). Only 13.3% (<italic>n</italic> = 47) of the groups earned a III/Good (12.7%, <italic>n</italic> = 45) or IV/Fair (0.6%, <italic>n</italic> = 2), and no bands earned a V/Poor (see <xref ref-type="table" rid="table3-0022429411434932">Table 3</xref>). Individual judges’ scores also reflected low variability in both concert performance and sight-reading ratings. Of the total number of individual judges’ ratings issued in each event (concert performance, <italic>N</italic> = 1,059; sight-reading, <italic>N</italic> = 706), most (concert performance: 81.9%, <italic>n</italic> = 867; sight-reading: 88.4%, <italic>n</italic> = 624) were either a I/Superior (concert-performance: 37.7%, <italic>n</italic> = 399; sight-reading: 52.3%, <italic>n</italic> = 369) or II/Excellent (concert-performance: 44.2%, <italic>n</italic> = 468; sight-reading: 36.1%, <italic>n</italic> = 255). Only a small number of individual judges’ ratings equaled a III/Good (concert performance: 15.7%, <italic>n</italic> = 166; sight-reading, 11.0%, <italic>n</italic> = 78), IV/Fair (concert performance: 2.4%, <italic>n</italic> = 25; sight-reading, 0.6%, <italic>n</italic> = 4), or V/Poor (concert performance: 0.1%, <italic>n</italic> = 1; sight reading, 0.0%, <italic>n</italic> = 0). Average final ratings and the percentage of bands earning a I/Superior were higher for each advancing classification with the exception of bands in Class 3, of which 14.1% earned Superior ratings compared to 16.1% of ensembles in Class 2 (see <xref ref-type="table" rid="table3-0022429411434932">Table 3</xref>).</p>
<table-wrap id="table2-0022429411434932" position="float">
<label>Table 2.</label>
<caption>
<p>Interrater Reliability and Mean Ratings by Site and Year</p>
</caption>
<graphic alternate-form-of="table2-0022429411434932" xlink:href="10.1177_0022429411434932-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th/>
<th align="center" colspan="9">Interrater reliability</th>
</tr>
<tr>
<th/>
<th/>
<th/>
<th align="center" colspan="4">Concert performance</th>
<th align="center" colspan="3">Sight-reading</th>
<th align="center" colspan="2">Final ratings</th>
</tr>
<tr>
<th align="left">Year</th>
<th align="center">Site</th>
<th align="center"><italic>n</italic> bands</th>
<th align="center"><italic>r<sub>s</sub></italic><sup><xref ref-type="table-fn" rid="table-fn2-0022429411434932">a</xref>,<xref ref-type="table-fn" rid="table-fn3-0022429411434932">b</xref></sup></th>
<th align="center">IRA<sub>pw</sub><sup><xref ref-type="table-fn" rid="table-fn4-0022429411434932">c</xref></sup></th>
<th align="center">IRA<sub>co</sub><sup><xref ref-type="table-fn" rid="table-fn5-0022429411434932">d</xref></sup></th>
<th align="center">α</th>
<th align="center"><italic>r<sub>s</sub></italic><sup><xref ref-type="table-fn" rid="table-fn3-0022429411434932">b</xref></sup></th>
<th align="center">IRA<sup><xref ref-type="table-fn" rid="table-fn6-0022429411434932">e</xref></sup></th>
<th align="center">α</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>2008</td>
<td>1</td>
<td>27</td>
<td>.84</td>
<td>.88</td>
<td>.94</td>
<td>.94</td>
<td>.86</td>
<td>.93</td>
<td>.92</td>
<td>1.52</td>
<td>0.51</td>
</tr>
<tr>
<td/>
<td>2</td>
<td>29</td>
<td>.74</td>
<td>.61</td>
<td>.80</td>
<td>.90</td>
<td>.94</td>
<td>.93</td>
<td>.97</td>
<td>1.86</td>
<td>0.74</td>
</tr>
<tr>
<td/>
<td>3</td>
<td>60</td>
<td>.73</td>
<td>.64</td>
<td>.82</td>
<td>.88</td>
<td>.85</td>
<td>.82</td>
<td>.94</td>
<td>1.87</td>
<td>0.72</td>
</tr>
<tr>
<td/>
<td>Combined</td>
<td>116</td>
<td>.77</td>
<td>.71</td>
<td>.84</td>
<td>.91</td>
<td>.89</td>
<td>.87</td>
<td>.94</td>
<td>1.78</td>
<td>0.70</td>
</tr>
<tr>
<td>2009</td>
<td>1</td>
<td>47</td>
<td>.76</td>
<td>.65</td>
<td>.82</td>
<td>.90</td>
<td>.91</td>
<td>.91</td>
<td>.95</td>
<td>1.57</td>
<td>0.72</td>
</tr>
<tr>
<td/>
<td>2</td>
<td>34</td>
<td>.74</td>
<td>.70</td>
<td>.85</td>
<td>.92</td>
<td>.95</td>
<td>.94</td>
<td>.97</td>
<td>1.76</td>
<td>0.70</td>
</tr>
<tr>
<td/>
<td>3</td>
<td>40</td>
<td>.79</td>
<td>.68</td>
<td>.84</td>
<td>.91</td>
<td>.84</td>
<td>.83</td>
<td>.91</td>
<td>1.73</td>
<td>0.78</td>
</tr>
<tr>
<td/>
<td>Combined</td>
<td>121</td>
<td>.76</td>
<td>.67</td>
<td>.83</td>
<td>.91</td>
<td>.91</td>
<td>.89</td>
<td>.94</td>
<td>1.68</td>
<td>0.73</td>
</tr>
<tr>
<td>2010</td>
<td>1</td>
<td>33</td>
<td>.44</td>
<td>.54</td>
<td>.74</td>
<td>.70</td>
<td>.77</td>
<td>.76</td>
<td>.87</td>
<td>1.82</td>
<td>0.58</td>
</tr>
<tr>
<td/>
<td>2</td>
<td>38</td>
<td>.86</td>
<td>.80</td>
<td>.81</td>
<td>.94</td>
<td>.66</td>
<td>.82</td>
<td>.82</td>
<td>1.92</td>
<td>0.77</td>
</tr>
<tr>
<td/>
<td>3</td>
<td>45</td>
<td>.78</td>
<td>.82</td>
<td>.91</td>
<td>.91</td>
<td>.65</td>
<td>.82</td>
<td>.82</td>
<td>1.51</td>
<td>0.54</td>
</tr>
<tr>
<td/>
<td>Combined</td>
<td>116</td>
<td>.72</td>
<td>.72</td>
<td>.85</td>
<td>.85</td>
<td>.70</td>
<td>.80</td>
<td>.84</td>
<td>1.73</td>
<td>0.66</td>
</tr>
<tr>
<td>2008–2010</td>
<td/>
<td>353</td>
<td>.75</td>
<td>.70</td>
<td>.84</td>
<td>.89</td>
<td>.85</td>
<td>.86</td>
<td>.91</td>
<td>1.73</td>
<td>0.70</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0022429411434932">
<label>a</label>
<p>aAveraged for three judges using Fischer’s <italic>z</italic> transformation (<xref ref-type="bibr" rid="bibr58-0022429411434932">Silver &amp; Dunlap, 1987</xref>).</p>
</fn>
<fn id="table-fn3-0022429411434932">
<label>b</label>
<p>Combined <italic>r<sub>s</sub></italic> calculated using Fischer’s <italic>z</italic> transformation.</p>
</fn>
<fn id="table-fn4-0022429411434932">
<label>c</label>
<p>Average percentage of pairwise interrater agreement (IRA<sub>pw</sub>).</p>
</fn>
<fn id="table-fn5-0022429411434932">
<label>d</label>
<p>Combined interrater agreement (IRA<sub>co</sub>).</p>
</fn>
<fn id="table-fn6-0022429411434932">
<label>e</label>
<p>Percentage of interrater agreement (IRA).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table3-0022429411434932" position="float">
<label>Table 3.</label>
<caption>
<p>Final Ratings and Interrater Agreement by Classification</p>
</caption>
<graphic alternate-form-of="table3-0022429411434932" xlink:href="10.1177_0022429411434932-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th/>
<th/>
<th align="center" colspan="5">Frequency (<italic>n</italic>, %)</th>
<th align="center" colspan="2">Interrater agreement</th>
</tr>
<tr>
<th align="center">Classification</th>
<th align="center"><italic>n</italic> bands</th>
<th align="center"><italic>M</italic></th>
<th align="center"><italic>SD</italic></th>
<th align="center">I</th>
<th align="center">II</th>
<th align="center">III</th>
<th align="center">IV</th>
<th align="center">V</th>
<th align="center">Concert performance<sup><xref ref-type="table-fn" rid="table-fn7-0022429411434932">a</xref></sup></th>
<th align="center">Sight-reading</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>6</td>
<td>2.17</td>
<td>0.75</td>
<td>116.7</td>
<td>350.0</td>
<td>333.3</td>
<td>00.0</td>
<td>00.0</td>
<td>.94<sup><xref ref-type="table-fn" rid="table-fn8-0022429411434932">b</xref></sup></td>
<td>.67<sup><xref ref-type="table-fn" rid="table-fn8-0022429411434932">b</xref></sup></td>
</tr>
<tr>
<td>3</td>
<td>85</td>
<td>2.11</td>
<td>0.64</td>
<td>1214.1</td>
<td>5362.4</td>
<td>1922.4</td>
<td>11.2</td>
<td>00.0</td>
<td>.85</td>
<td>.87</td>
</tr>
<tr>
<td>4</td>
<td>140</td>
<td>1.74</td>
<td>0.67</td>
<td>5438.6</td>
<td>6848.6</td>
<td>1812.9</td>
<td>00.0</td>
<td>00.0</td>
<td>.83</td>
<td>.83</td>
</tr>
<tr>
<td>5</td>
<td>72</td>
<td>1.64</td>
<td>0.70</td>
<td>3447.2</td>
<td>3143.1</td>
<td>68.3</td>
<td>11.4</td>
<td>00.0</td>
<td>.79</td>
<td>.85</td>
</tr>
<tr>
<td>6/6M</td>
<td>50</td>
<td>1.14</td>
<td>0.35</td>
<td>4386.0</td>
<td>714.0</td>
<td>00.0</td>
<td>00.0</td>
<td>00.0</td>
<td>.91</td>
<td>.94</td>
</tr>
<tr>
<td>Combined</td>
<td>353</td>
<td>1.73</td>
<td>0.70</td>
<td>14440.8</td>
<td>16245.9</td>
<td>4512.7</td>
<td>20.6</td>
<td>00.0</td>
<td>.84</td>
<td>.86</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-0022429411434932">
<label>a</label>
<p>Represents combined interrater agreement (IRA<sub>co</sub>).</p>
</fn>
<fn id="table-fn8-0022429411434932">
<label>b</label>
<p>Interpret with caution due to low <italic>n</italic>.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section11-0022429411434932">
<title>Interrater Reliability</title>
<p>In the concert-performance segment of the contest, average pairwise IRC between individual judges’ ratings ranged from a Spearman rank order coefficient (<italic>r<sub>s</sub></italic>) of .44 to .86 with an average of .75. Average IRA<sub>pw</sub> varied from .54 to .88 for an average of .70. IRA<sub>co</sub> considered ratings within each performance without comparing specific judges’ decisions. This coefficient ranged from .74 to .94 among sites and .79 to .94 among classifications, and it equaled .84 for all three years combined. Internal consistency as measured by Cronbach’s alpha (α) varied from .70 to .94 for an average of .89 (see <xref ref-type="table" rid="table2-0022429411434932">Tables 2</xref> and <xref ref-type="table" rid="table3-0022429411434932">3</xref>).</p>
<p>In the sight-reading event, IRC ranged from Spearman rank order coefficient (<italic>r<sub>s</sub></italic>) of .65 to .95 with an average of .85. Percentage of IRA varied from .76 to .94 among sites and .67 to .94 among classifications, and equaled .86 for all years combined. Internal consistency as measured by Cronbach’s alpha (α) ranged from .82 to .97 for an average of .91 among the nine contest locations (see <xref ref-type="table" rid="table2-0022429411434932">Tables 2</xref> and <xref ref-type="table" rid="table3-0022429411434932">3</xref>).</p>
<p>None of these procedures considered the magnitude of the difference among individual judges’ ratings. However, only six bands in the concert performance and none in sight-reading received scores more than one rating apart. Furthermore, 55.2% (<italic>n</italic> = 195) of bands earned identical ratings from the three judges for the concert performance, and 85.6% (<italic>n</italic> = 302) earned the same rating from both evaluators in the sight-reading event.</p>
</sec>
<sec id="section12-0022429411434932">
<title>Ratings Comparisons</title>
<p>Results of Friedman ANOVAs revealed significant differences among individual judges’ ratings within 8 of the 18 adjudication panels (see <xref ref-type="table" rid="table4-0022429411434932">Table 4</xref>), indicating that some evaluators graded at a higher degree of severity than others did. Nonetheless, results of Kruskal-Wallis ANOVAs indicated no significant differences in final ratings between contest sites in 2008 (<italic>n</italic> = 116, <italic>df</italic> = 2, χ<sup>2</sup> = 4.57, <italic>p</italic> = .102) or 2009 (<italic>n</italic> = 121, <italic>df</italic> = 2, χ<sup>2</sup> = 1.86, <italic>p</italic> = .396), and only one difference in 2010 (<italic>n</italic> = 116, <italic>df</italic> = 2, χ<sup>2</sup> = 7.82, <italic>p</italic> = .02), with bands at Site 3 earning significantly higher ratings than those at Sites 1 and 2 (Site 1 vs. Site 2: <italic>n</italic> = 71, <italic>U</italic> = 658.5, <italic>p</italic> = .719; Site 2 vs. Site 3: <italic>n</italic> = 83, <italic>U</italic> = 612.5, <italic>p</italic> = .027; Site 1 vs. Site 3: <italic>n</italic> = 78, <italic>U</italic> = 544.5, <italic>p</italic> = .046). No significant differences were found between the mean final ratings for contests held in 2008, 2009, or 2010 (<italic>N</italic> = 353, <italic>df</italic> = 2, χ2 = 1.94, <italic>p</italic> = .380).</p>
<table-wrap id="table4-0022429411434932" position="float">
<label>Table 4.</label>
<caption>
<p>Friedman ANOVA Comparisons of Judging Panels</p>
</caption>
<graphic alternate-form-of="table4-0022429411434932" xlink:href="10.1177_0022429411434932-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th/>
<th align="center" colspan="2">Concert performance<sup><xref ref-type="table-fn" rid="table-fn9-0022429411434932">a</xref></sup></th>
<th align="center" colspan="2">Sight-reading<sup><xref ref-type="table-fn" rid="table-fn10-0022429411434932">b</xref></sup></th>
</tr>
<tr>
<th align="center">Year</th>
<th align="center">Site</th>
<th align="center"><italic>n</italic></th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>p</italic></th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>2008</td>
<td>1</td>
<td>27</td>
<td>8.40</td>
<td>.015</td>
<td>2.00</td>
<td>.157</td>
</tr>
<tr>
<td/>
<td>2</td>
<td>29</td>
<td>5.06</td>
<td>.08</td>
<td>2.00</td>
<td>.157</td>
</tr>
<tr>
<td/>
<td>3</td>
<td>60</td>
<td>4.06</td>
<td>.131</td>
<td>0.09</td>
<td>.763</td>
</tr>
<tr>
<td>2009</td>
<td>1</td>
<td>47</td>
<td>0.24</td>
<td>.887</td>
<td>4.00</td>
<td>.046</td>
</tr>
<tr>
<td/>
<td>2</td>
<td>34</td>
<td>23.13</td>
<td>.000</td>
<td>2.00</td>
<td>.157</td>
</tr>
<tr>
<td/>
<td>3</td>
<td>40</td>
<td>7.05</td>
<td>.029</td>
<td>7.00</td>
<td>.008</td>
</tr>
<tr>
<td>2010</td>
<td>1</td>
<td>33</td>
<td>6.71</td>
<td>.035</td>
<td>4.50</td>
<td>.034</td>
</tr>
<tr>
<td/>
<td>2</td>
<td>38</td>
<td>0.24</td>
<td>.889</td>
<td>0.14</td>
<td>.705</td>
</tr>
<tr>
<td/>
<td>3</td>
<td>45</td>
<td>7.17</td>
<td>.028</td>
<td>0.50</td>
<td>.480</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn9-0022429411434932">
<label>a</label>
<p><italic>df</italic> = 2.</p></fn>
<fn id="table-fn10-0022429411434932">
<label>b</label>
<p><italic>df</italic> = 1.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Significant differences also were found in the mean final ratings for the different classifications (<italic>N</italic> = 353, <italic>df</italic> = 4, <italic>χ2</italic> = 69.67, <italic>p</italic> &lt; .001). Therefore, I conducted a post hoc analysis using a series of Mann-Whitney <italic>U</italic> tests to identify the significant differences among these groups. The conservative Bonferroni correction was applied to attempt to control for the greater chance of Type I error that results from multiple comparisons. Accordingly, an alpha level of .005 was considered the threshold for statistical significance (i.e., α = .05/10 comparisons). This analysis found that Class 3 bands scored significantly lower than ensembles in Classes 4, 5, or 6 (<italic>p</italic> &lt; .001) and that Class 6 bands received ratings significantly higher than those in all other classifications (<italic>p</italic> &lt; .001; see <xref ref-type="table" rid="table5-0022429411434932">Table 5</xref>). Data also indicated a moderately low but significant negative correlation between classification and final rating (<italic>r<sub>s</sub></italic> = –.42, <italic>p</italic> &lt; .001), with the final rating higher for each advancing classification (see <xref ref-type="table" rid="table3-0022429411434932">Table 3</xref>).</p>
<table-wrap id="table5-0022429411434932" position="float">
<label>Table 5.</label>
<caption>
<p>Mean Ratings Comparison by Classification: 2008–2010</p>
</caption>
<graphic alternate-form-of="table5-0022429411434932" xlink:href="10.1177_0022429411434932-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Classification</th>
<th align="center"><italic>n</italic></th>
<th align="center"><italic>U</italic></th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>2 vs. 3</td>
<td>91</td>
<td>239.5</td>
<td>.775</td>
</tr>
<tr>
<td>2 vs. 4</td>
<td>146</td>
<td>287.0</td>
<td>.150</td>
</tr>
<tr>
<td>2 vs. 5</td>
<td>78</td>
<td>130.5</td>
<td>.078</td>
</tr>
<tr>
<td>2 vs. 6</td>
<td>56</td>
<td>39.0</td>
<td>&lt;.001</td>
</tr>
<tr>
<td>3 vs. 4</td>
<td>225</td>
<td>4,283.0</td>
<td>&lt;.001</td>
</tr>
<tr>
<td>3 vs. 5</td>
<td>157</td>
<td>1,929.0</td>
<td>&lt;.001</td>
</tr>
<tr>
<td>3 vs. 6</td>
<td>135</td>
<td>527.5</td>
<td>&lt;.001</td>
</tr>
<tr>
<td>4 vs. 5</td>
<td>212</td>
<td>15,378.0</td>
<td>.223</td>
</tr>
<tr>
<td>4 vs. 6</td>
<td>190</td>
<td>1,777.0</td>
<td>&lt;.001</td>
</tr>
<tr>
<td>5 vs. 6</td>
<td>122</td>
<td>4,283.0</td>
<td>&lt;.001</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn11-0022429411434932">
<p>Note: <italic>df</italic> = 1.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section13-0022429411434932" sec-type="discussion">
<title>Discussion</title>
<p>This study was designed to examine procedures for analyzing the consistency and fairness of large-group festivals using data from SCBDA high school concert band contests from 2008 to 2010. The following discussion considers the findings from this research as well as their implications for improving large-group festival adjudication. Readers should generalize these results and read the subsequent recommendations with caution, however, due to differences in contest procedures and evaluation from place to place and due to the fact that the data do not always indicate definitive conclusions.</p>
<sec id="section14-0022429411434932">
<title>Distribution of Ratings</title>
<p>The data do not explain why the mean final rating (1.78) and frequency of Division I (40.8%) and II ratings (45.9%) were so high, in light of the fact that there were five potential ratings. Perhaps these scores reflected the quality of the ensembles, and if so, stakeholders in music education should be pleased that there are so many good band programs in South Carolina. It also is possible that adjudicators consciously or subconsciously boosted ratings in an attempt to encourage students and directors (e.g., <xref ref-type="bibr" rid="bibr11-0022429411434932">Boeckman, 2002</xref>) or that only the best bands in the state attended the festivals. In 2009, for example, just 48.7% (<italic>n</italic> = 91) of the 187 public high schools in South Carolina (<xref ref-type="bibr" rid="bibr61-0022429411434932">South Carolina Department of Education, 2010</xref>) took part in SCBDA contests. Furthermore, eight bands slated to appear cancelled their performances prior to the event. Maybe directors of less accomplished groups chose not to attend and risk their reputations or the morale of the groups on a poor result (e.g., <xref ref-type="bibr" rid="bibr5-0022429411434932">Batey, 2002</xref>). If this is the case, organizers might encourage developing bands to perform for comments only, meaning that judges offer recommendations for improvement without awarding a final rating. This option is common in large-group contests (e.g., <xref ref-type="bibr" rid="bibr34-0022429411434932">Illinois High School Association, 2009</xref>; <xref ref-type="bibr" rid="bibr60-0022429411434932">SCBDA, 2009</xref>) and allows students and directors the benefit of judges’ expertise without having to worry about the consequences of a low rating. Between 2008 and 2010, only 11 bands entered SCBDA senior division contests under this designation. There could have been other explanations for nonparticipation, as well. Some directors or administrators might have believed that festival participation was too costly or not the best way to evaluate student achievement, while other schools may simply not have had a band program.</p>
</sec>
<sec id="section15-0022429411434932">
<title>Interrater Reliability</title>
<p>Interrater reliability coefficients for the concert portion of the contests indicated that judges reached only a moderately high level of agreement in ranking bands (<italic>r<sub>s</sub></italic> = .75) and issuing specific ratings (IRA<sub>pw</sub> = .70). Although neither IRC (<italic>r<sub>s</sub></italic>) nor IRA<sub>pw</sub> achieved the minimum benchmark of .80, internal consistency was much higher (α = .89), meaning that individual adjudicators were reliable in how they determined final ratings. Furthermore, the fact that IRA<sub>co</sub> reached .80 at all but one site and .84 for all three years combined suggests that panels of three judges provided a system of checks and balances that compensated for measurement error and helped ensure a fair result. It is possible that readings for IRC were deflated due to low variability rather than to inconsistent adjudication (<xref ref-type="bibr" rid="bibr63-0022429411434932">Stemler, 2004</xref>) in both portions of the contest, given that 81.9% (<italic>n</italic> = 867) of concert-performance and 88.4% (<italic>n</italic> = 624) of sight-reading ratings were either a I or II.</p>
<p>Previous studies have found that reliability improved as the number of evaluators increased for both solo (<xref ref-type="bibr" rid="bibr6-0022429411434932">Bergee, 2003</xref>) and large-group (<xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel, 2006</xref>) performance. In this study, however, all measures of reliability indicated that two-member panels judging sight-reading were more reliable than were three-member panels assessing the concert performance. Further analysis applying the Spearman-Brown prophesy formula to Cronbach’s alpha (<xref ref-type="bibr" rid="bibr14-0022429411434932">Brown, 2001</xref>) predicted that adding one judge to each of the panels would increase internal consistency only by an average of .05 for concert performance and .03 for sight-reading. Although this change would put the combined alpha over .90 for the concert portion, the added expense of hiring additional judges would probably not be worth such a small gain. The Spearman-Brown formula also indicated that removing one judge from the concert performance panel would have decreased alpha by only .05, meaning that by this estimation, a two-member concert-performance panel still would have provided acceptable reliability (α = .84). A panel of this size, however, would theoretically result in much lower IRA (IRA<sub>pw</sub> = .70 vs. IRA<sub>co</sub> = .84) and remove the system of checks and balances now in place for this event. These findings support the assertion that three judges for concert-performance and two judges for sight-reading are appropriate-size panels for SCBDA contests.</p>
<p>All reliability coefficients for the three years combined were somewhat skewed due to one concert-performance panel and all sight-reading panels in 2010 (see <xref ref-type="table" rid="table2-0022429411434932">Table 2</xref>). Further analysis indicated relatively low readings for all measures of pairwise reliability for the concert-performance panel at Site 1 (Judge 1 vs. Judge 2: <italic>r<sub>s</sub></italic> = .48, IRA<sub>pw</sub> = .58, α = .64; Judge 2 vs. Judge 3: <italic>r<sub>s</sub></italic> = .42, IRA<sub>pw</sub> = .52, α = .60; Judge 1 vs. Judge 3: <italic>r<sub>s</sub></italic> = .42, IRA<sub>pw</sub> = .52, α = .59), suggesting that at least two of the three judges did not score consistently from one performance to the next. In addition, reliability coefficients for all sight-reading panels in 2010 were lower than those of 2008 and 2009, with four of the nine individual measures failing to meet the .80 benchmark. These results may have been due to the experience (e.g., <xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel, 2006</xref>; <xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al., 1991</xref>) or ability of the judges, the particular combinations of individuals on the panels, or a number of other factors (e.g., <xref ref-type="bibr" rid="bibr21-0022429411434932">Conrad, 2003</xref>; <xref ref-type="bibr" rid="bibr43-0022429411434932">McPherson &amp; Thompson, 1998</xref>). <xref ref-type="bibr" rid="bibr29-0022429411434932">Forbes (1994)</xref> recommended hiring only judges who demonstrated high reliability but admitted that the identification of reliable evaluators is difficult because a standardized procedure for certifying adjudicators had yet to be developed.</p>
</sec>
<sec id="section16-0022429411434932">
<title>Group Differences</title>
<p>Data from this study concurred with data from previous research (<xref ref-type="bibr" rid="bibr3-0022429411434932">Baker, 2004</xref>; <xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel, 2006</xref>); groups performing more advanced repertoires earned significantly higher ratings than those playing less difficult selections. Although this result may be due to adjudicator bias against smaller or less experienced ensembles (<xref ref-type="bibr" rid="bibr64-0022429411434932">Sullivan, 2003</xref>), it is more likely that ratings accurately reflected musical quality. The fact that three out of four measures of IRA for Class 2 and 3 ensembles were higher than those for Class 4 and 5 groups (see <xref ref-type="table" rid="table3-0022429411434932">Table 3</xref>) suggests that bias against easier repertoire was probably not a factor unless this attitude was pervasive among many of the judges over the three-year period.</p>
</sec>
<sec id="section17-0022429411434932">
<title>Implications</title>
<p>Adjudicators in this study and others (e.g., <xref ref-type="bibr" rid="bibr11-0022429411434932">Boeckman, 2002</xref>) awarded a preponderance of Division I and II ratings. Although this practice may increase festival participation and provide a source of encouragement for students and directors, it probably does not adequately differentiate ensembles at various levels of achievement and, therefore, may actually weaken the validity of these ratings. Contest organizers facing this situation might consider experimenting with different adjudication models in order to ensure that final ratings reflect performance quality. For example, authors have recommended adjudication rubrics with specific descriptors of what constitutes a given score in each category (<xref ref-type="bibr" rid="bibr41-0022429411434932">Latimer et al., 2010</xref>; <xref ref-type="bibr" rid="bibr49-0022429411434932">Norris &amp; Borst, 2007</xref>). This change in procedure might reduce the tendency of judges to consider nonperformance factors (<xref ref-type="bibr" rid="bibr31-0022429411434932">Garman et al., 1991</xref>) and encourage the use of lower ratings when appropriate. The Indiana State School Music Association implemented a system that replaced traditional I and II ratings with gold, silver, and bronze designations (<xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel, 2006</xref>). The gold rating corresponded to a Division I, while the silver and bronze corresponded to a high Division II and a low Division II, respectively. This system divided the total point spread of the top two ratings into three categories, thus creating more accurate assessment labels without lowering scores.</p>
<p>Data from this study emphasized the need for multiple measures of reliability due to the wide range of coefficients various statistics can yield for the same observations. In the concert-performance event, for example, IRC (<italic>r<sub>s</sub></italic> = .75) and IRA<sub>pw</sub> (.70) did not meet the threshold for good reliability (.80) even though Cronbach’s alpha (.89) and IRA<sub>co</sub> (.84) indicated satisfactory levels. Low variability rather than inconsistent adjudication, however, may have resulted in deflated measures of both IRC and alpha. Consequently, reliance on consistency estimates alone might lead to false conclusions that interrater reliability was unacceptable (<xref ref-type="bibr" rid="bibr63-0022429411434932">Stemler, 2004</xref>), given that restricted range is common in contest evaluation (e.g., <xref ref-type="bibr" rid="bibr9-0022429411434932">Bergee &amp; Platt, 2003</xref>; <xref ref-type="bibr" rid="bibr11-0022429411434932">Boeckman, 2002</xref>; <xref ref-type="bibr" rid="bibr69-0022429411434932">Yarbrough et al., 2007</xref>). Likewise, IRA<sub>pw</sub> may not accurately measure the reliability for three adjudicators because this calculation does not account for the checks and balances created by panels of this size, as does IRA<sub>co</sub>. IRA<sub>pw</sub>, however, was useful for comparing two- versus three-member panels in this study.</p>
<p>The data hold implications for the size of adjudication panels as well. Cronbach’s alpha as examined through the Spearman-Brown prophesy formula suggested that two-member panels (α = .84) were almost as consistent as three-member panels (α = .89). IRA<sub>pw</sub>, however, indicated that two-member panels would not reach an acceptable level of reliability (.70). This was not the case in the sight-reading event, where all measures of reliability were more stable. These findings suggest that the number of judges necessary for acceptable reliability might depend on the type of event under evaluation. Organizations that currently utilize three adjudicators for sight-reading (e.g., <xref ref-type="bibr" rid="bibr65-0022429411434932">University Interscholastic League, 2010</xref>) or two judges for concert performance (e.g., <xref ref-type="bibr" rid="bibr47-0022429411434932">Music in the Parks, 2010</xref>) may reconsider this practice in light of these data.</p>
<p>Higher reliabilities for sight-reading in this study might be due to the fact that adjudicators listened to the same selections multiple times, which allowed them to become familiar with the repertoire and more effectively compare one band to another. <xref ref-type="bibr" rid="bibr39-0022429411434932">Kinney’s (2009)</xref> results supported this assertion, given that familiarity with the material under evaluation led to increased internal consistency. Therefore, including one specific required selection for each classification with which judges are familiar prior to the contest may lead to greater reliability in this type of event (<xref ref-type="bibr" rid="bibr43-0022429411434932">McPherson &amp; Thompson, 1998</xref>). Although early school band contests required a specific piece for each classification (<xref ref-type="bibr" rid="bibr45-0022429411434932">Moore, 1968</xref>), organizers eventually dropped this requirement in favor of allowing directors to choose from a number of selections on a repertoire list such as the one used by the <xref ref-type="bibr" rid="bibr60-0022429411434932">SCBDA (2009)</xref>.</p>
<p>The results of this study concurred with results of other research (e.g., <xref ref-type="bibr" rid="bibr3-0022429411434932">Baker, 2004</xref>) that found that ensembles whose directors programmed easier repertoires earned lower ratings than ensembles that performed music at higher levels of difficulty. Although the data do not provide reasons for this phenomenon, it is possible that some of these groups were from rural, urban, or private schools where students of all abilities participated in the same ensemble. Some directors of these organizations also may have had less experience than those from schools capable of the most difficult literature or simply held lower expectations for performance (e.g., <xref ref-type="bibr" rid="bibr3-0022429411434932">Baker, 2004</xref>). Organizations that sponsor contests might consider offering workshops and other support systems targeted at novice teachers (e.g., <xref ref-type="bibr" rid="bibr52-0022429411434932">Presley, n.d.</xref>) or directors working under challenging conditions in an effort to close ratings gaps and improve music education for all students.</p>
</sec>
<sec id="section18-0022429411434932">
<title>Need for Further Research</title>
<p>All organizations that sponsor adjudicated events should demonstrate interrater reliability and general consistency through similar studies. Researchers also should consider different techniques of reliability analysis that may be more comprehensive and accurate than the traditional methods used in this study and others (e.g., <xref ref-type="bibr" rid="bibr13-0022429411434932">Brakel, 2006</xref>). <xref ref-type="bibr" rid="bibr7-0022429411434932">Bergee (2007)</xref>, for example, cautioned against the limitations of classical test theory approaches and instead used generalizability theory (G theory) to establish reliability for high school solo wind performance.</p>
<p>Data on school size, director experience, and other demographics were unavailable for this study but also could serve as a basis for further research. Case studies examining successful novice directors, bands from small schools, or programs in challenging rural or urban environments could be useful for teachers in similar situations. Additional research into the relationship between director experience and effectiveness also might suggest avenues for in-service workshops and preservice instrumental methods curricula.</p>
<p>Future studies should continue to examine the number of judges needed to effectively evaluate concert performance versus sight-reading, as well as different types of vocal and instrumental ensembles (e.g., <xref ref-type="bibr" rid="bibr38-0022429411434932">King &amp; Burnsed, 2009</xref>). Other topics might include the effect of ensemble size (e.g., <xref ref-type="bibr" rid="bibr55-0022429411434932">Rickels, 2009</xref>), repertoire (e.g., <xref ref-type="bibr" rid="bibr3-0022429411434932">Baker, 2004</xref>), required selections, and criterion- versus non-criterion-based adjudication forms on reliability (e.g., <xref ref-type="bibr" rid="bibr49-0022429411434932">Norris &amp; Borst, 2007</xref>), as well as the purpose and value of contests and the attitudes of directors, administrators, students, and parents toward these events (e.g., <xref ref-type="bibr" rid="bibr62-0022429411434932">Stamer, 2006</xref>). Some directors, for example, may feel pressure to participate and succeed in contests (e.g., <xref ref-type="bibr" rid="bibr4-0022429411434932">Barnes &amp; McCashin, 2005</xref>) even though they would prefer to focus on other instructional priorities such as students’ individual musical development. Additional studies are needed to determine trends in contest procedures and participation, measure the effectiveness of adjudicator training, and develop possible methods of testing potential judges’ ability to provide reliable assessments (<xref ref-type="bibr" rid="bibr26-0022429411434932">Fiske, 1978</xref>; <xref ref-type="bibr" rid="bibr29-0022429411434932">Forbes, 1994</xref>). Future researchers also should examine the relationship between category and final ratings (e.g., <xref ref-type="bibr" rid="bibr15-0022429411434932">Burnsed et al., 1985</xref>) and the quality and content of judges’ comments usually provided as part of festival evaluations (e.g., <xref ref-type="bibr" rid="bibr23-0022429411434932">Ellis, 2007</xref>).</p>
<p>This study demonstrated the interrater reliability of high school band contests sponsored by the SCBDA as a way of examining effective methods for measuring reliability and improving adjudication in other large-group festivals. It is likely that the statistical reliability of these types of events will become more important due to the increased emphasis on assessment (e.g., <xref ref-type="bibr" rid="bibr24-0022429411434932">Fautley, 2010</xref>) and the possible role festival ratings may eventually play in teacher evaluation (e.g., <xref ref-type="bibr" rid="bibr48-0022429411434932">NAfME, 2011</xref>). Music educators should consider this information when revising festival procedures, responding to stakeholders concerned with the fairness of the adjudication process (e.g., <xref ref-type="bibr" rid="bibr21-0022429411434932">Conrad, 2003</xref>), and designing reliability analyses of other large-group contests.</p>
</sec>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<p>The author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0022429411434932">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Adler</surname><given-names>S. A.</given-names></name>
<name><surname>Clark</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <source>How it’s done: An invitation to social research</source> (<edition>3rd ed.</edition>). <publisher-loc>Belmont, CA</publisher-loc>: <publisher-name>Thompson Wadsworth</publisher-name>.</citation>
</ref>
<ref id="bibr2-0022429411434932">
<citation citation-type="journal">
<collab>American Recovery and Reinvestment Act</collab>, <comment>Section 14005-6, Title XIV, Public Law 111-5</comment> (<year>2009</year>).</citation>
</ref>
<ref id="bibr3-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Baker</surname><given-names>V.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The effect of repertoire selection on university interscholastic league choral concert ratings</article-title>. <source>Texas Music Education Research</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tmea.org/assets/pdf/research/Bak2004.pdf">http://www.tmea.org/assets/pdf/research/Bak2004.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr4-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Barnes</surname><given-names>G. V.</given-names></name>
<name><surname>McCashin</surname><given-names>R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Practices and procedures in state adjudicated orchestra festivals</article-title>. <source>Update: Applications of Research in Music Education</source>, <volume>23</volume>(<issue>2</issue>), <fpage>34</fpage>–<lpage>41</lpage>. doi:10.1177/87551233050230020105<pub-id pub-id-type="doi">10.1177/87551233050230020105</pub-id></citation>
</ref>
<ref id="bibr5-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Batey</surname><given-names>A. L.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Take the terror out of adjudication</article-title>. <source>Teaching Music</source>, <volume>10</volume>(<issue>3</issue>), <fpage>40</fpage>–<lpage>46</lpage>.</citation>
</ref>
<ref id="bibr6-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bergee</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Faculty interjudge reliability of music performance evaluation</article-title>. <source>Journal of Research in Music Education</source>, <volume>51</volume>, <fpage>137</fpage>–<lpage>150</lpage>. doi:10.2307/3345847<pub-id pub-id-type="doi">10.2307/3345847</pub-id></citation>
</ref>
<ref id="bibr7-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bergee</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Performer, rater, occasion, and sequence as sources of variability in music performance assessment</article-title>. <source>Journal of Research in Music Education, 55, 344–358</source>. doi:10.1177/0022429408317515<pub-id pub-id-type="doi">10.1177/0022429408317515</pub-id></citation>
</ref>
<ref id="bibr8-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bergee</surname><given-names>M. J.</given-names></name>
<name><surname>McWhirter</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Selected influences on solo and small-ensemble festival ratings: Replication and extension</article-title>. <source>Journal of Research in Music Education</source>, <volume>53</volume>, <fpage>177</fpage>–<lpage>190</lpage>. doi:10.1177/002242940505300207<pub-id pub-id-type="doi">10.1177/002242940505300207</pub-id></citation>
</ref>
<ref id="bibr9-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bergee</surname><given-names>M. J.</given-names></name>
<name><surname>Platt</surname><given-names>M. C.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Influence of selected variables on solo and small-ensemble festival ratings</article-title>. <source>Journal of Research in Music Education</source>, <volume>51</volume>, <fpage>342</fpage>–<lpage>353</lpage>. doi:10.2307/3345660<pub-id pub-id-type="doi">10.2307/3345660</pub-id></citation>
</ref>
<ref id="bibr10-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bergee</surname><given-names>M. J.</given-names></name>
<name><surname>Westfall</surname><given-names>C. R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Stability of a model explaining selected extramusical influences on solo and small-ensemble festival ratings</article-title>. <source>Journal of Research in Music Education</source>, <volume>53</volume>, <fpage>358</fpage>–<lpage>374</lpage>. doi:10.1177/002242940505300407<pub-id pub-id-type="doi">10.1177/002242940505300407</pub-id></citation>
</ref>
<ref id="bibr11-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boeckman</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Grade inflation in band contest ratings: A trend study</article-title>. <source>Journal of Band Research</source>, <volume>38</volume>(<issue>1</issue>), <fpage>25</fpage>–<lpage>36</lpage>.</citation>
</ref>
<ref id="bibr12-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boyle</surname><given-names>J. D.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Program evaluation for secondary school music programs</article-title>. <source>NASSP Bulletin</source>, <volume>76</volume>(<issue>544</issue>), <fpage>63</fpage>–<lpage>68</lpage>.</citation>
</ref>
<ref id="bibr13-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brakel</surname><given-names>T. D.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Inter-judge reliability of the Indiana State School Music Association high school instrumental festival</article-title>. <source>Journal of Band Research</source>, <volume>42</volume>(<issue>1</issue>), <fpage>59</fpage>–<lpage>69</lpage>.</citation>
</ref>
<ref id="bibr14-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Can we use the Spearman-Brown prophecy formula to defend low reliability?</article-title> <source>Shiken: JALT Testing and Evaluation SIG Newsletter</source>, <volume>4</volume>(<issue>3</issue>), <fpage>4</fpage>–<lpage>7</lpage>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://jalt.org/test/bro_9.htm">http://jalt.org/test/bro_9.htm</ext-link></comment></citation>
</ref>
<ref id="bibr15-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Burnsed</surname><given-names>V.</given-names></name>
<name><surname>Hinkle</surname><given-names>D.</given-names></name>
<name><surname>King</surname><given-names>S.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Performance evaluation reliability at selected concert festivals</article-title>. <source>Journal of Band Research</source>, <volume>21</volume>(<issue>1</issue>), <fpage>22</fpage>–<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr16-0022429411434932">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Carmines</surname><given-names>E. G.</given-names></name>
<name><surname>Zeller</surname><given-names>R. A.</given-names></name>
</person-group> (<year>1979</year>). <source>Reliability and validity assessment</source>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr17-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cassidy</surname><given-names>J. W.</given-names></name>
<name><surname>Sims</surname><given-names>W. L.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Effects of special education labels on peers’ and adults’ evaluations of a handicapped youth choir. Journal of Research in Music Education</article-title>, <volume>39</volume>, <fpage>23</fpage>–<lpage>34</lpage>. doi:10.2307/3344606<pub-id pub-id-type="doi">10.2307/3344606</pub-id></citation>
</ref>
<ref id="bibr18-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Cavitt</surname><given-names>M. E.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Effects of expectations on evaluators’ judgments of music performance</article-title>. <source>Texas Music Education Research</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tmea.org/assets/pdf/research/Cav1997.pdf">http://www.tmea.org/assets/pdf/research/Cav1997.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr19-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Cavitt</surname><given-names>M. E.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Differential expectation effects as factors in evaluations and feedback of musical performance</article-title>. <source>Texas Music Education Research</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tmea.org/assets/pdf/research/Cav2002.pdf">http://www.tmea.org/assets/pdf/research/Cav2002.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr20-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ciorba</surname><given-names>C. R.</given-names></name>
<name><surname>Smith</surname><given-names>N. Y.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Juries using a multidimensional assessment rubric measurement of instrumental and vocal undergraduate performance juries</article-title>. <source>Journal of Research in Music Education</source>, <volume>57</volume>, <fpage>5</fpage>–<lpage>15</lpage>. doi:10.1177/0022429409333405<pub-id pub-id-type="doi">10.1177/0022429409333405</pub-id></citation>
</ref>
<ref id="bibr21-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Conrad</surname><given-names>D.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Judging the judges: Improving rater reliability at music contests</article-title>. <source>NFHS Music Association Journal</source>, <volume>20</volume>(<issue>2</issue>), <fpage>27</fpage>-<lpage>31</lpage></citation>
</ref>
<ref id="bibr22-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Elliott</surname><given-names>C. A.</given-names></name>
</person-group> (<year>1995/1996</year>). <article-title>Race and gender as factors in judgments of musical performance</article-title>. <source>Bulletin of the Council for Research in Music Education</source>, <volume>127</volume>, <fpage>50</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr23-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ellis</surname><given-names>M. C.</given-names></name>
</person-group> (<year>2007</year>). <article-title>An analysis of taped comments from a high school jazz band festival</article-title>. <source>Contributions to Music Education</source>, <volume>34</volume>, <fpage>35</fpage>–<lpage>49</lpage>.</citation>
</ref>
<ref id="bibr24-0022429411434932">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fautley</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <source>Assessment in music education</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr25-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fiske</surname><given-names>H. E.</given-names></name>
</person-group> (<year>1975</year>). <article-title>Judge–group differences in the rating of secondary school trumpet performances</article-title>. <source>Journal of Research in Music Education</source>, <volume>23</volume>, <fpage>186</fpage>–<lpage>189</lpage>. doi:10.2307/3344643<pub-id pub-id-type="doi">10.2307/3344643</pub-id></citation>
</ref>
<ref id="bibr26-0022429411434932">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fiske</surname><given-names>H. E.</given-names></name>
</person-group> (<year>1978</year>). <source>The effect of a training procedure in music performance evaluation on judge reliability</source>. <publisher-name>Ontario Educational Research Council Report, Canada</publisher-name>.</citation>
</ref>
<ref id="bibr27-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fiske</surname><given-names>H. E.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Judging musical performance: Method or madness?</article-title> <source>Update: Applications of Research in Music Education</source>, <volume>1</volume>(<issue>3</issue>), <fpage>7</fpage>–<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr28-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Flores</surname><given-names>R. G.</given-names></name>
<name><surname>Ginsburgh</surname><given-names>V. A.</given-names></name>
</person-group> (<year>1996</year>). <article-title>The Queen Elisabeth musical competition: How fair is the final ranking?</article-title> <source>The Statistician</source>, <volume>45</volume>, <fpage>97</fpage>–<lpage>104</lpage>.</citation>
</ref>
<ref id="bibr29-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Forbes</surname><given-names>G. W.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Evaluative music festivals and contests—Are they fair?</article-title> <source>Update: Applications of Research in Music Education</source>, <volume>12</volume>(<issue>2</issue>), <fpage>16</fpage>–<lpage>20</lpage>. doi:10.1177/875512339401200203<pub-id pub-id-type="doi">10.1177/875512339401200203</pub-id></citation>
</ref>
<ref id="bibr30-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Freelon</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>ReCal: Reliability calculation for the masses [Online calculator]</article-title>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://dfreelon.org/utils/recalfront/">http://dfreelon.org/utils/recalfront/</ext-link></comment></citation>
</ref>
<ref id="bibr31-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Garman</surname><given-names>B. R.</given-names></name>
<name><surname>Boyle</surname><given-names>J. D.</given-names></name>
<name><surname>DeCarbo</surname><given-names>N. J.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Orchestra festival evaluations: Interjudge agreement and relationships between performance categories and final ratings</article-title>. <source>Research Perspectives in Music Education</source>, <volume>2</volume>, <fpage>19</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr32-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Hassel</surname><given-names>B. E.</given-names></name>
<name><surname>Hassel</surname><given-names>E. A.</given-names></name>
</person-group> (<year>2009</year>, <month>December</month>). <source>Race to the top: Accelerating college and career readiness in states</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.achieve.org/RTTT-TeacherEffectiveness">http://www.achieve.org/RTTT-TeacherEffectiveness</ext-link></comment></citation>
</ref>
<ref id="bibr33-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hunter</surname><given-names>D.</given-names></name>
<name><surname>Russ</surname><given-names>M.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Peer assessment in performance studies</article-title>. <source>British Journal of Music Education</source>, <volume>13</volume>, <fpage>67</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr34-0022429411434932">
<citation citation-type="book">
<collab>Illinois High School Association</collab>. (<year>2009</year>). <source>2009-2010 music rule book</source>. <publisher-loc>Bloomington, IL</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr35-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Killian</surname><given-names>J. N.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Characteristics of successful choirs in a contest setting</article-title>. <source>Texas Music Education Research</source>, <fpage>39</fpage>–<lpage>43</lpage>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tmea.org/assets/pdf/research/Kil1998.pdf">http://www.tmea.org/assets/pdf/research/Kil1998.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr36-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Killian</surname><given-names>J. N.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Music selection of successful choirs at UIL and non-UIL contests</article-title>. <source>Texas Music Education Research</source>, <fpage>51</fpage>–<lpage>56</lpage>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tmea.org/assets/pdf/research/Kil1999.pdf">http://www.tmea.org/assets/pdf/research/Kil1999.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr37-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Killian</surname><given-names>J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Effect of music selection on contest ratings: Year three of a continuing study</article-title>. <source>Texas Music Education Research</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.tmea.org/assets/pdf/research/Kil2000.pdf">http://www.tmea.org/assets/pdf/research/Kil2000.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr38-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>King</surname><given-names>S. E.</given-names></name>
<name><surname>Burnsed</surname><given-names>V.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A study of the reliability of adjudicator ratings at the 2005 Virginia band and orchestra directors association state marching band festivals</article-title>. <source>Journal of Band Research</source>, <volume>45</volume>(<issue>1</issue>), <fpage>27</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr39-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kinney</surname><given-names>D. W.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Internal consistency of performance evaluations as a function of music expertise and excerpt familiarity</article-title>. <source>Journal of Research in Music Education</source>, <volume>56</volume>, <fpage>322</fpage>–<lpage>337</lpage>. doi:10.1177/0022429408328934<pub-id pub-id-type="doi">10.1177/0022429408328934</pub-id></citation>
</ref>
<ref id="bibr40-0022429411434932">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Krippendorff</surname><given-names>K.</given-names></name>
</person-group> (<year>2004</year>). <source>Content analysis: An introduction to its methodology</source> (<edition>2nd ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr41-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Latimer</surname><given-names>M. E.</given-names></name>
<name><surname>Bergee</surname><given-names>M. J.</given-names></name>
<name><surname>Cohen</surname><given-names>M. L.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Performance assessment rubric reliability and perceived pedagogical utility of a weighted music performance assessment rubric</article-title>. <source>Journal of Research in Music Education</source>, <volume>58</volume>, <fpage>168</fpage>–<lpage>183</lpage>. doi:10.1177/0022429410369836<pub-id pub-id-type="doi">10.1177/0022429410369836</pub-id></citation>
</ref>
<ref id="bibr42-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>, <month>December</month> <day>10</day>). <article-title>Districts cautious with state’s “race to the top.”</article-title> <source>Illinois Statehouse News</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://illinois.statehousenewsonline.com/426/districts-cautious-with-state“race-to-the-top”">http://illinois.statehousenewsonline.com/426/districts-cautious-with-state“race-to-the-top”</ext-link></comment></citation>
</ref>
<ref id="bibr43-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McPherson</surname><given-names>G. E.</given-names></name>
<name><surname>Thompson</surname><given-names>W. F.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Assessing music performance: Issues and influences</article-title>. <source>Research Studies in Music Education</source>, <volume>10</volume>, <fpage>12</fpage>–<lpage>24</lpage>. doi:10.1177/1321103X9801000102<pub-id pub-id-type="doi">10.1177/1321103X9801000102</pub-id></citation>
</ref>
<ref id="bibr44-0022429411434932">
<citation citation-type="web">
<collab>MENC: The National Association for Music Education</collab>. (<year>n.d.</year>). <article-title>[Band adjudication form]</article-title>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.menc.org/documents/nmac/addforms/band.pdf">http://www.menc.org/documents/nmac/addforms/band.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr45-0022429411434932">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Moore</surname><given-names>J. E.</given-names></name>
</person-group> (<year>1968</year>). <source>National school band contests between 1926 and 1931</source> (<comment>Unpublished doctorial dissertation</comment>). <publisher-name>University of Michigan, Ann Arbor</publisher-name>.</citation>
</ref>
<ref id="bibr46-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Morrison</surname><given-names>S. J.</given-names></name>
<name><surname>Price</surname><given-names>H. E.</given-names></name>
<name><surname>Geiger</surname><given-names>C. G.</given-names></name>
<name><surname>Cornacchio</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The effect of conductor expressivity on ensemble performance evaluation</article-title>. <source>Journal of Research in Music Education</source>, <volume>57</volume>, <fpage>37</fpage>–<lpage>49</lpage>. doi:10.1177/0022429409332679<pub-id pub-id-type="doi">10.1177/0022429409332679</pub-id></citation>
</ref>
<ref id="bibr47-0022429411434932">
<citation citation-type="web">
<collab>Music in the Parks</collab>. (<year>2010</year>). <source>Festival performance guidelines</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.festivalsofmusic.net/ext/MIP/DisplayFile.aspx?pfil=82">http://www.festivalsofmusic.net/ext/MIP/DisplayFile.aspx?pfil=82</ext-link></comment></citation>
</ref>
<ref id="bibr48-0022429411434932">
<citation citation-type="web">
<collab>National Association for Music Education</collab>. (<year>2011</year>). <source>NAfME position statement on teacher evaluation</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.menc.org/about/view/teacher-evaluation">http://www.menc.org/about/view/teacher-evaluation</ext-link></comment>.</citation>
</ref>
<ref id="bibr49-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Norris</surname><given-names>C. E.</given-names></name>
<name><surname>Borst</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>An examination of the reliabilities of two choral festival adjudication forms</article-title>. <source>Journal of Research in Music Education</source>, <volume>55</volume>, <fpage>237</fpage>–<lpage>251</lpage>. doi:10.1177/002242940705500305<pub-id pub-id-type="doi">10.1177/002242940705500305</pub-id></citation>
</ref>
<ref id="bibr50-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Orman</surname><given-names>E. K.</given-names></name>
<name><surname>Yarbrough</surname><given-names>C.</given-names></name>
<name><surname>Neill</surname><given-names>S.</given-names></name>
<name><surname>Whitaker</surname><given-names>J. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Time usage of middle and high school band directors in sight-reading adjudication</article-title>. <source>Update: Applications of Research in Music Education</source>, <volume>25</volume>(<issue>2</issue>), <fpage>36</fpage>–<lpage>46</lpage>. doi:10.1177/8755123307250020105<pub-id pub-id-type="doi">10.1177/8755123307250020105</pub-id></citation>
</ref>
<ref id="bibr51-0022429411434932">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Phillips</surname><given-names>K. H.</given-names></name>
</person-group> (<year>2008</year>). <source>Exploring research in music education and music therapy</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr52-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Presley</surname><given-names>D.</given-names></name>
</person-group> (<year>n.d.</year>). <source>The mentor project</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.scmea.net/mentoring.htm">http://www.scmea.net/mentoring.htm</ext-link></comment></citation>
</ref>
<ref id="bibr53-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Price</surname><given-names>H. E.</given-names></name>
<name><surname>Chang</surname><given-names>E. C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Conductor and ensemble performance expressivity and state festival ratings</article-title>. <source>Journal of Research in Music Education</source>, <volume>53</volume>, <fpage>66</fpage>–<lpage>77</lpage>. doi:10.1177/002242940505300106<pub-id pub-id-type="doi">10.1177/002242940505300106</pub-id></citation>
</ref>
<ref id="bibr54-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Radocy</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1976</year>). <article-title>Effects of authority figure biases on changing judgments of musical events</article-title>. <source>Journal of Research in Music Education</source>, <volume>24</volume>, <fpage>119</fpage>–<lpage>128</lpage>. doi:10.2307/3345155<pub-id pub-id-type="doi">10.2307/3345155</pub-id></citation>
</ref>
<ref id="bibr55-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rickels</surname><given-names>D. A.</given-names></name>
</person-group> (<year>2009</year>). <source>A multivariate analysis of nonperformance variables as predictors of marching band contest results</source> (<comment>Doctoral dissertation</comment>). <comment>Retrieved from Dissertations and Theses database. (UMI No. 3353883)</comment></citation>
</ref>
<ref id="bibr56-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rohrer</surname><given-names>T. P.</given-names></name>
</person-group> (<year>2002</year>). <article-title>The debate on competition in music in the twentieth century</article-title>. <source>Update: Applications of Research in Music Education</source>, <volume>21</volume>(<issue>1</issue>), <fpage>38</fpage>–<lpage>47</lpage>. doi:10.1177/87551233020210010501<pub-id pub-id-type="doi">10.1177/87551233020210010501</pub-id></citation>
</ref>
<ref id="bibr57-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saunders</surname><given-names>T. C.</given-names></name>
<name><surname>Holahan</surname><given-names>J. M.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Criteria-specific rating scales in the evaluation of high school instrumental performance</article-title>. <source>Journal of Research in Music Education</source>, <volume>45</volume>, <fpage>259</fpage>–<lpage>272</lpage>. doi:10.2307/3345585<pub-id pub-id-type="doi">10.2307/3345585</pub-id></citation>
</ref>
<ref id="bibr58-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Silver</surname><given-names>N. C.</given-names></name>
<name><surname>Dunlap</surname><given-names>W. P.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Averaging correlation coefficients: Should Fischer’s z transformation by used?</article-title> <source>Journal of Applied Psychology</source>, <volume>72</volume>, <fpage>146</fpage>–<lpage>148</lpage>.</citation>
</ref>
<ref id="bibr59-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Silvey</surname><given-names>B. A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The effects of band labels on evaluators’ judgments of musical performance</article-title>. <source>Update: Applications of Research in Music Education</source>, <volume>28</volume>(<issue>1</issue>), <fpage>47</fpage>–<lpage>52</lpage>. doi:10.1177/8755123309344111<pub-id pub-id-type="doi">10.1177/8755123309344111</pub-id></citation>
</ref>
<ref id="bibr60-0022429411434932">
<citation citation-type="book">
<collab>South Carolina Band Directors Association</collab>. (<year>2009</year>). <source>SCBDA handbook</source>. <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr61-0022429411434932">
<citation citation-type="book">
<collab>South Carolina Department of Education</collab>. (<year>2010</year>). <source>Quick facts: Education of South Carolina</source>.<publisher-loc>Columbia, SC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr62-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Stamer</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Changes in choral student perceptions of the music contest experience</article-title>. <source>Update: Applications of Research in Music Education</source>, <volume>25</volume>(<issue>1</issue>), <fpage>46</fpage>–<lpage>55</lpage>. doi:10.1177/87551233060250010106<pub-id pub-id-type="doi">10.1177/87551233060250010106</pub-id></citation>
</ref>
<ref id="bibr63-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Stemler</surname><given-names>S. E.</given-names></name>
</person-group> (<year>2004</year>). <article-title>A comparison of consensus, consistency, and measurement approaches to estimating interrater reliability</article-title>. <source>Practical Assessment, Research and Evaluation</source>, <volume>9</volume>(<issue>4</issue>). <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://PAREonline.net/getvn.asp?v=9&amp;n=4">http://PAREonline.net/getvn.asp?v=9&amp;n=4</ext-link></comment></citation>
</ref>
<ref id="bibr64-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sullivan</surname><given-names>T. M.</given-names></name>
</person-group> (<year>2003</year>). <source>Factors influencing participation of Arizona high school marching bands in regional and state festivals</source> <comment>(Doctorial dissertation). Retrieved from Dissertations and Theses database. (UMI No. 3080892)</comment></citation>
</ref>
<ref id="bibr65-0022429411434932">
<citation citation-type="web">
<collab>University Interscholastic League</collab>. (<year>2010</year>). <source>Constitution and contest rules</source>. <publisher-loc>Austin, TX</publisher-loc>: <publisher-name>Author</publisher-name>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.uil.utexas.edu/policy/constitution/index.html">http://www.uil.utexas.edu/policy/constitution/index.html</ext-link></comment></citation>
</ref>
<ref id="bibr66-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>VanWeelden</surname><given-names>K.</given-names></name>
<name><surname>McGee</surname><given-names>I. R.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The influence of music style and conductor race on perceptions of ensemble and conductor performance</article-title>. <source>International Journal of Music Education</source>, <volume>25</volume>, <fpage>7</fpage>–<lpage>17</lpage>. doi:10.1177/0255761407074886<pub-id pub-id-type="doi">10.1177/0255761407074886</pub-id></citation>
</ref>
<ref id="bibr67-0022429411434932">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Williams</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>, <month>December</month> <day>14</day>). <article-title>Who would have guesses that the race would look like this?</article-title> [<source>Web log post</source>] <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.dfer.org/2009/12/who_would_have.php">http://www.dfer.org/2009/12/who_would_have.php</ext-link></comment></citation>
</ref>
<ref id="bibr68-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Winter</surname><given-names>N.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Music performance assessment: A study of the effects of training and experience on the criteria used by music examiners</article-title>. <source>International Journal of Music Education</source>, <volume>22</volume>, <fpage>34</fpage>–<lpage>39</lpage>.</citation>
</ref>
<ref id="bibr69-0022429411434932">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yarbrough</surname><given-names>C.</given-names></name>
<name><surname>Orman</surname><given-names>E. K.</given-names></name>
<name><surname>Neill</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Time usage by choral directors prior to sight-singing adjudication</article-title>. <source>Update: Applications of Research in Music Education</source>, <volume>25</volume>(<issue>1</issue>), <fpage>27</fpage>–<lpage>35</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>