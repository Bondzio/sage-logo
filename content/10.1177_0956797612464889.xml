<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612464889</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612464889</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Reports</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Face-Space Architectures</article-title>
<subtitle>Evidence for the Use of Independent Color-Based Features</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Nestor</surname><given-names>Adrian</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Plaut</surname><given-names>David C.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Behrmann</surname><given-names>Marlene</given-names></name>
</contrib>
<aff id="aff1-0956797612464889">Center for the Neural Basis of Cognition and Department of Psychology, Carnegie Mellon University</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0956797612464889">Adrian Nestor, Center for the Neural Basis of Cognition, Carnegie Mellon University, 4400 Fifth Ave., Pittsburgh, PA, 15213-3890 E-mail: <email>anestor@andrew.cmu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>24</volume>
<issue>7</issue>
<fpage>1294</fpage>
<lpage>1300</lpage>
<history>
<date date-type="received">
<day>21</day>
<month>5</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>9</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>The concept of psychological face space lies at the core of many theories of face recognition and representation. To date, much of the understanding of face space has been based on principal component analysis (PCA); the structure of the psychological space is thought to reflect some important aspects of a physical face space characterized by PCA applications to face images. In the present experiments, we investigated alternative accounts of face space and found that independent component analysis provided the best fit to human judgments of face similarity and identification. Thus, our results challenge an influential approach to the study of human face space and provide evidence for the role of statistically independent features in face encoding. In addition, our findings support the use of color information in the representation of facial identity, and we thus argue for the inclusion of such information in theoretical and computational constructs of face space.</p>
</abstract>
<kwd-group>
<kwd>color perception</kwd>
<kwd>face recognition</kwd>
<kwd>independent component analysis</kwd>
<kwd>principal component analysis</kwd>
<kwd>face perception</kwd>
<kwd>facial features</kwd>
<kwd>computer simulation</kwd>
<kwd>face space</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>The concept of face space (<xref ref-type="bibr" rid="bibr27-0956797612464889">Valentine, 1991</xref>) has been seminal in the study of human face recognition, providing a theoretical framework for extensive empirical work and connecting this work with the computational study of face recognition (<xref ref-type="bibr" rid="bibr22-0956797612464889">O’Toole, 2011</xref>). In essence, psychological face space refers to an internal multidimensional space in which individual faces can be represented as single points. This space enables the description of individual faces with respect to a limited number of perceptual dimensions and with respect to each other in terms of space-based distances.</p>
<p>Much research has focused on characterizing the structure and the dimensionality of a representational space, given their role in efficient encoding and recognition. By far the most popular tool in such research has been principal component analysis (PCA) because of its relative simplicity and because of its early success as a method for automatic face recognition (<xref ref-type="bibr" rid="bibr24-0956797612464889">Sirovich &amp; Meytlis, 2009</xref>; <xref ref-type="bibr" rid="bibr26-0956797612464889">Turk &amp; Pentland, 1991</xref>). Briefly, PCA achieves dimensionality reduction by projecting stimuli from their original domain (i.e., pixel-based coordinates) onto a markedly smaller space of uncorrelated dimensions or features—to be clear, “features” refer here to global whole-face image structures corresponding to the dimensions of face space (e.g., “eigenfaces,” in the case of PCA). Extensive work has shown that PCA can capture some important aspects of psychological face space, as reflected in a range of perceptual tasks, including similarity rating and identification (<xref ref-type="bibr" rid="bibr8-0956797612464889">Dailey &amp; Cottrell, 1999</xref>; <xref ref-type="bibr" rid="bibr15-0956797612464889">Hancock, Bruce, &amp; Burton, 1998</xref>; <xref ref-type="bibr" rid="bibr17-0956797612464889">Lacroix, Murre, Postma, &amp; van den Herik, 2006</xref>; <xref ref-type="bibr" rid="bibr23-0956797612464889">O’Toole, Phillips, Cheng, Ross, &amp; Wild, 2000</xref>), as well as expression recognition (<xref ref-type="bibr" rid="bibr7-0956797612464889">Calder, Burton, Miller, Young, &amp; Akamatsu, 2001</xref>; <xref ref-type="bibr" rid="bibr9-0956797612464889">Dailey, Cottrell, Padgett, &amp; Adolphs, 2002</xref>), race recognition (<xref ref-type="bibr" rid="bibr11-0956797612464889">Furl, Phillips, &amp; O’Toole, 2002</xref>), and gender recognition (<xref ref-type="bibr" rid="bibr12-0956797612464889">Graf, Wichmann, Bülthoff, &amp; Schölkopf, 2006</xref>).</p>
<p>However, PCA is but one of a larger family of methods that can be used to convert high-dimensional representations into lower-dimensional ones (<xref ref-type="bibr" rid="bibr30-0956797612464889">Zhao, Chellappa, Phillips, &amp; Rosenfeld, 2003</xref>). A comparison of alternative architectures based on these methods can be highly informative and desirable for a number of reasons. First, some of these methods promote encoding principles valuable for recognition, such as the statistical independence of the features enforced through independent component analysis (ICA) or the discriminatory power of the features for face identification enforced through linear discriminant analysis (LDA). Second, several projection methods, including ICA (<xref ref-type="bibr" rid="bibr2-0956797612464889">Bartlett, Movellan, &amp; Sejnowski, 2002</xref>) and LDA (<xref ref-type="bibr" rid="bibr3-0956797612464889">Belhumeur, Hespanha, &amp; Kriegman, 1997</xref>), are able to outperform PCA in automatic face recognition, at least in certain conditions (but see <xref ref-type="bibr" rid="bibr10-0956797612464889">Delac, Grgic, &amp; Grgic, 2005</xref>). Last, PCA applications to face perception have certain limitations, such as oversensitivity to low-level pictorial properties (<xref ref-type="bibr" rid="bibr15-0956797612464889">Hancock et al., 1998</xref>), and these limitations may be overcome by alternative approaches.</p>
<p>Of particular relevance is the comparison between PCA and ICA (for early work on this comparison in face perception, see <xref ref-type="bibr" rid="bibr14-0956797612464889">Hancock, 2002</xref>). In the study of early vision, ICA has become widely successful by accounting for basic processes, such as edge filtering and color opponency (<xref ref-type="bibr" rid="bibr4-0956797612464889">Bell &amp; Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bibr19-0956797612464889">Lee, Wachtler, &amp; Sejnowski, 2002</xref>). Assuming that low-level visual representations can be approximated by ICA encoding, it may seem surprising that higher-level visual representations, such as face representations, would revert to a less constraining encoding schema approximated by PCA—feature decorrelation implemented by PCA does not achieve the full statistical independence targeted by ICA (<xref ref-type="bibr" rid="bibr1-0956797612464889">Bartlett, 2007</xref>).</p>
<p>In the present work, we investigated the psychological plausibility of three candidate architectures of facial-identity representation based on PCA, LDA, and ICA, respectively. Identifying the method that accounts best for behavioral data can provide key insights into the architecture of face space. To this end, we used well-established behavioral paradigms to collect a comprehensive data set of similarity and identification judgments for pairs of carefully selected face images. Human data were then compared with objective face differences as quantified by PCA, LDA, and ICA. Critically, we implemented a method of feature selection that allowed us (a) to obtain a more principled estimate of the fit of each method with the behavioral data, (b) to compare the relative dimensionality of different face-space architectures, and (c) to rank features based on their explanatory power with regard to human performance.</p>
<p>Last, we considered the importance of color information for face representations by constructing color-based and luminance-based face spaces and assessing the difference between them. Because faces can display complex color patterns that are part of people’s common experience with faces as a category, it is important to determine whether and how color information contributes to representations of facial identity. Thus, we aimed to clarify fundamental properties of face space with regard to its featural makeup, its statistical structure, and its informational content.</p>
<sec id="section1-0956797612464889" sec-type="methods">
<title>Method</title>
<sec id="section2-0956797612464889">
<title>Participants</title>
<p>Forty-four Caucasian adults (23 females, 21 males; age range: 19–34 years) with normal or corrected-to-normal vision participated in the experiments in exchange for payment or course credit. Informed consent was obtained prior to participation.</p>
</sec>
<sec id="section3-0956797612464889">
<title>Stimuli</title>
<p>The stimulus set consisted of 480 color images (240 facial identities × two expressions: neutral and happy) extracted from multiple face databases (<xref ref-type="fig" rid="fig1-0956797612464889">Fig. 1</xref>). Particular care was taken in the selection and processing of the stimuli to ensure their homogeneity (see Supplemental Method and Results in the Supplemental Material available online). Both the homogeneity and the size of the stimulus set were intended to prevent face comparisons based on gross image dissimilarity or idiosyncratic differences. A quarter of the stimuli were used for behavioral testing, whereas the entire stimulus set was used for constructing candidate face spaces.</p>
<fig id="fig1-0956797612464889" position="float">
<label>Fig. 1.</label>
<caption>
<p>Examples of neutral (top row) and happy (bottom row) face stimuli used in the two experiments. The original, unaltered versions of the images shown here were selected from the Radboud Faces Database (<xref ref-type="bibr" rid="bibr18-0956797612464889">Langner et al., 2010</xref>).</p>
</caption>
<graphic xlink:href="10.1177_0956797612464889-fig1.tif"/>
</fig>
</sec>
<sec id="section4-0956797612464889">
<title>Procedure</title>
<p>On each trial, participants were presented with pairs of faces and asked either to rate their similarity on a 5-point scale (Experiment 1) or to decide whether the faces belonged to the same or to a different individual (Experiment 2). An equal number of participants (<italic>n</italic> = 22) took part in each experiment. The two tasks offer complementary advantages in that the former provides direct estimates of similarity, whereas the latter is a simpler, more natural task.</p>
<p>In Experiment 1, stimuli were presented side by side until the participant pressed a key; in Experiment 2, stimuli were presented sequentially, each for 400 ms, and then the participant made a response. In all other respects, the two experiments followed the same design. Each trial started with a 100-ms fixation cross, and each stimulus subtended approximately 3.1° × 4.6°. On any given trial, each pair of faces consisted of a neutral version of an individual and the happy expression of the same or another individual. The use of different expressions was intended to preclude equating the task to single-image matching and, thus, to minimize reliance on low-level cues uncharacteristic of everyday recognition. Each participant viewed at least one version of each pair of facial identities and completed 2,000 trials across two 1-hr sessions, each split into five experimental blocks (with short breaks between blocks). Behavioral responses (similarity ratings from Experiment 1 and accuracy from Experiment 2) were encoded into confusability matrices recording the relationship between all identity pairs.</p>
</sec>
<sec id="section5-0956797612464889">
<title>Facial-feature computation</title>
<p>All images were converted to Commission Internationale de l’Éclairage (CIE) 1976 L*a*b* color space, which best approximates the color-opponent properties of human vision (<xref ref-type="bibr" rid="bibr6-0956797612464889">Brainard, 2003</xref>). Briefly, the L* component of the space encodes lightness, whereas the a* and b* components encode red:green and yellow:blue ratios, respectively.</p>
<p>Two versions of the stimuli, one using only L* information and the other using complete L*a*b* color information, were subjected to PCA, LDA, and ICA—see Supplemental Method and Results in the Supplemental Material for more information. The three methods delivered different sets of features (<xref ref-type="fig" rid="fig2-0956797612464889">Fig. 2</xref>) associated with different candidate face spaces. In each space, a given stimulus can be represented as a vector of coefficients associated with each of these features, and the similarity between stimuli in a pair can be quantified based on these coefficients (here, we quantified similarity using a city-block metric). Such differences were computed across all pairs of stimuli.</p>
<fig id="fig2-0956797612464889" position="float">
<label>Fig. 2.</label>
<caption>
<p>Examples of color-based face features derived through principal component analysis (i.e., eigenfaces), (b) linear discriminant analysis (i.e., Fisherfaces), and (c) independent component analysis. The three color components, Commission Internationale de l’Éclairage (CIE) 1976 L*, a*, and b*, are separately presented for each feature; the components correspond to the three channels of human vision—lightness, red:green, and yellow:blue. Features are ordered on the basis of their explanatory performance (most to least from left to right) with respect to behavioral data from Experiment 1.</p>
</caption>
<graphic xlink:href="10.1177_0956797612464889-fig2.tif"/>
</fig>
</sec>
<sec id="section6-0956797612464889">
<title>Computation of fits to empirical data</title>
<p>To estimate model fits with empirical data, we separately averaged pairwise similarity ratings (Experiment 1) and accuracy-based identification scores (Experiment 2) across participants and correlated these results with model-based face distances. Because only certain subsets of features are likely to reflect perceptual features of human recognition, we implemented a recursive method for feature selection.</p>
<p>First, features were left out one by one, and the outcome of each feature’s elimination was assessed by recomputing face distances with the aid of the remaining features and by correlating these distances with the behavioral data. Second, we discarded the feature that produced the best fit among the remaining features, and third, we repeated this process until all features were discarded. Over the course of this process, we initially noticed an increase in the size of the fits (presumably because uninformative features were eliminated first) followed by a decrease (due to the elimination of informative features).</p>
<p>This method yielded a ranking of the explanatory power of the features as well as an approximation of the optimal subset of features (i.e., the one that maximized the correlation with behavioral data). These computations were performed using only half of the experimental stimuli, whereas the second half was used to compute unbiased estimates of fit based on optimal feature subsets. Cross-validation was repeated 20 times using random split halves.</p>
</sec>
</sec>
<sec id="section7-0956797612464889" sec-type="results">
<title>Results</title>
<p>Model fits to behavioral data in Experiment 1 (<xref ref-type="fig" rid="fig3-0956797612464889">Fig. 3a</xref>) showed main effects of architecture (PCA, LDA, ICA), <italic>F</italic>(2, 38) = 160.78, <italic>p</italic> &lt; .001, η<sup>2</sup> = .54, and color (L*, L*a*b*), <italic>F</italic>(1, 19) = 234.85, <italic>p</italic> &lt; .001, η<sup>2</sup> = .20, as well as an interaction between these two factors, <italic>F</italic>(2, 38) = 33.36, <italic>p</italic> &lt; .001, η<sup>2</sup> = .05. Additional pairwise comparisons showed that ICA outperformed both PCA, <italic>t</italic>(19) = 10.75, <italic>p</italic> &lt; .001, and LDA, <italic>t</italic>(19) = 21.63, <italic>p</italic> &lt; .001, whereas PCA outperformed LDA, <italic>t</italic>(19) = 6.66, <italic>p</italic> &lt; .001. Also, the use of color led to an advantage for all architectures (<italic>p</italic>s &lt; .001), although this advantage was markedly larger for PCA and LDA than for ICA.</p>
<fig id="fig3-0956797612464889" position="float">
<label>Fig. 3.</label>
<caption>
<p>Results of Experiment 1 (top row) and Experiment 2 (bottom row). The graphs in (a) and (d) show model fits based on optimal sets of features, separately for principal component analysis (PCA), linear discriminant analysis (LDA), and independent component analysis (ICA), both for models including only the L* component and for models including the L*, a*, and b* components. The graphs in (b) and (e) show fits of PCA, LDA, and ICA models that included all features, both for models including only the L* component and for models including the L*, a*, and b* components. The graphs in (c) and (f) show the average number of features required by each type of model (for optimal feature subsets) as well as the total (initial) number of features. Dotted lines mark estimates of intersubject consistency. Error bars show ±1 <italic>SE</italic>.</p>
</caption>
<graphic xlink:href="10.1177_0956797612464889-fig3.tif"/>
</fig>
<p>Model fits were next compared with estimates of the average consistency of single subjects’ similarity ratings and accuracy with the behavioral group data. These estimates were computed by correlating the data of each participant with an average based on the combined data of the other participants. This procedure was repeated for each of the 20 stimulus sets used during cross-validation. The comparison between model and participant consistency showed that ICA but not other architectures surpassed participant consistency, <italic>t</italic>(19) = 4.15, <italic>p</italic> &lt; .001; that is, the ICA model approximated the group data better than the data from actual single subjects did.</p>
<p>The advantage of ICA over other architectures and the advantage of color over lightness alone were replicated in Experiment 2 (<xref ref-type="fig" rid="fig3-0956797612464889">Fig. 3d</xref>) although, this time, both PCA and ICA architectures surpassed participant consistency (<italic>p</italic>s &lt; .001). The advantages noted for optimal subsets of features were also replicated using entire sets of features, further attesting to the robustness of our results. However, in these cases, model fits were markedly smaller (<xref ref-type="fig" rid="fig3-0956797612464889">Figs. 3b</xref> and <xref ref-type="fig" rid="fig3-0956797612464889">3e</xref>).</p>
<p>Next, a comparison of space dimensionality showed that both ICA and LDA required smaller feature sets than PCA (<xref ref-type="fig" rid="fig3-0956797612464889">Figs. 3c</xref> and <xref ref-type="fig" rid="fig3-0956797612464889">3f</xref>). The reduction in dimensionality relative to the original size of the feature set was particularly notable for ICA (about six times smaller). Finally, an examination of ICA features showed that, as expected, they were sparser than PCA or LDA features (<xref ref-type="fig" rid="fig2-0956797612464889">Fig. 2</xref>). That is, few pixels had large positive or negative values, whereas the remaining ones were close to zero as a result of ICA’s reliance on a sparse source model (<xref ref-type="bibr" rid="bibr2-0956797612464889">Bartlett et al., 2002</xref>). However, we note that ICA features were not limited to local information—for instance, many encoded symmetrical facial structures corresponding to the eyes or the nostrils. Thus, despite their sparsity, ICA features can, in principle, encode configural information.</p>
<p>For additional analyses of the behavioral data, aimed at quantifying their robustness and consistency, see Supplemental Method and Results, Figure S1, and Figure S2 in the Supplemental Material.</p>
</sec>
<sec id="section8-0956797612464889" sec-type="discussion">
<title>Discussion</title>
<p>What type of perceptual code underlies human face representations, and what general principles govern its organization? To address these issues, we compared the ability of three face-space architectures based on PCA, LDA, and ICA, respectively, to account for behavioral data. These architectures involve considerable differences in the statistical properties of face-space structure and in the featural makeup of internal representations. Our results point to the superior explanatory power of ICA-based features by showing that an ICA architecture accounts for the properties of human face space to a larger extent and with a more parsimonious feature base than its alternatives. These results are significant in two main respects.</p>
<p>First, they provide evidence for statistically independent features in face encoding and, implicitly, for the role of information maximization as implemented by ICA (<xref ref-type="bibr" rid="bibr1-0956797612464889">Bartlett, 2007</xref>)—here, the maximization of the amount of information that representations carry about objective stimulus properties. Previously, these ideas have been explored theoretically in relation to perceptual phenomena such as the other-race effect and face adaptation (<xref ref-type="bibr" rid="bibr25-0956797612464889">Tanaka, Kantner, &amp; Bartlett, 2012</xref>; <xref ref-type="bibr" rid="bibr28-0956797612464889">Webster &amp; MacLeod, 2011</xref>). Our results yielded empirical support for the validity of this theoretical framework in the study of face perception—for instance, if statistical independence can account for fine-grained individual recognition, then its extension to more general effects (e.g., the other-race effect) becomes all the more plausible.</p>
<p>Second, the present findings may help to bridge the study of high-level visual recognition and that of low-level perception. Because early visual processes such as edge filtering and color opponency (<xref ref-type="bibr" rid="bibr4-0956797612464889">Bell &amp; Sejnowski, 1997</xref>; <xref ref-type="bibr" rid="bibr19-0956797612464889">Lee et al., 2002</xref>) are relatively well explained by ICA, our results lend credence to the hypothesis that statistical independence describes a general property of visual representations rather than a domain-specific or stage-specific one.</p>
<p>A further issue examined here concerns the role of color in face perception. Whereas representations of facial identity are largely thought to discard color information (<xref ref-type="bibr" rid="bibr16-0956797612464889">Kemp, Pike, White, &amp; Musselman, 1996</xref>; <xref ref-type="bibr" rid="bibr29-0956797612464889">Yip &amp; Sinha, 2002</xref>), recent results point to the use of color in face detection (<xref ref-type="bibr" rid="bibr5-0956797612464889">Bindemann &amp; Burton, 2009</xref>) and gender categorization (<xref ref-type="bibr" rid="bibr21-0956797612464889">Nestor &amp; Tarr, 2008</xref>). The present findings suggest that the use of color extends to facial identity, as we noted a systematic color advantage for all architectures and experimental manipulations.</p>
<p>Face encoding and recognition are highly flexible processes (<xref ref-type="bibr" rid="bibr13-0956797612464889">Griffin, McOwan, &amp; Johnston, 2011</xref>; <xref ref-type="bibr" rid="bibr20-0956797612464889">Miellet, Caldara, &amp; Schyns, 2011</xref>) that require a robust representational base. The present results suggest that color-based independent features can serve this function. As an important caveat, though, we note that our data were acquired in tightly controlled experimental settings both with respect to the nature of the tasks and of the stimuli tested. Thus, future research is necessary to establish the generality of our findings by exploring a variety of tasks as well as more naturalistic stimuli exhibiting other types of variability (e.g., due to lighting or viewpoint).</p>
<p>In conclusion, our work yields support for an ICA-based account of face space and for the role of color in the representation of facial identity. These results prompt a reconsideration of fundamental aspects of the structure and informational content of face space. More generally, they provide new evidence for efficient information encoding grounded in the structure of the visual input in the domain of face perception.</p>
</sec>
</body>
<back>
<ack>
<p>We thank Gary Cottrell and the other members of the Perceptual Expertise Network for helpful comments.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research was supported by the National Science Foundation (Grant BCS0923763 to M. B. and D. C. P. and Grant SBE-0542013 to M. B. from the Temporal Dynamics of Learning Center).</p>
</fn>
<fn fn-type="supplementary-material">
<label>Supplemental Material</label>
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bartlett</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Information maximization in face processing</article-title>. <source>Neurocomputing</source>, <volume>70</volume>, <fpage>2204</fpage>–<lpage>2217</lpage>.</citation>
</ref>
<ref id="bibr2-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bartlett</surname><given-names>M. S.</given-names></name>
<name><surname>Movellan</surname><given-names>J. R.</given-names></name>
<name><surname>Sejnowski</surname><given-names>T. J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Face recognition by independent component analysis</article-title>. <source>IEEE Transactions on Neural Networks</source>, <volume>13</volume>, <fpage>1450</fpage>–<lpage>1464</lpage>.</citation>
</ref>
<ref id="bibr3-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Belhumeur</surname><given-names>P. N.</given-names></name>
<name><surname>Hespanha</surname><given-names>J. P.</given-names></name>
<name><surname>Kriegman</surname><given-names>D. J.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>19</volume>, <fpage>711</fpage>–<lpage>720</lpage>.</citation>
</ref>
<ref id="bibr4-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bell</surname><given-names>A. J.</given-names></name>
<name><surname>Sejnowski</surname><given-names>T. J.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The “independent components” of natural scenes are edge filters</article-title>. <source>Vision Research</source>, <volume>37</volume>, <fpage>3327</fpage>–<lpage>3338</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bindemann</surname><given-names>M.</given-names></name>
<name><surname>Burton</surname><given-names>A. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The role of color in human face detection</article-title>. <source>Cognitive Science</source>, <volume>33</volume>, <fpage>1144</fpage>–<lpage>1156</lpage>.</citation>
</ref>
<ref id="bibr6-0956797612464889">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brainard</surname><given-names>D. H.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Color appearance and color difference specification</article-title>. In <person-group person-group-type="editor">
<name><surname>Shevell</surname><given-names>S. K.</given-names></name>
</person-group> (Ed.), <source>The science of color</source> (pp. <fpage>191</fpage>–<lpage>216</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Optical Society of America</publisher-name>.</citation>
</ref>
<ref id="bibr7-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Calder</surname><given-names>A. J.</given-names></name>
<name><surname>Burton</surname><given-names>A. M.</given-names></name>
<name><surname>Miller</surname><given-names>P.</given-names></name>
<name><surname>Young</surname><given-names>A. W.</given-names></name>
<name><surname>Akamatsu</surname><given-names>S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>A principal component analysis of facial expressions</article-title>. <source>Vision Research</source>, <volume>41</volume>, <fpage>1179</fpage>–<lpage>1208</lpage>.</citation>
</ref>
<ref id="bibr8-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dailey</surname><given-names>M. N.</given-names></name>
<name><surname>Cottrell</surname><given-names>G. W.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Organization of face and object recognition in modular neural network models</article-title>. <source>Neural Networks</source>, <volume>12</volume>, <fpage>1053</fpage>–<lpage>1074</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dailey</surname><given-names>M. N.</given-names></name>
<name><surname>Cottrell</surname><given-names>G. W.</given-names></name>
<name><surname>Padgett</surname><given-names>C.</given-names></name>
<name><surname>Adolphs</surname><given-names>R.</given-names></name>
</person-group> (<year>2002</year>). <article-title>EMPATH: A neural network that categorizes facial expressions</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>14</volume>, <fpage>1158</fpage>–<lpage>1173</lpage>.</citation>
</ref>
<ref id="bibr10-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Delac</surname><given-names>K.</given-names></name>
<name><surname>Grgic</surname><given-names>M.</given-names></name>
<name><surname>Grgic</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Independent comparative study of PCA, ICA, and LDA on the FERET data set</article-title>. <source>International Journal of Imaging Systems and Technology</source>, <volume>15</volume>, <fpage>252</fpage>–<lpage>260</lpage>.</citation>
</ref>
<ref id="bibr11-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Furl</surname><given-names>N.</given-names></name>
<name><surname>Phillips</surname><given-names>P. J.</given-names></name>
<name><surname>O’Toole</surname><given-names>A. J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Face recognition algorithms and the other-race effect: Computational mechanisms for a developmental contact hypothesis</article-title>. <source>Cognitive Science</source>, <volume>26</volume>, <fpage>797</fpage>–<lpage>815</lpage>.</citation>
</ref>
<ref id="bibr12-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Graf</surname><given-names>A. B.</given-names></name>
<name><surname>Wichmann</surname><given-names>F. A.</given-names></name>
<name><surname>Bülthoff</surname><given-names>H. H.</given-names></name>
<name><surname>Schölkopf</surname><given-names>B.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Classification of faces in man and machine</article-title>. <source>Neural Computation</source>, <volume>18</volume>, <fpage>143</fpage>–<lpage>165</lpage>.</citation>
</ref>
<ref id="bibr13-0956797612464889">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Griffin</surname><given-names>H. J.</given-names></name>
<name><surname>McOwan</surname><given-names>P. W.</given-names></name>
<name><surname>Johnston</surname><given-names>A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Relative faces: Encoding of family resemblance relative to gender means in face space</article-title>. <source>Journal of Vision</source>, <volume>11</volume>(<issue>12</issue>), Article <fpage>8</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/11/12/8.abstract">http://www.journalofvision.org/content/11/12/8.abstract</ext-link></citation>
</ref>
<ref id="bibr14-0956797612464889">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hancock</surname><given-names>P. J. B.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Psychological correlates of face shapes</article-title>. In <person-group person-group-type="editor">
<name><surname>Bullinaria</surname><given-names>J. A.</given-names></name>
<name><surname>Lowe</surname><given-names>W.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the Seventh Neural Computation and Psychology Workshop: Connectionist models of cognition and perception</source> (pp. <fpage>145</fpage>–<lpage>155</lpage>). <publisher-loc>Singapore</publisher-loc>: <publisher-name>World Scientific</publisher-name>.</citation>
</ref>
<ref id="bibr15-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hancock</surname><given-names>P. J. B.</given-names></name>
<name><surname>Bruce</surname><given-names>V.</given-names></name>
<name><surname>Burton</surname><given-names>M. A.</given-names></name>
</person-group> (<year>1998</year>). <article-title>A comparison of two computer-based face identification systems with human perceptions of faces</article-title>. <source>Vision Research</source>, <volume>38</volume>, <fpage>2277</fpage>–<lpage>2288</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kemp</surname><given-names>R.</given-names></name>
<name><surname>Pike</surname><given-names>G.</given-names></name>
<name><surname>White</surname><given-names>P.</given-names></name>
<name><surname>Musselman</surname><given-names>A.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Perception and recognition of normal and negative faces: The role of shape from shading and pigmentation cues</article-title>. <source>Perception</source>, <volume>25</volume>, <fpage>37</fpage>–<lpage>52</lpage>.</citation>
</ref>
<ref id="bibr17-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lacroix</surname><given-names>J. P.</given-names></name>
<name><surname>Murre</surname><given-names>J. M.</given-names></name>
<name><surname>Postma</surname><given-names>E. O.</given-names></name>
<name><surname>Herik</surname><given-names>H. J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Modeling recognition memory using the similarity structure of natural input</article-title>. <source>Cognitive Science</source>, <volume>30</volume>, <fpage>121</fpage>–<lpage>145</lpage>.</citation>
</ref>
<ref id="bibr18-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Langner</surname><given-names>O.</given-names></name>
<name><surname>Dotsch</surname><given-names>R.</given-names></name>
<name><surname>Bijlstra</surname><given-names>G.</given-names></name>
<name><surname>Wigboldus</surname><given-names>D. H.</given-names></name>
<name><surname>Hawk</surname><given-names>S. T.</given-names></name>
<name><surname>van Knippenberg</surname><given-names>A.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Presentation and validation of the Radboud faces database</article-title>. <source>Cognition &amp; Emotion</source>, <volume>24</volume>, <fpage>1377</fpage>–<lpage>1388</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>T. W.</given-names></name>
<name><surname>Wachtler</surname><given-names>T.</given-names></name>
<name><surname>Sejnowski</surname><given-names>T. J.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Color opponency is an efficient representation of spectral properties in natural scenes</article-title>. <source>Vision Research</source>, <volume>42</volume>, <fpage>2095</fpage>–<lpage>2103</lpage>.</citation>
</ref>
<ref id="bibr20-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Miellet</surname><given-names>S.</given-names></name>
<name><surname>Caldara</surname><given-names>R.</given-names></name>
<name><surname>Schyns</surname><given-names>P. G.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Local Jekyll and global Hyde: The dual identity of face identification</article-title>. <source>Psychological Science</source>, <volume>22</volume>, <fpage>1518</fpage>–<lpage>1526</lpage>.</citation>
</ref>
<ref id="bibr21-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nestor</surname><given-names>A.</given-names></name>
<name><surname>Tarr</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Gender recognition of human faces using color</article-title>. <source>Psychological Science</source>, <volume>19</volume>, <fpage>1242</fpage>–<lpage>1246</lpage>.</citation>
</ref>
<ref id="bibr22-0956797612464889">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>O’Toole</surname><given-names>A. J.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Cognitive and computational approaches to face recognition</article-title>. In <person-group person-group-type="editor">
<name><surname>Calder</surname><given-names>A. J.</given-names></name>
<name><surname>Rhodes</surname><given-names>G.</given-names></name>
<name><surname>Johnson</surname><given-names>M. H.</given-names></name>
<name><surname>Haxby</surname><given-names>J. V.</given-names></name>
</person-group> (Eds.), <source>The Oxford handbook of face perception</source> (pp. <fpage>15</fpage>–<lpage>30</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr23-0956797612464889">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>O’Toole</surname><given-names>A. J.</given-names></name>
<name><surname>Phillips</surname><given-names>P. J.</given-names></name>
<name><surname>Cheng</surname><given-names>Y.</given-names></name>
<name><surname>Ross</surname><given-names>B.</given-names></name>
<name><surname>Wild</surname><given-names>H. A.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Face recognition algorithms as models of human face processing</article-title>. In <source>Proceedings of the 4th IEEE International Conference on Automatic Face and Gesture Recognition</source> (pp. <fpage>552</fpage>–<lpage>557</lpage>). <publisher-loc>Los Alamitos, CA</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr24-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sirovich</surname><given-names>L.</given-names></name>
<name><surname>Meytlis</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Symmetry, probability, and recognition in face space</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>106</volume>, <fpage>6895</fpage>–<lpage>6899</lpage>.</citation>
</ref>
<ref id="bibr25-0956797612464889">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Tanaka</surname><given-names>J. W.</given-names></name>
<name><surname>Kantner</surname><given-names>J.</given-names></name>
<name><surname>Bartlett</surname><given-names>M.</given-names></name>
</person-group> (<year>2012</year>). <article-title>How category structure influences the perception of object similarity: The atypicality bias</article-title>. <source>Frontiers in Psychology</source>, <volume>3</volume>, <fpage>147</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3368386/">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3368386/</ext-link></citation>
</ref>
<ref id="bibr26-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turk</surname><given-names>M.</given-names></name>
<name><surname>Pentland</surname><given-names>A.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Eigenfaces for recognition</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>3</volume>, <fpage>71</fpage>–<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr27-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Valentine</surname><given-names>T.</given-names></name>
</person-group> (<year>1991</year>). <article-title>A unified account of the effects of distinctiveness, inversion, and race in face recognition</article-title>. <source>The Quarterly Journal of Experimental Psychology A: Human Experimental Psychology</source>, <volume>43</volume>, <fpage>161</fpage>–<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr28-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Webster</surname><given-names>M. A.</given-names></name>
<name><surname>MacLeod</surname><given-names>D. I.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Visual adaptation and face perception</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>366</volume>, <fpage>1702</fpage>–<lpage>1725</lpage>.</citation>
</ref>
<ref id="bibr29-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yip</surname><given-names>A. W.</given-names></name>
<name><surname>Sinha</surname><given-names>P.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Contribution of color to face recognition</article-title>. <source>Perception</source>, <volume>31</volume>, <fpage>995</fpage>–<lpage>1003</lpage>.</citation>
</ref>
<ref id="bibr30-0956797612464889">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhao</surname><given-names>W.</given-names></name>
<name><surname>Chellappa</surname><given-names>R.</given-names></name>
<name><surname>Phillips</surname><given-names>P. J.</given-names></name>
<name><surname>Rosenfeld</surname><given-names>A.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Face recognition: A literature survey</article-title>. <source>ACM Computing Surveys</source>, <volume>35</volume>, <fpage>399</fpage>–<lpage>458</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>