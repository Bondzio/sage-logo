<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612458528</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612458528</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Attentional Capture Does Not Depend on Feature Similarity, but on Target-Nontarget Relations</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Becker</surname><given-names>Stefanie I.</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612458528">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Folk</surname><given-names>Charles L.</given-names></name>
<xref ref-type="aff" rid="aff2-0956797612458528">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Remington</surname><given-names>Roger W.</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612458528">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0956797612458528"><label>1</label>The University of Queensland</aff>
<aff id="aff2-0956797612458528"><label>2</label>Villanova University</aff>
<author-notes>
<corresp id="corresp1-0956797612458528">Stefanie I. Becker, School of Psychology, McElwain Building, St. Lucia, Queensland 4072, Australia E-mail: <email>s.becker@psy.uq.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>24</volume>
<issue>5</issue>
<fpage>634</fpage>
<lpage>647</lpage>
<history>
<date date-type="received">
<day>14</day>
<month>12</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>7</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>What factors determine which stimuli of a scene will be visually selected and become available for conscious perception? The currently prevalent view is that attention operates on specific feature values, so attention will be drawn to stimuli that have features similar to those of the sought-after target. Here, we show that, instead, attentional capture depends on whether a distractor’s feature relationships match the target-nontarget relations (e.g., redder). In three spatial-cuing experiments, we found that (a) a cue with the target color (e.g., orange) can fail to capture attention when the cue–cue-context relations do not match the target-nontarget relations (e.g., redder target vs. yellower cue), whereas (b) a cue with the nontarget color can capture attention when its relations match the target-nontarget relations (e.g., both are redder). These results support a relational account in which attention is biased toward feature relationships instead of particular feature values, and show that attentional capture by an irrelevant distractor does not depend on feature similarity, but rather depends on whether the distractor matches or mismatches the target’s relative attributes (e.g., relative color).</p>
</abstract>
<kwd-group>
<kwd>attention</kwd>
<kwd>cognitive processes</kwd>
<kwd>color perception</kwd>
<kwd>visual attention</kwd>
<kwd>visual search</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Visual attention comprises a set of mechanisms that modulate sensory and cognitive processing to select the most behaviorally relevant stimuli for further limited-capacity processing. Which of the many stimuli impinging on the retina will be selected is determined both by task goals (top-down factors; e.g., <xref ref-type="bibr" rid="bibr18-0956797612458528">Folk, Remington, &amp; Johnston, 1992</xref>, <xref ref-type="bibr" rid="bibr19-0956797612458528">1993</xref>) and by ongoing stimulus processing (bottom-up factors; e.g., <xref ref-type="bibr" rid="bibr22-0956797612458528">Itti &amp; Koch, 2000</xref>; <xref ref-type="bibr" rid="bibr23-0956797612458528">Jonides, 1981</xref>; <xref ref-type="bibr" rid="bibr26-0956797612458528">Li, 2002</xref>). How top-down and bottom-up mechanisms interact in selection is still intensely debated (e.g., <xref ref-type="bibr" rid="bibr7-0956797612458528">Beck &amp; Kastner, 2009</xref>; <xref ref-type="bibr" rid="bibr32-0956797612458528">Navalpakkam &amp; Itti, 2006</xref>).</p>
<p>A widely held view is that salient items with a high feature contrast can automatically attract attention, independently of top-down goals (e.g., <xref ref-type="bibr" rid="bibr6-0956797612458528">Beck &amp; Kastner, 2005</xref>; <xref ref-type="bibr" rid="bibr22-0956797612458528">Itti &amp; Koch, 2000</xref>; <xref ref-type="bibr" rid="bibr33-0956797612458528">Navalpakkam &amp; Itti, 2007</xref>; <xref ref-type="bibr" rid="bibr35-0956797612458528">Reynolds &amp; Desimone, 2003</xref>; <xref ref-type="bibr" rid="bibr41-0956797612458528">Theeuwes, 2010</xref>; <xref ref-type="bibr" rid="bibr48-0956797612458528">Zhang, Zhaoping, Zhou, &amp; Fang, 2012</xref>), whereas top-down knowledge about the features of an object (e.g., its color) can modulate attention to ignore task-irrelevant salient items and predominantly select task-relevant items. Top-down tuning of attention aids searching in situations in which the attributes of a sought-after object are known, but its exact location is not, such as a search for a book on a shelf or a taxi on a busy street (cf. <xref ref-type="bibr" rid="bibr13-0956797612458528">Carrasco, 2011</xref>). In these cases, knowledge about the features of the search target (e.g., that the book or taxi is yellow) will direct attention to locations in the visual field that contain the feature (e.g., <xref ref-type="bibr" rid="bibr21-0956797612458528">Hwang, Higgins, &amp; Pomplun, 2009</xref>). Allocating attention to a particular location will enhance processing of the object at that location, at the expense of processing of items at other locations (e.g., <xref ref-type="bibr" rid="bibr34-0956797612458528">Posner, 1980</xref>; for reviews, see <xref ref-type="bibr" rid="bibr13-0956797612458528">Carrasco, 2011</xref>, and <xref ref-type="bibr" rid="bibr7-0956797612458528">Beck &amp; Kastner, 2009</xref>).</p>
<p>To study the factors determining selection, for example, in a visual search task, it is necessary to add an irrelevant item (a distractor) to the search display and to test whether and to what extent the distractor can attract attention (attentional capture) while observers are searching for a predefined target among other irrelevant nontargets. Distinguishing top-down factors from bottom-up factors in attentional capture can be difficult, as adding a distractor to the search display can alter both the bottom-up feature contrasts of the target and the distractor, and the top-down settings themselves (e.g., because observers actively suppress the distractor feature; e.g., <xref ref-type="bibr" rid="bibr8-0956797612458528">Becker, 2007</xref>). However, studies that systematically varied the features of the irrelevant distractor have uniformly shown that target-similar distractors can attract attention more strongly than target-dissimilar distractors (e.g., <xref ref-type="bibr" rid="bibr1-0956797612458528">Anderson &amp; Folk, 2010</xref>; <xref ref-type="bibr" rid="bibr3-0956797612458528">Ansorge &amp; Heumann, 2003</xref>; <xref ref-type="bibr" rid="bibr16-0956797612458528">Eimer, Kiss, Press, &amp; Sauter, 2009</xref>; <xref ref-type="bibr" rid="bibr17-0956797612458528">Folk &amp; Remington, 1998</xref>; <xref ref-type="bibr" rid="bibr28-0956797612458528">Ludwig &amp; Gilchrist, 2002</xref>). For example, in search for a red target, a red distractor will capture attention more strongly than a green distractor, and in search for a green target, a green distractor will attract attention more strongly than a red distractor (e.g., <xref ref-type="bibr" rid="bibr17-0956797612458528">Folk &amp; Remington, 1998</xref>). This <italic>similarity effect</italic> demonstrates that attentional capture by an irrelevant distractor critically depends on the top-down attentional control settings (e.g., <xref ref-type="bibr" rid="bibr18-0956797612458528">Folk et al., 1992</xref>).</p>
<p>Several different mechanisms have been proposed to account for top-down contingent attentional capture. However, most theories assume that top-down mechanisms modulate selection by activating or inhibiting specific feature maps (e.g., red, green) or by modulating the output of neurons that respond to specific feature values (e.g., <xref ref-type="bibr" rid="bibr15-0956797612458528">Duncan &amp; Humphreys, 1989</xref>; <xref ref-type="bibr" rid="bibr17-0956797612458528">Folk &amp; Remington, 1998</xref>; <xref ref-type="bibr" rid="bibr18-0956797612458528">Folk et al., 1992</xref>; <xref ref-type="bibr" rid="bibr24-0956797612458528">Koch &amp; Ullman, 1985</xref>; <xref ref-type="bibr" rid="bibr30-0956797612458528">Maunsell &amp; Treue, 2006</xref>; <xref ref-type="bibr" rid="bibr33-0956797612458528">Navalpakkam &amp; Itti, 2007</xref>; <xref ref-type="bibr" rid="bibr42-0956797612458528">Treisman &amp; Gelade, 1980</xref>; <xref ref-type="bibr" rid="bibr43-0956797612458528">Treisman &amp; Sato, 1990</xref>; <xref ref-type="bibr" rid="bibr44-0956797612458528">Wolfe, 1994</xref>). For example, according to the <italic>feature-similarity view</italic>, attention will be tuned to the feature values of a sought-after target, increasing the response gain of features similar to the target features (e.g., <xref ref-type="bibr" rid="bibr29-0956797612458528">Martinez-Trujillo &amp; Treue, 2004</xref>; see also <xref ref-type="bibr" rid="bibr17-0956797612458528">Folk &amp; Remington, 1998</xref>). Alternatively, it has been proposed that observers can bias attention against the features of the irrelevant nontargets (<italic>nontarget- inhibition view</italic>).</p>
<p>The <italic>attentional-engagement theory</italic> (<xref ref-type="bibr" rid="bibr15-0956797612458528">Duncan &amp; Humphreys, 1989</xref>) holds that both the target and the nontarget features are important for top-down tuning, and correspondingly predicts that attention is most strongly attracted to items that are similar to the target and, at the same time, dissimilar to the nontargets. Yet another theory that has gained popularity is the <italic>optimal-tuning account</italic>, which holds that attention will be tuned to the feature value that optimally distinguishes the target from the nontargets. Especially when the target and nontargets are very similar, it can be more optimal to tune attention toward a more extreme feature value that is shifted away from the nontargets, as this will reduce the overlap between the target and nontarget feature-value distributions and increase the signal-to-noise ratio (e.g., <xref ref-type="bibr" rid="bibr25-0956797612458528">Lee, Itti, Koch, &amp; Braun, 1999</xref>; <xref ref-type="bibr" rid="bibr33-0956797612458528">Navalpakkam &amp; Itti, 2007</xref>; <xref ref-type="bibr" rid="bibr36-0956797612458528">Scolari &amp; Serences, 2009</xref>, <xref ref-type="bibr" rid="bibr37-0956797612458528">2010</xref>).</p>
<p>Although accounts of top-down selection differ with respect to the feature value that will be prioritized in a given instance, they concur in that once attention is set to a particular feature value, the item that is most similar to this feature value should attract attention most strongly.</p>
<p>By contrast, we have recently proposed that top-down selection can be based on relational or contextual information that specifies how the target differs from irrelevant items (e.g., <xref ref-type="bibr" rid="bibr9-0956797612458528">Becker, 2010a</xref>; <xref ref-type="bibr" rid="bibr11-0956797612458528">Becker, Folk, &amp; Remington, 2010</xref>). According to this <italic>relational account</italic>, the visual system can directly evaluate the relationship between the target feature and the feature (or features) of the irrelevant nontarget items (e.g., the target is represented as redder, larger, or darker) and direct attention toward items that have the same feature relationships. For instance, in search for an orange target, attention could be set for all yellower items when the background contains mostly red items, or set for all redder items when the background consists mostly of yellow items. If attention is set for “redder,” the reddest item in the visual field will have the highest attention-driving capacity; the next-reddest item will have the second-highest attention- driving capacity, and so forth. Thus, whether an orange item will be selected as the first item critically depends on whether it is the reddest item (in which case it will be selected first) or whether the context contains a redder-than-orange item (in which case that item will be selected first).</p>
<p>In the relational account, the context plays an important dual role for how attention is allocated: First, the context in which the target most frequently appears determines the expectations of the observers, and with this, the direction in which attention will be tuned (as attention will be biased to the relationship that can most reliably single out the search target). Second, the items present in the actual context (e.g., of a trial) determine whether a given feature captures attention (for a more formal description, see the relational-vector account in <xref ref-type="bibr" rid="bibr9-0956797612458528">Becker, 2010a</xref>).</p>
<p>A central prediction of the relational account is that an item does not need to be similar to the target-defining feature to be able to capture attention. Even target- dissimilar items can capture attention, provided that they differ in the same way from the context as the target differs from the nontarget context. An earlier study (<xref ref-type="bibr" rid="bibr11-0956797612458528">Becker et al., 2010</xref>) has already provided some support for this prediction, by demonstrating that target-dissimilar distractors can capture attention more strongly than target-similar distractors: In search for an orange target among yellow-orange (gold) nontargets, a red distractor captured attention more than an orange distractor, a result consistent with a relational set for “redder.” Similarly, search for a gold target among orange nontargets led to more attentional capture by yellow distractors than by gold ones, a result consistent with a top-down set for “yellower.”</p>
<p>These and other previous findings, however, do not provide strong evidence for the relational account, as they are still consistent with some feature-based theories. For instance, in the case of search for an orange target among gold nontargets, the attentional-engagement theory could explain more attentional capture by a red cue than by an orange cue as due to the fact that the red cue was more dissimilar from the gold nontargets than the orange cue (whereas the red cue was still sufficiently similar to orange to be included in the top-down set for orange; e.g., <xref ref-type="bibr" rid="bibr15-0956797612458528">Duncan &amp; Humphreys, 1989</xref>). Similarly, theories explaining attentional capture by an interplay of bottom-up and top-down factors could account for this result by the claim that red had a larger bottom-up feature contrast than orange and therefore captured attention more strongly than orange did (e.g., <xref ref-type="bibr" rid="bibr44-0956797612458528">Wolfe, 1994</xref>). Optimal-tuning accounts can also account for stronger attentional capture by red than orange in this search by proposing that attention is set for a color between orange and full red (e.g., <xref ref-type="bibr" rid="bibr9-0956797612458528">Becker, 2010a</xref>).</p>
<p>The case for the relational account so far rests mainly on its theoretical merits, as it is able to explain a larger set of results in a more parsimonious manner than most feature-based accounts (see <xref ref-type="bibr" rid="bibr9-0956797612458528">Becker, 2010a</xref>). However, there is currently no decisive evidence favoring the relational view over a feature-specific account of top-down control. Hence, it is still an open question whether attention can indeed be genuinely tuned to feature relationships.</p>
<p>In the present study, we conducted a more decisive test of the relational account, for instance, by testing attentional capture by distractors that had the same color as the nontargets and yet had the same relative color as the target (e.g., redder). To vary the feature values (colors) and the feature relationships (relative colors) of the target and the distractor independently of one another, we used a variant of the spatial-cuing paradigm, in which the distractor (called the <italic>cue</italic>) is presented in a separate cue display before the target display is presented. Observers had to search for a singleton target of a unique, predefined color (e.g., orange) that was presented among three nontargets of a different color (e.g., yellow-orange, which would yield the target-nontarget relation of “redder”). Prior to the target display, we presented a four-dot singleton cue of a unique color that was surrounded by three other cues of a different color (<italic>cue context</italic>). The relation between the singleton cue and the cue context could either match the target-nontarget relation (e.g., singleton cue redder than cue context) or mismatch the target-nontarget relation (e.g., singleton cue yellower than cue context), and the cue could have either the same color as or a different color than the target (e.g., orange or yellow-orange).</p>
<p>In all experiments, the target color, nontarget color, and target-nontarget relationship remained constant across a block of trials, so that it was equally possible to locate the target by tuning attention to the target feature value (i.e., target color) or to the target-nontarget relationship (i.e., relative color of the target). Given that the singleton cue will attract attention to the extent that it matches the top-down control settings (if bottom-up factors are controlled), top-down tuning to the target’s color could be distinguished from tuning to the target’s relative color by comparing attentional capture by singleton cues that had the same color as the target but a different relative color with capture by singleton cues that had a different color than the target but the same relative color.</p>
<p>In the spatial-cuing paradigm, attentional capture by the irrelevant singleton is inferred if response times (RTs) to the target are shorter on valid trials, when the singleton cue is at the same location as the target, than on invalid trials, when the singleton cue is presented at a different location and attention has to be redirected to the target in a time-consuming process (e.g., <xref ref-type="bibr" rid="bibr18-0956797612458528">Folk et al., 1992</xref>; <xref ref-type="bibr" rid="bibr34-0956797612458528">Posner, 1980</xref>). In the present experiments, observers had to respond to a different feature of the target (<italic>T</italic>/<italic>L</italic> letter discrimination task) than the search-related feature (color), to ensure that shorter RTs on valid trials could be attributed to early, attentional processes and were not due to facilitation in later, decisional or response-related processes (e.g., <xref ref-type="bibr" rid="bibr45-0956797612458528">Yantis, 1993</xref>, <xref ref-type="bibr" rid="bibr46-0956797612458528">2000</xref>). Moreover, the singleton cue was presented at all locations equally frequently and was therefore uncorrelated with the target location, which ensured that participants did not have an incentive to actively attend to singleton cues (e.g., <xref ref-type="bibr" rid="bibr47-0956797612458528">Yantis &amp; Egeth, 1999</xref>).</p>
<sec id="section1-0956797612458528">
<title>Experiment 1</title>
<p>In Experiment 1, stimuli had one of four different colors: red, orange, gold, or yellow. The colors were chosen such that orange, gold, and yellow were successively more dissimilar from red (see <xref ref-type="fig" rid="fig1-0956797612458528">Fig. 1b</xref> for a description of the colors). In two separate blocks of Experiment 1, observers searched for an orange target letter among three gold nontarget letters (target-nontarget relationship = redder; <xref ref-type="fig" rid="fig1-0956797612458528">Fig. 1a</xref>) or for a gold target letter among three orange nontarget letters (target-nontarget relationship = yellower). Prior to the target display (100 ms), we briefly presented a cue display (100 ms) consisting of a four-dot singleton cue and three four-dot cues of a different color that surrounded the other three locations (cue context). The singleton cue either had the same relation to the three other cues in the cue display as the target had to the nontargets (same-relation cue; e.g., singleton cue redder than the cue context and target redder than nontargets) or differed in the opposite direction from the other objects in the cue display (different-relation cue; e.g., singleton cue yellower than the cue context and target redder than nontargets).</p>
<fig id="fig1-0956797612458528" position="float">
<label>Fig. 1.</label>
<caption>
<p>Trial sequence and stimuli in Experiment 1. A fixation display was presented for 500 to 2,000 ms (a) and followed by a 100-ms cue display, which consisted of a four-dot color singleton (cue) plus three four-dot stimuli of a different color (cue context). The target display (containing the letters <italic>T</italic> and <italic>L</italic>) was presented following another 100-ms fixation display. In different blocks, the target letter was either orange among gold nontargets (target redder than nontargets) or gold among orange nontargets (target yellower than nontargets). The task was to report the identity of the odd-colored letter (<italic>T</italic> or <italic>L</italic>) in the target display, which was validly precued by the singleton cue on 25% of the trials. Cue and cue-context colors were combined to create four kinds of cue displays: red cue with orange context, gold cue with yellow context, orange cue with red context, and yellow cue with gold context. The graph in (b) shows the RGB (red, green, blue) values of the colors used (values in parentheses) and their positions in CIE (Commission International de l’Éclairage) 1976 color space (<italic>u′</italic>- and <italic>v′</italic>-coordinates).</p>
</caption>
<graphic xlink:href="10.1177_0956797612458528-fig1.tif"/>
</fig>
<p>Capture was assessed for four cue displays: (a) a red cue in an orange context, (b) a gold cue in a yellow context, (c) an orange cue in a red context, and (d) a yellow cue in a gold context (see <xref ref-type="fig" rid="fig1-0956797612458528">Fig. 1a</xref>). The red and gold singleton cues were both redder than their respective cue contexts, whereas the orange and yellow singleton cues were yellower than their cue contexts.</p>
<p>According to the relational account, a singleton cue should capture attention only when its relation to the cue context matches the target-nontarget relation. Hence, in search for the orange (redder) target, only the red and gold (i.e., redder) singleton cues should capture attention, whereas in search for the gold (yellower) target, the orange and yellow (i.e., yellower) singleton cues should capture attention.</p>
<p>According to feature-based accounts, search for an orange target among gold nontargets could result in a top-down bias for orange (e.g., feature-similarity accounts) or a top-down bias against gold (nontarget- inhibition account), or attention could be tuned to a feature value between orange and full red (e.g., optimal-tuning account). If attention is biased to the target feature value or against the nontarget feature value, the predicted results would be directly opposite to the predictions of the relational account. In search for the orange (redder) target, the redder singleton cues should both fail to capture attention: the red singleton cue because it is presented in a cue context that matches the target color (orange), and the gold singleton cue because it matches the (inhibited) nontarget color. Optimal-tuning accounts could accommodate attentional capture by the red singleton cue because attention could be biased more toward red than toward the target color (orange). However, attention could not be simultaneously biased toward red and the nontarget color (gold) because a selection criterion that favors both the (shifted) target color and the nontarget color would compromise target-nontarget discrimination. Hence, if the red singleton cue captures attention, the gold singleton cue that matches the nontarget color should fail to capture attention.</p>
<sec id="section2-0956797612458528">
<title>Method</title>
<p>Twelve participants from the University of Queensland, Australia, participated in the study for a payment of AU$10. One participant was excluded because of a high mean error rate of 30%.</p>
<p>Stimuli were generated with Presentation software (Neurobehavioral Systems, Albany, CA) and presented on a 17-in. CRT monitor with a resolution of 1,024 × 768 pixels (85 Hz). The letters (<italic>T</italic> and <italic>L</italic>; 1.4° × 1.4°) and four placeholder boxes (2° × 2°) were placed 6° from the center of the display, and the cues consisted of four dots (diameter: 0.4°) that surrounded each placeholder in a diamond configuration. In two of the four cue displays, the different-colored cue was redder than the other cues (red singleton cue among orange cues, gold singleton cue among yellow cues), and in the other two displays, the cue was yellower than the other cues (orange singleton cue among red cues, yellow singleton cue among gold cues; see <xref ref-type="fig" rid="fig1-0956797612458528">Fig. 1a</xref>).</p>
<p>Two different target conditions were presented in separate blocks (orange target among gold nontargets or vice versa), and the order of blocks was counterbalanced across participants. Participants were instructed to search for the odd-colored letter in the target display and to report whether it was a <italic>T</italic> or <italic>L</italic> by pressing the right or left mouse button, respectively. Target position and letter identity were controlled such that each target letter (<italic>T, L</italic>) appeared at all possible target locations equally often. Prior to the target display (100 ms), we presented a cue display (100 ms), followed by a fixation display (100 ms). The location of the singleton cue coincided with the target location at chance level (25% of trials were valid), and each cue display was presented 128 times (32 valid trials; 1,024 trials per subject).</p>
<p>To ensure that results reflected covert attention shifts and not overt gaze shifts, we instructed observers to maintain fixation on the central fixation cross during a trial, and fixation was monitored with an eye tracker (Eyelink 1000, SR Research, Ontario, Canada). In this and subsequent experiments, trials in which an eye movement occurred were excluded from all analyses (1.5% in Experiment 1), and prior to data analysis, RTs were trimmed for outliers (RT &lt; 200 ms or &gt; 1,500 ms; 0.9% in Experiment 1); trials with manual errors were excluded from all RT analyses. Mean accuracy was 85%, and there were no speed-accuracy trade-offs (see the Supplemental Material available online for a detailed report of the error scores).</p>
</sec>
<sec id="section3-0956797612458528">
<title>Results</title>
<p>The results for Experiment 1 are depicted in <xref ref-type="fig" rid="fig2-0956797612458528">Figure 2</xref>. A 2 (target: orange vs. gold target) × 4 (cue color: red vs. gold vs. orange vs. yellow singleton cue) × 2 (validity: valid vs. invalid singleton cue) within-subjects analysis of variance (ANOVA) was computed over the mean RT. The results showed a significant main effect of validity, <italic>F</italic>(1, 10) = 5.5, <italic>p</italic> = .041, η<sup>2</sup> = .35, and the predicted three-way interaction among all variables, <italic>F</italic>(3, 30) = 14.7, <italic>p</italic> &lt; .001, η<sup>2</sup> = .59 (all other <italic>p</italic>s &gt; .062). Follow-up two-tailed <italic>t</italic> tests showed that all same-relation cues captured attention, as reflected in significantly shorter RTs on valid trials than on invalid trials. Specifically, in search for the redder target (orange), the redder singleton cues (red and gold) captured attention, whereas in search for the yellower target (gold), the yellower singleton cues (orange and yellow) captured attention, all <italic>t</italic>s(10) &gt; 2.6, all <italic>p</italic>s &lt; .05. By contrast, the different-relation cues all failed to capture attention: Specifically, in search for an orange target, the orange and yellow singleton cues failed to capture attention, and in search for a gold target, the red and gold singleton cues failed to capture attention.</p>
<fig id="fig2-0956797612458528" position="float">
<label>Fig. 2.</label>
<caption>
<p>Results from Experiment 1. Mean response time (RT) and percentage of errors are plotted as a function of cue validity and the cue and cue-context colors. The graphs in the top row show results for the orange target presented among gold nontargets, and the graphs in the bottom row show results for the gold target presented among orange nontargets. The graphs on the left show results for same-relation cues, which had the same relation to the cue context as the target had to the nontargets; the graphs on the right show results for different-relation cues, which had the opposite relation to the cue context as the target had to the nontargets. Error bars depict the 95% confidence intervals for the respective within-subjects contrasts (<xref ref-type="bibr" rid="bibr27-0956797612458528">Loftus &amp; Masson, 1994</xref>).</p>
</caption>
<graphic xlink:href="10.1177_0956797612458528-fig2.tif"/>
</fig>
<p>These results indicate that attentional capture was independent of the feature value of the singleton cue and depended only on whether its relationship to the context matched or mismatched the target-nontarget relationship. The findings cannot be attributed to differences in bottom-up saliency (e.g., <xref ref-type="bibr" rid="bibr22-0956797612458528">Itti &amp; Koch, 2000</xref>; <xref ref-type="bibr" rid="bibr26-0956797612458528">Li, 2002</xref>; <xref ref-type="bibr" rid="bibr39-0956797612458528">Theeuwes, 1992</xref>, <xref ref-type="bibr" rid="bibr40-0956797612458528">1994</xref>), as the same singleton cues that captured attention in search for the orange (redder) target failed to capture attention in search for the gold (yellower) target, and vice versa. The fact that capture depended on the target-nontarget relation and not on the properties of the singleton cues themselves indicates that the observed effects cannot have been due to bottom-up effects of the cues but must have been due to differences in top-down tuning (e.g., <xref ref-type="bibr" rid="bibr17-0956797612458528">Folk &amp; Remington, 1998</xref>; <xref ref-type="bibr" rid="bibr18-0956797612458528">Folk et al., 1992</xref>).</p>
<p>Note that a same-relation singleton cue captured attention even when it had the same color as the nontargets (e.g., gold, in search for orange), and that the different-relation singleton cue failed to capture attention even when it had the same color as the target. These results indicate that the propensity to capture attention is not proportional to similarity to the target (or dissimilarity from the nontargets; e.g., <xref ref-type="bibr" rid="bibr15-0956797612458528">Duncan &amp; Humphreys, 1989</xref>). Contrary to the feature-based views, the target is apparently not selected by activating the target feature value, inhibiting the nontarget feature value, or tuning attention to a more optimal feature value shifted away from the nontargets.</p>
<p>However, it may be possible to explain the results of Experiment 1 with a combined optimal-tuning/optimal-suppression account.<sup><xref ref-type="fn" rid="fn1-0956797612458528">1</xref></sup> For instance, if we assume that attention is biased toward an exaggerated target feature value and against an exaggerated nontarget feature value (i.e., a feature value shifted away from the nontarget), then only the most extreme feature values in Experiment 1 would have been subject to activation and suppression (i.e., red and yellow). Attentional capture by a singleton cue with the nontarget color (e.g., gold) could then be explained by optimal suppression: Inhibition of the exaggerated nontarget color would have suppressed the context cues that had the exaggerated nontarget color (yellow or red) but not the singleton cue itself (gold or orange). Hence, the singleton cue with the nontarget color may have captured attention because it was not suppressed and was presented among other, inhibited cues.</p>
<p>The data of Experiment 1 did not show any evidence for active suppression of yellow or red (e.g., no inverse validity effects or elevated baseline RTs for yellow or red singleton cues). We conducted Experiments 2 and 3 to provide a more comprehensive test, by testing conditions in which a combined optimal-tuning/optimal-suppression account would make the same predictions as the simple optimal-tuning account and a feature-similarity account.</p>
</sec>
</sec>
<sec id="section4-0956797612458528">
<title>Experiment 2</title>
<p>Experiment 2 examined whether the target-nontarget relationship also determines top-down selection in a feature search task when the target does not have an extreme feature value relative to the nontarget context (e.g., reddest or yellowest item), but is a color intermediate between more extreme colors. In Experiment 2, the target was always orange and presented among two yellow and two red nontargets, so that the target had a color intermediate between the colors of the nontargets. From a relational point of view, the target differed in two opposing directions from the nontargets, as it was yellower than half of the nontargets and redder than the other half. To select the orange target by virtue of its target-nontarget relationships, it would be necessary to combine information from these two directions to pick out the color intermediate between redder and yellower items.</p>
<p>It is currently unknown whether search for intermediate targets can be based on target-nontarget relationships. However, if it is possible to apply relational tuning to the stimuli in this experiment, then singleton cues should capture attention only when they (a) have a color intermediate between red and yellow (e.g., orange or gold) and (b) are bounded by two redder cues and two yellower cues (e.g., <xref ref-type="bibr" rid="bibr9-0956797612458528">Becker, 2010a</xref>). However, cues with an intermediate color should fail to capture attention when the cue context renders them the reddest or yellowest item in the cue display.</p>
<p>To test these predictions, we varied the colors of the singleton cue and the cue context such that the singleton cue either matched or mismatched the target feature value (orange vs. gold singleton cue, respectively) and either matched or mismatched the target’s relations to the nontarget (intermediate color vs. reddest or yellowest color, respectively). In the same-relation condition, the colors of the cue context were chosen such that the singleton cue had the same relation to the cue context as the target had to the nontargets (i.e., intermediate color). In the different-relation condition, the context colors rendered the singleton cue the reddest or yellowest item in the display, so that the relation of the singleton cue did not match the target-nontarget relations.</p>
<p>The critical comparisons involved four cue displays that all contained orange and gold, one as the singleton cue color and the other as a context color (see <xref ref-type="fig" rid="fig3-0956797612458528">Fig. 3a</xref>). According to the relational account, the orange singleton cue should capture attention in the same-relation condition, when it is embedded among gold and red items (intermediate color), but it should fail to capture in the different-relation condition, where it is embedded among gold and yellow items (singleton cue is the reddest item in the cue display). The target-dissimilar gold singleton cue should capture attention when it is presented among orange and yellow items and has an intermediate feature value (same-relation condition) but fail to capture attention when it is presented among orange and red items (different-relation condition; singleton cue is the yellowest item).</p>
<fig id="fig3-0956797612458528" position="float">
<label>Fig. 3.</label>
<caption>
<p>Schematic of the target and cue displays in Experiment 2 and results of this experiment. Participants had to search for a letter that was always orange and always presented among two yellow and two red nontargets (a); the task was to report whether the orange letter was a <italic>T</italic> or an <italic>L</italic>. Each cue display consisted of a four-dot color singleton (cue) presented among four-dot stimuli of two other colors (cue context). We compared attentional capture by a target-similar cue (orange; left column) with attentional capture by a cue whose color was unrelated to the target displays (gold; right column). In same-relation trials, the cue had the same color relation to its context as the target did to its context (i.e., it had an intermediate color); either the cue-context colors matched the nontarget colors (baseline trials) or two of the cue-context stimuli were of a different color. In different-relation trials, the cue was either the reddest or the yellowest item in the cue display. The graphs (b) show mean response time (RT) and percentage of errors as a function of cue validity (valid, invalid) and the combination of cue color and cue context. The graphs on the left show results for trials in which the relation between the cue and cue context was the same as the relation between the target and nontargets; the graphs on the right show results for trials in which the relation between the cue and cue context was different from the relation between the target and nontargets. Error bars depict the 95% confidence intervals for the respective contrasts (<xref ref-type="bibr" rid="bibr27-0956797612458528">Loftus &amp; Masson, 1994</xref>) and may be smaller than the plotting symbol.</p>
</caption>
<graphic xlink:href="10.1177_0956797612458528-fig3.tif"/>
</fig>
<p>A corresponding pattern of results would demonstrate that attentional capture depends only on the relationships between the singleton cue and the cue context, and that capture is independent of whether the singleton cue has a target-similar color (orange) or a target-dissimilar color (gold), and whether competing cues in the cue context have a target-similar color (orange) or not (gold). Such a pattern would provide strong evidence for a relational account and would be very difficult to reconcile with a feature-specific, top-down selection bias. According to feature-based accounts, attention could be tuned to the target color (orange) or to a different color that is shifted away from orange (e.g., gold). Consequently, either the orange singleton cues could capture attention or the gold singleton cues could capture attention. However, as the critical cue displays contained either a gold singleton cue among two orange cues or an orange singleton cue among two gold cues, a feature-based account could not account for the outcome predicted by the relational account—that is, capture by the same-relation cues and no capture by the different-relation cues. This is because, according to feature-based accounts, the singleton cue (e.g., gold cue) would be inhibited or activated by virtue of having a particular color value, independently of whether it is the yellowest item in the cue display or an item with an intermediate color (relative to the cue context).</p>
<p>In addition to the four critical cue displays, Experiment 2 included two cue displays that contained either an orange or a gold singleton cue embedded in a context of two red and two yellow cues (the same colors as the nontargets). These cue displays were not critical, as both a feature-based account and a relational account would predict attentional capture for these cues. These cue displays were included only to establish a baseline for capture by the orange and gold singleton cues when they were embedded in a context of the nontarget colors.</p>
<sec id="section5-0956797612458528">
<title>Method</title>
<p>Fourteen new participants from the University of Queensland, Australia, participated in the study for a payment of AU$10.</p>
<p>The design, stimuli, and procedure were the same as in Experiment 1, with the following exceptions: In the target displays, the target was always orange. The four nontargets always consisted of two red and two yellow stimuli (see <xref ref-type="fig" rid="fig3-0956797612458528">Fig. 3a</xref>). Feature-specific versus relational top-down settings were probed by presenting, prior to the target display, a singleton cue that either matched the target color (orange) or mismatched the target color (gold). The cue context consisted of four stimuli of two different colors. Either the cue context rendered the (orange or gold) singleton cue the reddest or yellowest item in the display (different-relation cue: orange singleton cue among two gold and two yellow cues or gold singleton cue among two orange and two red cues), or the cue context consisted of two more extreme colors that rendered the singleton cue intermediate in color, so that it had the same feature relationships as the target (four same-relation singleton cues: orange singleton cue among two yellow and two red cues or among two gold and two red cues; gold singleton cue among two red and two yellow cues or among two orange and two yellow cues; see <xref ref-type="fig" rid="fig3-0956797612458528">Fig. 3a</xref> for an illustration of the cue displays).</p>
<p>The cue location coincided with the target location at chance level (20% of trials were valid), and each participant completed 600 trials. Prior to data analysis, 0.81% of all data were removed because of eye movements, and 0.14% of all data were excluded because of RT outliers (&lt; 200 ms or &gt; 1,500 ms). Mean accuracy was 93%, and there were no speed-accuracy trade-offs (see the Supplemental Material for a detailed report of the error scores).</p>
</sec>
<sec id="section6-0956797612458528">
<title>Results</title>
<p>The results for Experiment 2 are depicted in <xref ref-type="fig" rid="fig3-0956797612458528">Figure 3b</xref>. Baseline RTs ranged between 621 ms and 630 ms in Experiment 2 and did not differ as a function of the cue display (six-way ANOVA, <italic>F</italic> &lt; 1). A 2 (cue color: orange vs. gold singleton cue) × 3 (relation: same-relation singleton cue vs. different-relation singleton cue) × 2 (validity: valid vs. invalid) ANOVA computed over the mean RT showed significant main effects of cue color, <italic>F</italic>(1, 13) = 6.1, <italic>p</italic> = .028, η<sup>2</sup> = .32; relation, <italic>F</italic>(2, 26) = 7.0, <italic>p</italic> = .005, η<sup>2</sup> = .35; and validity, <italic>F</italic>(1, 13) = 152.4, <italic>p</italic> &lt; .001, η<sup>2</sup> = .92. Attentional capture by the singleton cue was not modulated by its color, as reflected in the nonsignificant Cue Color × Validity interaction, <italic>F</italic>(1, 13) = 2.6, <italic>p</italic> = .13. However, the Relation × Validity interaction was highly significant, <italic>F</italic>(2, 26) = 19.2, <italic>p</italic> &lt; .001, η<sup>2</sup> = .60 (all other <italic>p</italic>s &gt; .16).</p>
<p>Validity effects did not differ between the baseline same-relation condition (<italic>M</italic> = 58 ms) and the critical same-relation condition (<italic>M</italic> = 45 ms), <italic>F</italic>(1, 13) = 2.1, <italic>p</italic> = .17, but were significantly larger in both same-relation conditions than in the different-relation condition (<italic>M</italic> = 0 ms), <italic>F</italic>(1, 13) = 12.7, <italic>p</italic> = .003, η<sup>2</sup> = .49, and <italic>F</italic>(1, 13) = 19.7, <italic>p</italic> = .001, η<sup>2</sup> = .60, for the baseline and critical same- relation conditions, respectively. As predicted by the relational account, all same-relation cues captured attention—all <italic>t</italic>s(13) &gt; 2.5, all <italic>p</italic>s &lt; .05 (pairwise two-tailed <italic>t</italic> tests)—whereas the different-relation cues all failed to capture attention—<italic>t</italic>s(13) &lt; 1, <italic>p</italic>s ≥ .9 (see <xref ref-type="fig" rid="fig3-0956797612458528">Fig. 3b</xref>). Comparing differences in capture (RT on invalid trials – RT on valid trials) between the critical same-relation displays and the different-relation displays showed that validity effects were significantly stronger for same-relation singleton cues than for different-relation singleton cues, both for the orange singleton cue, <italic>t</italic>(13) = 3.9, <italic>p</italic> = .002, and for the gold singleton cue, <italic>t</italic>(13) = 3.1, <italic>p</italic> = .009.</p>
<p>Taken together, the results show that attentional capture was not significantly modulated by the color of the cue context (baseline displays vs. critical same-relation displays) or the color of the singleton cue (orange vs. gold). Both the orange and the gold singleton cues captured attention when they had an intermediate feature value, but not when they were the reddest or yellowest item in the cue display. These results demonstrate that even in search for an intermediate target, capture is determined by target-nontarget relationships rather than the feature value of the target.</p>
<p>The reported findings also cannot be attributed to differences in bottom-up feature contrast: The critical cue displays all contained gold and orange cues. Additionally, the second context color was more dissimilar to the singleton cue in different-relation displays (e.g., red context cues when the singleton cue was gold) than in same-relation displays (e.g., yellow context cues when the singleton cue was gold; see <xref ref-type="fig" rid="fig3-0956797612458528">Fig. 3a</xref>). Thus, the singleton cue should have been more visually salient in the different-relation condition, in which it failed to capture attention, than in the same-relation condition, in which it captured attention. Hence, the results of Experiment 2 can be safely attributed to a top-down selection bias for the target’s feature relationships.</p>
<p>However, a remaining uncertainty is whether the results were due to target-nontarget relationships in the luminance dimension or in the color dimension. Note that the colors used in Experiments 1 and 2 also differed systematically in luminance, as the lightness of the stimuli decreased from yellow to gold, orange, and red. Hence, it is possible that the observed results were due to top-down tuning to luminance relations, not color relations. To address this question, we conducted a control experiment with equiluminant colors.</p>
</sec>
</sec>
<sec id="section7-0956797612458528">
<title>Experiment 3</title>
<sec id="section8-0956797612458528">
<title>Method</title>
<p>Experiment 3 was identical to Experiment 2 except that it used equiluminant colors that varied along the red-green continuum. Prior to the experiment, red and green were first equated for luminance using a flicker test in which participants adjusted the luminance of green until it matched that of full red, which had RGB (red, green blue) values of 255, 0, 0 (same procedure as used in <xref ref-type="bibr" rid="bibr11-0956797612458528">Becker et al., 2010</xref>). In the experiment, equiluminant red and green were used as the nontarget colors. The search target was presented in an intermediate color, created by flickering between equiluminant red and green on each refresh (144 Hz). Presenting the sequence red-red-green over consecutive cycles resulted in a reddish-brown color, which was the target color (labeled RRG). The sequence green-green-red resulted in a greenish-brown color (labeled GGR). RRG and GGR were used as the colors for the target-matching and the nonmatching singleton cues, respectively, and served as the colors for two of the context cues in the critical cue displays. Cue and target displays were created in the same manner as in Experiment 2, except that the stimulus colors red, orange, gold, and yellow from Experiment 2 were replaced with equiluminant red (R), RRG, GGR, and green (G) in Experiment 3.</p>
<p>Fourteen new observers participated in Experiment 3. Two participants were excluded because of high mean error rates. Excluding trials with RT outliers and eye movement led to a loss of 0.99% and 0.92% of data, respectively. Mean accuracy was 94%, and there were no speed-accuracy trade-offs (see the Supplemental Material for a detailed report of the error scores).</p>
</sec>
<sec id="section9-0956797612458528">
<title>Results</title>
<p>The results for Experiment 3 are depicted in <xref ref-type="fig" rid="fig4-0956797612458528">Figure 4</xref>. Data were analyzed in the same way as in Experiment 2, and the results replicated the main findings from Experiment 2: A 2 (cue color: RRG vs. GGR singleton cue) × 3 (relation: same-relation vs. different-relation singleton cue) × 2 (validity: valid vs. invalid) ANOVA computed over the mean RT showed significant main effects of relation, <italic>F</italic>(3, 22) = 10.7, <italic>p</italic> = .002, η<sup>2</sup> = .49, and validity, <italic>F</italic>(1, 11) = 56.9, <italic>p</italic> &lt; .001, η<sup>2</sup> = .83, whereas the main effect of cue color just failed to reach significance, <italic>F</italic>(1, 11) = 4.0, <italic>p</italic> = .07. The Cue Color × Validity interaction reached significance, <italic>F</italic>(1, 11) = 14.8, <italic>p</italic> = .003, η<sup>2</sup> = .57; validity effects were larger for the target-similar RRG cue (<italic>M</italic> = 60 ms) than for the GGR cue (<italic>M</italic> = 23 ms). Moreover, the theoretically important Relation × Validity interaction was significant, <italic>F</italic>(2, 22) = 29.8, <italic>p</italic> &lt; .001, η<sup>2</sup> = .73 (all other <italic>p</italic>s &gt; .56). Validity effects were larger in the baseline same-relation condition (<italic>M</italic> = 83 ms) than in the critical same-relation condition (<italic>M</italic> = 44 ms), <italic>F</italic>(1, 11) = 17.3, <italic>p</italic> &lt; .002, η<sup>2</sup> = .61, and the different-relation condition (<italic>M</italic> = −2 ms), <italic>F</italic>(1, 11) = 44.2, <italic>p</italic> &lt; .001, η<sup>2</sup> = .80. Validity effects were also significantly larger in the critical same-relation condition (<italic>M</italic> = 44 ms) than in the different-relation condition (<italic>M</italic> = −2 ms), <italic>F</italic>(1, 11) = 18.9, <italic>p</italic> = .001, η<sup>2</sup> = .63.</p>
<fig id="fig4-0956797612458528" position="float">
<label>Fig. 4.</label>
<caption>
<p>Results of Experiment 3: mean response time (RT) and percentage of errors as a function of cue validity (valid, invalid) and the combination of cue color and cue context. In Experiment 3, the target was a red-red-green stimulus (RRG) presented among red (R) and green (G) nontargets. Singleton cues were either RRG or green-green-red (GGR) stimuli, presented in different cue contexts (R, G, GGR, RRG). The graphs on the left show results for trials in which the relation between the cue and cue context was the same as the relation between the target and nontargets; the graphs on the right show results for trials in which the relation between the cue and cue context was different from the relation between the target and nontargets. Error bars depict the 95% confidence intervals (<xref ref-type="bibr" rid="bibr27-0956797612458528">Loftus &amp; Masson, 1994</xref>) and may be smaller than the plotting symbol.</p>
</caption>
<graphic xlink:href="10.1177_0956797612458528-fig4.tif"/>
</fig>
<p>In line with the relational account, follow-up <italic>t</italic> tests showed that both the target-similar RRG and the target-dissimilar GGR singleton cues captured attention in the same-relation conditions, as reflected in significant validity effects for all same-relation cues, all <italic>t</italic>s(11) &gt; 2.6, all <italic>p</italic>s &lt; .024. By contrast, the GGR and RRG cues both failed to capture attention in the different-relation condition: The target-dissimilar GGR cue showed an inverse validity effect, with significantly longer RTs on valid trials than on invalid trials, <italic>t</italic>(11) = 2.8, <italic>p</italic> = .017. The target-similar RRG cue led to numerically shorter RTs on valid trials than on invalid trials; this difference, however, was far from significant, <italic>t</italic>(11) = 1.2, <italic>p</italic> = .22. The difference in attentional capture (RT on invalid trials – RT on valid trials) between the critical same-relation condition and the different-relation condition was also significant, both for the RRG cue, <italic>t</italic>(11) = 2.9, <italic>p</italic> = .015, and for the GGR cue, <italic>t</italic>(11) = 3.4, <italic>p</italic> = .006. These results corroborate the findings from Experiment 2, indicating that attention is also tuned to target-nontarget relationships in a feature search among equiluminant colors.</p>
<p>Unlike in Experiment 2, validity effects were larger for the target-similar (RRG) cue than for the target-dissimilar (GGR) cue in Experiment 3. However, a feature-specific bias for RRG or against GGR is unlikely to account for stronger attentional capture by the RRG cue: If there were such a bias, the GGR singleton cue should not have captured attention in the critical same-relation condition, in which it was presented together with two RRG context cues, yet the GGR cue captured attention in this condition. Attentional capture was clearly determined by the relationships between the singleton cue and the cue context, but it appears that the GGR cue generally slowed responses to the target on valid trials, regardless of whether or not it captured attention. This led to a reduced validity effect for the GGR cue in the same-relation conditions and an inverse validity effect in the different-relation condition.</p>
<p>Several recent spatial-cuing studies have documented instances of selective slowing on valid trials (<xref ref-type="bibr" rid="bibr1-0956797612458528">Anderson &amp; Folk, 2010</xref>, <xref ref-type="bibr" rid="bibr2-0956797612458528">2012</xref>; <xref ref-type="bibr" rid="bibr12-0956797612458528">Belopolsky, Schreij, &amp; Theeuwes, 2010</xref>) and have shown this effect to be independent of attention shifts to the cue (<xref ref-type="bibr" rid="bibr2-0956797612458528">Anderson &amp; Folk, 2012</xref>). It is possible that presenting a different-colored cue at the target location can in some instances interfere with target- identification processes or decisional processes that commence immediately after attention has been shifted to the target (e.g., <xref ref-type="bibr" rid="bibr10-0956797612458528">Becker, 2010b</xref>; <xref ref-type="bibr" rid="bibr20-0956797612458528">Huang, Holcombe, &amp; Pashler, 2004</xref>). Regardless of the exact locus of this effect, the important point is that it cannot account for the main finding (i.e., that attentional capture was determined by the relationships of the singleton cue), as the singleton cue’s color did not modulate effects of the singleton cue’s relation to its context (as reflected by the nonsignificant three-way interaction). Moreover, differences between the RRG and GGR singleton cues cannot account for the observation that each one of them considered in isolation captured attention only when its relationships matched the target-nontarget relations (and failed to capture attention when its relationships did not match the target- nontarget relations). These results clearly support the relational account and are inconsistent with the central claim of extant feature-based theories that attention is usually biased to a particular feature value.</p>
</sec>
</sec>
<sec id="section10-0956797612458528" sec-type="discussion">
<title>General Discussion</title>
<p>The present study yielded several important findings. First, contrary to the prevalent view, we demonstrated that attentional capture by irrelevant items does not depend on similarity to the target feature. Target similarity was neither necessary nor sufficient for attentional capture: It was not necessary because items dissimilar to the target (and similar to the nontargets) captured attention (Experiment 1). It was not sufficient because a target- similar singleton cue failed to capture attention when it did not have target-matching relational properties (Experiments 1, 2, and 3). The finding that target-dissimilar, same-relation cues captured attention whereas target-similar, different-relation cues failed to capture attention also cannot be explained by the specific color of the singleton cue, or with reference to the feature contrast or saliency of the stimuli. Thus, the present results provide the first decisive evidence for a relational account of top-down control.</p>
<p>Second, this is also the first study to demonstrate that attention is tuned to the target-nontarget relationships both in pop-out search, in which a singleton target is presented among same-colored nontargets (Experiment 1), and in feature search, in which the target is presented among different-colored nontargets (Experiments 2 and 3). These results indicate that search for a singleton target (Experiment 1) and search for a feature target (Experiments 2 and 3) do not automatically give rise to different search modes, namely, a singleton search mode in which the visual system is susceptible to distraction by all salient items and a feature search mode in which only target-similar items capture attention (e.g., <xref ref-type="bibr" rid="bibr5-0956797612458528">Bacon &amp; Egeth, 1994</xref>).</p>
<p>Even more important, the results of Experiments 2 and 3 demonstrated, for the first time, that information from different target-nontarget relations (redder, yellower) can be combined to guide attention, either by integrating relational information prior to selection or by tuning simultaneously against the most extreme stimuli in the visual field (e.g., rejection of the yellowest and reddest stimuli). The ability to combine information from different target-nontarget relationships is theoretically important because it means that the relational account can explain fine-grained top-down selectivity on the basis of a few cardinal tuning directions (e.g., redder, greener, yellower). This obviates the need to propose a multitude of different feature detectors or topographic feature maps, which has been recognized to be a major problem of current feature-based theories (cf. <xref ref-type="bibr" rid="bibr30-0956797612458528">Maunsell &amp; Treue, 2006</xref>).</p>
<p>The finding that top-down tuning can be based on target-nontarget relations, however, should not be taken to mean that it is impossible to adopt a feature-specific setting. Testing for this possibility would require rendering a relational search strategy futile (e.g., by randomly varying the nontargets between red and yellow across trials), which was not the case in our study. However, most of the available behavioral and neurophysiological evidence that has been interpreted in support of a feature-based account also seems consistent with a relational account (behavioral evidence—e.g., <xref ref-type="bibr" rid="bibr1-0956797612458528">Anderson &amp; Folk, 2010</xref>; <xref ref-type="bibr" rid="bibr3-0956797612458528">Ansorge &amp; Heumann, 2003</xref>; <xref ref-type="bibr" rid="bibr16-0956797612458528">Eimer et al., 2009</xref>; <xref ref-type="bibr" rid="bibr17-0956797612458528">Folk &amp; Remington, 1998</xref>; neurophysiological evidence—e.g., <xref ref-type="bibr" rid="bibr4-0956797612458528">Atiani, Elhilali, David, Fritz, &amp; Shamma, 2009</xref>; <xref ref-type="bibr" rid="bibr14-0956797612458528">David, Hayden, Mazer, &amp; Gallant, 2008</xref>; <xref ref-type="bibr" rid="bibr30-0956797612458528">Maunsell &amp; Treue, 2006</xref>; <xref ref-type="bibr" rid="bibr31-0956797612458528">Motter, 1994</xref>; <xref ref-type="bibr" rid="bibr37-0956797612458528">Scolari &amp; Serences, 2010</xref>; <xref ref-type="bibr" rid="bibr38-0956797612458528">Spitzer, Desimone, &amp; Moran, 1988</xref>, respectively), as none of the studies systematically varied the feature relationships between the target and the context, or between the distractor and the context. Hence, it is possible that effects previously attributed to feature-specific settings are due to top-down settings for feature relationships.</p>
</sec>
</body>
<back>
<ack>
<p>We thank Magda Lechowicz for helping with data collection and Paul Dux, Jason Mattingley, and Troy Visser for commenting on earlier drafts of this manuscript.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The study was supported by an Australian Research Council Discovery Grant and postdoctoral fellowship (DP110100588) awarded to S. I. B. and by an Australian Research Council Discovery Grant (DP120103721) awarded to R. W. R., S. I. B., and C.L.F.</p>
</fn>
<fn fn-type="supplementary-material">
<label>Supplemental Material</label>
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0956797612458528">
<label>1.</label>
<p>We would like to thank a reviewer for suggesting this possibility. A combined optimal-tuning/optimal-suppression account can perhaps be derived from <xref ref-type="bibr" rid="bibr36-0956797612458528">Scolari and Serences (2009)</xref>. They proposed that contrast detection thresholds could be lowered for both the shifted target feature value and the shifted nontarget feature value, as changes in the activity of these neurons would be equally informative (according to a Fisher metric). Target selection could then be achieved by applying a <italic>min</italic> decision rule to nontarget activation, which prevents selection of the nontargets, and a <italic>max</italic> decision rule to target activation, which aids target selection (<xref ref-type="bibr" rid="bibr36-0956797612458528">Scolari &amp; Serences, 2009</xref>, p. 11941).</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>B. A.</given-names></name>
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Variations in the magnitude of attentional capture: Testing a two-process account</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>72</volume>, <fpage>342</fpage>–<lpage>352</lpage>.</citation>
</ref>
<ref id="bibr2-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>B. A.</given-names></name>
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Dissociating location-specific inhibition and attention shifts: Evidence against the disengagement account of contingent capture</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>74</volume>, <fpage>1183</fpage>–<lpage>1198</lpage>.</citation>
</ref>
<ref id="bibr3-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ansorge</surname><given-names>U.</given-names></name>
<name><surname>Heumann</surname><given-names>M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Top-down contingencies in peripheral cuing: The roles of color and location</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>29</volume>, <fpage>937</fpage>–<lpage>948</lpage>.</citation>
</ref>
<ref id="bibr4-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Atiani</surname><given-names>S.</given-names></name>
<name><surname>Elhilali</surname><given-names>M.</given-names></name>
<name><surname>David</surname><given-names>S. V.</given-names></name>
<name><surname>Fritz</surname><given-names>J. B.</given-names></name>
<name><surname>Shamma</surname><given-names>S. A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Task difficulty and performance induce diverse adaptive patterns in gain and shape of primary auditory cortical receptive fields</article-title>. <source>Neuron</source>, <volume>61</volume>, <fpage>467</fpage>–<lpage>480</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bacon</surname><given-names>W. F.</given-names></name>
<name><surname>Egeth</surname><given-names>H. E.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Local processes in preattentive feature detection</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>17</volume>, <fpage>77</fpage>–<lpage>90</lpage>.</citation>
</ref>
<ref id="bibr6-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Beck</surname><given-names>D. M.</given-names></name>
<name><surname>Kastner</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Stimulus context modulates competition in human extrastriate cortex</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>, <fpage>1110</fpage>–<lpage>1116</lpage>.</citation>
</ref>
<ref id="bibr7-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Beck</surname><given-names>D. M.</given-names></name>
<name><surname>Kastner</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Top-down and bottom-up mechanisms in biasing competition in the human brain</article-title>. <source>Vision Research</source>, <volume>49</volume>, <fpage>1154</fpage>–<lpage>1165</lpage>.</citation>
</ref>
<ref id="bibr8-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Becker</surname><given-names>S. I.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Irrelevant singletons in pop-out search: Attentional capture or filtering costs?</article-title> <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>33</volume>, <fpage>764</fpage>–<lpage>787</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Becker</surname><given-names>S. I.</given-names></name>
</person-group> (<year>2010a</year>). <article-title>The role of target-distractor relationships in guiding attention and the eyes in visual search</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>139</volume>, <fpage>247</fpage>–<lpage>265</lpage>.</citation>
</ref>
<ref id="bibr10-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Becker</surname><given-names>S. I.</given-names></name>
</person-group> (<year>2010b</year>). <article-title>Testing a post-selectional account of across-dimension switch costs</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>17</volume>, <fpage>853</fpage>–<lpage>861</lpage>.</citation>
</ref>
<ref id="bibr11-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Becker</surname><given-names>S. I.</given-names></name>
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
<name><surname>Remington</surname><given-names>R. W.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The role of relational information in contingent capture</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>36</volume>, <fpage>1460</fpage>–<lpage>1476</lpage>.</citation>
</ref>
<ref id="bibr12-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Belopolsky</surname><given-names>A. V.</given-names></name>
<name><surname>Schreij</surname><given-names>D.</given-names></name>
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2010</year>). <article-title>What is top-down about contingent capture?</article-title> <source>Attention, Perception, &amp; Psychophysics</source>, <volume>72</volume>, <fpage>326</fpage>–<lpage>341</lpage>.</citation>
</ref>
<ref id="bibr13-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carrasco</surname><given-names>M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Visual attention: The past 25 years</article-title>. <source>Vision Research</source>, <volume>51</volume>, <fpage>1484</fpage>–<lpage>1525</lpage>.</citation>
</ref>
<ref id="bibr14-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>David</surname><given-names>S. V.</given-names></name>
<name><surname>Hayden</surname><given-names>B. Y.</given-names></name>
<name><surname>Mazer</surname><given-names>J. A.</given-names></name>
<name><surname>Gallant</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision</article-title>. <source>Neuron</source>, <volume>59</volume>, <fpage>509</fpage>–<lpage>521</lpage>.</citation>
</ref>
<ref id="bibr15-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Duncan</surname><given-names>J.</given-names></name>
<name><surname>Humphreys</surname><given-names>G. W.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Visual search and stimulus similarity</article-title>. <source>Psychological Review</source>, <volume>96</volume>, <fpage>433</fpage>–<lpage>458</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eimer</surname><given-names>M.</given-names></name>
<name><surname>Kiss</surname><given-names>M.</given-names></name>
<name><surname>Press</surname><given-names>C.</given-names></name>
<name><surname>Sauter</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The roles of feature-specific task set and bottom-up salience in attentional capture: An ERP study</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>35</volume>, <fpage>1316</fpage>–<lpage>1328</lpage>.</citation>
</ref>
<ref id="bibr17-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
<name><surname>Remington</surname><given-names>R.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Selectivity in distraction by irrelevant featural singletons: Evidence for two forms of attentional capture</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>24</volume>, <fpage>847</fpage>–<lpage>858</lpage>.</citation>
</ref>
<ref id="bibr18-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
<name><surname>Remington</surname><given-names>R. W.</given-names></name>
<name><surname>Johnston</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Involuntary covert orienting is contingent on attentional control settings</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>18</volume>, <fpage>1030</fpage>–<lpage>1044</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Folk</surname><given-names>C. L.</given-names></name>
<name><surname>Remington</surname><given-names>R. W.</given-names></name>
<name><surname>Johnston</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Contingent attentional capture: A reply to Yantis</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>19</volume>, <fpage>682</fpage>–<lpage>685</lpage>.</citation>
</ref>
<ref id="bibr20-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Huang</surname><given-names>L.</given-names></name>
<name><surname>Holcombe</surname><given-names>A.</given-names></name>
<name><surname>Pashler</surname><given-names>H.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Repetition priming in visual search: Episodic retrieval, not feature priming</article-title>. <source>Memory &amp; Cognition</source>, <volume>32</volume>, <fpage>12</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr21-0956797612458528">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Hwang</surname><given-names>A. D.</given-names></name>
<name><surname>Higgins</surname><given-names>E. C.</given-names></name>
<name><surname>Pomplun</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A model of top-down attentional control during visual search in complex scenes</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>5</issue>), Article 25. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/9/5/25.full.pdf+html">http://www.journalofvision.org/content/9/5/25.full.pdf+html</ext-link></citation>
</ref>
<ref id="bibr22-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Itti</surname><given-names>L.</given-names></name>
<name><surname>Koch</surname><given-names>C.</given-names></name>
</person-group> (<year>2000</year>). <article-title>A saliency-based search mechanism for overt and covert shifts of visual attention</article-title>. <source>Vision Research</source>, <volume>40</volume>, <fpage>1489</fpage>–<lpage>1506</lpage>.</citation>
</ref>
<ref id="bibr23-0956797612458528">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jonides</surname><given-names>J.</given-names></name>
</person-group> (<year>1981</year>). <article-title>Voluntary vs. automatic control over the mind’s eye’s movement</article-title>. In <person-group person-group-type="editor">
<name><surname>Long</surname><given-names>J. B.</given-names></name>
<name><surname>Baddeley</surname><given-names>A. D.</given-names></name>
</person-group> (Eds.), <source>Attention and performance IX</source> (pp. <fpage>197</fpage>–<lpage>203</lpage>). <publisher-loc>Hills-dale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr24-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Koch</surname><given-names>C.</given-names></name>
<name><surname>Ullman</surname><given-names>S.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Shifts in visual selective attention: Towards the underlying neural circuitry</article-title>. <source>Human Neurobiology</source>, <volume>4</volume>, <fpage>219</fpage>–<lpage>227</lpage>.</citation>
</ref>
<ref id="bibr25-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>D. K.</given-names></name>
<name><surname>Itti</surname><given-names>L.</given-names></name>
<name><surname>Koch</surname><given-names>C.</given-names></name>
<name><surname>Braun</surname><given-names>J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Attention activates winner-take-all competition among visual filters</article-title>. <source>Nature Neuroscience</source>, <volume>2</volume>, <fpage>375</fpage>–<lpage>381</lpage>.</citation>
</ref>
<ref id="bibr26-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>Z.</given-names></name>
</person-group> (<year>2002</year>). <article-title>A saliency map in primary visual cortex</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>6</volume>, <fpage>9</fpage>–<lpage>16</lpage>.</citation>
</ref>
<ref id="bibr27-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Loftus</surname><given-names>G. R.</given-names></name>
<name><surname>Masson</surname><given-names>M.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Using confidence intervals in within-subject designs</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>4</volume>, <fpage>476</fpage>–<lpage>490</lpage>.</citation>
</ref>
<ref id="bibr28-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ludwig</surname><given-names>C. J. H.</given-names></name>
<name><surname>Gilchrist</surname><given-names>I. D.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Stimulus-driven and goal-driven control over visual selection</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>28</volume>, <fpage>902</fpage>–<lpage>912</lpage>.</citation>
</ref>
<ref id="bibr29-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Martinez-Trujillo</surname><given-names>J. C.</given-names></name>
<name><surname>Treue</surname><given-names>S.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Feature-based attention increases the selectivity of population responses in primate visual cortex</article-title>. <source>Current Biology</source>, <volume>14</volume>, <fpage>744</fpage>–<lpage>751</lpage>.</citation>
</ref>
<ref id="bibr30-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Maunsell</surname><given-names>J. H. R.</given-names></name>
<name><surname>Treue</surname><given-names>S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Feature-based attention in visual cortex</article-title>. <source>Trends in Neuroscience</source>, <volume>29</volume>, <fpage>317</fpage>–<lpage>322</lpage>.</citation>
</ref>
<ref id="bibr31-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Motter</surname><given-names>B. C.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Neural correlates of attentive selection for color and luminance in extrastriate area V4</article-title>. <source>The Journal of Neuroscience</source>, <volume>14</volume>, <fpage>2178</fpage>–<lpage>2189</lpage>.</citation>
</ref>
<ref id="bibr32-0956797612458528">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Navalpakkam</surname><given-names>V.</given-names></name>
<name><surname>Itti</surname><given-names>L.</given-names></name>
</person-group> (<year>2006</year>). <article-title>An integrated model of top-down and bottom-up attention for optimizing detection speed</article-title>. In <source>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source> (<volume>Vol. 2</volume>, pp. <fpage>2049</fpage>–<lpage>2056</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>IEEE Computer Society</publisher-name>.</citation>
</ref>
<ref id="bibr33-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Navalpakkam</surname><given-names>V.</given-names></name>
<name><surname>Itti</surname><given-names>L.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Search goals tunes visual features optimally</article-title>. <source>Neuron</source>, <volume>53</volume>, <fpage>605</fpage>–<lpage>617</lpage>.</citation>
</ref>
<ref id="bibr34-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Posner</surname><given-names>M. I.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Orienting of attention</article-title>. <source>Quarterly Journal of Experimental Psychology</source>, <volume>32</volume>, <fpage>3</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr35-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reynolds</surname><given-names>J. R.</given-names></name>
<name><surname>Desimone</surname><given-names>R.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Interacting roles of attention and visual salience in V4</article-title>. <source>Neuron</source>, <volume>37</volume>, <fpage>853</fpage>–<lpage>863</lpage>.</citation>
</ref>
<ref id="bibr36-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scolari</surname><given-names>M.</given-names></name>
<name><surname>Serences</surname><given-names>J. T.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Adaptive allocation of attentional gain</article-title>. <source>The Journal of Neuroscience</source>, <volume>29</volume>, <fpage>11933</fpage>–<lpage>11942</lpage>.</citation>
</ref>
<ref id="bibr37-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scolari</surname><given-names>M.</given-names></name>
<name><surname>Serences</surname><given-names>J. T.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Basing perceptual decisions on the most informative sensory neurons</article-title>. <source>Journal of Neurophysiology</source>, <volume>104</volume>, <fpage>2266</fpage>–<lpage>2273</lpage>.</citation>
</ref>
<ref id="bibr38-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spitzer</surname><given-names>H.</given-names></name>
<name><surname>Desimone</surname><given-names>R.</given-names></name>
<name><surname>Moran</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Increased attention enhances both behavioral and neuronal performance</article-title>. <source>Science</source>, <volume>240</volume>, <fpage>338</fpage>–<lpage>340</lpage>.</citation>
</ref>
<ref id="bibr39-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Perceptual selectivity for color and form</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>51</volume>, <fpage>599</fpage>–<lpage>606</lpage>.</citation>
</ref>
<ref id="bibr40-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Stimulus-driven capture and attentional set: Selective search for color and visual abrupt onsets</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>20</volume>, <fpage>799</fpage>–<lpage>806</lpage>.</citation>
</ref>
<ref id="bibr41-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Theeuwes</surname><given-names>J.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Top-down and bottom-up control of visual selection</article-title>. <source>Acta Psychologica</source>, <volume>135</volume>, <fpage>97</fpage>–<lpage>135</lpage>.</citation>
</ref>
<ref id="bibr42-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treisman</surname><given-names>A.</given-names></name>
<name><surname>Gelade</surname><given-names>G.</given-names></name>
</person-group> (<year>1980</year>). <article-title>A feature integration theory of attention</article-title>. <source>Cognitive Psychology</source>, <volume>12</volume>, <fpage>97</fpage>–<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr43-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treisman</surname><given-names>A.</given-names></name>
<name><surname>Sato</surname><given-names>S.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Conjunction search revisited</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>16</volume>, <fpage>459</fpage>–<lpage>478</lpage>.</citation>
</ref>
<ref id="bibr44-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wolfe</surname><given-names>J. M.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Guided search 2.0: A revised model of visual search</article-title>. <source>Psychonomic Bulletin &amp; Review</source>, <volume>1</volume>, <fpage>202</fpage>– <lpage>238</lpage>.</citation>
</ref>
<ref id="bibr45-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yantis</surname><given-names>S.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Stimulus-driven attentional capture and attentional control settings</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>19</volume>, <fpage>676</fpage>–<lpage>681</lpage>.</citation>
</ref>
<ref id="bibr46-0956797612458528">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Yantis</surname><given-names>S.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Goal-directed and stimulus-driven determinants of attentional control</article-title>. In <person-group person-group-type="editor">
<name><surname>Monsell</surname><given-names>S.</given-names></name>
<name><surname>Driver</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Attention and performance XVIII</source> (pp. <fpage>73</fpage>–<lpage>103</lpage>). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr47-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yantis</surname><given-names>S.</given-names></name>
<name><surname>Egeth</surname><given-names>H. E.</given-names></name>
</person-group> (<year>1999</year>). <article-title>On the distinction between visual salience and stimulus-driven attentional capture</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>25</volume>, <fpage>661</fpage>–<lpage>676</lpage>.</citation>
</ref>
<ref id="bibr48-0956797612458528">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Zhaoping</surname><given-names>L.</given-names></name>
<name><surname>Zhou</surname><given-names>T.</given-names></name>
<name><surname>Fang</surname><given-names>F.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Neural activities in V1 create a bottom-up saliency map</article-title>. <source>Neuron</source>, <volume>71</volume>, <fpage>183</fpage>–<lpage>192</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>