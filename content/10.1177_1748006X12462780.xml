<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PIO</journal-id>
<journal-id journal-id-type="hwp">sppio</journal-id>
<journal-title>Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability</journal-title>
<issn pub-type="ppub">1748-006X</issn>
<issn pub-type="epub">1748-0078</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1748006X12462780</article-id>
<article-id pub-id-type="publisher-id">10.1177_1748006X12462780</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Special Issue Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Uncertainty assessment of reliability estimates for safety-instrumented systems</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Jin</surname><given-names>Hui</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Lundteigen</surname><given-names>Mary Ann</given-names></name>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Rausand</surname><given-names>Marvin</given-names></name>
</contrib>
<aff id="aff1-1748006X12462780">Department of Production and Quality Engineering, Norwegian University of Science and Technology, Trondheim, Norway</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-1748006X12462780">Marvin Rausand, Department of Production and Quality Engineering, Norwegian University of Science and Technology, S.P. Andersens veg 5, Trondheim, NO 7491, Norway. Email: <email>marvin.rausand@ntnu.no</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>226</volume>
<issue>6</issue>
<issue-title>Special issue of selected articles from ESREL 2011</issue-title>
<fpage>646</fpage>
<lpage>655</lpage>
<history>
<date date-type="received">
<day>30</day>
<month>3</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>9</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© IMechE 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Institution of Mechanical Engineers</copyright-holder>
</permissions>
<abstract>
<p>Reliability estimates play a crucial role in decision making related to the design and operation of safety-instrumented systems. A safety-instrumented system is often a complex system whose performance is seldom fully understood. The safety-instrumented system reliability estimation is influenced by several simplifications and assumptions, both about the safety-instrumented system and its operating context, and therefore subject to uncertainty. If the decision makers are not aware of the level of uncertainty, they may misinterpret the results and select a safety-instrumented system design that is either too complex or too simple, or with an inadequate testing strategy, to provide the required risk reduction. This article elucidates the uncertainties related to safety-instrumented system reliability estimation. The article is limited to safety-instrumented systems that are operated in a low-demand mode, for which the probability of failure on demand is the standard reliability measure. The uncertainty of the probability of failure on demand estimate is classified as completeness uncertainty, model uncertainty, and parameter uncertainty and each category is thoroughly discussed. It is argued that the completeness uncertainty is the most important for safety-instrumented system reliability analyses, followed by parameter and model uncertainty. It is further argued that uncertainty assessment should be an integrated part of any safety-instrumented system reliability analysis, and that the analyst should communicate her judgment about the uncertainty to the decision-makers as part of the analysis results.</p>
</abstract>
<kwd-group>
<kwd>Reliability</kwd>
<kwd>uncertainty</kwd>
<kwd>safety systems</kwd>
<kwd>complexity</kwd>
<kwd>reliability models</kwd>
<kwd>reliability data</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1748006X12462780" sec-type="intro">
<title>Introduction</title>
<p>Safety-instrumented systems (SISs) are used in many industrial sectors to detect hazardous events and prevent them from developing into accidents. Reliability requirements for the safety-instrumented functions (SIFs), that are performed by a SIS, shall, according to IEC 61508, be deduced from hazard and risk analyses.</p>
<p>A SIS generally consists of one or more input elements, one or more logic units, and one or more final elements. A very simple SIS configuration is shown in <xref ref-type="fig" rid="fig1-1748006X12462780">Figure 1</xref>. It should, however, be realized that the configurations used in practical applications are often far more complicated. A SIS is installed to protect a system, which we will refer to as the equipment under control (EUC). The design, construction, implementation, and operation of a SIS are subject to the requirements in the generic standard IEC 61508<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> and in application-specific standards, such as IEC 61511<sup><xref ref-type="bibr" rid="bibr2-1748006X12462780">2</xref></sup> for the process industry, IEC 61513<sup><xref ref-type="bibr" rid="bibr3-1748006X12462780">3</xref></sup> for the nuclear industry, ISO 26262<sup><xref ref-type="bibr" rid="bibr4-1748006X12462780">4</xref></sup> for the automobile industry, IEC 62278<sup><xref ref-type="bibr" rid="bibr5-1748006X12462780">5</xref></sup> for the railway industry, and IEC 62061<sup><xref ref-type="bibr" rid="bibr6-1748006X12462780">6</xref></sup> for machinery systems. The standards require that the SIS reliability is calculated and give guidance on how this can be done.</p>
<fig id="fig1-1748006X12462780" position="float">
<label>Figure 1.</label>
<caption>
<p>The main SIS elements.</p>
</caption>
<graphic xlink:href="10.1177_1748006X12462780-fig1.tif"/>
</fig>
<p>SIS reliability calculations are based on simplifications and assumptions about the system and its operating context, and the reliability values are therefore subject to uncertainty. Without being aware of the level of uncertainty, decision makers, such as SIS suppliers and end-users, may make improper decisions regarding system configuration, component selection, and testing and maintenance strategies.</p>
<p>SIFs are classified according to how often they are demanded and IEC 61508<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> distinguish between low-demand and high-demand SIFs. A low-demand SIF is demanded less often than once per year and remains dormant until it is activated. The first edition of IEC 61508 also referred to the proof test frequency in the classification into high-demand and low-demand. This requirement has been removed in the second edition, but the scientific community is still debating this matter. Failures may occur and remain undetected until a proof test is carried out. The reliability of a low-demand SIF is measured by the average probability of failure on demand (PFD<sub>avg</sub>) and this measure is used to express the reliability requirement to the SIF. In this article, reliability is used as a general term that is synonymous to the term dependability.<sup><xref ref-type="bibr" rid="bibr7-1748006X12462780">7</xref></sup> The reliability of a system can, for example, be measured by its availability. The PFD<sub>avg</sub> is here a measure of the system’s unavailability. It should be noted that several SIFs may be implemented by the same SIS.</p>
<p>The IEC standards classify the reliability requirements into four safety integrity levels (SILs), as shown in <xref ref-type="table" rid="table1-1748006X12462780">Table 1</xref>. To meet the reliability requirement of SIL 3, for example, the SIF must, on average, not fail more than once per 1000 demands.</p>
<table-wrap id="table1-1748006X12462780" position="float">
<label>Table 1.</label>
<caption>
<p>SIL requirements for low-demand SIFs<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup></p>
</caption>
<graphic alternate-form-of="table1-1748006X12462780" xlink:href="10.1177_1748006X12462780-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">SIL</th>
<th align="left">PFD<sub>avg</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>
<inline-formula id="inline-formula1-1748006X12462780">
<mml:math display="inline" id="math1-1748006X12462780">
<mml:mrow>
<mml:mo>≥</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mn>10</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula2-1748006X12462780"><mml:math display="inline" id="math2-1748006X12462780"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>2</td>
<td>
<inline-formula id="inline-formula3-1748006X12462780">
<mml:math display="inline" id="math3-1748006X12462780">
<mml:mrow>
<mml:mo>≥</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mn>10</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>3</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula4-1748006X12462780"><mml:math display="inline" id="math4-1748006X12462780"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>3</td>
<td>
<inline-formula id="inline-formula5-1748006X12462780">
<mml:math display="inline" id="math5-1748006X12462780">
<mml:mrow>
<mml:mo>≥</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mn>10</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>4</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula6-1748006X12462780"><mml:math display="inline" id="math6-1748006X12462780"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>4</td>
<td>
<inline-formula id="inline-formula7-1748006X12462780">
<mml:math display="inline" id="math7-1748006X12462780">
<mml:mrow>
<mml:mo>≥</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mn>10</mml:mn>
</mml:mrow>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>5</mml:mn>
</mml:mrow>
</mml:msup>
</mml:mrow>
</mml:math>
</inline-formula> to <inline-formula id="inline-formula8-1748006X12462780"><mml:math display="inline" id="math8-1748006X12462780"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1748006X12462780">
<p>PDF<sub>avg</sub>: average probability of failure on demand; SIL: safety integrity level.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>In addition to the quantitative requirements in <xref ref-type="table" rid="table1-1748006X12462780">Table 1</xref>, the SIS must also meet several qualitative requirements related to architectural constraints, safety management, and so on. These requirements are not discussed in this article; neither are reliability requirements to high-demand SIFs discussed.</p>
<p>Many authors<sup><xref ref-type="bibr" rid="bibr8-1748006X12462780">8</xref><xref ref-type="bibr" rid="bibr9-1748006X12462780"/><xref ref-type="bibr" rid="bibr10-1748006X12462780"/>–<xref ref-type="bibr" rid="bibr11-1748006X12462780">11</xref></sup> discuss uncertainty in risk and reliability analysis, but very few discuss aspects related directly to SIS reliability, and these are mainly limited to the uncertainty of input parameters.<sup><xref ref-type="bibr" rid="bibr12-1748006X12462780">12</xref><xref ref-type="bibr" rid="bibr13-1748006X12462780"/>–<xref ref-type="bibr" rid="bibr14-1748006X12462780">14</xref></sup> Other types of uncertainty have not been addressed, even though they may have a significant influence on the reliability estimates. Janbu<sup><xref ref-type="bibr" rid="bibr15-1748006X12462780">15</xref></sup> and Jin et al.<sup><xref ref-type="bibr" rid="bibr16-1748006X12462780">16</xref></sup> are two of the few who investigate the uncertainty related to SIF reliability estimates from an overall perspective.</p>
<p>IEC 61508 and IEC 61511 give few requirements that address uncertainty in decision making. The standards add some conservatism to the reliability estimation, by requiring that the failure rates data used should have a confidence level of at least 70%.<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref>,<xref ref-type="bibr" rid="bibr2-1748006X12462780">2</xref></sup> To meet this requirement, it is necessary to consider the failure rate as a random variable <inline-formula id="inline-formula9-1748006X12462780"><mml:math display="inline" id="math9-1748006X12462780"><mml:mrow><mml:mi>Λ</mml:mi></mml:mrow></mml:math></inline-formula> with a probability distribution that describes our knowledge/belief about the failure rate.<sup><xref ref-type="bibr" rid="bibr17-1748006X12462780">17</xref></sup> The value <inline-formula id="inline-formula10-1748006X12462780"><mml:math display="inline" id="math10-1748006X12462780"><mml:mrow><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that is used in the calculations, must fulfill <inline-formula id="inline-formula11-1748006X12462780"><mml:math display="inline" id="math11-1748006X12462780"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>Λ</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula>, to have a 70% confidence level. IEC 61508 also requires that a confidence level of at least 90% shall be demonstrated on the reliability estimates, in the selection of hardware architectures for the so-called route “<inline-formula id="inline-formula12-1748006X12462780"><mml:math display="inline" id="math12-1748006X12462780"><mml:mrow><mml:msub><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>”.<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> Some suppliers use “best estimates”, but add conservatism by making the SIL requirement more strict, such that compliance with, for example, SIL 3, is only claimed when PFD<sub>avg</sub><inline-formula id="inline-formula13-1748006X12462780"><mml:math display="inline" id="math13-1748006X12462780"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>7</mml:mn><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
<p>The PDS-method<sup><xref ref-type="bibr" rid="bibr18-1748006X12462780">18</xref></sup> (PDS is the Norwegian abbreviation for “Reliability of computer-based safety systems” For more information, see <ext-link ext-link-type="uri" xlink:href="http://www.sintef.no/PDS">http://www.sintef.no/PDS</ext-link>.) extends the formulas in IEC 61508 by introducing additional measures to account for factors that are often left out in the SIF reliability calculations: (i) test independent failures (TIFs) that may remain unrevealed owing to limitations of the proof testing, and (ii) inclusion of systematic failures in the failure rate estimates.</p>
<p>These efforts attempt to reduce/compensate the uncertainty of SIS reliability estimates, but the scope is rather limited. A thorough uncertainty assessment approach for SIS reliability estimates seems to be lacking.</p>
<p>Our point of departure is that we do not believe that it is possible to quantify the uncertainty of a PFD<sub>avg</sub> estimate in any objective way. The person most capable of making judgments about the uncertainty is the analyst and they should communicate to the decision-makers their “degree of belief” about the uncertainty, together with the results from the SIS reliability analysis.</p>
<p>The objectives of this article are to elucidate the concept of uncertainty in SIS reliability analysis and to highlight problematic issues related to the three categories: completeness uncertainty, model uncertainty, and parameter uncertainty. After having read the article, we hope that the analyst will be in a better position to judge and present their judgment about the uncertainty of a PFD<sub>avg</sub> estimate.</p>
<p>The rest of this article is organized as follows. ‘Reliability calculation’ presents the various categories of failures that may occur in a SIS, outlines the SIS reliability analysis process, and lists the main simplifications and assumptions that are made. A brief overview of the most common methods for SIS reliability analysis is also given. ‘What is uncertainty?’ introduces the concept of uncertainty and discusses the classification of uncertainty with respect to SIS reliability estimation. ‘Completeness uncertainty’ presents and discusses main issues related to completeness uncertainty. This is followed by ‘Model uncertainty’ and ‘Parameter Uncertainty’. Finally, concluding remarks are given.</p>
</sec>
<sec id="section2-1748006X12462780">
<title>Reliability calculation</title>
<p>Reliability calculations are based on a number of assumptions and simplifications, affecting the scope of the analysis and the ability to represent physical and operational properties of the components and the system. One important assumption is that reliability is best calculated by the use of statistical models, rather than physics-of-failure models. Statistical models express component and system performance by the use of time-to-failure (or repair) distributions. Parameters of the models need to be assigned, and system reliability theory is used to aggregate the information from these models into an overall reliability estimate, such as the PFD<sub>avg</sub>.</p>
<sec id="section3-1748006X12462780">
<title>Component failures</title>
<p>Failures of SIS elements may be classified as dangerous and safe failures. Dangerous failures can be further split into dangerous detected (DD) failures and dangerous undetected (DU) failures. DD failures are revealed (almost immediately) by diagnostic testing, while DU failures are only revealed by proof testing or in real demands.</p>
<p>Many of the formulas used for SIS reliability calculation cover only DU failures, under the assumption that DD failures have a negligible impact on the PFD<sub>avg</sub>. This assumption may be realistic if the diagnostic test interval is negligible and the EUC enters a safe state while the DD failure is repaired. Omitting DD failures when these assumptions are not fulfilled will lead to a biased PFD<sub>avg</sub> value.</p>
<p>A safe failure is a failure that does not prevent the execution of a SIF or leads to an unsafe state of the EUC. A spurious failure of a SIS element, such as a false alarm or a spurious valve closure, often takes the EUC to a safe state, owing to “fail-safe” design principles. Still, safe failures may have some undesired effects, such as downtime of the (production) system and excessive stresses to SIS elements.</p>
<p>Failures of SIS elements, be it dangerous or safe, can further be split into random hardware failures and systematic failures.</p>
<list id="list1-1748006X12462780" list-type="bullet">
<list-item><p>A <italic>random hardware failure</italic> is a failure, occurring at a random time, which results from one or more of the possible degradation mechanisms in the hardware.<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup></p></list-item>
<list-item><p>A <italic>systematic failure</italic> is a failure, related in a deterministic way to a certain cause, which can only be eliminated by a modification of the design or of the manufacturing process, operational procedures, documentation, or other relevant factors.<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup></p></list-item>
</list>
<p>An example of a random hardware failures is a valve that leaks owing to wear of the valve seat. An example of a systematic failure is a gas detector that is installed in an inappropriate location, for example close to a fan, such that it is not able to detect a gas release. The systematic failure is not easily detected during normal operation or regular proof testing and diagnostic testing. The probability of such failures is often very difficult to estimate.</p>
<p>A systematic failure is also called a <italic>functional failure</italic>, i.e. a failure where the item is still able to operate, but does not perform its specified function. A systematic failure is not caused by physical degradation and is therefore sometimes called a <italic>non-physical failure</italic>.</p>
<p>IEC 61508<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> requires only random hardware failures to be considered in PFD<sub>avg</sub> calculations, while systematic failures should be controlled and managed by a dedicated safety management program. The main argument for this approach is that systematic failures do not follow the same failure processes as random hardware failures. In principle, a systematic failure is a non-recurring event if the cause of failure is successfully identified and corrected. The standard<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> gives a number of requirements that shall reduce (or ideally prevent) the occurrence of systematic failures.</p>
<p>Several data collection exercises<sup><xref ref-type="bibr" rid="bibr19-1748006X12462780">19</xref>,<xref ref-type="bibr" rid="bibr20-1748006X12462780">20</xref></sup> have indicated that many SIS failures are systematic rather than random hardware failures. Based on such data collections, it is sometimes argued that also systematic failures, when studied en bloc, can be considered as random events as they tend to repeat themselves. Even if a systematic failure is corrected, similar types of failure seem to reoccur. One example is that even if a calibration procedure is improved to avoid a particular type of failure, the personnel may have established work habits that does not prevent the failure to reoccur. Consequently, two factors associated with the treatment of random and systematic failures are of interest in the discussion of uncertainty: (i) should or should not systematic failures be included in the SIS reliability estimation, and (ii) if systematic failures are included, to what extent is the assumption about their randomness adequate?</p>
<p>It may be remarked that the IEC 61508 approach will, because of the exclusion of systematic failures, inevitably, give a too-low and non-conservative value for the PFD<sub>avg</sub>. Some of the systematic failures, however, are manifested as common-cause failures (CCFs) and TIFs, which are partly accounted for in the PFD<sub>avg</sub> calculation.</p>
</sec>
<sec id="section4-1748006X12462780">
<title>CCFs</title>
<p>A CCF is a failure, which is the result of one or more events, causing concurrent failures of two or more separate channels in a multiple channel system, leading to system failure.<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> CCFs may occur because redundant channels have components of the same type, or because they have the same type of design deficiency or inadequate maintenance, or are located in the same area (and therefore subject to the same exposure). In a SIS, where redundancy is often introduced to enhance reliability, it is important to cater for such failures and this is also a well established practice. Related factors that create uncertainty are linked to the modeling of CCFs and the sparse access to data to support the models.</p>
</sec>
<sec id="section5-1748006X12462780">
<title>Test-independent failures</title>
<p>Proof testing is not always fully realistic, since a realistic test may be dangerous or give excessive stresses to the equipment. It is, for example, not relevant to fill a production room with toxic gas to test a gas detector. Instead a small amount of non-toxic gas is injected directly into the detector through a test-pipe. Such a test will reveal most DU failures, but some DU failures may also pass the test and remain undetected. Such failures are called TIF and were introduced as part of the PDS-method.<sup><xref ref-type="bibr" rid="bibr18-1748006X12462780">18</xref></sup> When a TIF is present, the system will not be as-good-as-new after a proof test.</p>
</sec>
<sec id="section6-1748006X12462780">
<title>Analysis process</title>
<p>In most cases, a SIS is modeled as a series system of three independent subsystems.</p>
<list id="list2-1748006X12462780" list-type="order">
<list-item><p>Input elements.</p></list-item>
<list-item><p>Logic solver.</p></list-item>
<list-item><p>Final elements.</p></list-item>
</list>
<p>Each subsystem is then analyzed separately.</p>
<p>Two different types of models are required: Component models and system models. Component models build on time-to-failure distributions and input parameters, such as failure rate, test interval, mean test-time, mean time to repair (MTTR), diagnostic coverage, and proof test coverage. System models describe the interactions between the various SIS elements within a subsystem. Some models are static (e.g. reliability block diagrams and fault trees), while others can describe dynamic features (e.g. Markov models and Petri nets). The models must again be supplemented by a suitable CCF model (e.g. beta-factor or multiple beta-factor model). A range of input parameters are required.</p>
</sec>
<sec id="section7-1748006X12462780">
<title>Assumptions and simplifications</title>
<p>All the models are subject to a number of assumptions and simplifications. Typical basic assumptions are given below (the assumptions may be slightly different for individual cases). The impact of these assumptions on the uncertainty need to be studied case by case.</p>
<list id="list3-1748006X12462780" list-type="bullet">
<list-item><p>All elements have constant failure rates (<inline-formula id="inline-formula14-1748006X12462780"><mml:math display="inline" id="math14-1748006X12462780"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:math></inline-formula>).</p></list-item>
<list-item><p>All elements are proof tested at the same time at regular proof test intervals (<inline-formula id="inline-formula15-1748006X12462780"><mml:math display="inline" id="math15-1748006X12462780"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula>).</p></list-item>
<list-item><p>All failures are revealed by the test.</p></list-item>
<list-item><p>The test time is negligible.</p></list-item>
<list-item><p>The repair time of a failure revealed by a proof test is negligible.</p></list-item>
<list-item><p>After a test/repair, all elements are as-good-as-new.</p></list-item>
<list-item><p>Common-cause failures can be adequately modeled by the standard beta-factor model (<inline-formula id="inline-formula16-1748006X12462780"><mml:math display="inline" id="math16-1748006X12462780"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula>).</p></list-item>
<list-item><p>No other types of dependency between elements are relevant.</p></list-item>
<list-item><p>DD failures are revealed immediately and a repair action is immediately initiated.</p></list-item>
<list-item><p>The system (EUC) is in a safe state when a DD failure is repaired.</p></list-item>
<list-item><p>Safe failures are not considered.</p></list-item>
<list-item><p>Systematic failures are not considered (but partly included in the estimate of the <inline-formula id="inline-formula17-1748006X12462780"><mml:math display="inline" id="math17-1748006X12462780"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula>-factor).</p></list-item>
<list-item><p>Human and organizational errors are not considered.</p></list-item>
<list-item><p>Maintenance errors are disregarded.</p></list-item>
<list-item><p>The SIS is not influenced by any factors outside (rather limited) physical boundaries of the SIS.</p></list-item>
</list>
</sec>
<sec id="section8-1748006X12462780">
<title>Conflicting objectives</title>
<p>A supplier may sign a contract with an end-user (e.g. an oil company) about a SIS for which a certain number of SIFs and associated SIL-requirements (e.g. SIL 3) have been specified with a basis in standards, such as IEC 61508. The supplier sometimes considers the job to be accomplished when the PFD<sub>avg</sub> estimate is within the constraints of the SIL requirement and the other requirements in relation to SIL, such as architectural constraints, have been met. The approximation formulas in IEC 61508 disregard systematic failures, but include the contribution from CCFs. This simplistic approach fulfills the requirements in IEC 61508, but may not give a realistic PFD<sub>avg</sub> estimate (i.e. the PFD<sub>avg</sub> that is later to be experienced in the operational phase). It can be argued that the avoidance of systematic failures is mainly in the hands of the end-user, since the end-user is often to “blame” for overlooking key requirements (that would have impacted the SIS design) and for introducing failures or not revealing failures during operation and maintenance. The supplier seldom aims towards a status as “world champion” in PFD<sub>avg</sub> calculations, and from the supplier’s perspective, it is of interest to fulfill the end-user’s requirements as fast and cheap as possible.</p>
<p>On the other hand, the end-user is responsible for the safety of the installation and should base decisions on a realistic (and usually conservative) estimate of the PFD<sub>avg</sub>. This will then require more realistic models and a more careful examination of the SIFs and their operating and environmental conditions.</p>
</sec>
</sec>
<sec id="section9-1748006X12462780">
<title>What is uncertainty?</title>
<sec id="section10-1748006X12462780">
<title>What do we mean by uncertainty?</title>
<p>Uncertainty is a common word in our daily parlance, but is used with different meanings in different contexts. Related to reliability and risk assessments, the interpretation of uncertainty is still debated.<sup><xref ref-type="bibr" rid="bibr21-1748006X12462780">21</xref>,<xref ref-type="bibr" rid="bibr22-1748006X12462780">22</xref></sup> According to our view, probabilities in risk and reliability assessment must be interpreted as subjective probabilities. This also applies for the PFD<sub>avg</sub>. We use the knowledge available to select appropriate models and input parameters, calculate a value for PFD<sub>avg</sub>, and call this value our PFD<sub>avg</sub> estimate. In this process, we are aware that we make a lot of simplifications and approximations that will influence the PFD<sub>avg</sub> estimate and make our estimate uncertain. As part of the SIS reliability analysis, we should assess this uncertainty and communicate our assessment to the decision maker. The objective of the remaining part of this article is to give guidance to analysts on issues to be aware of when assessing the uncertainty of the PFD<sub>avg</sub> estimate.</p>
<p>Uncertainty may stem from two main causes, natural variation and the lack of knowledge about the system or process. These categories of uncertainty are referred to as aleatory and epistemic uncertainty, respectively.<sup><xref ref-type="bibr" rid="bibr10-1748006X12462780">10</xref>,<xref ref-type="bibr" rid="bibr23-1748006X12462780">23</xref></sup></p>
<list id="list4-1748006X12462780" list-type="bullet">
<list-item><p><italic>Aleatory uncertainty</italic>: uncertainty arising from or associated with, the inherent, irreducible, and natural randomness of a system or process.</p></list-item>
<list-item><p><italic>Epistemic uncertainty</italic>: uncertainty owing to lack of knowledge about the performance of a system or process.</p></list-item>
</list>
<p>The epistemic uncertainty will be reduced when new knowledge becomes available, while the aleatory uncertainty cannot, in principle, be reduced.<sup><xref ref-type="bibr" rid="bibr24-1748006X12462780">24</xref></sup> However, several types of uncertainty that in the past were classified as aleatory, are now considered to be epistemic, indicating that the uncertainty classification is not fixed, but may vary as fundamental understanding of natural phenomena increases.<sup><xref ref-type="bibr" rid="bibr25-1748006X12462780">25</xref></sup> Some authors therefore take the stand that all uncertainty is epistemic.<sup><xref ref-type="bibr" rid="bibr21-1748006X12462780">21</xref>,<xref ref-type="bibr" rid="bibr26-1748006X12462780">26</xref></sup> Despite its limitations, the classification gives a conceptual allocation of uncertainties into controllable and not so easily controllable categories.</p>
</sec>
<sec id="section11-1748006X12462780">
<title>How do we assess uncertainty?</title>
<p>Statistical models are used as a basis for estimating risk and reliability parameters, for example, to determine the frequency of accident scenarios in a probabilistic risk assessment (PRA) and the reliability of a SIS. By using statistical models for this purpose, we acknowledge the existence of uncertainty.<sup><xref ref-type="bibr" rid="bibr10-1748006X12462780">10</xref></sup> Epistemic uncertainty as a source of uncertainty is related to how well our models and data are able to assess the behavior of the system. This uncertainty may be addressed by an additional process, i.e. uncertainty assessment. In this article, we use uncertainty assessment to account for the epistemic uncertainty of SIS reliability.</p>
</sec>
<sec id="section12-1748006X12462780">
<title>Complexity</title>
<p>New generations of SISs are generally more complex than previous generations. This is especially related to the programmable parts (i.e. logic solvers and smart sensors), the number of components involved, and the interaction with other systems. The complexity makes it difficult for the analyst to fully understand the behavior of the system (both technical and operational), to identify all failure modes, and to establish adequate models. This issue is addressed in IEC 61508<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> where it is distinguished between type A and type B subsystems, where type B subsystems are more complex and therefore need more careful considerations.</p>
<p>Johansen and Rausand<sup><xref ref-type="bibr" rid="bibr27-1748006X12462780">27</xref></sup> list a set of complexity attributes related to (a) the physical system, and (b) the operation of the system. These attributes can be used to indicate the complexity and be a guide to the awareness we should have related to complexity.</p>
</sec>
<sec id="section13-1748006X12462780">
<title>Uncertainty classification</title>
<p>The nuclear industry<sup><xref ref-type="bibr" rid="bibr10-1748006X12462780">10</xref></sup> distinguishes between three sources of epistemic uncertainty: completeness uncertainty, model uncertainty, and parameter uncertainty. The same categories are adopted in this article and each of them is discussed in the following.</p>
<p>There is no clear cut borderline between the three categories of uncertainty, and attempts to reduce the uncertainty within one category may influence the uncertainty in another category. For example, the choice of a multi-parameter distribution instead of an exponential distribution may reduce the model uncertainty, but more parameter uncertainty may be introduced. This is also the case with advanced CCF models. The categorization is therefore pragmatic.</p>
</sec>
</sec>
<sec id="section14-1748006X12462780">
<title>Completeness uncertainty</title>
<p>Completeness uncertainty is about factors that are not properly included in the analysis. Failing to include all relevant factors in the analysis will give an incorrect estimate of the reliability, even if the data and model selection is “perfect”. We may distinguish between the following.<sup><xref ref-type="bibr" rid="bibr10-1748006X12462780">10</xref></sup></p>
<list id="list5-1748006X12462780" list-type="order">
<list-item><p><italic>Known completeness uncertainty</italic>, which is owing to factors that are known, but deliberately not included. Reasons for exclusion may be lack of understanding the limitations of the system in its operating context, time or cost constraints, lack of models, lack of data to support the models, or lack of competence in using the models. The known completeness uncertainty reflects assumptions and simplifications that have been made in a trade-off of costs, available resources, competence of analysts, and the state of knowledge about the system and its operating environment. The number of exclusions and their impacts (need to be assessed) may be a measure of the level of uncertainty in a SIS reliability estimate.</p></list-item>
<list-item><p><italic>Unknown completeness uncertainty</italic>, which is owing to factors that are not known or not identified. The factors are truly unknown, and are therefore difficult to account for or make judgments about. The unknown completeness is problematic, as its contribution is invisible. However, indirect factors, i.e. factors that may impact to what extent “we don’t know”, may give an indication of contribution. The use of new technology, or the use of existing technology in new application areas may suggest that the contribution from unknown completeness uncertainty is high compared with when proven technology is used. The classification in <xref ref-type="table" rid="table2-1748006X12462780">Table 2</xref> of the newness of technology may here be useful. In <xref ref-type="table" rid="table2-1748006X12462780">Table 2</xref>,<sup><xref ref-type="bibr" rid="bibr28-1748006X12462780">28</xref></sup> the newness is classified in four categories, ranging from 1 to 4, where 4 represents the most new and unfamiliar technology. The SIS technology is developing fast, and new and more advanced logic solvers and smart sensors are launched and implemented at high pace.</p></list-item>
</list>
<table-wrap id="table2-1748006X12462780" position="float">
<label>Table 2.</label>
<caption>
<p>The degree of newness of technology.<sup><xref ref-type="bibr" rid="bibr28-1748006X12462780">28</xref></sup></p>
</caption>
<graphic alternate-form-of="table2-1748006X12462780" xlink:href="10.1177_1748006X12462780-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="left" colspan="3">Level of technology maturity<hr/></th>
</tr>
<tr>
<th align="left">Experience with the operating condition</th>
<th align="left">Proven</th>
<th align="left">Limited field history or not used by company/user</th>
<th align="left">New or unproven</th>
</tr>
</thead>
<tbody>
<tr>
<td>Previous experience</td>
<td>1</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>No experience by company/user</td>
<td>2</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>No industry experience</td>
<td>3</td>
<td>4</td>
<td>4</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Additional factors/issues that may influence the (known and unknown) completeness uncertainty include the following.</p>
<list id="list6-1748006X12462780" list-type="bullet">
<list-item><p>Interactions with external systems: a SIS is integrated in a bigger system (e.g. the process) and may interact with the EUC and with other safety and control systems. These interactions may influence (enhance or reduce) the SIS reliability.</p></list-item>
<list-item><p>Failure mechanisms that are not known and/or not catered for: such failure mechanisms may be related to stresses during operation and maintenance, and to environmental conditions. Failure mechanisms may also be forgotten owing to inadequate failure analysis, e.g. as part of a FMECA.</p></list-item>
<list-item><p>Side-effects of diagnostic testing: diagnostic testing is a means to timely reveal dangerous failures, and thereby increase the SIS reliability. Whether or not such a testing leads to side-effects is seldom evaluated.</p></list-item>
<list-item><p>Placement of input elements (e.g. sensors, transmitters): installing input elements at places other than where they should may expose the components to different environmental stresses, and hence the components may get a different reliability behavior.</p></list-item>
<list-item><p>Testing and maintenance strategies: complex testing and maintenance strategies are difficult to model in SIS reliability analyses and are therefore simplified in the calculations. This may increase the completeness uncertainty.</p></list-item>
<list-item><p>Human and organizational factors: several studies have indicated that human and organizational factors are strongly influencing the SIS reliability,<sup><xref ref-type="bibr" rid="bibr29-1748006X12462780">29</xref>,<xref ref-type="bibr" rid="bibr30-1748006X12462780">30</xref></sup> but such factors are usually not included in the analyses.</p></list-item>
</list>
<p>Including these factors generally leads to a higher PFD<sub>avg</sub> estimate, but the amount of PFD<sub>avg</sub> increases the need to be assessed case by case.</p>
</sec>
<sec id="section15-1748006X12462780">
<title>Model uncertainty</title>
<p>Model uncertainty arises from the fact that any model, conceptual or mathematical, will inevitably be a simplification of the reality it is designed to represent.<sup><xref ref-type="bibr" rid="bibr22-1748006X12462780">22</xref>,<xref ref-type="bibr" rid="bibr24-1748006X12462780">24</xref></sup> Several authors have discussed the choice of models for SIS reliability analysis and given recommendations. One of the first articles with this purpose is Rouvroye and Brombacher.<sup><xref ref-type="bibr" rid="bibr31-1748006X12462780">31</xref></sup></p>
<sec id="section16-1748006X12462780">
<title>Component models</title>
<p>Almost all SIS reliability analyses assume that the elements have constant failure rates. This assumption implies that the elements do not show any deterioration and that they are as-good-as-new as long as they are functioning. This assumption may be adequate for electronic and some electrical items, and for mechanical items that are regularly maintained and upgraded/replaced if deterioration is revealed.</p>
<p>In some applications, such as in subsea oil and gas production systems, the items are left alone for a long time (e.g. 8 years) without any type of preventive maintenance. Some of these are mechanical items that are exposed to sea water, high pressure, and a corrosive environment. This indicates that the items will deteriorate with time and that the constant failure rate assumption is not adequate. If, for example, the life distribution of a component is Weibull distributed with a shape parameter that is greater than one, and we use a constant failure rate model, we will over-estimate the probability of failure (i.e. be conservative) in the first part of the item’s life and under-estimate the probability of failure in the last part of the item’s life. A deteriorating item will not be as-good-as-new after a successful proof test.<sup><xref ref-type="bibr" rid="bibr32-1748006X12462780">32</xref></sup> This problem is very seldom catered for in SIS reliability analyses.</p>
<p>Component models must also take into consideration issues related to testing and maintenance, such as the coverage of diagnostic tests, the coverage of proof tests, the possible use of partial stroke testing of shutdown valves,<sup><xref ref-type="bibr" rid="bibr33-1748006X12462780">33</xref></sup> and several more.</p>
</sec>
<sec id="section17-1748006X12462780">
<title>System models and methods</title>
<p>Several methods are used to model the interactions between SIS elements, and calculate PFD<sub>avg</sub>. The most common are:</p>
<list id="list7-1748006X12462780" list-type="alpha-lower">
<list-item><p>simple approximation formulas;<sup><xref ref-type="bibr" rid="bibr34-1748006X12462780">34</xref></sup></p></list-item>
<list-item><p>IEC 61508 approximation formulas;<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup></p></list-item>
<list-item><p>the PDS method;<sup><xref ref-type="bibr" rid="bibr18-1748006X12462780">18</xref></sup></p></list-item>
<list-item><p>reliability block diagrams;</p></list-item>
<list-item><p>fault tree analysis;</p></list-item>
<list-item><p>Markov methods;</p></list-item>
<list-item><p>Petri nets.<sup><xref ref-type="bibr" rid="bibr35-1748006X12462780">35</xref></sup></p></list-item>
</list>
<p>These methods can be used to analyze both low-demand and high-demand SIFs. The sequence in which the methods are listed indicates their applicability to an increasing system complexity. The first five methods consider the SIS to be a static system without any dynamic properties, while Markov methods and Petri nets can incorporate some dynamic effects owing to testing and maintenance.</p>
<p>It is often claimed<sup><xref ref-type="bibr" rid="bibr36-1748006X12462780">36</xref></sup> that CCFs are more crucial for the SIS reliability than independent element failures. It is therefore important how CCFs are incorporated into the methods. In some methods, we may choose among various implicit CCF models,<sup><xref ref-type="bibr" rid="bibr37-1748006X12462780">37</xref></sup> while other methods come with a dedicated CCF model (the PDS method). Some methods are mainly based on explicit modeling of CCFs, rather than implicit. To choose the “best” CCF model is a difficult task and the SIS analysts therefore too often select the simplest possible model—the beta-factor model. The beta-factor model is listed as an adequate CCF model in IEC 61508<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> and the choice of this model is therefore compatible with the standard; and in most cases, leads to conservative PFD<sub>avg</sub> estimates.</p>
<p>In most cases, other dependencies than CCFs are not covered in the SIS reliability analyses. Among such dependencies are cascading failures and negative dependencies. A special challenge is related to modeling dependencies that are partly within and partly between SIS channels and modules. As discussed by Lundteigen and Rausand,<sup><xref ref-type="bibr" rid="bibr36-1748006X12462780">36</xref></sup> these dependencies can be an important source of model uncertainty.</p>
<p>Dynamic effects may be related to phased mission, the effect of DD failures and safe failures, and testing procedures. An example of such a testing procedure is to carry out a full proof test of similar channels each time a DD failure is restored—either in addition to the planned proof test or by postponing the planned proof test.<sup><xref ref-type="bibr" rid="bibr38-1748006X12462780">38</xref></sup> Another example is to supplement the proof testing with regular inspections (e.g. once a week) where some failure possibilities may be partly examined. The inspection may, for example, involve moving a valve slightly to check that it is not stuck.</p>
</sec>
<sec id="section18-1748006X12462780">
<title>Model selection</title>
<p>The most adequate model/method is determined by the assumptions and simplifications. As long as the analyst is competent and familiar with the different methods, it does not matter very much if they choose the most simple method that fits the assumptions or they choose a more advanced method. Liu et al.<sup><xref ref-type="bibr" rid="bibr38-1748006X12462780">38</xref></sup> analyze the same SIS with different methods and find that the reliability estimates are similar. The approximation formulas give the most conservative estimates and the conservativeness decreases with increasing method complexity—which is expected since the complex methods are based on more detailed modeling. Too few and simple systems are, however, analyzed to give any firm and general conclusions. Rouvroye and Brombacher<sup><xref ref-type="bibr" rid="bibr31-1748006X12462780">31</xref></sup> and Rouvroye and van den Bliek<sup><xref ref-type="bibr" rid="bibr39-1748006X12462780">39</xref></sup> find that different methods may result in different SILs. Their findings are in conflict with the conclusion in Liu et al.<sup><xref ref-type="bibr" rid="bibr38-1748006X12462780">38</xref></sup> and may be caused by the differences in the level of complexity of the systems that were studied.</p>
<p>The causes (assumptions and simplifications) of model uncertainty are the same as for known completeness uncertainty. It is, therefore, not obvious that uncertainties can uniquely be classified as model uncertainty or known completeness uncertainty. Some sources, such as the PDS method,<sup><xref ref-type="bibr" rid="bibr18-1748006X12462780">18</xref></sup> do not differentiate between completeness uncertainty and model uncertainty, but use the term model uncertainty to represent both.</p>
<p>Model uncertainty has been studied by several authors. Among these are Zio and Apostolakis<sup><xref ref-type="bibr" rid="bibr40-1748006X12462780">40</xref></sup> and Droguett and Mosleh.<sup><xref ref-type="bibr" rid="bibr41-1748006X12462780">41</xref></sup> The model uncertainty issue is not significant in SIS reliability analysis, as we account for the uncertainty by the assumptions and simplifications (the main causes of model uncertainty) as part of known completeness uncertainty.</p>
</sec>
</sec>
<sec id="section19-1748006X12462780">
<title>Parameter uncertainty</title>
<p>Parameter uncertainty is related to uncertainty of the parameter values used in the quantification.<sup><xref ref-type="bibr" rid="bibr10-1748006X12462780">10</xref></sup> In the current context, these parameters comprise component failure rates, mean repair times, common-cause beta-factors, test coverage factors, and so on. Failure rates are available in data sources, such as OREDA<sup><xref ref-type="bibr" rid="bibr19-1748006X12462780">19</xref></sup> and Hauge and Onshus.<sup><xref ref-type="bibr" rid="bibr20-1748006X12462780">20</xref></sup> (A survey of reliability data sources is given on <ext-link ext-link-type="uri" xlink:href="http://www.ntnu.edu/ross/info/data">http://www.ntnu.edu/ross/info/data</ext-link>.) Estimates of some of the other parameters may be found in Hauge and Onshus,<sup><xref ref-type="bibr" rid="bibr20-1748006X12462780">20</xref></sup> partly based on expert judgment.</p>
<p>Several uncertainties are related to the provision of input parameters, and we discuss some of these.</p>
<sec id="section20-1748006X12462780">
<title>Failure rates</title>
<p>A SIS is often a vital safety barrier and is designed to be highly reliable. Few failures are, therefore, expected to occur even during a long operating period, and the failure rate estimates based on experience data become rather uncertain. Another problem is that the failure rates we find, for example, in OREDA,<sup><xref ref-type="bibr" rid="bibr19-1748006X12462780">19</xref></sup> are based on data from components that were installed several years (often 10–15 years) before the data collection exercise was terminated. Owing to the rapid technological development of, for example, smart sensors, the failure rate estimates may not at all represent the technology that will be used in the new SIS.</p>
<p>The operational and environmental conditions of the elements used in a new SIS are sometimes very different from the conditions under which the data were collected. For electronic components, this issue is handled by the approach outlined in MIL-HDBK-217F.<sup><xref ref-type="bibr" rid="bibr42-1748006X12462780">42</xref></sup> For more complicated equipment, such as SIS elements, the approach in MIL-HDBK-217F is too simple and we have to use more advanced approaches, such as the one described by Brissaud et al.<sup><xref ref-type="bibr" rid="bibr43-1748006X12462780">43</xref></sup></p>
<p>OREDA<sup><xref ref-type="bibr" rid="bibr19-1748006X12462780">19</xref></sup> and Hauge and Onshus<sup><xref ref-type="bibr" rid="bibr20-1748006X12462780">20</xref></sup> are both based on recorded maintenance actions. These data sources will, therefore, not contain all safe failures, since some of these can be reset without any formal maintenance action.</p>
</sec>
<sec id="section21-1748006X12462780">
<title>Common cause failure rates</title>
<p>Very few data sources are available for CCF rates. The only exception is for the nuclear power industry<sup><xref ref-type="bibr" rid="bibr44-1748006X12462780">44</xref></sup> that has established the International Common-Cause Data Exchange database.<sup><xref ref-type="bibr" rid="bibr45-1748006X12462780">45</xref></sup> The authors are not aware of any similar initiative for any non-nuclear sector. CCF rates are highly dependent on the physical conditions and the operational and environmental conditions and it is therefore difficult to claim that a CCF rate in one installation will be similar to the CCF rate in another installation.</p>
<p>IEC61508<sup><xref ref-type="bibr" rid="bibr1-1748006X12462780">1</xref></sup> has, therefore, chosen another approach where the factor <inline-formula id="inline-formula18-1748006X12462780"><mml:math display="inline" id="math18-1748006X12462780"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula> of the beta-factor model is determined by a checklist in Part 6 of the standard. The checklist is based on around 40 questions, is generic, and is intended for all types of SIS. An immediate observation is that the number of questions related to human and organizational factors is low compared with the importance of these factors. This issue is further discussed by Rahimi et al.<sup><xref ref-type="bibr" rid="bibr29-1748006X12462780">29</xref></sup> Several other approaches may be used to determine the <inline-formula id="inline-formula19-1748006X12462780"><mml:math display="inline" id="math19-1748006X12462780"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula>-factor. Among these is the unified partial method (UPM) that has been extended by using influence diagrams.<sup><xref ref-type="bibr" rid="bibr46-1748006X12462780">46</xref></sup></p>
</sec>
<sec id="section22-1748006X12462780">
<title>Test coverage</title>
<p>Several authors<sup><xref ref-type="bibr" rid="bibr33-1748006X12462780">33</xref>,<xref ref-type="bibr" rid="bibr47-1748006X12462780">47</xref><xref ref-type="bibr" rid="bibr48-1748006X12462780"/>–<xref ref-type="bibr" rid="bibr49-1748006X12462780">49</xref></sup> have discussed possible approaches to determine the test coverage—mainly based on expert judgment. These efforts are, however, limited and little guidance is available on how to estimate the test coverage factor, both for diagnostic and proof testing, and the values used in many SIS reliability analyses are therefore, at best, guesstimates.</p>
</sec>
<sec id="section23-1748006X12462780">
<title>Uncertainty propagation</title>
<p>Parameter uncertainty is the most studied type of uncertainty<sup><xref ref-type="bibr" rid="bibr22-1748006X12462780">22</xref>,<xref ref-type="bibr" rid="bibr24-1748006X12462780">24</xref>,<xref ref-type="bibr" rid="bibr50-1748006X12462780">50</xref></sup> and is usually analyzed by Monte Carlo simulation. An uncertainty distribution—sometimes expressed by an error factor—is given for the main parameters, a value from each distribution is chosen at random and an output value is generated. This is repeated a high number of times and we say that the uncertainty is propagated through the model to give an uncertainty distribution of the output measure of interest (in our case the PFD<sub>avg</sub>).<sup><xref ref-type="bibr" rid="bibr51-1748006X12462780">51</xref></sup> Such a simulation module is a separate module of many computer programs for reliability analysis, such as fault tree analysis. Some authors also use an approach based on fuzzy number arithmetic<sup><xref ref-type="bibr" rid="bibr52-1748006X12462780">52</xref></sup> and Dempster-Shafer theory<sup><xref ref-type="bibr" rid="bibr53-1748006X12462780">53</xref></sup> to propagate the uncertainty of the parameters.</p>
</sec>
</sec>
<sec id="section24-1748006X12462780">
<title>Concluding remarks</title>
<p>This article has discussed a number of issues related to the uncertainty of the PFD<sub>avg</sub> estimate of a SIS operating in low-demand mode. The authors believe that this type of discussion is important, as it may frame future development of methods and models for treating uncertainty in reliability analyses. It is argued that the three perspectives of uncertainty contributions, the completeness, model, and parameter uncertainty, are very useful for this purpose and a thorough discussion about the possible ways to treat them in the analyses have been made for each category. It is argued that uncertainty assessment is an important part of a SIS reliability analysis and that the uncertainty should be communicated to the relevant decision makers together with the PFD<sub>avg</sub> estimate.</p>
<p>The persons who are best capable of assessing the uncertainty is the analyst who knows how the various attributes of the SIS are implemented in the models and in the analyses. The authors do not believe that it is possible to present any objective estimate of the uncertainty, so the analyst has to judge the different contributors to the uncertainty and present their best “degree of belief”.</p>
<p>Of the three categories, completeness uncertainty is judged to be the most important. We realize that parameter uncertainty may lead to different SIL ratings, but the decision maker normally has a good picture of this category of uncertainty. It is those we do not know that we fail to consider in decision making. This completeness category is split into two sub-categories; known and unknown completeness uncertainty. For the known completeness uncertainty, the analyst is aware of the relevant issues and has deliberately excluded them from the analysis. This type of simplifications can, in some cases, be compensated for by using conservative approximations. For the unknown completeness uncertainty, the analyst does not know what they do not know and does not include. This uncertainty is most prevalent for new technology and partly known technology in new areas of application. The analyst may be warned about the possible uncertainty by using the classification of newness proposed by Det Norske Veritas (DNV).<sup><xref ref-type="bibr" rid="bibr28-1748006X12462780">28</xref></sup></p>
<p>Model uncertainty is linked to the completeness uncertainty. The choice of model/method is strongly dependent on the assumptions and simplifications made. If the analyst is competent and familiar with the limitations of the various methods, it is not very important which method they choose, as long as the method fits to the assumptions made.</p>
<p>As for parameter uncertainty, the technological development in the SIS area is running fast, and the failure rate estimates in data sources may, therefore, be outdated. Another issue is that the failure rate estimates may not fit to the current operational context and we may need to extrapolate the estimates from the known to the new application, this is also discussed in Sallak et al.<sup><xref ref-type="bibr" rid="bibr12-1748006X12462780">12</xref></sup> An approach for this purpose is outlined by Brissaud et al.<sup><xref ref-type="bibr" rid="bibr43-1748006X12462780">43</xref></sup></p>
<p>An important area of further research is to develop new frameworks and methods that integrate uncertainty assessment with SIS reliability analyses. Current approaches that focus primarily on the treatment of parameter uncertainty are not sufficient, as they do not include other, and sometimes more important, sources of uncertainty. In fact, current approaches in the literature may be a false comfort and give inappropriate guidance in the decision making about SIS design and risk management of facilities in which the SIS is installed.</p>
</sec>
</body>
<back>
<ack><p>The authors would like to thank the reviewers of this article for well considered and useful inputs and comments.</p></ack>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial of not-for-profit sectors.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1748006X12462780">
<label>1.</label>
<citation citation-type="book">
<collab>IEC 61508</collab>. <article-title>Functional safety of electrical/electronic/programmable electronic safety-related systems. Part 1-7</article-title>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>International Electrotechnical Commission</publisher-name>, <year>2010</year>.</citation>
</ref>
<ref id="bibr2-1748006X12462780">
<label>2.</label>
<citation citation-type="book">
<collab>IEC 61511</collab>. <article-title>Functional safety: safety instrumented systems for the process industry sector. Part 1-3</article-title>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>International Electrotechnical Commission</publisher-name>, <year>2003</year>.</citation>
</ref>
<ref id="bibr3-1748006X12462780">
<label>3.</label>
<citation citation-type="book">
<collab>IEC 61513</collab> <article-title>Nuclear power plants – instrumentation and control for systems important to safety – general requirements for systems</article-title>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>International Electrotechnical Commission</publisher-name>, <year>2004</year>.</citation>
</ref>
<ref id="bibr4-1748006X12462780">
<label>4.</label>
<citation citation-type="book">
<collab>ISO 26262</collab> <article-title>Road vehicles – functional safety</article-title>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>International Electrotechnical Commission</publisher-name>, <year>2011</year>.</citation>
</ref>
<ref id="bibr5-1748006X12462780">
<label>5.</label>
<citation citation-type="book">
<collab>IEC 62278</collab> <article-title>Railway applications – specification and demonstration of reliability, availability, maintainability and safety (RAMS)</article-title>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>International Electrotechnical Commission</publisher-name>, <year>2002</year>.</citation>
</ref>
<ref id="bibr6-1748006X12462780">
<label>6.</label>
<citation citation-type="book">
<collab>IEC 62061</collab> <article-title>Safety of machinery – functional safety of safety-related electrical, electronic and programmable electronic control systems</article-title>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>International Electrotechnical Commission</publisher-name>, <year>2005</year>.</citation>
</ref>
<ref id="bibr7-1748006X12462780">
<label>7.</label>
<citation citation-type="book">
<collab>IEC 60050-191</collab> <article-title>International electrotechnical vocabulary. Chapter 191: dependability and quality of service</article-title>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>International Electrotechnical Commission</publisher-name>, <year>1990</year>.</citation>
</ref>
<ref id="bibr8-1748006X12462780">
<label>8.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>De Rocquigny</surname><given-names>E</given-names></name>
<name><surname>Devictor</surname><given-names>N</given-names></name>
<name><surname>Tarantola</surname><given-names>S</given-names></name>
</person-group>. <source>Uncertainty in industrial practice: a guide to quantitative uncertainty management</source>. <publisher-loc>Chichester, UK</publisher-loc>: <publisher-name>Wiley</publisher-name>, <year>2008</year>.</citation>
</ref>
<ref id="bibr9-1748006X12462780">
<label>9.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mosleh</surname><given-names>A</given-names></name>
<name><surname>Siu</surname><given-names>N</given-names></name>
<name><surname>Smidts</surname><given-names>C</given-names></name><etal/>
</person-group>. <source>Model uncertainty: its characterization and quantification</source>. <publisher-loc>Washington DC</publisher-loc>: <publisher-name>US Nuclear Regulatory Commission</publisher-name>, <edition>2nd ed.</edition> <year>1994</year>.</citation>
</ref>
<ref id="bibr10-1748006X12462780">
<label>10.</label>
<citation citation-type="book">
<collab>NUREG 1885</collab>. <article-title>Guidance on the treatment of uncertainties associated with PRAs in risk-informed decision</article-title>. Technical report, <publisher-loc>Washington DC</publisher-loc>: <publisher-name>US Nuclear Regulatory Commission</publisher-name>, <year>2009</year>.</citation>
</ref>
<ref id="bibr11-1748006X12462780">
<label>11.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Winkler</surname><given-names>RL</given-names></name>
</person-group>. <article-title>Uncertainty in probabilistic risk assessment</article-title>. <source>Rel Engng Sys Saf</source> <year>1999</year>; <volume>54</volume>: <fpage>127</fpage>–<lpage>132</lpage>.</citation>
</ref>
<ref id="bibr12-1748006X12462780">
<label>12.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sallak</surname><given-names>M</given-names></name>
<name><surname>Simon</surname><given-names>C</given-names></name>
<name><surname>Aubry</surname><given-names>J-F</given-names></name>
</person-group>. <article-title>A fuzzy probabilistic approach for determining safety intergrity level</article-title>. <source>IEEE Trans Fuzzy Sys</source> <year>2008</year>; <volume>16</volume>: <fpage>239</fpage>–<lpage>248</lpage>.</citation>
</ref>
<ref id="bibr13-1748006X12462780">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wang</surname><given-names>Y</given-names></name>
<name><surname>West</surname><given-names>HH</given-names></name>
<name><surname>Mannan</surname><given-names>MS</given-names></name>
</person-group>. <article-title>The impact of data uncertainty in determining safety intergrity level</article-title>. <source>Process Saf Environm Protection</source> <year>2004</year>; <volume>82</volume>: <fpage>393</fpage>–<lpage>397</lpage>.</citation>
</ref>
<ref id="bibr14-1748006X12462780">
<label>14.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brissaud</surname><given-names>F</given-names></name>
<name><surname>Barros</surname><given-names>A</given-names></name>
<name><surname>Bérenguer</surname><given-names>C</given-names></name>
</person-group>. <article-title>Handling parameter and model uncertainties by continuous gates in fault tree analyses</article-title>. <source>Proc IMechE, Part O: J Risk and Reliability</source> <year>2010</year>; <volume>224</volume>: <fpage>253</fpage>–<lpage>265</lpage>.</citation>
</ref>
<ref id="bibr15-1748006X12462780">
<label>15.</label>
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Janbu</surname><given-names>AF</given-names></name>
</person-group>. <source>Treatment of uncertainty in reliability assessment of safety instrumented systems</source>. Master’s thesis, <publisher-name>Norwegian University of Science and Technology</publisher-name>, <publisher-loc>Trondheim, Norway</publisher-loc>, <year>2009</year>.</citation>
</ref>
<ref id="bibr16-1748006X12462780">
<label>16.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jin</surname><given-names>H</given-names></name>
<name><surname>Lundteigen</surname><given-names>MA</given-names></name>
<name><surname>Rausand</surname><given-names>M</given-names></name>
</person-group>. <article-title>Uncertainty assessment of reliability estimates for safety instrumented systems</article-title>. In: <source>Advances in safety, reliability and risk management ESREL 2011</source> (ed. <person-group person-group-type="editor">
<name><surname>Guedes Soares</surname><given-names>C.</given-names></name>
</person-group>), <publisher-name>CRC Press</publisher-name>, <publisher-loc>London</publisher-loc>: <fpage>2213</fpage>–<lpage>2221</lpage>.</citation>
</ref>
<ref id="bibr17-1748006X12462780">
<label>17.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hamada</surname><given-names>MS</given-names></name>
<name><surname>Wilson</surname><given-names>AGSRC</given-names></name>
<name><surname>Martz</surname><given-names>HF</given-names></name>
</person-group>. <source>Bayesian reliability</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2008</year>.</citation>
</ref>
<ref id="bibr18-1748006X12462780">
<label>18.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hauge</surname><given-names>S</given-names></name>
<name><surname>Lundteigen</surname><given-names>MA</given-names></name>
<name><surname>Hokstad</surname><given-names>P</given-names></name><etal/>
</person-group>. <article-title>Reliability prediction method for safety instrumented systems – PDS method handbook</article-title>. SINTEF report A13503, <publisher-name>SINTEF Safety Research</publisher-name>, <publisher-loc>Trondheim, Norway</publisher-loc>, <year>2010</year>.</citation>
</ref>
<ref id="bibr19-1748006X12462780">
<label>19.</label>
<citation citation-type="book">
<collab>OREDA</collab>. <source>OREDA reliability data</source>. OREDA Participants, Available from: <publisher-name>Det Norske Veritas</publisher-name>, <publisher-loc>NO 1322 Høvik, Norway</publisher-loc>, <edition>5th ed.</edition>, <year>2009</year>.</citation>
</ref>
<ref id="bibr20-1748006X12462780">
<label>20.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hauge</surname><given-names>S</given-names></name>
<name><surname>Onshus</surname><given-names>T</given-names></name>
</person-group>. <article-title>Reliability data for safety instrumented systems – PDS data handbook</article-title>. SINTEF report A13502, <publisher-name>SINTEF Safety Research</publisher-name>, <publisher-loc>Trondheim, Norway</publisher-loc>, <year>2010</year>.</citation>
</ref>
<ref id="bibr21-1748006X12462780">
<label>21.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Aven</surname><given-names>T</given-names></name>
</person-group>. <source>Risk analysis: assessing uncertainties beyond expected values and probabilities</source>. <publisher-loc>Chichester, UK</publisher-loc>: <publisher-name>Wiley</publisher-name>, <year>2008</year>.</citation>
</ref>
<ref id="bibr22-1748006X12462780">
<label>22.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rausand</surname><given-names>M</given-names></name>
</person-group>. <source>Risk assessment; theory, methods, and applications</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>Wiley</publisher-name>, <year>2011</year>.</citation>
</ref>
<ref id="bibr23-1748006X12462780">
<label>23.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Parry</surname><given-names>GW</given-names></name>
</person-group>. <article-title>The characterization of uncertainty in probabilistic risk assessments of complex systems</article-title>. <source>Rel Engng Sys Saf</source> <year>1996</year>; <volume>54</volume>: <fpage>119</fpage>–<lpage>126</lpage>.</citation>
</ref>
<ref id="bibr24-1748006X12462780">
<label>24.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Abrahamsson</surname><given-names>M</given-names></name>
</person-group>. <source>Uncertainty in quantitative risk analysis - characterisation and methods of treatment</source>. PhD thesis, <publisher-name>Department of Fire Safety Engineering</publisher-name>, <publisher-loc>Lund University, Lund, Sweden</publisher-loc>, <year>2002</year>.</citation>
</ref>
<ref id="bibr25-1748006X12462780">
<label>25.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Faber</surname><given-names>M</given-names></name>
</person-group>. <article-title>On the treatment of uncertainties and probabilities in engineering decision analysis</article-title>. <source>J Offshore Mechanics and Arctic Engng</source> <year>2005</year>; <volume>127</volume>: <fpage>243</fpage>–<lpage>248</lpage>.</citation>
</ref>
<ref id="bibr26-1748006X12462780">
<label>26.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kieureghian</surname><given-names>A</given-names></name>
<name><surname>Ditlevsen</surname><given-names>O</given-names></name>
</person-group>. <article-title>Aleatory or epistemic? Does it matter?</article-title> <source>Structural Safety</source> <year>2009</year>; <volume>31</volume>: <fpage>102</fpage>–<lpage>112</lpage>.</citation>
</ref>
<ref id="bibr27-1748006X12462780">
<label>27.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Johansen</surname><given-names>IL</given-names></name>
<name><surname>Rausand</surname><given-names>M</given-names></name>
</person-group>. <article-title>Complexity in risk assessment of sociotechnical systems</article-title>. In: <conf-name>PSAM11 &amp; ESREL 2012</conf-name> <conf-date>June 25–29</conf-date>, <year>2012</year> (Proceedings from this conference has yet not been published), <conf-loc>Helsinki, Finland</conf-loc>, <year>2012</year>.</citation>
</ref>
<ref id="bibr28-1748006X12462780">
<label>28.</label>
<citation citation-type="book">
<collab>DNV Qualification of new technoloy</collab>. <article-title>Recommended Practice DNV-RP-A203</article-title>, <publisher-name>Det Norske Veritas</publisher-name>, <publisher-loc>Høvik, Norway</publisher-loc>, <year>2011</year>.</citation>
</ref>
<ref id="bibr29-1748006X12462780">
<label>29.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rahimi</surname><given-names>M</given-names></name>
<name><surname>Rausand</surname><given-names>M</given-names></name>
<name><surname>Lundteigen</surname><given-names>MA</given-names></name>
</person-group>. <article-title>Management factors that influence common-cause failures of safety-instrumented systems in the operational phase</article-title>. In: <source>Advances in Safety, Reliability, and Risk Management, ESREL 2011</source> (ed. <person-group person-group-type="editor">
<name><surname>Guedes Soares</surname><given-names>C.</given-names></name>
</person-group>), pp. <fpage>2036</fpage>–<lpage>2044</lpage>. <publisher-loc>London</publisher-loc>, <publisher-name>CRC Press</publisher-name>.</citation>
</ref>
<ref id="bibr30-1748006X12462780">
<label>30.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schönbeck</surname><given-names>M</given-names></name>
<name><surname>Rausand</surname><given-names>M</given-names></name>
<name><surname>Rouvroye</surname><given-names>J</given-names></name>
</person-group>. <article-title>Human and organisational factors in the operational phase of safety instrumented systems: A new approach</article-title>. <source>Safety Sci</source> <year>2010</year>; <volume>48</volume>(<issue>3</issue>): <fpage>310</fpage>–<lpage>318</lpage>.</citation>
</ref>
<ref id="bibr31-1748006X12462780">
<label>31.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rouvroye</surname><given-names>JL</given-names></name>
<name><surname>Brombacher</surname><given-names>AC</given-names></name>
</person-group>. <article-title>New quantitative safety standards: different techniques, different results</article-title>. <source>Rel Engng Sys Saf</source> <year>1999</year>; <volume>66</volume>: <fpage>121</fpage>–<lpage>125</lpage>.</citation>
</ref>
<ref id="bibr32-1748006X12462780">
<label>32.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rausand</surname><given-names>M</given-names></name>
<name><surname>Vatn</surname><given-names>J</given-names></name>
</person-group>. <article-title>Reliability modeling of surface controlled subsurface safety valves</article-title>. <source>Rel Engng Sys Saf</source> <year>1998</year>; <volume>61</volume>: <fpage>159</fpage>–<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr33-1748006X12462780">
<label>33.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lundteigen</surname><given-names>MA</given-names></name>
<name><surname>Rausand</surname><given-names>M</given-names></name>
</person-group>. <article-title>Partial stroke testing of process shutdown valves: How to determine the test coverage</article-title>. <source>J Loss Prevention in the Process Ind</source> <year>2008</year>; <volume>21</volume>: <fpage>579</fpage>–<lpage>588</lpage>.</citation>
</ref>
<ref id="bibr34-1748006X12462780">
<label>34.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rausand</surname><given-names>M</given-names></name>
<name><surname>Høyland</surname><given-names>A</given-names></name>
</person-group>. <source>System reliability theory; models, statistical methods, and applications</source>. <edition>2nd ed.</edition> <year>2004</year>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr35-1748006X12462780">
<label>35.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dutuit</surname><given-names>Y</given-names></name>
<name><surname>Rauzy</surname><given-names>A</given-names></name>
<name><surname>J-P</surname><given-names>S</given-names></name>
</person-group>. <article-title>A snapshot of methods and tools to assess safety integrity levels of high-integrity protection systems</article-title>. <source>Proc IMechE, Part O: J Risk and Reliability</source> <year>2008</year>; <volume>222</volume>: <fpage>371</fpage>–<lpage>379</lpage>.</citation>
</ref>
<ref id="bibr36-1748006X12462780">
<label>36.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lundteigen</surname><given-names>MA</given-names></name>
<name><surname>Rausand</surname><given-names>M</given-names></name>
</person-group>. <article-title>Common cause failures in safety instrumented systems on oil and gas installations: Implementing defense measures through function testing</article-title>. <source>J Loss Prevention in the Process Ind</source> <year>2007</year>; <volume>20</volume>: <fpage>218</fpage>–<lpage>229</lpage>.</citation>
</ref>
<ref id="bibr37-1748006X12462780">
<label>37.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hokstad</surname><given-names>P</given-names></name>
<name><surname>Rausand</surname><given-names>M</given-names></name>
</person-group>. <article-title>Common cause failure modeling: Status and trends</article-title>. In: <person-group person-group-type="editor">
<name><surname>Misra</surname><given-names>KB</given-names></name>
</person-group> (ed.) <source>Handbook of performability engineering</source>, <year>2008</year>, pp.<fpage>621</fpage>–<lpage>640</lpage>. <publisher-loc>London</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr38-1748006X12462780">
<label>38.</label>
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Liu</surname><given-names>Y</given-names></name>
<name><surname>Jin</surname><given-names>H</given-names></name>
<name><surname>Lundteigen</surname><given-names>MA</given-names></name><etal/>
</person-group>. <article-title>Reliability modeling of safety-instrumented systems by Petri nets</article-title>. <year>2012</year>. Submitted for publication. Submitted to Reliability Engineering and System Safety.</citation>
</ref>
<ref id="bibr39-1748006X12462780">
<label>39.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rouvroye</surname><given-names>JL</given-names></name>
<name><surname>van den Bliek</surname><given-names>EG</given-names></name>
</person-group>. <article-title>Comparing safety analysis techniques</article-title>. <source>Rel Engng Sys Saf</source> <year>2002</year>; <volume>75</volume>: <fpage>289</fpage>–<lpage>294</lpage>.</citation>
</ref>
<ref id="bibr40-1748006X12462780">
<label>40.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zio</surname><given-names>E</given-names></name>
<name><surname>Apostolakis</surname><given-names>G</given-names></name>
</person-group>. <article-title>Two methods for the structured assessment of model uncertainty by experts in performance assessments of radioactive waste repositories</article-title>. <source>Rel Engng Sys Saf</source> <year>1996</year>; <volume>54</volume>: <fpage>225</fpage>–<lpage>241</lpage>.</citation>
</ref>
<ref id="bibr41-1748006X12462780">
<label>41.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Droguett</surname><given-names>E</given-names></name>
<name><surname>Mosleh</surname><given-names>A</given-names></name>
</person-group>. <article-title>Bayesian methodology for model uncertainty using model performance data</article-title>. <source>Risk Analysis</source> <year>2008</year>; <volume>28</volume>: <fpage>1457</fpage>–<lpage>1476</lpage>.</citation>
</ref>
<ref id="bibr42-1748006X12462780">
<label>42.</label>
<citation citation-type="book">
<collab>MIL-HDBK-217F Reliability prediction of electronic equipment</collab>. <article-title>Handbook</article-title>, <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>US Department of Defense</publisher-name>, <year>1991</year>.</citation>
</ref>
<ref id="bibr43-1748006X12462780">
<label>43.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brissaud</surname><given-names>F</given-names></name>
<name><surname>Charpentier</surname><given-names>D</given-names></name>
<name><surname>Fouladirad</surname><given-names>M</given-names></name><etal/>
</person-group>. <article-title>Failure rate evaluation with influencing factors</article-title>. <source>J Loss Prevention in the Process Ind</source> <year>2010</year>; <volume>23</volume>: <fpage>187</fpage>–<lpage>193</lpage>.</citation>
</ref>
<ref id="bibr44-1748006X12462780">
<label>44.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rasmuson</surname><given-names>D</given-names></name>
<name><surname>Mosleh</surname><given-names>A</given-names></name>
<name><surname>Wierman</surname><given-names>T</given-names></name>
</person-group>. <article-title>Insight from analysing the nuclear regulatory commission’s common-cause failure database</article-title>. <source>Proc IMechE, Part O: J Risk and Reliability</source> <year>2008</year>; <volume>222</volume>: <fpage>533</fpage>–<lpage>544</lpage>.</citation>
</ref>
<ref id="bibr45-1748006X12462780">
<label>45.</label>
<citation citation-type="book">
<collab>NEA International common-cause failure data exchange</collab>. <article-title>ICDE general coding guidelines</article-title>. Technical report R(<year>2004</year>)4, <publisher-name>Nuclear Energy Agency</publisher-name>, <publisher-loc>Paris</publisher-loc>, <year>2004</year>.</citation>
</ref>
<ref id="bibr46-1748006X12462780">
<label>46.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zitrou</surname><given-names>A</given-names></name>
<name><surname>Bedford</surname><given-names>T</given-names></name>
<name><surname>Walls</surname><given-names>L</given-names></name>
</person-group>. <article-title>An influence diagram extension of the unified partial method for common cause failures</article-title>. <source>Quality Technol Quantitative Mgmt</source> <year>2007</year>; <volume>4</volume>: <fpage>111</fpage>–<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr47-1748006X12462780">
<label>47.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hokstad</surname><given-names>P</given-names></name>
<name><surname>Fløtten</surname><given-names>P</given-names></name>
<name><surname>Holmstrøm</surname><given-names>S</given-names></name><etal/>
</person-group>. <article-title>A reliability model for optimization of test schemes for fire and gas detectors</article-title>. <source>Rel Engng Sys Saf</source> <year>1995</year>; <volume>47</volume>: <fpage>15</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr48-1748006X12462780">
<label>48.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Bukowski</surname><given-names>JV</given-names></name>
</person-group>. <article-title>Impact of proof test effectiveness on safety instrumented system performance</article-title>. In: <conf-name>Proceedings of the annual reliability and maintainability symposium (RAMS 2009)</conf-name>, <year>2009</year>.</citation>
</ref>
<ref id="bibr49-1748006X12462780">
<label>49.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Brissaud</surname><given-names>F</given-names></name>
<name><surname>Barros</surname><given-names>A</given-names></name>
<name><surname>Bérenguer</surname><given-names>C</given-names></name>
</person-group>. <article-title>Probability of failure of safety-critical systems subject to partial tests</article-title>. In: <conf-name>Proceedings of 2010 the annual reliability and maintainability symposium (RAMS 2010)</conf-name>, <conf-loc>Fort Worth, TX, San Jose, CA</conf-loc>, <conf-date>January 25–28</conf-date>, <year>2010</year>.</citation>
</ref>
<ref id="bibr50-1748006X12462780">
<label>50.</label>
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Thunnissen</surname><given-names>DP</given-names></name>
</person-group>. <source>Propagating and mitigating uncertainty in the design of complex multidisciplinary systems</source>. PhD thesis, <publisher-name>California Institute of Technology</publisher-name>, <publisher-loc>Pasadena, CA</publisher-loc>, <year>2005</year>.</citation>
</ref>
<ref id="bibr51-1748006X12462780">
<label>51.</label>
<citation citation-type="book">
<collab>NASA Probabilistic risk assessment procedures guide for NASA managers and practitioners</collab>. <article-title>Technical report, NASA Office of Safety and Mission Assurance</article-title>, <publisher-loc>Washington, DC</publisher-loc>, <year>2002</year>.</citation>
</ref>
<ref id="bibr52-1748006X12462780">
<label>52.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>W Mechri</surname><given-names>W</given-names></name>
<name><surname>Simon</surname><given-names>C</given-names></name>
<name><surname>Othman</surname><given-names>K</given-names></name>
</person-group>. <article-title>Uncertainty analysis of common cause failure in safety instrumented systems</article-title>. <source>Proc IMechE, Part O: J Risk and Reliability</source> <year>2011</year>; <volume>225</volume>: <fpage>450</fpage>–<lpage>460</lpage>.</citation>
</ref>
<ref id="bibr53-1748006X12462780">
<label>53.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Simon</surname><given-names>C</given-names></name>
<name><surname>Weber</surname><given-names>P</given-names></name>
</person-group>. <article-title>Imprecise reliability by evidential networks</article-title>. <source>Proc IMechE, Part O: J Risk and Reliability</source> <year>2009</year>; <volume>223</volume>: <fpage>119</fpage>–<lpage>131</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>