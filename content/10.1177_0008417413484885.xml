<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">CJO</journal-id>
<journal-id journal-id-type="hwp">spcjo</journal-id>
<journal-title>Canadian Journal of Occupational Therapy</journal-title>
<issn pub-type="ppub">0008-4174</issn>
<issn pub-type="epub">1911-9828</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0008417413484885</article-id>
<article-id pub-id-type="publisher-id">10.1177_0008417413484885</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Original Articles / Articles originaux</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Observation patterns of dynamic occupational performance</article-title>
<subtitle>Modes d’observation du rendement occupationnel dynamique</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>MacKenzie</surname>
<given-names>Diane E.</given-names>
</name>
<xref ref-type="corresp" rid="corresp1-0008417413484885"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Westwood</surname>
<given-names>David A.</given-names>
</name>
</contrib>
<bio>
<title>Author Biographies</title>
<p>
<bold>Diane E. MacKenzie</bold>, PhD, MAEd, OT Reg(NS), is Assistant Professor, School of Occupational Therapy, Faculty of Health Professions, Dalhousie University, Box 15000, Halifax, Nova Scotia, B3H 4R2, Canada. Telephone: 902-494-2612. E-mail: <email>diane.mackenzie@dal.ca</email>. At the time of this study, Diane was a doctoral candidate in the Interdisciplinary PhD, Faculty of Graduate Studies, Dalhousie University.</p>
<p>
<bold>David A. Westwood</bold>, PhD, is Associate Professor, School of Health and Human Performance, Faculty of Health Professions, Dalhousie University, Box 15000, Halifax, Nova Scotia, B3H 4R2, Canada.</p>
</bio>
</contrib-group>
<author-notes>
<corresp id="corresp1-0008417413484885">Diane E. MacKenzie, School of Occupational Therapy, Faculty of Health Professions, Dalhousie University, Box 15000, Halifax, Nova Scotia, B3H 4R2, Canada. Telephone: 902-494-2612. E-mail: <email>diane.mackenzie@dal.ca</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2013</year>
</pub-date>
<volume>80</volume>
<issue>2</issue>
<fpage>92</fpage>
<lpage>100</lpage>
<permissions>
<copyright-statement>© CAOT 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="other">Canadian Association of Occupational Therapists (CAOT)</copyright-holder>
</permissions>
<abstract>
<sec>
<title>Background</title>
<p>Visual observation is a key component of both formal and informal occupational performance assessment, but it is unknown how therapists gather this visual information.</p>
</sec>
<sec>
<title>Purpose</title>
<p>The purpose of this study was to explore observational behaviour of occupational therapists and non–health care professionals when watching videos of simulated clients post-stroke participating in everyday activity.</p>
</sec>
<sec>
<title>Method</title>
<p>Ten licensed occupational therapists and 10 age-, gender-, and education level–matched participants completed this eye-tracking study.</p>
</sec>
<sec>
<title>Findings</title>
<p>Contrary to our past work with static image viewing, we found limited evidence of differences in eye movement characteristics between the two groups, although results did support the role of bottom-up information, such as visual motion, as a determinant of looking behaviour. </p>
</sec>
<sec>
<title>Implications</title>
<p>These results suggest that understanding observational behaviour in therapists can be aided with eye-tracking methodology, but future studies should probe a broad range of factors that might influence observational behaviour and performance, such as assessment goals, knowledge, and therapist experience.</p>
</sec>
</abstract>
<abstract xml:lang="fr">
<sec>
<title>Description</title>
<p>L’observation visuelle est un élément clé de l’évaluation formelle et informelle du rendement occupationnel, mais on ne sait pas comment les ergothérapeutes recueillent cette information visuelle. </p>
</sec>
<sec>
<title>But</title>
<p>Le but de cette étude était d’examiner les comportements d’observation d’ergothérapeutes et de professionnels n’étant pas du domaine de la santé pendant qu’ils regardaient des vidéos représentant des clients post-AVC simulés participant à des activités de la vie quotidienne.</p>
</sec>
<sec>
<title>Méthodologie</title>
<p>Dix ergothérapeutes agréés et dix participants jumelés à ces ergothérapeutes en fonction de leur âge, leur sexe et leur degré d’éducation ont participé à cette étude oculométrique.</p>
</sec>
<sec>
<title>Résultats</title>
<p>Contrairement à nos travaux antérieurs sur le visionnement d’images fixes, nous avons trouvé peu de données probantes permettant d’établir des différences entre les deux groupes face aux caractéristiques des mouvements des yeux, bien que les résultats indiquent que l’information ascendante (perceptive), comme les mouvements visuels, joue un rôle a en tant que déterminant du comportement d’observation.</p>
</sec>
<sec>
<title>Conséquences</title>
<p> Ces résultats suggèrent que la méthodologie de l’oculométrie peut permettre de mieux comprendre les comportements d’observation des ergothérapeutes, mais il faudra effectuer d’autres études en vue d’examiner un large éventail de facteurs susceptibles d’influencer les comportements d’observation et le rendement, notamment, les objectifs de l’évaluation, les connaissances et l’expérience de l’ergothérapeute.</p>
</sec>
</abstract>
<kwd-group>
<kwd>Activities of daily living</kwd>
<kwd>Observation</kwd>
<kwd>Occupational therapy</kwd>
<kwd>Task performance and analysis</kwd>
</kwd-group>
<kwd-group>
<kwd>activités de la vie quotidienne</kwd>
<kwd>ergothérapie</kwd>
<kwd>exécution et analyse des tâches</kwd>
<kwd>observation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Observation of a client’s occupational performance is a key component of both formal and informal assessment capturing the interaction between person, environment, and occupation. Observation can be conceptualized as the purposeful gathering of information or data by an observer through the use of various sensory systems. Visual observation is a key component of both formal and informal occupational performance assessment. Consistent with the importance of observation in assessment, studies have looked to understand and improve the assessment process through standardization of scoring guidelines and measurement of reliability and validity (e.g., <xref ref-type="bibr" rid="bibr2-0008417413484885">Brentnall &amp; Bundy, 2009</xref>). However, few studies have actually explored the actions carried out by the observer during the observation process itself. Consequently, it remains unknown how occupational therapists gather their observation data and whether they gather it differently than (or in a manner that is superior to) an untrained observer simply “watching.”</p>
<p>Drawing on the rich literature on eye tracking as a tool for exploring the role of eye movements in visual tasks, such as reading, and search and memory tasks, we compared the observational behaviour of occupational therapists (trained) and non–health care professionals (non-trained) when freely inspecting static images consisting of domain-relevant stimuli (i.e., simulated clients post-stroke) and domain-neutral stimuli (i.e., landscapes) (<xref ref-type="bibr" rid="bibr13-0008417413484885">MacKenzie &amp; Westwood, 2013</xref>). Participants were instructed to simply view the images for a recognition memory task that would be administered after all images had been seen. We found the trained group was significantly different and demonstrated increased fixation counts, shorter fixation durations, and more saccade counts for a variety of domain-relevant (simulated client post-stroke) and domain-neutral images (naturalistic scenes). Since the only measureable difference between the two groups studied was their occupational therapy training, the finding that the trained group viewed both types of images differently than the non-trained suggests that a top-down direction of their visual attention played a dominant role for allocation of attention (<xref ref-type="bibr" rid="bibr8-0008417413484885">Henderson, Brockmole, Castelhano, &amp; Mack, 2007</xref>). However, our previous task was quite limited with respect to the context in which assessment takes place in occupational therapy practice. For example, assessment is rarely if ever carried out with static images since movement is a key element of occupational performance.</p>
<p>Dynamic scenes are not only more clinically relevant than static images but also much richer in terms of visual information, containing time-varying information, such as motion (<xref ref-type="bibr" rid="bibr19-0008417413484885">Tatler, Hayhoe, Land, &amp; Ballard, 2011</xref>). Research suggests that “bottom-up” features present in the stimulus (and independent of observer knowledge), such as motion and continuous change of scene content, are strong predictors of the likelihood that an observer will look toward a stimulus (<xref ref-type="bibr" rid="bibr9-0008417413484885">Itti, 2005</xref>). Accordingly, one would predict that all observers, regardless of skill or ability, would tend to look at similar features of dynamic scenes, such as things that are moving. However, “top-down” knowledge or experience of the observer has also been shown to impact the viewing behaviour by directing visual attention toward scene features that are thought to be important (<xref ref-type="bibr" rid="bibr19-0008417413484885">Tatler et al., 2011</xref>). Thus, there is reason to believe that trained therapists would tend to look at different features of a dynamic scene than untrained observers, based on the therapist’s knowledge and experience with particular tasks and contexts.</p>
<p>Our previous finding of group differences during static image viewing supports the importance of top-down influences, while the finding that picture type influenced eye mechanics suggests a role for bottom-up factors. What is not known is the visual behaviour of therapists during observation of dynamic scenes. Dynamic scenes might afford an expanded role for top-down knowledge since this knowledge could help the observer infer the client’s intentions and anticipate the possible consequences of critical incidents that unfold over time on the basis of, for example, knowledge of physical principles, such as inertia and static/dynamic balance (e.g., base of support).</p>
<p>The present study was designed to gauge if there are differences in eye movement patterns (i.e., saccade count, fixation count, fixation durations) between occupational therapists (“trained”) and non-trained/non-health observers (“non-trained”) when viewing occupational performance of a simulated client post-stroke during three different videotaped tasks. A secondary goal was to explore differences in self-reported observation strategies between the two groups. More specific measures of observation were also explored by breaking down videos into particular regions of interest (“features”) and phases (“activity epochs”). Two broad hypotheses were proposed based on our past work with static images: (a) Observational behaviour would be affected by bottom-up characteristics of the videos, as evidenced by main effects of features and epochs for both groups, and (b) observational behaviour would be affected by the top-down knowledge of the observers, as evidenced by group main effects or interactions between group and features or epochs.</p>
<sec id="section1-0008417413484885" sec-type="methods">
<title>Method</title>
<sec id="section2-0008417413484885">
<title>Participants</title>
<p>Participants recruited for the trained group were licensed occupational therapists with 5 or more years of neurorehabilitation experience. The non-trained group included participants matched to the trained group for age, gender, and highest degree attained but with no previous health care education or experience. The initial contact for both groups was purposive and triggered a word-of-mouth recruitment process. Eligible participants were required to meet the following conditions: (a) normal or corrected-to-normal visual acuity wearing contact lenses; (b) no known visual or neurological condition restricting any of the following: coordinated eye movements, visual and cognitive processing skills, head and neck control in a seated position, or coordinated upper-limb fine motor control. All participants provided informed, written consent. This study was reviewed and approved by the Dalhousie University’s Office of Human Research Ethics Administration.</p>
<p>Twenty participants completed this study and ranged in age from 30 to 50 years. Each group consisted of 8 females and 2 males. The trained group was equally distributed in practice experience (all had more than 6 years of neurological occupational therapy experience), practice setting (acute, rehabilitation, private practice, or combination thereof), and client caseload (infancy/childhood, adolescent/adult, older adult, or combination thereof). Participants reported similar observational experience profiles suggesting that the major identifiable difference between the matched groups was the domain-specific training and practice experience of occupational therapy.</p>
</sec>
<sec id="section3-0008417413484885">
<title>Data Collection</title>
<sec id="section4-0008417413484885">
<title>Apparatus and stimuli</title>
<p>The SR Research Experiment Builder software (Version 1.1.1.1 RC) was used in combination with EyeLink<sup>®</sup>II (<xref ref-type="bibr" rid="bibr16-0008417413484885">SR Research Ltd., n.d.</xref>) video-based eye-tracking system to create and carry out this study. The EyeLink<sup>®</sup>II has a sampling rate of 500 Hz, spatial precision &lt; 0.01°, and spatial accuracy &lt; 0.8° root mean square error. Calibration of the EyeLink II was carried out in the same horizontal viewing plane used to display the video images. The eye tracker recorded eye position and movement duration as well as compensations for head movement. Viewing was binocular but only the right eye was tracked to streamline data analysis.</p>
<p>EyeLink DataViewer<sup>TM</sup> software (<xref ref-type="bibr" rid="bibr16-0008417413484885">SR Research Ltd., n.d.</xref>) was used to extract four key dependent measures related to eye movement during the study: fixation count, fixation duration, and saccadic activity (eye movements: both number [saccade count] and distance spanned by the movement [saccade amplitude]; <xref ref-type="bibr" rid="bibr4-0008417413484885">Castelhano, Mack, &amp; Henderson, 2009</xref>). Data management and statistical analysis were completed with Microsoft Excel 2007 and PASW Statistics 17.0 software. Videos were recorded using a CanonXM2 Mini DV camcorder and subsequently edited using Adobe Premiere Pro 2.0. Natural uncut scenes, more representative of natural viewing situations, were used instead of professionally edited change-of-viewpoint videos (<xref ref-type="bibr" rid="bibr6-0008417413484885">Dorr, Martinetz, Gegenfurtner, &amp; Barth, 2010</xref>). The videos were presented in full colour and random order on a 32-in. monitor with a refresh rate of 140 Hz. The audio component of each video was purposefully removed to ensure the measurable components of visual attention were driven by visual features of the videos. While auditory alerting may enhance visual search performance (<xref ref-type="bibr" rid="bibr22-0008417413484885">Zou, Muller, &amp; Shi, 2012</xref>), auditory cues cannot be localized in an image, making it difficult to link the auditory cue influence on the observation, visual attention parameters.</p>
<p>The video stimuli of three different typical daily living events used for this study were developed by the primary author. Two of the videos included a female simulated client (post-stroke with left hemiparesis) completing a kitchen task (standing on a kitchen step stool reaching into the second shelf of a cupboard) and a transfer task (sit-pivot transfer from a toilet to wheelchair). The third video portrayed a male simulated client (post-stroke with right hemiparesis) pushing a manual lawnmower. The motor impairment level for each video was guided by the Chedoke-McMaster Stroke Assessment (<xref ref-type="bibr" rid="bibr14-0008417413484885">Miller et al., 2008</xref>). The upper extremity was portrayed as Chedoke-McMaster Stage 2 hand and arm for both clients in all three videos. The lower extremity and foot were portrayed at a Chedoke-McMaster Stage 5 for the toilet transfer and kitchen videos and a Stage 6 leg and foot for the grass-cutting video. The kitchen and toilet video had a static camera viewing perspective, while the grass-cutting video had a changing viewing perspective as the client moved toward the camera.</p>
</sec>
<sec id="section5-0008417413484885">
<title>Procedure</title>
<p>Participants donned the EyeLink<sup>®</sup>II head-mounted eye-tracking system and sat approximately 36 in. (91 cm) from the monitor. There was no restriction in head movement so as to provide a naturalistic viewing condition for the participants. The EyeLink<sup>®</sup>II calibration and validation process was completed at the beginning of the experiment to ensure point-of-gaze verification could be achieved from all nine eye calibration features on the computer screen. To decrease the eye-tracking error, each video presentation began with a drift correction procedure. This study was purposefully devoid of instructions or specific assessment tasks for the viewing period (a) to allow for tracking of what drew the observer’s attention, as opposed to directing their attention for task completion, and (b) to avoid bias against the non-trained group due to content expertise differences. Participants were told only that they would be watching three videos of a simulated client post-stroke and, following the viewing, they would be asked a reflective observation question (i.e., “Please list up to three strategies or points you used to assist with observing the video contents”).</p>
</sec>
<sec id="section6-0008417413484885">
<title>Data coding and epoch descriptions</title>
<p>The four dependent eye measures were extracted for all videos. The data viewer software provides the coordinate locations of the eye gaze within the frame of reference of the viewing screen but is unable to automatically marry the subjects’ visual gaze to specific features within the moving scene since the coordinates of objects in the video do not remain in the same screen location as the action unfolds over time. Fixation locations (coded as “features”) were achieved by reviewing the gaze cursor overlay at 5% of real-time speed and identifying the initial feature within the video associated with the fixation coordinates, frame number, and time stamp (in milliseconds). Given that critical incidents occurred within each video at different points in time, our analyses were conducted on fixations by feature and also by features and epochs, where <italic>epoch</italic> refers to a period of time in the video when key activity components occurred. <xref ref-type="table" rid="table1-0008417413484885">Table 1</xref> contains a description of each epoch’s activity components per video and epoch length (in milliseconds). These analyses allowed us to focus more precisely on differences in observational performance between groups, since these differences could potentially be specific to particular features at particular instants in time.</p>
<table-wrap id="table1-0008417413484885" position="float">
<label>Table 1</label>
<caption>
<p>Video Epoch Descriptions and Durations</p>
</caption>
<graphic alternate-form-of="table1-0008417413484885" xlink:href="10.1177_0008417413484885-table1.tif"/>
<table>
<thead>
<tr>
<th>Epoch</th>
<th>Time (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grass video</td>
<td>
</td>
</tr>
<tr>
<td>1. Gait cycle × 2 (left toe off)</td>
<td>3,558</td>
</tr>
<tr>
<td>2. Gait cycle × 2 (left toe off)</td>
<td>3,496</td>
</tr>
<tr>
<td>3. Gait cycle × 2 (left toe off)</td>
<td>3,930</td>
</tr>
<tr>
<td>Kitchen video</td>
<td>
</td>
</tr>
<tr>
<td>1. Foot adjustment on step stool. Right hand on counter. Head/neck/body begins to extend in preparation of hand lift off counter.</td>
<td>7,220</td>
</tr>
<tr>
<td>2. Right hand lifts off from counter, reach to second open shelf and manipulates cups attempting to retrieve one cup off shelf.</td>
<td>3,260</td>
</tr>
<tr>
<td>3. Cups start to fall off shelf and into sink, right hand leaves second shelf and returns back to counter.</td>
<td>4,435</td>
</tr>
<tr>
<td>Toilet video</td>
<td>
</td>
</tr>
<tr>
<td>1. Head seeking and right arm movement between the wall bar and wheelchair for upper extremity support. Epoch ends with right-hand grasp on wall grab bar.</td>
<td>5,460</td>
</tr>
<tr>
<td>2. Trunk folding/unfolding and feet preparation for transfer movement. Right hand still holding wall bar.</td>
<td>4,454</td>
</tr>
<tr>
<td>3. Lift off toilet seat and pivot to wheelchair. Right hand still holding wall bar.</td>
<td>2,208</td>
</tr>
<tr>
<td>4. Posterior thigh contact with wheelchair surface. Release of wall grab bar.</td>
<td>2,935</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>All video stimuli were reviewed three to four times each by the primary author to code features, and an independent reviewer (occupational therapist not part of the study) randomly sampled each coded video to validate the feature coding schemes. The saccade count and saccade amplitude could not be included in the specific feature analysis because they are measures of eye movement <italic>between</italic> features. For the purpose of these analyses, a feature was composed of several coding locations. For example, the elbow and wrist on the right arm and knee and foot on the right leg were grouped together as the “R UE/LE” (right upper extremity/lower extremity) feature rather than having separate features for each location on the right-sided limbs.</p>
</sec>
</sec>
<sec id="section7-0008417413484885">
<title>Data Analyses</title>
<p>A mixed analysis of variance (ANOVA) was first completed for each video to explore the effects of group and epoch for all of the dependent measures. Next, a mixed ANOVA of group, epoch, and feature was completed for the fixation count and fixation duration dependent measures (as noted earlier, saccade count and amplitude are not included in the epoch by feature analysis as they are movements between the fixated features). An alpha threshold of .05 was used for all analyses. Based on the results of Mauchly’s test (alpha = .05), the Greenhouse-Geisser correction was applied for any violation of sphericity, and the adjusted degrees of freedom are reported. Significant effects will be presented for each video separately. Effect sizes are presented as partial eta squared. These values can be interpreted using the following parameters: Values between 0.01and 0.05 indicate a small effect, values between 0.06 and 0.13 indicate a medium effect, and values of 0.14 and greater indicate a large effect (<xref ref-type="bibr" rid="bibr5-0008417413484885">Cohen, 1988</xref>).</p>
</sec>
</sec>
<sec id="section8-0008417413484885">
<title>Findings</title>
<sec id="section9-0008417413484885">
<title>Grass Video</title>
<p>One of the non-trained cases was removed from the analysis during the feature coding due to a technical error in the gaze overlay synchronization with the video content. There were no significant main effects or interactions in the Group × Epoch ANOVA. Please refer to <xref ref-type="table" rid="table2-0008417413484885">Table 2</xref> for all statistically significant results arising from the complete Group × Epoch × Feature analysis.</p>
<table-wrap id="table2-0008417413484885" position="float">
<label>Table 2</label>
<caption>
<p>Grass Video: Dependent Variable Mean and Standard Error of Fixation Count and Duration</p>
</caption>
<graphic alternate-form-of="table2-0008417413484885" xlink:href="10.1177_0008417413484885-table2.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th colspan="2">Head/face</th>
<th colspan="2">Neck/upper/lower trunk</th>
<th colspan="2">R UE/LE</th>
<th colspan="2">L UE/LE</th>
<th colspan="2">Feet and area</th>
<th colspan="2">Environment</th>
</tr>
<tr>
<th>Variable</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fixation count<sup>a</sup> <italic>M</italic> (<italic>SE</italic>)</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td> Epoch 1</td>
<td>0.60 (0.28)</td>
<td>1.11 (0.29)</td>
<td>2.70 (0.38)</td>
<td>2.89 (0.40)</td>
<td>1.60 (0.27)</td>
<td>1.44 (0.29)</td>
<td>0.30 (0.11)</td>
<td>0.00 (0.12)</td>
<td>1.40 (0.40)</td>
<td>0.56 (0.42)</td>
<td>2.70 (0.47)</td>
<td>2.11 (0.49)</td>
</tr>
<tr>
<td> Epoch 2</td>
<td>0.80 (0.43)</td>
<td>0.67 (0.45)</td>
<td>1.00 (0.39)</td>
<td>2.00 (0.41)</td>
<td>2.30 (0.33)</td>
<td>1.33 (0.34)</td>
<td>0.10 (0.13)</td>
<td>0.33 (0.14)</td>
<td>1.20 (0.37)</td>
<td>1.00 (0.39)</td>
<td>2.80 (0.57)</td>
<td>2.89 (0.60)</td>
</tr>
<tr>
<td> Epoch 3</td>
<td>0.20 (0.31)</td>
<td>0.44 (0.32)</td>
<td>1.70 (0.33)</td>
<td>1.11 (0.35)</td>
<td>1.90 (0.42)</td>
<td>2.11 (0.44)</td>
<td>0.80 (0.31)</td>
<td>0.89 (0.33)</td>
<td>1.60 (0.40)</td>
<td>1.44 (0.42)</td>
<td>3.10 (0.53)</td>
<td>3.00 (0.55)</td>
</tr>
<tr>
<td>Fixation duration<sup>b</sup> <italic>M</italic> (<italic>SE</italic>)</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td> Epoch 1</td>
<td>284.81 (129.88)</td>
<td>373.30 (136.91)</td>
<td>827.62 (204.47)</td>
<td>1244.84 (215.53)</td>
<td>574.62 (102.38)</td>
<td>411.74 (107.92)</td>
<td>92.42 (46.06)</td>
<td>0.00 (48.56)</td>
<td>469.99 (146.90)</td>
<td>224.23 (154.85)</td>
<td>956.01 (211.83)</td>
<td>984.17 (223.28)</td>
</tr>
<tr>
<td> Epoch 2</td>
<td>198.82 (97.33)</td>
<td>183.11 (102.60)</td>
<td>390.81 (147.13)</td>
<td>684.93 (155.09)</td>
<td>876.17 (176.71)</td>
<td>566.87 (186.26)</td>
<td>28.40 (46.54)</td>
<td>120.00 (49.06)</td>
<td>637.21 (187.00)</td>
<td>339.37 (197.12)</td>
<td>985.19 (260.07)</td>
<td>1252.74 (274.13)</td>
</tr>
<tr>
<td> Epoch 3</td>
<td>16.41 (74.77)</td>
<td>114.22 (78.82)</td>
<td>609.56 (118.72)</td>
<td>337.75 (125.14)</td>
<td>871.17 (240.02)</td>
<td>795.99 (253.00)</td>
<td>253.18 (121.75)</td>
<td>316.46 (128.34)</td>
<td>698.36 (180.06)</td>
<td>608.44 (189.80)</td>
<td>1015.65 (285.81)</td>
<td>1236.88 (301.27)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0008417413484885">
<p>
<italic>Note.</italic> For epoch descriptions, see <xref ref-type="table" rid="table1-0008417413484885">Table 1</xref>. R = right; L = left; LE = lower extremity; UE = upper extremity; OT = occupational therapist; Non = non-trained, non–health professional.</p>
</fn>
<fn id="table-fn2-0008417413484885">
<p>
<sup>a</sup>Main effect of feature, significant at <italic>p</italic> &lt; .01; interaction of epoch and feature, significant at <italic>p</italic> &lt; .01.</p>
</fn>
<fn id="table-fn3-0008417413484885">
<p>
<sup>b</sup>Main effect of epoch, significant at <italic>p </italic>&lt; .01; main effect of feature, significant at <italic>p </italic>&lt; .01; interaction of epoch and feature, significant at <italic>p</italic> &lt; .05.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Interestingly, there was a significant main effect of epoch for average fixation duration, <italic>F</italic>(2, 34) = 131.16, η<sub>p</sub>
<sup>2</sup> = .45, <italic>p</italic> &lt; .01, but not fixation count; the mean fixation duration in the third epoch was significantly longer. Although each epoch in this video was essentially the same in terms of client movement, the image grew in size as the client approached the camera in successive epochs, which might account for the change in fixation durations.</p>
<p>A significant main effect of feature was found for fixation count, <italic>F</italic>(5, 85) = 21.27, η<sub>p</sub>
<sup>2</sup> = 0.56, <italic>p</italic> &lt; .01, and fixation duration, <italic>F</italic>(2.79, 47.34) = 16.08, η<sub>p</sub>
<sup>2</sup> = .49, <italic>p</italic> &lt; .01: The “environment” feature had more fixations and longer duration than any of the other features, with “L UE/LE” (left upper extremity/lower extremity; the unaffected arm and leg) having the fewest fixations and shortest fixation durations. The significant interaction of epoch and feature for fixation count, <italic>F</italic>(5.84, 99.19) = 3.27, η<sub>p</sub>
<sup>2</sup> = .16, <italic>p</italic> &lt; .01, and fixation duration, <italic>F</italic>(4.79, 81.41) = 2.41, η<sub>p</sub>
<sup>2</sup> = .12, <italic>p</italic> &lt; .05, was driven by the increased attention to the feature “neck/upper/lower trunk” (compared to other features) during Epoch 1 and to L UE/LE (compared to other features) during Epoch 3. Again, this finding may be related to movement of the client toward the camera in successive epochs, perhaps making different features more salient.</p>
</sec>
<sec id="section10-0008417413484885">
<title>Kitchen Video</title>
<p>All significant results for the kitchen video are reported in <xref ref-type="table" rid="table3-0008417413484885">Table 3</xref>. In the Group × Epoch ANOVA, there was a significant main effect of epoch for fixation count, <italic>F</italic>(2, 36) = 87.00, η<sub>p</sub>
<sup>2</sup> = .83, <italic>p</italic> &lt; .05, and saccade count, <italic>F</italic>(2, 36) = 82.85, η<sub>p</sub>
<sup>2</sup> = .82, <italic>p</italic> &lt; .01, but this was expected because the epochs had different durations (7,220 ms, 3,260 ms, and 4,435 ms). Of greater interest are the results from the Group × Epoch × Feature ANOVA. The main effect of feature indicated participants had the most fixations, <italic>F</italic>(2.37, 42.61) = 21.27, η<sub>p</sub>
<sup>2</sup> = .71, <italic>p</italic> &lt; .01, and longest fixation durations, <italic>F</italic>(2.37, 42.61) = 16.08, η<sub>p</sub>
<sup>2</sup> = .61, <italic>p</italic> &lt; .01, on the environment and R UE/LE, while L UE/LE and neck/upper/lower trunk were the least inspected. The significant interaction of epoch and feature for fixation count, <italic>F</italic>(4.39, 79.06) = 3.27, η<sub>p</sub>
<sup>2</sup> = .25, <italic>p</italic> &lt; .01, and fixation duration, <italic>F</italic>(3.65, 79.06) = 2.41, η<sub>p</sub>
<sup>2</sup> = .25, <italic>p</italic> &lt; .05, is driven by a high level of attention given to the feet (compared to other features) in Epoch 1, in which the client is adjusting her feet on a narrow stool, and also to the environment (compared to other features) in Epoch 3, in which the cups fall from the shelf.</p>
<table-wrap id="table3-0008417413484885" position="float">
<label>Table 3</label>
<caption>
<p>Kitchen Video: Dependent Variable Mean and Standard Error of Fixation Count and Duration</p>
</caption>
<graphic alternate-form-of="table3-0008417413484885" xlink:href="10.1177_0008417413484885-table3.tif"/>
<table>
<thead>
<tr>
<th> </th>
<th colspan="2">Head/face</th>
<th colspan="2">Neck/upper/lower trunk</th>
<th colspan="2">R UE/LE</th>
<th colspan="2">L UE/LE</th>
<th colspan="2">Feet and area</th>
<th colspan="2">Environment</th>
</tr>
<tr>
<th>Variable</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="13">Fixation count<sup>a</sup> <italic>M</italic> (<italic>SE</italic>)</td>
</tr>
<tr>
<td> Epoch 1</td>
<td>3.00 (0.45)</td>
<td>2.60 (0.45)</td>
<td>1.60 (0.49)</td>
<td>1.40 (0.49)</td>
<td>3.50 (0.55)</td>
<td>4.80 (0.55)</td>
<td>1.70 (0.37)</td>
<td>1.40 (0.37)</td>
<td>4.90 (0.92)</td>
<td>3.20 (0.92)</td>
<td>5.70 (1.08)</td>
<td>4.40 (1.08)</td>
</tr>
<tr>
<td> Epoch 2</td>
<td>0.70 (0.21)</td>
<td>1.00 (0.21)</td>
<td>0.50 (0.24)</td>
<td>1.00 (0.24)</td>
<td>2.40 (0.43)</td>
<td>1.80 (0.43)</td>
<td>0.60 (0.45)</td>
<td>0.70 (0.45)</td>
<td>0.80 (0.25)</td>
<td>0.30 (0.25)</td>
<td>4.50 (0.62)</td>
<td>3.10 (0.62)</td>
</tr>
<tr>
<td> Epoch 3</td>
<td>1.40 (0.35)</td>
<td>1.30 (0.35)</td>
<td>0.50 (0.19)</td>
<td>0.70 (0.19)</td>
<td>3.50 (0.69)</td>
<td>3.00 (0.69)</td>
<td>0.50 (0.22)</td>
<td>0.30 (0.22)</td>
<td>0.30 (0.24)</td>
<td>0.40 (0.24)</td>
<td>6.80 (0.66)</td>
<td>5.90 (0.66)</td>
</tr>
<tr>
<td colspan="13">Fixation duration<sup>b</sup> <italic>M</italic> (<italic>SE</italic>)</td>
</tr>
<tr>
<td> Epoch 1</td>
<td>1080.82 (212.60)</td>
<td>1002.30 (212.60)</td>
<td>598.35 (166.02)</td>
<td>392.44 (166.02)</td>
<td>1036.77 (357.63)</td>
<td>1804.81 (357.63)</td>
<td>526.31 (168.48)</td>
<td>704.32 (168.48)</td>
<td>1680.01 (358.93)</td>
<td>1494.40 (358.93)</td>
<td>1505.28 (288.75)</td>
<td>1162.34 (288.75)</td>
</tr>
<tr>
<td> Epoch 2</td>
<td>170.80 (113.97)</td>
<td>459.19 (113.97)</td>
<td>146.44 (68.29)</td>
<td>222.01 (68.29)</td>
<td>756.02 (197.33)</td>
<td>663.16 (197.33)</td>
<td>157.21 (91.53)</td>
<td>135.60 (91.53)</td>
<td>254.22 (85.52)</td>
<td>114.82 (85.52)</td>
<td>1311.61 (238.03)</td>
<td>1384.85 (238.03)</td>
</tr>
<tr>
<td> Epoch 3</td>
<td>364.00 (100.85)</td>
<td>338.04 (100.85)</td>
<td>133.23 (66.94)</td>
<td>240.00 (66.94)</td>
<td>1170.36 (260.72)</td>
<td>1306.77 (260.72)</td>
<td>97.64 (85.56)</td>
<td>168.42 (85.56)</td>
<td>109.82 (72.28)</td>
<td>104.01 (72.28)</td>
<td>2051.58 (202.22)</td>
<td>1793.60 (202.22)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0008417413484885">
<p>
<italic>Note.</italic> For epoch descriptions, see <xref ref-type="table" rid="table1-0008417413484885">Table 1</xref>. R = right; L = left; LE = lower extremity; UE = upper extremity; OT = occupational therapist; Non = non-trained, non–health professional.</p>
</fn>
<fn id="table-fn5-0008417413484885">
<p>
<sup>a</sup>Main effect of epoch, significant at <italic>p</italic> &lt; .05; main effect of feature, significant at <italic>p</italic> &lt; .01; interaction of epoch and feature, significant at <italic>p</italic> &lt; .01.</p>
</fn>
<fn id="table-fn6-0008417413484885">
<p>
<sup>b</sup>Main effect of feature, significant at <italic>p </italic>&lt; .01; interaction of epoch and feature, significant at <italic>p</italic> &lt; .05.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section11-0008417413484885">
<title>Toilet Video</title>
<p>All significant results are reported in <xref ref-type="table" rid="table4-0008417413484885">Table 4</xref>. For the Group × Epoch ANOVA, significant main effects of epoch for fixation, <italic>F</italic>(3, 54) = 155.98, η<sub>p</sub>
<sup>2</sup> = .90, <italic>p</italic> &lt; .01, and saccade counts, <italic>F</italic>(3, 54) = 143.98, η<sub>p</sub>
<sup>2</sup> = .89, <italic>p</italic> &lt; .05, are likely due to the different durations of the epochs (5,460 ms, 4,454 ms, 2,208 ms, and 2,935 ms). However, in the shortest epochs (3 and 4), the fixation duration and saccade amplitude are markedly different from Epochs 1 and 2, indicating participants are not moving their eyes across the same distance as in the other epochs and are dwelling longer at features. Interestingly, Epochs 3 and 4 represent the most total body movement as well as safety concerns in the video.</p>
<table-wrap id="table4-0008417413484885" position="float">
<label>Table 4</label>
<caption>
<p>Toilet Video: Dependent Variable Mean and Standard Error of Fixation Count and Duration</p>
</caption>
<graphic alternate-form-of="table4-0008417413484885" xlink:href="10.1177_0008417413484885-table4.tif"/>
<table>
<thead>
<tr>
<th> </th>
<th colspan="2">Head/face</th>
<th colspan="2">Neck/upper/lower trunk</th>
<th colspan="2">R UE/LE</th>
<th colspan="2">L UE/LE</th>
<th colspan="2">Feet</th>
<th colspan="2">R-side WC</th>
<th colspan="2">L-side WC</th>
<th colspan="2">Environment</th>
</tr>
<tr>
<th>Variable</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
<th>OT</th>
<th>Non</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="15">Fixation count<sup>a</sup> <italic>M</italic> (<italic>SE</italic>)</td>
</tr>
<tr>
<td> Epoch 1</td>
<td>2.40 (0.54)</td>
<td>2.10 (0.54)</td>
<td>2.20 (0.50)</td>
<td>2.80 (0.50)</td>
<td>5.70 (0.62)</td>
<td>4.70 (0.62)</td>
<td>0.30 (0.17)</td>
<td>0.10 (0.17)</td>
<td>0.50 (0.20)</td>
<td>0.10 (0.20)</td>
<td>2.60 (0.47)</td>
<td>1.80 (0.47)</td>
<td>0.30 (0.25)</td>
<td>0.90 (0.25)</td>
<td>4.10 (0.58)</td>
<td>3.90 (0.58)</td>
</tr>
<tr>
<td> Epoch 2</td>
<td>1.10 (0.47)</td>
<td>1.90 (0.47)</td>
<td>1.10 (0.37)</td>
<td>1.20 (0.37)</td>
<td>5.30 (0.56)</td>
<td>4.60 (0.56)</td>
<td>0.40 (0.22)</td>
<td>0.40 (0.22)</td>
<td>1.00 (0.22)</td>
<td>0.50 (0.22)</td>
<td>1.20 (0.50)</td>
<td>1.10 (0.50)</td>
<td>0.20 (0.12)</td>
<td>0.10 (0.12)</td>
<td>2.40 (0.50)</td>
<td>1.70 (0.50)</td>
</tr>
<tr>
<td> Epoch 3</td>
<td>0.10 (0.20)</td>
<td>0.70 (0.20)</td>
<td>0.40 (0.19)</td>
<td>0.40 (0.19)</td>
<td>2.30 (0.39)</td>
<td>1.20 (0.39)</td>
<td>0.00 (0.00)</td>
<td>0.00 (0.00)</td>
<td>0.20 (0.16)</td>
<td>0.10 (0.16)</td>
<td>0.80 (0.31)</td>
<td>0.30 (0.31)</td>
<td>0.10 (0.10)</td>
<td>0.10 (0.10)</td>
<td>0.70 (0.28)</td>
<td>0.80 (0.28)</td>
</tr>
<tr>
<td> Epoch 4</td>
<td>1.50 (0.39)</td>
<td>1.40 (0.39)</td>
<td>0.20 (0.13)</td>
<td>0.20 (0.13)</td>
<td>2.40 (0.44)</td>
<td>2.40 (0.44)</td>
<td>0.00 (0.07)</td>
<td>0.10 (0.07)</td>
<td>1.20 (0.26)</td>
<td>0.10 (0.26)</td>
<td>2.10 (0.54)</td>
<td>1.00 (0.54)</td>
<td>0.30 (0.24)</td>
<td>0.30 (0.24)</td>
<td>1.20 (0.39)</td>
<td>1.00 (0.39)</td>
</tr>
<tr>
<td colspan="15">Fixation duration<sup>b</sup> <italic>M</italic> (<italic>SE</italic>)</td>
</tr>
<tr>
<td> Epoch 1</td>
<td>764.05  (196.38)</td>
<td>732.70  (196.38)</td>
<td>615.14  (164.86)</td>
<td>846.77  (164.86)</td>
<td>1502.34  (171.60)</td>
<td>1259.98  (171.60)</td>
<td>73.60  (72.48)</td>
<td>89.99  (72.48)</td>
<td>109.62  (48.07)</td>
<td>29.61  (48.07)</td>
<td>678.74  (132.02)</td>
<td>553.65  (132.02)</td>
<td>67.22  (73.74)</td>
<td>282.75  (73.74)</td>
<td>1069.60  (174.78)</td>
<td>1032.81  (174.78)</td>
</tr>
<tr>
<td> Epoch 2</td>
<td>213.19  (154.48)</td>
<td>590.04  (154.48)</td>
<td>233.22  (136.55)</td>
<td>435.20  (136.55)</td>
<td>1474.05  (202.68)</td>
<td>1339.19  (202.68)</td>
<td>115.17  (74.99)</td>
<td>104.38  (74.99)</td>
<td>332.39  (76.58)</td>
<td>134.85  (76.58)</td>
<td>349.20  (169.51)</td>
<td>332.80  (169.51)</td>
<td>57.19  (29.07)</td>
<td>10.02  (29.07)</td>
<td>669.59  (174.04)</td>
<td>489.58  (174.04)</td>
</tr>
<tr>
<td> Epoch 3</td>
<td>23.19  (71.41)</td>
<td>218.82  (71.41)</td>
<td>172.39  (83.87)</td>
<td>144.80  (83.87)</td>
<td>926.18  (196.08)</td>
<td>542.00  (196.08)</td>
<td>0.00  (0.00)</td>
<td>0.00  (0.00)</td>
<td>43.60  (30.83)</td>
<td>0.80  (30.83)</td>
<td>196.78  (124.30)</td>
<td>150.99  (124.30)</td>
<td>31.59  (23.47)</td>
<td>10.19  (23.47)</td>
<td>208.22  (92.38)</td>
<td>289.40  (92.38)</td>
</tr>
<tr>
<td> Epoch 4</td>
<td>333.62  (115.86)</td>
<td>433.21  (115.86)</td>
<td>31.60  (54.85)</td>
<td>103.19  (54.85)</td>
<td>596.59  (129.17)</td>
<td>755.25  (129.17)</td>
<td>0.00  (11.03)</td>
<td>15.60  (11.03)</td>
<td>293.61  (62.49)</td>
<td>16.81  (62.49)</td>
<td>614.82  (218.19)</td>
<td>637.01  (218.19)</td>
<td>65.21  (64.21)</td>
<td>83.40  (64.21)</td>
<td>234.20  (93.59)</td>
<td>250.98  (93.59)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-0008417413484885">
<p>
<italic>Note.</italic> For epoch descriptions, see <xref ref-type="table" rid="table1-0008417413484885">Table 1</xref>. R = right; L = left; LE = lower extremity; UE = upper extremity; WC = wheelchair; OT = occupational therapist; Non = non-trained, non–health professional.</p>
</fn>
<fn id="table-fn8-0008417413484885">
<p>
<sup>a</sup>Main effect of group, significant at <italic>p</italic> &lt; .05; main effect of epoch, significant at <italic>p</italic> &lt; .01; main effect of feature, significant at <italic>p</italic> &lt; .01; interaction of epoch and feature, significant at <italic>p</italic> &lt; .01.</p>
</fn>
<fn id="table-fn9-0008417413484885">
<p>
<sup>b</sup>Main effect of group, significant at <italic>p</italic> &lt; .05; main effect of epoch, significant at <italic>p</italic> &lt; .01; main effect of feature, significant at <italic>p</italic> &lt; .01; interaction of epoch and feature, significant at <italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>In the Group × Epoch × Feature ANOVA, a main effect of group, <italic>F</italic>(1, 18) = 7.83, η<sub>p</sub>
<sup>2</sup> = .30, <italic>p</italic> &lt; .05, indicated that the trained group (<italic>M</italic> = 1.38) made significantly more fixations than the non-trained group (<italic>M</italic> = 1.12). The main effect of feature for fixations, <italic>F</italic>(3.73, 67.04) = 45.73, η<sub>p</sub>
<sup>2</sup> = .72, <italic>p</italic> &lt; .01, and fixation duration, <italic>F</italic>(3.57, 64.34) = 29.24, <italic>p</italic> &lt; .01, showed that R UE/LE had the most fixations, with the L UE/LE the least frequented. There was a significant main effect of epoch for both fixation count, <italic>F</italic>(3, 54) = 213.53, η<sub>p</sub>
<sup>2</sup> = .92, <italic>p</italic> &lt; .01, and fixation duration, <italic>F</italic>(3, 54) = 345.62, η<sub>p</sub>
<sup>2</sup> = .95, <italic>p</italic> &lt; .01. The fixation count result was expected given the differing epoch durations, but the fixation duration result indicates there may be something in the scene changing how long participants dwelled on features. The interaction of epoch and feature for fixation count, <italic>F</italic>(7.67, 138.01) = 8.20, η<sub>p</sub>
<sup>2</sup> = .31, <italic>p</italic> &lt; .01, and fixation duration, <italic>F</italic>(7.86, 141.44) = 5.51, η<sub>p</sub>
<sup>2</sup> = .23, <italic>p</italic> &lt; .01, indicates that the participants changed their visual behaviour in response to the dynamic content within the epoch. The R UE/LE and environment garnered the largest fixation count and longest duration during Epochs 1 and 2 when the client was searching for upper-extremity support prior to the transfer movement, while the right side of the wheelchair and environment received an increased amount of fixations and time spent during the final epoch as the client descended into the wheelchair.</p>
</sec>
<sec id="section12-0008417413484885">
<title>Self-Reported Observation Strategy</title>
<p>Participants were purposefully given no specific instructions for their observation, which allowed them to freely use their own viewing strategy (<xref ref-type="bibr" rid="bibr18-0008417413484885">Tatler, Baddeley, &amp; Gilchrist, 2005</xref>). Seventeen participants (10 trained and 7 non-trained) self-reported three viewing strategies; 1 non-trained participant provided two strategies, and 2 non-trained participants reported only one strategy. In the absence of task instructions, it is interesting to note that the observation strategies reported by the participants were similar. The self-reported strategies from both groups clustered into three themes, indicating an attempt to view the quality of the person’s movement or posture (10 trained, 6 non-trained), the safety of the person to complete the task (9 trained, 5 non-trained), and a scan of the environment beyond the person and task items (6 trained, 4 non-trained). Note that some of the participants’ responses fell into the same theme.</p>
</sec>
</sec>
<sec id="section13-0008417413484885">
<title>Discussion</title>
<p>Observations of performance are a mainstay of assessment in occupational therapy, but there is limited information about therapist behaviour in this context. Drawing on our past work with static image observation, the present study sought to discern if there was a difference in eye movement patterns between occupational therapists and non–health care professionals during free observation of occupational performance using dynamic scenes. It was predicted that there would be some similarities in the observational performance of the trained and non-trained groups with regard to highly salient visual stimuli, such as motion. However, differences between groups were anticipated for specific features at certain times during the videos, as these could be salient to trained occupational therapists due to the potential for safety concerns.</p>
<p>Our measurements identified several observational characteristics shared by both groups and that are consistent with studies from other observational contexts that found that motion influences visual attention (e.g., <xref ref-type="bibr" rid="bibr9-0008417413484885">Itti, 2005</xref>). As evidenced by the main effects of feature and epoch, and the interaction between feature and epoch in all three videos, fixations were not random or equally distributed throughout all the features and all epochs. These findings indicate that both groups of observers were consistently drawn to features associated with movement during different phases of the videos, with one exception. In the activity epochs with large movements (Toilet Epochs 3 and 4) or where the movement becomes larger on the screen (Grass Epoch 3), the fixation duration increased and the saccade amplitude decreased. Saccade amplitude has been reported to be most affected by the size of stimulus presented (<xref ref-type="bibr" rid="bibr21-0008417413484885">von Wartburg et al., 2007</xref>), so the amplitude change could simply be related to the relative size of the client as he or she moves toward the camera (Toilet Epochs 3 and 4 and Grass Epoch 3).</p>
<p>The relatively great number of fixations directed toward the environment in all of the videos suggests that visual behaviour is guided, in part, by anticipation of the targets for current or future actions. Fixations on features in the environment during the toilet transfer, or looking at the shelf during the kitchen video, might be directed by top-down influences in anticipation of interactions with the task object(s). Consistent with this possibility, other studies of dynamic observation have shown that observers demonstrate “look-ahead” fixations anticipating their own movement or the next task component they must achieve (<xref ref-type="bibr" rid="bibr11-0008417413484885">Land &amp; Hayhoe, 2001</xref>). Additionally, “joint attention” behaviour (<xref ref-type="bibr" rid="bibr15-0008417413484885">Mundy &amp; Newell, 2007</xref>), wherein the observer is drawn to look at the same features being attended to by others, may also be influencing the participants’ visual behaviour (e.g., searching the same locations as the client in the video when placing the hand for support during Toilet Epoch 1 and looking into the cupboard prior to reaching for a cup in Kitchen Epoch 2). Joint attention could serve to facilitate observation or distract the observer from searching other critical locations of inquiry, similar to how magicians are able to successfully use gaze to misdirect the visual attention of their audience (<xref ref-type="bibr" rid="bibr10-0008417413484885">Kuhn, Tatler, &amp; Cole, 2009</xref>).</p>
<p>In contrast to our previous work with static images, the results of the present study reveal surprisingly few differences between groups in observational behaviour. No group main effect or interactions involving group were seen for the kitchen or grass videos, whereas a significant main effect of group was found for the toilet video. The lack of difference for the kitchen and grass video may be due to the commonplace nature of the activities viewed, whereas the significant main effect of group in the toilet video, with the trained group making more fixations than the non-trained, might indicate top-down influences on visual guidance related to knowledge of the trained group for a sit-pivot toilet transfer. Others have found that scene familiarity has elicited increased fixations and short durations for experts and is suggested to be related to efficiency in grasping the gist of the scene (<xref ref-type="bibr" rid="bibr17-0008417413484885">Stevens et al., 2010</xref>).</p>
<p>Moreover, we did not identify any interactions with group in any of the videos, indicating that the trained and non-trained groups tended to fixate on the same features of the videos in the various epochs. While the self-reported viewing “strategies” were similar for both groups, there was likely considerable variability in gaze behaviour between individuals within groups due to the lack of specific task viewing instructions, which may have masked group differences. It has been well documented that instructions can affect viewing strategies and that there is a link between task instructions and where one looks (<xref ref-type="bibr" rid="bibr12-0008417413484885">Land, Mennie, &amp; Rusted, 1999</xref>). In this case, because we did not provide instructions to the groups, the trained group may not have adapted its viewing strategy to the cognitive and behavioural activity required for the assigned task (<xref ref-type="bibr" rid="bibr1-0008417413484885">Boot, Becic, &amp; Kramer, 2009</xref>) or tap into its task knowledge and subsequent specificity of directing eye movements (<xref ref-type="bibr" rid="bibr3-0008417413484885">Castelhano &amp; Henderson, 2007</xref>).</p>
<p>One concern highlighted in the current study was the lack of visual fixations directed toward the affected upper extremity in both the kitchen and toilet videos. Whereas the affected arm did not contribute to task completion due to portrayed recovery status, it is of concern that the trained group did not at least investigate the limb differently than the non-trained group given the potential for safety concerns. Specific studies aimed at identifying safety hazards during viewing of dynamic stimuli require further investigation. Additionally, the use of instructions or specific assessment forms for the trained group to complete following dynamic stimuli is important to explore for how they impact visual gaze behaviour for features viewed and the timing of the fixations.</p>
<sec id="section14-0008417413484885">
<title>Study Limitations</title>
<p>There are several limitations with this study. The results may not be representative of the larger population due to the limited range of dynamic scenes representing all activities of daily living and small sample size, which limited the power and post hoc analysis for trends in the data suggesting differences in how the groups viewed the respective video features during activity epochs. It is important to recognize that eye-tracking methodology can determine what a person is looking at but not necessarily to what they are paying attention (<xref ref-type="bibr" rid="bibr7-0008417413484885">Duchowski, 2007</xref>). It is recognized that attention is not only overt (fixating on a feature); it can be deployed covertly (no eye movement). The viewing angle and format of the videos were purposefully constructed to provide the perspective one might have if observing a client completing the task. However, inherent to providing this vantage point is the introduction of a photographer’s central bias (<xref ref-type="bibr" rid="bibr20-0008417413484885">Tseng, Carmi, Cameron, Munoz, &amp; Itti, 2009</xref>). This bias may unwittingly direct the observer’s attention to areas that the photographer found interesting, instead of exploring other areas of interest. The client was trained as per standard simulated client protocol, though the use of a simulation may raise potential for inaccuracies with diagnosis portrayal. Finally, there may be other differences within each observer’s ability, acquired knowledge or experience, and practice that were not captured with this study and might affect the ability to filter or attend to this study’s components.</p>
</sec>
</sec>
<sec id="section15-0008417413484885">
<title>Conclusion</title>
<p>In this study, only the toilet-transfer video evoked differences of eye movement between the trained and non-trained groups. While all videos contained a simulated client post-stroke, only the toilet-transfer activity likely drew upon specific knowledge and experiences of the trained group. This may be the reason that group differences were found for only the toilet-transfer video and not the others. The new methodology of this eye-tracking study provides the groundwork for future studies that attempt to deepen understanding of the nature of observation by occupational therapists (and indeed other health care professionals) with the goal of improving observational assessment performance and training.</p>
</sec>
<sec id="section16-0008417413484885">
<title>Key Messages</title>
<list list-type="bullet">
<list-item>
<p>Observers were guided by relatively specific features associated with the client’s movement during different epoch times of activities, regardless of the type of activity portrayed by the simulated client post-stroke.</p>
</list-item>
<list-item>
<p>Eye-tracking methodology is a viable research tool for investigating the dynamic nature of observation used by occupational therapists during assessment.</p>
</list-item>
</list>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure" id="fn1-0008417413484885">
<p>Funding: No funding was received in support of this study.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Boot</surname>
<given-names>W. R.</given-names>
</name>
<name>
<surname>Becic</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Kramer</surname>
<given-names>A. F.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Stable individual differences in search strategy? The effect of task demands and motivational factors on scanning strategy in visual search</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>3</issue>), <fpage>Article</fpage> 7. <comment>doi:10.1167/9.3.7</comment>
</citation>
</ref>
<ref id="bibr2-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brentnall</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Bundy</surname>
<given-names>A. C.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>The concept of reliability in the context of observational assessments</article-title>. <source>OTJR: Occupation, Participation and Health</source>, <volume>29</volume>, <fpage>63</fpage>–<lpage>71</lpage>. <comment>doi:10.3928/15394492-20090301-01</comment>
</citation>
</ref>
<ref id="bibr3-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Castelhano</surname>
<given-names>M. S.</given-names>
</name>
<name>
<surname>Henderson</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Initial scene representations facilitate eye movement guidance in visual search</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>33</volume>, <fpage>753</fpage>–<lpage>763</lpage>. <comment>doi:10.1037/0096-1523.33.4.753</comment>
</citation>
</ref>
<ref id="bibr4-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Castelhano</surname>
<given-names>M. S.</given-names>
</name>
<name>
<surname>Mack</surname>
<given-names>M. L.</given-names>
</name>
<name>
<surname>Henderson</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Viewing task influences eye movement control during active scene perception</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>3</issue>), <fpage>Article</fpage> 6. <comment>doi:10.1167/9.3.6</comment>
</citation>
</ref>
<ref id="bibr5-0008417413484885">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cohen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1988</year>). <source>Statistical power analysis for the behavioral sciences</source> (<edition>2nd ed</edition>.). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr6-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dorr</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Martinetz</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Gegenfurtner</surname>
<given-names>K. R.</given-names>
</name>
<name>
<surname>Barth</surname>
<given-names>E.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Variability of eye movements when viewing dynamic natural scenes</article-title>. <source>Journal of Vision</source>, <volume>10</volume>(<issue>10</issue>), <fpage>Article</fpage> 28. <comment>doi:10.1167/10.10.28</comment>
</citation>
</ref>
<ref id="bibr7-0008417413484885">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Duchowski</surname>
<given-names>A. T.</given-names>
</name>
</person-group> (<year>2007</year>). <source>Eye tracking methodology</source> (<edition>2nd ed</edition>.). <publisher-loc>London, UK</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr8-0008417413484885">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Henderson</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Brockmole</surname>
<given-names>J. R.</given-names>
</name>
<name>
<surname>Castelhano</surname>
<given-names>M. S.</given-names>
</name>
<name>
<surname>Mack</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Visual saliency does not account for eye movements during visual search in real-world scenes</article-title>. In R. van Gompel, M. Fischer, W. Murray, &amp; R. Hill (Eds.), <source>Eye movements: A window on mind and brain</source> (pp. <fpage>537</fpage>–<lpage>562</lpage>). <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr9-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Itti</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Quantifying the contribution of low-level saliency to human eye movements in dynamic scenes</article-title>. <source>Visual Cognition</source>, <volume>12</volume>, <fpage>1093</fpage>–<lpage>1123</lpage>. <comment>doi:10.1080/13506280444000661</comment>
</citation>
</ref>
<ref id="bibr10-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kuhn</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Tatler</surname>
<given-names>B. W.</given-names>
</name>
<name>
<surname>Cole</surname>
<given-names>G. G.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>You look where I look! Effect of gaze cues on overt and covert attention in misdirection</article-title>. <source>Visual Cognition</source>, <volume>17</volume>, <fpage>925</fpage>–<lpage>944</lpage>. <comment>doi:10.1080/13506280902826775</comment>
</citation>
</ref>
<ref id="bibr11-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Land</surname>
<given-names>M. F.</given-names>
</name>
<name>
<surname>Hayhoe</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>In what ways do eye movements contribute to everyday activities?</article-title> <source>Vision Research</source>, <volume>41</volume>, <fpage>3559</fpage>–<lpage>3565</lpage>. <comment>doi:10.1016/S0042-6989(01)00102-X</comment>
</citation>
</ref>
<ref id="bibr12-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Land</surname>
<given-names>M. F.</given-names>
</name>
<name>
<surname>Mennie</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Rusted</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>The roles of vision and eye movements in the control of activities of daily living</article-title>. <source>Perception</source>, <volume>28</volume>, <fpage>1311</fpage>–<lpage>1328</lpage>. <comment>doi:10.1068/p2935</comment>
</citation>
</ref>
<ref id="bibr13-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>MacKenzie</surname>
<given-names>D. E.</given-names>
</name>
<name>
<surname>Westwood</surname>
<given-names>D. A.</given-names>
</name>
</person-group> (<year>2013</year>). <article-title>Occupational therapists and observation: What are you looking at?</article-title> <source>OTJR: Occupation, Participation and Health</source>, <volume>33</volume>, <fpage>4</fpage>–<lpage>11</lpage>. <comment>doi:10.3928/15394492-20120928-01</comment>
</citation>
</ref>
<ref id="bibr14-0008417413484885">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Miller</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Huijbregts</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Gowland</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Barreca</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Torresin</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Moreland</surname>
<given-names>J., </given-names>
</name>
</person-group>…<person-group>
<name>
<surname>Barclay-Goddard</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Chedoke-McMaster Stroke Assessment: Development, validation and administration manual</source> <edition>(Rev. ed.)</edition>. <publisher-loc>Hamilton, ON</publisher-loc>: <publisher-name>McMaster University and Hamilton Health Sciences</publisher-name>.</citation>
</ref>
<ref id="bibr15-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mundy</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Newell</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Attention, joint attention, and social cognition</article-title>. <source>Current Directions in Psychological Science</source>, <volume>16</volume>, <fpage>269</fpage>–<lpage>274</lpage>. <comment>doi:10.1111/j.1467-8721.2007.00518.x</comment>
</citation>
</ref>
<ref id="bibr16-0008417413484885">
<citation citation-type="web">
<collab collab-type="author">SR Research Ltd</collab>. (<year>n.d</year>.). <source>Complete eye tracking solutions</source>. <comment>Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.sr-research.com">http://www.sr-research.com</ext-link>
</citation>
</ref>
<ref id="bibr17-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stevens</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Winskel</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Studies</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Howell</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Vidal</surname>
<given-names>L.-M.</given-names>
</name>
<name>
<surname>Latimer</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Milne-Home</surname>
<given-names>J.</given-names>
</name>
</person-group>, (<year>2010</year>). <article-title>Perceiving dance: Schematic expectations guide experts’ scanning of contemporary dance film</article-title>. <source>Journal of Dance Medicine and Science</source>, <volume>14</volume>, <fpage>19</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr18-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tatler</surname>
<given-names>B. W.</given-names>
</name>
<name>
<surname>Baddeley</surname>
<given-names>R. J.</given-names>
</name>
<name>
<surname>Gilchrist</surname>
<given-names>I. D.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Visual correlates of fixation selection: Effects of scale and time</article-title>. <source>Vision Research</source>, <volume>46</volume>, <fpage>1857</fpage>–<lpage>1862</lpage>. <comment>doi:10.1016/j.visres.2004.09.017</comment>
</citation>
</ref>
<ref id="bibr19-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tatler</surname>
<given-names>B. W.</given-names>
</name>
<name>
<surname>Hayhoe</surname>
<given-names>M. M.</given-names>
</name>
<name>
<surname>Land</surname>
<given-names>M. F.</given-names>
</name>
<name>
<surname>Ballard</surname>
<given-names>D. H.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Eye guidance in natural vision: Reinterpreting salience</article-title>. <source>Journal of Vision</source>, <volume>11</volume>(<issue>5</issue>), <fpage>Article</fpage> 5. <comment>doi:10.1167/11.5.5</comment>
</citation>
</ref>
<ref id="bibr20-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tseng</surname>
<given-names>P.-H.</given-names>
</name>
<name>
<surname>Carmi</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Cameron</surname>
<given-names>I. G. M.</given-names>
</name>
<name>
<surname>Munoz</surname>
<given-names>D. P.</given-names>
</name>
<name>
<surname>Itti</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Quantifying center bias of observers in free viewing of dynamic natural scenes</article-title>. <source>Journal of Vision</source>, <volume>9</volume>(<issue>7</issue>), <fpage>Article</fpage> 4. <comment>doi:10.1167/9.7.4</comment>
</citation>
</ref>
<ref id="bibr21-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>von Wartburg</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Wurtz</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Pflugshaupt</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Nyffeler</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Lüthi</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Müri</surname>
<given-names>R. M.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Size matters: Saccades during scene perception</article-title>. <source>Perception</source>, <volume>36</volume>, <fpage>355</fpage>–<lpage>365</lpage>. <comment>doi:10.1068/p5552</comment>
</citation>
</ref>
<ref id="bibr22-0008417413484885">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zou</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Muller</surname>
<given-names>H. J.</given-names>
</name>
<name>
<surname>Shi</surname>
<given-names>Z.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Non-spatial sounds regulate eye movements and enhance visual search</article-title>. <source>Journal of Vision</source>, <volume>12</volume>(<issue>5</issue>), <fpage>Article</fpage> 2. <comment>doi:10.1167/12.5.2</comment>
</citation>
</ref>
</ref-list>
</back>
</article>