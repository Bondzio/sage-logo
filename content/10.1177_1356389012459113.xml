<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EVI</journal-id>
<journal-id journal-id-type="hwp">spevi</journal-id>
<journal-title>Evaluation</journal-title>
<issn pub-type="ppub">1356-3890</issn>
<issn pub-type="epub">1461-7153</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1356389012459113</article-id>
<article-id pub-id-type="publisher-id">10.1177_1356389012459113</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The use and usability of evaluation outputs: A social practice approach</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Saunders</surname><given-names>Murray</given-names></name>
</contrib>
<aff id="aff1-1356389012459113">Lancaster University, UK</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-1356389012459113">Murray Saunders, HERE@Lancaster, Department of Educational Research, County South, Lancaster University, Lancaster LA14YD, UK. Email: <email>m.saunders@lancaster.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>18</volume>
<issue>4</issue>
<fpage>421</fpage>
<lpage>436</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This article contributes to the discussion of evaluation use. It argues for a social practice approach to the analysis of evaluation use that enables a discerning and fine-grained understanding of how evaluations might be used by real people in real time. It suggests a distinction between two dimensions of the way an evaluation might be used. It offers an interpretation of ‘use’, which focuses on the context and the capacity of the organizational setting in which evaluation outputs are used; and ‘usability’, which emphasizes the extent to which the evaluation design itself militates against or encourages the use of its outputs in the broadest sense. The two dimensions are distinct yet closely interrelated. The article concludes with a consideration of various approaches and tools that highlight the dimensions of use and usability from a social practice perspective.</p>
</abstract>
<kwd-group>
<kwd>evaluation use</kwd>
<kwd>usability</kwd>
<kwd>social practice</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p><disp-quote>
<p>A review of evaluations of these programmes in different Member States reveals their potential importance. At a basic level, the information generated by evaluations of these programmes can help the Commission and Member States monitor their impact. This, in turn, can inform debates at EU and national levels on the merits and future of Cohesion policy. Used effectively, evaluation offers a range of specific benefits: stronger public sector planning; more efficient deployment of resources; improved management and implementation of programmes or other policy interventions; stronger ownership and partnership amongst actors with a stake in programmes; greater understanding of the factors determining the success of programmes; and broader scope to assess the value and costs of interventions. (<xref ref-type="bibr" rid="bibr9-1356389012459113">Ferry and Olejniczak, 2008</xref>: 6)</p>
</disp-quote></p>
<sec id="section1-1356389012459113" sec-type="intro">
<title>Introduction</title>
<p>The quote introducing this article is from a recent report on the way evaluations were used in the context of social cohesion<sup><xref ref-type="fn" rid="fn1-1356389012459113">1</xref></sup> and social fund programmes<sup><xref ref-type="fn" rid="fn2-1356389012459113">2</xref></sup> in Europe (taking Poland as a country case study). It embodies the high expectations of evaluations. At issue is their capacity to yield resources for managing developments in policy and practice. In short, evaluations ought to offer much by way of responsive policy development but this resource is often under-used or its potential remains opaque. The report (<xref ref-type="bibr" rid="bibr9-1356389012459113">Ferry and Olejniczak, 2008</xref>) is a serious attempt to grapple with the issue of evaluation use in the European context and heralds a move to engage with an issue that has a substantial American literature, including some important syntheses or meta reviews, but has hitherto been relatively undeveloped in Europe.</p>
<p>This article addresses the development of this thinking by contributing some reflections that might be useful in building a systematic consideration of use into the design of evaluations in the European policy context. While it seems axiomatic that evaluations should be used, a planned and rehearsed approach to the way this may happen is not always undertaken as part of a design brief or proposal. Usually the discussion does not go beyond a description of the way in which the output from an evaluation might be disseminated. This said, we know, as others have suggested (e.g. <xref ref-type="bibr" rid="bibr10-1356389012459113">Fleischer and Christie, 2009</xref>; <xref ref-type="bibr" rid="bibr35-1356389012459113">Weiss, 1979</xref>), use practices are not necessarily predictable, positive or controllable.</p>
<p>This article emphasizes a distinction between the contexts of <italic>use</italic> of evaluations on the one hand, which refers here to the practices associated with the way in which the outputs of an evaluation may or may not be used in an organizational setting or policy context as a resource for onward practice, policy or decision making. It highlights the context of an evaluation; in particular, the organizational capacity of potential ‘users’ to respond to evaluation ‘outputs’ in its broadest sense (it also refers to what has been termed ‘process use’ (<xref ref-type="bibr" rid="bibr22-1356389012459113">Patton, 1998</xref>) or the way in which an evaluation interacts with its ‘social context’ to create effects in an organization policy context while it is being undertaken). On the other hand, I refer to the <italic>usability</italic> of evaluation (the extent to which the design of an evaluation – both its output and the way it is undertaken – maximizes, facilitates or disables its potential use).</p>
<p>The focus, in particular, is the interplay between the processes and practices associated with use in the way I define above (which may be informal and tacit as well as more conventional conceptions associated with a text in the form of a report) and the processes and practices associated with usability. It will do so by suggesting that foregrounding evaluation as a social practice is a useful frame for considering the distinction. In turn, the argument for giving this approach attention is based on a view that the impact of an evaluation, in particular the use of its outputs, can be measured in terms of the changes in practice it might shape or influence (including reified practice in the form of systems or depicted in economic performance). To adopt this approach, we need to have a robust understanding of what practices are and how they might change.</p>
<p>The article has been developed from three sources: first, evaluative research on higher education within the UK and internationally (see <xref ref-type="bibr" rid="bibr28-1356389012459113">Saunders et al., 2011</xref>); second, from participation in the exploration of evaluation use as part of the work of DG of Regional Policy in the European Union<sup><xref ref-type="fn" rid="fn3-1356389012459113">3</xref></sup>; third, the approach was developed and trialled in a large EU funded R&amp;D project named PALETTE in which the evaluation work package had an important part. The focus, summarized in the final report on the project (<xref ref-type="bibr" rid="bibr20-1356389012459113">PALETTE, 2009</xref>), synthesizes the extensive research undertaken in the project of responses of members of the PALETTE project to formative evaluation. The conclusion of the report offers summative observations and recommendations concerning evaluation, change and its use in supporting learning within Communities of Practice. It argues that:
<disp-quote>
<p>Integrating evaluation more solidly in decision-making raises questions about the ‘usability’ of evaluation outputs for such a task. By usability we understand the relationship between the ‘design’ of the evaluative activity and the use to which it is put. (<xref ref-type="bibr" rid="bibr20-1356389012459113">PALETTE, 2009</xref>: 37)</p>
</disp-quote></p>
</sec>
<sec id="section2-1356389012459113">
<title>A note on the background to studies of ‘use’</title>
<p>As I note above, there has been an extensive meta-analytical literature from the USA on use and utilization of evaluation. This meta-analytical preoccupation reflects the difficulties in using the concept of ‘use’. The term has its problems, as <xref ref-type="bibr" rid="bibr14-1356389012459113">Kirkhart (2000)</xref> reminds us. In the estimation of the effects of an evaluation, Kirkhart suggests that the term is an:
<disp-quote>
<p>awkward, inadequate, and imprecise fit with non results based applications, the production of unintended effects, and the gradual emergence of impact over time. Second, when the history of influence is traced from the approach of results based use, the historical roots of other dimensions of evaluation impact are erased. Process use, for example, incorrectly appears as an afterthought, a late arrival. Third, fitting other types of influence under a results based paradigm continues to privilege the concept of results based use. Other types of use are secondary, ‘tacked on’ or seen as important primarily in the service of results based use. (<xref ref-type="bibr" rid="bibr14-1356389012459113">Kirkhart, 2000</xref>: 6)</p>
</disp-quote></p>
<p><xref ref-type="bibr" rid="bibr14-1356389012459113">Kirkhart (2000</xref>: 7), in commenting on the work of <xref ref-type="bibr" rid="bibr35-1356389012459113">Weiss (1979)</xref> some 20 years before, suggests that reconstructing our thinking, as Weiss does, by making a distinction between utilization and use, thus widening the scope and implications of the focus on how evaluations might create ‘effects’, still requires a discursive shift to the idea of ‘influence’. However, in examining broad questions derived from reviews of the literature about the use of what we might call ‘enquiry based knowledge’ from social research (later applied to evaluation), it reveals that a diverse array of meanings is attached to the term. Weiss suggests that much of the ambiguity in the discussion of ‘research utilization and conflicting interpretations of its prevalence and the routes by which it occurs-derives from conceptual confusion’. She argues that ‘if we are to gain a better understanding of the extent to which social science research has affected public policy in the past, and learn how to make its contribution more effective in the future, we need to clarify the concept’ (<xref ref-type="bibr" rid="bibr35-1356389012459113">Weiss, 1979</xref>: 426).</p>
<p>The synthesizing work of both Kirkhart and Weiss (also in <xref ref-type="bibr" rid="bibr36-1356389012459113">Weiss, 1998</xref>) provides us with an overview of studies on ‘use’ and ‘utilization’ that act as a resource for further overviews in which the issue of use is reconsidered. They draw our attention to the diversity of use, its unpredictability and its political nature. They identify many potentially relevant factors, but are equivocal about which are most related to <italic>increasing</italic> positive evaluation use. They tend to emphasize the diverse ways in which evaluations are used rather than the conditions under which positive use (i.e. for development) seems to occur or why, theoretically, practices of use are as they are (exceptionally, <xref ref-type="bibr" rid="bibr21-1356389012459113">Patton’s [1997]</xref> work significantly identifies a range of factors; see below).</p>
<p>Recent commentators have asserted, however, that the phenomenon of ‘use’ is multifaceted (<xref ref-type="bibr" rid="bibr16-1356389012459113">Ledermann, 2011</xref>) and prescriptions are of limited value. Also relevant is the assertion from <xref ref-type="bibr" rid="bibr12-1356389012459113">Hofstetter and Alkin (2003)</xref>, with reference to the US environment, that:
<disp-quote>
<p>for as long as modern-day evaluations have existed, so too have questions about evaluation use. As with social science research and knowledge, there remain ongoing concerns about the level of importance of conducting evaluation and producing information that leads to action, be it in creating policy, decision making, or changing how someone may regard a particular issue. We assume that our efforts are rewarded with some importance or action, although measuring the effects evaluation has will continue to prove problematic. (<xref ref-type="bibr" rid="bibr12-1356389012459113">Hofstetter and Alkin, 2003</xref>: 219)</p>
</disp-quote></p>
<p>A more recent review by <xref ref-type="bibr" rid="bibr15-1356389012459113">Johnson et al. (2009)</xref>, for example, with reference to the research on evaluation use between 1986 and 2005, point to shifts in focus and concern:
<disp-quote>
<p>Moving beyond the first quarter century of use research, the new millennium has witnessed theoretical activity that has reconceptualised the field’s understanding of its impact. Scholars now view evaluations as having intangible influence on individuals, programs, and communities. Focusing solely on the direct use of either evaluation results or processes has not adequately captured broader level influences. (<xref ref-type="bibr" rid="bibr15-1356389012459113">2009</xref>: 378)</p>
</disp-quote></p>
<p>These wider considerations in the discussion on use refer to the way the knowledge resources produced by evaluations have effects on practice that have a more subtle nuance.</p>
<p>Important for the Johnson et al. study was the earlier <xref ref-type="bibr" rid="bibr5-1356389012459113">Cousins and Leithwood (1986)</xref> work in which they identified 65 empirical studies of evaluation use conducted between 1971 and 1985 through computerized searches of keywords including ‘evaluation utilization’, ‘data use’, ‘decision making’, and ‘knowledge utilization’ (<xref ref-type="bibr" rid="bibr15-1356389012459113">Johnson et al., 2009</xref>: 379). Of particular interest to them were two groups of factors that impacted on use; namely, (a) characteristics of evaluation implementation, and (b) characteristics of the decision or policy setting (p. 379). <xref ref-type="bibr" rid="bibr10-1356389012459113">Fleischer and Christie (2009)</xref>, building on this, predominantly, American literature on use (see above), demonstrate the different emphases and preoccupations.</p>
<p>We also have the important contribution to this debate offered by successive editions of <italic>Utilization-Focused Evaluation</italic>, by <xref ref-type="bibr" rid="bibr21-1356389012459113">Patton (1997)</xref>, in which he asserts that the potential for use is largely determined a long time before ‘a study is completed’, thus pointing to the importance of ‘design’, or usability. He argues that ‘utilization-focused evaluation emphasises that what happens from the very beginning of a study will determine its eventual impact long before a final report is produced’ (<xref ref-type="bibr" rid="bibr21-1356389012459113">1997</xref>: 20).</p>
<p>For Patton, an evaluation that is focused on use, ‘concerns how real people in the real world apply evaluation findings and experience the evaluation process . . . it is about intended use by intended users’ (<xref ref-type="bibr" rid="bibr21-1356389012459113">1997</xref>: 20). He acknowledges that in an evaluative environment there are likely to be an array of evaluation users as well as uses of programme outputs, but he argues for a particular emphasis on primary intended users and their commitment to specific concrete uses. Further, he argues for the highly contextualized nature of these kinds of decisions, along with the overview offered by <xref ref-type="bibr" rid="bibr31-1356389012459113">Shulha and Cousins (1997)</xref> in which they identify the rise of considerations of context as critical to understanding and explaining use.</p>
<p>This is broadly the stance taken in this article. While Patton offers us an emphatic approach to the problem of use, he emphasizes, in particular, a relatively narrow concern with potential users as commissioners. The enduring issue of use is graphically illustrated by the following observation by Weiss in which she emphasizes the expectations of use we may have as evaluators:
<disp-quote>
<p>As evaluators, we undertake our studies with the intention of helping decision makers make wiser decisions. We provide evidence that shows the successes and shortcomings of programs. We identify some of the factors that are associated with better and worse outcomes. And, we often try to explain how the program works in practice and why it leads to the effects we observe. We expect that these data will feed into the decision making process and influence the actions that people take at the staff level, at management levels, or in the higher reaches of decision making. (<xref ref-type="bibr" rid="bibr37-1356389012459113">Weiss, 2005</xref>: 286)</p>
</disp-quote></p>
<p><xref ref-type="bibr" rid="bibr10-1356389012459113">Fleischer and Christie (2009)</xref> note these important reviews of studies of ‘use’ in the US evaluation literature. Building on their ‘overview of overviews’ it is possible to identify the typology of use presented in <xref ref-type="fig" rid="fig1-1356389012459113">Figure 1</xref>.</p>
<fig id="fig1-1356389012459113" position="float">
<label>Figure 1.</label>
<caption>
<p>Typologies of use.</p>
</caption>
<graphic xlink:href="10.1177_1356389012459113-fig1.tif"/>
</fig>
<p>The synthesis of ‘use domains’ identified by <xref ref-type="bibr" rid="bibr10-1356389012459113">Fleischer and Christie (2009)</xref> are helpful, but from the approach of this article, these domains can be understood as clusters of ‘use practices’. What is missing is a way of approaching the interplay between context and design in the discussion of evaluation use and usability. This article argues that a social practice approach suggests that ‘use’ and ‘usability’ are essentially about the intersection of forms of practice and the way they may be changed. If we are to improve the capacity of evaluations to be used (to contribute to changes in practice) then we need, not only to typologize forms of use, which has a rich and well rehearsed history, but have a robust understanding of what practices are, how they might change and, further, identify practices of evaluation design that might encourage use.</p>
</sec>
<sec id="section3-1356389012459113">
<title>Using social practice theory to examine use and usability</title>
<p>To emphasize use and usability as practice requires an understanding of practice as situated or contextualized. The following observation made by Ledermann in her recently completed research, is persuasive:
<disp-quote>
<p>The study reported in this article follows the calls in recent years that it is time to abandon the ambition of finding ‘the important’ characteristic for use and to adopt a focus on context-bound mechanisms of use instead. (<xref ref-type="bibr" rid="bibr16-1356389012459113">2011</xref>: 160)</p>
</disp-quote></p>
<p>The implication of adopting this approach is that it is important to discern the domain and the clusters of practices that are emphasized or inhabit that domain. It allows for a broad focus on the kinds of ‘effect’ an evaluation might have but draws our attention to the ‘situatedness’ of these effects and the practices that a) might bring them about and b) emphasize on the effects as essentially concerned with changes in ‘practice’.</p>
<p>The focus of this article is that people are carriers of practices: they enact in a specific reservoir of behaving, understanding and responding in ways that are to a certain extent particular to them in a social field (<xref ref-type="bibr" rid="bibr2-1356389012459113">Bernstein, 1999</xref>, distinguishes between ‘reservoir’ and ‘repertoire’ to describe this process of individuation from structural characteristics). Individuals articulate repertoires of recognizable practices: ‘patterns of bodily behaviour and routinized ways of understanding, knowing and desiring’ (<xref ref-type="bibr" rid="bibr25-1356389012459113">Reckwitz, 2002</xref>: 250). The focus on analyses of use is therefore best located in the practices, not the individuals involved.</p>
<p>As phenomena, practices can be defined as:
<disp-quote>
<p>A ‘practice’ (Praktik) is a routinized type of behaviour which consists of several elements, interconnected to one another: forms of bodily activities, forms of mental activities, ‘things’ and their use, a background knowledge in the form of understanding, know-how, states of emotion and motivational knowledge . . . [A] practice represents a pattern which can be filled out with a multitude of single and often unique actions reproducing the practice . . . the single individual – as a bodily and mental agent – then acts as the ‘carrier’ of a practice – and, in fact, of many different practices which need not be coordinated with one another. (<xref ref-type="bibr" rid="bibr25-1356389012459113">Reckwitz, 2002</xref>: 249–50)</p>
</disp-quote></p>
<p>By social practice we mean the recurrent, often unconsidered, sets of behaviours or ‘constellations’ that together constitute daily life. These behaviours are shaped by knowledge resources on which actors routinely draw. These knowledge resources can be tacit and informal as well as more explicit knowledge areas and skills. In that evaluations produce knowledge resources, it is these knowledge resources to which we refer in discussing ‘use’ and ‘usability’. For the purposes of this article then, practices can be usefully conceptualized as sets or clusters of behaviours forming ways of ‘thinking and doing’ associated with evaluation use.</p>
<p>The application of social practice theory to evaluation use leads us to take an approach that emphasizes the way evaluation use is embedded in specific national and regional contexts. Without this appreciation of the contextual factors that affect use, use strategies – as part of the design of an evaluation – are likely to be less effective (<xref ref-type="bibr" rid="bibr24-1356389012459113">Pawson and Tilley, 1997</xref>: 114). This means that in this approach, evaluation use:</p>
<list id="list1-1356389012459113" list-type="bullet">
<list-item><p>can be considered as a subset of evaluation practice in any social policy area;</p></list-item>
<list-item><p>involves dimensions of practice consisting of symbolic structures, particular orders of meaning in particular places, and has unintended effects;</p></list-item>
<list-item><p>consists of practices that use implicit, tacit or unconscious knowledge as well as explicit knowledge;</p></list-item>
<list-item><p>can have progressive enabling characteristics but are also perceived and experienced as controlling, as part of a ‘surveillance’ culture.</p></list-item>
</list>
<p><xref ref-type="bibr" rid="bibr32-1356389012459113">Stern (2006</xref>: 293) has outlined the complexities of the widely shared view that evaluation is a ‘practical craft’ by identifying at least seven taken for granted assumptions about where the emphasis should be. These range from evaluation as technical practice, as judgement, as management, as polemical and as social practice.</p>
<p>What is the added value of the ‘social practice’ dimension? I argue that the value is in three ways. First, understanding use and usability as ‘practice’ provides analytical purchase on the knowledge resources stakeholders use to make sense of evaluation and the response (as practice) they might have to both its process and outputs (these might be negative as well as positive). Second, it enables a nuanced approach to how evaluations have <italic>influence</italic>, leading to change, as well as how the experience of an evaluation becomes a knowledge resource for action. In that we know ‘practice’ tends to be ‘engrooved’ (see <xref ref-type="bibr" rid="bibr13-1356389012459113">Huberman, 1994</xref>), it is difficult to get practices to change. If we adopt the perspective that use is about ‘changing practice’, what might be the nature of the knowledge resources available from an evaluation that creates the momentum for change (i.e. for it to be used)? Third, it enables the identification of practices of ‘engagement’ (with evaluation outputs, for example) that facilitate the process of establishing what changes in practice flow from the knowledge resources an evaluation might yield.</p>
<p>In sum, the social practice approach enables analysis that focuses on the underlying dynamic of change embedded in the idea of ‘use’.</p>
</sec>
<sec id="section4-1356389012459113">
<title>Modelling evaluation use and usability practices</title>
<p>Using the social practice approach to think about use practices in Higher Education, recent research suggested evaluation use inhabits at least four ‘domains’ of practice. This model was based on the analysis of 16 international case studies of evaluative practice in the Higher Education policy sector (see <xref ref-type="bibr" rid="bibr28-1356389012459113">Saunders et al., 2011</xref>). I address these domains in detail below in <xref ref-type="fig" rid="fig2-1356389012459113">Figure 2</xref>.</p>
<fig id="fig2-1356389012459113" position="float">
<label>Figure 2.</label>
<caption>
<p>Use Practice clusters.</p>
</caption>
<graphic xlink:href="10.1177_1356389012459113-fig2.tif"/>
</fig>
<p>These domains are not definitive but serve the purposes of this article by illustrating how the contexts of evaluation use produce different types of ‘practice cluster’. It is in line with the view of <xref ref-type="bibr" rid="bibr16-1356389012459113">Ledermann (2011)</xref>, above, concerning the ‘situatedness’ of evaluation use. As <xref ref-type="bibr" rid="bibr32-1356389012459113">Stern (2006)</xref> points out, these expressions may well be very different in focus and not particularly consistent; it is these differences that <xref ref-type="fig" rid="fig2-1356389012459113">Figure 2</xref> illustrates in the context of use.</p>
<p>The indicative differences depicted by using a social practice approach in <xref ref-type="fig" rid="fig2-1356389012459113">Figure 2</xref> are drawn from the Higher Education sector. If this broad model of domains of practice is generalizable to other areas (health, criminal justice, general education, social services), then it suggests how the practice clusters associated with evaluation use differ between the domains in which evaluative practice occurs (to regulate national or regional systems or to enhance quality within an organization). Further research is required to test the hypothesis that these domains of evaluative practice associated with use are embedded in social policy areas in general.</p>
<p>The <xref ref-type="bibr" rid="bibr9-1356389012459113">Ferry and Olejniczak report (2008)</xref> is based on a study that proceeded by addressing five propositions about the factors that might influence effective use within the paradigm identified above, and is a systematic and thorough overview of a range of methodological, paradigmatic and strategic considerations.</p>
<p>What is helpful for the purposes of this article are the propositions the report contains about the characteristics that determine ‘use practice’ summarized below:</p>
<list id="list2-1356389012459113" list-type="bullet">
<list-item><p>the characteristics of the learner or receiver of the evaluation;</p></list-item>
<list-item><p>the characteristics of the policy being evaluated;</p></list-item>
<list-item><p>the timing of evaluations;</p></list-item>
<list-item><p>the approach taken to evaluation;</p></list-item>
<list-item><p>the quality of the report.</p></list-item>
</list>
<p>I suggest that these factors are conflating a distinction that is important for a use-based evaluation design. In the discourse offered in this article, <xref ref-type="bibr" rid="bibr9-1356389012459113">Ferry and Olejniczak (2008)</xref> demonstrate the existence of practice clusters that can be further understood by distinguishing between ‘use’ and ‘usability’.</p>
<sec id="section5-1356389012459113">
<title>Use practices</title>
<p>This refers to the interaction between the organizational environment into which an evaluation output might be intervening and the design of the evaluation output itself. Both these features interact to determine the extent to which an evaluation output or an evaluation process can create effects (i.e. change practices). <xref ref-type="bibr" rid="bibr4-1356389012459113">Contandriopoulos and Brouselle (2012)</xref>, in their extensive review of collective level knowledge exchange, conceptualize two dimensions of use context. On the one hand they point to how the nature, degree and power of the ideological dimension prompts differential polarization, which, in turn determines the extent to which potential users share similar outlooks and preferences. The second dimension they identify is associated with the extent to which potential users estimate the costs and benefits of changing practices based on the resources offered by new knowledge. Both these features suggest the centrality of ‘practice’ in considering the way knowledge resources produced by evaluation might be used.</p>
<p>The way an evaluation is used depends on the capacity of the potential users to respond to the messages an evaluation might contain. In social practice terms, it is the degree to which the evaluation can provide knowledge resources for new practices. The extent to which knowledge resources are recognized as ‘useful’ by potential users has ideological, temporal and organizational dimensions. The following characteristics of this ‘use environment’ are drawn from case studies of evaluation use (see above) and demonstrate the factors that seem particularly pertinent in terms of maximizing use:</p>
<list id="list3-1356389012459113" list-type="bullet">
<list-item><p>The timing and nature of the ‘release’ of the evaluation output is embedded in decision-making cycles (this requires clear knowledge on when decisions take place and who makes them);</p></list-item>
<list-item><p>The evaluators and commissioners have a clear understanding of the organizational or sectoral memory and are able to locate the evaluation within an accumulation of evaluative knowledge;</p></list-item>
<list-item><p>The evaluation has reflexive knowledge of the capacity of an organization or commissioning body to respond. This requires the following dimensions:
<list id="list4-1356389012459113" list-type="bullet">
<list-item><p>the evaluation output connects effectively with systemic processes. This means the messages are able to feed into structures that are able to identify and act on implications;</p></list-item>
<list-item><p>organizations that are lightly bureaucratized, sometimes called adaptive systems, are better placed to respond to ‘tricky’ or awkward evaluations because their practices are more fluid, less mechanistic and have a ‘history’ of responding to new knowledge;</p></list-item>
<list-item><p>evaluations that are strongly connected to power structures are more likely to have effects because they have champions who have a stake in using evaluations to change practices;</p></list-item>
<list-item><p>evaluation outputs that identify or imply possible changes that are congruent or build on what is already in place have a greater chance of maximizing use.</p></list-item>
</list></p></list-item>
</list>
</sec>
<sec id="section6-1356389012459113">
<title>Usability practice</title>
<p>Prosaically, most research into evaluation usability refers to the form the evaluation outputs take and the extent to which they ‘speak’ to the intended user in an appropriate way (i.e. enable their use as knowledge resources for new practices). The contextual dimensions of this we have called the ‘use’ domain. The other key dimension refers to the design of the vehicle of the message to maximize engagement (a single, cold unresponsive text, a presentation, working in a workshop, working alongside a user to draw out practice based implications, etc.) but also the way in which the design of the evaluation lends itself to communicability and is formed in such a way that its potential as a knowledge resource is made more apparent.</p>
<p>In social practice discourse, this is sometimes referred to as a ‘boundary object’ or a ‘bridging tool’. In an important sense, usability is a reference to a learning process in which an ‘evaluation artefact’ (such as a report, web-based material or a social artefact like an event) is used as a knowledge resource for new practice. The term ‘bridging tool’ is guided by a specific learning theory. The idea that people might engage in a change on the basis of the use of an evaluation (moving from one activity system to another) has resonance with the notion identified by Engestrom and others (see <xref ref-type="bibr" rid="bibr33-1356389012459113">Tuomi-Grohn and Engestrom, 2003</xref> ) concerning the metaphor of ‘boundary crossing’. Conventionally associated with the experience of moving between different ‘activity systems’ and the learning processes, opportunities and, indeed, requirements, such crossing implies, activity theory has provided a fertile resource for depicting this process. Building on the work of <xref ref-type="bibr" rid="bibr34-1356389012459113">Vygotsky (1999)</xref>, <xref ref-type="bibr" rid="bibr6-1356389012459113">Engestrom and his collaborators (1999</xref>, <xref ref-type="bibr" rid="bibr7-1356389012459113">2001</xref>, <xref ref-type="bibr" rid="bibr8-1356389012459113">2004)</xref> have drawn our attention to ways of thinking that emphasize how learning takes place in a social setting involving practices shaped by tools and resources (in this case knowledge resources provided through an evaluation), communities, divisions of labour and rules such that individuals and groups experience tension, creative problem solving and resolution that utilizes these elements toward an ‘object’ or ‘project’. Often, in evaluation environments, this may be improvement or development (<xref ref-type="bibr" rid="bibr3-1356389012459113">Chelimsky, 1997</xref>).</p>
<p><xref ref-type="bibr" rid="bibr4-1356389012459113">Contandriopoulos and Brouselle (2012)</xref> focus on an approach that positions <italic>evaluative products</italic> or artefacts in the context of knowledge use and exchange. This conceptual reconfiguration is welcome, and is captured in this short extract:
<disp-quote>
<p>We thus suggest a definition of collective-level knowledge use as the process by which users incorporate specific information into action proposals to influence others’ thought, practices and collective action rules. (<xref ref-type="bibr" rid="bibr4-1356389012459113">Contandriopoulos and Brouselle, 2012</xref>: 63)</p>
</disp-quote></p>
<p>However, as suggested above, what strengthens this formulation is a robust conceptualization of what constitutes a ‘practice’. <italic>Action rules</italic> form the knowledge base on which potential users draw in new practices prompted by knowledge resources derived from an evaluation. In a recent case study of evaluation use in the context of Arts within higher education (<xref ref-type="bibr" rid="bibr30-1356389012459113">Shreeve, 2011</xref>), a high usability approach to evaluation based on an interactive and user responsive design is outlined. Interestingly, the learning points Shreeve identifies have considerable resonance with the design issues prompted by adopting the practice-based approach. Although at the ‘high risk’ end of a strategy to maximize usability, her observations are telling:
<disp-quote>
<p>We had to devolve power and control and let the respondents determine what they wanted to tell us and how they wanted to tell us. Taking a risk in this way resulted in new formats and unexpected outcomes of evaluation . . . evaluation does not always have to follow text based and verbal processes and dissemination or the ‘usability’ of evaluation is an important part of the outcome and this is dependent on the format the evaluation takes. We found acts of translation are required between different kinds of medium (words and objects) and also between different people with different previous experiences. Evaluation is also a context dependent and cultural activity. (<xref ref-type="bibr" rid="bibr30-1356389012459113">Shreeve, 2011</xref>: 185)</p>
</disp-quote></p>
<p>While commissioners and users of an evaluation are not synonymous, they can be collapsed for the purposes of this commentary. It is self evident that once an evaluation enters the public domain, if it does, then anybody can be a potential user of the evaluation if they have access to it. This is one of evaluation’s great potentialities (see <xref ref-type="bibr" rid="bibr23-1356389012459113">Patton, 2002</xref>) and suggests the urgency of establishing the levels of public access to evaluations very early on. Using the categories for evaluation design associated with RUFDATA (<xref ref-type="bibr" rid="bibr26-1356389012459113">Saunders, 2000</xref>), it is possible to chart practices that are likely to enhance the usability of the evaluation and its outputs. The framework presented in <xref ref-type="fig" rid="fig3-1356389012459113">Figure 3</xref> is intended to rehearse the practices associated with evaluation at each stage of the framing and ‘pitch’ of an evaluation. The framework has been used in the context of the PALETTE project, which was a European project associated with professional learning in a variety of contexts (<xref ref-type="bibr" rid="bibr18-1356389012459113">McCluskey et al., 2008</xref>).</p>
<fig id="fig3-1356389012459113" position="float">
<label>Figure 3.</label>
<caption>
<p>Usability practices in evaluation design.</p>
</caption>
<graphic xlink:href="10.1177_1356389012459113-fig3.tif"/>
</fig>
<p>The frames of reference of evaluators and those of evaluation commissioners are likely to be different from each other, and both differ from those emanating from practices on the ground as ‘users’ attempt to ‘enact’ policy or a programme idea. In particular, these differences mean that ‘engagement strategies’ with evaluation processes and outputs are best embedded in evaluation designs to a much greater extent than conventionally undertaken. These participative dimensions of evaluation design should be central, with the use of outputs at all stages ‘design savvy’.</p>
<p>So, as <xref ref-type="bibr" rid="bibr21-1356389012459113">Patton (1997)</xref> has so persuasively suggested, evaluators need to engage with the commissioners and with users from the start of the evaluative process and at intervals throughout it. Simply delivering a report at the end is nowhere near enough, from a social practice point of view (we return to this issue below in considering the ways in which the voice of programme recipients can be heard). Building on previous work (<xref ref-type="bibr" rid="bibr1-1356389012459113">Bamber et al., 2009</xref>), we know that engagement with evaluation output can be understood in the form of a continuum (from relatively low engagement and use to relatively high engagement and use). As presented in <xref ref-type="fig" rid="fig4-1356389012459113">Figure 4</xref>, at the low engagement end of the continuum, we have dissemination or distributive practices (reliance on written texts alone in the form of articles, reports, summaries); in the middle of the continuum are what we could call presentational practices (seminars, presentations, conferences); and at the engaged end of the continuum we have interactional practices (working alongside users to identify situated implications).</p>
<fig id="fig4-1356389012459113" position="float">
<label>Figure 4.</label>
<caption>
<p>Use as engagement.</p>
</caption>
<graphic xlink:href="10.1177_1356389012459113-fig4.tif"/>
</fig>
<p>With these differences in thinking about supporting and incentivizing a thorough approach to evaluation use and usability, it is possible to undertake research of ‘use’ by using a typology of practice-based responses to an evaluation. This mechanism is particularly appropriate when an agency is involved in commissioning a series of evaluations or is overseeing a number of evaluations in devolved environments. The circumstances might involve, for example, a funding criterion that requires a development proposal or specification to have evaluative components built in or the requirement for an external evaluation to take place at various points in the intervention or programme life cycle. What is important in this formulation is that the evaluation output is assumed to have an effect on development or, put another way, is a resource for potential changes in practice. This may happen during an intervention or programme or in subsequent programmes. Either way, donors or funding agencies (like the EU) should have an expectation that evaluations paid for by the European tax payer, should have a use and the use is not rhetorical or tokenistic.</p>
<p><xref ref-type="fig" rid="fig5-1356389012459113">Figure 5</xref> builds on a framework adapted from the work of <xref ref-type="bibr" rid="bibr11-1356389012459113">Hall and Louks (1978)</xref> and used in the context of the national evaluation of a Higher Education Policy and programme intervention (see e.g. <xref ref-type="bibr" rid="bibr17-1356389012459113">Lent and Machell, 2011</xref>) and in an evaluation of a governance programme in Chile (<xref ref-type="bibr" rid="bibr27-1356389012459113">Saunders, 2006</xref>). It is conventionally used as a framework for depicting the responses of a target group to a change initiative or policy intervention. It is adapted here to focus on the stages through which potential users of an evaluation output might go as their use of the output becomes more sophisticated or better developed. This is a reference to the extent to which the knowledge resource implications of an evaluation are understood in increasing complexity. Note that the engagement with the implications of an evaluation report begins with simple awareness of an evaluation (2, 3) and proceeds through examining implications (3, 4), acting on them (5, 6) and finally, the creative adaptation of the implication of findings in onward action (7, 8).</p>
<fig id="fig5-1356389012459113" position="float">
<label>Figure 5.</label>
<caption>
<p>Stages of concern with use.</p>
</caption>
<graphic xlink:href="10.1177_1356389012459113-fig5.tif"/>
</fig>
<p>It is important to note that this approach is aspirational and explores a highly extended use of evaluation as a resource for development. In the cases reviewed in the UK-based evaluation of the CETL (Centres for Excellence in Teaching and Learning) (<xref ref-type="bibr" rid="bibr29-1356389012459113">Saunders et al., 2009</xref>: 86), for example, use was confined to practices between stages 2 and 6. This tool might be discussed with programme managers as part of the preparatory stage of project management to alert them to the range of possibilities expected from the use of evaluation components. Reaching stages beyond 5 will require programme or project managers to have a well-developed understanding of the use and usability practices I have outlined above.</p>
</sec>
</sec>
<sec id="section7-1356389012459113">
<title>Overview</title>
<p>This article has presented an approach to evaluation use from social practice theory that highlights ‘use’ as a practice within clusters of evaluative practice determined by the domain context and policy area. The theoretical point here is that this approach foregrounds use as ‘behaviour’, situated in particular contexts and enables the identification of instruments that might encourage more effective use.</p>
<p>It has undertaken an analysis that builds on existing work by making a distinction between the <italic>use</italic> of an evaluation output and its <italic>usability</italic>. These two dimensions are based on a distinction between the organizational and political context of use on the one hand, with a particular concern with the capacity of stakeholders within an organization to make use of the knowledge resources made available through evaluation along with the systemic characteristics of the environment. On the other hand, usability refers to the dimensions of evaluation design, within the power of evaluators to affect, which are likely to inhibit or enhance the chances of evaluation output being used. The article included instruments and an epistemology that are intended to aid more effective use and usability in design and in a more active and thoughtful engagement strategy with evaluation outputs.</p>
<p>The argument for considering a social practice approach to the problems of use and usability is based on a concept of evaluation impact (the use of an evaluation to produce positive change) that identifies ‘changes in practice’ as central. According to this view, for evaluations to be used positively, they need to be understood as a knowledge resource for new practices (i.e. a change). The article has identified the organizational practices that are most likely to create ‘use’ and the design practices most likely to create usability. The bridging idea of ‘engagement’ itself involves practices. Engagement is used in this article to denote the practices that involve ways in which the characteristics of the evaluation design (usability) intersect with the characteristics of the organizational context of potential use. It is this interplay that is important.</p>
<p>Building on the work of <xref ref-type="bibr" rid="bibr19-1356389012459113">Ojha et al. (2011)</xref>, it is possible to provide an overview of these considerations influencing the extent to which use and usability are enhanced or inhibited. <xref ref-type="fig" rid="fig6-1356389012459113">Figure 6</xref> identifies the evaluation processes (usability factors) and user characteristics and the nature of the organizational context.</p>
<fig id="fig6-1356389012459113" position="float">
<label>Figure 6.</label>
<caption>
<p>Factors affecting evaluation use and usability.</p>
</caption>
<graphic xlink:href="10.1177_1356389012459113-fig6.tif"/>
</fig>
<p>Each of these factors embodies practices that can determine the extent to which evaluations are used. The suggestion implicit in this article is that awareness of the constellations of potential use practices will result in a more effective political role for evaluation.</p>
<p>The European context is one in which a regional policy is focused on using a series of instruments and mechanisms to achieve better integration of member states. The extent to which these mechanisms work effectively toward this aim is critical. Evaluation outputs are an important social capital resource for the attribution of value to these various mechanisms. It is a policy imperative, therefore, that their use becomes discerning, intelligent and better managed.</p>
</sec>
</body>
<back>
<ack>
<p>Due recognition and thanks should go to the Evaluation Unit of DG Regional Policy of the European Union for their support in developing drafts for this paper. All views expressed in the paper remain the sole responsibility of the author.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-1356389012459113">
<label>1.</label>
<p>See <ext-link ext-link-type="uri" xlink:href="http://ec.europa.eu/regional_policy/sources/docoffic/official/reports/cohesion4/index_en.htm">http://ec.europa.eu/regional_policy/sources/docoffic/official/reports/cohesion4/index_en.htm</ext-link> for the 2007 progress report on European Social cohesion policy. The Report provides an update on the progress made towards achieving economic, social and territorial cohesion, and on the manner in which Member States’ and Community’s policies have contributed to it.</p>
</fn>
<fn fn-type="other" id="fn2-1356389012459113">
<label>2.</label>
<p>The European Social Fund (ESF) was set up to improve employment opportunities in the European Union and so help raise standards of living. It aims to help people fulfil their potential by giving them better skills and better job prospects. As one of the EU’s Structural Funds, ESF seeks to reduce differences in prosperity across the EU and enhance economic and promote social cohesion. See <ext-link ext-link-type="uri" xlink:href="http://www.dwp.gov.uk/esf/about-esf/">http://www.dwp.gov.uk/esf/about-esf/</ext-link></p>
</fn>
<fn fn-type="other" id="fn3-1356389012459113">
<label>3.</label>
<p>See <ext-link ext-link-type="uri" xlink:href="http://ec.europa.eu/regional_policy/sources/docgener/evaluation/tech_en.htm">http://ec.europa.eu/regional_policy/sources/docgener/evaluation/tech_en.htm</ext-link> for details of the Evaluation Network meetings hosted by Evaluation Unit of DG Regional Policy.</p>
</fn>
</fn-group>
</notes>
<bio>
<p><bold>Murray Saunders</bold> is Co Director of HERE@lancaster, and Chair of Evaluation in Education and Work. He is past president and Council member of the UK Evaluation Society, Board member and past President of the European Evaluation Society and Vice President of the IOCE (International Organisation for Cooperation in Evaluation).</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bamber</surname><given-names>V</given-names></name>
<name><surname>Trowler</surname><given-names>P</given-names></name>
<name><surname>Saunders</surname><given-names>M</given-names></name>
<name><surname>Knight</surname><given-names>P</given-names></name>
</person-group> (<year>2009</year>) <source>Enhancing Learning, Teaching, Assessment and Curriculum in Higher Education: Theory, cases, practices</source>. <publisher-loc>Buckingham</publisher-loc>: <publisher-name>Open University Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bernstein</surname><given-names>B</given-names></name>
</person-group> (<year>1999</year>) <article-title>Vertical and horizontal discourse: an essay</article-title>. <source>British Journal of Sociology of Education</source> <volume>20</volume>(<issue>2</issue>): <fpage>157</fpage>–<lpage>73</lpage>.</citation>
</ref>
<ref id="bibr3-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chelimsky</surname><given-names>E</given-names></name>
</person-group> (<year>1997</year>) <article-title>Thoughts for a new evaluation society</article-title>. <source>Evaluation</source> <volume>3</volume>: <fpage>97</fpage>–<lpage>109</lpage>.</citation>
</ref>
<ref id="bibr4-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Contandriopoulos</surname><given-names>D</given-names></name>
<name><surname>Brouselle</surname><given-names>A</given-names></name>
</person-group> (<year>2012</year>) <article-title>Evaluation models and evaluation use</article-title>. <source>Evaluation</source> <volume>18</volume>(<issue>1</issue>): <fpage>61</fpage>–<lpage>67</lpage>.</citation>
</ref>
<ref id="bibr5-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cousins</surname><given-names>JB</given-names></name>
<name><surname>Leithwood</surname><given-names>KA</given-names></name>
</person-group> (<year>1986</year>) <article-title>Current empirical research on evaluation utilization</article-title>. <source>Review of Educational Research</source> <volume>56</volume>: <fpage>331</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr6-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Engestrom</surname><given-names>Y</given-names></name>
</person-group> (<year>1999</year>) <article-title>Innovative learning in work team: analysing cycles of knowledge creation in practice</article-title>. In: <person-group person-group-type="editor">
<name><surname>Engestrom</surname><given-names>Y</given-names></name>
<name><surname>Miettinen</surname><given-names>R</given-names></name>
<name><surname>Punamaki</surname><given-names>R</given-names></name>
</person-group> (eds) <source>Perspectives on Activity Theory</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr7-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Engestrom</surname><given-names>Y</given-names></name>
</person-group> (<year>2001</year>) <article-title>Expansive learning at work: toward an activity theoretical reconceptualisation</article-title>. <source>Journal of Education and Work</source> <volume>14</volume>(<issue>1</issue>): <fpage>133</fpage>–<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr8-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Engestrom</surname><given-names>Y</given-names></name>
</person-group> (<year>2004</year>) <article-title>New forms of learning in co-configuration work</article-title>. <source>Journal of Workplace Learning</source> <volume>16</volume>(<issue>1–2</issue>): <fpage>11</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr9-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ferry</surname><given-names>M</given-names></name>
<name><surname>Olejniczak</surname><given-names>K</given-names></name>
</person-group> (<year>2008</year>) <source>The Use of Evaluation in the Management of EU Programmes in Poland</source>. <publisher-loc>Warsaw</publisher-loc>: <publisher-name>Ernst and Young, Warsaw</publisher-name>.</citation>
</ref>
<ref id="bibr10-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fleischer</surname><given-names>DN</given-names></name>
<name><surname>Christie</surname><given-names>CA</given-names></name>
</person-group> (<year>2009</year>) <article-title>Members evaluation use: results from a survey of U.S. American Evaluation Association</article-title>. <source>American Journal of Evaluation</source> <volume>30</volume>(<issue>2</issue>): <fpage>158</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr11-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hall</surname><given-names>GE</given-names></name>
<name><surname>Loucks</surname><given-names>SF</given-names></name>
</person-group> (<year>1978</year>) <article-title>Teacher concerns as a basis for facilitating and personalizing staff development</article-title>. <source>Teachers College Record</source> <volume>80</volume>(<issue>4</issue>): <fpage>36</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr12-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hofstetter</surname><given-names>C</given-names></name>
<name><surname>Alkin</surname><given-names>M</given-names></name>
</person-group> (<year>2003</year>) <article-title>Evaluation use revisited</article-title>. In: <person-group person-group-type="editor">
<name><surname>Kellaghan</surname><given-names>T</given-names></name>
<name><surname>Stufflebeam</surname><given-names>D</given-names></name>
<name><surname>Wingate</surname><given-names>L</given-names></name>
</person-group> (eds) <source>International Handbook of Educational Evaluation</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Kluwer Academic</publisher-name>.</citation>
</ref>
<ref id="bibr13-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Huberman</surname><given-names>M</given-names></name>
</person-group> (<year>1994</year>) <article-title>Research utilization: the state of the art</article-title>. <source>Knowledge and Policy</source> <volume>7</volume>(<issue>4</issue>): <fpage>13</fpage>–<lpage>33</lpage>.</citation>
</ref>
<ref id="bibr14-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kirkhart</surname><given-names>K</given-names></name>
</person-group> (<year>2000</year>) <article-title>Reconceptualising evaluation use: an integrated theory of influence</article-title>. <source>New Directions for Evaluation</source> <volume>88</volume>: <fpage>5</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr15-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Johnson</surname><given-names>LK</given-names></name>
<name><surname>Greenseid</surname><given-names>O</given-names></name>
<name><surname>Toal</surname><given-names>SA</given-names></name>
<name><surname>King</surname><given-names>JA</given-names></name>
<name><surname>Lawrenz</surname><given-names>F</given-names></name>
<name><surname>Volkov</surname><given-names>B</given-names></name>
</person-group> (<year>2009</year>) <article-title>Research on evaluation use: a review of the empirical literature from 1986 to 2005</article-title>. <source>American Journal of Evaluation</source> <volume>30</volume>(<issue>3</issue>): <fpage>377</fpage>–<lpage>410</lpage>.</citation>
</ref>
<ref id="bibr16-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ledermann</surname><given-names>S</given-names></name>
</person-group> (<year>2011</year>) <article-title>Exploring the necessary conditions for evaluation use in program change</article-title>. <source>American Journal of Evaluation</source> <volume>33</volume>(<issue>2</issue>): <fpage>159</fpage>–<lpage>78</lpage>.</citation>
</ref>
<ref id="bibr17-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lent</surname><given-names>N</given-names></name>
<name><surname>Machell</surname><given-names>J</given-names></name>
</person-group> (<year>2011</year>) <article-title>A social practice approach to the evaluation of enhancement strategies in the Quality Enhancement Framework for Scottish universities</article-title>. In: <person-group person-group-type="editor">
<name><surname>Saunders</surname><given-names>M</given-names></name>
<name><surname>Trowler</surname><given-names>P</given-names></name>
<name><surname>Bamber</surname><given-names>V</given-names></name>
</person-group> (eds) <source>Reconceptualising Evaluative Practices in Higher Education: The Practice Turn</source>. <publisher-loc>Maidenhead</publisher-loc>: <publisher-name>McGraw-Hill, Open University Press</publisher-name>, <fpage>105</fpage>–<lpage>13</lpage>.</citation>
</ref>
<ref id="bibr18-1356389012459113">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>McCluskey</surname><given-names>A</given-names></name>
<name><surname>Saunders</surname><given-names>M</given-names></name>
<name><surname>Charlier</surname><given-names>B</given-names></name>
</person-group> (<year>2008</year>) <article-title>D.EVA.05 PALETTE evaluation framework and instruments</article-title> (EU: Part of the PALETTE Project Deliverable).</citation>
</ref>
<ref id="bibr19-1356389012459113">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Ojha</surname><given-names>GP</given-names></name>
<name><surname>Tuladhar</surname><given-names>R</given-names></name>
<name><surname>Khanal</surname><given-names>RC</given-names></name>
</person-group> (<year>2011</year>) <article-title>Is evaluation serving developmental process</article-title>. <conf-name>Paper given at the Sri Lankan Evaluation Society International Conference on Evaluation impact</conf-name>, <conf-loc>Colombo, Sri Lanka</conf-loc>.</citation>
</ref>
<ref id="bibr20-1356389012459113">
<citation citation-type="journal">
<collab>PALETTE (Pedagogically sustained Adaptive LEarning Through the exploitation of Tacit and Explicit knowledge)</collab> (<year>2009</year>) <article-title>D.EVA.06; Final report focusing on describing PALETTE practices and approach, lessons learned about formative evaluation and lessons on how learning within a CoP can be effectively supported (EU)</article-title>.</citation>
</ref>
<ref id="bibr21-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Patton</surname><given-names>MQ</given-names></name>
</person-group> (<year>1997</year>) <source>Utilization-Focused Evaluation: The New Century Text</source>, <edition>3rd edn.</edition> <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr22-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Patton</surname><given-names>MQ</given-names></name>
</person-group> (<year>1998</year>) <article-title>Discovering process use</article-title>. <source>Evaluation</source> <volume>4</volume>(<issue>2</issue>): <fpage>225</fpage>–<lpage>33</lpage>.</citation>
</ref>
<ref id="bibr23-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Patton</surname><given-names>MQ</given-names></name>
</person-group> (<year>2002</year>) <article-title>A Vision of Evaluation that Strengthens Democracy</article-title>. <source>Evaluation</source> <volume>8</volume>(<issue>1</issue>): <fpage>125</fpage>–<lpage>139</lpage>.</citation>
</ref>
<ref id="bibr24-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pawson</surname><given-names>R</given-names></name>
<name><surname>Tilley</surname><given-names>N</given-names></name>
</person-group> (<year>1997</year>) <source>Realistic Evaluation</source>. <publisher-loc>London</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr25-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reckwitz</surname><given-names>A</given-names></name>
</person-group> (<year>2002</year>) <article-title>Toward a theory of social practices: a development in culturalist theorizing</article-title>. <source>European Journal of Social Theory</source> <volume>5</volume>(<issue>2</issue>): <fpage>243</fpage>–<lpage>63</lpage>.</citation>
</ref>
<ref id="bibr26-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saunders</surname><given-names>M</given-names></name>
</person-group> (<year>2000</year>) <article-title>Beginning an evaluation with RUFDATA: theorising a practical approach to evaluation planning</article-title>. <source>Evaluation</source> <volume>6</volume>(<issue>1</issue>): <fpage>7</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr27-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saunders</surname><given-names>M</given-names></name>
</person-group> (<year>2006</year>) <article-title>The presence of evaluation theory and practice in educational and social development: toward an inclusive approach</article-title>. <source>London Review of Education</source> <volume>4</volume>(<issue>2</issue>): <fpage>197</fpage>–<lpage>215</lpage>.</citation>
</ref>
<ref id="bibr28-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Saunders</surname><given-names>M</given-names></name>
<name><surname>Trowler</surname><given-names>P</given-names></name>
<name><surname>Bamber</surname><given-names>V</given-names></name>
</person-group> (<year>2011</year>) <source>Reconceptualising Evaluative Practices in Higher Education: The Practice Turn</source>. <publisher-loc>Maidenhead</publisher-loc>: <publisher-name>McGraw-Hill, Open University Press</publisher-name>.</citation>
</ref>
<ref id="bibr29-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saunders</surname><given-names>M</given-names></name>
<name><surname>Trowler</surname><given-names>P</given-names></name>
<name><surname>Ashwin</surname><given-names>P</given-names></name>
<name><surname>Machell</surname><given-names>J</given-names></name>
<name><surname>Williams</surname><given-names>S</given-names></name>
<name><surname>Allaway</surname><given-names>D</given-names></name>
<name><surname>Fanghanel</surname><given-names>J</given-names></name>
<name><surname>McKee</surname><given-names>A</given-names></name>
</person-group> (<year>2009</year>) <article-title>D2: The national evaluation of the 2005-2010 CETL programme Interim report</article-title>, <month>October</month> <year>2007</year>, <collab>The national CETL evaluation team (HEFCE)</collab>.</citation>
</ref>
<ref id="bibr30-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shreeve</surname><given-names>A</given-names></name>
</person-group> (<year>2011</year>) <article-title>Tell us about it! Evaluating what helps students from diverse backgrounds to succeed at the University of the Arts London</article-title>. <person-group person-group-type="editor">
<name><surname>Saunders</surname><given-names>M</given-names></name>
<name><surname>Trowler</surname><given-names>P</given-names></name>
<name><surname>Bamber</surname><given-names>V</given-names></name>
</person-group> (eds) <source>Reconceptualising Evaluation in Higher Education: The Practice Turn</source>. <publisher-loc>Maidenhead</publisher-loc>: <publisher-name>SRHE and Open University Press</publisher-name>.</citation>
</ref>
<ref id="bibr31-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shulha</surname><given-names>LM</given-names></name>
<name><surname>Cousins</surname><given-names>JB</given-names></name>
</person-group> (<year>1997</year>) <article-title>Evaluation use: theory, research, and practice since 1986</article-title>. <source>American Journal of Evaluation</source> <volume>18</volume>(<issue>1</issue>): <fpage>195</fpage>–<lpage>208</lpage>.</citation>
</ref>
<ref id="bibr32-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stern</surname><given-names>E</given-names></name>
</person-group> (<year>2006</year>) <article-title>Contextual challenges for evaluation practice</article-title>. <person-group person-group-type="editor">
<name><surname>Shaw</surname><given-names>I</given-names></name>
<name><surname>Green</surname><given-names>J</given-names></name>
<name><surname>Mark</surname><given-names>M</given-names></name>
</person-group> (eds) <source>The SAGE Handbook of Evaluation</source>. <publisher-loc>London</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr33-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tuomi-Grohn</surname><given-names>T</given-names></name>
<name><surname>Engestrom</surname><given-names>Y</given-names></name>
</person-group> (<year>2003</year>) <source>Between School and Work: New Perspectives on Transfer and Boundary Crossing</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Pergamon</publisher-name>.</citation>
</ref>
<ref id="bibr34-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vygotsky</surname><given-names>LS</given-names></name>
</person-group> (<year>1999</year>) <source>The Collected Works of L. S. Vygotsky, Vol. 6: Scientific Legacy</source> (trans. by <person-group person-group-type="translator">
<name><surname>Hall</surname><given-names>MJ</given-names></name>
</person-group>). <publisher-loc>New York</publisher-loc>: <publisher-name>Kluwer Academic</publisher-name>.</citation>
</ref>
<ref id="bibr35-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weiss</surname><given-names>C</given-names></name>
</person-group> (<year>1979</year>) <article-title>The many meanings of research utilization</article-title>. <source>Public Administration Review</source> <volume>39</volume>(<issue>5</issue>): <fpage>426</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr36-1356389012459113">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weiss</surname><given-names>C</given-names></name>
</person-group> (<year>1998</year>) <article-title>Have we learned anything new about the use of evaluation?</article-title> <source>American Journal of Evaluation</source> <volume>19</volume>(<issue>1</issue>): <fpage>21</fpage>–<lpage>33</lpage>.</citation>
</ref>
<ref id="bibr37-1356389012459113">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Weiss</surname><given-names>C</given-names></name>
</person-group> (<year>2005</year>) <article-title>Evaluation for decisions: is anybody there? Does anybody care?</article-title> In: <person-group person-group-type="editor">
<name><surname>Stern</surname><given-names>E</given-names></name>
</person-group> (ed.) <source>Evaluation Research Methods</source>, <volume>Vol. 4</volume>. <publisher-loc>London</publisher-loc>: <publisher-name>SAGE</publisher-name>, <fpage>286</fpage>–<lpage>99</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>