<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JBX</journal-id>
<journal-id journal-id-type="hwp">spjbx</journal-id>
<journal-id journal-id-type="nlm-ta">J Biomol Screen</journal-id>
<journal-title>Journal of Biomolecular Screening</journal-title>
<issn pub-type="ppub">1087-0571</issn>
<issn pub-type="epub">1552-454X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1087057111426902</article-id>
<article-id pub-id-type="publisher-id">10.1177_1087057111426902</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Technical Notes</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Automated Analysis and Classification of Infected Macrophages Using Bright-Field Amplitude Contrast Data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Adiga</surname><given-names>Umesh</given-names></name>
<xref ref-type="aff" rid="aff1-1087057111426902">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Taylor</surname><given-names>Debbie</given-names></name>
<xref ref-type="aff" rid="aff2-1087057111426902">2</xref>
<xref ref-type="aff" rid="aff3-1087057111426902">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bell</surname><given-names>Brian</given-names></name>
<xref ref-type="aff" rid="aff1-1087057111426902">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Ponomareva</surname><given-names>Larissa</given-names></name>
<xref ref-type="aff" rid="aff4-1087057111426902">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kanzlemar</surname><given-names>Stephen</given-names></name>
<xref ref-type="aff" rid="aff5-1087057111426902">5</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kramer</surname><given-names>Ryan</given-names></name>
<xref ref-type="aff" rid="aff2-1087057111426902">2</xref>
<xref ref-type="aff" rid="aff3-1087057111426902">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Saldanha</surname><given-names>Roland</given-names></name>
<xref ref-type="aff" rid="aff1-1087057111426902">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Nelson</surname><given-names>Sandra</given-names></name>
<xref ref-type="aff" rid="aff4-1087057111426902">4</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lamkin</surname><given-names>Thomas J.</given-names></name>
<xref ref-type="aff" rid="aff2-1087057111426902">2</xref>
<xref ref-type="aff" rid="aff3-1087057111426902">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-1087057111426902"><label>1</label>UES, Inc., Dayton, OH, USA</aff>
<aff id="aff2-1087057111426902"><label>2</label>Air Force Research Laboratory, 711th HPW/RHPCB, Wright Patterson Air Force Base, OH, USA</aff>
<aff id="aff3-1087057111426902"><label>3</label>Department of Molecular Genetics, Biochemistry and Microbiology, The University of Cincinnati, Cincinnati, OH, USA</aff>
<aff id="aff4-1087057111426902"><label>4</label>Drug Discovery Center, The University of Cincinnati, Cincinnati, OH, USA</aff>
<aff id="aff5-1087057111426902"><label>5</label>Henry M. Jackson Foundation for the Advancement of Military Medicine, Bethesda, MD, USA</aff>
<author-notes>
<corresp id="corresp1-1087057111426902">Dr. Thomas J. Lamkin, 711th HPW/RHPCB, 2510 Fifth Street, Area B, Building 840, Room W220, Wright Patterson AFB, OH 45433-7913 Email: <email>Thomas.lamkin@wpafb.af.mil</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2012</year>
</pub-date>
<volume>17</volume>
<issue>3</issue>
<fpage>401</fpage>
<lpage>408</lpage>
<history>
<date date-type="received">
<day>1</day>
<month>6</month>
<year>2011</year>
</date>
<date date-type="rev-recd">
<day>11</day>
<month>8</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>9</month>
<year>2011</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012 Society for Laboratory Automation and Screening</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Society for Laboratory Automation and Screening</copyright-holder>
</permissions>
<abstract>
<p>This article presents a methodology for acquisition and analysis of bright-field amplitude contrast image data in high-throughput screening (HTS) for the measurement of cell density, cell viability, and classification of individual cells into phenotypic classes. We present a robust image analysis pipeline, where the original data are subjected to image standardization, image enhancement, and segmentation by region growing. This work develops new imaging and analysis techniques for cell analysis in HTS and successfully addresses a particular need for direct measurement of cell density and other features without using dyes.</p>
</abstract>
<kwd-group>
<kwd>imaging technologies</kwd>
<kwd>image analysis</kwd>
<kwd>high-content screening</kwd>
<kwd>microscopy</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1087057111426902" sec-type="intro">
<title>Introduction</title>
<p>Bright-field microscopy is a nontoxic, dye-free technique that uses direct observation of the illuminated objects and is widely used to observe cells under the microscope. A number of studies have been published on the usefulness of bright-field imaging in cell detection and analysis. Although several researchers<sup><xref ref-type="bibr" rid="bibr1-1087057111426902">1</xref>–<xref ref-type="bibr" rid="bibr3-1087057111426902">3</xref></sup> have used phase contrast microscopy to visualize the phase shift introduced by the interaction of light with the objects, these techniques require special objectives and devices. Conversely, bright-field amplitude contrast microscopy does not require special devices, but the image contrast is quite poor due to the transparency of the cells.</p>
<p>Bright-field images in the current study are the result of overall amplitude contrast caused by the cellular objects in the light path. The same image analysis methodology also works well for phase contrast images. Because the cells are generally semi-transparent, and cell organelles are too small to be distinguished clearly with the wavelength of a white light source, the cell images in bright-field data look somewhat like texture patches. Several previous object recognition techniques have attempted to detect and count sf9 cells in bright-field images using active contour models.<sup><xref ref-type="bibr" rid="bibr4-1087057111426902">4</xref>,<xref ref-type="bibr" rid="bibr5-1087057111426902">5</xref></sup> Active contour models do not work satisfactorily in the presence of strong texture and pixel intensity variation, which is a characteristic of the amplitude contrast images. The broader literature review shows that not much work has been done on automatic analysis of bright-field cell data. No work was found on amplitude contrast bright-field images of primary cells in a high-throughput screening (HTS) context. In this article, we propose a set of techniques that will convert the bright-field cytological image data into relatively high-contrast images facilitating the use of region-growing techniques to identify and label individual cells automatically. These were used to calculate hundreds of unbiased features for each segmented cell. We have demonstrated the utility of the method in counting the number of macrophages and classifying them as infected and uninfected based on the phenotypic measurements.</p>
</sec>
<sec id="section2-1087057111426902" sec-type="materials|methods">
<title>Material and Methods</title>
<sec id="section3-1087057111426902">
<title>Monocyte-Derived Macrophage Culture</title>
<p>Negatively selected, apheresed CD14-positive monocytes were thawed quickly at 37 °C and washed in complete media. Complete media consisted of RPMI 1640 (HyClone, Logan, UT) supplemented with 10% fetal bovine sera (FBS; Sigma, St. Louis, MO), 2 mM L-glutamine (Invitrogen, Carlsbad, CA), 40 U/mL GM-CSF (R&amp;D Systems, Minneapolis, MN), and 100 U/mL M-CSF (R&amp;D Systems). Cell density was adjusted to 7 × 10<sup><xref ref-type="bibr" rid="bibr5-1087057111426902">5</xref></sup> cells/mL, and 50 µL of cell suspension (35 000 cells) was added to each well of a 384-well Cell Carrier plate (PerkinElmer, Waltham, MA). Plated cells were incubated at 37 °C in humidified atmosphere with 5% CO<sub>2</sub> for 24 h. Typical monocyte adherence was 25% to 30%. Media were replaced 24 h and 96 h postplating. Cells were differentiated for a total of 7 days.</p>
<p>On day 7, monocyte-derived macrophages (MDMs) were infected with <italic>Francisella tularensis</italic> Δ<italic>blaB</italic>::GFP, a Ft LVS mutant that stably expresses green fluorescent protein (GFP) integrated into the chromosome under the control of a constitutive promoter at a multiplicity of infection (MOI, bacteria/cell) of 100. MDMs and bacteria were incubated at 37 °C/5% CO<sub>2</sub> for 2 h to allow for phagocytosis, after which unincorporated bacteria were removed by washing 3 times with Hank’s buffered saline solution (HBSS). Infected cells were incubated at 37 °C and 5% CO<sub>2</sub> for 50 to 72 h.</p>
<p>After fixation, the cells were washed once in HBSS and stained with final concentrations of 1 µM Cell Trace BODIPY TR methyl ester (Invitrogen), 0.25 µg/mL Cell Mask Deep Red (Invitrogen), and 1 µM final Hoechst (Sigma) in HBSS. MDMs were stained for 30 min at 37 °C in humidified atmosphere with 5% CO<sub>2</sub>. After staining, the cells were washed in HBSS and incubated in 1% bovine serum albumin (BSA) in phosphate-buffered saline (PBS; Thermo Fisher, Waltham, MA) for 20 min at room temperature or 37 °C. Cells were washed and stained with 165 nM final concentration of BODIPY 650/665 phalloidin (Invitrogen) prepared in PBS and 1% BSA for 30 min at room temperature or 37 °C. Cells were washed again, and 50 µL of PBS containing 1 µM Hoechst was added to each well. Each stain used in this protocol provides specific textural information of different cellular components.</p>
</sec>
<sec id="section4-1087057111426902">
<title>Image Acquisition</title>
<p>An Opera Confocal imaging system (<ext-link ext-link-type="uri" xlink:href="http://www.cellularImaging.com/products/Opera">www.cellularImaging.com/products/Opera</ext-link>) was modified with a simple blue LED flashlight hung over the 384-well plate to act as a bright-field light source. First, the bright-field images were collected from all of the preselected image fields in a well, and then the same image fields were reacquired in fluorescent mode by scanning over the well a second time. Although there is a shift in the cell locations between bright-field imaging and fluorescent imaging due to slight movement of the well-plate before fluorescent imaging, one can visually ascertain the common cells between the same field of view in bright-field and fluorescent images. Although the flashlight light source is unreliable and the image quality from such a locally modified system is very poor, it serves the basic HTS purpose of cell density estimation and general feature measurement if another bright-field imaging system is unavailable in the lab. Approximately 5000 infected cells and 5000 control cells were collected for each sample analyzed. Images were collected with a 40× water-immersed objective with a numerical aperture (NA) = 0.9. The pixel intensities were binned by a factor of 2 during the camera-level quantization, resulting in images of 20× magnification with improved signal-to-noise ratio.</p>
</sec>
<sec id="section5-1087057111426902">
<title>Image Analysis</title>
<p>The general image analytics flow diagram to detect and classify the cells is shown in <xref ref-type="fig" rid="fig1-1087057111426902"><bold>Figure 1</bold></xref>. Image analysis methods were divided into image enhancement, cell segmentation, feature measurement, and classification. The main reason for the failure of the traditional segmentation technique on the amplitude contrast data is that at a higher resolution, the data are highly textured due to uneven light absorption by the cellular ultra-structures, and the background is bright and noisy.</p>
<fig id="fig1-1087057111426902" position="float">
<label>Figure 1.</label>
<caption>
<p>Control flow diagram of bright-field cell image analysis process.</p>
</caption>
<graphic xlink:href="10.1177_1087057111426902-fig1.tif"/>
</fig>
</sec>
<sec id="section6-1087057111426902">
<title>Image Enhancement</title>
<p>The first step in the image enhancement is the application of a sequential pipeline of filters such as gradient magnitude image generation, smoothing, histogram equalization, and image recombination, followed by gray-level scaling, in that order. The goal here is to transform the image to a different scale where most of the higher frequency information that is not required for the segmentation is reduced and the background is converted to a darker low-intensity region. The brightness and the smoothness of the cellular region that facilitates better cell segmentation are simultaneously enhanced by the filtering process.</p>
<p>Let <italic>I</italic>(<italic>x, y</italic>) be the input image function that shows image pixel intensity <italic>I</italic> at pixel location (<italic>x, y</italic>). The discrete gradient magnitude map of <italic>I</italic>(<italic>x, y</italic>) is given by</p>
<p><disp-formula id="disp-formula1-1087057111426902">
<label>(1)</label>
<mml:math display="block" id="math1-1087057111426902">
<mml:mrow>
<mml:mi>g</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msqrt>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo>+</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo>+</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:msqrt>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-1087057111426902" xlink:href="10.1177_1087057111426902-eq1.tif"/>
</disp-formula>
</p>
<p>Apply histogram equalization on the gradient magnitude image <italic>g</italic>(<italic>x, y</italic>). Histogram equalization fundamentally tries to redistribute the gray level such that the resulting data have a flatter histogram. In practice, this enhances the brightness of the objects in the image. Let <italic>g<sub>eq</sub></italic> be the histogram-equalized gradient magnitude image and <italic>G</italic>(<italic>x, y</italic>) be the Gaussian-smoothed <italic>g<sub>eq</sub></italic> with σ = 21. The σ value is selected based on experiments with a small set of data. For any new data that are very different in quality from the small set of experimental data, one may have to reset the σ appropriately.</p>
<p>The contrast-enhanced image <italic>I<sub>eq</sub></italic> is given by</p>
<p><disp-formula id="disp-formula2-1087057111426902">
<label>(2)</label>
<mml:math display="block" id="math2-1087057111426902">
<mml:mrow>
<mml:msub>
<mml:mi>I</mml:mi>
<mml:mrow>
<mml:mi>c</mml:mi>
<mml:mi>e</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>⋅</mml:mo>
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mtext>MIN</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>⋅</mml:mo>
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mtext>MAX</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>⋅</mml:mo>
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>−</mml:mo>
<mml:mtext>MIN</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>⋅</mml:mo>
<mml:mi>G</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:mfrac>
<mml:mo>×</mml:mo>
<mml:mtext>MAX</mml:mtext>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-1087057111426902" xlink:href="10.1177_1087057111426902-eq2.tif"/>
</disp-formula>
</p>
<p><xref ref-type="fig" rid="fig2-1087057111426902"><bold>Figure 2A</bold></xref> shows an example image, and <xref ref-type="fig" rid="fig2-1087057111426902"><bold>Figure 2B</bold></xref> shows the contrast-enhanced image by the above method. The contrast-enhanced image is further smoothed to reduce the pixel intensity offshoots, noisy dark patches, and so on within the cell regions.</p>
<fig id="fig2-1087057111426902" position="float">
<label>Figure 2.</label>
<caption>
<p>(A) Original bright-field amplitude contrast image (B) after contrast enhancement and (C) after background subtraction and all stages of preprocessing have been completed. (D) Binary mask by minimum error thresholding. (E) Result of segmentation. (F) Masks are dilated by two pixels to correct for watershed and the boundary of the masks superposed on the original image.</p>
</caption>
<graphic xlink:href="10.1177_1087057111426902-fig2.tif"/>
</fig>
<p>In the second step, background intensity variation is reduced. This is accomplished by reconstructing a background image as a very low-frequency version of the contrast-enhanced image. The background image is then subtracted from the contrast-enhanced image. The negative values are clipped to zero, and the result is rescaled. The resulting image is further diffused using a Gaussian filter (σ = 3) to facilitate binarization. <xref ref-type="fig" rid="fig2-1087057111426902"><bold>Figure 2C</bold></xref> shows a background-suppressed and smoothed foreground image. In general, some form of smoothing is done after every contrast manipulation stage to reduce the undesired effects of contrast manipulation. The preprocessing filters are used to manipulate the images and standardize the image quality and the pixel statistics in such a way that a consistent input is provided to the segmentation process.</p>
</sec>
<sec id="section7-1087057111426902">
<title>Image Segmentation</title>
<p>The images are thresholded to obtain a global mask with a clear separation of background and foreground pixels using a minimum error thresholding method.<sup><xref ref-type="bibr" rid="bibr6-1087057111426902">6</xref></sup> The holes in the binary image are filled by analyzing the negative binary image where objects under a certain size are considered holes in the positive image, and hence those pixels are restored to foreground intensity in the binary image. <xref ref-type="fig" rid="fig2-1087057111426902"><bold>Figure 2D</bold></xref> shows the result of thresholding by a minimum error method followed by hole filling. A modified watershed algorithm described in Adiga et al.<sup><xref ref-type="bibr" rid="bibr7-1087057111426902">7</xref></sup> was implemented to segment the individual cells from the cluster of touching and overlapping cells. <xref ref-type="fig" rid="fig2-1087057111426902"><bold>Figure 2E</bold></xref> shows the result of a watershed-type region growing on the binary image of <xref ref-type="fig" rid="fig2-1087057111426902"><bold>Figure 2D</bold></xref>. The watershed algorithm uses the modified distance map where Euclidean distance of the foreground pixels is calculated as the distance from the object boundary pixels. This generally results in reduced-size object masks. We dilated the object masks by two pixels to correct for this error, and the mask boundary was superposed on the original data, as shown in <xref ref-type="fig" rid="fig2-1087057111426902"><bold>Figure 2F</bold></xref>.</p>
<p>Despite all the preprocessing efforts and modifications to the watershed algorithm to avoid multiple markers per cell, it is not possible to completely remove the cell fragmentation caused by noisy markers. The noisy peaks near the cell boundary caused by the less than perfect convex surface of the cells would still cause fragmentation of the cell. These fragments are quite small and can be detected by size and shape parameters. To detect the fragments, we have calculated the depth of objects as the maximum distance value within the labeled objects. If this distance value is less than a certain preset distance, the object is considered a fragment. If the depth of an average cell is <italic>r</italic> pixel units, then the objects whose depth values are less than <italic>r</italic>/4 pixel units are considered cell fragments. The fragment is merged with a cell that has a longest shared boundary with the fragment.</p>
<p>To demonstrate the utility of bright-field imaging and analysis over fluorescent imaging in the HTS context, we have also acquired data from the same samples in a fluorescent mode using a combination of stains described earlier to highlight nucleus, cell membrane, and many other organelles and membranes in the cell cytoplasm. <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3A</bold></xref> shows the bright-field image with a segmented boundary superposed on it. <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3B</bold></xref> shows the corresponding fluorescent image with the segmented boundary superposed. <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3C</bold></xref> shows the result of segmentation based on the fluorescent nucleus stain (Hoechst). It is observed that some of the cells present in <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3B</bold></xref> are not represented in <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3C</bold></xref> because the corresponding nuclei are not visible. The segmentation method used for fluorescent images is the same method used for bright-field data with appropriate parameter tuning to obtain the best possible result. The bright-field and fluorescent images do not perfectly correspond because the well-plate was moved between acquiring the images in two different imaging modes. A few cells are numbered in <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3A</bold></xref> and <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3B</bold></xref> to provide the match for the visual convenience. The cells without numbers printed on them are not common to both images.</p>
<fig id="fig3-1087057111426902" position="float">
<label>Figure 3.</label>
<caption>
<p>Comparison of segmentation process for a bright-field image and its corresponding fluorescent image. (A) Bright-field image with superposed segmented mask boundary. (B) Fluorescent image with a superposed segmented mask boundary. (C) Segmentation of the same specimen in B using nuclear stain only. The numbered cells in A and B are the corresponding cells in the specimen. (D) An example of the bright-field image superimposed with the corresponding fluorescent image highlighting the possible errors in fluorescentbased image analysis for cellular feature measurement.</p>
</caption>
<graphic xlink:href="10.1177_1087057111426902-fig3.tif"/>
</fig>
</sec>
<sec id="section8-1087057111426902">
<title>Feature Analysis</title>
<p>Bright-field data of macrophages, unlike fluorescent imaging data, do not show any deterministic and/or functional feature(s) that can be used to characterize a cell. It is thus important to quantify all possible features from the cell image and to design a suitable classifier. Cell size, shape features, and their derivatives depend on the scaling and hence are suitably normalized before classification. We have calculated more than 1000 features for each cell image. These features include basic size and shape descriptors, multiscale features,<sup><xref ref-type="bibr" rid="bibr8-1087057111426902">8</xref></sup> invariant moment features,<sup><xref ref-type="bibr" rid="bibr9-1087057111426902">9</xref></sup> statistical texture features,<sup><xref ref-type="bibr" rid="bibr10-1087057111426902">10</xref></sup> Laws texture features (statistical texture features of Laws texture images),<sup><xref ref-type="bibr" rid="bibr11-1087057111426902">11</xref></sup> differential features of the intensity surface (features of local gradient magnitude, local gradient orientation, Laplacian, isophote, flowline, brightness, shape index, etc.),<sup><xref ref-type="bibr" rid="bibr12-1087057111426902">12</xref></sup> frequency domain features, histogram features, distribution features (radial, angular, etc. of intensity distribution, gradient magnitude distribution), local binary pattern image features,<sup><xref ref-type="bibr" rid="bibr13-1087057111426902">13</xref></sup> local contrast pattern image features,<sup><xref ref-type="bibr" rid="bibr14-1087057111426902">14</xref></sup> cell boundary features, edge features, and other heuristic features such as spottiness, χ<sup><xref ref-type="bibr" rid="bibr2-1087057111426902">2</xref></sup> distance between histograms of pixel patches and between concentric circular areas within the cell, gray class distance, heuristic and problem-specific features, and so on. Features were calculated for each cell and normalized to have a value between 0.0 and 1.0.</p>
</sec>
<sec id="section9-1087057111426902">
<title>Classification</title>
<p>Our classification problem is a simple one. We have to reduce the dimensionality of the feature vector in the first step and find a threshold to classify the objects into two classes. The multidimensional scaling (MDS) introduced by Kruskal<sup><xref ref-type="bibr" rid="bibr15-1087057111426902">15</xref></sup> overcomes the inflexibility of principal component analysis (PCA) to use different similarity index poor preservation of interclass (dis)similarity. The purpose of the MDS is to provide a structure for the sample distribution, which attempts to satisfy the dissimilarity matrix constructed from the sample features. We have used Primer software (<ext-link ext-link-type="uri" xlink:href="http://www.primer-e.com">www.primer-e.com</ext-link>), which provides MDS as one of the data reduction techniques followed by hierarchical clustering to classify the data.</p>
</sec>
</sec>
<sec id="section10-1087057111426902" sec-type="results|discussion">
<title>Results and Discussion</title>
<p>To demonstrate the accuracy of the segmentation against manually marked boundaries and against automatic segmentation using the fluorescent data, 100 cells from 50 different images were selected. The accuracy of segmentation can be measured as a percentage of symmetric difference<sup><xref ref-type="bibr" rid="bibr7-1087057111426902">7</xref></sup> in the area of the cell given by automatic segmentation in bright-field data, automatic segmentation in fluorescent data, and the corresponding manual segmentation. If <italic>A</italic><sub>1</sub> is the set of all pixels in the cell segmented by method 1 and <italic>A</italic><sub>2</sub> is the set of pixels in the cell segmented by method 2, then the percentage of symmetric difference is given by</p>
<p><disp-formula id="disp-formula3-1087057111426902">
<mml:math display="block" id="math3-1087057111426902">
<mml:mrow>
<mml:mn>100</mml:mn>
<mml:mo>×</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>{</mml:mo>
<mml:msub>
<mml:mi>A</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo>∪</mml:mo>
<mml:msub>
<mml:mi>A</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo>}</mml:mo>
<mml:mo>\</mml:mo>
<mml:mo>{</mml:mo>
<mml:msub>
<mml:mi>A</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo>∩</mml:mo>
<mml:msub>
<mml:mi>A</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo>}</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>/</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo>#</mml:mo>
<mml:msub>
<mml:mi>A</mml:mi>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-1087057111426902" xlink:href="10.1177_1087057111426902-eq3.tif"/>
</disp-formula>
</p>
<p>where ∪ is the set union operator, ∩ is the set intersection operator, #<italic>A</italic><sub>2</sub> is the number of pixels in set <italic>A</italic><sub>2</sub>, and \ is the set difference operator.</p>
<p>In bright-field images, the average percentage symmetric difference between an automatically marked cell area in the bright-field images and the cell area in manually marked cells is 11%, with a range of 2% to 16%. The average percentage symmetric difference between a cell area in the manually marked cells and the automatically segmented cells from the fluorescent images is 18%, with a range of 7% to 26%. The higher level of accuracy while using bright-field data instead of fluorescent data is also obvious from the composite image shown in <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3D</bold></xref>. This composite image shows that staining-based measurement of global features such as size, shape, and so on can be inaccurate.</p>
<p>To analyze the usefulness of the system, we classified features from the images of uninfected control cells and infected cells using MDS. <xref ref-type="fig" rid="fig4-1087057111426902"><bold>Figure 4A</bold></xref>, <xref ref-type="fig" rid="fig4-1087057111426902"><bold>B</bold></xref> shows the MDS ordination plot of more than 400 cells from infected and control wells. Dissimilarity measures, such as Euclidean distance between the cell sample features and binomial deviance, were used to demonstrate that by using a different or problem-specific dissimilarity measure, one can better classify the cell samples in the phenotypic feature space. Large variance among the infected cells is a result of the presence of many uninfected cells and cells at different infection stages in the infected wells. The variance within the control cells is arguably due to inherent structural differences within the same kind of primary cells and also possible error in automatic segmentation of the cells. <xref ref-type="fig" rid="fig4-1087057111426902"><bold>Figure 4C</bold></xref>,<xref ref-type="fig" rid="fig4-1087057111426902"><bold>D</bold></xref> shows the MDS ordination plot of a smaller set of cells that are classified as live (indicated by 0) or dead (indicated by 1). The gold standard for live–dead classification is based on Sytox orange dye. From the feature-based classification shown in <xref ref-type="fig" rid="fig4-1087057111426902"><bold>Figure 4C</bold></xref>, <xref ref-type="fig" rid="fig4-1087057111426902"><bold>D</bold></xref>, one can argue that several cells that are actually marked live due to lack of Sytox staining of the nuclear DNA are very close to the dead cell class center. This can be due to uneven uptake of Sytox orange dye or because cells that are preapoptotic show major features of the dead cells but are not dead at the time of fixing.</p>
<fig id="fig4-1087057111426902" position="float">
<label>Figure 4.</label>
<caption>
<p>(A) Two-dimensional multidimensional scaling (MDS) ordination plot (MDS component1 along x-axis and MDS component2 along y-axis) of infected and control sample points with (A) Euclidean distance used as a dissimilarity measure (2D stress = 0.09; C, control; I, infected) and (B) binomial deviance used as dissimilarity measure (2D stress = 0.11; C, control; I, infected). (C) Live–dead cell classification with Euclidean distance as a dissimilarity measure (2D stress = 0.15; 1, dead; 0, live). (D) Live–dead cell classification with distance metric as a dissimilarity measure (2D stress = 0.1; 1, dead; 0, live).</p>
</caption>
<graphic xlink:href="10.1177_1087057111426902-fig4.tif"/>
</fig>
<p>In general, we have demonstrated that it is possible to use bright-field amplitude contrast image data to measure cell density and, in certain cases, classify the cells into different phenotypic classes. If image analysis is used for more complex challenges, such as measurement of several cell features, fluorescent imaging requires additional dyes to mark the whole cell region, in addition to the dyes depicting organelles of interest and so on. Because the dyes may not stain the complete cytoplasm as shown in <xref ref-type="fig" rid="fig3-1087057111426902"><bold>Figure 3D</bold></xref>, the morphological and textural features that quantify the cell phenotype can be quite compromised in the fluorescent data. Some of these dyes are toxic in nature, and hence live-cell imaging using these dyes may result in biased data. The process of fixation and staining often adds experimental artifacts to the cell images. By using bright-field data, one can save on the cost of dyes as well as time for multispectral image acquisition and data storage. The reduction in computational cost is straightforward because we are dealing with just one image channel instead of multispectral data. The methodology proposed in this article allows for rapid assay development and actually preserves spectral options for stains if increased dimensionality is desired.</p>
<p>The image analysis routines and feature descriptors are written in .NET framework using C# language, making them widely accessible. The prototype software application will be made available on request after the publication.</p>
</sec>
</body>
<back>
<ack>
<p>We thank Ms. Amy Allen and Mr. Mike Sanders of UES, Inc. for helping with manuscript proofreading and other managerial aspects of the project work.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The authors disclosed receipt of the following financial support for the research and/or authorship of this article: This work was supported by the Transformational Medical Technologies program contract B102387M from the Department of Defense Chemical and Biological Defense program through the Defense Threat Reduction Agency (DTRA).</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1087057111426902">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ambriz-Colin</surname><given-names>F.</given-names></name>
<name><surname>Torres-Cisneros</surname><given-names>M.</given-names></name>
<name><surname>Avina-Cervantes</surname><given-names>J. G.</given-names></name>
<name><surname>Saavedra-Martinez</surname><given-names>J. E.</given-names></name>
<name><surname>Debeir</surname><given-names>O.</given-names></name>
<name><surname>Sanchez-Mondragon</surname><given-names>J. J.</given-names></name>
</person-group> <article-title>Detection of Biological Cells in Phase-Contrast Microscopy Images</article-title>. <source>IEEE Artif. Intell. Conf</source>. <article-title>2006</article-title>, <source>5</source>, <fpage>68</fpage>–<lpage>77</lpage>.</citation>
</ref>
<ref id="bibr2-1087057111426902">
<label>2.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hand</surname><given-names>A. J.</given-names></name>
<name><surname>Sun</surname><given-names>T.</given-names></name>
<name><surname>Barber</surname><given-names>D. C.</given-names></name>
<name><surname>Hose</surname><given-names>D. R.</given-names></name>
<name><surname>MacNeil</surname><given-names>S.</given-names></name>
</person-group> <article-title>Automated Tracking of Migrating Cells in Phase-Contrast Video Microscopy Sequences Using Image Registration</article-title>. <source>J. Microsc</source>. <article-title>2009</article-title>, <source>234</source>, <fpage>62</fpage>–<lpage>79</lpage>.</citation>
</ref>
<ref id="bibr3-1087057111426902">
<label>3.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Debeir</surname><given-names>O.</given-names></name>
<name><surname>Ham</surname><given-names>P. V.</given-names></name>
<name><surname>Kiss</surname><given-names>R.</given-names></name>
<name><surname>Decaestecker</surname><given-names>C.</given-names></name>
</person-group> <article-title>Tracking of Migrating Cells under Phase Contrast Video Microscopy with Combined Mean Shift Processes</article-title>. <source>IEEE Trans. Med. Imaging</source> <year>2005</year>, <volume>24</volume>, <fpage>697</fpage>–<lpage>711</lpage>.</citation>
</ref>
<ref id="bibr4-1087057111426902">
<label>4.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tscherepanow</surname><given-names>M.</given-names></name>
<name><surname>Zollner</surname><given-names>F.</given-names></name>
<name><surname>Hillebrand</surname><given-names>M.</given-names></name>
<name><surname>Kummert</surname><given-names>F.</given-names></name>
</person-group> <article-title>Automatic Segmentation of Unstained Living Cells in Bright Field Microscope Images</article-title>. In <source>MDA 2008</source> <person-group person-group-type="editor">
<name><surname>Perner</surname><given-names>P.</given-names></name>
<name><surname>Salvetti</surname><given-names>O.</given-names></name>
</person-group>, Eds. <publisher-name>Springer-Verlag</publisher-name>: <publisher-loc>Heidelberg, Germany</publisher-loc>, <year>2008</year> pp. <fpage>158</fpage>–<lpage>172</lpage>.</citation>
</ref>
<ref id="bibr5-1087057111426902">
<label>5.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tscherepanow</surname><given-names>M.</given-names></name>
<name><surname>Zollner</surname><given-names>F.</given-names></name>
</person-group> <article-title>Classification of Segmented Regions in Bright Field Microscope Images</article-title>. <source>Proc. Int. Conf.</source> <article-title>Pattern Recognition</article-title> <year>2006</year>, <volume>3</volume>, <fpage>972</fpage>–<lpage>975</lpage>.</citation>
</ref>
<ref id="bibr6-1087057111426902">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kittler</surname><given-names>J.</given-names></name>
<name><surname>Illingworth</surname><given-names>J.</given-names></name>
</person-group> <article-title>Minimum Error Thresholding</article-title>. <source>Pattern Recognition</source> <year>1986</year>, <volume>19</volume>, <fpage>141</fpage>–<lpage>147</lpage>.</citation>
</ref>
<ref id="bibr7-1087057111426902">
<label>7.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Adiga</surname><given-names>U.</given-names></name>
<name><surname>Malladi</surname><given-names>R.</given-names></name>
<name><surname>Gonzalez</surname><given-names>R. F.</given-names></name>
<name><surname>Solorzano</surname><given-names>C.</given-names></name>
</person-group> <article-title>High-Throughput Analysis of Multispectral Images of Breast Cancer Tissue</article-title>. <source>IEEE Trans. Image Processing</source> <year>2006</year>, <volume>15</volume>, <fpage>2259</fpage>–<lpage>2269</lpage>.</citation>
</ref>
<ref id="bibr8-1087057111426902">
<label>8.</label>
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Laptev</surname><given-names>I.</given-names></name>
<name><surname>Lindberg</surname><given-names>T.</given-names></name>
</person-group> In <source>Tracking of Multi-State Hand Models Using Particle Filtering and a Hierarchy of Multi-Scale Features</source>, <conf-name>Proceedings of the Third International Conference on Scale-Space and Morphology in Computer Vision</conf-name>, <conf-loc>Vancouver, Canada</conf-loc>, <conf-date>July 7–8, 2001</conf-date>; <person-group person-group-type="editor">
<name><surname>Kerkhove</surname><given-names>M.</given-names></name>
</person-group>, Ed. <publisher-name>Springer-Verlag</publisher-name>: <publisher-loc>Heidelberg, Germany</publisher-loc>, <year>2001</year> pp. <fpage>63</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr9-1087057111426902">
<label>9.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rizon</surname><given-names>M.</given-names></name>
<name><surname>Yazid</surname><given-names>H.</given-names></name>
<name><surname>Saad</surname><given-names>P.</given-names></name>
<name><surname>Shakaff</surname><given-names>A. Y.</given-names></name>
<name><surname>Saad</surname><given-names>A. R.</given-names></name>
<name><surname>Mamat</surname><given-names>M. R.</given-names></name>
<name><surname>Yaacob</surname><given-names>S.</given-names></name>
<name><surname>Desa</surname><given-names>H.</given-names></name>
<name><surname>Karthigayan</surname><given-names>M.</given-names></name>
</person-group> <article-title>Object Detection Using Geometric Invariant Moment</article-title>. <source>Am. J. Appl. Sci</source>. <year>2006</year>, <volume>2</volume>, <fpage>1876</fpage>–<lpage>1878</lpage>.</citation>
</ref>
<ref id="bibr10-1087057111426902">
<label>10.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Haralick</surname><given-names>R. M.</given-names></name>
</person-group> <article-title>Statistical and Structural Approaches to Texture</article-title>. <source>Proc. IEEE</source> <year>1979</year>, <volume>67</volume>, <fpage>786</fpage>–<lpage>804</lpage>.</citation>
</ref>
<ref id="bibr11-1087057111426902">
<label>11.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Varela</surname><given-names>C.</given-names></name>
<name><surname>Karssemeijer</surname><given-names>N.</given-names></name>
<name><surname>Tahoces</surname><given-names>P. G.</given-names></name>
</person-group> <article-title>Classification of Breast Tumors on Digital Mammograms Using Laws’ Texture Features</article-title>. In <source>Medical Image Computing and Computer-Assisted Intervention—MICCAI 2001</source>; <person-group person-group-type="editor">
<name><surname>Niessen</surname><given-names>W. J.</given-names></name>
<name><surname>Viergever</surname><given-names>M. A.</given-names></name>
</person-group>, Eds. <publisher-name>Springer-Verlag</publisher-name>: <publisher-loc>Heidelberg, Germany</publisher-loc>, <year>2001</year> pp. <fpage>1391</fpage>–<lpage>1392</lpage>.</citation>
</ref>
<ref id="bibr12-1087057111426902">
<label>12.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Ravela</surname><given-names>S.</given-names></name>
</person-group> <article-title>On Multi-Scale Differential Features and Their Representations for Image Retrieval and Recognition</article-title> (<month>January</month> <day>1</day>, <year>2003</year>). <source>Electronic Doctoral Dissertations for UMass Amherst</source>. <comment><ext-link ext-link-type="uri" xlink:href="http://scholarworks.umass.edu/dissertations/AAI3078716">http://scholarworks.umass.edu/dissertations/AAI3078716</ext-link></comment></citation>
</ref>
<ref id="bibr13-1087057111426902">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pietikäinen</surname><given-names>M.</given-names></name>
</person-group> <article-title>Image Analysis with Local Binary Patterns</article-title>. <source>Image Anal. Lect. Notes Comput. Sci</source>. <year>2005</year>, <volume>3540</volume>, <fpage>115</fpage>–<lpage>118</lpage>.</citation>
</ref>
<ref id="bibr14-1087057111426902">
<label>14.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bashar</surname><given-names>M. K.</given-names></name>
<name><surname>Ohnishi</surname><given-names>N.</given-names></name>
</person-group> <article-title>Image Retrieval by Local Contrast Patterns and Color</article-title>. In <source>Advances in Visual Computing</source> <person-group person-group-type="editor">
<name><surname>Bebis</surname><given-names>G.</given-names></name>
<name><surname>Boyle</surname><given-names>R.</given-names></name>
<name><surname>Parvin</surname><given-names>B.</given-names></name>
<name><surname>Koracin</surname><given-names>D.</given-names></name>
<name><surname>Remagnino</surname><given-names>P.</given-names></name>
<name><surname>Nefian</surname><given-names>A.</given-names></name>
<name><surname>Meenakshisundaram</surname><given-names>G.</given-names></name>
<name><surname>Pascucci</surname><given-names>V.</given-names></name>
<name><surname>Zara</surname><given-names>J.</given-names></name>
<name><surname>Molineros</surname><given-names>J.</given-names></name>
<etal/>
</person-group>, Eds. <publisher-name>Springer-Verlag</publisher-name>: <publisher-loc>Heidelberg, Germany</publisher-loc>, <year>2006</year> pp. <fpage>136</fpage>–<lpage>145</lpage>.</citation>
</ref>
<ref id="bibr15-1087057111426902">
<label>15.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kruskal</surname><given-names>J. B.</given-names></name>
</person-group> <article-title>Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis</article-title>. <source>Psychometrika</source> <year>1964</year>, <volume>29</volume>, <fpage>1</fpage>–<lpage>27</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>