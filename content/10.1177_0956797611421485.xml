<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<journal-subtitle>Research, Theory, &amp; Application in Psychology and Related Sciences</journal-subtitle>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797611421485</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797611421485</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Early Conceptual and Linguistic Processes Operate in Independent Channels</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Endress</surname><given-names>Ansgar D.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Potter</surname><given-names>Mary C.</given-names></name>
</contrib>
<aff id="aff1-0956797611421485">Department of Brain &amp; Cognitive Sciences, Massachusetts Institute of Technology</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0956797611421485">Ansgar D. Endress, Massachusetts Institute of Technology, Department of Brain &amp; Cognitive Sciences, Room 46-4127, 43 Vassar St., Cambridge, MA 02139 E-mail: <email>ansgar.endress@m4x.org</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2012</year>
</pub-date>
<volume>23</volume>
<issue>3</issue>
<fpage>235</fpage>
<lpage>245</lpage>
<history>
<date date-type="received">
<day>11</day>
<month>4</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>7</month>
<year>2011</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>Language and concepts are intimately linked, but how do they interact? In the study reported here, we probed the relation between conceptual and linguistic processing at the earliest processing stages. We presented observers with sequences of visual scenes lasting 200 or 250 ms per picture. Results showed that observers understood and remembered the scenes’ abstract gist and, therefore, their conceptual meaning. However, observers remembered the scenes at least as well when they simultaneously performed a linguistic secondary task (i.e., reading and retaining sentences); in contrast, a nonlinguistic secondary task (equated for difficulty with the linguistic task) impaired scene recognition. Further, encoding scenes interfered with performance on the nonlinguistic task and vice versa, but scene processing and performing the linguistic task did not affect each other. At the earliest stages of conceptual processing, the extraction of meaning from visually presented linguistic stimuli and the extraction of conceptual information from the world take place in remarkably independent channels.</p>
</abstract>
<kwd-group>
<kwd>language</kwd>
<kwd>cognitive processes</kwd>
<kwd>attention</kwd>
<kwd>memory</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Language and concepts are intimately linked. For example, conceptual real-world knowledge or even just seeing visual arrays of objects can affect how people initially interpret the grammatical structure of sentences that refer to those objects (e.g., <xref ref-type="bibr" rid="bibr24-0956797611421485">Tanenhaus, Spivey-Knowlton, Eberhard, &amp; Sedivy, 1995</xref>; <xref ref-type="bibr" rid="bibr25-0956797611421485">Trueswell, Tanenhaus, &amp; Garnsey, 1994</xref>; but see <xref ref-type="bibr" rid="bibr4-0956797611421485">Clifton et al., 2003</xref>; <xref ref-type="bibr" rid="bibr21-0956797611421485">Rayner, Carlson, &amp; Frazier, 1983</xref>). Further, the semantic or conceptual meaning of words can affect even low-level perception. For example, when listening to verbs describing upward motion, observers are impaired in detecting actual downward motion, and vice versa (e.g., <xref ref-type="bibr" rid="bibr12-0956797611421485">Meteyard, Bahrami, &amp; Vigliocco, 2007</xref>; for effects of language on visual processes, such as attention and search, see <xref ref-type="bibr" rid="bibr8-0956797611421485">Huettig &amp; Altmann, 2007</xref>, and <xref ref-type="bibr" rid="bibr23-0956797611421485">Spivey, Tyler, Eberhard, &amp; Tanenhaus, 2001</xref>, among many others).</p>
<p>Such results raise the question of whether processes that derive meaning from sensory data, be these data linguistic or nonlinguistic, rely on shared mechanisms that are interdependent at all levels, from the lowest levels of, say, motion perception to the highest level of actually representing meaning. Different research traditions offer a spectrum of positions on this venerable question. Traditions affirming such an interdependence include the Whorfian view that language constrains the concepts and percepts that people can entertain (<xref ref-type="bibr" rid="bibr26-0956797611421485">Whorf, 1956</xref>) and the embodied, simulationist view that understanding any concept involves mentally simulating its referent (e.g., to understand the meaning of “upward,” people mentally simulate what upward motion looks like; e.g., see <xref ref-type="bibr" rid="bibr3-0956797611421485">Barsalou, 1999</xref>). Other researchers hold that conceptual information and linguistic information are processed by completely modular and encapsulated processors (e.g., <xref ref-type="bibr" rid="bibr6-0956797611421485">Fodor, 1983</xref>; <xref ref-type="bibr" rid="bibr20-0956797611421485">Pylyshyn, 1999</xref>). In between these two views, still other accounts suggest that linguistic stimuli are analyzed by dedicated processors, but that these processors can also incorporate nonlinguistic information when it is available (<xref ref-type="bibr" rid="bibr24-0956797611421485">Tanenhaus et al., 1995</xref>; <xref ref-type="bibr" rid="bibr25-0956797611421485">Trueswell et al., 1994</xref>).</p>
<p>Although these results suggest that conceptual and linguistic processing interact in important ways, they do not address the question of whether the underlying processors are shared. In fact, in previous experiments, both the linguistic and nonlinguistic information mapped onto related meanings. Consequently, conceptual information derived from linguistic or nonlinguistic sources provided a prior context, which might have exerted top-down effects on both linguistic and nonlinguistic processes without these processes being shared or identical. As top-down effects have been observed even at the level of the thalamus (<xref ref-type="bibr" rid="bibr13-0956797611421485">O’Connor, Fukui, Pinsk, &amp; Kastner, 2002</xref>), such effects could occur at the earliest processing stages. For example, in the studies conducted by <xref ref-type="bibr" rid="bibr12-0956797611421485">Meteyard et al. (2007)</xref>, participants continuously listened to verbs representing a direction of motion at a rate of one verb per second; visual stimuli moved only during randomly spaced periods of 150 ms. Hence, listening to upward or downward verbs might have placed participants in upward or downward mind-sets, in which thinking about upward and downward motion might have influenced their motion perception. To test the interdependence of linguistic and nonlinguistic processes, researchers therefore need to create a situation in which top-down effects are precluded and in which one kind of process cannot establish a context for the subsequent one.</p>
<p>In the experiments reported here, we probed the relation between language and nonlinguistic concepts at the earliest processing stages. We precluded top-down effects by having both kinds of information processed simultaneously and under time pressure, and by feeding the two kinds of processes information that was largely unrelated. In each trial, participants viewed a sequence of six unrelated scenes presented at a rate of 200 ms or 250 ms per picture (see <xref ref-type="fig" rid="fig1-0956797611421485">Fig. 1</xref>). Observers can encode visual scenes presented at this rate at a rather abstract conceptual level. Not only do they succeed on recognition tests of such scenes (<xref ref-type="bibr" rid="bibr19-0956797611421485">Potter, Staub, Rado, &amp; O’Connor, 2002</xref>), but they also succeed even when tested on descriptions of the scenes; for example, they can decide whether they viewed a scene corresponding with the description “people in street” (<xref ref-type="bibr" rid="bibr18-0956797611421485">Potter, Staub, &amp; O’Connor, 2004</xref>). Such findings indicate that people extract not only low-level visual information, as in traditional studies of visual short-term memory, but also the conceptual gist of the scenes (e.g., <xref ref-type="bibr" rid="bibr9-0956797611421485">Intraub, 1981</xref>; <xref ref-type="bibr" rid="bibr15-0956797611421485">Potter, 1976</xref>).</p>
<fig id="fig1-0956797611421485" position="float">
<label>Fig. 1.</label>
<caption>
<p>Illustration of the display sequence and the scene-recognition test used in all experiments. In each trial, participants saw a rapidly presented sequence of six scenes (left) at a rate of either 200 ms per picture (Experiments 1–4) or 250 ms per picture (Experiments 5a and 5b); a small white box appeared in the center of each scene. In Experiments 1 and 3, the box contained grid lines (shown here). In Experiment 3, participants had to detect changes in the density of the grid lines. In Experiments 2 and 4, each of the six boxes contained a word, and the six words formed a sentence; participants were instructed to remember the sentence. The scene-recognition test consisted of 10 items (5 new, 5 old). Half of the test items used pictures, and half used descriptions of the scenes. Experiments 5a and 5b were similar to Experiments 2 and 3, respectively, except that memory was tested only on scenes and not on scene descriptions.</p>
</caption>
<graphic xlink:href="10.1177_0956797611421485-fig1.tif"/></fig>
<p>After viewing the six rapidly presented scenes, participants completed a yes/no recognition test of scene memory consisting either of 10 scenes (all experiments) or 10 descriptions of scenes (first four experiments only) presented one at a time (<xref ref-type="fig" rid="fig1-0956797611421485">Fig. 1</xref>). In the latter trials, participants had to decide whether they had viewed scenes corresponding with the descriptions. In both types of trial, half of the test items were old, and half were new.</p>
<p>In Experiment 1, we established that observers can extract the gist of scenes presented at a rate of 200 ms per picture when no secondary task is involved; this finding replicates the results of previous experiments investigating conceptual short-term memory (<xref ref-type="bibr" rid="bibr19-0956797611421485">Potter et al., 2002</xref>; <xref ref-type="bibr" rid="bibr18-0956797611421485">Potter et al., 2004</xref>). In Experiment 2, we tested whether a linguistic secondary task interferes with scene memory. Specifically, a written word was presented in the center of each scene; the resulting sequence of six words formed a sentence that was syntactically acceptable but made little sense, such as “miners duly locate truly tired ladies.” Such sentences are likely to trigger linguistic processing, as shown in earlier studies using rapid serial visual presentation (RSVP), in which words were presented one by one at rates of up to 12 words per second (<xref ref-type="bibr" rid="bibr16-0956797611421485">Potter, Kroll, &amp; Harris, 1980</xref>; <xref ref-type="bibr" rid="bibr17-0956797611421485">Potter &amp; Lombardi, 1990</xref>). Following each sequence, participants were tested on their memory either for the scenes or for the sentence.</p>
<p>In Experiment 3, we asked whether a nonlinguistic secondary task interferes with memory for scenes. The center of each scene contained a small box with grid lines. Participants were instructed to press a key when they detected a change in the density of the grid lines. Experiment 4 replicated Experiment 2, but using sentences that were semantically more sensible. Experiments 5a and 5b provided additional controls.</p>
<sec id="section1-0956797611421485" sec-type="methods">
<title>General Method</title>
<sec id="section2-0956797611421485">
<title>Participants</title>
<p>Ninety-six native speakers of English (55 women, 41 men; mean age = 23.3 years) from the Massachusetts Institute of Technology community participated in the study. Participants were distributed evenly among the six experiments (i.e., 16 participants in each experiment). No participant took part in more than one experiment.</p>
</sec>
<sec id="section3-0956797611421485">
<title>Stimuli</title>
<p>Scenes used in the primary task of each experiment were created following the methodology of <xref ref-type="bibr" rid="bibr18-0956797611421485">Potter et al. (2004)</xref>. These scenes consisted of color photographs collected from the World Wide Web and commercial sources. Descriptions corresponding with these pictures were generated by two research assistants. Scenes (and the corresponding descriptions) were randomly organized into sets of 11 pictures (6 items for the study phase and recognition task of each trial, and 5 new items for the recognition task only), with the constraint that the items in a set had no obvious relation with each other. The center of each scene contained a white box with stimuli that varied across the six experiments according to the secondary task.</p>
</sec>
<sec id="section4-0956797611421485">
<title>Procedure</title>
<p>Each experiment comprised 80 trials. Each trial began with an RSVP phase: Following a central fixation cross, a sequence of six scenes appeared on a computer screen (<xref ref-type="fig" rid="fig1-0956797611421485">Fig. 1</xref>). Participants then completed a scene-recognition test by pressing premarked “Yes” and “No” keys on a keyboard. In all but the first experiment, participants also completed a secondary task, the nature of which varied according to the purpose of the experiment. Participants received four practice trials before starting the experiment.</p>
</sec>
<sec id="section5-0956797611421485">
<title>Data analysis</title>
<p>Our primary dependent measure was the percentage of correct responses. The percentage of correct responses in the scene-recognition task was analyzed in a repeated measures analysis of variance (ANOVA) with relative test position (i.e., old test picture’s position among the five old test pictures or new test picture’s position among the five new test pictures) and test modality (scene vs. description) as within-subjects factors. The percentage of correct responses in the secondary task was compared across experiments using ANOVAs. Further, we ran <italic>t</italic> tests to compare performance in both the primary and the secondary tasks against the chance level of 50%.</p>
<p>Scene-recognition performance was compared across experiments using a logistic mixed-effects model (<xref ref-type="bibr" rid="bibr1-0956797611421485">Baayen, Davidson, &amp; Bates, 2008</xref>; <xref ref-type="bibr" rid="bibr14-0956797611421485">Pinheiro &amp; Bates, 2000</xref>) with secondary task (i.e., experiment), test modality (scene vs. description), absolute test position (1–10), and all interactions between these factors entered into the model as predictors. The initial model included intercept adjustments for participants, trial number, and test item as random-effects predictors; slope adjustment for test items relative to the slope of the test-modality predictor was also included as a random-effects predictor. The final model included only those (fixed- and random-effects) predictors that contributed significantly to the likelihood of the model.</p>
<p>All participants were included in the analyses of the primary task of Experiment 1. Across Experiments 2 through 5, a total of 8 participants were excluded from the analyses of the primary task because their performance on the secondary task did not differ from chance according to a one-tailed binomial test; using this criterion guaranteed that the remaining participants paid attention to the secondary task. (The pattern of results was qualitatively unchanged when these participants were included.)</p>
</sec>
</sec>
<sec id="section6-0956797611421485">
<title>Experiment 1</title>
<sec id="section7-0956797611421485">
<title>Method</title>
<p>In Experiment 1, scenes were presented at a rate of 200 ms each. The central box (35 × 35 pixels) contained a regular grid pattern of 2 × 2, 3 × 3, 5 × 5, 11 × 11, or 17 × 17 equally spaced horizontal and vertical lines. The box appeared in synchrony with the scenes. On half of the trials, the density of the grid lines in one box changed relative to the previous box. This change occurred equally often on the second, third, fourth, and fifth scene. After a change, the box changed back to its original grid line density when the next scene appeared, and no further density changes occurred in that trial. Density changes always crossed two density steps (e.g., from 2 to 5 lines, from 3 to 11 lines).</p>
<p>Participants were instructed to look at the center box and also remember the scenes. Following the presentation of the six scenes, participants completed a recognition task consisting of 10 test items. Half of these items had appeared in the RSVP sequence, and half were new. In a random half of the trials, participants were tested on scenes; in the remaining trials, they were tested on verbal descriptions of the scenes. The sixth scene never appeared in the test phase of any trial because it was not masked by a subsequent scene and was therefore easily remembered (<xref ref-type="bibr" rid="bibr19-0956797611421485">Potter et al., 2002</xref>). No picture appeared in more than one trial.</p>
</sec>
<sec id="section8-0956797611421485">
<title>Results and discussion</title>
<p>To analyze an equal number of scene test trials in Experiments 1 through 4, we considered only those trials in Experiment 1 in which the density of the grid lines in the center of the scenes did not change. Our analysis showed that participants successfully remembered scenes (<xref ref-type="fig" rid="fig2-0956797611421485">Fig. 2</xref> and <xref ref-type="table" rid="table1-0956797611421485">Table 1</xref>; see Fig. S1 in the Supplemental Material available online for proportions of hits and false alarms). However, they performed better when tested on scenes than when tested on descriptions, presumably because pictures provide participants with visual and conceptual information in addition to the gist of the scenes (the only information carried by the descriptions). Replicating the findings of earlier work (<xref ref-type="bibr" rid="bibr19-0956797611421485">Potter et al., 2002</xref>; <xref ref-type="bibr" rid="bibr18-0956797611421485">Potter et al., 2004</xref>), our results showed that participants performed worse on items in later test positions than in earlier test positions, probably because of decay or interference. However, both when tested on scenes and tested on descriptions, participants performed significantly above chance on items in all test positions.<sup><xref ref-type="fn" rid="fn1-0956797611421485">1</xref></sup></p>
<fig id="fig2-0956797611421485" position="float">
<label>Fig. 2.</label>
<caption>
<p>Results from Experiments 1 through 4: mean percentage of correct responses on the recognition test as a function of the type of test item (scene or description) and the item’s relative test position. In Experiment 1 (a), participants did not perform a secondary task. In the other experiments, the secondary task consisted of recalling nonsense sentences (Experiment 2; b), detecting changes in the density of grid lines (Experiment 3; c), and recalling sentences that made semantic sense (Experiment 4; d). Error bars represent standard errors of the mean.</p>
</caption>
<graphic xlink:href="10.1177_0956797611421485-fig2.tif"/>
</fig>
<table-wrap id="table1-0956797611421485" position="float">
<label>Table 1.</label>
<caption>
<p>Results of Analyses of Scene-Recognition Performance in All Experiments</p>
</caption>
<graphic alternate-form-of="table1-0956797611421485" xlink:href="10.1177_0956797611421485-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">Effect</th>
</tr>
<tr>
<th align="left">Experiment</th>
<th align="center">Relative test position</th>
<th align="center">Test modality</th>
<th align="center">Relative Test Position × Test Modality</th>
</tr>
</thead>
<tbody>
<tr>
<td>Experiment 1</td>
<td><italic>F</italic>(4, 60) = 11.1, <italic>p</italic> &lt; .0001, η<sub><italic>p</italic></sub><sup>2</sup> = .426</td>
<td><italic>F</italic>(1, 15) = 19.3, <italic>p</italic> &lt; .0005, η<sub><italic>p</italic></sub><sup>2</sup> = .563</td>
<td>n.s.</td>
</tr>
<tr>
<td>Experiment 2</td>
<td><italic>F</italic>(4, 60) = 15.7, <italic>p</italic> &lt; .0001, η<sub><italic>p</italic></sub><sup>2</sup> = .511</td>
<td><italic>F</italic>(1, 15) = 14.8, <italic>p</italic> &lt; .002, η<sub><italic>p</italic></sub><sup>2</sup> = .496</td>
<td>n.s.</td>
</tr>
<tr>
<td>Experiment 3</td>
<td><italic>F</italic>(4, 60) = 6.4, <italic>p</italic> &lt; .0002, η<sub><italic>p</italic></sub><sup>2</sup> = .299</td>
<td><italic>F</italic>(1, 15) = 12.5, <italic>p</italic> &lt; .003, η<sub><italic>p</italic></sub><sup>2</sup> = .454</td>
<td><italic>F</italic>(4, 60) = 2.55, <italic>p</italic> &lt; .048, η<sub><italic>p</italic></sub><sup>2</sup> = .145</td>
</tr>
<tr>
<td>Experiment 4</td>
<td><italic>F</italic>(4, 60) = 5.2, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .257</td>
<td><italic>F</italic>(1, 15) = 20.7, <italic>p</italic> &lt; .0004, η<sub><italic>p</italic></sub><sup>2</sup> = .58</td>
<td><italic>F</italic>(4, 60) = 2.6, <italic>p</italic> &lt; .045, η<sub><italic>p</italic></sub><sup>2</sup> = .148</td>
</tr>
<tr>
<td>Experiment 5a</td>
<td><italic>F</italic>(4, 60) = 22.1, <italic>p</italic> &lt; .0001, η<sub><italic>p</italic></sub><sup>2</sup> = .595</td>
<td align="center">—</td>
<td align="center">—</td>
</tr>
<tr>
<td>Experiment 5b</td>
<td><italic>F</italic>(4, 60) = 4.2, <italic>p</italic> &lt; .005, η<sub><italic>p</italic></sub><sup>2</sup> = .219</td>
<td align="center">—</td>
<td align="center">—</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0956797611421485"><p>Note: In Experiments 1 through 4, participants were tested on scenes and descriptions of scenes. In Experiments 5a and 5b, participants were tested on scenes only.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section9-0956797611421485">
<title>Experiment 2</title>
<sec id="section10-0956797611421485">
<title>Method</title>
<p>Experiment 2 addressed the question of how linguistic and conceptual mechanisms interact at early processing stages. The procedure was the same as in Experiment 1, with two exceptions. First, the central box in the RSVP sequence (3.5 × 0.8 degrees of visual angle) showed a word rather than grid lines. One word was presented with each scene, and the six words formed a sentence that was syntactically acceptable but made little sense, such as “miners duly locate truly tired ladies.”</p>
<p>Second, in half of the trials, participants were tested on their memory for the sentences; immediately after the RSVP sequence, they saw an entire sentence on the screen and had to decide whether or not a word had been changed. In half of these trials, one word had been replaced by a new word that preserved the same grammatical structure as in the original sentence. In the remaining trials, participants were tested on their recognition of scenes and scene descriptions as in Experiment 1.</p>
<p>The sentences were composed according to 10 different grammatical templates by drawing quasirandomly from lists of words in the relevant form classes (nouns, verbs, adverbs, adjectives, and prepositions). Words selected were reasonably frequent (COBUILD frequencies between 100 and 10,000 according to the CELEX corpus; <xref ref-type="bibr" rid="bibr2-0956797611421485">Baayen, Piepenbrock, &amp; Gulikers, 1995</xref>) and had two syllables and four to six letters.</p>
<p>If understanding images involves linguistic resources, we would expect a large decrement in scene-memory performance between Experiments 1 and 2. In contrast, if people grasp the conceptual meaning of scenes by virtue of nonlinguistic mechanisms, we would expect either no decrease in scene-memory performance or a limited decrease because of the attentional demands of performing two tasks simultaneously.</p>
</sec>
<sec id="section11-0956797611421485">
<title>Results and discussion</title>
<p>In the primary (scene-memory) task in Experiment 2, participants performed better when tested on scenes than when tested on descriptions (as in Experiment 1). Further, they performed worse on items in later test positions than in earlier test positions (<xref ref-type="fig" rid="fig2-0956797611421485">Fig. 2</xref> and <xref ref-type="table" rid="table1-0956797611421485">Table 1</xref>; see Fig. S1 for proportions of hits and false alarms). Comparing the results of the primary tasks of Experiments 1 and 2 using a mixed-effects model (<xref ref-type="table" rid="table2-0956797611421485">Table 2</xref>) showed no main effect of experiment, but participants performed numerically better in Experiment 2 despite having to attend to a secondary task (see Tables S1 and S2 in the Supplemental Material for similar comparisons using hits and correct rejections). Across both experiments, recognition was better for scenes than for descriptions and for earlier than for later test positions. In addition, the separation between performance for scenes and performance for descriptions diminished for later test positions.<sup><xref ref-type="fn" rid="fn2-0956797611421485">2</xref></sup> In the secondary task in Experiment 2, participants successfully detected changed words (<xref ref-type="fig" rid="fig3-0956797611421485">Fig. 3</xref> and <xref ref-type="table" rid="table3-0956797611421485">Table 3</xref>; see Fig. S2 in the Supplemental Material for proportions of hits and false alarms).</p>
<fig id="fig3-0956797611421485" position="float">
<label>Fig. 3.</label>
<caption>
<p>Percentage of correct responses in the secondary tasks in Experiments 2 through 4. Dots indicate individual participants' means, and diamonds indicate averages across the sample. The dotted line denotes the chance level of performance across the sample (50%), and the dashed line shows the chance level of performance for individual participants (65%, as determined by a one-tailed binomial test).</p>
</caption>
<graphic xlink:href="10.1177_0956797611421485-fig3.tif"/>
</fig>
<table-wrap id="table2-0956797611421485" position="float">
<label>Table 2.</label>
<caption>
<p>Results of the Logistic Mixed-Effects Model Comparing Scene-Recognition Performance Between Experiments</p>
</caption>
<graphic alternate-form-of="table2-0956797611421485" xlink:href="10.1177_0956797611421485-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="6">Predictor</th>
</tr>
<tr>
<th align="left">Experiments compared</th>
<th align="left">Experiment</th>
<th align="left">Absolute test position</th>
<th align="left">Test modality</th>
<th align="left">Experiment × Absolute Test Position</th>
<th align="left">Experiment × Test Modality</th>
<th align="left">Absolute Test Position × Test Modality</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 vs. 2</td>
<td>n.s.</td>
<td><italic>Z</italic> = −9.2, <italic>p</italic> = .0001</td>
<td><italic>Z</italic> = 6.4, <italic>p</italic> &lt; .0001</td>
<td><italic>Z</italic> = −2.7, <italic>p</italic> = .007</td>
<td>n.s.</td>
<td><italic>Z</italic> = 2.2, <italic>p</italic> = .026</td>
</tr>
<tr>
<td>1 vs. 3</td>
<td><italic>Z</italic> = 2.0, <italic>p</italic> = .044</td>
<td><italic>Z</italic> = −9.6, <italic>p</italic> &lt; .0001</td>
<td><italic>Z</italic> = 7.7, <italic>p</italic> &lt; .0001</td>
<td>n.s.</td>
<td>n.s.</td>
<td><italic>Z</italic> = 3.4, <italic>p</italic> = .0006</td>
</tr>
<tr>
<td>2 vs. 3</td>
<td><italic>Z</italic> = 2.8, <italic>p</italic> = .005</td>
<td><italic>Z</italic> = −10.8, <italic>p</italic> &lt; .0001</td>
<td><italic>Z</italic> = 6.7, <italic>p</italic> &lt; .0001</td>
<td><italic>Z</italic> = −2.6, <italic>p</italic> = .008</td>
<td>n.s.</td>
<td><italic>Z</italic> = 3.7, <italic>p</italic> = .0003</td>
</tr>
<tr>
<td>2 vs. 4</td>
<td>n.s.</td>
<td><italic>Z</italic> = −13.1, <italic>p</italic> &lt; .0001</td>
<td><italic>Z</italic> = 6.0, <italic>p</italic> &lt; .0001</td>
<td><italic>Z</italic> = 2.0, <italic>p</italic> = .048</td>
<td><italic>Z</italic> = 2.4, <italic>p</italic> = .014</td>
<td><italic>Z</italic> = 3.8, <italic>p</italic> = .0001</td>
</tr>
<tr>
<td>3 vs. 4</td>
<td><italic>Z</italic> = 2.4, <italic>p</italic> = .016</td>
<td><italic>Z</italic> = −12.2, <italic>p</italic> &lt; .0001</td>
<td><italic>Z</italic> = 7.1, <italic>p</italic> &lt; .0001</td>
<td>n.s.</td>
<td><italic>Z</italic> = 2.5, <italic>p</italic> = .011</td>
<td><italic>Z</italic> = 4.7, <italic>p</italic> = .0001</td>
</tr>
<tr>
<td>5a vs. 5b</td>
<td><italic>Z</italic> = 2.9, <italic>p</italic> = .003</td>
<td><italic>Z</italic> = −4.0, <italic>p</italic> &lt; .0001</td>
<td align="center">—</td>
<td>n.s.</td>
<td align="center">—</td>
<td align="center">—</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table3-0956797611421485" position="float">
<label>Table 3.</label>
<caption>
<p>Percentage of Correct Responses in the Secondary Tasks of All Experiments and Results of Analyses Comparing Performance Against Chance</p>
</caption>
<graphic alternate-form-of="table3-0956797611421485" xlink:href="10.1177_0956797611421485-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Experiment</th>
<th align="left"><italic>M</italic> (%)</th>
<th align="left"><italic>SD</italic> (%)</th>
<th align="left"><italic>t</italic> test</th>
<th align="left">Cohen’s <italic>d</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Experiment 2</td>
<td>72.0</td>
<td>14.7</td>
<td><italic>t</italic>(19) = 6.7, <italic>p</italic> &lt; .0001</td>
<td>1.5</td>
</tr>
<tr>
<td>Experiment 3</td>
<td>78.2</td>
<td>8.6</td>
<td><italic>t</italic>(16) = 13.5, <italic>p</italic> &lt; .0001</td>
<td>3.3</td>
</tr>
<tr>
<td>Experiment 4</td>
<td>85.3</td>
<td>11.5</td>
<td><italic>t</italic>(16) = 12.6, <italic>p</italic> &lt; .0001</td>
<td>3.1</td>
</tr>
<tr>
<td>Experiment 5a</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> As secondary task</td>
<td>83.3</td>
<td>13.6</td>
<td><italic>t</italic>(17) = 10.4, <italic>p</italic> &lt; .0001</td>
<td>2.4</td>
</tr>
<tr>
<td> As sole task</td>
<td>86.8</td>
<td>14.2</td>
<td><italic>t</italic>(16) = 10.7, <italic>p</italic> &lt; .0001</td>
<td>2.6</td>
</tr>
<tr>
<td>Experiment 5b</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> As secondary task</td>
<td>86.4</td>
<td>8.8</td>
<td><italic>t</italic>(15) = 16.5, <italic>p</italic> &lt; .0001</td>
<td>4.1</td>
</tr>
<tr>
<td> As sole task</td>
<td>98.5</td>
<td>1.8</td>
<td><italic>t</italic>(14) = 102.0, <italic>p</italic> &lt; .0001</td>
<td>26.0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0956797611421485"><p>Note: In Experiments 2 through 4, the secondary task was always performed as a secondary task. In Experiments 5a and 5b, the secondary tasks were performed both as a secondary task and as the sole task. The <italic>t</italic> tests compared the percentage of correct responses against the chance level of 50%.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Compared with Experiment 1, Experiment 2 revealed no decrease in performance: Participants performed numerically (if not significantly) better than in Experiment 1, even though they had to read sentences in addition to monitoring the scenes. Previous research has shown that, at least after massive training, some types of natural scene processing can occur with very limited attentional involvement (<xref ref-type="bibr" rid="bibr5-0956797611421485">Fei-Fei, VanRullen, Koch, &amp; Perona, 2005</xref>; <xref ref-type="bibr" rid="bibr11-0956797611421485">Li, VanRullen, Koch, &amp; Perona, 2002</xref>; but see <xref ref-type="bibr" rid="bibr27-0956797611421485">Yi, Woodman, Widders, Marois, &amp; Chun, 2004</xref>); regardless, we would expect performance in Experiment 2 to be worse than in Experiment 1 simply because participants had to complete two tasks rather than one. However, if understanding of scenes and understanding of language rely on different sets of processes, participants might complete both tasks without any detrimental effect of one task on the other.</p>
<p>In Experiment 3, we further explored the question of why understanding of scenes was not impaired by the linguistic secondary task of Experiment 2. As in Experiment 2, participants completed a secondary task; in contrast with Experiment 2, however, this task was nonlinguistic. If understanding of scenes is simply unaffected by secondary tasks, we would expect to replicate the results of Experiment 2 and observe no impairment in scene recognition. In contrast, scene-recognition performance might be affected by nonlinguistic secondary tasks that tap into processes required for understanding scenes (e.g., visual processing), even if scene-recognition performance is not affected by linguistic secondary tasks.</p>
</sec>
</sec>
<sec id="section12-0956797611421485">
<title>Experiment 3</title>
<sec id="section13-0956797611421485">
<title>Method</title>
<p>The procedure for Experiment 3 was the same as for Experiment 1, except that participants were instructed to press a key when they detected a change in the density of the grid lines in the center box (the density was changed on half of the trials). Before starting the experiment, participants received four practice trials in which only the center box was presented, without any scenes. Scene-recognition was tested as in Experiment 1.</p>
</sec>
<sec id="section14-0956797611421485">
<title>Results and discussion</title>
<p>To analyze an equal number of scene test trials in Experiments 1 through 4, we considered only those trials in Experiment 3 in which the density of the grid lines in the center of the scenes did not change. In the primary task of Experiment 3, participants performed better when tested on scenes than when tested on descriptions (<xref ref-type="fig" rid="fig2-0956797611421485">Fig. 2</xref> and <xref ref-type="table" rid="table1-0956797611421485">Table 1</xref>; see Fig. S1 for proportions of hits and false alarms); further, they performed worse on items in later test positions than in earlier test positions, mirroring the results of Experiment 1.</p>
<p>Unsurprisingly, given that participants had to perform a secondary task in Experiment 3 but not in Experiment 1, results of the mixed-effects model comparing scene-recognition performance across these experiments showed that participants performed worse in Experiment 3 than in Experiment 1 (<xref ref-type="table" rid="table2-0956797611421485">Table 2</xref>; see Tables S1 and S2 for similar comparisons using hits and correct rejections). These results contrast with the comparison of Experiments 1 and 2, in which participants performed numerically (if not significantly) better in Experiment 2 although they had to complete a secondary task. We surmise that the crucial difference between Experiments 2 and 3 is that participants completed a linguistic secondary task in Experiment 2 and a visual-attention secondary task in Experiment 3, and that some mechanisms involved in the visual task, but not language, are needed to understand scenes. Accordingly, participants performed significantly better in Experiment 2 than in Experiment 3 (<xref ref-type="table" rid="table2-0956797611421485">Table 2</xref>).</p>
<p>Regarding the secondary task, our analysis showed that participants successfully detected density changes in the grid lines (<xref ref-type="fig" rid="fig3-0956797611421485">Fig. 3</xref> and <xref ref-type="table" rid="table3-0956797611421485">Table 3</xref>; see Fig. S2 for proportions of hits and false alarms); performance did not differ from that on the secondary task in Experiment 2, <italic>F</italic>(1, 35) = 2.3, <italic>p</italic> = .137, η<sup>2</sup> = .062, although performance on the sentence task in Experiment 2 was numerically worse.</p>
</sec></sec>
<sec id="section15-0956797611421485">
<title>Experiment 4</title>
<p>Although the difficulty of the secondary tasks in Experiments 2 and 3 was matched in terms of task performance (at least when each secondary task was presented with the same primary task of remembering scenes), participants were significantly better at recognizing scenes in Experiment 2 than in Experiment 3. This finding suggests that language processing is largely independent of scene comprehension. It is possible, however, that participants did not fully process the nonsense sentences used in Experiment 2.</p>
<sec id="section16-0956797611421485">
<title>Method</title>
<p>In Experiment 4, we controlled for the possibility that participants did not fully process nonsense sentences by replicating Experiment 2 but using simple, semantically coherent six-word sentences (e.g., “Carol rants about the lousy food”). The only constraints imposed on the words were that they were reasonably frequent and had at most 8 letters. These more interpretable sentences would be more likely to trigger normal sentence processing than the less meaningful sentences in Experiment 2.</p>
</sec>
<sec id="section17-0956797611421485">
<title>Results and discussion</title>
<p>In the primary task of Experiment 4, participants performed better when tested on scenes than when tested on descriptions; further, they performed worse on items in later test positions than in earlier test positions (<xref ref-type="fig" rid="fig2-0956797611421485">Fig. 2</xref> and <xref ref-type="table" rid="table1-0956797611421485">Table 1</xref>; see Fig. S1 for proportions of hits and false alarms). In the secondary task of Experiment 4, participants successfully detected changed words (<xref ref-type="fig" rid="fig3-0956797611421485">Fig. 3</xref> and <xref ref-type="table" rid="table3-0956797611421485">Table 3</xref>; see Fig. S2 for proportions of hits and false alarms). Secondary task performance was better than in Experiment 2, <italic>F</italic>(1, 35) = 9.1, <italic>p</italic> = .005, η<sup>2</sup> = .207, and in Experiment 3, <italic>F</italic>(1, 32) = 4.2, <italic>p</italic> = .049, η<sup>2</sup> = .116. Scene-recognition performance was significantly better than in Experiment 3, but not compared with performance in Experiment 2 (<xref ref-type="table" rid="table2-0956797611421485">Table 2</xref>; see Tables S1 and S2 for similar comparisons using hits and correct rejections).<sup><xref ref-type="fn" rid="fn3-0956797611421485">3</xref></sup> Thus, making the sentences more normal and meaningful did not increase interference with picture processing.</p>
<p>A plausible conclusion from these data is that linguistic tasks involve processes that are independent from those involved in scene understanding. Alternatively, such tasks might prevent counterproductive verbal strategies that participants use to remember scenes. Participants sometimes report trying to find descriptions for scenes, thereby occupying resources that would no longer be available to encode the scenes. A similar observation has been made in experiments in which participants had to keep faces or colors in long-term memory; when instructed to verbally describe the face or the color during a retention period of several minutes, their recognition performance was substantially more impaired than in various control tasks that did not involve verbalization of the stimuli (<xref ref-type="bibr" rid="bibr22-0956797611421485">Schooler &amp; Engstler-Schooler, 1990</xref>). Similarly, a secondary language task of the sort used here in Experiments 2 and 4 might inhibit counterproductive verbal strategies, whereas a nonlinguistic secondary task would show the usual negative effect of having a secondary task.</p>
<p>Preventing verbal strategies might, therefore, offset the attentional costs associated with performing a secondary task. However, Experiments 1 through 4 might have encouraged such strategies because participants were tested not only on actual scenes but also on descriptions of those scenes. It is possible that a linguistic secondary task might reveal interference with scene understanding if participants’ performance on scenes had been tested only with actual scenes rather than with descriptions. We tested this possibility in Experiments 5a and 5b.</p>
</sec>
</sec>
<sec id="section18-0956797611421485">
<title>Experiment 5</title>
<sec id="section19-0956797611421485">
<title>Method</title>
<p>Experiments 5a and 5b replicated Experiments 2 and 3, respectively, with three crucial changes. First and most important, participants were never tested on descriptions of scenes, but only on actual scenes. As a result, the test items should no longer encourage a verbal memory strategy for the scenes.</p>
<p>Second, we made the two secondary tasks more similar. In Experiment 5a, participants read the same sentences as participants in Experiment 2 did; these sentences were again presented word by word in the center of the scenes. In a random half of the trials, participants were then tested on single words; that is, they had to decide whether or not a test word had occurred in the sentence (on half of the trials it had). On the other half of the trials, they were tested on scene memory. As in Experiment 3, participants in Experiment 5b had to detect changes of the density of grid lines in a small square; however, rather than pressing a key as soon as they saw a density change, on half of the trials, they had to report after the trial whether or not a density change had occurred. In the remaining trials, they were tested on their memory for the scenes.</p>
<p>Third, after they completed the experiments, participants were tested on the secondary task in isolation, with no primary task and no scenes shown. In addition, we increased the presentation duration to 250 ms per picture in Experiments 5a and 5b.<sup><xref ref-type="fn" rid="fn4-0956797611421485">4</xref></sup></p>
</sec>
<sec id="section20-0956797611421485">
<title>Results and discussion</title>
<p>Scene-recognition performance in Experiments 5a and 5b is shown in <xref ref-type="fig" rid="fig4-0956797611421485">Figure 4a</xref> (results of the analyses of scene-recognition performance are shown in <xref ref-type="table" rid="table1-0956797611421485">Table 1</xref>, and proportions of hits and false alarms are shown in Fig. S3a). Replicating the results of Experiments 2 and 3, the data showed that participants performed better in Experiment 5a than in Experiment 5b (<xref ref-type="table" rid="table2-0956797611421485">Table 2</xref>; see Tables S1 and S2 for similar comparisons using hits and false alarms), which suggests that the nonlinguistic secondary task of Experiment 5b interfered more with scene understanding than did the linguistic secondary task of Experiment 5a.</p>
<fig id="fig4-0956797611421485" position="float">
<label>Fig. 4.</label>
<caption>
<p>Results of Experiments 5a and 5b: mean percentage of correct responses on (a) the recognition test and (b) the secondary task. Results for the recognition test are presented as a function of relative test position and the nature of the secondary task (recall of nonsense sentences in Experiment 5a and change detection in Experiment 5b). Error bars represent standard errors of the mean. Results for the secondary task are presented as a function of experiment and whether or not a primary task was performed. Dots indicate individual participants’ means, and diamonds indicate averages across the sample. The dotted line denotes the chance level of performance across the sample (50%), and the dashed line shows the chance level of performance for individual participants (65%, as determined by a one-tailed binomial test).</p>
</caption>
<graphic xlink:href="10.1177_0956797611421485-fig4.tif"/>
</fig>
<p>Performance on the secondary task was analyzed in two ways: first, when the secondary task was performed in conjunction with the primary task and, second, when the secondary task was presented alone. When the secondary task was performed with the primary task in Experiment 5a, participants successfully discriminated words that had occurred in the sentence from words that had not (<xref ref-type="table" rid="table3-0956797611421485">Table 3</xref> and <xref ref-type="fig" rid="fig4-0956797611421485">Fig. 4b</xref>; see Fig. S3b in the Supplemental Material for proportions of hits and false alarms). When the secondary task was performed with the primary task in Experiment 5b, participants successfully detected density changes in the grid lines (<xref ref-type="fig" rid="fig4-0956797611421485">Fig. 4b</xref> and <xref ref-type="table" rid="table3-0956797611421485">Table 3</xref>; see Fig. S3b for proportions of hits and false alarms); this performance did not differ from that on the secondary task in Experiment 5a, <italic>F</italic>(1, 32) = 0.6, <italic>p</italic> = .447, η<sup>2</sup> = .018, although performance in the sentence task in Experiment 5a was numerically worse. Hence, although the two secondary tasks were matched for difficulty when used as secondary tasks, the nonlinguistic secondary task interfered more with scene processing than the linguistic secondary task did.</p>
<p>To investigate the possibility that one task might be easier than the other when tested in isolation (i.e., without a primary task), we had participants in Experiments 5a and 5b complete their respective secondary tasks without any interfering primary tasks after they finished the main experiment. Performance on the linguistic secondary task was similar when used as a secondary task and when it was presented in isolation (<xref ref-type="table" rid="table3-0956797611421485">Table 3</xref>). In contrast, performance on the change-detection task was almost perfect in the absence of interfering scenes. An ANOVA (excluding 2 participants, 1 in each experiment, who did not complete the second presentation of the secondary tasks) with task type (nonsense sentences vs. change detection) as a between-subjects predictor and task status (secondary task vs. sole task) as a within-subjects predictor revealed main effects of both task type, <italic>F</italic>(1, 30) = 4.4, <italic>p</italic> = .044, η<sub><italic>p</italic></sub><sup>2</sup> = .129, and task status, <italic>F</italic>(1, 30) = 29.2, <italic>p</italic> &lt; .0001, η<sub><italic>p</italic></sub><sup>2</sup> = .438, as well as an interaction between these factors, <italic>F</italic>(1, 30) = 7.5, <italic>p</italic> = .01, η<sub><italic>p</italic></sub><sup>2</sup> = .112.</p>
<p>Although performance on the linguistic task differed only marginally depending on whether participants had to complete a concomitant primary task, <italic>F</italic>(1, 16) = 3.9, <italic>p</italic> = .065, η<sub><italic>p</italic></sub><sup>2</sup> = .197, performance on the nonlinguistic secondary task was markedly improved when the task was presented in isolation, <italic>F</italic>(1, 16) = 36.0, <italic>p</italic> &lt; .0001, η<sub><italic>p</italic></sub><sup>2</sup> = .720. In other words, not only did the nonlinguistic secondary task interfere more with scene understanding than did the linguistic secondary task, but scene understanding also interfered more with the nonlinguistic secondary task than with the linguistic secondary task. Remarkably, performance on the linguistic secondary task was almost unaffected by concomitant scene understanding; likewise, the comparison of Experiments 1 and 2 revealed that scene understanding was unaffected by the presence of a linguistic secondary task. Hence, linguistic stimuli seem to be processed by mechanisms that are separate from those involved in scene understanding, even if both the scenes and the linguistic stimuli are presented visually.</p>
</sec>
</sec>
<sec id="section21-0956797611421485" sec-type="discussion">
<title>General Discussion</title>
<p>In the experiments presented here, we probed the relation between language and conceptual processing at the earliest stages using stimuli presented for durations of a single fixation. Previous research using similar presentation rates has revealed that observers extract and retain abstract conceptual information on top of visual information (<xref ref-type="bibr" rid="bibr18-0956797611421485">Potter et al., 2004</xref>). Using this paradigm, we showed that participants’ grasp of the conceptual meaning of scenes is almost unaffected by a linguistic secondary task and vice versa, but that scene understanding and a nonlinguistic secondary task mutually interfere.</p>
<p>There are three reasons why these results are not simply due to greater use of visual-processing resources or memory resources in the nonlinguistic secondary tasks than in the linguistic secondary tasks. First, stimuli for the linguistic secondary task occluded at least as much surface area in the scenes as did stimuli for the nonlinguistic secondary task, and both needed to be processed visually. Second, the nonlinguistic secondary task in Experiment 3 did not require any visual memory at all, as participants had to react to a stimulus change immediately. Third, the processing advantage for scene recognition with a linguistic secondary task was maintained even when participants were tested on descriptions, which (presumably) rely more on conceptual information than on visual information. Taken together, our results thus suggest that the nonlinguistic secondary task interferes with processes that are crucial to scene understanding, but the linguistic secondary task appears to be essentially irrelevant to scene understanding.</p>
<p>Further, previous results suggest that linguistic and nonlinguistic processes can remain independent not only initially, but also even in complex behaviors, such as communication. For example, in languages such as English, the canonical word order is subject-verb-object (e.g., Mary sees John), but languages such as Turkish have the word order subject-object-verb (e.g., Mary John sees). However, when people have to gesture to communicate events (rather than to encode them verbally), they use the subject-object-verb order—irrespective of the word order of their native language (<xref ref-type="bibr" rid="bibr7-0956797611421485">Goldin-Meadow, So, Ozyürek, &amp; Mylander, 2008</xref>; <xref ref-type="bibr" rid="bibr10-0956797611421485">Langus &amp; Nespor, 2010</xref>). This finding suggests that the linguistic use of concepts and roles, such as agents and patients, does not affect how other processes use the same concepts and roles.</p>
<p>Despite the intimate link between language and conceptual structure, initial linguistic and nonlinguistic processes that derive meaning from sensory data thus appear to operate in remarkably independent channels. Interactions between linguistic and nonlinguistic conceptual processes might reflect top-down effects that occur only if one set of processes establishes a prior context that is relevant to the other set of processes. For example, when listening to verbs describing upward motion, observers might be impaired in detecting actual downward motion (e.g., <xref ref-type="bibr" rid="bibr12-0956797611421485">Meteyard et al., 2007</xref>) because listening to such verbs might activate conceptual representations that exert top-down influences on motion perception. This could be true even though the processes used to understand verbs and to perceive motion are distinct and independent. In the absence of such top-down effects, linguistic stimuli appear to be analyzed by dedicated linguistic processors at the earliest processing stages. This finding provides further evidence for the remarkable modularity of processes that analyze different aspects of people’s environments.</p>
</sec>
</body>
<back>
<ack>
<p>We thank Á. Kovács for comments and Emily McCourt for assistance.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="supported-by">
<p>This research was supported by <grant-sponsor>National Institutes of Health</grant-sponsor> Grant <grant-num>MH47432</grant-num>.</p>
</fn>
<fn fn-type="supplementary-material">
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0956797611421485">
<label>1.</label>
<p>Most of the key findings in this experiment held across the other five experiments in this study. Participants performed better when tested on scenes than when tested on descriptions and performed worse on items in later test positions than on items in earlier test positions. However, both when tested on scenes and when tested on descriptions, participants performed significantly above chance on items in all test positions. As <xref ref-type="fig" rid="fig2-0956797611421485">Figure 2</xref>, <xref ref-type="fig" rid="fig4-0956797611421485">Figure 4a</xref>, and <xref ref-type="table" rid="table1-0956797611421485">Table 1</xref> show, analogous effects of test modality and test position were observed in each of the other experiments (these results are not reported in the text).</p>
</fn>
<fn fn-type="other" id="fn2-0956797611421485">
<label>2.</label>
<p>As <xref ref-type="table" rid="table2-0956797611421485">Table 2</xref> shows, analogous effects of test modality and test position were observed in each of the other between-experiments comparisons. The effects of test position and modality were more pronounced in some experiments than in others, but these differences are not relevant to the main questions addressed in this article.</p>
</fn>
<fn fn-type="other" id="fn3-0956797611421485">
<label>3.</label>
<p>The effect of test modality was somewhat more pronounced in Experiment 4 than in Experiment 2, and the effect of test position somewhat less pronounced. Although multiple factors might have contributed to these differences (e.g., the difficulty of the secondary tasks, the use of everyday meanings in Experiment 4), Experiments 2 and 4 both replicated our crucial result that scene recognition is better with a linguistic secondary task than with a nonlinguistic secondary task.</p>
</fn>
<fn fn-type="other" id="fn4-0956797611421485">
<label>4.</label>
<p>In Experiment 5a, we used the nonsense sentences from Experiment 2 rather than the more sensible sentences from Experiment 4 because this allowed us to equate task difficulty between the linguistic and the nonlinguistic secondary task. We increased the presentation duration to 250 ms to vary the stimulus parameters. </p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Baayen</surname><given-names>R.</given-names></name>
<name><surname>Davidson</surname><given-names>D.</given-names></name>
<name><surname>Bates</surname><given-names>D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Mixed-effects modeling with crossed random effects for subjects and items</article-title>. <source>Journal of Memory and Language</source>, <volume>59</volume>, <fpage>390</fpage>–<lpage>412</lpage>.</citation>
</ref>
<ref id="bibr2-0956797611421485">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Baayen</surname><given-names>R. H.</given-names></name>
<name><surname>Piepenbrock</surname><given-names>R.</given-names></name>
<name><surname>Gulikers</surname><given-names>L.</given-names></name>
</person-group> (<year>1995</year>). <source>The CELEX lexical database</source> <comment>[CD-ROM]</comment>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>Linguistic Data Consortium</publisher-name>.</citation>
</ref>
<ref id="bibr3-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Barsalou</surname><given-names>L. W.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Perceptual symbol systems</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>22</volume>, <fpage>577</fpage>–<lpage>660</lpage>.</citation>
</ref>
<ref id="bibr4-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Clifton</surname><given-names>C.</given-names></name>
<name><surname>Traxler</surname><given-names>M. J.</given-names></name>
<name><surname>Mohamed</surname><given-names>M. T.</given-names></name>
<name><surname>Williams</surname><given-names>R. S.</given-names></name>
<name><surname>Morris</surname><given-names>R. K.</given-names></name>
<name><surname>Rayner</surname><given-names>K.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The use of thematic role information in parsing: Syntactic processing autonomy revisited</article-title>. <source>Journal of Memory and Language</source>, <volume>49</volume>, <fpage>317</fpage>–<lpage>334</lpage>.</citation>
</ref>
<ref id="bibr5-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fei-Fei</surname><given-names>L.</given-names></name>
<name><surname>VanRullen</surname><given-names>R.</given-names></name>
<name><surname>Koch</surname><given-names>C.</given-names></name>
<name><surname>Perona</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Why does natural scene categorization require little attention? Exploring attentional requirements for natural and synthetic stimuli</article-title>. <source>Visual Cognition</source>, <volume>12</volume>, <fpage>893</fpage>–<lpage>924</lpage>.</citation>
</ref>
<ref id="bibr6-0956797611421485">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fodor</surname><given-names>J. A.</given-names></name>
</person-group> (<year>1983</year>). <source>The modularity of mind</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr7-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldin-Meadow</surname><given-names>S.</given-names></name>
<name><surname>So</surname><given-names>W. C.</given-names></name>
<name><surname>Ozyürek</surname><given-names>A.</given-names></name>
<name><surname>Mylander</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The natural order of events: How speakers of different languages represent events nonverbally</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>105</volume>, <fpage>9163</fpage>–<lpage>9168</lpage>.</citation>
</ref>
<ref id="bibr8-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Huettig</surname><given-names>F.</given-names></name>
<name><surname>Altmann</surname><given-names>G. T. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Visual-shape competition during language-mediated attention is based on lexical input and not modulated by contextual appropriateness</article-title>. <source>Visual Cognition</source>, <volume>15</volume>, <fpage>985</fpage>–<lpage>1018</lpage>.</citation>
</ref>
<ref id="bibr9-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Intraub</surname><given-names>H.</given-names></name>
</person-group> (<year>1981</year>). <article-title>Rapid conceptual identification of sequentially presented pictures</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>7</volume>, <fpage>604</fpage>–<lpage>610</lpage>.</citation>
</ref>
<ref id="bibr10-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Langus</surname><given-names>A.</given-names></name>
<name><surname>Nespor</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Cognitive systems struggling for word order</article-title>. <source>Cognitive Psychology</source>, <volume>60</volume>, <fpage>291</fpage>–<lpage>318</lpage>.</citation>
</ref>
<ref id="bibr11-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>F. F.</given-names></name>
<name><surname>VanRullen</surname><given-names>R.</given-names></name>
<name><surname>Koch</surname><given-names>C.</given-names></name>
<name><surname>Perona</surname><given-names>P.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Rapid natural scene categorization in the near absence of attention</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>99</volume>, <fpage>9596</fpage>–<lpage>9601</lpage>.</citation>
</ref>
<ref id="bibr12-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Meteyard</surname><given-names>L.</given-names></name>
<name><surname>Bahrami</surname><given-names>B.</given-names></name>
<name><surname>Vigliocco</surname><given-names>G.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Motion detection and motion verbs: Language affects low-level visual perception</article-title>. <source>Psychological Science</source>, <volume>18</volume>, <fpage>1007</fpage>–<lpage>1013</lpage>.</citation>
</ref>
<ref id="bibr13-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>O’Connor</surname><given-names>D. H.</given-names></name>
<name><surname>Fukui</surname><given-names>M. M.</given-names></name>
<name><surname>Pinsk</surname><given-names>M. A.</given-names></name>
<name><surname>Kastner</surname><given-names>S.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Attention modulates responses in the human lateral geniculate nucleus</article-title>. <source>Nature Neuroscience</source>, <volume>5</volume>, <fpage>1203</fpage>–<lpage>1209</lpage>.</citation>
</ref>
<ref id="bibr14-0956797611421485">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pinheiro</surname><given-names>J.</given-names></name>
<name><surname>Bates</surname><given-names>D.</given-names></name>
</person-group> (<year>2000</year>). <source>Mixed-effects models in S and S-PLUS</source>. <publisher-loc>Berlin, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr15-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Potter</surname><given-names>M. C.</given-names></name>
</person-group> (<year>1976</year>). <article-title>Short-term conceptual memory for pictures</article-title>. <source>Journal of Experimental Psychology: Human Learning and Memory</source>, <volume>2</volume>, <fpage>509</fpage>–<lpage>522</lpage>.</citation>
</ref>
<ref id="bibr16-0956797611421485">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Potter</surname><given-names>M. C.</given-names></name>
<name><surname>Kroll</surname><given-names>J.</given-names></name>
<name><surname>Harris</surname><given-names>C.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Comprehension and memory in rapid sequential reading</article-title>. In <person-group person-group-type="editor">
<name><surname>Nickerson</surname><given-names>R.</given-names></name>
</person-group> (Ed.), <source>Attention and performance VIII</source> (pp. <fpage>395</fpage>–<lpage>418</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr17-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Potter</surname><given-names>M. C.</given-names></name>
<name><surname>Lombardi</surname><given-names>L.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Regeneration in the short-term recall of sentences</article-title>. <source>Journal of Memory and Language</source>, <volume>29</volume>, <fpage>633</fpage>–<lpage>654</lpage>.</citation>
</ref>
<ref id="bibr18-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Potter</surname><given-names>M. C.</given-names></name>
<name><surname>Staub</surname><given-names>A.</given-names></name>
<name><surname>O’Connor</surname><given-names>D. H.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Pictorial and conceptual representation of glimpsed pictures</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>30</volume>, <fpage>478</fpage>–<lpage>489</lpage>.</citation>
</ref>
<ref id="bibr19-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Potter</surname><given-names>M. C.</given-names></name>
<name><surname>Staub</surname><given-names>A.</given-names></name>
<name><surname>Rado</surname><given-names>J.</given-names></name>
<name><surname>O’Connor</surname><given-names>D. H.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Recognition memory for briefly presented pictures: The time course of rapid forgetting</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>28</volume>, <fpage>1163</fpage>–<lpage>1175</lpage>.</citation>
</ref>
<ref id="bibr20-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pylyshyn</surname><given-names>Z.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Is vision continuous with cognition? The case for cognitive impenetrability of visual perception</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>22</volume>, <fpage>341</fpage>–<lpage>365</lpage>.</citation>
</ref>
<ref id="bibr21-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rayner</surname><given-names>K.</given-names></name>
<name><surname>Carlson</surname><given-names>M.</given-names></name>
<name><surname>Frazier</surname><given-names>L.</given-names></name>
</person-group> (<year>1983</year>). <article-title>The interaction of syntax and semantics during sentence processing: Eye movements in the analysis of semantically biased sentences</article-title>. <source>Journal of Verbal Learning and Verbal Behavior</source>, <volume>22</volume>, <fpage>358</fpage>–<lpage>374</lpage>.</citation>
</ref>
<ref id="bibr22-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schooler</surname><given-names>J. W.</given-names></name>
<name><surname>Engstler-Schooler</surname><given-names>T. Y.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Verbal overshadowing of visual memories: Some things are better left unsaid</article-title>. <source>Cognitive Psychology</source>, <volume>22</volume>, <fpage>36</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr23-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spivey</surname><given-names>M.</given-names></name>
<name><surname>Tyler</surname><given-names>M.</given-names></name>
<name><surname>Eberhard</surname><given-names>K. M.</given-names></name>
<name><surname>Tanenhaus</surname><given-names>M. K.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Linguistically mediated visual search</article-title>. <source>Psychological Science</source>, <volume>12</volume>, <fpage>282</fpage>–<lpage>286</lpage>.</citation>
</ref>
<ref id="bibr24-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tanenhaus</surname><given-names>M. K.</given-names></name>
<name><surname>Spivey-Knowlton</surname><given-names>M. J.</given-names></name>
<name><surname>Eberhard</surname><given-names>K. M.</given-names></name>
<name><surname>Sedivy</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Integration of visual and linguistic information in spoken language comprehension</article-title>. <source>Science</source>, <volume>268</volume>, <fpage>1632</fpage>–<lpage>1634</lpage>.</citation>
</ref>
<ref id="bibr25-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Trueswell</surname><given-names>J. C.</given-names></name>
<name><surname>Tanenhaus</surname><given-names>M. K.</given-names></name>
<name><surname>Garnsey</surname><given-names>S. M.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Semantic influences on parsing: Use of thematic role information in syntactic ambiguity resolution</article-title>. <source>Journal of Memory and Language</source>, <volume>33</volume>, <fpage>285</fpage>–<lpage>318</lpage>.</citation>
</ref>
<ref id="bibr26-0956797611421485">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Whorf</surname><given-names>B.</given-names></name>
</person-group> (<year>1956</year>). <source>Language, thought, and reality: Selected writings of Benjamin Lee Whorf</source> (<person-group person-group-type="editor">
<name><surname>Carroll</surname><given-names>J. B.</given-names></name>
</person-group>, Ed.). <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr27-0956797611421485">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yi</surname><given-names>D.-J.</given-names></name>
<name><surname>Woodman</surname><given-names>G. F.</given-names></name>
<name><surname>Widders</surname><given-names>D.</given-names></name>
<name><surname>Marois</surname><given-names>R.</given-names></name>
<name><surname>Chun</surname><given-names>M. M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Neural fate of ignored stimuli: Dissociable effects of perceptual and working memory load</article-title>. <source>Nature Neuroscience</source>, <volume>7</volume>, <fpage>992</fpage>–<lpage>996</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>