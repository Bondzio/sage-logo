<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="news">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EVI</journal-id>
<journal-id journal-id-type="hwp">spevi</journal-id>
<journal-title>Evaluation</journal-title>
<issn pub-type="ppub">1356-3890</issn>
<issn pub-type="epub">1461-7153</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1356389012462939</article-id>
<article-id pub-id-type="publisher-id">10.1177_1356389012462939</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>News from the community</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Case studies in development evaluation: Validity, generalization and learning: Highlights of an international workshop: Copenhagen, May 2012</article-title>
</title-group>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>18</volume>
<issue>4</issue>
<fpage>500</fpage>
<lpage>505</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<sec id="section1-1356389012462939" sec-type="intro">
<title>Introduction</title>
<p>This note brings together highlights from an international workshop organized by the Evaluation Department of DANIDA – Danish Ministry of Foreign Affairs; the University of Copenhagen, Faculty of Life Sciences; and the journal <italic>Evaluation</italic>, and held in May 2012.</p>
<p>The note does not summarize papers presented at the workshop as all of these have been circulated and are available on the web at: <ext-link ext-link-type="uri" xlink:href="http://um.dk/en/danida-en/results/eval/reference-documents/workshp12/">http://um.dk/en/danida-en/results/eval/reference-documents/workshp12/</ext-link></p>
<p>The main focus of this note is on the issues that came up in discussion in Copenhagen. These both signpost current practice and policy dilemmas and highlight areas where further thinking and methodological innovation is needed.<sup><xref ref-type="fn" rid="fn1-1356389012462939">1</xref></sup></p>
</sec>
<sec id="section2-1356389012462939">
<title>Background</title>
<p>The workshop arose from two sets of concerns:</p>
<list id="list1-1356389012462939" list-type="bullet">
<list-item><p>those of practitioners and policy makers about how case studies were currently used in evaluation and how they might be strengthened; and</p></list-item>
<list-item><p>those of researchers that serious methodological and theoretical debates about case studies were not yet widely disseminated among international development evaluators.</p></list-item>
</list>
<p>Case studies are widely used in evaluation of international development as well as in evaluations in other sectors. But there seems to be little common ground about appropriate methods and the purposes for which case studies are best suited. Doubts are often raised about how far case study-based evaluations can be used for learning beyond a particular case. Partly this is a matter of how far one can generalize from ‘cases’ (raising questions of external validity) and partly from concerns about ‘quality’, ‘reliability’ and ‘robustness’ of methods and designs.</p>
<p>Against this background the workshop was organized around three themes:</p>
<list id="list2-1356389012462939" list-type="bullet">
<list-item><p>Validity: How do we improve the quality, reliability and trustworthiness of case-study evaluations?</p></list-item>
<list-item><p>Generalization: How far can we expect to generalize from case studies and in what circumstances?</p></list-item>
<list-item><p>Learning: If we are dealing with multiple case studies how can they be aggregated or synthesized and when is this important?</p></list-item>
</list>
<p>The intention was also to confront existing examples of development evaluation that used case-study approaches with state-of-the-art research thinking. This was the rationale of plenary presentations by David Byrne (Durham University) who is at the forefront of current thinking about case studies in the social sciences; and by Michael Woolcock (World Bank) who together with colleagues has been re-thinking ideas of external validity in complex programmes.</p>
</sec>
<sec id="section3-1356389012462939">
<title>Workshop highlights</title>
<p>The debate at the workshop was vigorous and extensive. This note only picks up on a few of the main themes – many more topics were of course discussed. The few themes highlighted here featured in workshop discussions in different ways: sometimes they were points made by those presenting papers; sometimes comments and responses to presentations; and sometimes the ideas emerged out of plenary and small-group discussions, or even from coffee breaks.</p>
<sec id="section4-1356389012462939">
<title>What is a case?</title>
<p>Unsurprisingly the rationale for defining a case was probably the most persistent topic for discussion at the workshop. After many of the presentations, the initial question would be ‘What is the case here?’ or ‘Why did you define the case in this way?’. For some, the ‘taken-for-granted’ assumption was that cases were real-world entities: countries, other spatial territories, industrial sectors, organizations, etc. For others, the definition of a case followed from ‘the problem at hand’. Similar raw material might be defined and structured as a case differently depending on the evaluation question being asked. This definition was consistent with the pragmatic way systems and complexity thinkers define system boundaries. In his opening address, David Byrne emphasized the distinction between ‘cases’ and ‘variables’. This perspective also incorporates ideas of complexity – the single case is complex – and the importance of context. A case approach looks for internal relationships and structures within a complex case in context rather than focusing on ‘disembodied variables’.</p>
<p>Michael Woolcock also regarded complexity as a key issue especially with regard to generalization. He put forward an ‘operational definition’ of complexity based on four analytic questions.</p>
</sec>
<sec id="section5-1356389012462939">
<title>Purpose, use and disciplinary bases for methodology</title>
<p>Methodology and the foundation for methodological choice was another recurring point of discussion. For some, this depended on the purpose of a case study. Was it: to describe; to give a voice to groups without power; or to support causal analysis and explain? To some present at the workshop and at certain times, it was all of these things. Participatory, descriptive and explanatory methodologies all, therefore, featured in workshop presentations. Sometimes it was noted that restrictions to methodological choice may be set by evaluation commissioners who favour some approaches rather than others and often did not favour case studies even when they were suitable. The use-setting was another important factor that shaped the form that case studies took. For example, case studies that were intended to inform organizational or institutional decision making were approached differently from those that intended to inform global policy making. It was argued that the kind of in-depth understandings from some kinds of case studies could be useful when planning or implementing a new programme rather than trying to draw policy conclusions at the global level. Finally, most disciplines carried their own methodological preferences and assumptions. For example, there were active debates between economists and sociologists about whether econometric analysis or QCA (Qualitative Comparative Analysis) or ‘neural networks’ were best able to handle complex multi-strand programmes. Mutual understanding of methods across disciplines was not always high.</p>
</sec>
<sec id="section6-1356389012462939">
<title>Explanation and causal inference</title>
<p>If the purpose of case-based evaluation was explanation, then this raised the question of how to draw causal inference. There were those who argued that causal inference had to be based on counterfactual logics and analyses. Others saw counterfactuals as tending to focus on one cause rather than the multiple causes that were to be expected in complex programmes. Statisticians unsurprisingly looked for a sufficient number of cases to support statistical analysis. However, many of those present were keen advocates of ‘realist’ thinking. They tended to ask ‘what worked, when, for whom and why’ rather than more undifferentiated causal questions, and they were more comfortable with explanations built on fewer cases provided that causal mechanisms that worked in particular settings were identified. This required moving back and forth between frameworks and data and beyond the logics of induction and deduction. Some workshop participants thought retroduction and abduction were also relevant ways of thinking.</p>
<p>There were crossover points in these debates. First, the role of theory was often emphasized. It was seen to be difficult to explain without some kind of theory. This could be ‘substantive theory’, for example, about conflict prevention; grounded theory built up from the dynamics of a particular case; or a theory of change that tried to unpick a programme or interventions logic or ‘implementation pathway’. A second point of convergence was the importance of comparison. Contemporary approaches to case-study research valued comparison as much as other methodological ‘schools’. However, the unit of comparison is likely to be between internal configuration within cases – or clusters of cases – rather than across large numbers of variables. Another type of comparison that Byrne advocated was historical ‘tracing’ of how a case changes over time.</p>
<p>At the heart of discussions about causality and explanations were notions of external validity and generalization. Michael Woolcock suggested that while findings about interventions and outcomes may not be generalizable, the underlying principles often were. He also questioned the current preoccupation with causality that only focused on the design of programmes. He argued that implementation and context needed to be seen as an integral part of understanding ‘impacts’.</p>
</sec>
<sec id="section7-1356389012462939">
<title>Quality, reliability and rigour</title>
<p>Those who commission and read evaluations are always concerned about whether the quality is good enough to rely on and draw reliable conclusions. Many different barriers to quality were spoken of at the workshop. For example, large-scale evaluations will often have a division of labour between a central team and local ‘partners’ or agents. Ensuring that what is done at a distance is consistent with central intentions is difficult – and becomes even more important when evaluations rely on synthesis across multiple cases. Country settings also matter. Countries that are large and diverse, prone to violence, and lacking reliable administrative data will limit case-study quality.</p>
<p>Definitions of rigour ranged from being transparent and logical in the way data is collected and analysed through to technical definitions that were associated with particular methods or techniques. (This partly mirrored qualitative/quantitative-method preferences.) Some of the difficulties in this discussion stemmed from the assumptions of different disciplines. For example, many present at the workshop were concerned about evaluator ‘independence’ and the need to ‘avoid bias’. However, there was some tendency to see risks of bias as greatest in the practices of other people’s disciplines.</p>
<p>Case-study selection was seen as one important risk to quality, though which cases were to be included are often specified by commissioners of evaluations. The risk was that selected cases might be either unusually positive or unusually negative and would therefore lead to unbalanced conclusions. It was acknowledged that case selection was often made on pragmatic grounds – ease of access – rather than for methodological reasons; and sometimes for ethical reasons (‘do no harm’). One response was to avoid drawing firm conclusions from case-study evaluations and to use case studies to put forward interim or tentative findings that could be further explored elsewhere. On the other hand, various approaches were suggested to address case selection including sampling, using clear and transparent criteria, and following a theoretical framework (perhaps filling out a theory of change). In one evaluation that was intended to focus on improvements, a ‘positive deviance’ approach to case sampling had been tried out.</p>
<p>Two methods that are relatively unknown in development evaluation were presented at the workshop: QCA and Systems Modelling. QCA, with its emphasis on configurations within cases as well as its use of Boolean algebra to support stronger conclusions from a modest number of cases, was close to David Byrne’s frame of reference. Systems modelling relies on both qualitative and quantitative methods, and allows existing knowledge to be incorporated into models and then ‘run’ under different conditions. It was suggested that systems modelling is currently closer to being a planning tool rather than an evaluation tool. However, the potential of using modelling in future when evaluating complex phenomena was widely recognized.</p>
<p>It was generally held that combining methods and using different methods was a good way to improve the quality of case-based evaluations. For example, triangulation and combining different methods were advocated as ways of reducing threats to validity in case studies. However, arguments for combining different methods, using qualitative and quantitative techniques and for integrating case studies into broader evaluation strategies also provoked questions; for example, ‘under what circumstances was this necessary?’ and ‘when were the extra costs of justified?’</p>
</sec>
<sec id="section8-1356389012462939">
<title>Coping with cases that are changing and emergent</title>
<p>It was widely understood that cases are not static; they evolve and change over time. Important aspects of a case may only become obvious once fieldwork is underway. New properties of cases ‘emerge’ and cannot always be predicted when an evaluation begins. This posed particular problems for policy evaluations as they often rely on pre-planned protocols and evaluation questions. It was also noted that what is called a case study may involve very few days in the field, which will be insufficient to track change or to recognize when things don’t change (e.g. where there is ‘path dependency’).</p>
<p>Results of policy and programme ‘interventions’ are commonly only detectable long after an evaluation is over. This raised the question of when to evaluate in the project cycle. This partly came back to purpose: sometimes case studies were planning tools; sometimes ways of improving management or the implementation of existing programmes; and sometimes ex-post tools for accountability. While purpose might be one rationale for timing a case-based evaluation, another implication of the ‘emergence’ argument was that evaluations should sometimes be staged or iterative, reflecting different stages of change in institutions, programmes or communities.</p>
</sec>
<sec id="section9-1356389012462939">
<title>Synthesis approaches</title>
<p>This was a workshop theme and the explicit subject of a number of papers. Synthesis was linked to discussions of learning and generalizability: it was commonly assumed that it was by accumulating or aggregating specific case studies that general lessons could be learned. One difficulty with synthesis was the quality of the cases being synthesized. This could be less difficult if there was a degree of control or specification of how individual cases were undertaken, but this was rare. One view was that there should be consistency in questions asked and criteria for success but there could be more flexibility in the methods used to answer these questions.</p>
<p>Theoretical frameworks were used to structure syntheses that were presented. Again this was difficult if the cases being synthesized had themselves not developed a theory of change or intervention logic. One theoretical example of sampling followed a realist approach aiming to ensure that there was adequate variation between inputs and outcomes across different contexts and then identifying mechanisms in different contexts. Theory-based approaches to synthesis (as for drawing causal inference) were seen as an important way to derive stronger conclusions from a relatively small numbers of cases.</p>
</sec>
</sec>
<sec id="section10-1356389012462939" sec-type="conclusions">
<title>Conclusions</title>
<p>The workshop was as an opportunity to review the ‘state-of-the-art’ in how case studies are being used in development evaluation. It also identified many as-yet-unanswered questions. From the point of view of the organizing committee, there were a number of tentative conclusions as well as some lessons.</p>
<list id="list3-1356389012462939" list-type="bullet">
<list-item><p>Many basic lessons that were shared among workshop participants were not unique to case-study evaluations. For example, the need to be clear about the purpose of an evaluation and the questions that are being asked is one that all evaluators must address.</p></list-item>
<list-item><p>Similarly many of the disagreements that surfaced at the workshop were not distinctive to discussions about case studies. Rather they could be seen as ‘imported’ from broader debates between different disciplines, or between qualitative and quantitative researchers or between those exclusively committed to counterfactuals and those who took a broader view of causal inference.</p></list-item>
<list-item><p>Nonetheless, there were new approaches to answering the question ‘What is a case study?’ and to case-study design in general that had much to offer development evaluators. This was especially timely because of the way these newer approaches focused on complexity and the priority in development evaluation to evaluate complex phenomena such as peace building in fragile states, governance, gender equality and climate change.</p></list-item>
<list-item><p>The workshop highlighted two main ways that could make it easier to draw conclusions and put forward explanations on the basis of relatively few cases. First, the emphasis on looking at internal relationships within the case as well as comparisons across variables; and second, the role of theory – whether to test programmes against theory or to develop theory (and theories of change) during an evaluation.</p></list-item>
<list-item><p>The state of knowledge about theories and methods appears unevenly spread across the development evaluation community. There is considerable scope for further interdisciplinary dialogue – e.g. between econometricians and ethnographers – and the further dissemination of newer case-study approaches as well as the further refinement of newer methods such as QCA and systems modelling.</p></list-item>
<list-item><p>Despite the concerns that are often expressed about the ‘quality’ of case studies in evaluation, this was an area where much thinking had been done and interesting solutions to common problems (e.g. of validity, reliability and bias) were beginning to emerge. These ways of strengthening evaluations that used case studies need to be further developed and more widely applied.</p></list-item>
</list>
<p>Of course all present will have drawn their own conclusions from the workshop!</p>
</sec>
</body>
<back>
<notes>
<fn-group>
<fn fn-type="other">
<p>Contribution from Ole Winckler Andersen (DANIDA), Henrik Hansen (University of Copenhagen) and Elliot Stern (the journal Evaluation).</p>
</fn>
<fn fn-type="other" id="fn1-1356389012462939">
<label>1.</label>
<p>Although the note draws on contributions from all of those who attended the workshop, it has been prepared by the organizing committee and does not claim to be a consensus view.</p>
</fn>
</fn-group>
</notes>
</back>
</article>