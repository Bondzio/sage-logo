<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TOP</journal-id>
<journal-id journal-id-type="hwp">sptop</journal-id>
<journal-title>Teaching of Psychology</journal-title>
<issn pub-type="ppub">0098-6283</issn>
<issn pub-type="epub">1532-2802</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0098628311430171</article-id>
<article-id pub-id-type="publisher-id">10.1177_0098628311430171</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Faculty Forum</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Effect and Implications of a “Self-Correcting” Assessment Procedure</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Francis</surname>
<given-names>Alisha L.</given-names>
</name>
<xref ref-type="aff" rid="aff1-0098628311430171">1</xref>
<xref ref-type="corresp" rid="corresp1-0098628311430171"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Barnett</surname>
<given-names>Jerrold</given-names>
</name>
<xref ref-type="aff" rid="aff1-0098628311430171">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0098628311430171">
<label>1</label>Northwest Missouri State University, Maryville, MO, USA</aff>
<author-notes>
<corresp id="corresp1-0098628311430171">Alisha L. Francis, Northwest Missouri State University, 800 University Drive, Maryville, MO 64468 Email: <email>alishaf@nwmissouri.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2012</year>
</pub-date>
<volume>39</volume>
<issue>1</issue>
<fpage>38</fpage>
<lpage>41</lpage>
<permissions>
<copyright-statement>© Society for the Teaching of Psychology 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Society for the Teaching of Psychology</copyright-holder>
</permissions>
<abstract>
<p>We investigated Montepare’s (2005, 2007) self-correcting procedure for multiple-choice exams. Findings related to memory suggest this procedure should lead to improved retention by encouraging students to distribute the time spent reviewing the material. Results from a general psychology class (<italic>n</italic> = 98) indicate that the benefits are not as definitive as expected. Students’ initial performance moderated the benefits of the procedure such that comprehensive final exam scores were significantly higher for the self-correcting condition when controlling for initial quiz performance, with a marginally significant interaction (<italic>p</italic> = .06) between initial quiz scores and condition. The findings underscore the importance of using the scientist-educator model to evaluate pedagogical decisions while considering practical implications and return on investment.</p>
</abstract>
<kwd-group>
<kwd>formative assessment</kwd>
<kwd>memory</kwd>
<kwd>scientist-educator model</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Multiple-choice exams are widely used across disciplines, in part because they are an efficient evaluation mechanism (<xref ref-type="bibr" rid="bibr3-0098628311430171">DiBattista, Mitterer, &amp; Gosse, 2004</xref>). However, instructors question the degree to which multiple-choice exams simply test the ability to encode and retrieve basic knowledge (<xref ref-type="bibr" rid="bibr3-0098628311430171">DiBattista et al., 2004</xref>; <xref ref-type="bibr" rid="bibr11-0098628311430171">Stupans, 2006</xref>), encouraging a passive approach to evaluation (<xref ref-type="bibr" rid="bibr4-0098628311430171">Epstein et al., 2002</xref>). Given this concern, educators have proposed a number of strategies to improve the learning resulting from multiple-choice exams, including processes that allow students to revise answers prior to grading.</p>
<p>The Immediate Feedback Assessment Technique uses a “scratch off” answer sheet to provide feedback (Epstein et al., 2002). Epstein et al. (2002) reported increased retention of material repeated during initial testing sessions. <xref ref-type="bibr" rid="bibr7-0098628311430171">Montepare (2005</xref>, <xref ref-type="bibr" rid="bibr8-0098628311430171">2007</xref>) advocated an approach in which students review their responses to multiple-choice questions after the initial testing period but prior to grading—referring to their notes, readings, or discussing the material with colleagues—and then changing answers if they choose.</p>
<p>Montepare’s (2005, 2007) procedure is a recurrent topic on the PSYCHTEACHER discussion list (e.g., <xref ref-type="bibr" rid="bibr5-0098628311430171">Huxtable-Jester, 2007</xref>) in part because it is a logical application of findings related to memory. The revision procedure encourages students to distribute the time they spend reviewing the material, creating spaced intervals between study sessions and delaying feedback. Given findings related to the spacing effect in memory, this distributed review could be expected to result in more effective retention of the material (<xref ref-type="bibr" rid="bibr2-0098628311430171">Dempster, 1988</xref>), as should the delay in feedback (<xref ref-type="bibr" rid="bibr9-0098628311430171">Smith &amp; Kimball, 2010</xref>).</p>
<p>Although findings related to the spacing effect and to memory support the value of the self-correcting approach outlined by <xref ref-type="bibr" rid="bibr7-0098628311430171">Montepare (2005</xref>, <xref ref-type="bibr" rid="bibr8-0098628311430171">2007</xref>), there is no research on its effectiveness in the classroom. The purpose of this study is, therefore, to evaluate the degree to which that self-correcting process affects retention of material. We hypothesize that students who have the opportunity to revise their initial answers to quizzes and a cumulative midterm exam prior to receiving their grade will have higher scores on a cumulative final exam.</p>
<sec id="section1-0098628311430171">
<title>Method</title>
<sec id="section2-0098628311430171">
<title>Participants</title>
<p>Students enrolled in one section of a general psychology course during the Fall of 2007 (<italic>n</italic> = 46) semester had the option of using Montepare’s (2005, 2007) self-correcting process in completing weekly quizzes and a cumulative midterm exam. To provide a control group, students in a second section (<italic>n</italic> = 52) completed the same assessments without the self-correcting component. Both sections met for three 50-minute classes per week with the control section beginning at noon and the experimental condition beginning at 1 p.m. The majority of students enrolled in both sections were classified as 1st-year students (85% of the experimental group and 76% of the control group).</p>
<p>The participants were enrolled at a moderately selective, midsized university (5,037 in Fall 2007) in the Midwest. The general psychology course was offered in 12 sections during the same semester with a total enrollment of 677 students. The university advisement assistance office creates fall schedules for the majority of 1st-year students, reducing systematic differences between the sections.</p>
</sec>
<sec id="section3-0098628311430171">
<title>Procedure</title>
<p>Students in both sections completed 12 quizzes, a cumulative midterm exam, and a comprehensive final exam. Weekly quizzes were comprised of 20 questions, including one to three fill-in-the-blank questions and five to eight matching questions, with the remaining questions in multiple-choice format. The midterm exam included 50 questions with the same formats as the quizzes and in similar proportions.</p>
<p>During the first class meeting, students in both sections received a brief overview of the self-correcting process. Students in the experimental section received a handout detailing the procedure. This handout explained that they had the option of taking home a copy of the questions and their responses for review and that they could revise their responses as they felt necessary. To maintain consistency with Montepare’s (2005, 2007) method, the handout noted they could use their book, use their notes, or discuss the questions with classmates.</p>
<p>Students in the control section were briefly told about the self-correcting process being used with the experimental section. The instructor acknowledged that students in the control section may feel it was unfair that they did not have the same opportunity. To address that difference, their quiz scores would be adjusted by an amount equal to the average increase in scores for the other section.</p>
<p>Students in both sections initially completed the quizzes and exam at the beginning of the class periods as the course instructor monitored the session to discourage cheating. Students in the control section received feedback immediately after completing the quiz or exam and did not have access to the questions after the testing period. Students in the correcting section received feedback on their answers at the beginning of the following class period and did not have subsequent access to the questions.</p>
<p>Students in both sections completed a 100-question cumulative exam during the final exam period. The question formats were identical to those included on the quizzes and midterm. Questions sampled the same content areas as were included on the quizzes and midterm, but the questions themselves were not identical. Fill-in-the-blank questions (which covered definitions of key terms and concepts) were modified to change the key word required. Matching or multiple-choice questions were modified in a number of ways, including changing the examples provided for application questions, changing the specific format in which a concept was presented (e.g., matching instead of multiple choice), changing the order in which answer options were presented, or using the same answer items but with the question revised such that a response that was previously an incorrect distracter became the correct answer. Prior to the midterm and the final, students received a study guide listing the terms and concepts that could be included in exam questions. The study guides were compiled by listing terms and concepts represented in both the correct answers to quiz questions and in the distracters for multiple-choice items.</p>
</sec>
</sec>
<sec id="section4-0098628311430171">
<title>Results</title>
<p>The proportion of students who could benefit from correcting their quizzes and opted to participate in the revision process ranged from 65.9% to 97.6% across the semester. The result of those changes ranged from a loss of 2 points to a gain of 15 points (<italic>M</italic> = 4.48, <italic>SD</italic> = 1.09, Mode = 1, 2). <xref ref-type="table" rid="table1-0098628311430171">Table 1</xref>
 includes descriptive statistics for individual quizzes. Of students in the experimental section, 91% revised responses on the midterm exam, resulting in changes ranging from 1 point to 21 points (<italic>M</italic> = 8.21, <italic>SD</italic> = 4.93, Mode = 14). A two-way contingency tables analysis evaluated whether initial quiz scores were related to participation in the revision process. Across the semester, initial quiz scores and participation were significantly related, Pearson’s χ<sup>2</sup> (4, <italic>N</italic> = 553) = 33.49, <italic>p</italic> &lt; .001. Of the students who initially scored between 90% and 95%, 61.4% participated in the revision procedure compared to 81.3% who initially scored in the <italic>B</italic> range, 83.3% in the <italic>C</italic> range, 88.2% in the <italic>D</italic> range, and 86.2% in the <italic>F</italic> range. Follow-up pairwise comparisons indicated that students who initially earned an <italic>A</italic> but could still improve their score, were significantly less likely to participate in the process compared to students who initially earned lower grades. Differences between other groups were not significant.</p>
<table-wrap id="table1-0098628311430171" position="float">
<label>Table 1.</label>
<caption>
<p>Mean Quiz and Midterm Scores, Mean Change, and Percentage of the Experimental Group Who Participated in the Change Process</p>
</caption>
<graphic alternate-form-of="table1-0098628311430171" xlink:href="10.1177_0098628311430171-table1.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>Quiz 2</th>
<th>Quiz 3</th>
<th>Quiz 4</th>
<th>Quiz 5</th>
<th>Quiz 6</th>
<th>Quiz 7</th>
<th>Quiz 8</th>
<th>Quiz 9</th>
<th>Quiz 10</th>
<th>Quiz 11</th>
<th>Quiz 12</th>
<th>Quiz 13</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean score: control group</td>
<td>16.39</td>
<td>16.46</td>
<td>17.76</td>
<td>15.50</td>
<td>15.98</td>
<td>15.42</td>
<td>17.83</td>
<td>17.40</td>
<td>14.91</td>
<td>15.73</td>
<td>15.96</td>
<td>16.29</td>
</tr>
<tr>
<td>Mean score: experimental group</td>
<td>16.29</td>
<td>15.70</td>
<td>17.04</td>
<td>13.67</td>
<td>14.11</td>
<td>14.17</td>
<td>16.59</td>
<td>14.80</td>
<td>13.64</td>
<td>13.21</td>
<td>14.13</td>
<td>14.30</td>
</tr>
<tr>
<td>Mean change (experimental group)</td>
<td>1.91</td>
<td>2.36</td>
<td>1.59</td>
<td>4.16</td>
<td>4.67</td>
<td>2.46</td>
<td>2.72</td>
<td>3.36</td>
<td>4.58</td>
<td>5.06</td>
<td>4.48</td>
<td>3.50 </td>
</tr>
<tr>
<td>Percentage correcting</td>
<td align="center">73.2</td>
<td align="center">65.9</td>
<td align="center">68.3</td>
<td align="center">75.0</td>
<td align="center">97.6</td>
<td align="center">65.2</td>
<td align="center">81.6</td>
<td align="center">70.5</td>
<td align="center">83.7</td>
<td align="center">93.2</td>
<td align="center">88.1</td>
<td align="center">73.2</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0098628311430171">
<p>Quiz 1 was an orientation and practice quiz completed the second day of class. As a result, the data were not included in this study.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The mean score on the final exam for students in the self-correcting condition was 81.86 points (out of 100, <italic>SD</italic> = 12.21), compared to a mean of 80.09 (<italic>SD</italic> = 12.47) for the noncorrecting condition. An independent samples <italic>t</italic> test evaluated the hypothesis that students who had the opportunity to revise their initial answers would have higher scores on a cumulative final exam. The test was not significant, <italic>t</italic>(94) = –0.697, <italic>p</italic> = .49, 95% CI (–6.78, 3.26), <italic>d</italic> = –0.14. The average initial quiz scores for the two groups were, however, significantly different, <italic>t</italic>(96) = 3.40, <italic>p</italic> = .001, 95% CI (0.62, 2.44), <italic>d</italic> = 0.69. Initially (across all quizzes), students in the self-correcting condition (<italic>n</italic> = 46) averaged 14.77 (out of 20, <italic>SD</italic> = 2.56), compared to 16.30 (<italic>SD</italic> = 1.86) for the noncorrecting condition (<italic>n</italic> = 52).</p>
<p>Given the significant difference between initial quiz scores averaged across quizzes 2 through 13, we conducted a multiple regression interaction analysis (<xref ref-type="bibr" rid="bibr1-0098628311430171">Aiken &amp; West, 1991</xref>). Final exam score was regressed on quiz average, condition, and the interaction between quiz average and condition. The overall model was significant, <italic>F</italic>(3, 92) = 20.71, <italic>p</italic> &lt; .001, <italic>f</italic><sup>2</sup> = 0.68, explaining 38.4% of the variance in final exam scores (see <xref ref-type="table" rid="table2-0098628311430171">Table 2</xref>
). There was a significant effect of condition indicating that, when controlling for initial quiz scores, students who were allowed to correct their quizzes had higher final exam scores (<italic>β</italic> = 0.26, <italic>p</italic> = .003). There was also a significant effect of quiz scores indicating that students who had higher quiz scores had higher final exam scores (<italic>β</italic> = 0.44, <italic>p</italic> = .001). The interaction between condition and quiz scores was marginally significant (<italic>β</italic> = 0.25, <italic>p</italic> = .06), suggesting that a positive relationship between condition and final exam scores was stronger for students who had higher quiz scores than for students who had lower quiz scores (see <xref ref-type="fig" rid="fig1-0098628311430171">Figure 1</xref>
).</p>
<fig id="fig1-0098628311430171" position="float">
<label>Figure 1.</label>
<caption>
<p>Relationship between condition and predicted final scores for values of one standard deviation above and below the mean of quiz scores for each condition given results of a multiple regression interaction analysis.</p>
</caption>
<graphic alternate-form-of="fig1-0098628311430171" xlink:href="10.1177_0098628311430171-fig1.tif"/>
</fig>
<table-wrap id="table2-0098628311430171" position="float">
<label>Table 2.</label>
<caption>
<p>Regression Analyses for Predicting Final Exam Scores From Condition, Quiz Average, and the Interaction Between Condition and Quiz Average for the General Psychology Sample</p>
</caption>
<graphic alternate-form-of="table2-0098628311430171" xlink:href="10.1177_0098628311430171-table2.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>
<italic>B</italic>
</th>
<th>SE <italic>B</italic>
</th>
<th>
<italic>β</italic>
</th>
<th>η<italic>p</italic>
<sup>2</sup>
</th>
<th>95% CI for <italic>B</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>Condition</td>
<td>6.36</td>
<td>2.08</td>
<td>0.26**</td>
<td>.092</td>
<td>2.22—10.50</td>
</tr>
<tr>
<td>Quiz average</td>
<td>5.63</td>
<td>1.71</td>
<td>0.44***</td>
<td>.106</td>
<td>2.43—9.02</td>
</tr>
<tr>
<td>Condition × Quiz</td>
<td>4.15</td>
<td>2.19</td>
<td>0.25*</td>
<td>.037</td>
<td>−0.21—8.50</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0098628311430171">
<p>SE = standard error; CI = confidence interval. Quiz average was mean centered. Condition was coded such that 0 = <italic>no correction </italic>and 1 = <italic>correction</italic>. The overall model was significant, <italic>F</italic>(3, 92) = 20.71, <italic>p</italic> &lt; .001. Constant = 78.41. Adjusted <italic>R</italic>
<sup>2</sup> = .384.</p>
</fn>
<fn id="table-fn3-0098628311430171">
<p>*<italic>p</italic> = .06. **<italic>p</italic> ≤ .01. ***<italic>p</italic> ≤ .001.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section5-0098628311430171">
<title>Discussion</title>
<p>The results provide partial support for the hypothesis that students in a general psychology course who had the opportunity to use Montepare’s (2005, 2007) self-correcting procedure for quizzes and a cumulative midterm exam would score higher on a cumulative final exam. A marginally significant interaction between initial quiz scores and quizzing condition indicates that students who do not initially perform well on the quizzes are less likely to benefit from the procedure.</p>
<p>The interaction between initial quiz scores and benefit from the procedure may reflect the ways in which academic engagement affects social interactions. Students with high initial quiz scores may be more academically oriented, which may affect the nature of their friendships and conversations. These students may be more likely to discuss exam questions with friends within and across the two sections. Those discussions have the potential to facilitate both the spacing effect and semantic encoding.</p>
<p>The interaction effect may also represent the complex connections between motivation, metacognition, and memory. Students who did not initially perform well on the quizzes may have experienced reduced motivation with the knowledge that they could revise the answers at a later date. That initial lack of motivation may, in turn, reduce the likelihood that students engaged in semantic encoding during the correction procedure. In other words, although the self-correcting process created a structure for students to benefit from the spacing effect, they may not have engaged with the material in a way that allowed them to realize the benefit (<xref ref-type="bibr" rid="bibr10-0098628311430171">Son, 2010</xref>). In the event that students were inclined to engage with the material during the correction procedure, the lack of initial motivation could reduce the associated benefits by reducing the number of initially correct responses available for retention (<xref ref-type="bibr" rid="bibr9-0098628311430171">Smith &amp; Kimball, 2010</xref>).</p>
<p>Collectively, the findings suggest that Montepare’s (2005, 2007) method represents a relatively poor return on investment. Implementation requires additional class time to explain the process and collect revised responses. Additional grading time is also required if both sets of answers contribute to the grade. Depending on the response format used, additional paper may be required, increasing the financial cost of the procedure. Similarly, the procedure requires a trade-off between using additional class time for students to revise their answers and allowing students to take question sheets to revise outside of class, increasing the potential for questions to enter the public domain. Indeed, the process outlined by Montepare—and followed in this research—allows students to take the questions from the classroom, creating the potential that copies are not only retained for studying at a later date but also made available to students in other sections (present and future).</p>
<p>The considerations noted above, coupled with the findings, reinforce the need for ongoing research. Analysis of students’ social patterns and networks may offer insights into the ways in which learning and assessment are influenced by social interactions, including encoding-related benefits as well as diffusion of test questions. Alternative processes for self-correction may also result in improved retention for a larger group of students. Processes requiring students to explain their rationale for an answer may encourage more meaningful encoding of material. Similarly, approaches that limit access to supplemental resources may lead to improved retention due to testing effects (<xref ref-type="bibr" rid="bibr6-0098628311430171">Karpicke &amp; Roediger, 2007</xref>).</p>
<p>The implications of our findings, however, transcend that specific procedure and underscore the importance of using the scientist-educator model in pedagogical decision making. The process we used was initially published in a teaching advice column in a magazine distributed to all members of an international professional organization in psychology. It was also republished as a chapter in a book designed to offer advice for teachers of psychology. Publication in those venues suggests an underlying expert knowledge that validates the procedure. The considerations and findings noted above suggest otherwise, offering a reminder that ongoing empirical investigations specific to teaching methods provide critical insight.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict" id="fn1-0098628311430171">
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article. </p>
</fn>
<fn fn-type="financial-disclosure" id="fn2-0098628311430171">
<p>The authors received no financial support for the research, authorship, and/or publication of this article. </p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0098628311430171">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Aiken</surname>
<given-names>L. S.</given-names>
</name>
<name>
<surname>West</surname>
<given-names>S. G.</given-names>
</name>
</person-group> (<year>1991</year>). <source>Multiple regression: Testing and interpreting interactions</source>. <publisher-loc>Newbury Park, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr2-0098628311430171">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dempster</surname>
<given-names>F. N.</given-names>
</name>
</person-group> (<year>1988</year>). <article-title>The spacing effect: A case study in the failure to apply the results of psychological research</article-title>. <source>American Psychologist</source>, <volume>43</volume>, <fpage>627</fpage>–<lpage>634</lpage>.</citation>
</ref>
<ref id="bibr3-0098628311430171">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>DiBattista</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Mitterer</surname>
<given-names>J. O.</given-names>
</name>
<name>
<surname>Gosse</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Acceptance by undergraduates of the Immediate Feedback Assessment Technique for multiple-choice testing</article-title>. <source>Teaching in Higher Education</source>, <volume>9</volume>, <fpage>17</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr4-0098628311430171">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Epstein</surname>
<given-names>M. L.</given-names>
</name>
<name>
<surname>Lazarus</surname>
<given-names>A. D.</given-names>
</name>
<name>
<surname>Calvano</surname>
<given-names>T. B.</given-names>
</name>
<name>
<surname>Matthews</surname>
<given-names>K. A.</given-names>
</name>
<name>
<surname>Hendel</surname>
<given-names>R. A.</given-names>
</name>
<name>
<surname>Epstein</surname>
<given-names>B. B.</given-names>
</name>
<name>
<surname>Brosvic</surname>
<given-names>G. M.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Immediate Feedback Assessment Technique promotes learning and corrects inaccurate first responses</article-title>. <source>Psychological Record</source>, <volume>52</volume>, <fpage>187</fpage>–<lpage>201</lpage>.</citation>
</ref>
<ref id="bibr5-0098628311430171">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Huxtable-Jester</surname>
<given-names>K</given-names>
</name>
</person-group>. (<year>2007, November 5</year>). <article-title>Strategies for self-correcting exams?</article-title> <comment>[Electronic mailing list message]. Retrieved February 23, 2008</comment>, <ext-link ext-link-type="uri" xlink:href="https://listkennesaw.edu/listserv/wa?A2=PSYCHTEACHER;b2a866f5.0711">https://listkennesaw.edu/listserv/wa?A2=PSYCHTEACHER;b2a866f5.0711</ext-link></citation>
</ref>
<ref id="bibr6-0098628311430171">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Karpicke</surname>
<given-names>J. D.</given-names>
</name>
<name>
<surname>Roediger</surname>
<given-names>H. L.</given-names>
<suffix>III</suffix>
</name>
</person-group> (<year>2007</year>). <article-title>Repeated retrieval during learning is the key to long-term retention</article-title>. <source>Journal of Memory and Language</source>, <volume>57</volume>, <fpage>151</fpage>–<lpage>162</lpage>.</citation>
</ref>
<ref id="bibr7-0098628311430171">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Montepare</surname>
<given-names>J. M</given-names>
</name>
</person-group>. (<year>2005, October</year>). <article-title>A self-correcting approach to multiple-choice tests</article-title>. <source>APS Observer</source>, <volume>18 (10)</volume>, <fpage>35</fpage>–<lpage>36</lpage>, <fpage>43</fpage>–<lpage>44</lpage>. <comment>Retrieved October 22, 2007, from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.psychologicalscience.org/observer/getArticle.cfm?id=1867">http://www.psychologicalscience.org/observer/getArticle.cfm?id=1867</ext-link>
</citation>
</ref>
<ref id="bibr8-0098628311430171">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Montepare</surname>
<given-names>J. M.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A self-correcting approach to multiple-choice tests</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Perlman</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>McCann</surname>
<given-names>L. I.</given-names>
</name>
<name>
<surname>McFadden</surname>
<given-names>S. H.</given-names>
</name>
</person-group> (Eds.), <source>Lessons learned</source> (Vol. <volume>3</volume>, pp. <fpage>143</fpage>–<lpage>154</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Association for Psychological Science</publisher-name>.</citation>
</ref>
<ref id="bibr9-0098628311430171">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname>
<given-names>T. A.</given-names>
</name>
<name>
<surname>Kimball</surname>
<given-names>D. R.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Learning from feedback: Spacing and the delay-retention effect</article-title>. <source>Journal of Experimental Psychology: Language, Memory, and Cognition</source>, <volume>36</volume>, <fpage>80</fpage>–<lpage>95</lpage>.</citation>
</ref>
<ref id="bibr10-0098628311430171">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Son</surname>
<given-names>L. K.</given-names>
</name>
</person-group> (<year>2010</year>). <article-title>Metacognitive control and the spacing effect</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>36</volume>, <fpage>255</fpage>–<lpage>262</lpage>.</citation>
</ref>
<ref id="bibr11-0098628311430171">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stupans</surname>
<given-names>I.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Multiple choice questions: Can they examine application of knowledge?</article-title> <source>Pharmacy Education</source>, <volume>6</volume>, <fpage>59</fpage>–<lpage>63</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>