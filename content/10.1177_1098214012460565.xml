<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="review-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AJE</journal-id>
<journal-id journal-id-type="hwp">spaje</journal-id>
<journal-title>American Journal of Evaluation</journal-title>
<issn pub-type="ppub">1098-2140</issn>
<issn pub-type="epub">1557-0878</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1098214012460565</article-id>
<article-id pub-id-type="publisher-id">10.1177_1098214012460565</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Dialogues</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Negotiating Measurement</article-title>
<subtitle>Methodological and Interpersonal Considerations in the Choice and Interpretation of Instruments</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Braverman</surname>
<given-names>Marc T.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1098214012460565">1</xref>
<xref ref-type="corresp" rid="corresp1-1098214012460565"/>
</contrib>
</contrib-group>
<aff id="aff1-1098214012460565">
<label>1</label>School of Social and Behavioral Health Sciences, Oregon State University, Corvallis, OR, USA</aff>
<author-notes>
<corresp id="corresp1-1098214012460565">Marc T. Braverman, School of Social and Behavioral Health Sciences, Oregon State University, 105 Ballard, Corvallis, OR 97331, USA. Email: <email>marc.braverman@oregonstate.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>34</volume>
<issue>1</issue>
<fpage>99</fpage>
<lpage>114</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Evaluation Association</copyright-holder>
</permissions>
<!--CL content="<italic>American Journal of Evaluation XX&#x00028;X&#x00029;</italic>"-->
<!--CL content="<italic>Braverman et al.</italic>"-->
<abstract>
<p>Sound evaluation planning requires numerous decisions about how constructs in a program theory will be translated into measures and instruments that produce evaluation data. This article, the first in a dialogue exchange, examines how decisions about measurement are (and should be) made, especially in the context of small-scale local program settings. Rigorous measurement strategies will increase the credibility of a study’s conclusions, but they usually entail various kinds of costs. In making measurement decisions, evaluators must establish standards for strength of evidence that a given measure produces, weigh alternative measurement options, and communicate carefully with clients and other stakeholders about the measurement requirements in a given evaluation.</p>
</abstract>
<kwd-group>
<kwd>evaluation methods</kwd>
<kwd>evaluation planning</kwd>
<kwd>measurement</kwd>
<kwd>impact evaluation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Social programs are called upon to produce evidence of their impacts, to a wide variety of interested parties that may include funders, agency partners, target audience members, legislators, internal administrators, and many others. When a program evaluation results in claims that desired impacts have been achieved, stakeholders’ acceptance of those claims will depend, in part, on their being confident that the evaluation study was well-planned, well-reasoned, well-executed, and focused on essential program outcome variables.<sup><xref ref-type="fn" rid="fn1-1098214012460565">1</xref></sup> Experienced evaluators can anticipate these eventual stakeholder judgments early on, during the evaluation planning process, and address them through features of the evaluation design. In this article, I examine one aspect of that planning process: the measurement-related decisions that will have a bearing on the strength of the conclusions about program effectiveness and success.</p>
<p>The evaluations that I am concerned with here are, for the most part, local evaluations of programs that are delivered on a relatively limited scale. I work in the Cooperative Extension system, and there are dozens, if not hundreds, of programs fitting this description delivered in most states in any given year. Many of the evaluations are not high-visibility in the sense of reaching audiences beyond the direct scope of the programs. Outside of their intended local audiences, most of these evaluations do not get wider dissemination through publication or other means. Nevertheless, they often influence the fate of the programs in question (<xref ref-type="bibr" rid="bibr7-1098214012460565">Braverman, Engle, Arnold, &amp; Rennekamp, 2008</xref>). Similar conditions probably hold for many program evaluations conducted across a range of nonprofit and for-profit organizations.</p>
<p>The local nature and under-the-radar quality of these evaluations can raise challenging questions about rigor and the quality of evidence. In the absence of requirements for journal publication and other peer review mechanisms, the planning decisions about evaluation rigor can be perplexing. At present, decisions about rigor are often made by the evaluator, sometimes in consultation with others, sometimes not. I argue that the decisions should be resolved in the context of the local program setting with the involvement of local stakeholders. My discussion will concentrate specifically on the area of measurement and instrumentation. Certainly, every other aspect of the evaluation can be subjected to this analysis as well.</p>
<p>An example may serve to illustrate the kinds of measurement-related decisions that are involved. Many impact evaluations of community-based educational programs use participants’ ratings of their own skills and knowledge as the strategy for measuring the learning outcomes of interest. These self-ratings, considered a form of indirect assessment (<xref ref-type="bibr" rid="bibr3-1098214012460565">Banta, 2004</xref>; <xref ref-type="bibr" rid="bibr40-1098214012460565">Suskie, 2009</xref>), are often used in a standard pretest–posttest design, in which they are collected before the program begins and again after it has ended. Self-ratings are also sometimes used in a retrospective pretest design, in which both sets of ratings are collected after the program’s conclusion and participants are asked to reflect on and report their prior, preprogram status on the variables. But self-assessments of learning are susceptible to numerous potential biases and sources of error that are difficult to appraise. “True” pretest ratings and retrospective ratings have been found to be prone to different kinds of bias (<xref ref-type="bibr" rid="bibr1-1098214012460565">Aiken &amp; West, 1990</xref>; <xref ref-type="bibr" rid="bibr20-1098214012460565">Hill &amp; Betz, 2005</xref>; <xref ref-type="bibr" rid="bibr27-1098214012460565">Lam &amp; Bengo, 2003</xref>; <xref ref-type="bibr" rid="bibr32-1098214012460565">Nimon, Zigarmi, &amp; Allen, 2011</xref>; <xref ref-type="bibr" rid="bibr34-1098214012460565">Pelfrey &amp; Pelfrey, 2009</xref>; <xref ref-type="bibr" rid="bibr42-1098214012460565">Taylor, Russ-Eft, &amp; Taylor, 2009</xref>). An alternative strategy would be to develop a direct assessment or test of relevant skills and/or knowledge, to be administered before and after the program (possibly involving parallel forms to avoid repetition). The two alternative strategies of direct assessment and participant self-ratings may both target the same construct of interest, for example, learning within a specified content domain, but they approach the task in distinctly different ways. Direct assessment will generally yield a far superior quality of evidence, but will also entail considerably more time, resources, and expertise. Thus, resource-strapped local program staff may ask whether a strategy of indirect assessment, by itself, can ever be considered adequate. In view of considerations regarding measurement rigor and implementation feasibility, arriving at an informed, defensible, and satisfactory answer to this question for a given program evaluation setting is complicated. As stated at the outset, the judgments of important stakeholder audiences—actual or anticipated—will have a significant influence on the answer.</p>
<p>I begin with an examination of the concept of methodological rigor and its relationship to properties of evidence and the larger concept of evaluation validity. Next, I discuss the development of evaluation measures and I consider the factors that may influence the adequacy of measures for achieving particular evaluation purposes in particular contexts. The final section explores challenges faced by the evaluation planning team and the ways that the evaluator can address and negotiate rigor-related measurement decisions. Some of the examples I consider are drawn from comprehensive, high-resource evaluation studies, but for the most part my focus is on local, small-scale evaluation settings.</p>
<sec id="section1-1098214012460565">
<title>Rigor, Evidence, and the Validity of Evaluation Studies</title>
<p>Building from a previous formulation (<xref ref-type="bibr" rid="bibr6-1098214012460565">Braverman &amp; Arnold, 2008</xref>), I view <italic>methodological rigor</italic> as a set of characteristics of an evaluation study, which support the study’s underlying logic and influence the confidence with which conclusions can be drawn. In a similar vein, <xref ref-type="bibr" rid="bibr10-1098214012460565">Coryn (2007)</xref> states that rigor concerns “the means by which integrity and competence are confirmed” (p. 26). <xref ref-type="bibr" rid="bibr23-1098214012460565">Johnson, Kirkhart, Madison, Noley, and Solano-Flores (2008)</xref> note, “Rigor addresses the extent to which an evaluation adheres to strict standards of research methodology” (p. 199).</p>
<p>A rigorously conducted evaluation will be convincing as a presentation of evidence in support of an evaluation’s conclusions, and will presumably be more successful in withstanding scrutiny from critics. Rigor is multifaceted and relates to multiple dimensions of the evaluation. For example, in the area of study design, it relates to the determination of how, when, and from whom data will be collected, and the structure of the critical comparisons that address the questions of interest. In the area of measurement, it relates to the internal logic of the linkages among constructs and outcomes, as well as the adequacy with which those outcomes have been translated into measurement strategies and instruments. Methodological rigor can be assessed from the evaluation plan and the quality of the evaluation’s implementation.</p>
<p>The concept of rigor is understood and interpreted within the larger context of validity, which “concerns the soundness or trustworthiness of the inferences that are made from the results of the information gathering process” (<xref ref-type="bibr" rid="bibr24-1098214012460565">Joint Committee on Standards for Educational Evaluation, 1994</xref>, p. 145). The concept of validity has its origins in theories of measurement, but has been extended to apply also to research and evaluation studies. There is relatively broad consensus that validity is a property of an inference, knowledge claim, or intended use, rather than a property either of a research or evaluation study (<xref ref-type="bibr" rid="bibr9-1098214012460565">Chen, Donaldson, &amp; Mark, 2011</xref>; <xref ref-type="bibr" rid="bibr38-1098214012460565">Shadish, Cook, &amp; Campbell, 2002</xref>) or of a test or measure (<xref ref-type="bibr" rid="bibr2-1098214012460565">American Educational Research Association, American Psychological Association, and National Council on Measurement in Education, 1999</xref>; <xref ref-type="bibr" rid="bibr26-1098214012460565">Kane, 2006</xref>; <xref ref-type="bibr" rid="bibr28-1098214012460565">Lissitz, 2009</xref>).</p>
<p>The technical aspects of an evaluation study that are associated with methodological rigor are directly linked to the quality of evidence that the study is able to produce. <xref ref-type="bibr" rid="bibr37-1098214012460565">Schwandt (2009)</xref> has described three properties of evidence that relate to its value for supporting inferences: (a) the <italic>relevance</italic> of the evidence to the inference being made, (b) its <italic>credibility</italic> or believability, and (c) its <italic>probative (inferential) force</italic>, that is, the strength with which it supports the inference. An evaluation’s measurement-related planning decisions and implementation activities, that is, its measurement-related rigor, will influence the quality of evidence with respect to each of these three dimensions.</p>
<p>The relevance, credibility, and strength of evidence form the basis for the evaluation study’s subsequent interpretations and inferences. Those interpretations may involve the meaning of the measurement scores, relationships between variables, integrity of treatment, and representativeness of samples. The validity of the inferences will be assessed based on judgments about the evidential support for those inferences. Thus, validity does not come into play until the study is completed and one or more inferences, such as whether an intended program impact has been achieved, are drawn from the study’s findings.</p>
<p>In recent years, a great deal of attention has been given to the concepts of rigor and standards of evidence, but those discussions have been focused almost exclusively on the relative merits of experimental and quasi-experimental designs in comparison to alternative approaches for generating evidence in support of causal claims (e.g., <xref ref-type="bibr" rid="bibr8-1098214012460565">Chatterji, 2007</xref>; <xref ref-type="bibr" rid="bibr12-1098214012460565">Donaldson, Christie, &amp; Mark, 2009</xref>; <xref ref-type="bibr" rid="bibr25-1098214012460565">Julnes &amp; Rog, 2007</xref>; <xref ref-type="bibr" rid="bibr31-1098214012460565">Mosteller &amp; Boruch, 2002</xref>). Much less attention among evaluators has been directed to how standards might be developed for rigorous measurement in order to yield valid interpretations about desired impacts.</p>
</sec>
<sec id="section2-1098214012460565">
<title>Moving From Constructs to Variables to Measures</title>
<sec id="section3-1098214012460565">
<title>Measurement as Simplification</title>
<p>The sociologist Hubert <xref ref-type="bibr" rid="bibr4-1098214012460565">Blalock (1979</xref>, <xref ref-type="bibr" rid="bibr5-1098214012460565">1982</xref>), writing about the relationship between theory and measurement, noted that measurement in social science, by its nature, is a process of simplification and ordering of the world. Simplification serves the purpose of advancing the generalizability of concepts across units, so that claims may be made about groups rather than individuals. For example, a questionnaire measuring weekly exercise and physical activity may use a single metric, such as minutes per week of vigorous exercise, to classify individuals along a single dimension to establish comparability, even though their actual patterns of exercise will vary from each other in multiple ways that the metric ignores. This simplification process, however helpful, will necessarily entail assumptions and potential biases. Blalock noted that social science theories are, explicitly or implicitly, accompanied by “auxiliary measurement theories” that underlie the use of whatever specific measures have been chosen. “In short,” he wrote, “we must become much more attentive to the need for stating explicit auxiliary measurement theories and for examining comparability of measurement, just as we must also be concerned about the generalizability of our substantive theories… If either process lags too far behind the other, we shall find ourselves stymied” (<xref ref-type="bibr" rid="bibr5-1098214012460565">Blalock, 1982</xref>, p. 31).</p>
<p>Blalock’s view of measurement theory places importance on the ways that measures reflect their underlying constructs, and thus it has obvious parallels to construct validity, which many psychometricians consider the most fundamental form of validity in the measurement literature, incorporating aspects of both criterion and content validity (<xref ref-type="bibr" rid="bibr26-1098214012460565">Kane, 2006</xref>). “Arguably in its most common current use, construct validity refers to the degree to which inferences can be made legitimately from the observed scores to the theoretical constructs about which these observations are supposed to contain information… The term ‘construct validity’ has therefore evolved to be shorthand for the expression ‘an articulated argument in support of the inferences made from scores.’” (<xref ref-type="bibr" rid="bibr49-1098214012460565">Zumbo, 2009</xref>, pp. 68–69). In other words, construct validity is concerned with how well desired outcomes are represented by the scales, tests, survey items, observation forms, scoring rubrics, and other measures that evaluations employ.</p>
</sec>
<sec id="section4-1098214012460565">
<title>The Specification Process</title>
<p>The measurement specification process generally begins with the broadly conceived target construct, which reflects, often in everyday language, the issue that the program is designed to address. Many of the stakeholders who come together to create social programs express their goals for the desired anticipated outcomes in broad terms. For example, they may want to reduce obesity, increase student interest and enrollment in science courses, or promote safe sexual practices and reduce incidence of sexually transmitted diseases. In stakeholders’ conversations about program goals, their conceptions might not be more precise than this. These general constructs need to be interpreted and operationalized before the program and the evaluation begin. This process presents a chain of specific decision points, each of which can present a wide array of options for the evaluation planning team.</p>
<p>
<xref ref-type="fig" rid="fig1-1098214012460565">Figure 1</xref> presents a model and illustration of this process in four phases, mapping a chain of potential decision options for a program focused on nutrition education and healthy eating. The phases are marked by increasing specificity, and the expansion of options at each of the steps is readily apparent. The figure’s identification of those options is necessarily incomplete, as many more options can be suggested for each phase.</p>
<fig id="fig1-1098214012460565" position="float">
<label>Figure 1.</label>
<caption>
<p>Illustration of the possible progression from construct to instrument (some paths left incomplete).</p>
</caption>
<graphic xlink:href="10.1177_1098214012460565-fig1.tif"/>
</fig>
</sec>
<sec id="section5-1098214012460565">
<title>The Choice of Variables</title>
<p>A variable is a specific characteristic of a unit—person, family, and so on—that has differing values within a population of those units. The variables selected for an evaluation must be more precisely specified and measurable than the constructs from which they are drawn. One aspect of rigor in this step of the translation process will be the presence of a clear and logical link between the construct and the variable. Ideally there will be general agreement among the evaluation team and the primary stakeholders that the variable is a satisfactory representation of the guiding construct.</p>
<p>In small-scale, low-resource evaluations, one common scenario in which this link may be open to challenge can occur when a program aimed at producing behavioral change does not have the resources to follow participants for the long term, and must end data collection concurrently with the conclusion of the program. In such cases, the staff may feel constrained to measure a short-term outcome identified in a program’s logic model and claim it as a proxy for a long-term outcome further down the outcome chain (see, e.g., <xref ref-type="bibr" rid="bibr16-1098214012460565">Funnell &amp; Rogers, 2011</xref>; <xref ref-type="bibr" rid="bibr45-1098214012460565">Weiss, 2000</xref>). For example, the behavioral construct of greatest interest, healthy eating, may be replaced by a presumed psychological mediator, dietary intentions, as the evaluation’s primary outcome variable. This substitution may or may not be justifiable in terms of the overall goal of the evaluation, and there may or may not be reasonable alternatives given the available resources. But program staff, evaluators, and other stakeholders need to be aware of the ways that the substitution weakens the ability to draw conclusions about the program. In this example, the program might produce impressive increases in behavioral intentions, but the original target construct—eating behavior—will remain unexamined.</p>
</sec>
<sec id="section6-1098214012460565">
<title>The Choice of Approach</title>
<p>The options for a measurement approach will vary depending on the program context and the nature of the variable. For variables that reflect individual behaviors, the available approaches might include self-report by the participant, direct observation by the evaluation team, reports by others (e.g., parents or teachers on behalf of children), and physiological or biochemical measures, among others. For variables that reflect participants’ skills and aptitudes, the options might include direct tests, self-assessment by the individual, assessments by others, or existing indicators (e.g., standardized test scores). For variables that reflect internal psychological states (e.g., motivation, interest in a topic, satisfaction with a service), there may not be viable alternatives to self-report.</p>
<p>To measure variables that reflect presumably objective facts or status, such as body mass index, weight, or grade in math class, there might seem to be little choice of appropriate measures. But even in these cases, decisions must be made that will affect the reliability and accuracy of the results. In the case of weight, for example, evaluation planners will need to decide how carefully to control the time of day of the measurements, and whether the weight should be self-reported or recorded by a member of the evaluation team. In the case of grades, they may decide to ask the student or may seek more direct access to grading information.</p>
<p>A convergent validity study by <xref ref-type="bibr" rid="bibr41-1098214012460565">Sussman and Stacy (1994)</xref> provides a valuable illustration of the divergences that can exist across different measurement strategies for the same conceptual variable. They compared alternative strategies to derive estimates of two institutional-level variables: school-level student cigarette use and school-level student alcohol use. For cigarettes, the five strategies were students’ self-reports of their own use, students’ estimates of school prevalence, staff estimates of school prevalence, structured direct observation of student behavior at locations near the school, and refuse evidence observed at selected sites such as parking lots and sports fields. For alcohol, they used the same strategies except direct observation. (The strategies of student self-reports, student prevalence estimates, and staff prevalence estimates entailed interviews with small samples of students and staff.) The researchers found the correlations between these measures to be strikingly divergent, ranging between .01 and .54 for cigarette use and between .09 and .63 for alcohol use. One is impressed not only by the lack of correlation in some of these pairings but also by the moderate limits on the high end: none of the correlations exceeded .63, indicating that the two most strongly related strategies (student and staff estimates of alcohol use prevalence) shared only 40% of their variance. Considering that these strategies were all intended to assess the same variables, this example illustrates the dramatic implications that the choice of measurement strategy can have on the apparent levels of an evaluation’s outcome variables.</p>
<sec id="section7-1098214012460565">
<title>The use of multiple approaches to supplement self-report</title>
<p>Because all measurement approaches have characteristic limitations, plans that combine strategies will provide a stronger evidence base. This is particularly true for self-report, which is almost ubiquitous in local evaluations. Self-report information is easy and fast to collect, and it can address states of mind and private behaviors that may be impossible to obtain in other ways, certainly not with the same degree of efficiency. However, self-report also has numerous well-known limitations and problems, most importantly the susceptibility to bias due to errors of memory, misperception, or intentional misleading (e.g., <xref ref-type="bibr" rid="bibr13-1098214012460565">Donaldson &amp; Grant-Vallone, 2002</xref>). Therefore, when possible, measurement strategies that buttress self-report with another method to assess the variable of interest, such as biological or physiological measurement (when relevant), will considerably increase validity.</p>
</sec>
</sec>
<sec id="section8-1098214012460565">
<title>The Selection or Development of Instruments</title>
<p>Finally, when the variable and measurement approach have been decided, the evaluation planning team must decide the structure, format, and (if appropriate) wording of the instrumentation. Recommendations about instrument design are beyond the scope of this article, but a few examples can illustrate the decisions to be considered:</p>
<sec id="section9-1098214012460565">
<title>The use of multiple measures and triangulation</title>
<p>Any given measure will have limitations, which might include the kind of populations it can be used with, its reliability (as measured by internal consistency, interrater consistency, consistency over repeated administrations, etc.), its dependence on human memory or other fallible sources, the specificity of information it provides, and so on. The use of more than one measure can help neutralize the limitations or weaknesses of any single measure.</p>
</sec>
<sec id="section10-1098214012460565">
<title>Multiple-item versus single-item measures</title>
<p>Some variables, such as age, gender, and other demographics, can be addressed completely through the use of a single item. For more complex variables, including many kinds of behaviors, the necessary number of items within a measure will be less clear. It can be tempting to cover the information of interest with a single item (<italic>e.g., How much do you exercise each week?</italic>), but single items are susceptible to significant problems of reliability. The use of multiple items, if appropriate for the variable under consideration, generally increases reliability by increasing the sampling of both content and respondent performances (<xref ref-type="bibr" rid="bibr11-1098214012460565">Cronbach, 1990</xref>). However, this also increases the time required for response, so if a questionnaire covers a great deal of ground, this is a trade-off that needs to be weighed.</p>
</sec>
</sec>
</sec>
<sec id="section11-1098214012460565">
<title>The Adequacy of Measures</title>
<p>The evaluation planning team needs to consider an array of factors when deciding how the program’s constructs will be represented through specific measures. Two strategies can be immediately highlighted as paths to avoid. The first is simply to select the easiest measurement option—the one that minimizes planning time, testing time, program resources, and due diligence—and hope for the best when the study is done, in terms of its usefulness and influence. The second strategy, which sometimes occurs when local programs work jointly with an outside evaluator, involves the evaluator making the decisions on what approach is required—based on his or her methodological training, desire for a publishable study, or other reasons—and expecting program staff and participants to conform. The first strategy runs the risk of completing an irrelevant evaluation study, whereas the second risks implementation challenges and the breakdown of the plan.</p>
<sec id="section12-1098214012460565">
<title>Ideal and Adequate Rigor: Considerations of Cost for Alternative Measurement Strategies</title>
<p>In cases of well-funded, high-stakes evaluation studies, there is typically consensus that the evaluation plan needs to be as rigorous as possible in order to produce strong, credible evidence and a high level of confidence for the study’s interpretations, inferences, and conclusions. This may mean, for example, very close monitoring of the program’s operation, careful attention to sample selection and retention, multiple measurement points that can track the linkages specified in the program theory, and selection of measures that are the most valid and comprehensive representations of the program theory’s constructs.</p>
<p>However, rigor will involve costs of various kinds, and in smaller local evaluation contexts it may be productive to ask what may be an <italic>adequate</italic> level of rigor for measurement or other aspects of the evaluation study. <xref ref-type="bibr" rid="bibr8-1098214012460565">Chatterji (2007)</xref> argued along similar lines in discussing the strength of evidence afforded by different kinds of research designs for purposes of inferring causality. She maintained that the quality of evidence should not be judged simplistically as either strong or weak, but rather in terms of “grades of evidence” that vary along certain dimensions.</p>
<p>A concern for identifying criteria for acceptable levels of rigor, as well as for the strength and credibility of evidence, can result in useful standards or benchmarks as the planning proceeds. These determinations will be situated within the local context of what the program is trying to do. The primary stakeholders, and the forms of information that they need and expect, must be clarified. Program stakeholders can be drawn into preliminary discussions in which they clarify their own intended uses for the information and the strength of evidence they require—questions they may not at first be readily able to answer. The importance of engaging primary stakeholders is widely acknowledged, but matters of evaluation methodology are not always considered in these discussions.</p>
<sec id="section13-1098214012460565">
<title>Inadequate rigor</title>
<p>An obvious point, which nevertheless deserves emphasis, is that throughout the process of negotiation and decision making, care must be taken that all parts of the study are at least minimally adequate to fulfill the evaluation’s purposes and to be consistent with established standards for effective evaluation practice (<xref ref-type="bibr" rid="bibr47-1098214012460565">Yarbrough, Shulha, Hopson, &amp; Caruthers, 2011</xref>). Options brought to the table that are judged to have inadequate rigor, such that their prospects for producing credible evidence are unacceptably low, must be rejected, independent of other considerations. Poor evaluation practice provides no benefits, and can easily do harm to programs and their stakeholders by creating inaccurate or misleading information.</p>
</sec>
</sec>
<sec id="section14-1098214012460565">
<title>The Rigor-Feasibility Dynamic</title>
<p>
<xref ref-type="bibr" rid="bibr6-1098214012460565">Braverman and Arnold (2008)</xref> enumerated several categories of costs associated with increases in evaluation rigor—money and other material resources, time, and participant burden. <italic>Time</italic> can constitute a cost in multiple respects, including preparation and planning time (which may translate to money), the time available with participants during data collection, and the time horizon available for tracking change in the target groups. <italic>Participant burden</italic> can take several forms. Filling out questionnaires takes time and effort, and in some evaluation studies the questionnaires can become quite long. There may be multiple measures, as well as items that ask for revealing or sensitive information. The data collection may need to be completed at specific points in time, with which participants need to comply. If the program’s participants decide that they do not wish to tolerate these infringements, they will decline, and the evaluation will suffer accordingly.</p>
<p>In sum, these increased costs of rigorous methodology may lead to the judgments that some measurement strategy options are more feasible for the evaluation than others. These considerations may persuade program staff, target audiences, and others involved in the planning to reject the more demanding approaches in favor of the simpler ones.</p>
<p>Here is a thought experiment: If we can conceive of both the rigor and feasibility concepts as single, linear dimensions, one may imagine the relationship between them as depicted in <xref ref-type="fig" rid="fig2-1098214012460565">Figure 2</xref>. This is a highly simplified representation, because there is no reason to assume that either of these concepts must be either unidimensional or linear. Nevertheless, the figure is offered to illustrate a basic conceptual point. Rigor in evaluation is typically associated with increased demand on people and/or resources. To the extent that the feasibility of any approach implies certain resource requirements and acceptance by affected audiences, feasibility and rigor are probably negatively related, more often than not, so that high levels of one will be associated with low levels of the other. As <xref ref-type="fig" rid="fig2-1098214012460565">Figure 2</xref> illustrates, an overemphasis on one of these dimensions may result in the other being so low that the evaluation becomes unacceptable or unworkable. (The existence of a discrete point signifying the change from acceptability to unacceptability is another oversimplification for purposes of illustration.) Thus, if rigor is too low, the level of confidence will be so compromised as to render the evaluation not worth doing. On the other hand, if feasibility is too low, there will be uncertainty about whether the evaluation can be completed.</p>
<fig id="fig2-1098214012460565" position="float">
<label>Figure 2.</label>
<caption>
<p>Hypothesized (and simplified) conceptual representation of the relationship between evaluation rigor and implementation feasibility.</p>
</caption>
<graphic xlink:href="10.1177_1098214012460565-fig2.tif"/>
</fig>
<p>This leaves a range of options between the two thresholds, in which both rigor and feasibility may be relatively higher or lower, in opposite relation to each other, but nevertheless acceptable. This intermediate range, however the concepts may be operationalized in terms of individual evaluation decision options, is the territory for negotiation and for attentive, thoughtful decision making—a “sweet spot” for planning, so to speak. In my experience, this deliberative decision-making process about methodology does not take place as often as it should during evaluation planning—at least not in a way that is explicit, conscious, and inclusive of diverse stakeholder influence.</p>
<p>A final consideration is how the thresholds for acceptability get set. In the local program context, this also can derive from discussion and negotiation, guided by the expertise and methodological sophistication of the evaluator in advancing arguments to help parties understand the bases for credibility of evidence. Of course, as the scale, complexity, stakes, visibility, and resources for an evaluation increase, questions about acceptable rigor quickly become considerably more complicated, and, perhaps, less negotiable.</p>
<p>When feasibility issues are taken into consideration, it may be determined that a moderate level of rigor—that is, something less than ideal—may be acceptable for the primary stakeholders and planned uses of the evaluation evidence. In the next section, I turn to the interpersonal aspects of the evaluator’s role, particularly the processes of guiding discussion and building consensus. In some cases, it may be appropriate for the evaluator to make the case that the rigor, and the ensuing evidence base, should be stronger.</p>
</sec>
</sec>
<sec id="section15-1098214012460565">
<title>Interpersonal Processes and the Evaluator’s Active Role</title>
<p>In arriving at an evaluation plan, the evaluator may need to engage in a range of interpersonal and group activities—negotiating, teaching, consensus building, advocating—to bring various stakeholder perspectives and priorities into focus. Ideally, the resulting evaluation plan will represent a consensus among the program’s stakeholder groups. But at a minimum, it will reflect the thoughtful weighing of concerns, resulting in an evaluation that should be maximally useful given the identified challenges, constraints, and opportunities. The process can entail several components.</p>
<p>Underlying this conception of the planning process is a perspective that evaluation is fundamentally a process of argument (e.g., <xref ref-type="bibr" rid="bibr17-1098214012460565">Greene, 2011</xref>; <xref ref-type="bibr" rid="bibr21-1098214012460565">House, 1980</xref>; <xref ref-type="bibr" rid="bibr43-1098214012460565">Wallace, 2011</xref>). Ernest House cogently characterized this position: “Evaluation techniques are often presented as being nonargumentative, as, for example, being based on valid and reliable instruments, as employing sound statistical procedures, and so on. In fact, all statements made on the basis of an evaluation are subject to challenge and are arguable—if properly challenged. The more technical and quantitative the evaluation, the less a naïve audience will be able to challenge it, and the evaluation will appear to be more certain than it is” (<xref ref-type="bibr" rid="bibr21-1098214012460565">House, 1980</xref>, p. 74). In this spirit, the recommendations I offer for evaluator activities can, hopefully, help to reduce stakeholders’ naiveté and increase the sophistication with which they approach questions about rigor.</p>
<sec id="section16-1098214012460565">
<title>Establishing the Standards for Evidence</title>
<p>I have participated in numerous evaluation planning meetings in which the program staff’s primary questions center on their concerns about the ease of implementing data collection. They wish to know how long various instruments may take to administer, whether the data collection might generate boredom or frustration, how much time the data collection will take away from program delivery, and so on. These are legitimate local concerns, which can justifiably be the basis for accepting some measurement options and rejecting others. However, the concerns can often obscure the longer term aims of the evaluation to produce strong and credible evidence that can support useful conclusions about the level of program success.</p>
<p>Thus, one of the evaluator’s tasks is to make sure that the challenges inherent in conducting the evaluation are balanced against the evaluation’s potential long-term utility and value. The planning group and the program stakeholders who are being consulted must take explicit account of the broader utilization context. The evaluator will also be able to remind the group about the parties <italic>not</italic> in the room who will nevertheless be important audiences for the study, which might include—depending on the individual case—program funders, target audience members, state- or county-level legislators, clearinghouse committees, journal editors, and so on. The evidence standards for these audiences will need to be identified or estimated, ideally through direct consultation but, at a minimum, through knowledgeable appraisal.</p>
<p>
<xref ref-type="bibr" rid="bibr33-1098214012460565">Patton (2008)</xref> has described procedures and exercises in which the evaluator can lead stakeholders through a futuring process, to anticipate potential findings and what the utilization response will be in each case. Issues about the credibility and strength of evidence need to be part of these discussions, the result of which will be guidance for determining how rigorous the evaluation needs to be. Thus, taking into account all of the reasons for which the evaluation is being done, the group will determine the criteria for the strength of the evidence base resulting from the evaluation, in order to support the conclusions they may wish to draw with a desired level of confidence.</p>
<p>This process suggests that the adequacy of evidence—in terms of its relevance, credibility, and strength (<xref ref-type="bibr" rid="bibr37-1098214012460565">Schwandt, 2009</xref>)—lies in the eyes of the beholders, who in this case are the eventual audiences for the evaluation. This is consistent with the view that evaluation is, indeed, a process of argument. Consider the example of a smoking cessation program, for which the primary outcome of interest is postprogram smoking levels. Given an expected social desirability bias that may lead to underreporting of smoking, the planning team must decide whether participant self-report, by itself, will be a sufficiently rigorous approach to address the questions and concerns of the evaluation’s primary audiences. If those audiences consist of community members and organizations, the answer may be yes. However, if the primary audiences also include public health researchers, editors, or critics who have already expressed an interest in defunding the program, the answer may be no. In the latter case, the evaluation planning team will need to consider supplementing the self-report with biochemical validations such as saliva tests for cotinine or expired air samples for carbon monoxide (<xref ref-type="bibr" rid="bibr22-1098214012460565">Hukkanen, Jacob, &amp; Benowitz, 2005</xref>). The general point is that evidence cannot be judged as sufficiently or insufficiently credible in the absence of the context in which it is being used, the audiences who review it, and the purposes to which it is being put.</p>
</sec>
<sec id="section17-1098214012460565">
<title>Weighing Alternatives</title>
<p>Based on the evaluator’s training, acquired knowledge, and experience, he or she will probably be in the best position to educate and advise about the strengths and shortcomings of various kinds of evidence. Consider the example presented earlier, in which the long-term outcome variable of primary interest is healthy eating behavior (e.g., 6 months postprogram), but a potential short-term proxy under consideration is dietary intentions, measured at the program’s immediate conclusion. The difference in costs between these alternatives will be considerable and will favor the shorter term option. A determination must be made with respect to the net benefit of the longer term (behavioral) option, whether it is worth those added costs, and how much credibility would be lost if only the intentions variable were used. In this case, the evaluator might need to describe the relationship that has been found previously between intentions and behaviors in various domains. In general, the relationship is positive and significant, but certainly limited (e.g., <xref ref-type="bibr" rid="bibr15-1098214012460565">Fishbein, 2008</xref>; <xref ref-type="bibr" rid="bibr44-1098214012460565">Webb &amp; Sheeran, 2006</xref>). More specifically, the findings have been mixed for the relationship between intentions and behaviors in the specific area of eating behavior (e.g., <xref ref-type="bibr" rid="bibr29-1098214012460565">Lohse, Wall, &amp; Gromis, 2011</xref>; <xref ref-type="bibr" rid="bibr39-1098214012460565">Shaikh, Yaroch, Nebeling, Yeh, &amp; Resnicow, 2008</xref>; <xref ref-type="bibr" rid="bibr48-1098214012460565">Zoellner, Estabrooks, Davy, Chen, &amp; You, 2012</xref>). Depending on the audiences for the evaluation and the discussions about this research base, the use of intentions as an outcome variable may or may not be sufficiently convincing to enable the evaluation study to achieve its intended purposes.</p>
<p>The evaluator can also lead the planning team through the process of identifying and weighing uncertainties regarding the consequences of alternative decision options. For example, the benefits for data analysis inherent in using a more comprehensive and detailed set of measures may be offset by issues of sample representativeness if substantial numbers of participants refuse to participate in the evaluation. However, the effects of each of the alternatives on participation rates may not be easy to predict beforehand. If a high participant-demand measurement strategy results in lowering participation by, say, 20–30% compared to less demanding options—for example, smokers declining to participate in a study that includes saliva tests compared to only self-report—the study’s rigor, as a whole, will not be enhanced by the ostensibly more rigorous option. The variety of consequences associated with each option cannot be predicted with precision, thus introducing degrees of uncertainty into the mix. Examining the experiences of prior evaluation studies will be useful in reaching reasoned predictions, and the evaluator is probably in the best position to identify these precedents and assess their relevance to the context at hand.</p>
</sec>
<sec id="section18-1098214012460565">
<title>Negotiating the Selection of Measures</title>
<p>Bringing the planning team to a set of final decisions about which measures to use in the evaluation will incorporate judgments about the most critical variables for capturing the program theory’s central constructs, the most appropriate measurement strategies, the required strength of evidence, and the anticipated feasibility of the approach. In Blalock’s (1982) terminology, these decisions will reflect the auxiliary measurement theory that supports the primary program theory driving the evaluation. If there is an initial consensus about the measures, the team will be fortunate. If not, the group will need to resolve its differences in some way that is reasonably acceptable to all. The evaluator will play a key role in this consensus-building process, but may or may not be the individual who leads it.</p>
<p>Consider again the scenario mentioned earlier in this section, in which the program staff’s immediate focus is the ease of implementing the data collection procedures. It may be that the evaluator believes that more rigorous measurement is needed, compared to what the staff and/or other members of the planning team are advocating. The evaluator can remind the group that if they are fortunate enough to wind up with an evaluation that shows program impact, the demonstrable rigor of the study’s plan and its implementation will be of tremendous political benefit for the program. It may be that the feasibility concerns still carry the day. In that case, the decision will at least have been well considered, and the long-term consequences, in terms of relative reductions in the validity or persuasiveness of the study’s conclusions, will presumably have been anticipated.</p>
</sec>
<sec id="section19-1098214012460565">
<title>Explaining the Measurement Requests to Program Participants</title>
<p>When describing the measurement requirements of impact evaluations to program participants or, in the case of children, to their parents, it may be helpful to stress to them the value of the information that is being sought about the program and/or the social condition being addressed. To the extent that the participants whom it serves value the program, an argument that the program needs the opportunity to convincingly demonstrate its effectiveness may be persuasive.</p>
<p>A colleague of mine, Dr. Katherine Gunter, studies bone health in children and has developed physical activity programs that emphasize high-impact exercises such as jumping and running. These exercises, when performed with sufficient frequency, produce long-lasting benefits in bone density and mineral content in the hip, spine, and other skeletal areas (<xref ref-type="bibr" rid="bibr19-1098214012460565">Gunter &amp; Kasianchuk, 2011</xref>; <xref ref-type="bibr" rid="bibr18-1098214012460565">Gunter et al., 2008</xref>). In evaluations of these programs, the measurement of the primary outcome variable of bone mass entails the use of dual-energy X-ray absorptiometry (DXA), a scan that requires an extremely low dose of radiation, approximately the amount that an individual would receive from background environmental radiation exposure in a normal day’s activity. In the course of program evaluations over several years, Dr. Gunter has spoken to many parents of the children who participate in the programs. She explains the procedure, in accord with her university’s Institutional Review Board (IRB) protocols, and the parents make an informed decision about whether to let their children undergo the DXA scans. The great majority of parents and children do provide their consent, but, in many cases, only after considerable discussion to address the parents’ concerns and questions.</p>
<p>This example illustrates a case in which the presentation of the evaluation’s measurement procedures to prospective participants requires significant knowledge and expertise. In the case of DXA scans, there are no alternative procedures that can address the primary outcome variable of bone mass. The evaluator, researcher, or other individuals charged with this task must communicate with honesty and objectivity so that participants can reach an informed decision.</p>
<p>In some program settings, the ethical issues involved in discussions with participants will be particularly challenging. <xref ref-type="bibr" rid="bibr30-1098214012460565">Mitchell, Nakamanya, Kamali, and Whitworth (2002)</xref> report on a process evaluation they conducted to examine the local acceptability of a randomized trial, implemented in several dozen villages in rural Uganda, of an intervention aimed at reducing HIV/AIDS prevalence through education and the improved management of other sexually transmitted diseases. The outcome measures for the randomized controlled trial included, in addition to psychometric and behavioral variables assessed through questionnaires, the collection of blood samples to assess HIV prevalence. Mitchell et al. describe the intensive efforts that were needed to dispel rumors and misinformation about the serological survey. One widespread rumor was that the blood was being collected, not for its stated purpose, but for resale abroad. In addition, they note, “There was evidence that enthusiastic supporters of the survey within the community sometimes used the promise of an impending cure to cajole neighbours and peers into providing blood” (p. 1085). With continuing communication efforts as the study progressed, the rumors and misstatements subsided over time. Nevertheless, faced with these substantial problems, the question arises as to whether the blood collection was necessary or advisable for ensuring the validity of the study’s findings. The authors believe that it was: “One might argue that alternative methods of measuring HIV (such as urine or saliva) may have engendered less controversy. The drawback of these alternative methods is that they do not allow for so many tests for other STD’s” (which constituted an important component of their program theory). “Furthermore, earlier pilot studies conducted in the study population found neither urine nor saliva to be any more acceptable than blood” (p. 1088). This example dramatically illustrates the difficulties that can exist in explaining to participants the need for a particular choice of measures.</p>
<p>The utilization of a measurement protocol that entails high participant burden is frequently addressed by offering compensation to participants in the form of cash or other incentives. Compensation can increase participation in evaluation studies and communicate to participants that their time and good will are valued. Federal research regulations provide latitude for IRBs to set their own standards about compensation, but they direct IRBs to insure against “undue inducement” (<xref ref-type="bibr" rid="bibr14-1098214012460565">Dunn &amp; Gordon, 2005</xref>; <xref ref-type="bibr" rid="bibr35-1098214012460565">Permuth-Wey &amp; Borenstein, 2009</xref>; <xref ref-type="bibr" rid="bibr46-1098214012460565">Wertheimer &amp; Miller, 2008</xref>), a term that is generally interpreted to mean a level of compensation so disproportionate that prospective participants might be induced to act against their own best interests. Ultimately, for any individual evaluation study, the decision about whether to compensate participants, and in what amounts, will need to be resolved in consultation with the IRB that is overseeing the project.</p>
</sec>
<sec id="section20-1098214012460565">
<title>Defense of the Measurement Decisions</title>
<p>A final function that may fall to the evaluator is defending the decisions that were made during the evaluation planning. When the study has been completed, there may be skeptics, critics, or simply interested stakeholders who ask why a more rigorous approach was not taken in some area of the study, such as the choice of variable, the timing of measurement, or the technical characteristics of instruments. If the choice can be defended in terms of the balance it provides across critical planning considerations, it can make an important difference in the credibility afforded to the study and the extent to which the study is eventually used. Criticisms regarding disputable levels of rigor can be countered by identifying the significant feasibility challenges that existed. A further strength in this debate would be the opportunity (if true) to report that the measurement choices reflect the input gathered from multiple stakeholders.</p>
</sec>
</sec>
<sec id="section21-1098214012460565">
<title>Summary</title>
<p>In this article, I have addressed two primary issues. First, the level of rigor associated with the plan for measuring an evaluation’s outcome variables needs to be determined during the planning process. Rigor can be explicitly negotiated during the evaluation planning phase between the evaluator, program staff, program funder, and other primary stakeholders. A range of levels of acceptable rigor is possible, assuming that due consideration has been given to different measurement options and their feasibility. Rigor-related decisions should be guided by the information needs of the most important audiences for the evaluation and the study’s planned uses.</p>
<p>Second, the evaluator should provide leadership for a variety of critical functions with regard to planning measurement strategies and choosing or developing particular measurement instruments. The evaluator must have the requisite expertise needed to communicate with a variety of audiences, build consensus, guide the decision-making processes, and defend the evaluation plan against potential critiques.</p>
<p>The measurement specification tasks that I describe in this article—starting with one or more constructs and translating them into one or more measures that form the basis of data collection—are done in virtually every impact evaluation and can scarcely be avoided. However, the evaluation will benefit if this process is accomplished consciously, thoughtfully, with identification of competing options, and with broad stakeholder input. In local, small-scale program settings, these topics might be the springboard for some valuable discussions about the specific ways in which concrete planning decisions can affect the ultimate usefulness of program evaluations.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>I thank Norman Constantine, Michael Hendricks, Roger Rennekamp, Darlene Russ-Eft, Thomas Schwandt, and Jana Kay Slater for their valuable comments on earlier versions of this article. I am also grateful to Katherine Gunter, Lisa Leventhal, and Siew Sun Wong for sharing their expertise with regard to specific sections.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn2-1098214012460565">
<label>Declaration of Conflicting Interests</label>
<p>The author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn3-1098214012460565">
<label>Funding</label>
<p>The author received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<notes>
<title>Note</title>
<fn-group>
<fn fn-type="other" id="fn1-1098214012460565">
<label>1.</label>
<p>The terms <italic>outcome</italic> and <italic>impact</italic> are often used interchangably. Here, however, I distinguish them as follows: Drawing from <xref ref-type="bibr" rid="bibr36-1098214012460565">Rossi, Lipsey, and Freeman (2004)</xref>, I use “outcome” or “outcome variable” to refer to a variable that a program is expected to change, and “impact” to refer to a program effect, that is, “that portion of an outcome change that can be attributed uniquely to a program” (<xref ref-type="bibr" rid="bibr36-1098214012460565">Rossi, Lipsey, &amp; Freeman, 2004</xref>, p. 431). An “impact evaluation” is an evaluation that examines whether a program has produced change in its participants on one or more outcome variables.</p></fn></fn-group></notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aiken</surname>
<given-names>L. S.</given-names>
</name>
<name>
<surname>West</surname>
<given-names>S. G.</given-names>
</name>
</person-group> (<year>1990</year>). <article-title>Invalidity of true experiments: Self-report pretest biases</article-title>. <source>Evaluation Review</source>, <volume>14</volume>, <fpage>374</fpage>–<lpage>390</lpage>.</citation>
</ref>
<ref id="bibr2-1098214012460565">
<citation citation-type="book">
<collab collab-type="author">American Educational Research Association, American Psychological Association, and National Council on Measurement in Education</collab>. (<year>1999</year>). <source>Standards for educational and psychological testing</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr3-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Banta</surname>
<given-names>T. W.</given-names>
</name>
</person-group> (Ed.). (<year>2004</year>). <source>Hallmarks of effective outcomes assessment</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr4-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blalock</surname>
<given-names>H. M.</given-names>
</name>
</person-group> (<year>1979</year>). <article-title>The presidential address: Measurement and conceptualization problems: The major obstacle to integrating theory and research</article-title>. <source>American Sociological Review</source>, <volume>44</volume>, <fpage>881</fpage>–<lpage>894</lpage>.</citation>
</ref>
<ref id="bibr5-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Blalock</surname>
<given-names>H. M.</given-names>
</name>
</person-group> (<year>1982</year>). <source>Conceptualization and measurement in the social sciences</source>. <publisher-loc>Newbury Park, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr6-1098214012460565">
<citation citation-type="other">
<person-group person-group-type="author">
<name>
<surname>Braverman</surname>
<given-names>M. T.</given-names>
</name>
<name>
<surname>Arnold</surname>
<given-names>M. E.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>An evaluator’s balancing act: Making decisions about methodological rigor</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Braverman</surname>
<given-names>M. T.</given-names>
</name>
<name>
<surname>Engle</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Arnold</surname>
<given-names>M. E.</given-names>
</name>
<name>
<surname>Rennekamp</surname>
<given-names>R. A.</given-names>
</name>
</person-group> (Eds.), <source>Program evaluation in a complex organizational system: Lessons from Cooperative Extension. New Directions for Evaluation</source>
<volume>120</volume>, <fpage>71</fpage>–<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr7-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Braverman</surname>
<given-names>M. T.</given-names>
</name>
<name>
<surname>Engle</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Arnold</surname>
<given-names>M. E.</given-names>
</name>
<name>
<surname>Rennekamp</surname>
<given-names>R. A.</given-names>
</name>
</person-group> (Eds.). (<year>2008</year>). <article-title>Program evaluation in a complex organizational system: Lessons from Cooperative Extension</article-title>. <source>New Directions for Evaluation</source>, <volume>120</volume>.</citation>
</ref>
<ref id="bibr8-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chatterji</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Grades of evidence: Variability in quality of findings in effectiveness studies of complex field interventions</article-title>. <source>American Journal of Evaluation</source>, <volume>28</volume>, <fpage>239</fpage>–<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr9-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>H. T.</given-names>
</name>
<name>
<surname>Donaldson</surname>
<given-names>S. I.</given-names>
</name>
<name>
<surname>Mark</surname>
<given-names>M. M.</given-names>
</name>
</person-group> (Eds.). (<year>2011</year>). <article-title>Advancing validity in outcome evaluation: Theory and practice</article-title>. <source>New Directions for Evaluation</source>, <volume>130</volume>.</citation>
</ref>
<ref id="bibr10-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Coryn</surname>
<given-names>C. L. S.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>The holy trinity of methodological rigor: A skeptical view</article-title>. <source>Journal of Multidisciplinary Evaluation</source>, <volume>4</volume>, <fpage>26</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr11-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cronbach</surname>
<given-names>L. J.</given-names>
</name>
</person-group> (<year>1990</year>). <source>Essentials of psychological testing</source> (<edition>5th ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Harper Collins</publisher-name>.</citation>
</ref>
<ref id="bibr12-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Donaldson</surname>
<given-names>S. I.</given-names>
</name>
<name>
<surname>Christie</surname>
<given-names>C. A.</given-names>
</name>
<name>
<surname>Mark</surname>
<given-names>M. M.</given-names>
</name>
</person-group> (Eds.) (<year>2009</year>). <source>What counts as credible evidence in applied research and evaluation practice?</source> <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr13-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Donaldson</surname>
<given-names>S. I.</given-names>
</name>
<name>
<surname>Grant-Vallone</surname>
<given-names>E. J.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Understanding self-report bias in organizational behavior research</article-title>. <source>Journal of Business and Psychology</source>, <volume>17</volume>, <fpage>245</fpage>–<lpage>260</lpage>.</citation>
</ref>
<ref id="bibr14-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dunn</surname>
<given-names>L. B.</given-names>
</name>
<name>
<surname>Gordon</surname>
<given-names>N. E.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Improving informed consent and enhancing recruitment for research by understanding economic behavior</article-title>. <source>Journal of the American Medical Association</source>, <volume>293</volume>, <fpage>609</fpage>–<lpage>612</lpage>.</citation>
</ref>
<ref id="bibr15-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fishbein</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>A reasoned action approach to health promotion</article-title>. <source>Medical Decision Making</source>, <volume>28</volume>, <fpage>834</fpage>–<lpage>844</lpage>.</citation>
</ref>
<ref id="bibr16-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Funnell</surname>
<given-names>S. C.</given-names>
</name>
<name>
<surname>Rogers</surname>
<given-names>P. J.</given-names>
</name>
</person-group> (<year>2011</year>). <source>Purposeful program theory: Effective use of theories of change and logic models</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr17-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Greene</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>The construct(ion) of validity as argument</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Chen</surname>
<given-names>H. T.</given-names>
</name>
<name>
<surname>Donaldson</surname>
<given-names>S. I.</given-names>
</name>
<name>
<surname>Mark</surname>
<given-names>M. M.</given-names>
</name>
</person-group> (Eds.), <source>Advancing validity in outcome evaluation: Theory and practice. New Directions for Evaluation</source> <volume>130</volume>, <fpage>81</fpage>–<lpage>91</lpage>.</citation>
</ref>
<ref id="bibr18-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gunter</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Baxter-Jones</surname>
<given-names>A. D. G.</given-names>
</name>
<name>
<surname>Mirwald</surname>
<given-names>R. L.</given-names>
</name>
<name>
<surname>Almstedt</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Fuller</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Durski</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Snow</surname>
<given-names>C.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Jump starting skeletal health: A 4-year longitudinal study assessing the effects of jumping on skeletal development in pre and circum pubertal children</article-title>. <source>Bone</source>, <volume>42</volume>, <fpage>710</fpage>–<lpage>718</lpage>.</citation>
</ref>
<ref id="bibr19-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gunter</surname>
<given-names>K. B.</given-names>
</name>
<name>
<surname>Kasianchuk</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Examining the influence of participation in a community-based running program on skeletal health in growing girls</article-title>. <source>Osteoporosis International</source>, <volume>22</volume>, <fpage>S438</fpage>.</citation>
</ref>
<ref id="bibr20-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hill</surname>
<given-names>L. G.</given-names>
</name>
<name>
<surname>Betz</surname>
<given-names>D. L.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Revisiting the retrospective pretest</article-title>. <source>American Journal of Evaluation</source>, <volume>26</volume>, <fpage>501</fpage>–<lpage>517</lpage>.</citation>
</ref>
<ref id="bibr21-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>House</surname>
<given-names>E. R.</given-names>
</name>
</person-group> (<year>1980</year>). <source>Evaluating with validity</source>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr22-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hukkanen</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Jacob</surname>
<given-names>III, P.</given-names>
</name>
<name>
<surname>Benowitz</surname>
<given-names>N. L.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Metabolism and disposition kinetics of nicotine</article-title>. <source>Pharmacological Reviews</source>, <volume>57</volume>, <fpage>79</fpage>–<lpage>115</lpage>.</citation>
</ref>
<ref id="bibr23-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Johnson</surname>
<given-names>E. C.</given-names>
</name>
<name>
<surname>Kirkhart</surname>
<given-names>K. E.</given-names>
</name>
<name>
<surname>Madison</surname>
<given-names>A. M.</given-names>
</name>
<name>
<surname>Noley</surname>
<given-names>G. B.</given-names>
</name>
<name>
<surname>Solano-Flores</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>The impact of narrow views of scientific rigor on evaluation practices for underrepresented groups</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Smith</surname>
<given-names>N. L.</given-names>
</name>
<name>
<surname>Brandon</surname>
<given-names>P. R.</given-names>
</name>
</person-group> (Eds.), <source>Fundamental issues in evaluation</source> (pp. <fpage>197</fpage>–<lpage>218</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr24-1098214012460565">
<citation citation-type="book">
<collab collab-type="author">Joint Committee on Standards for Educational Evaluation</collab>. (<year>1994</year>). <source>The program evaluation standards</source> (<edition>2nd ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr25-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Julnes</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Rog</surname>
<given-names>D. J.</given-names>
</name>
</person-group> (Eds.). (<year>2007</year>). <article-title>Informing federal policies on evaluation methodology: Building the evidence base for method choice in government sponsored evaluation</article-title>. <source>New Directions for Evaluation</source>, <volume>113</volume>.</citation>
</ref>
<ref id="bibr26-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kane</surname>
<given-names>M. T.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Validation</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Brennan</surname>
<given-names>R. L.</given-names>
</name>
</person-group> (Ed.), <source>Educational measurement</source> (<edition>4th ed.</edition>, pp. <fpage>17</fpage>–<lpage>64</lpage>). <publisher-loc>Westport, CT</publisher-loc>: <publisher-name>American Council on Education and Praeger</publisher-name>.</citation>
</ref>
<ref id="bibr27-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lam</surname>
<given-names>T. C. M.</given-names>
</name>
<name>
<surname>Bengo</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>A comparison of three retrospective self-reporting methods of measuring change in instructional practice</article-title>. <source>American Journal of Evaluation</source>, <volume>24</volume>, <fpage>65</fpage>–<lpage>80</lpage>.</citation>
</ref>
<ref id="bibr28-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Lissitz</surname>
<given-names>R. W.</given-names>
</name>
</person-group> (Ed.). (<year>2009</year>). <source>The concept of validity: Revisions, new directions, and applications</source>. <publisher-loc>Charlotte, NC</publisher-loc>: <publisher-name>Information Age</publisher-name>.</citation>
</ref>
<ref id="bibr29-1098214012460565">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Lohse</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Wall</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Gromis</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Intention to consume fruits and vegetables is not a proxy for intake in low-income women from Pennsylvania</article-title>. <source>Journal of Extension</source> <comment>[online], <italic>49</italic>, Article 5FEA5. Retrieved from</comment> <ext-link ext-link-type="uri" xlink:href="http://www.joe.org/joe/2011october/a5.php">http://www.joe.org/joe/2011october/a5.php</ext-link>
</citation>
</ref>
<ref id="bibr30-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mitchell</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Nakamanya</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Kamali</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Whitworth</surname>
<given-names>J. A. G.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Balancing rigour and acceptability: The use of HIV incidence to evaluate a community-based randomized trial in rural Uganda</article-title>. <source>Social Science and Medicine</source>, <volume>54</volume>, <fpage>1081</fpage>–<lpage>1091</lpage>.</citation>
</ref>
<ref id="bibr31-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Mosteller</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Boruch</surname>
<given-names>R.</given-names>
</name>
</person-group> (Eds.). (<year>2002</year>). <source>Evidence matters: Randomized trials in education research</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Brookings Institution Press</publisher-name>.</citation>
</ref>
<ref id="bibr32-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nimon</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Zigarmi</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Allen</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Measures of program effectiveness based on retrospective pretest data: Are all created equal?</article-title> <source>American Journal of Evaluation</source>, <volume>32</volume>, <fpage>8</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr33-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Patton</surname>
<given-names>M. Q.</given-names>
</name>
</person-group> (<year>2008</year>). <source>Utilization-focused evaluation</source> (<edition>4th ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr34-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pelfrey</surname>
<given-names>W. V.</given-names>
<suffix>Sr.</suffix>
</name>
<name>
<surname>Pelfrey</surname>
<given-names>W. V.</given-names>
<suffix>Jr.</suffix>
</name>
</person-group> (<year>2009</year>). <article-title>Curriculum evaluation and revision in a nascent field: The utility of the retrospective pretest-posttest model in a homeland security program of study</article-title>. <source>Evaluation Review</source>, <volume>33</volume>, <fpage>54</fpage>–<lpage>82</lpage>.</citation>
</ref>
<ref id="bibr35-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Permuth-Wey</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Borenstein</surname>
<given-names>A. R.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Financial remuneration for clinical and behavioral research participation: Ethical and practical considerations</article-title>. <source>Annals of Epidemiology</source>, <volume>19</volume>, <fpage>280</fpage>–<lpage>285</lpage>.</citation>
</ref>
<ref id="bibr36-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Rossi</surname>
<given-names>P. H.</given-names>
</name>
<name>
<surname>Lipsey</surname>
<given-names>M. W.</given-names>
</name>
<name>
<surname>Freeman</surname>
<given-names>H. E.</given-names>
</name>
</person-group> (<year>2004</year>). <source>Evaluation: A systematic approach</source> (<edition>7th ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr37-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Schwandt</surname>
<given-names>T. A.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Toward a practical theory of evidence for evaluation</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Donaldson</surname>
<given-names>S. I.</given-names>
</name>
<name>
<surname>Christie</surname>
<given-names>C. A.</given-names>
</name>
<name>
<surname>Mark</surname>
<given-names>M. M.</given-names>
</name>
</person-group> (Eds.), <source>What counts as credible evidence in applied research and evaluation practice?</source> (pp. <fpage>197</fpage>–<lpage>212</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr38-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Shadish</surname>
<given-names>W. R.</given-names>
</name>
<name>
<surname>Cook</surname>
<given-names>T. D.</given-names>
</name>
<name>
<surname>Campbell</surname>
<given-names>D. T.</given-names>
</name>
</person-group> (<year>2002</year>). <source>Experimental and quasi-experimental designs for generalized causal inference</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Houghton-Mifflin</publisher-name>.</citation>
</ref>
<ref id="bibr39-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shaikh</surname>
<given-names>A. R.</given-names>
</name>
<name>
<surname>Yaroch</surname>
<given-names>A. L.</given-names>
</name>
<name>
<surname>Nebeling</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Yeh</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Resnicow</surname>
<given-names>K.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Psychosocial predictors of fruit and vegetable consumption in adults: A review of the literature</article-title>. <source>American Journal of Preventive Medicine</source>, <volume>34</volume>, <fpage>535</fpage>–<lpage>543</lpage>.</citation>
</ref>
<ref id="bibr40-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Suskie</surname>
<given-names>L.</given-names>
</name>
</person-group> (<year>2009</year>). <source>Assessing student learning: A common sense guide</source> (<edition>2nd ed.</edition>). <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr41-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sussman</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Stacy</surname>
<given-names>A. W.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Five methods of assessing school-level daily use of cigarettes and alcohol by adolescents at continuation high schools</article-title>. <source>Evaluation Review</source>, <volume>18</volume>, <fpage>741</fpage>–<lpage>755</lpage>.</citation>
</ref>
<ref id="bibr42-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Taylor</surname>
<given-names>P. J.</given-names>
</name>
<name>
<surname>Russ-Eft</surname>
<given-names>D. F.</given-names>
</name>
<name>
<surname>Taylor</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Gilding the outcome by tarnishing the past: Inflationary biases in retrospective pretests</article-title>. <source>American Journal of Evaluation</source>, <volume>30</volume>, <fpage>31</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr43-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wallace</surname>
<given-names>T. L.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>An argument-based approach to validity in evaluation</article-title>. <source>Evaluation</source>, <volume>17</volume>, <fpage>233</fpage>–<lpage>246</lpage>.</citation>
</ref>
<ref id="bibr44-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Webb</surname>
<given-names>T. L.</given-names>
</name>
<name>
<surname>Sheeran</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Does changing behavioral intentions engender behavior change? A meta-analysis of the experimental evidence</article-title>. <source>Psychological Bulletin</source>, <volume>132</volume>, <fpage>249</fpage>–<lpage>268</lpage>.</citation>
</ref>
<ref id="bibr45-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Weiss</surname>
<given-names>C. H.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Which links in which theories shall we evaluate?</article-title> In <person-group person-group-type="editor">
<name>
<surname>Rogers</surname>
<given-names>P. J.</given-names>
</name>
<name>
<surname>Hacsi</surname>
<given-names>T. A.</given-names>
</name>
<name>
<surname>Petrosino</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Huebner</surname>
<given-names>T. A.</given-names>
</name>
</person-group> (Eds.), <source>Program theory in evaluation: Challenges and opportunities. New Directions for Evaluation</source> <volume>87</volume>, <fpage>35</fpage>–<lpage>45</lpage>.</citation>
</ref>
<ref id="bibr46-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wertheimer</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Miller</surname>
<given-names>F. G.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Payment for research participation: A coercive offer?</article-title> <source>Journal of Medical Ethics</source>, <volume>34</volume>, <fpage>389</fpage>–<lpage>392</lpage>.</citation>
</ref>
<ref id="bibr47-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Yarbrough</surname>
<given-names>D. B.</given-names>
</name>
<name>
<surname>Shulha</surname>
<given-names>L. M.</given-names>
</name>
<name>
<surname>Hopson</surname>
<given-names>R. K.</given-names>
</name>
<name>
<surname>Caruthers</surname>
<given-names>F. A.</given-names>
</name>
</person-group> (<year>2011</year>). <source>The program evaluation standards: A guide for evaluators and evaluation users</source> (<edition>3rd ed.</edition>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr48-1098214012460565">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zoellner</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Estabrooks</surname>
<given-names>P. A.</given-names>
</name>
<name>
<surname>Davy</surname>
<given-names>B. M.</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>Y.</given-names>
</name>
<name>
<surname>You</surname>
<given-names>W.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Exploring the theory of planned behavior to explain sugar-sweetened beverage consumption</article-title>. <source>Journal of Nutrition Education and Behavior</source>, <volume>44</volume>, <fpage>172</fpage>–<lpage>177</lpage>.</citation>
</ref>
<ref id="bibr49-1098214012460565">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Zumbo</surname>
<given-names>B. D.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Validity as contextualized and pragmatic explanation, and its implications for validation practice</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Lissitz</surname>
<given-names>R. W.</given-names>
</name>
</person-group> (Ed.), <source>The concept of validity: Revisions, new directions, and applications</source> (pp. <fpage>65</fpage>–<lpage>82</lpage>). <publisher-loc>Charlotte, NC</publisher-loc>: <publisher-name>Information Age</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>