<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342011428146</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342011428146</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Directionally unsplit hydrodynamic schemes with hybrid MPI/OpenMP/GPU parallelization in AMR</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Schive</surname>
<given-names>Hsi-Yu</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342011428146">1</xref>
<xref ref-type="aff" rid="aff2-1094342011428146">2</xref>
<xref ref-type="aff" rid="aff3-1094342011428146">3</xref>
<xref ref-type="corresp" rid="corresp1-1094342011428146"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhang</surname>
<given-names>Ui-Han</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342011428146">1</xref>
<xref ref-type="aff" rid="aff2-1094342011428146">2</xref>
<xref ref-type="aff" rid="aff3-1094342011428146">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chiueh</surname>
<given-names>Tzihong</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342011428146">1</xref>
<xref ref-type="aff" rid="aff2-1094342011428146">2</xref>
<xref ref-type="aff" rid="aff3-1094342011428146">3</xref>
</contrib>
<bio>
<title>Author’s Biographies</title>
<p>
<italic>Hsi-Yu Schive</italic> is a PhD student in the Department of Physics at National Taiwan University. His research focuses on using multiple graphic processing units for large-scale astrophysical simulations, including both particle-based and mesh-based algorithms. He has developed one of the first parallel GPU-accelerated direct <italic>N</italic>-body programs, which was executed successfully in a 32-GPU cluster and achieved a sustained performance of 7.1 TFLOPS. It was 200 times faster than the same computation using only CPUs. As the main topic of his PhD thesis research, he has been working on exploiting the computational power of GPU in adaptive-mesh-refinement (AMR) simulations. He has developed the first GPU-accelerated, astrophysics-dedicated, and parallelized code, named GAMER (GPU-accelerated Adaptive-MEsh-Refinement code), which aims at fully exploiting the computing power of a multi-GPU/CPU system. Two orders of magnitude performance improvement has been demonstrated.</p>
<p>
<italic>Ui-Han Zhang</italic> is a PhD student supervised by Prof. Tzihong Chiueh in the Department of Physics at National Taiwan University.</p>
<p>
<italic>Tzihong Chiueh</italic> is a professor in the Department of Physics and Graduate Institute of Astrophysics at National Taiwan University.</p>
</bio>
</contrib-group>
<aff id="aff1-1094342011428146">
<label>1</label>Department of Physics, National Taiwan University, Taipei, Taiwan</aff>
<aff id="aff2-1094342011428146">
<label>2</label>Center for Theoretical Sciences, National Taiwan University, Taipei, Taiwan</aff>
<aff id="aff3-1094342011428146">
<label>3</label>Leung Center for Cosmology and Particle Astrophysics (LeCosPA), National Taiwan University, 10617, Taipei, Taiwan</aff>
<author-notes>
<corresp id="corresp1-1094342011428146">Hsi-Yu Schive, Center for Theoretical Sciences, National Taiwan University, 10617, Taipei, Taiwan. Email: <email>b88202011@ntu.edu.tw</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>11</month>
<year>2012</year>
</pub-date>
<volume>26</volume>
<issue>4</issue>
<issue-title>Special Issue: Manycore and Accelerator-based High-performance Scientific Computing</issue-title>
<fpage>367</fpage>
<lpage>377</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>We present the implementation and performance of a class of directionally unsplit Riemann-solver-based hydrodynamic schemes on graphics processing units (GPUs). These schemes, including the MUSCL-Hancock method, a variant of the MUSCL-Hancock method, and the corner-transport-upwind method, are embedded into the adaptive-mesh-refinement (AMR) code GAMER. Furthermore, a hybrid MPI/OpenMP model is investigated, which enables the full exploitation of the computing power in a heterogeneous CPU/GPU cluster and significantly improves the overall performance. Performance benchmarks are conducted on the Dirac GPU cluster at NERSC/LBNL using up to 32 Tesla C2050 GPUs. A single GPU achieves speed-ups of 101 (25) and 84 (22) for uniform-mesh and AMR simulations, respectively, as compared with the performance using one (four) CPU core(s), and the excellent performance persists in multi-GPU tests. In addition, we make a direct comparison between GAMER and the widely adopted CPU code Athena in adiabatic hydrodynamic tests and demonstrate that, with the same accuracy, GAMER is able to achieve two orders of magnitude performance speed-up.</p>
</abstract>
<kwd-group>
<kwd>adaptive mesh refinement</kwd>
<kwd>graphics processing unit</kwd>
<kwd>hybrid MPI/OpenMP</kwd>
<kwd>hydrodynamics</kwd>
<kwd>numerical methods</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342011428146">
<title>1 Introduction</title>
<p>Manycore and accelerator-based programming models have become promising techniques in high-performance computing. In particular, novel uses of graphics processing units (GPU) with the parallel computing frameworks such as Compute Unified Device Architecture (CUDA; <xref ref-type="bibr" rid="bibr20-1094342011428146">NVIDIA 2011</xref>) and Open Computing Language (OpenCL; <xref ref-type="bibr" rid="bibr17-1094342011428146">Khronos Group 2011</xref>) have revealed the potential of general-purpose GPU (GPGPU) computing. Moreover, nowadays the developments of GPU applications have moved beyond the single GPU stage, and both performance and parallel efficiency of the applications must be optimized. In astrophysical simulations, considerable performance speed-ups in multi-GPU systems have been demonstrated in a broad range of applications, for example, the direct <italic>N</italic>-body simulations (e.g. <xref ref-type="bibr" rid="bibr24-1094342011428146">Schive et al. 2008</xref>; <xref ref-type="bibr" rid="bibr12-1094342011428146">Gaburov et al. 2009</xref>; <xref ref-type="bibr" rid="bibr28-1094342011428146">Spurzem et al. 2011</xref>), Barnes–Hut tree algorithm (<xref ref-type="bibr" rid="bibr14-1094342011428146">Hamada et al. 2009</xref>), reionization simulations (<xref ref-type="bibr" rid="bibr1-1094342011428146">Aubert and Teyssier 2010</xref>), adaptive mesh refinement (AMR; <xref ref-type="bibr" rid="bibr25-1094342011428146">Schive et al. 2010</xref>; <xref ref-type="bibr" rid="bibr32-1094342011428146">Wang et al. 2010</xref>), general relativistic magnetohydrodynamics (MHD) (<xref ref-type="bibr" rid="bibr34-1094342011428146">Zink 2011</xref>), and interactive visualization of large 3D astronomical data (<xref ref-type="bibr" rid="bibr15-1094342011428146">Hassan et al. 2011</xref>).</p>
<p>The AMR technique plays an indispensable role in modern astrophysical simulations, especially for applications requiring large dynamical ranges. This technique, originally developed by <xref ref-type="bibr" rid="bibr4-1094342011428146">Berger and Oliger (1984)</xref> and <xref ref-type="bibr" rid="bibr3-1094342011428146">Berger and Colella (1989)</xref> for two dimensions and <xref ref-type="bibr" rid="bibr2-1094342011428146">Bell et al. (1994)</xref> for three dimensions, has been extensively implemented in many codes targeting different applications. For example, Enzo (<xref ref-type="bibr" rid="bibr5-1094342011428146">Bryan and Norman 1996</xref>; <xref ref-type="bibr" rid="bibr19-1094342011428146">Norman et al. 2007</xref>) adopts the AMR algorithm of <xref ref-type="bibr" rid="bibr3-1094342011428146">Berger and Colella (1989)</xref> and is designed for simulating cosmological structure formation. The code has also been extended to support numerous physics packages, such as MHD (<xref ref-type="bibr" rid="bibr8-1094342011428146">Collins et al. 2010</xref>) and radiation transfer (<xref ref-type="bibr" rid="bibr33-1094342011428146">Wise and Abel 2011</xref>). FLASH (<xref ref-type="bibr" rid="bibr11-1094342011428146">Fryxell et al. 2000</xref>) adopts a different AMR implementation, in which the AMR hierarchy is built with grid patches and each of which has the same number of cells. These patches form an oct-tree data structure, enabling a very efficient AMR manipulation. In addition to astrophysics-oriented codes, there is also software providing AMR frameworks which can be integrated with existing uniform-mesh codes. Examples include Cactus<sup>
<xref ref-type="fn" rid="fn1-1094342011428146">1</xref>
</sup>, Chombo<sup>
<xref ref-type="fn" rid="fn2-1094342011428146">2</xref>
</sup>, SAMRAI<sup>
<xref ref-type="fn" rid="fn3-1094342011428146">3</xref>
</sup>, and PARAMESH<sup>
<xref ref-type="fn" rid="fn4-1094342011428146">4</xref>
</sup>.</p>
<p>
<xref ref-type="bibr" rid="bibr25-1094342011428146">Schive et al. (2010)</xref> present a parallel GPU-accelerated AMR code named <italic>GAMER</italic> (GPU-accelerated Adaptive-MEsh-Refinement), which is dedicated to high-performance and high-resolution astrophysical simulations. The AMR implementation is based on constructing a hierarchy of grid patches with an oct-tree data structure similar to FLASH, and the relaxing total variation diminishing scheme (RTVD; <xref ref-type="bibr" rid="bibr16-1094342011428146">Jin and Xin 1995</xref>) with directional splitting is adopted as the hydrodynamic solver. A hybrid CPU/GPU model is adopted in the code, in which both hydrodynamic and gravity solvers are implemented into GPU and the AMR data structure is manipulated by CPU. An order of magnitude performance speed-up is demonstrated on the <italic>Laohu</italic> GPU cluster at the High Performance Computing Center at National Astronomical Observatories of China (NAOC), using up to 128 Tesla C1060 GPUs (<xref ref-type="bibr" rid="bibr28-1094342011428146">Spurzem et al. 2011</xref>). The code has been applied to different aspects of astrophysics (<xref ref-type="bibr" rid="bibr27-1094342011428146">Shukla et al. 2011</xref>).</p>
<p>Directionally unsplit algorithms have the advantage of maintaining spatial symmetry and have been extended to MHD simulations, where the divergence-free constraint is preserved (e.g. <xref ref-type="bibr" rid="bibr29-1094342011428146">Stone et al. 2008</xref>). However, due to the requirement of 3D stencils, it was unclear whether these schemes could be implemented into GPU with high performance. Accordingly, in this work, we describe the extensions of the original GAMER code, including the implementation of several directionally unsplit Riemann-solver-based hydrodynamic schemes and the hybrid MPI/OpenMP/GPU parallelization. Benchmarks using up to 32 Tesla C2050 GPUs on the <italic>Dirac</italic> GPU cluster at the National Energy Research Scientific Computing Center at Lawrence Berkeley National Laboratory (NERSC/LBNL) are reported.</p>
<p>The structure of this paper is as follows. In Section 2, we introduce the numerical algorithms of the directionally unsplit hydrodynamic schemes implemented in the code. In Section 3, we describe the GPU implementation and the performance comparison between CPU and GPU hydrodynamic solvers. Several optimizations, especially the hybrid MPI/OpenMP/GPU parallelization, are described in Section 4. In Section 5, we present the overall performance speed-ups in both uniform-mesh simulations and AMR simulations, using up to 32 Fermi GPUs. Finally, we summarize the results in Section 6.</p>
</sec>
<sec id="section2-1094342011428146">
<title>2 Numerical algorithms</title>
<p>In this work, three directionally unsplit hydrodynamic algorithms are implemented into GAMER, including the MUSCL-Hancock method (MHM; see <xref ref-type="bibr" rid="bibr31-1094342011428146">Toro 2009</xref>, for an introduction), a variant of the MUSCL-Hancock method (hereafter referred to as the VL scheme; <xref ref-type="bibr" rid="bibr10-1094342011428146">Falle 1991</xref>), and the corner-transport-upwind scheme (CTU; <xref ref-type="bibr" rid="bibr6-1094342011428146">Colella 1990</xref>). The three-dimensional CTU scheme adopted in the code is a simplified version proposed by <xref ref-type="bibr" rid="bibr13-1094342011428146">Gardiner and Stone (2008)</xref>, which requires only six solutions to the Riemann problem per cell per time step. Also note that the VL and CTU schemes have also been implemented in the widely adopted code <italic>Athena</italic> (<xref ref-type="bibr" rid="bibr29-1094342011428146">Stone et al. 2008</xref>; <xref ref-type="bibr" rid="bibr30-1094342011428146">Stone and Gardiner 2009</xref>), and hence enables a direct comparison between GAMER and Athena in terms of both accuracy and performance.</p>
<p>Since the CTU scheme is more complicated than MHM and VL, in the following we highlight the main procedures in CTU for updating solutions by one time step <inline-formula id="inline-formula1-1094342011428146">
<mml:math id="mml-inline1-1094342011428146">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula>, and emphasize the major differences between CTU and the other two schemes. For a more comprehensive description of the CTU implementation, please see <xref ref-type="bibr" rid="bibr29-1094342011428146">Stone et al. (2008)</xref>.<list list-type="order">
<list-item>
<p>Evaluate the left and right interface values for all cell interfaces in all three spatial directions by the 1D spatial data reconstruction. For CTU, an intermediate step advanced by the characteristics is also included to evaluate the interface values at <inline-formula id="inline-formula2-1094342011428146">
<mml:math id="mml-inline2-1094342011428146">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula>.</p>
</list-item>
<list-item>
<p>Evaluate the fluxes across all cell interfaces by solving the Riemann problem.</p>
</list-item>
<list-item>
<p>Correct the half-step cell interface values obtained in step (1) by computing the transverse flux gradients, and solve the Riemann problem with the corrected data to obtain the new fluxes across all cell interfaces. This step is only necessary in CTU.</p>
</list-item>
<list-item>
<p>Update solutions by <inline-formula id="inline-formula3-1094342011428146">
<mml:math id="mml-inline3-1094342011428146">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> by the conservative integration.</p>
</list-item>
<list-item>
<p>Store the fluxes across the boundaries of all grid patches. This step is only necessary in AMR simulations.</p>
</list-item>
</list>
</p>
<p>The characteristic tracing step and the evaluation of the transverse flux gradients are not required in either MHM or VL. In MHM, the half-step solutions are obtained by first computing the fluxes from the reconstructed interface values in step (1) without Riemann solvers and then advancing the interface values by <inline-formula id="inline-formula4-1094342011428146">
<mml:math id="mml-inline4-1094342011428146">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula> by the flux differences in all three directions. In VL, the half-step solutions are obtained by computing the first-order fluxes with Riemann solver prior to step (1).</p>
<p>For the data reconstruction in step (1), GAMER supports both piecewise linear method (PLM) and piecewise parabolic method (PPM; <xref ref-type="bibr" rid="bibr7-1094342011428146">Colella and Woodware 1984</xref>). In general, the latter is less diffusive and can resolve the density cusps in self-gravity systems with higher spatial resolution. Several slope limiters are implemented in both methods, including the generalized minmod limiter, van Leer-type limiter, and van Albada-type limiter (see <xref ref-type="bibr" rid="bibr31-1094342011428146">Toro 2009</xref>, for an introduction). We also support the hybrid limiter adopted in Athena, which combines the generalized minmod and van Leer-type limiters. The spatial interpolation can be applied to either primitive variables or characteristic variables. Performing interpolation on characteristic variables can reduce the non-physical oscillations in some cases, for example, in shock tube problems. However, in cosmological simulations where low-density voids will always form, this method is found to be less robust and may yield negative density more easily.</p>
<p>The code supports several Riemann solvers, including the exact solver based on <xref ref-type="bibr" rid="bibr31-1094342011428146">Toro (2009)</xref>, HLLE solver (<xref ref-type="bibr" rid="bibr9-1094342011428146">Einfeldt et al. 1991</xref>), HLLC solver (see <xref ref-type="bibr" rid="bibr31-1094342011428146">Toro 2009</xref>, for an introduction), and Roe’s solver (<xref ref-type="bibr" rid="bibr23-1094342011428146">Roe 1981</xref>). The exact solver is much more time-consuming compared with other solvers and is implemented primarily for obtaining reference solutions. The HLLC and Roe’s solvers give comparable accuracy, while the HLLE solver is more diffusive in circumstances requiring the information of contact waves. However, <xref ref-type="bibr" rid="bibr9-1094342011428146">Einfeldt et al. (1991)</xref> showed that the HLLE solver is positively conservative; that is, the density and pressure are positive-definite. He also showed that in certain initial data the linearization approximation adopted in Roe’s solver will fail and lead to negative density or pressure in the intermediate region of the Riemann-problem solution. If this failure is detected during simulations, we follow the same strategy adopted in Athena, in which either HLLE, HLLC, or exact solver is used to calculate the fluxes at the cell interfaces where the Roe’s solver fails.</p>
</sec>
<sec id="section3-1094342011428146">
<title>3 GPU computing</title>
<p>In this section, we describe the strategy to map the directionally unsplit hydrodynamic schemes described in Section 2 into the CUDA parallel computing architecture. We also present the performance comparison between CPU and GPU solvers.</p>
<sec id="section4-1094342011428146">
<title>3.1 CUDA implementation</title>
<p>In GAMER, the fluid data are always decomposed into grid patches, each of which consists of <inline-formula id="inline-formula5-1094342011428146">
<mml:math id="mml-inline5-1094342011428146">
<mml:msup>
<mml:mn>8</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> cells. In addition, due to the oct-tree data structure, we can always group the eight nearby patches into a single <italic>patch group</italic> (which contains <inline-formula id="inline-formula6-1094342011428146">
<mml:math id="mml-inline6-1094342011428146">
<mml:msup>
<mml:mn>16</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> cells) before updating data, so that the additional workload to prepare the ghost-cell data can be reduced in comparison with calculating different patches individually. In CUDA implementation, each patch group will be computed by one thread block. We do not require to store all simulation data in the GPU global memory. Instead, we only need to ensure that the workload in each GPU is high enough to fully exploit its computing power. Accordingly, the number of patch groups sent into GPU at a time depends on hardware specifications rather than simulation scale, and the maximum simulation scale will not be limited by the relatively small memory in GPUs (4 GB in Tesla C1060 and 3 GB in Tesla C2050). Furthermore, since all simulation data still reside in the CPU memory, it reveals the potential of combining GPU computing with the out-of-core technique, by which the large storage space of multiple hard disks can be utilized as the additional virtual memory to significantly increase the total amount of memory available (<xref ref-type="bibr" rid="bibr26-1094342011428146">Schive et al. 2011</xref>).</p>
<p>The number of threads per thread block is a free parameter and the optimal value is also hardware-dependent. Typically, we use 128 threads per thread block in Tesla C1060 and 512 threads per thread block in Tesla C2050, in which cases each thread will compute the solutions of multiple cells. For a typical 3D loop in C language, we have<disp-quote>
<p>
<bold>for (unsigned int</bold> k=k_start; k&lt;k_start+k_size; k++) {</p>
<p>
<bold>for (unsigned int</bold> j=j_start; j&lt;j_start+j_size; j++) {</p>
<p>
<bold>for (unsigned int</bold> i=i_start; i&lt;i_start+i_size; i++) {</p>
<p>Array3D[k][j][i] = ...;</p>
<p>}}}</p>
<p>which is converted to the following GPU kernel: </p>
<p>
<bold>unsigned int</bold> count = threadIdx.x;</p>
<p>
<bold>unsigned int</bold> blocksize = blockDim.x;</p>
<p>
<bold>unsigned int</bold> i, j, k, index;</p>
<p>
<bold>while</bold> (count&lt;i_size*j_size*k_size) {</p>
<p>i = i_start + count%i_size;</p>
<p>j = j_start + count%(i_size* j_size)/i_size;</p>
<p>k = k_start + count/(i_size*j_size);</p>
<p>index = (k*NY + j)*NX + i;</p>
<p>Array1D[index] = ...;</p>
<p>count += blocksize;</p>
<p>}</p>
</disp-quote>
</p>
<p>Note that additional workload, mainly associated with the expensive integer division and modulo operations, is introduced into the GPU kernel. However, it will not have a significant adverse performance impact as long as the number of arithmetic operations performed on the 1D array is large enough. Furthermore, since this kind of conversion poses no constraints on the number of threads per thread block, it makes the code much more flexible and also makes the performance tuning for different generations of GPUs more easily.</p>
<p>In GPU computing, one of the most important keys to achieving high performance is to efficiently utilize the small shared memory (16 kB per multiprocessor in Tesla C1060 and 48 kB per multiprocessor in Tesla C2050) in order to hide the latency of accessing data from the global memory. For the directionally split schemes, taking advantage of the shared memory is more straightforward and efficient since the integration only requires 1D stencils and hence the data can be decomposed into many small 1D segments to fit into the shared memory. Moreover, the number of ghost cells in both ends of the 1D data segment can be much smaller than the total number of cells in one segment, and therefore the computational overhead resulting from this 1D data decomposition is small. The shared memory has been fully utilized in the directionally split hydrodynamic schemes and the GPU Poisson solver in GAMER (<xref ref-type="bibr" rid="bibr25-1094342011428146">Schive et al. 2010</xref>).</p>
<p>In comparison, for the directionally unsplit hydrodynamic schemes described in Section 2, some calculations are essentially 3D operations which require 3D stencils (e.g. the steps (3) and (4) in CTU), while some are still 1D operations (e.g. the step (1) in CTU). For MHM, the half-step prediction also requires a 3D stencil since the solutions are updated by taking account of the flux differences in all three directions in a single step. Utilizing the shared memory in these 3D operations is more tricky, and the most straightforward solution is to decompose the data into small 3D tiles to fit into the shared memory. However, it is arguable whether this method can significantly improve the overall performance, especially when we consider the relatively high surface/volume ratio in each small 3D tile and the high arithmetic intensity in the Riemann-solver-based schemes. In some preliminary tests we find that no substantial performance improvement is obtained by using the shared memory. Therefore, in the current implementation, we only use the global memory for the directionally unsplit hydrodynamic schemes. Despite that, considerable performance speed-up is still achieved and will be detailed in the next section. This approach makes the code more flexible to be run in GPUs with smaller shared memory (e.g. Tesla C1060), and it also makes it relatively straightforward to convert CPU hydrodynamic solvers to GPU solvers. Implementation of the shared memory in the directionally unsplit schemes will be investigated in the future. Also note that temporary global memory arrays are allocated to store the intermediate results during the integration (e.g. the fluxes and the left and right interface values). These arrays do not need to be transferred between host (CPU) memory and device (GPU) memory.</p>
<p>The integration procedure is carefully organized in order to minimize the performance impact of the high latency and relatively low bandwidth of the global memory access. For example, in the half-step prediction in MHM, the six interface values of one cell are all evaluated <italic>before</italic> the thread proceeds to the next cell, so that the fluxes can be stored in the registers and the interface values can be updated without reloading data from the global memory. Another example is that the fluxes across the patch boundaries are stored immediately after solving the Riemann problem at these boundaries. By doing so, we do not need to perform additional memory copies in the global memory after all fluxes are evaluated, despite the fact that the latter method is more straightforward to implement.</p>
<p>In Fermi GPUs, the following tips are found to be able to fine-tune the performance. The compilation flag <italic>-Xptxas -dlcm=ca</italic> is applied to enable both L1 and L2 caches. The flags <italic>-prec-div=false -ftz=true</italic> are also used since they are found to improve the performance by 17% without sacrificing the overall accuracy in most cases. During the initialization of CUDA devices, the cache configurations of the kernels of the directionally unsplit hydrodynamic solvers are set to <italic>cudaFuncCachePreferL1</italic> via the function <italic>cudaFuncSetCacheConfig</italic> in order to have a larger L1 cache. In a few cases, we find that the _ <italic>forceinline</italic> function qualifier can enhance the performance, especially for computationally expensive functions (e.g. the Roe’s Riemann solver). In addition, it is observed that using structures instead of arrays to store the temporary five-element fluid data delivers higher performance and also reduces the local memory usage in pre-Fermi GPUs.</p>
<p>Finally, we note that many key components in different integration schemes are identical, for example, the data reconstruction, Riemann solver, full-step update, and storing fluxes. Therefore, in GAMER these functions are developed with flexible arguments so that they can be efficiently shared among different schemes. It makes the code more maintainable and extensible.</p>
</sec>
<sec id="section5-1094342011428146">
<title>3.2 GPU performance</title>
<p>All of the tests in this work are performed on Tesla C2050 GPUs and Intel Xeon E5530 CPUs, except for the tests aiming to compare the performances between different hardware configurations. The CPU codes are compiled with <italic>gcc v4.4.2</italic> and the GPU codes are compiled with <italic>nvcc v3.2</italic>. The <italic>-O3</italic> optimization flag is adopted in both CPU and GPU tests. Single-precision variables are adopted in all performance experiments. In addition, in order to have a fair comparison, the timing measurements of GPU always include the time to copy data between host and device memory.</p>
<p>
<xref ref-type="fig" rid="fig1-1094342011428146">Figure 1</xref> shows the performance comparisons between one GPU and one CPU core as a function of the number of cells sent into GPU at a time. The top left panel shows the speed-ups of different hydrodynamic schemes. For comparison, we also include the results of two directionally split schemes: the RTVD scheme and the weighted average flux scheme (WAF; see <xref ref-type="bibr" rid="bibr31-1094342011428146">Toro 2009</xref>, for an introduction). Note that in the directionally split schemes we utilize the fast shared memory, while in the unsplit schemes we only use the global memory. Nevertheless, the latter still achieve substantially higher performance speed-ups due to their higher arithmetic intensity. Maximum speed-ups of 114, 122, and 111 are demonstrated in MHM, VL, and CTU, respectively. Also note that both VL and CTU require six Riemann solvers per cell per time step, while MHM only requires three. Therefore, the MHM scheme is measured to be about 1.5 times faster than the other two schemes.</p>
<fig id="fig1-1094342011428146" position="float">
<label>Figure 1.</label>
<caption>
<p>Performance of the GPU hydrodynamic solvers. The top left panel shows the performances of different hydrodynamic schemes, including both the directionally unsplit schemes (MHM, VL, and CTU) with the PPM data reconstruction and Roe’s Riemann solver and the split schemes (RTVD and WAF). The top right panel shows the performances of the CTU scheme with different data reconstruction methods (PLM and PPM) and different Riemann solvers (HLLE, HLLC, and Roe’s solvers). The bottom left panel shows the performances of the CTU scheme with different numbers of CUDA streams. The bottom right panel shows the performances of the CTU scheme in different GPU systems (see the text for details). The PPM data reconstruction and Roe’s Riemann solver are adopted in the tests shown in the lower panels.</p>
</caption>
<graphic xlink:href="10.1177_1094342011428146-fig1.tif"/>
</fig>
<p>The <italic>CUDA stream</italic> can be used to overlap the memory copy between host and device memory with the kernel execution, and, in general, using more CUDA streams can improve the efficiency of overlapping and hence deliver higher performance. The performances of the CTU scheme with different numbers of CUDA streams are given in the bottom left panel in <xref ref-type="fig" rid="fig1-1094342011428146">Figure 1</xref>. The performance is improved by 26% by using four CUDA streams as compared with the result using only one CUDA stream. It is also found that using more than four CUDA streams does not further improve the overall performance. Therefore, throughout the tests in this work, four CUDA streams are always adopted unless the number is explicitly specified. The method to implement the CUDA stream in GAMER is described in Section 4.1.</p>
<p>The top right panel in <xref ref-type="fig" rid="fig1-1094342011428146">Figure 1</xref> compares the performances of the CTU scheme with different data reconstruction methods (PLM and PPM) and different Riemann solvers (HLLE, HLLC, and Roe’s solvers). The highest speed-ups achieved in different methods range between 96 and 111, which do not differ significantly. In general, the PPM data reconstruction gives higher speed-ups than the PLM data reconstruction, and the Roe’s solver delivers higher speed-ups than the HLLE and HLLC solvers. In addition, we find that the optimal performance is achieved when we have one to two patch groups per multiprocessor, in which case the CTU scheme with the PPM data reconstruction only requires about 110–220 MB memory in Tesla C2050. Having too many patch groups per multiprocessor can deteriorate the performance.</p>
<p>In order to have more comprehensive performance measurements, we also compare the timing results conducted in three different GPU systems: the <italic>Dirac</italic> GPU cluster at NERSC/LBNL, the <italic>Laohu</italic> GPU cluster at NAOC, and the GPU cluster at the National Center for High-performance Computing of Taiwan (hereafter referred to as <italic>NCHC</italic>). The Dirac system contains 48 nodes connected by QDR InfiniBand. Each node is equipped with two Intel Xeon E5530 CPUs and one NVIDIA Tesla C2050 GPU. The Laohu system has 85 nodes, each with two Intel Xeon E5520 CPUs and two NVIDIA Tesla C1060 GPUs. The experimental GPU system in NCHC has two Intel Xeon X5670 CPUs and four NVIDIA Tesla M2070 GPUs in each node. The measured speed-ups in different systems are presented in the bottom right panel in <xref ref-type="fig" rid="fig1-1094342011428146">Figure 1</xref>. The maximum number of cell updates per second achieved by one Fermi GPU is <inline-formula id="inline-formula7-1094342011428146">
<mml:math id="mml-inline7-1094342011428146">
<mml:mn>3.0</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:msup>
<mml:mn>10</mml:mn>
<mml:mn>7</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>, which is about 1.72 times faster than the performance achieved by one Tesla C1060 GPU.</p>
<p>Note that the timing experiments presented above only measure the performance of the hydrodynamic solver. They do not include the elapsed time of all other operations in AMR simulations. Therefore, the values in <xref ref-type="fig" rid="fig1-1094342011428146">Figure 1</xref> can be considered as the optimal overall speed-ups we can achieve in any AMR simulation using GAMER. Also note that so far the performance comparisons are all based on one GPU versus one CPU core. The overall performance speed-up with multiple GPUs and CPU cores is presented in Section 5.</p>
</sec>
</sec>
<sec id="section6-1094342011428146">
<title>4 Optimization</title>
<p>In this section, we describe several performance optimizations applied <italic>outside</italic> the GPU kernel. Since GAMER adopts a hybrid CPU/GPU implementation, these optimizations have been demonstrated to be extremely important for achieving higher overall performance.</p>
<sec id="section7-1094342011428146">
<title>4.1 Asynchronous memory copy</title>
<p>As already mentioned in the previous section, the memory copy between host and device memory can be performed concurrently with the kernel execution by managing the CUDA stream. Specifically, the kernel launch and memory copy associated with different CUDA streams can be performed in parallel, while the operations associated with the same CUDA stream will be performed sequentially. The following code shows an example using <inline-formula id="inline-formula8-1094342011428146">
<mml:math id="mml-inline8-1094342011428146">
<mml:mi>n</mml:mi>
<mml:mi>s</mml:mi>
</mml:math>
</inline-formula> CUDA streams and assigning <inline-formula id="inline-formula9-1094342011428146">
<mml:math id="mml-inline9-1094342011428146">
<mml:mi>n</mml:mi>
<mml:mi>p</mml:mi>
</mml:math>
</inline-formula> patch groups to each stream.<disp-quote>
<p>// input array format: float HostArray_In[ns*np] [SizePerPatchGroup_In] // output array format: float HostArray_Out[ns*np] [SizePerPatchGroup_Out]</p>
<p>
<bold>const unsigned int</bold> MemSize_In = np*SizePerPatchGroup_In *<bold>sizeof(float)</bold>;</p>
<p>
<bold>const unsigned int</bold> MemSize_Out = np*SizePerPatchGroup_Out*<bold>sizeof(float)</bold>;</p>
<p>
<bold>for</bold> (<bold>unsigned int</bold> s=0; s&lt;ns; s++) {</p>
<p>cudaMemcpyAsync(DeviceArray_In + s*np, HostArray_In + s*np, MemSize_In,</p>
<p>cudaMemcpyHostToDevice, Stream[s]);</p>
<p>Kernel &lt;&lt;&lt; np, 512, 0, Stream[s] &gt;&gt;&gt; (DeviceArray_In + s*np, DeviceArray_Out + s*np, ...);</p>
<p>cudaMemcpyAsync(HostArray_Out + s*np, DeviceArray_Out + s*np, MemSize_Out,</p>
<p>cudaMemcpyDeviceToHost, Stream[s]);</p>
<p>}</p>
</disp-quote>
</p>
</sec>
<sec id="section8-1094342011428146">
<title>4.2 Concurrent execution between CPU and GPU</title>
<p>In GAMER, before invoking the GPU solver, we need to prepare the input array in CPU, which stores the interior and ghost-cell data of the patch groups to be calculated. This step is referred to as the <italic>preparation step</italic>. In addition, after receiving the updated data from the GPU, we need to copy the data of different patches to their corresponding arrays in CPU. This step is referred to as the <italic>closing step</italic>. Experiments show that the preparation step can be very expensive, especially in AMR simulations where spatial and temporal interpolations are required to obtain the ghost-cell data for patches adjacent to the coarse–fine boundaries. This step is the performance bottleneck in the previous version of GAMER and can take up to three times longer than the GPU computation.</p>
<p>To alleviate this issue, we note that in principle the preparation and closing steps and the GPU solver can be performed concurrently, provided that they are targeting different patches. Therefore, by taking advantage of the fact that the GPU solver is an asynchronous function, we can overlap the executions of the preparation and closing steps in CPU with the GPU computation. The implementation and efficiency of this optimization are described in more detail in <xref ref-type="bibr" rid="bibr25-1094342011428146">Schive et al. (2010)</xref>.</p>
</sec>
<sec id="section9-1094342011428146">
<title>4.3 Hybrid MPI/OpenMP/GPU parallelization</title>
<p>The optimization of making CPU and GPU work in parallel greatly improves the overall performance, especially in the cases where the computation times of CPU and GPU are comparable (e.g. when relatively low-end GPUs are adopted). However, on both Dirac and Laohu systems, this method does not completely eliminate the performance bottleneck described above due to the fact that the preparation step in CPU can take significantly longer than the GPU hydrodynamic solver. As a result, the overall performance speed-up is still much lower than the optimal value given in Section 3.2. To solve this issue, in this work we further implement the OpenMP parallelization in CPU computation.</p>
<p>In most GPU applications, one CPU core can take charge of one GPU, and the multi-GPU parallelization is achieved using MPI (e.g. <xref ref-type="bibr" rid="bibr24-1094342011428146">Schive et al. 2008</xref>, <xref ref-type="bibr" rid="bibr25-1094342011428146">2010</xref>). Accordingly, in the GPU clusters with more CPU cores than GPUs, some CPU cores will be idle during simulations. It is not an issue for the kind of GPU applications where the overall speed-up is dominated by the GPU performance. However, for complicated programs such as GAMER, the overall performance is determined by the performance of the hybrid CPU/GPU computing, and therefore it is crucial to fully exploit the computing power of both the multi-core CPUs and GPUs. To this end, we have implemented the hybrid MPI/OpenMP/GPU parallelization in GAMER. Each MPI process is responsible for one GPU, and the CPU computation allocated to each MPI process is further parallelized with OpenMP. For example, in a GPU cluster with <inline-formula id="inline-formula10-1094342011428146">
<mml:math id="mml-inline10-1094342011428146">
<mml:msub>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">n</mml:mi>
<mml:mi mathvariant="normal">o</mml:mi>
<mml:mi mathvariant="normal">d</mml:mi>
<mml:mi mathvariant="normal">e</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> nodes and each node is equipped with <inline-formula id="inline-formula11-1094342011428146">
<mml:math id="mml-inline11-1094342011428146">
<mml:msub>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">c</mml:mi>
<mml:mi mathvariant="normal">o</mml:mi>
<mml:mi mathvariant="normal">r</mml:mi>
<mml:mi mathvariant="normal">e</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> CPU cores and <inline-formula id="inline-formula12-1094342011428146">
<mml:math id="mml-inline12-1094342011428146">
<mml:msub>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">G</mml:mi>
<mml:mi mathvariant="normal">P</mml:mi>
<mml:mi mathvariant="normal">U</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> GPUs, we can run the simulation with <inline-formula id="inline-formula13-1094342011428146">
<mml:math id="mml-inline13-1094342011428146">
<mml:msub>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">n</mml:mi>
<mml:mi mathvariant="normal">o</mml:mi>
<mml:mi mathvariant="normal">d</mml:mi>
<mml:mi mathvariant="normal">e</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">×</mml:mo>
<mml:msub>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">G</mml:mi>
<mml:mi mathvariant="normal">P</mml:mi>
<mml:mi mathvariant="normal">U</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> MPI processes and launch <inline-formula id="inline-formula14-1094342011428146">
<mml:math id="mml-inline14-1094342011428146">
<mml:msub>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">c</mml:mi>
<mml:mi mathvariant="normal">o</mml:mi>
<mml:mi mathvariant="normal">r</mml:mi>
<mml:mi mathvariant="normal">e</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mo>/</mml:mo>
</mml:mrow>
<mml:msub>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">G</mml:mi>
<mml:mi mathvariant="normal">P</mml:mi>
<mml:mi mathvariant="normal">U</mml:mi>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:msub>
</mml:math>
</inline-formula> OpenMP threads in each process.</p>
<p>In GAMER, the AMR implementation is realized by constructing a hierarchy of grid patches, in which different patches at the same level can be evaluated in parallel. Accordingly, the MPI implementation is achieved by domain decomposition, and the OpenMP implementation is based on the patch-level parallelization. OpenMP is applied to most CPU computations, including both the preparation and closing steps, correcting the coarse-grid data, calculating the evolution time step, and the grid refinement. Since different patches contain the same number of cells, the computation workload associated with each patch is approximately the same. Therefore, in most cases, OpenMP can be implemented straightforwardly and high parallel efficiency can be achieved.</p>
</sec>
</sec>
<sec id="section10-1094342011428146">
<title>5 Overall performance</title>
<p>In this section, we present the overall performance comparisons between CPUs and GPUs in both uniform-mesh simulations and AMR simulations. In each case, we compare the achieved speed-ups with different optimization levels described in Section 4, and show both single-GPU and multi-GPU performances. The CTU scheme with the PPM data reconstruction and Roe’s Riemann solver is adopted in all tests presented in this section. Performance is measured on the Dirac system.</p>
<sec id="section11-1094342011428146">
<title>5.1 Uniform-mesh performance</title>
<p>
<xref ref-type="fig" rid="fig2-1094342011428146">Figure 2</xref> shows the overall performance speed-up in the uniform-mesh 3D blast wave test as a function of spatial resolution (<inline-formula id="inline-formula15-1094342011428146">
<mml:math id="mml-inline15-1094342011428146">
<mml:mi>N</mml:mi>
</mml:math>
</inline-formula>). First, we verify that the performance of GAMER without GPU acceleration and OpenMP is as fast as Athena. The performance difference is less than 6% when <inline-formula id="inline-formula16-1094342011428146">
<mml:math id="mml-inline16-1094342011428146">
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">≥</mml:mo>
<mml:msup>
<mml:mn>32</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>. This result is very important as it makes the performance comparison between CPU and GPU in GAMER more convincing. The CPU-only performance scales linearly with the number of OpenMP threads when there are at least one patch groups per thread. For the performance with GPU acceleration, results with different optimization levels are shown together for comparison. The unoptimized code still achieves a speed-up of 35, and factors of 39, 63, and 101 speed-ups are further demonstrated when different optimizations described in Section 4 are implemented successively. This is a very encouraging result since the speed-up achieved by the fully optimized code closely approaches the optimal value given in Section 3.2. The maximum number of cell updates per second is <inline-formula id="inline-formula17-1094342011428146">
<mml:math id="mml-inline17-1094342011428146">
<mml:mn>2.9</mml:mn>

<mml:mo stretchy="false">×</mml:mo>

<mml:msup>
<mml:mn>10</mml:mn>
<mml:mn>7</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>. It corresponds to 206 GFLOP s<inline-formula id="inline-formula18-1094342011428146">
<mml:math id="mml-inline18-1094342011428146">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, which is 40% of the peak single-precision performance of C2050 (without considering the multiply–add operations). The global memory overall throughput is 107 GB s<inline-formula id="inline-formula19-1094342011428146">
<mml:math id="mml-inline19-1094342011428146">
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, achieving 74% of the peak bandwidth, and the L1 cache hit rate is 64%. The GPU performance is measured to be 25 times faster than the CPU-only performance using four cores. Note that since no shared memory is adopted for the hydrodynamic solver, further optimization of the cache hit rate is extremely important and requires further investigation. We also compare the accuracies of physical results obtained separately by GAMER and Athena, and the relative differences are on the order of the machine precision in both single-precision and double-precision experiments.</p>
<fig id="fig2-1094342011428146" position="float">
<label>Figure 2.</label>
<caption>
<p>Overall performance speed-up in single-GPU uniform-mesh simulations. The filled diamonds and thick dashed line show the CPU-only results with and without OpenMP, respectively. The unoptimized GPU performance is shown by the open squares. The open circles, triangles, and inverted triangles show the GPU performances, in which different optimizations are implemented successively. The abbreviation ‘async’ represents the optimization of the concurrent execution between CPU and GPU. Four threads are used when OpenMP is enabled. We also show the results obtained by Athena (open diamonds) for comparison. In all data points, the performance of GAMER using a single CPU core is regarded as the reference performance.</p>
</caption>
<graphic xlink:href="10.1177_1094342011428146-fig2.tif"/>
</fig>
<p>We also measure the overall performances using HLLE and HLLC solvers, and both of them achieve <inline-formula id="inline-formula20-1094342011428146">
<mml:math id="mml-inline20-1094342011428146">
<mml:mn>3.1</mml:mn>

<mml:mo stretchy="false">×</mml:mo>

<mml:msup>
<mml:mn>10</mml:mn>
<mml:mn>7</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> cell updates per second. The speed-up between one GPU and one CPU core is about 88, which is slightly lower than that achieved by Roe’s solver. This result is consistent with the top right panel in <xref ref-type="fig" rid="fig1-1094342011428146">Figure 1</xref>.</p>
<p>Another important feature in GAMER is that it is very memory efficient as compared with Athena; this is expected due to the patch-based decomposition employed in the code. For the directionally unsplit hydrodynamic schemes described in Section 2, usually we need to store the left and right interface values and fluxes at <italic>all</italic> cell interfaces, which can be very memory consuming. However, in GAMER we only need to store these data for the <italic>patches being computed in parallel</italic>, the number of which is generally much smaller than the total number of patches. Experiments show that for the adiabatic hydrodynamic simulations with <inline-formula id="inline-formula21-1094342011428146">
<mml:math id="mml-inline21-1094342011428146">
<mml:mi>N</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:msup>
<mml:mn>400</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>, Athena consumes about 13 GB of memory, while GAMER only consumes roughly 3 GB of memory.</p>
<p>
<xref ref-type="fig" rid="fig3-1094342011428146">Figure 3</xref> shows the parallel scalability of GAMER in uniform-mesh simulations. The result is excellent for weak scaling, in which we let each GPU compute <inline-formula id="inline-formula22-1094342011428146">
<mml:math id="mml-inline22-1094342011428146">
<mml:msup>
<mml:mn>512</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> cells. The 32-GPU run is 31.6 times faster than the single-GPU performance, which corresponds to a parallel efficiency of 98.8%. Strong scaling is much more challenging. By fixing the simulation resolution to 1024 × 1024 × 512, the 32-GPU run still achieves a factor of 27.2 speed-up over the single-GPU performance, giving a parallel efficiency of 85.0%.</p>
<fig id="fig3-1094342011428146" position="float">
<label>Figure 3.</label>
<caption>
<p>Parallel scalability in uniform-mesh simulations. The open circles and triangles show the weak scaling and strong scaling, respectively. The ideal scaling is also shown for comparison (thick dashed line).</p>
</caption>
<graphic xlink:href="10.1177_1094342011428146-fig3.tif"/>
</fig>
</sec>
<sec id="section12-1094342011428146">
<title>5.2 AMR performance</title>
<p>To demonstrate the performance in the AMR simulations with sufficiently complicated grid structure, we follow the similar timing experiments performed in <xref ref-type="bibr" rid="bibr25-1094342011428146">Schive et al. (2010)</xref> and measure the performance in purely baryonic cosmological simulations. The root-level resolution is <inline-formula id="inline-formula23-1094342011428146">
<mml:math id="mml-inline23-1094342011428146">
<mml:msup>
<mml:mn>256</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> and up to four refinement levels are used, giving <inline-formula id="inline-formula24-1094342011428146">
<mml:math id="mml-inline24-1094342011428146">
<mml:msup>
<mml:mn>4096</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> effective resolution. Note that the self-gravity is included here, and a GPU Poisson solver based on the successive overrelaxation method (SOR; see <xref ref-type="bibr" rid="bibr21-1094342011428146">Press et al. 2007</xref>, for an introduction) is adopted for the refinement levels. The GPU SOR solver alone is measured to be 84 times faster than the CPU counterpart on the Dirac system.</p>
<p>
<xref ref-type="fig" rid="fig4-1094342011428146">Figure 4</xref> shows the overall performance speed-up at redshift <inline-formula id="inline-formula25-1094342011428146">
<mml:math id="mml-inline25-1094342011428146">
<mml:mi>z</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>2</mml:mn>
</mml:math>
</inline-formula> using a single GPU. The fully optimized GPU code demonstrates speed-ups of 84.0 and 21.6 as compared with the CPU-only single-core and quad-core performances, respectively. The maximum speed-up is slightly lower than the value obtained in the uniform-mesh simulations, which is mainly due to the relatively lower speed-up ratio achieved by the GPU Poisson solver. We emphasize that the fully optimized performance (inverted triangles) is 2.3 times faster than the partially optimized performance without OpenMP (triangles), which demonstrates the importance of adopting the hybrid MPI/OpenMP parallelization. We also find that using more than four OpenMP threads does not further improve the overall performance, which is expected since the overall performance will be limited by the GPU performance when more than four OpenMP threads are used and the concurrent execution between CPU and GPU is enabled. This fact reveals the plausibility of installing more than one high-end GPUs in each multi-core computing node, such as the hardware configuration adopted on the Laohu system.</p>
<fig id="fig4-1094342011428146" position="float">
<label>Figure 4.</label>
<caption>
<p>Overall performance speed-up in single-GPU AMR simulations. Self-gravity is included in these tests. The root-level resolution is <inline-formula id="inline-formula29-1094342011428146">
<mml:math id="mml-inline29-1094342011428146">
<mml:msup>
<mml:mn>256</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> and up to four refinement levels are used. Four threads are adopted when OpenMP is enabled. In all data points, the performance using a single CPU core is regarded as the reference performance.</p>
</caption>
<graphic xlink:href="10.1177_1094342011428146-fig4.tif"/>
</fig>
<p>
<xref ref-type="fig" rid="fig5-1094342011428146">Figure 5</xref> shows the overall performance speed-up using multiple GPUs as a function of the number of MPI ranks. For example, the 8-GPU performance is compared with the CPU-only performance using eight cores, each of which resides on a different computing node. In the 32-GPU test, the maximum speed-ups are 71.4 and 18.3 as compared with the CPU-only single-core and quad-core performances, respectively. The performance decrement results from the increasing MPI communication time, which takes about 11% of the total execution time in the 32-GPU test. This issue was found to be of minor importance in the simulations with higher spatial resolution (and, hence, lower surface/volume ratio). For example, for the 128-GPU benchmark on the Laohu GPU cluster at NAOC, the MPI communication time takes less than 2% of the total execution time (<xref ref-type="bibr" rid="bibr28-1094342011428146">Spurzem et al. 2011</xref>). Also note that this issue can potentially be largely alleviated by overlapping communication with computation.</p>
<fig id="fig5-1094342011428146" position="float">
<label>Figure 5.</label>
<caption>
<p>Overall performance speed-up in multi-GPU AMR simulations as a function of the number of MPI ranks. The GPU performances with different optimization levels are compared to the CPU performance without OpenMP. For the CPU-only tests, the total number CPU cores is equal to the number of MPI ranks times the number of OpenMP threads per rank. For the GPU tests, the total number of GPUs is equal to the number of MPI ranks. Four threads per MPI rank are adopted when OpenMP is enabled. The quad-core CPU performance (filled diamonds) and the ideal speed-up (thick dashed line) are also shown for comparison.</p>
</caption>
<graphic xlink:href="10.1177_1094342011428146-fig5.tif"/>
</fig>
<p>In <xref ref-type="fig" rid="fig5-1094342011428146">Figure 5</xref> we adopt the rectangular domain decomposition, which can result in the issue of load imbalance among different MPI ranks, especially when the spatial distribution of refined grids is highly inhomogeneous. To alleviate this issue, we also implement the Hilbert space-filling curve to distribute grid patches evenly to all ranks. Furthermore, in this approach we do not assume that the parent and child patches reside in the same rank, so that load balance can be achieved <italic>at each refinement level</italic>. This feature is specially important for the individual time-step integration, in which the calculations at different levels must be performed <italic>sequentially</italic>. Preliminary results show that for strong scaling the 32-GPU run achieves a parallel efficiency of 67% and a speed-up of 68 as compared with the performance using 32 CPU cores. Also note that in the 32-GPU run MPI takes only 17% of the total execution time, suggesting that even for strong scaling the code will be able to scale well to even larger number of GPUs. Details of this work will be reported in a future communication.</p>
<p>Finally, we emphasize that the SOR method, on the one hand, is more GPU-friendly due to its simplicity and higher arithmetic intensity, but on the other hand, has lower convergence rate than, e.g., fast Fourier transform and multigrid methods. For comparison, we also implement both CPU and GPU multigrid solvers in GAMER. For larger grids (<inline-formula id="inline-formula26-1094342011428146">
<mml:math id="mml-inline26-1094342011428146">
<mml:mo stretchy="false">≥</mml:mo>
<mml:msup>
<mml:mn>25</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>), multigrid outperforms SOR significantly as expected. However, for the timing experiments adopted in this work, the typical grid size is only <inline-formula id="inline-formula27-1094342011428146">
<mml:math id="mml-inline27-1094342011428146">
<mml:msup>
<mml:mn>18</mml:mn>
<mml:mn>3</mml:mn>
</mml:msup>
</mml:math>
</inline-formula>. In this region, SOR and multigrid give approximately the same performance when using CPU only. Moreover, for the GPU-accelerated solvers, preliminary tests show that SOR is 2.7 times faster than multigrid. Further optimization of the GPU multigrid solver is under investigation. In addition, currently GAMER adopts the so called ‘one-way interface’ Poisson solver, in which the coarse levels are advanced first, and then the boundary conditions of the fine-grid Poisson solver are obtained by performing both spatial and temporal interpolations on the coarse-grid data. In this approach, the coarse-grid solutions do not take into account the fine-grid corrections. Implementation of more advanced schemes dedicated to AMR hierarchy, for example, the projection method (<xref ref-type="bibr" rid="bibr18-1094342011428146">Martin and Colella 2000</xref>) and the multigrid method for finite-volume discretization with oct-tree AMR meshes (<xref ref-type="bibr" rid="bibr22-1094342011428146">Ricker 2008</xref>). will be investigated in the future.</p>
</sec>
</sec>
<sec id="section13-1094342011428146">
<title>6 Summary</title>
<p>We have introduced the directionally unsplit hydrodynamic schemes newly implemented in GAMER, including the MUSCL-Hancock method, a variant of the MUSCL-Hancock method, and the corner-transport-upwind scheme. In each scheme, we support different data reconstruction methods (PLM and PPM), different Riemann solvers (HLLE, HLLC, and Roe’s solvers), and also different slope limiters. All schemes have been implemented in GPU using NVIDIA CUDA, and up to two orders of magnitude performance speed-up has been demonstrated as compared to the performance using a single CPU core.</p>
<p>Several optimizations have been implemented in the code, including the asynchronous memory copy, the concurrent execution between CPU and GPU, and the hybrid MPI/OpenMP/GPU parallelization, by which we can fully exploit the computing power in a heterogeneous CPU/GPU system. OpenMP has been shown to be able to eliminate the performance bottleneck in the previous version of GAMER, and hence considerably improves the overall performance.</p>
<p>We have presented the overall performances of both uniform-mesh simulations and AMR simulations, measured on the Dirac cluster at NERSC/LBNL. In uniform-mesh tests, single GPU achieves a performance of <inline-formula id="inline-formula28-1094342011428146">
<mml:math id="mml-inline28-1094342011428146">
<mml:mn>2.9</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:msup>
<mml:mn>10</mml:mn>
<mml:mn>7</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> cell updates per second, which is 101 times faster than the performance using a single CPU core and 25 times faster than the quad-core performance. We also directly compare GAMER with the well-known code Athena in adiabatic hydrodynamic tests, in which two orders of magnitude performance speed-up is also demonstrated. Weak scaling with 98.8% parallel efficiency and strong scaling with 85.0% parallel efficiency are achieved in the 32-GPU experiments. In AMR tests, 32-GPU run achieves speed-ups of 71.4 and 18.3 as compared with the performances using 32 and 128 CPU cores, respectively. Some preliminary results using Hilbert space-filling curve for domain decomposition are also reported.</p>
<p>In Athena, both the VL and CTU schemes have been extended to MHD simulations (<xref ref-type="bibr" rid="bibr13-1094342011428146">Gardiner and Stone 2008</xref>; <xref ref-type="bibr" rid="bibr29-1094342011428146">Stone et al. 2008</xref>; <xref ref-type="bibr" rid="bibr30-1094342011428146">Stone and Gardiner 2009</xref>). Following their work, we have performed some preliminary tests on a MHD solver with the CTU scheme and constrained transport technique, and achieved a factor of 60 speed-up when compared with a single CPU core. Details of the implementation of MHD in GAMER will be reported in a future communication.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>A substantial part of the simulations presented in this work were performed on the Dirac GPU cluster at the National Energy Research Scientific Computing Center at Lawrence Berkeley National Laboratory (NERSC/LBNL). We would like to thank Hemant Shukla, John Shalf, and Horst Simon in the International Center for Computational Science (ICCS) for providing the access to this system. The special supercomputer Laohu at the High Performance Computing Center at National Astronomical Observatories of China, funded by Ministry of Finance under the grant ZDYZ2008-2, has also been used. We want to thank Rainer Spurzem, Peter Berczik, and Gao Wei for helping conduct simulations on this system. Simulations were also performed on the National Center for High-Performance Computing of Taiwan. Finally, we are grateful to Evghenii Gaburov for insightful suggestions and sharing the source code, and we also thank the referees for constructive comments that greatly improved this work.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure" id="fn5-1094342011428146">
<label>Funding</label>
<p>This work is supported in part by the National Science Council of Taiwan (grant number NSC97-2628-M-002-008-MY3).</p>
</fn>
</fn-group>
<notes>
<fn-group>
<title>Notes</title>
<fn fn-type="other" id="fn1-1094342011428146">
<label>1.</label>
<p>See <ext-link ext-link-type="uri" xlink:href="http://cactuscode.org/">http://cactuscode.org/</ext-link>
</p>
</fn>
<fn fn-type="other" id="fn2-1094342011428146">
<label>2.</label>
<p>See <ext-link ext-link-type="uri" xlink:href="https://seesar.Ibl.gov/anag/chombo/">https://seesar.Ibl.gov/anag/chombo/</ext-link>
</p>
</fn>
<fn fn-type="other" id="fn3-1094342011428146">
<label>3.</label>
<p>See <ext-link ext-link-type="uri" xlink:href="https://computation.Ilnl.gov/casc/SAMRAI/index.html">https://computation.Ilnl.gov/casc/SAMRAI/index.html</ext-link>
</p>
</fn>
<fn fn-type="other" id="fn4-1094342011428146">
<label>4.</label>
<p>See <ext-link ext-link-type="uri" xlink:href="http://www.physics.drexel.edu/~olson/paramesh-doc/Users_manual/amr.html">http://www.physics.drexel.edu/~olson/paramesh-doc/Users_manual/amr.html</ext-link>
</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aubert</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Teyssier</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Reionization simulations powered by graphics processing units</article-title>. <source>I. On the structure of the ultraviolet radiation field. <italic>ApJ</italic>
</source> <volume>724</volume>: <fpage>244</fpage>.</citation>
</ref>
<ref id="bibr2-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bell</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Berger</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Saltzman</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Welcome</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>1994</year>) <article-title>Three-dimensional adaptive mesh refinement for hyperbolic conservation laws</article-title>. <source>SIAM J Sci Comput</source> <volume>15</volume>: <fpage>127</fpage>.</citation>
</ref>
<ref id="bibr3-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berger</surname>
<given-names>MJ</given-names>
</name>
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>1989</year>) <article-title>Local adaptive mesh refinement for shock hydrodynamics</article-title>. <source>J Comput Phys</source> <volume>82</volume>: <fpage>64</fpage>.</citation>
</ref>
<ref id="bibr4-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Berger</surname>
<given-names>MJ</given-names>
</name>
<name>
<surname>Oliger</surname>
<given-names>J</given-names>
</name>
</person-group> (<year>1984</year>) <article-title>Adaptive mesh refinement for hyperbolic partial differential equations</article-title>. <source>J Comput Phys</source> <volume>53</volume>: <fpage>484</fpage>.</citation>
</ref>
<ref id="bibr5-1094342011428146">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bryan</surname>
<given-names>GL</given-names>
</name>
<name>
<surname>Norman</surname>
<given-names>ML</given-names>
</name>
</person-group> (<year>1996</year>) <article-title>Simulating X-ray clusters with adaptive mesh refinement</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Clarke</surname>
<given-names>DA</given-names>
</name>
<name>
<surname>West</surname>
<given-names>MJ</given-names>
</name>
</person-group> (eds), <source>Computational Astrophysics; 12th Kingston Meeting on Theoretical Astrophysics (<italic>ASP Conference Series</italic>
</source>, vol. <volume>123</volume>). <publisher-loc>San Francisco</publisher-loc>: <publisher-name>ASP</publisher-name>, p. <fpage>363</fpage>.</citation>
</ref>
<ref id="bibr6-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>1990</year>) <article-title>Multidimensional upwind methods for hyperbolic conservation laws</article-title>. <source>J Comput Phys</source> <volume>87</volume>: <fpage>171</fpage>.</citation>
</ref>
<ref id="bibr7-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Woodward</surname>
<given-names>PR</given-names>
</name>
</person-group> (<year>1984</year>) <article-title>The piecewise parabolic method (PPM) for gas-dynamical simulations</article-title>. <source>J Comput Phys</source> <volume>54</volume>, <fpage>174</fpage>.</citation>
</ref>
<ref id="bibr8-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Collins</surname>
<given-names>DC</given-names>
</name>
<name>
<surname>Xu</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Norman</surname>
<given-names>ML</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Cosmological adaptive mesh refinement magnetohydrodynamics with Enzo</article-title>. <source>ApJS</source> <volume>186</volume>: <fpage>308</fpage>.</citation>
</ref>
<ref id="bibr9-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Einfeldt</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Munz</surname>
<given-names>CD</given-names>
</name>
<name>
<surname>Roe</surname>
<given-names>PL</given-names>
</name>
<name>
<surname>Sjögreen</surname>
<given-names>B</given-names>
</name>
</person-group> (<year>1991</year>) <article-title>On Godunov-type methods near low densities</article-title>. <source>J Comput Phys</source> <volume>92</volume>: <fpage>273</fpage>.</citation>
</ref>
<ref id="bibr10-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Falle</surname>
<given-names>SAEG</given-names>
</name>
</person-group> (<year>1991</year>) <article-title>Self-similar jets</article-title>. <source>Mon Notices Roy Astron Soc</source> <volume>250</volume>: <fpage>581</fpage>.</citation>
</ref>
<ref id="bibr11-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fryxell</surname>
<given-names>B</given-names>
</name>
<name>
<surname>et</surname>
<given-names>al.</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>FLASH: an adaptive mesh hydrodynamics code for modeling astrophysical thermonuclear flashes</article-title>. <source>ApJS</source> <volume>131</volume>: <fpage>273</fpage>.</citation>
</ref>
<ref id="bibr12-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gaburov</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Harfst</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Portegies Zwart</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>SAPPORO: A way to turn your graphics cards into a GRAPE-6</article-title>. <source>New Astron</source> <volume>14</volume>: <fpage>630</fpage>.</citation>
</ref>
<ref id="bibr13-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gardiner</surname>
<given-names>TA</given-names>
</name>
<name>
<surname>Stone</surname>
<given-names>JM</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>An unsplit Godunov method for ideal MHD via constrained transport in three dimensions</article-title>. <source>J Comput Phys</source> <volume>227</volume>: <fpage>4123</fpage>.</citation>
</ref>
<ref id="bibr14-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hamada</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Nitadori</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Benkrid</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Ohno</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Morimoto</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Masada</surname>
<given-names>T</given-names>
</name>
<name>
<surname>et</surname>
<given-names>al.</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>A novel multiple-walk parallel algorithm for the Barnes-Hut treecode on GPUs - towards cost effective, high performance N-body simulation</article-title>. <source>CSRD</source> <volume>24</volume>: <fpage>21</fpage>.</citation>
</ref>
<ref id="bibr15-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hassan</surname>
<given-names>AH</given-names>
</name>
<name>
<surname>Fluke</surname>
<given-names>CJ</given-names>
</name>
<name>
<surname>Barnes</surname>
<given-names>DG</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Interactive visualization of the largest radioastronomy cubes</article-title>. <source>New Astron</source> <volume>16</volume>: <fpage>100</fpage>.</citation>
</ref>
<ref id="bibr16-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jin</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Xin</surname>
<given-names>Z</given-names>
</name>
</person-group> (<year>1995</year>) <article-title>The relaxation schemes for systems of conservation laws in arbitrary space dimensions</article-title>. <source>Commun Pure Appl Math</source> <volume>48</volume>: <fpage>235</fpage>.</citation>
</ref>
<ref id="bibr17-1094342011428146">
<citation citation-type="book">
<collab collab-type="author">Khronos Group</collab> (<year>2011</year>) <source>The OpenCL Specification (Version 1.1)</source>. <publisher-loc>Beaverton, OR</publisher-loc>: <publisher-name>Khronos Group</publisher-name>.</citation>
</ref>
<ref id="bibr18-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Martin</surname>
<given-names>DF</given-names>
</name>
<name>
<surname>Colella</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>A cell-centered adaptive projection method for the incompressible Euler equations</article-title>. <source>J Comput Phys</source> <volume>163</volume>: <fpage>271</fpage>.</citation>
</ref>
<ref id="bibr19-1094342011428146">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Norman</surname>
<given-names>ML</given-names>
</name>
<name>
<surname>Bordner</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Reynolds</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Wagner</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Bryan</surname>
<given-names>GL</given-names>
</name>
<name>
<surname>Harkness</surname>
<given-names>R</given-names>
</name>
<name>
<surname>et</surname>
<given-names>al.</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Simulating cosmological evolution with Enzo</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Bader</surname>
<given-names>D.</given-names>
</name>
</person-group> D (ed.), <source>Petascale Computing: Algorithms and Applications</source>. <publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>CRC Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-1094342011428146">
<citation citation-type="book">
<collab collab-type="author">NVIDIA</collab> (<year>2011</year>) <source>NVIDIA CUDA C Programming Guide (Version 4.0)</source>. <publisher-loc>Santa Clara, CA</publisher-loc>: <publisher-name>NVIDIA</publisher-name>.</citation>
</ref>
<ref id="bibr21-1094342011428146">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Press</surname>
<given-names>WH</given-names>
</name>
<name>
<surname>Teukolsky</surname>
<given-names>SA</given-names>
</name>
<name>
<surname>Vetterling</surname>
<given-names>WT</given-names>
</name>
<name>
<surname>Flannery</surname>
<given-names>BP</given-names>
</name>
</person-group> (<year>2007</year>) <source>Numerical Recipes. The Art of Scientific Computing</source> (<edition>3rd edn</edition>). <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr22-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ricker</surname>
<given-names>PM</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>A direct multigrid Poisson solver for oct-tree adaptive meshes</article-title>. <source>ApJS</source> <volume>176</volume>: <fpage>293</fpage>.</citation>
</ref>
<ref id="bibr23-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Roe</surname>
<given-names>PL</given-names>
</name>
</person-group> (<year>1981</year>) <article-title>Approximate Riemann solvers, parameter vectors, and difference schemes</article-title>. <source>J Comput Phys</source> <volume>43</volume>: <fpage>357</fpage>.</citation>
</ref>
<ref id="bibr24-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schive</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Chien</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Wong</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Tsai</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Chiueh</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Graphic-card cluster for astrophysics (GraCCA) - performance tests</article-title>. <source>New Astron</source> <volume>13</volume>: <fpage>418</fpage>.</citation>
</ref>
<ref id="bibr25-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schive</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Tsai</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Chiueh</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>GAMER: a graphic processing unit accelerated adaptive-mesh-refinement code for astrophysics</article-title>. <source>ApJS</source> <volume>186</volume>: <fpage>457</fpage>.</citation>
</ref>
<ref id="bibr26-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schive</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Tsai</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Chiueh</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>GAMER with out-of-core computation</article-title>. <source>Proc IAUS</source> <volume>270</volume>: <fpage>401</fpage>.</citation>
</ref>
<ref id="bibr27-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shukla</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Schive</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Woo</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Chiueh</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Multi-science applications with single codebase - GAMER - for massively parallel architectures</article-title>. In <source>Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC 2011)</source>, <publisher-loc>Seattle, WA</publisher-loc>, <comment>accepted for publication</comment>.</citation>
</ref>
<ref id="bibr28-1094342011428146">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Spurzem</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Berczik</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Berentzen</surname>
<given-names>I</given-names>
</name>
<name>
<surname>Ge</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Schive</surname>
<given-names>H</given-names>
</name>
<name>
<surname>et</surname>
<given-names>al.</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Accelerated many-core GPU computing for physics and astrophysics on three continents</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Dubitzky</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Kurowski</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Schott</surname>
<given-names>B</given-names>
</name>
</person-group> (eds), <source>Large Scale Computing Techniques for Complex Systems and Simulations</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr29-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stone</surname>
<given-names>JM</given-names>
</name>
<name>
<surname>Gardiner</surname>
<given-names>TA</given-names>
</name>
<name>
<surname>Teuben</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Hawley</surname>
<given-names>JF</given-names>
</name>
<name>
<surname>Simon</surname>
<given-names>JB</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Athena: a new code for astrophysical MHD</article-title>. <source>ApJS</source> <volume>178</volume>: <fpage>137</fpage>.</citation>
</ref>
<ref id="bibr30-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stone</surname>
<given-names>JM</given-names>
</name>
<name>
<surname>Gardiner</surname>
<given-names>TA</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>A simple unsplit Godunov method for multidimensional MHD</article-title>. <source>New Astron</source> <volume>14</volume>: <fpage>139</fpage>.</citation>
</ref>
<ref id="bibr31-1094342011428146">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Toro</surname>
<given-names>EF</given-names>
</name>
</person-group> (<year>2009</year>) <source>Riemann Solvers and Numerical Methods for Fluid Dynamics. A Practical Introduction</source> (<edition>3rd edn</edition>). <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr32-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Abel</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Kaehler</surname>
<given-names>R</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Adaptive mesh fluid simulations on GPU</article-title>. <source>New Astron</source> <volume>15</volume>: <fpage>581</fpage>.</citation>
</ref>
<ref id="bibr33-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wise</surname>
<given-names>JH</given-names>
</name>
<name>
<surname>Abel</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>ENZO+MORAY: radiation hydrodynamics adaptive mesh refinement simulations with adaptive ray tracing</article-title>. <source>Mon Notices Roy Astron Soc</source> <volume>414</volume>: <fpage>3458</fpage>.</citation>
</ref>
<ref id="bibr34-1094342011428146">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zink</surname>
<given-names>B</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>HORIZON: accelerated general relativistic magnetohydrodynamics</article-title>. <source>Preprint arXiv</source>:<volume>1102</volume>.<fpage>5202</fpage>.</citation>
</ref>
</ref-list>
</back>
</article>