<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPX</journal-id>
<journal-id journal-id-type="hwp">spepx</journal-id>
<journal-title>Educational Policy</journal-title>
<issn pub-type="ppub">0895-9048</issn>
<issn pub-type="epub">1552-3896</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0895904813475713</article-id>
<article-id pub-id-type="publisher-id">10.1177_0895904813475713</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Section 2: State</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>State Education Agencies, Information Systems, and the Expansion of State Power in the Era of Test-Based Accountability</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Jacobsen</surname><given-names>Rebecca</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Young</surname><given-names>Tamara V.</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Anagnostopoulos</surname><given-names>Dorothea</given-names></name>
<xref ref-type="aff" rid="aff1-0895904813475713">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Rutledge</surname><given-names>Stacey</given-names></name>
<xref ref-type="aff" rid="aff2-0895904813475713">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bali</surname><given-names>Valentina</given-names></name>
<xref ref-type="aff" rid="aff1-0895904813475713">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-0895904813475713"><label>1</label>Michigan State University, East Lansing, MI, USA</aff>
<aff id="aff2-0895904813475713"><label>2</label>Florida State University, Tallahassee, FL, USA</aff>
<author-notes>
<corresp id="corresp1-0895904813475713">Dorothea Anagnostopoulos, Associate Professor, Department of Teacher Education, Michigan State University, 620 Farm Lane, Room 331, East Lansing, MI 48824, USA. Email: <email>danagnos@msu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>27</volume>
<issue>2</issue>
<issue-title>Politics of Education Association Special Issue: The Politics of Accountability</issue-title>
<fpage>217</fpage>
<lpage>247</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This article examines how SEAs in three states designed, installed, and operated statewide, longitudinal student information systems (SLSIS). SLSIS track individual students’ progress in K-12 schools, college, and beyond and link it to individual schools and teachers. They are key components of the information infrastructure of test-based accountability. Drawing on science and technology studies, this study documents the strategies SEAs use to assemble and coordinate the vast amounts of technology and people across and beyond the educational system needed to collect, process, and disseminate test-based accountability data through SLSIS. We find that while SLSIS expand state power through what we refer to as informatic power, SEA control over these systems and the data they produce depends on whether SEA staff can manage the competing interests of federal, state, district, and external actors. SLSIS are thus sites of the ongoing contestation of state power within and beyond the educational system.</p>
</abstract>
<kwd-group>
<kwd>accountability</kwd>
<kwd>governance</kwd>
<kwd>state policies</kwd>
<kwd>state education agencies</kwd>
</kwd-group>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>March/April 2013</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>States have been pivotal to the development and implementation of test-based accountability in the United States. While the passage of No Child Left Behind (NCLB) in 2001 marked the most significant federal intervention in education in U.S. history, test-based accountability emerged first at the state level when, in the 1990s, a handful of states enacted policies that held individual schools accountable for their students’ performance on statewide learning standards and assessments. As it nationalized these policies and increased the stakes for districts and schools attached to state assessments, NCLB reinforced rather than limited the expansion of state power.</p>
<p>Much state-level research on test-based accountability has examined trends in state legislation and policy (<xref ref-type="bibr" rid="bibr9-0895904813475713">Desimone, Smith, Hayes, &amp; Frisvold, 2005</xref>; <xref ref-type="bibr" rid="bibr15-0895904813475713">Fusarelli &amp; Cooper, 2009</xref>; <xref ref-type="bibr" rid="bibr24-0895904813475713">McDermott, 2003</xref>), the effects of state accountability policies on school and classroom practices (<xref ref-type="bibr" rid="bibr14-0895904813475713">Firestone, Mayrowetz, &amp; Fairman, 1998</xref>; <xref ref-type="bibr" rid="bibr29-0895904813475713">Mintrop &amp; Trujilo, 2005</xref>; <xref ref-type="bibr" rid="bibr37-0895904813475713">Swanson &amp; Stevenson, 2002</xref>) or student outcomes (e.g., <xref ref-type="bibr" rid="bibr2-0895904813475713">Amerin &amp; Berliner, 2002</xref>; <xref ref-type="bibr" rid="bibr7-0895904813475713">Carnoy &amp; Loeb, 2002</xref>; <xref ref-type="bibr" rid="bibr16-0895904813475713">Hanushek &amp; Raymond, 2005</xref>). Few studies have examined the role of state education agencies (SEAs) in test-based accountability in the context of state power. Test-based accountability policies have expanded the administrative authority of SEAs over curriculum, assessment, teacher certification, and school improvement. The few studies that have examined how SEAs have responded to this increased authority have highlighted the challenges SEAs have faced meeting test-based accountability goals (<xref ref-type="bibr" rid="bibr27-0895904813475713">Minnici &amp; Hill, 2007</xref>; <xref ref-type="bibr" rid="bibr32-0895904813475713">Regional Educational Laboratory [REL], 2007</xref>). These challenges include limited fiscal and human resources and expanding policy mandates. In their study of accountability in six states, <xref ref-type="bibr" rid="bibr36-0895904813475713">Sunderman and Orfield (2006)</xref> found, for example, that inadequate fiscal and human resources constrained SEAs’ abilities to fulfill their expanded administrative responsibilities. Neither the state nor the federal government provided SEAs with sufficient fiscal support to administer the mandated testing, data reporting, and incentive and support systems. In addition, antibureaucratic sentiments, an emphasis on market-oriented solutions, and political disputes between governors, state superintendents, and state school boards combined with unprecedented state budget shortfalls resulted in significant SEA staff reductions. According to Sunderman and Orfield, SEAs responded to these competing pressures by devoting resources to developing student assessment systems and installing large-scale information systems that enabled expanded data collection and reporting efforts but delegated their responsibilities for supporting low-performing districts and schools to regional labs, districts, and the schools themselves.</p>
<p>In this article, we present findings from a study that builds on and extends these initial insights. During 2007 and 2008, we conducted case studies of how SEAs in three states designed, installed, and operated statewide, longitudinal student information systems (SLSIS). SLSIS track individual students’ achievement on state assessments and their progress across K-12 schools, college, and beyond. They also link this student performance data to individual schools and teachers. The development of SLSIS has been critical to the development of test-based accountability. SLSIS store and process the data used to measure, monitor, and regulate student, teacher, and school performance. As their storage and processing capacities have increased, SLSIS have made possible the widespread collection and dissemination of student achievement data, its disaggregation into various categories and classifications, and, most recently, the creation of value-added measures of teacher effectiveness central to the teacher evaluation and compensation systems promoted by the federal 2009 Race to the Top (RTT) initiative and currently being adopted by states and districts across the nation.</p>
<p>Given the centrality of SLSIS to test-based accountability, investigating how SEAs design, install, and operate these systems expands current understanding of the role SEAs play in test-based accountability. On the face of it, designing and operating SLSIS appear to be merely technical matters. Furthermore, as <xref ref-type="bibr" rid="bibr36-0895904813475713">Sunderman and Orfield (2006)</xref> note, data collecting and reporting have long been central administrative responsibilities for SEAs. Fulfilling them, according to <xref ref-type="bibr" rid="bibr36-0895904813475713">Sunderman and Orfield (2006)</xref>, reinforces rather than upsets established state–local relationships. Our case studies reveal, however, that designing and operating SLSIS are also political matters. They require SEAs to mobilize considerable amounts of fiscal, organizational, and political resources and to manage the often competing interests of the myriad actors involved in installing, operating, and utilizing SLSIS. These actors include testing companies, technology vendors and consultants, and state legislatures as well as schools and districts. Drawing on empirical and theoretical literature from science and technology studies, we conceive of this work as problematization, enrollment, and mobilization. Importantly, though we identify the strategies SEAs employ as they engage in this work, we find that central control by SEAs over SLSIS, and the data they collect and produce, is never entirely settled. SEA control of SLSIS is contingent on state and federal mandates, district and school compliance with those mandates, and the cooperation of an expanding private sector of external consultants. Thus, while SLSIS expand state power through what we refer to as informatic power, SLSIS and the information they collect and produce are the sites of the ongoing contestation and negotiation of this power within and beyond the educational system.</p>
<sec id="section1-0895904813475713">
<title>SLSIS and Test-Based Accountability</title>
<p>As we note above, SLSIS have been critical to the development of test-based accountability in the United States. These information systems enable states to collect large amounts of data on individual students, track individual students’ achievement and progress across the K-12 school system and beyond, and link them to individual schools and teachers. They are critical to the production of the performance measures and rankings of students, teachers, and schools at the center of test-based accountability policies. The Data Quality Campaign (<xref ref-type="bibr" rid="bibr11-0895904813475713">Data Quality Campaign, 2011</xref>) has identified 10 essential components of fully functioning SLSIS. These include unique student identifiers that track individual students across multiple years and databases, multiple data elements on each student including standardized assessments, demographics, program participation, grades and courses taken, dropout decisions, and data spanning a student’s educational career from Pre-K through higher education. High quality systems also match individual teachers with their students and include an audit system to maintain data quality and data warehousing capacities to store large amounts of data and make them available for further analysis by legislatures, researchers, and other interested parties.</p>
<p>SLSIS have developed differently across the 50 states. Texas and Florida, for example, have a long history of building extensive statewide data systems that include multiple data elements, whereas other states, such as Montana and Maine, have been slower to install them (<xref ref-type="bibr" rid="bibr11-0895904813475713">Data Quality Campaign [DQC], 2011</xref>). Beginning in 2005, the federal government offered SEAs funds, via a grant competition, for SLSIS development. Since then, the federal government has awarded more than a half a billion dollars through these grants. In 2009, the Obama administration further incentivized the expansion of SLSIS by stipulating that states would only be eligible for federal stimulus or RTT funding if they included, in their grant applications for the funds, plans to improve data collection and use. Specifically, the federal government called for improvements in both SLSIS and instructional improvement systems to assist teachers, principals, and administrators to use data to “identify student needs, fill curriculum gaps, and target professional development” as well as foster a culture of continuous improvement among schools and LEAs. In one of the more controversial requirements of RTT, states were required to build their systems’ capacity to link individual student information with individual teacher information as part of the administration’s efforts to promote the use of teacher pay-for-performance policies (<xref ref-type="bibr" rid="bibr40-0895904813475713">U.S. Department of Education, 2009</xref>). The federal government would only grant funds to those states that had enacted laws that enabled this linkage. As of 2012, all but 7 of the 50 states now have fully functioning SLSIS with this capacity.</p>
<p>Though SEAs are responsible for designing, installing, and maintaining SLSIS, SEAs must ultimately rely on schools and districts to put into place the technologies and people needed to collect and report the information demanded by state and federal accountability policies. The SEAs must therefore depend on schools and districts to make their existing information systems and data reporting practices consistent with SLSIS. A recent study sponsored by the <xref ref-type="bibr" rid="bibr40-0895904813475713">U.S. Department of Education (2009)</xref> that included a survey of 529 districts and in-depth qualitative case studies in 10, found that though districts and schools were installing their own local-level information systems operable with their states’ SLSIS, the capacities of these systems varied considerably and were constrained by both limited local collection and analytic capabilities.</p>
<p>In addition to district and schools, a wide array of external actors is also involved in the construction and operation of SLSIS. Private foundations, including the Bill and Melinda Gates Foundation, have played key roles in promoting and funding the creation of SLSIS (<xref ref-type="bibr" rid="bibr33-0895904813475713">Scott &amp; Jabbar, in press</xref>). States, districts, and schools also purchase technology from a variety of vendors and hire consultants to assist them in building, operating, and utilizing their information systems. This includes consultants who develop the complex mathematical models for measuring student, teacher, and school performance, such as teacher value-added measures, as well as those that assist states, districts, and schools with managing information. Finally, the centrality of state assessments to test-based accountability has increased SEAs’ reliance on testing companies in measuring student, teacher, and school performance. As test-based accountability moves toward a focus on holding individual teachers accountable for student performance such reliance will increase. More tests will need to be developed and administered to students in more grades and more subject areas to produce value-added measures of teacher and principal performance.</p>
<p>SLSIS thus constitute key components of an emergent educational information infrastructure that traverses multiple organizations and includes a vast number of people and technologies. To date, there have been few efforts to conceptualize the work that SEAs must engage in to design, install, and operate SLSIS. In this article, we draw on studies of science and technology (SST) to delineate a conceptual framework that helps illuminate the technical, organizational, and political dimensions of this work and its relationship to the expansion of state power under test-based accountability.</p>
</sec>
<sec id="section2-0895904813475713">
<title>Theoretical Framework</title>
<p>Studies of science and technology examine how the creation and implementation of new technologies both inscribe the practices, social relations, and knowledge used to create them and give rise to new practices, relations, and knowledge (<xref ref-type="bibr" rid="bibr4-0895904813475713">Bjiker &amp; Law, 1992</xref>; <xref ref-type="bibr" rid="bibr8-0895904813475713">Clarke &amp; Fujimara, 1992</xref>; <xref ref-type="bibr" rid="bibr13-0895904813475713">Epstein, 2008</xref>; <xref ref-type="bibr" rid="bibr17-0895904813475713">Lampland &amp; Star, 2009</xref>; <xref ref-type="bibr" rid="bibr20-0895904813475713">Latour, 2005</xref>; <xref ref-type="bibr" rid="bibr34-0895904813475713">Star, 1999</xref>).This research conceives of large-scale information technologies, like SLSIS, as sociotechnical networks of people, tools, and policies that stretch across geographic distance and organizational settings. Research in SST has explored the development and institutionalization of such technologies in health care, navigation, aeronautics, and science to understand the work required by central actors to assemble and coordinate the material and human resources needed to design and operate these technologies and to embed them into policy and practice in the settings that these technologies traverse (<xref ref-type="bibr" rid="bibr5-0895904813475713">Bruni, 2005</xref>; <xref ref-type="bibr" rid="bibr6-0895904813475713">Callon, 1986</xref>; <xref ref-type="bibr" rid="bibr19-0895904813475713">Latour, 1993</xref>; <xref ref-type="bibr" rid="bibr23-0895904813475713">Law, 2002</xref>).</p>
<p>Drawing on this research, especially <xref ref-type="bibr" rid="bibr6-0895904813475713">Callon (1986)</xref>, we identify three types of work that are particularly relevant to SLSIS. This includes <italic>problematization</italic>, which involves SEAs convincing people in the diverse organizations that SLSIS traverse, for example, schools, districts, testing companies, technology firms, and so forth, that their interests can only be secured by going through the SLSIS. SEAs seek to position SLSIS as “obligatory points of passage” for these diverse actors and organizations to attain their valued goals. <italic>Enrollment</italic> involves SEAs ensuring that personnel in schools, districts, and SEAs, collect and report information accurately and in the correct forms. This requires both embedding SLSIS into everyday work practices and tools and standardizing how people engage in these practices and utilize these tools. Finally, <italic>mobilization</italic> entails identifying the groups and individuals who will act as spokespeople for the SLSIS and help to garner fiscal, organizational, and political support for the information system. SST researchers have further identified three types of strategies that state-level actors can employ to attain the goals of these processes. These strategies include <italic>coercion</italic> by physical force, legal statute, or incentives, <italic>communication</italic>, and the <italic>standardization</italic> of personnel training and tools (<xref ref-type="bibr" rid="bibr19-0895904813475713">Latour, 1993</xref>; <xref ref-type="bibr" rid="bibr20-0895904813475713">2005</xref>; <xref ref-type="bibr" rid="bibr21-0895904813475713">Law, 1986</xref>, <xref ref-type="bibr" rid="bibr22-0895904813475713">1992</xref>, <xref ref-type="bibr" rid="bibr23-0895904813475713">2002</xref>).</p>
<p>As it highlights the work that central actors must engage in to install, operate, and maintain SLSIS, SST illuminates the ongoing negotiation and contestation of power through which this work occurs. On one hand, because SLSIS are critical parts of the information infrastructure of test-based accountability, control over them positions SEAs as centers of calculation (<xref ref-type="bibr" rid="bibr18-0895904813475713">Latour, 1987</xref>). Through installing and operating SLSIS, SEAs demand that schools and districts collect, encode, and report vast amounts of information on their students and teachers. SEAs then process this information into objective performance measures to which they attach sanctions for schools and require schools and districts to utilize in their school improvement efforts. SLSIS are thus not merely neutral information technologies but also technologies of state control.</p>
<p>On the other hand, SST suggests that because SLSIS stretch across numerous districts, schools, and other organizations, SEA control over these systems and the information they collect and make available is never entirely settled (<xref ref-type="bibr" rid="bibr34-0895904813475713">Star, 1999</xref>; <xref ref-type="bibr" rid="bibr35-0895904813475713">Star &amp; Ruhleder, 1996</xref>). With its emphasis on computing and measurement, test-based accountability extends the power of testing companies, computer firms, consultants, and researchers who create the metrics and models on which state and federal policy makers rely. Similarly, SEAs must rely on districts and schools with varying technological and personnel resources to collect and submit information to them. Furthermore, districts and schools do not simply submit to state accrual and deployment of power. Schools employ gaming strategies to raise student test scores without engaging in significant improvement efforts, and educators and other critics of test-based accountability openly resist the use of standardized test scores as the main measure of student, teacher, and school performance. As we show in this study, districts and schools also resist state attempts to collect information on and from them.</p>
<p>SST thus helps illuminate the technical, organizational, and political work that SEAs must engage in to design, install, and operate SLSIS within the context of test-based accountability. It offers a set of conceptual tools that can provide insight into how this work relates to the expansion and contestation of state power over K-12 education.</p>
</sec>
<sec id="section3-0895904813475713" sec-type="methods">
<title>Method</title>
<p>We explore these processes through an exploratory multiple case study design (<xref ref-type="bibr" rid="bibr25-0895904813475713">Merriam, 1998</xref>; <xref ref-type="bibr" rid="bibr42-0895904813475713">Yin, 2003</xref>) that examines the design and operation of SLSIS in three U.S. states. The multiple case design allows for insight into the particularities of each site as well as comparisons across sites that reveal the properties and dimensions of the politics of large-scale information systems in the context of test-based accountability.</p>
<p>We selected the three states to reflect differences in the development and comprehensiveness of SLSIS at the time of our study, 2007-2008, and as identified by the <xref ref-type="bibr" rid="bibr10-0895904813475713">Data Quality Campaign (2007)</xref>. In 2007, the <italic>Emergent State</italic> was in the early stages of developing its SLSIS and was focusing on creating unique student identifiers for all students (low DQC index); the <italic>Accelerated State</italic> had recently implemented a relatively comprehensive SLSIS with plans for further expansion, including the development of data warehousing capacities (medium DQC index); and the <italic>Established State</italic> had a highly comprehensive SLSIS that included a data warehouse and the capacity to link to other state data systems, including the personnel system that contained information on teachers (high DQC index). In each state, we conducted semistructured interviews with SEA staff and administrators directly responsible for overseeing the development and operation of each state’s SLSIS. We also interviewed relevant vendors and consultants and, in the Established State, a staff member who managed the information systems of one of the state’s largest school districts. In total, we interviewed 12 SEA and vendor/consultant administrators and staff members involved closely with designing, operating, and maintaining their respective state’s SLSIS. We also collected legislative, policy, and procedural documents regarding the SLSIS in each state.</p>
<p>We analyzed the data using both open and pattern coding (<xref ref-type="bibr" rid="bibr26-0895904813475713">Miles &amp; Huberman, 1994</xref>), coding for general themes that emerged from the data, such as the different state-level actors involved in the development and management of SLSIS as well as for specific concepts from the theoretical literature, including problematization, enrollment, and mobilization. This allowed us to identify both themes particular to each site and the work processes and strategies that SEAs engage in to install and operate SLSIS as identified in SST research. We jointly coded the interview data and created data matrices and counting to check our interpretations and identify disconfirming evidence (<xref ref-type="bibr" rid="bibr26-0895904813475713">Miles &amp; Huberman, 1994</xref>).</p>
<p>In addition to the case studies, we examined the relationship between SLSIS comprehensiveness, as measured by the DQC index of 2007-2008, and three state-level characteristics that we hypothesized would be relevant to SLSIS development: (a) legislative activity surrounding SLSIS, (b) state general fiscal conditions, and (c) degree of district centralization. Though the federal government and foundations have granted funds to some states to develop their SLSIS, states provide the vast majority of funding for SLSIS. Furthermore, the collection, processing, and dissemination of performance data are state-level administrative functions. We thus hypothesized that states with more legislative activity surrounding their SLSIS and with stronger general fiscal conditions would have more comprehensive SLSIS. We also hypothesized that states with more district centralization would also have more comprehensive SLSIS as district centralization would reduce the coordination demands. We derived our measure on legislative activity from the <xref ref-type="bibr" rid="bibr12-0895904813475713">Educational Commission of the States (2007)</xref>. The measure of a state’s fiscal conditions is the total state general fund balance as a percentage of expenditures (<xref ref-type="bibr" rid="bibr31-0895904813475713">National Association of State Budget Officers [NASBO], 2006</xref>). The degree of district centralization is the number of districts per 10,000 students. This captures the degree to which states have organized themselves into a fewer or larger number of districts.</p>
</sec>
<sec id="section4-0895904813475713">
<title>SLSIS Development Across the 50 States</title>
<p>The DQC has developed an index assessing the comprehensiveness of each state’s SLSIS that ranges from 1 to 10 elements from the list of 10 items the DQC deemed essential to fully functioning SLSIS. <xref ref-type="table" rid="table1-0895904813475713">Table 1</xref> summarizes the DQC index and the state characteristics that might relate to SLSIS comprehensiveness. We divide the states into “low” and “high” according to the number of elements included in their SLSIS. We classified 26 states as “low.” These states have between 1 and 6 SLSIS elements. We classified the remaining 24 states as “high” on their DQC rankings as they had from 7 to 10 SLSIS elements.</p>
<table-wrap id="table1-0895904813475713" position="float">
<label>Table 1.</label>
<caption>
<p>Classification of States by SLSIS Comprehensiveness and Selected State Characteristics, 2007.</p>
</caption>
<graphic alternate-form-of="table1-0895904813475713" xlink:href="10.1177_0895904813475713-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">DQC SLSIS Index</th>
<th align="center">Data system legislation/statute <italic>Percentage</italic></th>
<th align="center">District centralization <italic>Average</italic></th>
<th align="center">Percent budget balance <italic>Average</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Low (<italic>N</italic> = 26)</td>
<td>69.20%</td>
<td>6.25</td>
<td>11.4</td>
</tr>
<tr>
<td>High (<italic>N</italic> = 24)</td>
<td>87.5%</td>
<td>4.57</td>
<td>17.8</td>
</tr>
</tbody>
</table>
</table-wrap>
<p><xref ref-type="table" rid="table1-0895904813475713">Table 1</xref> suggests that, at the time of our study, there were distinct differences on how the states with “low” and “high” DQC index scores compared across the selected state characteristics. States with more comprehensive SLSIS, or high scorers, were more likely to have state legislation related to student data collection systems than were less established states. While state legislatures in 69.20% of the “low” index states had enacted a law or statute regarding the creation of a SLSIS, fully 87.5% of the state legislatures in the “high” index states had done so. Moreover, high index states were also more centralized than low index states. High index states had on average 4.57 districts per 10,000 students while low index states had 6.25 districts. Finally, the fiscal conditions of high index states were better (17.8% ending balances) compared to those of low index states (11.4% ending balances). Though the small sample size limits our ability to draw definitive conclusions from these percentages, the contrasts between high and low index states suggest that state legislative involvement, centralization, and healthier fiscal conditions might correspond with more developed and comprehensive SLSIS.</p>
</sec>
<sec id="section5-0895904813475713">
<title>The Design and Operation of SLSIS in Three States</title>
<p>The development of the SLSIS in our three case study states generally followed the patterns we found in our 50-state analysis. The Emergent State with the least developed SLSIS had low state legislative activity, a lower degree of centralization, and a relatively weaker fiscal condition than either the Accelerated or Established States, both of which had significantly more developed SLSIS. After providing a brief overview of each state’s SLSIS and the approach the SEA took to its development at the time of our study, we examine the technical, organizational, and political work that staff and administrators in the three SEAs engaged in to design, install, and operate their states’ SLSIS.</p>
<sec id="section6-0895904813475713">
<title>Emergent State</title>
<p>The Emergent State’s approach to developing its SLSIS had been relatively tentative. At the time of our study, the SEA had just created and assigned unique student identifiers to all students in the state. Like other low index states, the Emergent State lacked state legislation related to SLSIS, faced budget constraints, and had a relatively large number of districts, above 700. The relatively slow development of the state’s SLSIS also reflected the fragmentation of SLSIS oversight within the SEA. Formal authority over the SLSIS was located within the SEA’s fiscal offices, while policy oversight was located within the program areas, such as special education, ESL, and curriculum. Though this was intended to support SLSIS development by facilitating knowledge sharing and communication across SEA offices, SLSIS staff reported that it resulted in a lack of the leadership needed to promote and sustain SLSIS development.</p>
<p>A highly publicized testing failure further exacerbated this lack of SLSIS leadership within the SEA. Prior to our study, the SEA had hired a vendor to manage its testing system. Over the course of its multiyear contract with this vendor, several problems arose including the mis-scoring of tests and failure to administer the tests on time. While this testing failure negatively affected the SEA’s relationship with districts and schools, as we discuss below, it also eroded support for the SLSIS within the SEA itself. According to SEA staff we interviewed, the former director of the SLSIS resigned, in large part, as a result of the testing failure. SEA staff reported, “no one wanted to own the SLSIS”; it had become a “hot potato.” Perhaps not surprisingly, the SEA delegated primary responsibility for the design, installment, and daily operations of the state’s SLSIS to a team of consultants hired by the SEA.</p>
</sec>
<sec id="section7-0895904813475713">
<title>Accelerated State</title>
<p>While the Accelerated State had begun designing and installing its SLSIS just a year prior to our study, by 2007 its SLSIS already contained several elements and had an average DQC index rating. Though, like the Emergent State, the Accelerated State had a relatively high number of districts, the state had enacted SLSIS legislation and its fiscal health was relatively strong. When the SEA had failed to win a federal grant to develop its SLSIS, the state legislature allocated US$25 million for this work. Within the SEA, SLSIS staff had taken clear ownership of the SLSIS. The SLSIS was housed in the Information Technology (IT) department in the division of fiscal administration. To integrate the SLSS into the work of the SEA, however, the SLSIS manager created the Data Governance Board. The Board included directors from all program areas in the SEA as well as IT staff to oversee SLSIS development and operations.</p>
<p>The SEA in the Accelerated State did, however, face challenges maintaining highly qualified SLSIS staff. SLSIS staff reported that some of the personnel initially involved in creating the state’s SLSIS had left the SEA to pursue higher salaries in the private sector, taking with them special knowledge of the SLSIS’s architecture not easily transferred to other staff members. While the SLSIS staff had expanded significantly, more than doubling in the span of 3 years, current SLSIS staff characterized staff turnover as a “major pressing issue.”</p>
<p>The Accelerated State hired multiple consultants at different stages of design and implementation of the SLSIS to address its internal capacity issues. Unlike in the Emergent State, however, the Accelerated State’s SLSIS manager hired consultants primarily on a short-term basis to assist with the implementation of highly specialized technologies and to train state staff to utilize and manage these technologies. The SLSIS staff adopted this managed approach after mixed experiences with external consultants. While the staff were generally satisfied with some of the products and services provided by some consultants, they had to alter and sometimes fully “scrap” these products and services because they did not meet the state’s specific needs and were often simply too expensive to maintain.</p>
</sec>
<sec id="section8-0895904813475713">
<title>Established State</title>
<p>The development of the SLSIS in the Established State was supported by state SLSIS legislation, a high rate of district centralization and relatively strong state fiscal health. The Established State had long been at the vanguard of the movement to create SLSIS in the United States. The state began creating the system in the early 1990s. The SLSIS staff was housed in the assessment department of the K-12 school division with the SEA, reflecting its institutionalization into the central work and policy of the SEA program areas and, as we discuss below, the state’s districts and schools. At the time of our study, the state’s SLSIS integrated data across program areas and institutional domains, including higher education and the workplace, linked individual student achievement with individual teachers and schools, and included both data warehouse technologies and an audit system.</p>
<p>The Established State’s SLSIS emerged, in large part, from efforts begun in the late 1980s, by SEA staff to systematize data collection efforts through using computer technologies. The SLSIS staff members whom we interviewed, many of whom had been involved in the original design and implementation of the SLSIS in the 1990s and who continued to oversee SLSIS operations in the 2000s, described how they “learned on the run” as they were constructing a largely novel data collection system. Overtime, the SLSIS staff used external consultants but only to provide short-term, task specific support, such as generating reports during high demand periods, or initial technical expertise for creating its data warehouse. SLSIS staff members involved in the data warehouse initiative worked closely with the consultant to develop their own knowledge of the associated technologies and quickly took on responsibility for further development and management of the data warehouse.</p>
</sec>
</sec>
<sec id="section9-0895904813475713">
<title>SEAs and the Work of Designing, Installing, and Operating SLSIS</title>
<p>We turn now to examine how the SLSIS staff across the three SEAs managed the work of problematization, enrollment, and mobilization in their efforts to design, install, and operate the SLSIS. Though the SLSIS were at different stages of development, SLSIS staff in all three states continued to face challenges associated with this work, though to varying degrees. Importantly, SLSIS staff faced these challenges within the SEA and with external consultants as well as in their work with districts and schools.</p>
<sec id="section10-0895904813475713">
<title>Problematization</title>
<p>Problematization is the process through which SLSIS staff attempts to convince others that their valued interests can only be secured through participating with the SLSIS. NCLB reporting mandates compelled schools and districts across all three states to participate in their states’ SLSIS. State mandates in the Accelerated and Established States that tied school funding to data reporting placed further pressure on schools and districts to do so. Importantly, both state legislatures issued these mandates early in the development of each state’s SLSIS. This was particularly important in the Established State whose SEA began developing its SLSIS several years prior to the enactment of NCLB. At that time, installing a computerized data system that collected data at the individual student level marked a significant departure from the previous largely paper-based system that collected aggregate data. SLSIS staff reported that districts mounted considerable resistance to the SEA’s efforts to install a new information system. Years later, the staff members still referred to the period during which they rolled out the new systems as “Bloody September.” According to SLSIS staff, it was not until the state legislature tied school funding to participating with the SLSIS that most districts in the state began to do so. Across the three states, state and federal mandates were thus crucial to defining schools and districts’ interests in ways that made the SLSIS central to securing them. The SLSIS became an obligatory point of passage through which the schools and districts had to submit information to secure state and federal funding.</p>
<p>Though the coercive power of state and federal legislation facilitated the processes of problematization for the SEAs in relation to districts and schools, as the SLSIS became increasingly critical to schools and districts securing funding and avoiding state and federal accountability sanctions, its reliability and the SEA’sresponsibility for it also came under increased scrutiny. In the Emergent State, the highly publicized testing failure created considerable tensions between the SEA and districts. After the testing company the SEA had hired to administer and score the state assessments had failed to return accurate assessment results in time for the SEA and districts to meet federal reporting deadlines, the districts, facing significant revenue losses, lobbied the state legislature to investigate the incident. In response, the state legislature held public hearings in which they called SEA personnel to testify. The SEA personnel involved with the SLSIS that we interviewed reported that tensions between district personnel and SEA staff remained high at the time of our study. While federal mandates tied district interests to the SLSIS, this linkage also enabled districts to hold SEAs responsible for securing these interests. The districts could, in this sense, turn the coercive power of federal and state mandates on the SEA. Because of the stakes attached to SLSIS data, the districts could go through the state legislature to hold the SEA accountable.</p>
<p>Importantly, the testing failure and the highly publicized scrutiny of it raised problematization challenges within the SEA. As we note above, SLSIS staff reported that, as a result of the failure, the SLSIS had become a “hot potato” within the SEA. At the time of our study, there was little interest among SEA leadership in taking ownership of the SLSIS and few SEA staff wanted to be associated with the system. The fact that those staff charged with developing the SLSIS delegated the system’s development and everyday operations to external consultants highlights the marginalization of the SLSIS within the SEA itself.</p>
<p>In all three states, SLSIS staff also reported that district resistance continued to arise around the introduction of new data elements. Districts in the Emergent and Accelerated States challenged SEA demands that they submit student discipline data to the SLSIS. According to the SLSIS staff members in both states, the districts raised concerns about possible violations of privacy laws. One SLSIS staff member in the Accelerated State also attributed district resistance to the “environment of fear” created by test-based accountability. Districts feared that state and federal policy makers would use the student disciplinary information to label their schools unsafe, triggering the withdrawal of students and funding and, ultimately, possible state takeovers. In response to district resistance, the SEA in the Accelerated State decided to provide the districts and schools with individual-level reports but would not, themselves, report data at the student level. Though SLSIS staff in the Established State did not report recent incidents of intense district resistance, several noted that they tried to avoid requesting that districts send data not tied directly to state or federal mandates. Thus, while state and especially federal mandates had tied district and school interests to the SLSIS, they did not resolve the tensions that arose as data demands and the stakes attached to them for the districts and schools increased. Indeed, the mandates fueled these tensions.</p>
<p>SLSIS staff in the Accelerated State faced other challenges of problematization, especially those posed by external vendors and consultants. As we note above, the Accelerated State hired multiple consultants to assist with different stages of design and implementation of the SLSIS. Because the Accelerated State was not a large or highly populous state, consultants and vendors had little interest in tailoring their products to the state’s specific needs. In response, state’s SLSIS staff joined with SEA staff from other states in their geographic region to pressure one of the external consultants all the states had hired to make its product more responsive to each state’s needs and contexts.</p>
<sec id="section11-0895904813475713">
<title>Enrollment</title>
<p>Because they rely on the work of vast numbers of people and computers across and beyond the educational system, SLSIS pose significant coordination challenges for SEAs. Solving these challenges requires, in part, ensuring that people involved with the SLSIS have sufficient knowledge, skill, and technological resources to collect and report the large amounts of information collected through SLSIS. Ensuring the quality of this information requires that SEAs find ways to make the use of these resources predictable and reliable from afar. This is the work associated with enrollment.</p>
<p>The challenges of enrollment, particularly for SLSIS staff in the Emergent and Accelerated States, related to issues of both district and school interests and capacity. Despite state and federal mandates, SEA staff in these states reported that districts and schools in their states had not overwhelmingly embraced the need for the SLSIS nor had they incorporated data use into their routine activities. As one staff member in the Accelerated State noted, data collection and reporting were a “low priority” for districts and schools.</p>
<p>SLSIS staff in both states also identified limited and variable capacity in districts and schools as a major challenge to installing and operating their SLSIS. They reported that many districts lacked the requisite computer technology and skilled personnel. They also noted that many districts had high staff turnover, making it difficult to build district capacity related to working with and using the state’s SLSIS. A member of the SLSIS consultant team in the Emergent State noted that district personnel who interacted with the SLSIS ranged from secretaries to superintendents. SLSIS staff in the Accelerated State found it even more difficult to determine who actually did the data reporting in schools. They also found that district personnel in program areas—special education, bilingual education, and Title I—did not communicate with each other or with personnel responsible for uploading the data to the SLSIS. According to one SLSIS staff member, this lack of communication created considerable frustration among district and school personnel during the first year of SLSIS implementation in the Accelerated State and continued to impede full and accurate reporting of student information.</p>
<p>The SLSIS staff in the Emergent and Accelerated states employed a range of communication strategies to address the challenges of enrollment. They produced and distributed numerous guidebooks and alerted school and district personnel about system changes through postings on the SEA website. They maintained help desks, provided trainings and workshops across their states, maintained list-servs, and worked over the phone with district and school personnel. Though SLSIS staff in the Established State reported much fewer challenges in enrollment given that their state’s SLSIS had been in place for over a decade at the time of our study, they continued to offer similar technical support and held an annual data conference during which they interacted directly with district and school personnel to address the latter’s questions and assist them in meeting new demands and understanding system changes.</p>
<p>Importantly, the distrust toward the SEA that the testing failure created among districts in the Emergent State affected how the SEA engaged in these communication strategies. A member of the consultant team noted that she did not take SEA staff to her trainings and meetings with districts because SEA personnel had become “lightning rods” for district anger and resistance. Her team had developed strong relationships with districts and schools, in part, because the team had provided district and school personnel assistance directly through its help desk. The consultant team also attributed their positive relationships with districts to their “not being the state agency.” Members of the SLSIS consultant team carefully positioned themselves as “sympathetic” helpers rather than SEA representatives.</p>
<p>In addition to communication strategies, SLSIS staff in the Accelerated State also employed standardization strategies to tie both personnel in districts and schools and the myriad of external vendors and consultants they had hired to the SLSIS and embed it into their work practices and products, respectively. The staff manager created a voluntary certification program in SLSIS data collecting and reporting for district and school personnel to improve and standardize the skills, knowledge, and practices of the school and district personnel who worked with the SLSIS and raise the SLSIS’s status in the eyes of the district and school administrations. The SLSIS staff also instituted a voluntary certification program for software vendors and consultants that involved vendors testing the operability of their products with the state’s SLSIS during the summer. SLSIS staff listed certified vendors and consultants on their website. Vendor certification allowed the SLSIS staff to reduce the coordination costs raised by the myriad vendors and consultants hired by districts and schools in a state with a strong history of local control.</p>
<p>SLSIS staff in the three states viewed their efforts to build district and school capacity as critical to improving data quality. SLSIS staff across all three states, but particularly in the Emergent and Accelerated States, described improving data quality as a pressing issue. Yet, they reported having relatively few tools at their disposal to address it. Across all three states SLSIS staff felt that the best way they could improve the quality of the data that districts and schools submitted to them was by providing the districts and schools with useful reports. One SLSIS staff member in the Accelerated State noted,
<disp-quote>
<p>. . . all of our reports are downloadable to Excel. So (the districts and schools) certainly can pull that data back and use it in whatever way. We put it there so they will verify it. We totally understand that the only way to get high quality data is to give it back to them. And then they’re going to see what it looks like and make it even higher quality.</p>
</disp-quote></p>
<p>While the SLSIS staff in the Accelerated State had enacted a highly formalized governance structure to oversee SLSIS development and the flow of data across the state’s educational system that included a Data Governance Board, Data Manager Teams, and a Data Request Board, SLSIS staff members did not discuss this governance structure in relation to issues of data quality. Instead, like SLSIS staff in the other SEAs they focused largely on communication strategies and building a type of reporting reciprocity with districts and schools. Only one SLSIS staff member, from the Established State, described dealing directly with data inaccuracies. She reported that after exhausting efforts to address such inaccuracies with district or school personnel through phone conversations and e-mail communications, SLSIS staff “kicked (the problem) up” to program area staff, including those in the school finance division, who could enact negative sanctions on the district or school.</p>
<p>The SLSIS staffs’ emphasis on creating reciprocal relationships with districts and schools as a way to improve data quality reflects their position in the “middle between the federal and the local.” SLSIS staff across the three states repeatedly characterized themselves as “mediators,” and “go betweens.” Though they described themselves as the “messengers” of federal mandates, the SLSIS staff stressed that they, too, faced pressures from these mandates. SLSIS staff sought, in part, to gain district and school support for the SLSIS by positioning themselves as “sympathetic” mediators between federal mandates, on one hand, and the districts and schools, on the other. They believed the SLSIS assisted the districts and schools in meeting federal mandates efficiently. At the same time, the staff also noted that these mandates made districts and schools suspicious of the SLSIS. Districts and schools feared that the data collected through these systems could result in negative sanctions. This suspicion remained a challenge for the SLSIS staff even though in the Accelerated State, at least, districts and schools had begun, according to SLSIS staff, seeing the potential value of the data system.</p>
<p>Distrust associated with data collection and reporting also posed problems of enrollment within the SEAs. The SLSIS staff in both the Accelerated and the Established State described program area personnel within the SEA as initially resisting submitting information to the SLSIS. Because the SLSIS integrated data across program divisions, it threatened program areas personnel’s jurisdiction over program area data collection and reporting. In the Established State, the implementation of data warehousing technologies heightened this threat.</p>
<p>In the Accelerated State, the SLSIS staff deliberately included program area leaders in the SLSIS governance. They also took program area leaders with them to SLSIS trainings and conferences to increase the leaders’ knowledge of and interest in the SLSIS. In the Established State, SLSIS staff also provided and took program area staff to SLSIS-related trainings. They also emphasized the importance of the personal relationships they had developed with program area staff to tying the latter to the SLSIS. The SLSIS staff members we interviewed had been at the SEA for more than 15 years. As we note above, many had been involved in launching the state’s SLSIS in the early 1990s. A staff member in one program area noted that SLSIS staff had “enough background and experience and respect that people will come to the table.” Another SLSIS staff member stated,
<disp-quote>
<p>Everybody around you has hung around a long time. And we do not look at levels. We don’t look at tiers in the organizational chart. We look at what we need to accomplish. And we cross over whatever lines to get done whatever it is we need to get done.</p>
</disp-quote></p>
<p>The SLSIS staff’s longevity, knowledge, and personal relationships with program area personnel flattened organizational hierarchies within the SEA and facilitated the integration of data through the SLSIS.</p>
</sec>
<sec id="section12-0895904813475713">
<title>Mobilization</title>
<p>Given expanding demands for data and continual advancements in computer technologies, SLSIS will need continual updating. This will require continued and sometimes increased organizational and fiscal resources. Mobilizing support for the SLSIS is thus part of the ongoing work of SLSIS staff. Along with obtaining support from state legislators, district superintendents, and school personnel, mobilization involves identifying spokespeople who can promote the benefits of the SLSIS to the people and groups they interact with and whom they represent. These spokespeople are critical to the development and operations of SLSIS as their support can garner critical fiscal and organizational resources for SLSIS development and help SLSIS staff manage the diverse interests of the myriad organizations and actors involved with the SLSIS.</p>
<p>Perhaps not surprisingly, SLSIS staff in the Emergent State were not able to mobilize support for the SLSIS either within the SEA or beyond. Few people were willing to speak for a system that had failed. SLSIS staff in both the Established and Accelerated States, however, employed several strategies to identify spokespeople for the system. SLSIS staff in the Established State garnered critical organizational and fiscal support for the SLSIS by identifying spokespeople in districts and the state legislature, respectively. As we noted above, SLSIS staff in the Established State encountered considerable district resistance to their introduction of the SLSIS in the early1990s. In response, SLSIS staff identified a small group of both large and small districts to pilot the system. According to SLSIS staff, the large districts found that the new system increased efficiency while the small districts actually increased their state funds due to more consistent calculations. These districts became spokespeople for the system among the state’s school districts. As one SLSIS staff member noted, “[We] let them be our stars. They were the ones who carried the flag.”</p>
<p>SLSIS staff in the Established State also mobilized support for the system in the state legislature. Early in the SLSIS’s development, SLSIS staff talked with state legislative staff about the emerging system and invited them to the SEA to see their work. The legislative staff worked to ensure that language requiring school districts to allocate funds to the development of the SLSIS was put into a state appropriations act. According to the SLSIS staff, two of the appropriations staff had previously worked in the SEA and had been very supportive of the SLSIS. A SLSIS staff member had maintained positive relationships with these legislative staff. The longevity of the SLSIS staff thus proved critical to garnering legislative support for the SLSIS.</p>
<p>In the Accelerated State, SLSIS staff also worked to develop spokespeople for the system among district superintendents. Early in the development of the SLSIS, the director and staff met regularly with the statewide council of district superintendents to apprise them of the system’s development and to enlist their support. The SLSIS director noted that the superintendents on the council then “talked to their districts and to the set of districts that they represent” about the system and its relation to federal and state mandates. The director reported that she made it a policy to “always start” with the council of superintendents because district support was so critical to the success of the SLSIS.</p>
<p>Another key group in the Accelerated State comprised the program area leaders in the SEA itself. Because the SEA traditionally collected and reported data by individual program areas, changing to an integrated data collection system represented a significant shift in practice. To get “buy in” among program area staff for this shift, the SLSIS director created a data governance task force comprised of all the program area leaders and the SLSIS director and project managers. Because the SLSIS was located in the Instructional Technology (IT) division, the director felt it was imperative to get “a group that represented the business of education” involved in the planning and development of the SLSIS. If IT was seen as the driving force behind the system, the director felt that SEA program area staff would view the SLSIS as a burden rather than as a tool to aid the work of the program areas. Importantly, through her work with program area leaders on the task force, the SLSIS director was also able to obtain funds for the development of the SLSIS by writing requests for the funds into program area grant applications.</p>
</sec>
</sec>
</sec>
<sec id="section13-0895904813475713" sec-type="discussion">
<title>Discussion</title>
<p>Test-based accountability has expanded state power over K-12 education policy and practice. In this article, we have investigated the role SEAs play in this expansion by examining how staff in three SEAs designed, installed, and operated the student information systems, or SLSIS, that constitute key parts of the emerging information infrastructure of test-based accountability. Drawing on the ideas of problematization, enrollment, and mobilization from studies of science and technology, the study documents the technical, organizational, and political dimension of this work. In this section, we explore the implications of our findings focusing, in particular, on what they suggest about the distribution of power within and across the K-12 educational system under test-based accountability.</p>
<p>The role of SEAs in shaping and administering education policy has long been tenuous. <xref ref-type="bibr" rid="bibr39-0895904813475713">Timar (1999)</xref>, tracing the development of SEAs from the 19th century to the 1990s, argues that, while SEAs have grown from sparsely staffed offices to complex, differentiated organizations, their institutional capacity to facilitate comprehensive education reforms remains underdeveloped. Most relevant to this study, Timar concludes that the federal intervention in educational policy begun in the 1960s and 1970s, has fragmented SEA authority and contributed to contentious relationships between SEAs, on one hand, and local districts and schools, on the other hand. As federal regulations and programs have proliferated, the primary function of SEAs has become monitoring district and school compliance with expanding federal and state regulations. As a result, SEAs have developed little capacity to shape policy or manage the competing interests that characterize contemporary educational politics in the United States.</p>
<p><xref ref-type="bibr" rid="bibr36-0895904813475713">Sunderman and Orfield (2006)</xref> reach a similar conclusion in their study of SEA responses to NCLB. In particular, they argue that while NCLB places increased responsibility on SEAs for assisting low performing schools, it provides inadequate fiscal support to build SEAs’ capacity to deliver this support. Indeed, Sunderman and Orfield argue that at the same time NCLB expanded SEAs’ responsibility, conservative governors and state legislatures cut SEA budgets, with some even threatening to eliminate SEAs altogether. While state power over education policy and practice has expanded under test-based accountability, Sundermand and Orfield thus suggest that the influence SEAs have over policy and its implementation remains tenuous today.</p>
<p>Our findings also document this tenuousness. The staff who oversaw the design, implementation, and operation of SLSIS encountered several challenges in managing the competing interests of the various actors involved in this work. Within the SEAs, the SLSIS staff faced resistance from program area personnel concerned that the information systems threatened their jurisdiction over data reporting. SLSIS integrate the wide array of information that schools and districts collect on students across the different programs and categories to which students are assigned. This runs counter to the current organizational structure of SEAs, which is based primarily on programmatic divisions.</p>
<p>While our case studies document the strategies, including training and governance provisions, that SLSIS staff employed to tie program area personnel to the SLSIS and embed it into the latter’s work practices, they ultimately highlight the challenges that organizational fragmentation within the SEAs pose to the administration of test-based accountability by SEAs. This fragmentation and its challenges are illustrated most explicitly by where the SLSIS and their staffs were housed within the three SEAs in our study. In the Emergent State, administrative control over the SLSIS was divided across fiscal and program area offices. In the Accelerated State, the SLSIS staff was housed in the Instructional Technology division, a division itself housed in a separate building from the program areas. It was only in the Established State, the state with the most comprehensive SLSIS, that the SLSIS staff was located in the K-12 division. Our case studies suggests that where a SLSIS is located within an SEA matters to how readily or how difficult it can be embedded into the work of personnel across the SEA. The more organizationally integrated the SLSIS is with the K-12 program areas, the more readily SLSIS staff integrated its operation in the work of these areas. Indeed, in the Established State, the personal relationships the SLSIS staff developed with personnel in the program areas, combined with the staff’s longevity and expertise, facilitated the integrated problem solving that SLSIS, as complex, large-scale technologies, require. In contrast, SLSIS staff in the Accelerated State had to establish formal governance structures to facilitate their interaction with program area staff.</p>
<p>By illuminating the internal challenges that SLSIS staff encountered, our case studies suggest that SEAs should not be assumed to be unitary actors. Different units and divisions within SEAs are likely to respond to and administer test-based accountability mandates differently and often in contradictory ways. Furthermore, the work of administering these mandates is likely to be contested within the SEA. Test-based accountability is a systemic approach. It aims to focus the interests and work of the actors within the educational system around a small set of goals, most notably the goal of improving student scores on statewide standardized tests. As this involves SEA staff in collaborating across programmatic divisions, test-based accountability and its information infrastructure unsettles the existing division of work and the control over it within the SEA itself. SEAs’ role in expanding state power through test-based accountability will thus depend, in part, on the degree to which organizational fragmentation within SEAs can be overcome and to which competing interests within SEAs can be managed by the relevant SEA staff.</p>
<p>Our study also suggests that the role that SEAs play in expanding state power depends on whether SEAs can manage the interests of external actors, as well. In addition to the challenges SLSIS staff faced within the SEA, the SLSIS staff in our study also faced challenges managing the interests and the variety of practices and products of the myriad external consultants involved with building and operating the SLSIS both at the state and the district and school levels. SLSIS are sophisticated computing technologies. Designing, building, and operating them require technological resources and levels of technical expertise that SEAs often lack. Because computing technologies change so rapidly and because state and federal policy makers continue to increase their data demands, SEAs often have to turn to external consultants and vendors to maintain the capacities of their SLSIS and meet increasing data demands. <xref ref-type="bibr" rid="bibr38-0895904813475713">Thorn and Harris (in press)</xref> argue that the networks of for- and nonprofit firms that now provide states and districts with such assistance have become increasingly complex as test-based accountability moves toward holding individual educators as well as schools accountable for student achievement.</p>
<p>Our case studies document some of the coordination struggles that this expanding network of consultants and vendors gives rise to for SEAs. Because it is in the financial interests of external consultants and vendors to service the widest number of states, districts, and schools possible, many of their products and services lack the flexibility to address state-specific needs, particularly in less populous states, like the Accelerated State in our study. SEA staff in that state had to build a coalition with other similar states to pressure external vendors to increase the flexibility of their products. External vendors and consultants can thus have significant influence on what kind of information can get collected and how it is processed. SEAs must mobilize significant resources, some beyond their state itself, to shape this influence so that it meets their states’ particular mandates.</p>
<p>SEA reliance on and vulnerability to external consultants was most vividly illustrated by the Emergent State in our study. The power of the SEA staff to install and operate a comprehensive SLSIS was severely constrained by the tensions between the SEA, districts, and the state legislature that arose when a testing vendor failed to return accurate testing results on time for the districts and the state to meet federal reporting mandates. Though the SEA rescinded its contract with this testing company, during the time of the testing failure it could do little to obtain the test results from the company. The external consultant team the SEA had hired to design, implement, and operate the state’s emergent SLSIS, a team whose work SEA staff praised, did little to resolve the tensions between the districts and the SEA staff created by this testing failure. Instead, the team used the tensions to secure its own interests. It developed productive working relationships with the state’s districts and schools by explicitly differentiating itself from the SEA. It also maintained rather than transferred to SEA staff its in-depth knowledge of the SLSIS. In doing both, the consultant team became the obligatory passage through which the districts, schools, and SEA had to go to meet state and federal data collection and reporting demands.</p>
<p>The external vendors and consultants hired by districts and schools also posed coordination challenges for SLSIS staff. Again, this was particularly true in the Emergent and Accelerated States, both of which had, according to SLSIS staff, strong traditions of local school control. Both states had relatively high numbers of districts, reaching more than 700 in the Emergent State. The large number of districts combined with the variability of external vendors and consultants made it difficult to ensure the requisite interoperability between local district and school information systems and the SLSIS. The current move toward holding individual teachers and principals accountable for student test scores through value-added performance measures increases the data demands on schools, districts, and states. It is likely that schools, districts, and SEAs will continue to look to external vendors and consultants to meet these demands. As the number of vendors and consultants expand, tying their interests to each state’s SLSIS and making sure that the products and services they sell are operable with each SLSIS will pose ongoing challenges to SEAs’ control over the design and operation of SLSIS.</p>
<p>Most significantly, SLSIS staff in our study faced considerable resistance tying district interests to the SLSIS, particularly in the systems’ early stages. Though the SLSIS staff in the Accelerated State reported few incidents of district resistance during the time of our study, SLSIS staff across all three states encountered district opposition to a number of demands that installing and operating SLSIS placed on them. These included the costs of purchasing technology and personnel. Within all three states, SLSIS staff reported that district and school capacity to collect and report data through the SLSIS varied considerably. SLSIS staff and, in the Emergent State the external consultant team, addressed these issues largely through communication strategies, such as handbooks, help desks, training. The SLSIS staff in the Accelerated State also created a SLSIS certification program for district and school personnel in an attempt to standardize the knowledge and practices of these personnel across the state.</p>
<p>Despite these strategies, SLSIS staff across all three states felt that district and school capacity issues threatened data quality. SLSIS staff in each state identified data quality as a “pressing issue.” They reported few strategies, however, to address this issue, reflecting, again, their tenuous control over the SLSIS and the information they collect and produce. While the Established State’s SLSIS had an audit system, SLSIS staff in that state still relied largely on personal communication and persuasion to compel district personnel to correct data inaccuracies. When we asked SLSIS staff across the three states how they addressed issues of data quality, they said that the best way to do so was to provide the districts and schools with “useful” reports.</p>
<p>The limited capacity to ensure data quality that the SLSIS staff reported reflects, in part, the contingent nature of their administrative authority. SLSIS staff repeatedly characterized themselves as “go betweens” and “mediators” between federal mandates and local districts and schools. While federal mandates compelled districts and schools to report data to the SLSIS, the SLSIS staff recognized that the accuracy of that data, which the SEA used to measure and monitor district and school performance according to state and federal mandates, depended on those very districts and schools. State and federal mandates coerced districts and schools to report data to the SLSIS but appeared to have had a limited effect on the quality of that data. Furthermore, as the case of the Emergent State illustrates, issues of data accuracy and timeliness can erode the SEA’s ability to administer test-based accountability. In that case, districts were able to lobby the state legislature to investigate the testing failure and to hold the SEA accountable for it. Thus our findings illustrate that while the SEAs used state and federal mandates to coerce districts and schools to submit data to the SLSIS, the districts could use those same mandates to challenge and constrain SEAs’ ability to administer those mandates.</p>
<p>The challenges that the SLSIS staffs in our study encountered installing and operating their states’ SLSIS ultimately reflect the tensions between the use of information for rationalizing school improvement efforts and the use of information for control that are endemic to test-based accountability. On one hand, the proliferation of information on the nation’s students, teachers, and schools that test-based accountability has spurred can assist policy makers and educators in better assessing the effects of programs and practices and better targeting resources toward improving them. The emphasis that <xref ref-type="bibr" rid="bibr36-0895904813475713">Sunderman and Orfield (2006)</xref> and <xref ref-type="bibr" rid="bibr27-0895904813475713">Minnici and Hill (2007)</xref> place on SEAs assisting low performing schools reflects this view of information for rationalizing school improvement. On the other hand, test-based accountability also enables policy makers to use information to monitor, regulate, and negatively sanction teachers and schools. Over two decades ago, <xref ref-type="bibr" rid="bibr41-0895904813475713">Weiss and Gruber (1984)</xref> acknowledged these different uses of knowledge in educational policy, highlighting, in particular, how policy makers in arenas, like education, that are characterized by fragmented authority and widely dispersed resources, use information to change the behavior of people in districts and schools through persuasion and the manipulation of policy benefits and costs.</p>
<p>The rise of large-scale computerized information systems, like SLSIS, has intensified the tensions between information for improvement and information for control. Examining recent accountability regimes, <xref ref-type="bibr" rid="bibr3-0895904813475713">Ball (2003)</xref> and others (e.g., <xref ref-type="bibr" rid="bibr28-0895904813475713">Mintrop &amp; Sunderman, in press</xref>) develop the notion of performativity to argue that the hyperrational use of information technologies by state- and national-level policy makers in the context of outcomes-based management regimes, such as test-based accountability, represents a new mode of state regulation. Teachers and administrators alter and sometimes fabricate their work to meet the quantitative performance targets, indicators, and evaluations established by central state actors.</p>
<p>In another work (<xref ref-type="bibr" rid="bibr1-0895904813475713">Anagnostopoulos, Rutledge, &amp; Jacobsen, in press</xref>), we have argued that the use of information technologies under test-based accountability has given rise to a new type of informatic power. Extending <xref ref-type="bibr" rid="bibr30-0895904813475713">Mukerji’s (2010)</xref> ideas of strategic and logistic power, we conceive of informatic power as combining the coercive power of state-imposed incentives and sanctions with the control of measurement and computing technologies. Like logistic power, informatic power appears impersonal. State and federal education agencies use standardized tests, mathematical models, and computing technologies to produce performance measures that appear as transparent and accurate representations of the complex processes of teaching, learning, and schooling. As they define what kind of knowledge and ways of thinking matter and who counts as “good” teachers, students, and schools, these performance metrics shape how we practice, value, and think about education. States thus accrue informatic power not only through imposing incentives and sanctions on schools and districts but also through collecting, processing, and disseminating information from and on them.</p>
<p>While data collecting and reporting are traditional SEA administrative functions, with the rise of informatic power under test-based accountability SLSIS represent critical expressions of state power. As the capacities of SLSIS to collect and process increasing amounts of data on individual students, teachers, and schools expand, they make it possible for the state to measure and monitor schooling processes at ever finer-grain sizes. The movement from holding schools accountable to holding individual educators accountable for student test scores currently taking place illustrates the critical role that information technologies have played in the development of test-based accountability. The work of SEAs in designing, managing, and utilizing SLSIS, the central information technologies of test-based accountability, thus fundamentally extends the reach of state and federal power over K-12 education policy and practice.</p>
<p>At the same time, our study suggests that SLSIS are sites of the contestation and negotiation of this power within and beyond the educational system. SEA control over these information systems is never complete. SEAs must rely on district and school personnel to install the requisite technologies and to submit accurate information through SLSIS. Though our study illustrates how the coercive power of state and federal mandates compels districts and schools to submit information through the SLSIS, it also heightens rather than resolves issues of data quality. Districts and schools are being mandated to report the information that is used to monitor and potentially punish them. SLSIS thus assist districts and schools in securing state and federal funding at the same time the information systems make districts and schools vulnerable to state and federal policy makers who use the information districts and schools submit to exact negative sanctions on them. Yet, as our study suggests, districts can use these mandates and, in particular, the state legislature, to challenge the authority of the SEA and constrain its capacity to develop SLSIS.</p>
<p>The contestation over who controls information also occurs within the SEA as well as between the SEA and external consultants. As our study shows, SEA staff from different organizational divisions resisted reporting data to the SLSIS because they viewed it as ceding jurisdiction over their data. In addition, SEAs must rely on external actors, including testing companies and technology vendors and consultants, to provide the technologies and expertise required to design, install, and operate the SLSIS. While external consultants and vendors contribute to the states’ accrual of informatic power by providing the necessary technologies and expertise, they also garner informatic power for themselves. These organizations provide models and technical options that can compete with those endorsed by the state and, thus, become alternative centers of calculations. This was illustrated in our study by external consultants whose products constrained SEAs’ ability to address state specific needs and mandates and, in the case of the consultant team in the Emergent State, who maintained primary control over the expertise required to operate that state’s SLSIS. Though SLSIS can expand state power over K-12 education by making available informatic power, this power circulates within and across the K-12 educational systems. While SEAs play a key role in building the information systems that garner informatic power for the state, SEAs do not control how and who deploys this power.</p>
</sec>
<sec id="section14-0895904813475713" sec-type="conclusions">
<title>Conclusion</title>
<p>While states have been critical to the development of test-based accountability, SEAs have been largely overlooked by educational researchers. By examining how SEAs administer the data collection, reporting, and dissemination mandates of test-based accountability, we have provided initial insights into the ways in which power is being distributed, negotiated, and contested within and across the K-12 educational system. Though issues of data reporting and dissemination are often viewed as purely technical, as securing and installing the requisite technologies to upload, store, and process information, our study highlights the complex organizational and political work involved in such functions. In particular, our study helps illuminate the growth of an emergent information infrastructure in education. This infrastructure is both a key technology of state power and the site of the ongoing contestation of it.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: Two of the authors received a grant, in 2007, from the IBM Center for the Business of Government that supported data collection for the study presented in this chapter. The views presented here are solely the authors’ and do not represent those of the IBM Center.</p>
</fn>
</fn-group>
<bio>
<title>Author Biographies</title>
<p><bold>Dorothea Anagnostopoulos</bold> is an associate professor in the Department of Teacher Education at Michigan State University. Her research examines the implementation of accountability policy with a primary focus on urban districts and schools. This research has been widely published in journals including <italic>Research in the Teaching of English, Education Evaluation and Policy Analysis</italic>, and the <italic>American Educational Research Journal</italic>. She is coauthor of <italic>The Education Mayor</italic>, published by Georgetown University Press, and coeditor of the forthcoming volume, <italic>The Infrastructure of Accountability: Data Use and the Transformation of American Education</italic>, published by Harvard Education Press.</p>
<p><bold>Stacey Rutledge</bold> is an associate professor of educational leadership and policy at Florida State University. Her research explores how policies aimed at improving teaching and learning, such as test-based accountability and teacher quality, shape the work of district and school administrators and teachers, and ultimately, students’ learning opportunities. She is serving as a lead investigator in the Institute for Education Sciences-funded National Center for Research and Development on Scaling Up Effective Schools, which is aimed at identifying the policies and practices of effective high schools. She is coeditor of the forthcoming volume, <italic>The Infrastructure of Accountability: Data Use and the Transformation of American Education</italic>, published by Harvard Education Press.</p>
<p><bold>Valentina Bali</bold> is an associate professor in the Department of Political Science at Michigan State University. Her research focuses on public policy, education policy, state politics, and social science methodology. She is currently working on issues of segregation and educational opportunity in Argentine schools.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Anagnostopoulos</surname><given-names>D.</given-names></name>
<name><surname>Rutledge</surname><given-names>S.</given-names></name>
<name><surname>Jacobsen</surname><given-names>R.</given-names></name>
</person-group> (<year>in press</year>). <article-title>Introduction: Mapping the infrastructure of accountability</article-title>. In <person-group person-group-type="editor">
<name><surname>Anagnostopoulos</surname><given-names>D.</given-names></name>
<name><surname>Rutledge</surname><given-names>S. A.</given-names></name>
<name><surname>Jacobsen</surname><given-names>R.</given-names></name>
</person-group> (Eds.), <source>The infrastructure of accountability: Data use and the transformation of American Education</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard Education Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0895904813475713">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Amrein</surname><given-names>A.</given-names></name>
<name><surname>Berliner</surname><given-names>D.</given-names></name>
</person-group> (<month>March</month> <day>28</day>, <year>2002</year>). <article-title>High-stakes testing, uncertainty, and student learning</article-title>. <source>Education Policy Analysis Archives</source>, <volume>10</volume>(<issue>18</issue>). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://epaa.asu.edu/ojs/article/view/297/423">http://epaa.asu.edu/ojs/article/view/297/423</ext-link></citation>
</ref>
<ref id="bibr3-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ball</surname><given-names>S. J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The teacher’s soul and the terrors of performativity</article-title>. <source>Journal of Education Policy</source>, <volume>18</volume>, <fpage>215</fpage>-<lpage>228</lpage>.</citation>
</ref>
<ref id="bibr4-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Bijker</surname><given-names>W. E.</given-names></name>
<name><surname>Law</surname><given-names>J.</given-names></name>
</person-group> (Eds.). (<year>1992</year>). <source>Shaping technology/Building society</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr5-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bruni</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Shadowing software and clinical records: On the ethnography of non-humans and heterogeneous contexts</article-title>. <source>Organization</source>, <volume>12</volume>, <fpage>357</fpage>-<lpage>378</lpage>.</citation>
</ref>
<ref id="bibr6-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Callon</surname><given-names>M.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Some elements of a sociology of translation: Domestication of the scallops and the fishermen of St. Brieuc Bay</article-title>. In <person-group person-group-type="editor">
<name><surname>Law</surname><given-names>J.</given-names></name>
</person-group> (Ed.), <source>Power, action and belief: A new sociology of knowledge?</source> (pp. <fpage>196</fpage>-<lpage>233</lpage>). <publisher-loc>London, UK</publisher-loc>: <publisher-name>Routledge &amp; Kegan Paul</publisher-name>.</citation>
</ref>
<ref id="bibr7-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carnoy</surname><given-names>M.</given-names></name>
<name><surname>Loeb</surname><given-names>S.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Does external accountability affect student outcomes? A cross-state analysis</article-title>. <source>Education Evaluation and Policy Analysis</source>, <volume>24</volume>, <fpage>305</fpage>-<lpage>331</lpage>.</citation>
</ref>
<ref id="bibr8-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Clarke</surname><given-names>A. E.</given-names></name>
<name><surname>Fujimura</surname><given-names>J. H.</given-names></name>
</person-group> (<year>1992</year>). <article-title>What tools? Which jobs? Why right?</article-title> In <person-group person-group-type="editor">
<name><surname>Clark</surname><given-names>A. E.</given-names></name>
<name><surname>Fujimara</surname><given-names>J. H.</given-names></name>
</person-group> (Eds.), <source>The right tools for the job: At work in twentieth-century life sciences</source> (pp. <fpage>3</fpage>-<lpage>46</lpage>). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr9-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Desimone</surname><given-names>L.</given-names></name>
<name><surname>Smith</surname><given-names>T.</given-names></name>
<name><surname>Hayes</surname><given-names>S.</given-names></name>
<name><surname>Frisvold</surname><given-names>D.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Beyond accountability and average mathematics scores: Relating state education policy attributes to cognitive achievement domains</article-title>. <source>Educational Measurement: Issues and Practices</source>, <volume>24</volume>(<issue>4</issue>), <fpage>5</fpage>-<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr10-0895904813475713">
<citation citation-type="web">
<collab>Data Quality Campaign</collab>. (<year>2007</year>). <source>Creating a longitudinal data system: Using data to improve student achievement</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.dataqualitycampaign.org/tools/index.cfm#DQCResources">http://www.dataqualitycampaign.org/tools/index.cfm#DQCResources</ext-link></citation>
</ref>
<ref id="bibr11-0895904813475713">
<citation citation-type="web">
<collab>Data Quality Campaign</collab>. (<year>2011</year>). <source>Building and using data systems</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.dataqualitycampaign.org">http://www.dataqualitycampaign.org</ext-link></citation>
</ref>
<ref id="bibr12-0895904813475713">
<citation citation-type="web">
<collab>Education Council for the States</collab>. (<year>2007</year>). <source>State data systems</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://mb2.ecs.org/reports/Report.aspx?id=913">http://mb2.ecs.org/reports/Report.aspx?id=913</ext-link></citation>
</ref>
<ref id="bibr13-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Epstein</surname><given-names>S.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Culture and science/technology: Rethinking knowledge, power, materiality and nature</article-title>. <source>Annals of the American Academy of Political and Social Science</source>, <volume>619</volume>, <fpage>165</fpage>-<lpage>182</lpage>.</citation>
</ref>
<ref id="bibr14-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Firestone</surname><given-names>W.</given-names></name>
<name><surname>Mayrowetz</surname><given-names>D.</given-names></name>
<name><surname>Fairman</surname><given-names>J.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Performance-based assessment and instructional change: The effects of testing in Maine and Maryland</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>20</volume>(<issue>2</issue>), <fpage>95</fpage>-<lpage>113</lpage>.</citation>
</ref>
<ref id="bibr15-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Fusarelli</surname><given-names>B.</given-names></name>
<name><surname>Cooper</surname><given-names>S.</given-names></name>
</person-group> (Eds.). (<year>2009</year>). <source>The rising state: How state power is transforming our nation’s schools</source>. <publisher-loc>Albany</publisher-loc>: <publisher-name>State University of New York Press</publisher-name>.</citation>
</ref>
<ref id="bibr16-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hanushek</surname><given-names>E.</given-names></name>
<name><surname>Raymond</surname><given-names>M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Does school accountability lead to improved student performance?</article-title> <source>Journal of Policy Analysis and Management</source>, <volume>24</volume>, <fpage>297</fpage>-<lpage>327</lpage>.</citation>
</ref>
<ref id="bibr17-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lampland</surname><given-names>M.</given-names></name>
<name><surname>Star</surname><given-names>S. L.</given-names></name>
</person-group> (<year>2009</year>). <source>Standards and their stories: How quantifying, classifying, and formalizing practices shape everyday life</source>. <publisher-loc>Ithaca, NY</publisher-loc>: <publisher-name>Cornell University Press</publisher-name>.</citation>
</ref>
<ref id="bibr18-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Latour</surname><given-names>B.</given-names></name>
</person-group> (<year>1987</year>). <source>Science in action: How to follow scientists and engineers through society</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr19-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Latour</surname><given-names>B.</given-names></name>
</person-group> (<year>1993</year>). <source>The Pastuerization of France</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Latour</surname><given-names>B.</given-names></name>
</person-group> (<year>2005</year>). <source>Reassembling the social: An introduction to actor-network theory</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr21-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Law</surname><given-names>J.</given-names></name>
</person-group> (<year>1986</year>). <article-title>On the methods of long-distance control: Vessels, navigation, and the Portugese route to India</article-title>. In <person-group person-group-type="editor">
<name><surname>Law</surname><given-names>J.</given-names></name>
</person-group> (Ed.), <source>Power, action and belief: A new sociology of knowledge?</source> (pp. <fpage>234</fpage>-<lpage>263</lpage>). <publisher-loc>London, UK</publisher-loc>: <publisher-name>Routledge &amp; Kegan Paul</publisher-name>.</citation>
</ref>
<ref id="bibr22-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Law</surname><given-names>J.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Notes on the theory of the actor-network: Ordering, strategy, and heterogeneity</article-title>. <source>Systems Practice and Action Research</source>, <volume>5</volume>, <fpage>379</fpage>-<lpage>393</lpage>.</citation>
</ref>
<ref id="bibr23-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Law</surname><given-names>J.</given-names></name>
</person-group> (<year>2002</year>). <source>Aircraft stories: Decentering the object in technoscience</source>. <publisher-loc>Durham, NC</publisher-loc>: <publisher-name>Duke University Press</publisher-name>.</citation>
</ref>
<ref id="bibr24-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McDermott</surname><given-names>K.</given-names></name>
</person-group> (<year>2003</year>). <article-title>What causes variation in states’ accountability policies?</article-title> <source>Peabody Journal of Education</source>, <volume>78</volume>, <fpage>153</fpage>-<lpage>176</lpage>.</citation>
</ref>
<ref id="bibr25-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Merriam</surname><given-names>S. B.</given-names></name>
</person-group> (<year>1998</year>). <source>Qualitative research and case study applications in education</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr26-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Miles</surname><given-names>M. B.</given-names></name>
<name><surname>Huberman</surname><given-names>A. M.</given-names></name>
</person-group> (<year>1994</year>). <source>Qualitative data analysis: An expanded sourcebook</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr27-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Minnici</surname><given-names>A.</given-names></name>
<name><surname>Hill</surname><given-names>D.</given-names></name>
</person-group> (<year>2007</year>). <source>NCLB Year 5: Educational Architects: Do state education agencies have the tools necessary to implement NCLB?</source> <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Center on Education Policy</publisher-name>.</citation>
</ref>
<ref id="bibr28-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mintrop</surname><given-names>H.</given-names></name>
<name><surname>Sunderman</surname><given-names>G.</given-names></name>
</person-group> (<year>in press</year>). <article-title>The paradoxes of data-driven accountability: How centralized management fails education</article-title>. In <person-group person-group-type="editor">
<name><surname>Anagnostopoulos</surname><given-names>D.</given-names></name>
<name><surname>Rutledge</surname><given-names>S. A.</given-names></name>
<name><surname>Jacobsen</surname><given-names>R.</given-names></name>
</person-group> (Eds.), <source>The infrastructure of accountability: Data use and the transformation of American Education</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard Education Press</publisher-name>.</citation>
</ref>
<ref id="bibr29-0895904813475713">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Mintrop</surname><given-names>H.</given-names></name>
<name><surname>Trujilo</surname><given-names>T.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Corrective action in low-performing schools: Lessons for NCLB implementation from first-generation accountability systems</article-title>. <source>Education Policy Analysis Archives</source>, <volume>13</volume>(<issue>48</issue>). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://epaa.asu.edu/ojs/article/view/153/279">http://epaa.asu.edu/ojs/article/view/153/279</ext-link></citation>
</ref>
<ref id="bibr30-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mukerji</surname><given-names>C.</given-names></name>
</person-group> (<year>2010</year>). <article-title>The territorial state as a figured world of power: Strategies, logistics, and impersonal rule</article-title>. <source>Sociological Theory</source>, <volume>28</volume>, <fpage>402</fpage>-<lpage>424</lpage>.</citation>
</ref>
<ref id="bibr31-0895904813475713">
<citation citation-type="web">
<collab>National Association of State Budget Officers</collab>. (<year>2006</year>). <article-title>Fiscal survey of the states</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.nasbo.org/publications.php#fss2007">http://www.nasbo.org/publications.php#fss2007</ext-link></citation>
</ref>
<ref id="bibr32-0895904813475713">
<citation citation-type="gov">
<collab>Regional Educational Laboratory</collab>. (<year>2007</year>). <source>Getting the evidence for evidence-based initiatives: How the Midwest states use data systems to improve education processes and outcomes</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/edlabs/projects/project.asp?id=29">http://ies.ed.gov/ncee/edlabs/projects/project.asp?id=29</ext-link></citation>
</ref>
<ref id="bibr33-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Scott</surname><given-names>J.</given-names></name>
<name><surname>Jabbar</surname><given-names>H.</given-names></name>
</person-group> (<year>in press</year>). <article-title>Money and measures: The role of foundations in knowledge production</article-title>. In <person-group person-group-type="editor">
<name><surname>Anagnostopoulos</surname><given-names>D.</given-names></name>
<name><surname>Rutledge</surname><given-names>S. A.</given-names></name>
<name><surname>Jacobsen</surname><given-names>R.</given-names></name>
</person-group> (Eds.), <source>The infrastructure of accountability: Data use and the transformation of American education</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard Education Press</publisher-name>.</citation>
</ref>
<ref id="bibr34-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Star</surname><given-names>S. L.</given-names></name>
</person-group> (<year>1999</year>). <article-title>The ethnography of infrastructure</article-title>. <source>American Behavioral Scientist</source>, <volume>43</volume>, <fpage>377</fpage>-<lpage>391</lpage>.</citation>
</ref>
<ref id="bibr35-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Star</surname><given-names>S. L.</given-names></name>
<name><surname>Ruhleder</surname><given-names>K.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Steps towards an ecology of infrastructure: Design and access for large information spaces</article-title>. <source>Information Systems Research</source>, <volume>7</volume>(<issue>1</issue>), <fpage>111</fpage>-<lpage>134</lpage>.</citation>
</ref>
<ref id="bibr36-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sunderman</surname><given-names>G.</given-names></name>
<name><surname>Orfield</surname><given-names>G.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Domesticating a revolution: No Child Left Behind reforms and state administrative response</article-title>. <source>Harvard Educational Review</source> <volume>76</volume>, <fpage>526</fpage>-<lpage>563</lpage>.</citation>
</ref>
<ref id="bibr37-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Swanson</surname><given-names>C.</given-names></name>
<name><surname>Stevenson</surname><given-names>D.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Standards-based reform in practice: Evidence on state policy and classroom instruction from the NAEP state assessments</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>24</volume>(<issue>1</issue>), <fpage>1</fpage>-<lpage>27</lpage>.</citation>
</ref>
<ref id="bibr38-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thorn</surname><given-names>C.</given-names></name>
<name><surname>Harris</surname><given-names>D. N.</given-names></name>
</person-group> (<year>in press</year>). <article-title>The accidental revolution: Teacher accountability, value-added, and the shifting balance of power in the American school system</article-title>. In <person-group person-group-type="editor">
<name><surname>Anagnostopoulos</surname><given-names>D.</given-names></name>
<name><surname>Rutledge</surname><given-names>S. A.</given-names></name>
<name><surname>Jacobsen</surname><given-names>R.</given-names></name>
</person-group> (Eds.), <source>The infrastructure of accountability: Data use and the transformation of American education</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard Education Press</publisher-name>.</citation>
</ref>
<ref id="bibr39-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Timar</surname><given-names>T.</given-names></name>
</person-group> (<year>1999</year>). <article-title>The institutional role of state education departments: A historical perspective</article-title>. <source>American Journal of Education</source>, <volume>105</volume>, <fpage>231</fpage>-<lpage>260</lpage>.</citation>
</ref>
<ref id="bibr40-0895904813475713">
<citation citation-type="book">
<collab>U.S. Department of Education</collab>. (<year>2009</year>). <source>Race to the Top Program Preamble and Major Changes</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr41-0895904813475713">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weiss</surname><given-names>J.</given-names></name>
<name><surname>Gruber</surname><given-names>J.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Using knowledge for control in fragmented policy arenas</article-title>. <source>Journal of Policy Analysis and Management</source>, <volume>3</volume>, <fpage>225</fpage>-<lpage>247</lpage>.</citation>
</ref>
<ref id="bibr42-0895904813475713">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Yin</surname><given-names>R.</given-names></name>
</person-group> (<year>2003</year>). <source>Case study research: Design and methods</source> (<series>Applied social research methods series</series>, <volume>Vol.5</volume>). <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>