<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EDM</journal-id>
<journal-id journal-id-type="hwp">spedm</journal-id>
<journal-title>Journal of Cognitive Engineering and Decision Making</journal-title>
<issn pub-type="ppub">1555-3434</issn>
<issn pub-type="epub">XXXX-XXXX</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1555343412445054</article-id>
<article-id pub-id-type="publisher-id">10.1177_1555343412445054</article-id>
<title-group>
<article-title>Is More Information Better? How Dismounted Soldiers Use Video Feed From Unmanned Vehicles</article-title>
<subtitle>Attention Allocation and Information Extraction Considerations</subtitle>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Schmorrow</surname><given-names>Dylan D.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Bolstad</surname><given-names>Cheryl A.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>May</surname><given-names>Katrina A.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Cuevas</surname><given-names>Haydee M.</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Ophir-Arbelle</surname><given-names>Ronny</given-names></name>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Oron-Gilad</surname><given-names>Tal</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Borowsky</surname><given-names>Avinoam</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Parmet</surname><given-names>Yisrael</given-names></name>
</contrib>
<aff id="aff1-1555343412445054">Ben-Gurion University of the Negev</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-1555343412445054">Tal Oron-Gilad, Ben-Gurion University of the Negev, P.O. Box 653, Beer-Sheva, 84105, Israel, <email>orontal@bgu.ac.il</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>7</volume>
<issue>1</issue>
<issue-title>Special Section: Exploring Cognitive Readiness in Complex Operational Environments: Advances in Theory and Practice, Part III</issue-title>
<fpage>26</fpage>
<lpage>48</lpage>
<permissions>
<copyright-statement>© 2012, Human Factors and Ergonomics Society.</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<p>Operational tactics in urban areas are often aided by information from unmanned aerial vehicles (UAVs). A major challenge for dismounted soldiers, particularly in urban environments, is to understand the conflict area in general and particularly from the UAV feed. The UAV feed is usually used to enhance soldiers’ situation awareness abilities but less for identifying specific elements. A possible way to further enhance soldiers’ abilities is to provide them with multiple sources of information (e.g., aerial and ground views). This study examined the benefits of presenting video feed from UAVs and unmanned ground vehicles (UGVs) in a combined interface, relative to presenting aerial feed alone. Thirty former infantry soldiers with no experience in operating unmanned vehicles participated. Objective performance, subjective evaluations, and eye-tracking patterns were examined in two scenarios. In Scenario 1, performance scores in both identification and orientation tasks were superior in the combined configuration. In Scenario 2, performance scores in the identification tasks were improved, and the addition of the UGV feed did not harm performance in the orientation task. Eye movement scanning patterns reinforced that both UAV and UGV feeds were used for the mission. The combined configuration generated consistent benefits with regard to the identification tasks, perceived mental demand, and reduction of false reports without having any apparent cost on participants. Ground views may provide additional support to dismounted soldiers.</p>
</abstract>
<kwd-group>
<kwd>dismounted soldiers</kwd>
<kwd>unmanned vehicles</kwd>
<kwd>video feed</kwd>
<kwd>attention allocation</kwd>
<kwd>MOUT</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1555343412445054" sec-type="intro">
<title>Introduction</title>
<p>Lessons learned worldwide from the war on terror led to the development of new operation methods that are heavily aided by remote-controlled machines. There are two approaches toward enhancing soldiers’ orientation and awareness while using unmanned systems: adding sensors and tools to the unmanned system itself or adding sources of information (e.g., more unmanned systems) to generate a broader perspective of the environment (<xref ref-type="bibr" rid="bibr5-1555343412445054">Chadwick, 2008</xref>). When information from several types of unmanned systems is combined, it is important to minimize the visual load imposed on the soldier. Thus, although additional angles can increase operator comprehension of the situation, they may also cause overload and confusion. Often, too many choices, characteristics, and applications may harm the operator as much as lack of choices (<xref ref-type="bibr" rid="bibr26-1555343412445054">Mouloua, Gilson, Daskarolis-Kring, Kring, &amp; Hancock, 2001</xref>). <xref ref-type="bibr" rid="bibr3-1555343412445054">Chadwick (2005)</xref> showed that although operators thought that the addition of an unmanned ground vehicle (UGV) view facilitated better spatial awareness of the environment, particularly when crossing obstacles, the experimental results did not support this notion. Thus, choosing the most efficient means for transferring operational information is crucial for operators and information consumers in all echelons. In the current study, performance, subjective evaluations, and eye-scanning patterns of dismounted soldiers performing Military Operation Urban Terrain (MOUT) intelligence gathering tasks were examined in a single versus dual video feed environment, with the aim to identify the added value, that is, the benefits to mission performance and toll of the second asset to mission success.</p>
<p>A common way to examine allocation of attention in the visual domain is by focusing on visual sampling or monitoring behavior. Models based on such sampling have been suggested to be suitable for supervisory control tasks in which the observer is not looking for a single static target but is rather attending to a series of dynamic processes (<xref ref-type="bibr" rid="bibr25-1555343412445054">Moray, 1986</xref>; <xref ref-type="bibr" rid="bibr34-1555343412445054">Senders, 1964</xref>, <xref ref-type="bibr" rid="bibr35-1555343412445054">1983</xref>; <xref ref-type="bibr" rid="bibr36-1555343412445054">Sheridan, 1970</xref>; and more recently, <xref ref-type="bibr" rid="bibr38-1555343412445054">Wickens, Helleberg, Goh, Xu, &amp; Horrey, 2001</xref>). The key dependent variable is the proportion of visual attention distributed to various areas of interest (AOIs) as a function of the quantitative properties of those AOIs. As noted by Robin <xref ref-type="bibr" rid="bibr27-1555343412445054">Murphy (2009)</xref>,<disp-quote>
<p>everyone tacitly acknowledge that sensemaking is a problem—cameras are sprouting like mushrooms from every conceivable mounting space in the hopes of giving the operator that magic missing viewpoint. However, it is all ad hoc; there is no understanding of what viewpoints are needed for which tasks or how to transition between them. So-called perceptual interfaces remain neolithic.</p>
</disp-quote></p>
<p>This statement represents the general notion or dilemma between presenting operators or information consumers with multiple sensors or sources of information versus presenting them only with information relevant to the task at hand and avoiding information overload. Within this dilemma, we focus in the current study on a single case: whether the addition of UGV feed can facilitate or enhance the information retrieved from an unmanned aerial vehicle (UAV) feed by a dismounted soldier.</p>
<p>In the literature, it has already been acknowledged that UAVs are meant to deliver the “larger,” bird’s-eye picture and are necessary for orientation tasks. UGVs are meant to deliver more focused and specific images from a ground vehicle perspective. As described by <xref ref-type="bibr" rid="bibr39-1555343412445054">Wickens, Vincow, and Yeh (2001)</xref>, the camera of each UGV provides an egocentric perspective that supports navigation, but the camera on the UAV provides an exocentric view that would be expected to provide a better understanding of the location and its environment. Combination of the two should be advantageous when information is complex or ambiguous (<xref ref-type="bibr" rid="bibr33-1555343412445054">Salzman, Dede, Loftin, &amp; Ash, 1998</xref>). Different tasks can be facilitated with UAVs or UGVs, yet in combat, it is often not possible to separate between tasks. One may want to detect a target and then identify its features in more detail. Therefore, the potential of combining the images into one interface can be significant (<xref ref-type="bibr" rid="bibr7-1555343412445054">Chen &amp; Clark, 2008</xref>). The UAV does not replace the need for a map or GPS but rather extends it. <xref ref-type="bibr" rid="bibr32-1555343412445054">Redden, Elliott, Pettitt, and Carstens (2011)</xref>, for example, found that timed performance with a split screen (unmanned vehicle [UV] and map) and with a multimodal display (tactile belt used to replace the need to view the map for direction) was significantly better than timed performance with a toggle display (either map or UV). Furthermore, soldiers rated their situational awareness as lower with the toggle display than with the split-screen display or multimodal display. <xref ref-type="bibr" rid="bibr5-1555343412445054">Chadwick (2008)</xref>, for example, found that the best spatial awareness was achieved when the UAV was in high altitudes, where its angle was of less importance. <xref ref-type="bibr" rid="bibr7-1555343412445054">Chen and Clark (2008)</xref> examined addition of UAV feed to UGV operators in navigation and target detection tasks and found that navigation and target detection without the UAV feed slowed the UGV operators significantly. Thus, UAV feed may be essential when UGVs are involved (<xref ref-type="bibr" rid="bibr31-1555343412445054">Pazuchanics, Chadwick, Sapp, &amp; Gillan, 2008</xref>).</p>
<p>However, presenting multiple video feeds simultaneously can be efficient only when the task requires it and when there is enough time for operators to respond (<xref ref-type="bibr" rid="bibr26-1555343412445054">Mouloua et al., 2001</xref>). <xref ref-type="bibr" rid="bibr4-1555343412445054">Chadwick (2006)</xref> found that response time to identify a target with a single UGV was faster than when using two or four. <xref ref-type="bibr" rid="bibr13-1555343412445054">Dixon, Wickens, and Chang (2003)</xref> reported on degraded targets and navigation performance when multiple UAVs were used compared with a single UAV. <xref ref-type="bibr" rid="bibr37-1555343412445054">Thomas and Wickens (2000)</xref> reported that when operators were presented with two separate video feeds, they tended to ignore one of the feeds in favor of the other. Generally, operators preferred to focus on the UGV rather than on the UAV, which provided the “global picture” (i.e., cognitive tunneling, see <xref ref-type="bibr" rid="bibr3-1555343412445054">Chadwick, 2005</xref>; <xref ref-type="bibr" rid="bibr29-1555343412445054">Oron-Gilad, Redden, &amp; Minkov, 2011</xref>). Therefore, this study also examined (using the eye tracker) whether the presentation of two video feeds simultaneously on a single interface would cause the effect of cognitive tunneling.</p>
</sec>
<sec id="section2-1555343412445054">
<title>Experimental Aims and Hypotheses</title>
<p><xref ref-type="bibr" rid="bibr22-1555343412445054">McDermott, Luck, Allender, and Fisher (2005)</xref> argued that the UGV was best associated with providing descriptive information from remote environments. Subsequent research on this question appears to differ with respect to whether researchers were looking at individual operators or at teams. A study by <xref ref-type="bibr" rid="bibr9-1555343412445054">Chen, Durlach, Sloan, and Bowens (2008)</xref> reported that individual operators were equally capable of finding a target with UGV and UAV feeds. In contrast, in a team performance study that examined the unique effects of spatial ability according to UAV-UGV team roles, only the spatial ability of the UGV operator was associated with overall performance on a reconnaissance task (<xref ref-type="bibr" rid="bibr15-1555343412445054">Fincannon, Evans, Jentsch, &amp; Keebler, 2009</xref>). Furthermore, others have argued that the ground perspective is needed to verify the identity of objects when working with a UAV (<xref ref-type="bibr" rid="bibr19-1555343412445054">Goodrich et al., 2008</xref>). On the basis of these findings, we hypothesized the following:</p>
<list id="list1-1555343412445054" list-type="simple">
<list-item><p><italic>Hypothesis 1</italic>: The added value of the UGV feed will be expressed in an identification task that requires focusing on, pinpointing, and deciphering suspicious elements (explained in detail in the Operational Tasks section).</p></list-item></list>
<p>As mentioned earlier, <xref ref-type="bibr" rid="bibr22-1555343412445054">McDermott and colleagues (2005)</xref> argued that a UGV was best associated with providing descriptive information, but they also argued that the UAV was best associated with facilitation of localization of items in remote environments. Another study, by <xref ref-type="bibr" rid="bibr3-1555343412445054">Chadwick (2005)</xref>, found that the addition of UAV feed improved operators’ ability to localize objects. Further research with route and survey perspectives in visual-spatial cognition has also supported this position that an aerial perspective is optimal for localization (<xref ref-type="bibr" rid="bibr12-1555343412445054">Diaz &amp; Sims, 2003</xref>). On the basis of these findings associated with UAV operation, we hypothesized the following:</p>
<list id="list2-1555343412445054" list-type="simple">
<list-item><p><italic>Hypothesis 2</italic>: The UAV feed provides orientation information (e.g., location of an element), and the added value of the UGV feed to orientation is negligible.</p></list-item></list>
<p>These two hypotheses were previously supported by findings of <xref ref-type="bibr" rid="bibr16-1555343412445054">Fincannon, Keebler, Jentsch, Phillips, and William (2011)</xref>, who showed that when a team of operators was using an UAV and UGV together, the aerial perspective was optimal to determine object localization, and the ground perspective was optimal for determining object identity. The aim of the current study was to further examine whether the hypotheses applied also to a single passive information receiver.</p>
</sec>
<sec id="section3-1555343412445054" sec-type="methods">
<title>Method</title>
<p>This study simulated a situation in which a dismounted soldier performed MOUT intelligence-gathering tasks using data derived either from an UAV or simultaneously from both UAV and UGV. Participants could not directly control the UVs or their payloads. The operational mission consisted of identification of stationary and moving elements and of orientation to specific waypoints on the map. Such MOUT operations in Israel are currently supported by UAVs, and the utility of adding UGVs to the scene is always questioned because of their lower survivability. Our aim was to evaluate the potential benefit of adding UGV feed to MOUT missions. Thus, unlike previous studies (e.g., <xref ref-type="bibr" rid="bibr24-1555343412445054">Minkov, Lerner, Oron-Gilad, &amp; Ophir, 2010</xref>; <xref ref-type="bibr" rid="bibr29-1555343412445054">Oron-Gilad et al., 2011</xref>) in which the use of video feed from a single source (i.e., UV feed), either UAV or UGV, was examined, here, the focus was on situations in which the UGV could potentially provide added value (i.e., additional information and/or easier interpretation) that was not available from the UAV, all in the aim to examine whether indeed this potential benefit materialized and at what cost.</p>
<p>After consulting with subject matter experts (SMEs; senior dismounted commanders and UAV operators), we have identified two situations in which video from UAVs may be insufficient within the MOUT environment. Based on these investigations, two operational scenarios were created. In both, potentially there could be extra benefit from presenting the additional UGV video feed compared with presenting the UAV feed alone. The two operational scenarios that were investigated were as follows: (a) The first scenario involved a situation in which the UAV was flying a reconnaissance route at relatively high altitude while not zooming on any specific targets along the way. Zooming tends to decrease the spatial orientation. Also, it is not always clear to the UAV operator on what elements to zoom. Therefore, in this scenario, the role of the UGV was then to focus or zoom onto specific targets along the route and gather more information. (b) The second scenario involved flying over a densely populated area. Because of its density, the UAV experienced occasional incidents of concealment. It was assumed that the UGV would then cover for these concealed areas (see <xref ref-type="fig" rid="fig1-1555343412445054">Figure 1</xref> for examples of no-zoom and concealment scenarios).</p>
<fig id="fig1-1555343412445054" position="float">
<label>Figure 1.</label>
<caption><p>Examples from the experimental scenarios. Top: The right image shows a flight pattern whereby the unmanned aerial vehicle (UAV) is zooming in on parked vehicles, whereas the left image shows the target vehicles seen without zoom (Scenario 1). Bottom: The right image shows the densely populated area where concealments prevent the UAV from seeing suspects moving, and the left image shows a more open area where concealments from the building shade the moving suspects and make it difficult to identify them (Scenario 2). Red ellipses are shown for emphasis.</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig1.tif"/>
</fig>
<p>To examine the experimental hypotheses, we used a single interface and a combined interface configuration in a fixed design; that is, participants always experienced the single interface before the combined one. This fixed design was a thought-through decision that stemmed from the concern that the combined display potentially showed more information than did the single view. Our concerns were that if participants experienced the combined view first, they might (a) recall details from the combined view and use them to alter their response in the single view or alter their strategy in the single view or (b) feel frustrated or less motivated when the single view appeared (as it did not show information that they may have perceived as useful in the combined view) and thus be less inclined to perform the mission to their best abilities. Furthermore, (c) in the real operational world, in Israel, participants’ baseline is indeed the single view (UAV feed only); thus, from an operational perspective, it makes sense to look at the “added” value of the UGV (i.e., combined view).</p>
<sec id="section4-1555343412445054">
<title>Participants</title>
<p>Thirty engineering students, former Israeli Defense Force (IDF) soldiers, took part in Scenario 1. Of those, 22 participated in Scenario 2. The first 8 participants had to be discarded from the analysis of Scenario 2 because of modifications (see the Operational Scenario section for details). Participants received either course credit or monetary compensation. All served for at least 3 years in the IDF as combat soldiers and were on active reserve duty in the past year prior to the experiment. All had infantry training with various levels of navigation skills but no experience in operating UVs. Age range was 23 to 29 years. <xref ref-type="table" rid="table1-1555343412445054">Table 1</xref> rates participants’ military-related skills.</p>
<table-wrap id="table1-1555343412445054" position="float">
<label>Table 1:</label>
<caption><p>Rated Skill Level of Participants (<italic>n</italic>)</p></caption>
<graphic alternate-form-of="table1-1555343412445054" xlink:href="10.1177_1555343412445054-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="4">Skill Level<hr/></th>
</tr>
<tr>
<th align="left">Skill</th>
<th align="center">None</th>
<th align="center">Beginner</th>
<th align="center">Intermediate</th>
<th align="center">Expert</th>
</tr>
</thead>
<tbody>
<tr>
<td>Using computerized military systems</td>
<td>2</td>
<td>8</td>
<td>11</td>
<td>9</td>
</tr>
<tr>
<td>Deciphering maps and orientation</td>
<td/>
<td/>
<td>5</td>
<td>25</td>
</tr>
<tr>
<td>Land navigation</td>
<td/>
<td/>
<td>4</td>
<td>26</td>
</tr>
<tr>
<td>Target detection and identification</td>
<td>2</td>
<td>2</td>
<td>14</td>
<td>12</td>
</tr>
<tr>
<td>Execution of military tasks on the basis of information from unmanned aerial vehicles</td>
<td>9</td>
<td>16</td>
<td>5</td>
<td/>
</tr>
<tr>
<td>Execution of military tasks on the basis of information from unmanned ground vehicles</td>
<td>30</td>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section5-1555343412445054">
<title>Experimental System</title>
<p>A dedicated test bed consisting of a single screen divided into areas was developed, as shown in <xref ref-type="fig" rid="fig2-1555343412445054">Figure 2</xref>. It included the following components: (a) single or dual video feed windows from either an UAV or both UAV and UGV, depending on the experimental configuration, and (b) an aerial map of the conflict area, input buttons, input field, and status indicators. Buttons, input feed, and indicators are detailed in <xref ref-type="fig" rid="fig3-1555343412445054">Figure 3</xref>. Marked keys on the keyboard were assigned to each one of the input buttons, and input feed was entered via the number pad section of the keyboard.</p>
<fig id="fig2-1555343412445054" position="float">
<label>Figure 2.</label>
<caption><p>The experimental system’s interface. Left: Unmanned aerial and ground vehicle video feed windows. Right: Aerial map marking the planned route of the unmanned vehicles and the waypoint numbers (numbered rectangles) along the route. Bottom: Buttons, input feed, and status indicators.</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig2.tif"/>
</fig>
<fig id="fig3-1555343412445054" position="float">
<label>Figure 3.</label>
<caption><p>Status indicators, feeds, and buttons in the experimental system.</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig3.tif"/>
</fig>
</sec>
<sec id="section6-1555343412445054">
<title>Equipment</title>
<sec id="section7-1555343412445054">
<title>Eye tracking</title>
<p>Participants’ visual scan patterns were recorded via an eye-tracking system (ETS) (Applied System Laboratories, Remote Model D6), sampling the visual gaze at 60 Hz, with a nominal accuracy of 1°. Participants sat at an average distance of 65 cm, creating a visual angle of 35° horizontally and 22° vertically from the display.</p>
</sec>
<sec id="section8-1555343412445054">
<title>Participants’ station</title>
<p>Participants sat in a partly dimmed laboratory in front of a screen 20 in. wide while connected to the ETS. The screen area taken by the experimental system was smaller, 30 × 22 cm (generating a diagonal of 15 in.).</p>
</sec>
</sec>
<sec id="section9-1555343412445054">
<title>Operational Scenarios</title>
<p>Simulated video feeds from UVs were generated in two steps. The first step was staging. In this step, we placed specific objects (termed <italic>elements</italic>, e.g., vehicles and suspects) in a 3-dimensional model of a densely built Middle Eastern city using the VRForces™ structure. Then, the second step was recording. The scenario was filmed from the perspective of the appropriate UV.</p>
<sec id="section10-1555343412445054">
<title>Staging of Scenario 1 (no zooming)</title>
<p>This scenario presented a situation in which the UAV was flying a predetermined reconnaissance route along main roads of an urban area at relatively high altitude while not zooming on any specific targets along the way. The role of the UGV, which followed the same route, was then to focus on specific targets along the route and gather more information. This scenario was similar to the one used in <xref ref-type="bibr" rid="bibr29-1555343412445054">Oron-Gilad et al. (2011)</xref>, with the exception that here, the UAV did not zoom on elements along the route. Throughout the route, the following elements appeared in a randomly generated order: six waypoints (WPs) marked with the use of a 3-dimensional polygon (WP model), two moving elements of suspects, and four static elements of parked vehicles.</p>
</sec>
<sec id="section11-1555343412445054">
<title>Staging Scenario 2 (open vs</title>
<p><italic>closed densely built urban areas).</italic> In Scenario 2, the UAV and UGV also followed a planned route. This time, the route did not always follow main roads and included also more densely built areas, where the UAV experienced occasional incidents of concealment. The UGV, which also followed the same route, was expected to cover for these concealed areas. The route was divided into two terrain characteristics: a more open urban area and a more densely built district with narrow alleys; in the open urban area, the UVs followed a suspect’s car (this area resembled the route in Scenario 1). See <xref ref-type="fig" rid="fig4-1555343412445054">Figure 4</xref> for terrain characteristics samples. Throughout the route, the following elements appeared in a randomly generated order: four WP models, three moving elements of suspects, and two static elements of parked vehicles. In the open area, there were very few incidents of concealments. The second area was a closed, densely built district with narrow alleys. In this area, the UVs followed the suspect, who was now running by foot in the narrow passages. This route included two WP models, two moving elements of suspects, and two static elements of parked vehicles. In the closed area, there were many incidents of concealment for the UAV, and in some cases, it was almost impossible to follow the suspect. Furthermore, in the closed area, the actual field of view provided by the UGV was narrower than in the open area because of blockage from the buildings on both sides of the alley, as shown in <xref ref-type="fig" rid="fig4-1555343412445054">Figure 4</xref>.</p>
<fig id="fig4-1555343412445054" position="float">
<label>Figure 4.</label>
<caption><p>Terrain characteristics in Scenario 2 from the perspective of the unmanned aerial vehicle (UAV; top) and unmanned ground vehicle (bottom). Left: The open urban area where the unmanned vehicles are following a suspicious blue vehicle (marked for emphasis with an ellipse). Right: The closed, more densely built urban area where the unmanned vehicles are following a suspect running in the alleys (in the UAV image, the suspect is emphasized with an ellipse).</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig4.tif"/>
</fig>
<p>Filming of UV feed videos. For each scenario, two movies were created, one from the perspective of the UAV and one from the perspective of the UGV. The simulated UVs resembled typical UVs used by the military: a tactical UAV with flight altitude of 1,000 feet, flight velocity of 100 knots, and radial length of 500 m and a UGV with image taken (de facto) by a warrior model (first-person shooter) in which the user sees the world from the warrior’s perspective and is able to control both the warrior movement and direction of view, with a moving altitude of 1.8 m above ground and moving velocity of 10 km/h. All videos were recorded and saved. In the UAV feed videos, to ensure that participants noticed the WP model, the movies were edited. WP models were highlighted with a red pointer for the first 2 s of their appearance, as shown in <xref ref-type="fig" rid="fig5-1555343412445054">Figure 5</xref>. Initially, this edit was made only for Scenario 1, in which it was evident that it was difficult to detect the WP model because of the no-zooming flight path policy of the simulated UAV. After running the first 8 participants, it was noticed that several participants in Scenario 2 also had difficulties in identifying the WP models, and red pointers were edited into Scenario 2 as well. The duration of the scenarios was 6.5 min and 7.5 min, respectively, for Scenarios 1 and 2.</p>
<fig id="fig5-1555343412445054" position="float">
<label>Figure 5.</label>
<caption><p>An example of the waypoint (WP) model as seen in the unmanned aerial vehicle (UAV) feed. Left: With the added red pointer (that appeared for the first 2 s of WP model appearance). Right: Without the red pointer. Note that the UAV is following the path of the blue vehicle.</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig5.tif"/>
</fig>
<p>In addition to the difference in terrain characteristics and the operational situation (no zooming or incidents of concealment, respectively), Scenario 1 and Scenario 2 also differed in the quality of the video feed provided. Scenario 2 was recorded in a more advanced version of the VRForces simulation structure, which allowed for higher-quality video as reflected by the clarity of simulation. In addition, we presented Scenario 1 as a black-and-white video to maintain conditions similar to those in <xref ref-type="bibr" rid="bibr29-1555343412445054">Oron-Gilad et al. (2011)</xref>. Although both scenarios are valid in terms of the operational context, these differences should be acknowledged. It should be noted that our goal was not to compare between the two scenarios, as the spectrum of scenario possibilities is infinite, but rather to provide a thorough examination of two different operational situations to gain more understanding of the contribution and cost of UAV and UGV video feeds to overall mission performance.</p>
</sec>
</sec>
<sec id="section12-1555343412445054">
<title>Operational Tasks</title>
<p>While observing the scenarios, participants were required to perform three types of tasks: (a) Vehicle identification involved examining a cluster of parked vehicles (see <xref ref-type="fig" rid="fig6-1555343412445054">Figure 6</xref>, center). First, the participants had to identify the cluster of parked vehicles (i.e., press the input button that represented vehicles) and then decipher the number of vehicles in the cluster and specify it in the input feed (e.g., press the <italic>3</italic> number key for three vehicles in the cluster). (b) Suspect identification involved identifying moving suspects (see <xref ref-type="fig" rid="fig6-1555343412445054">Figure 6</xref>, right). When moving suspects appeared, participants had to press the “Suspect” button while ignoring other moving elements, such as camels, which were occasionally placed in the scenario as distractions (i.e., false alarms). In case participants were not sure what they had seen, they were instructed to press the question-mark button. When suspects were detected, the participants were instructed to decipher the number of suspects identified and to specify the number in the input field. (c) The orientation task involved correlating a WP model in the video (see <xref ref-type="fig" rid="fig6-1555343412445054">Figure 6</xref>, left) with a WP number on the map. WPs were marked on the map with numeric values. To make the orientation task more complex, we included more numbered WPs on the map than WP models in the video. The planned route of the UVs was shown on the map as a white line connecting all WPs, as shown in <xref ref-type="fig" rid="fig2-1555343412445054">Figure 2</xref>. Participants had to press the “WP” button and then determine which WP on the map was associated with the WP model in the video. The orientation task required the use of video feed and aerial map simultaneously.</p>
<fig id="fig6-1555343412445054" position="float">
<label>Figure 6.</label>
<caption><p>Elements in the scenario (waypoint models, vehicles, and moving suspects) from the viewpoint of the ground vehicle (top) and the aerial vehicle (bottom).</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig6.tif"/>
</fig>
<sec id="section13-1555343412445054">
<title>Measuring method</title>
<p>Qualitative performance measures were scored according to a predefined scoring index (for each task, the scoring was different and is explained in detail in the Results section). <italic>Questionnaires and subjective measures.</italic> Mental workload was assessed with the raw NASA Task Load Index (RTLX; <xref ref-type="bibr" rid="bibr20-1555343412445054">Hart &amp; Staveland, 1988</xref>), an unweighted average of the subscale values on a scale of 1 to 9 (with 9 being the highest). A subjective questionnaire evaluated the difficulty level of each task by interface configuration. The Spatial Abilities Cube test (SpA; <xref ref-type="bibr" rid="bibr14-1555343412445054">Ekstrom, French, &amp; Harman, 1976</xref>) was administered at the beginning of the experiment. The SpA has been reported previously as a significant predictor for target search tasks using unmanned vehicles (<xref ref-type="bibr" rid="bibr9-1555343412445054">Chen et al., 2008</xref>; <xref ref-type="bibr" rid="bibr7-1555343412445054">Chen &amp; Clark, 2008</xref>; <xref ref-type="bibr" rid="bibr17-1555343412445054">Fincannon, Ososky, Jentsch, Keebler, &amp; Phillips, 2010</xref>), UAV route replanting tasks (<xref ref-type="bibr" rid="bibr10-1555343412445054">Cook, Smallman, Lacson, &amp; Manes, 2010</xref>), and robotics control task performance (<xref ref-type="bibr" rid="bibr2-1555343412445054">Cassenti, Kelley, Swoboda, &amp; Patton, 2009</xref>; <xref ref-type="bibr" rid="bibr21-1555343412445054">Lathan &amp; Tracey, 2002</xref>; <xref ref-type="bibr" rid="bibr23-1555343412445054">Menchaca-Brandan, Liu, Oman, &amp; Natapoff, 2007</xref>). Recently, <xref ref-type="bibr" rid="bibr6-1555343412445054">Chappelle, Novy, Randall, and McDonald (2010)</xref> indicated that spatial ability was one of the main skills that should be taken into account when selecting personnel to operate UAVs.</p>
</sec>
</sec>
<sec id="section14-1555343412445054">
<title>Procedure</title>
<p>Each participant was invited separately to the Eye-Movements Laboratory. After briefing, SpA administration, and form filling, we executed a training scenario for both conditions (UAV only and UAV and UGV simultaneously). After basic understanding of the mission was accomplished and the ETS calibrated, the experiment began. Each participant underwent four trials. The first two displayed video feed from UAV only (Scenario 1 and Scenario 2, respectively), and the last two trials displayed video feed from UAV and UGV simultaneously. The order of the scenarios was counterbalanced among participants, but the combined view always followed the single view, as specified earlier. Workload and subjective questionnaires were administered between trials and after the last trial. A short briefing was conducted after completion.</p>
</sec>
</sec>
<sec id="section15-1555343412445054" sec-type="results">
<title>Results</title>
<p>The results reported here explore effects of presenting video feeds from both UAV and UGV as two windows in one interface in comparison to presenting only the single UAV feed. The objective variable analyses focused on two aspects of the operational mission: orientation and identification of elements (parked vehicles and moving suspects). In addition, workload and subjective estimates were examined. Finally, participants’ eye movement patterns were analyzed. In Scenario 1, objective and subjective performance measures were collected from 30 participants and eye movement measurements were obtained from 15 participants. In Scenario 2, objective and subjective performance measures were collected from 22 participants and eye movement measurements were collected from 13 participants. All analyses were conducted at a .05 significance level. Results for each scenario are presented separately, although the methods of analysis were the same. Eye-tracking analysis for Scenario 1 is detailed in <xref ref-type="bibr" rid="bibr28-1555343412445054">Ophir, Oron-Gilad, Borowsky, and Parmet (2011)</xref> and therefore is excluded from the analyses.</p>
<sec id="section16-1555343412445054">
<title>SpA</title>
<p>On the basis of their scores on the SpA, participants were categorized into one of three levels (low, medium, or high). The overall range of scores for this test is between −10 and 10. The low group included 8 participants with scores between −3 and 0, the medium group included 17 participants with scores between 1 and 5, and the high group included 5 participants whose scores were greater than 6. This categorization was used in the performance analysis as a predicting factor.</p>
</sec>
<sec id="section17-1555343412445054">
<title>Performance in the Orientation Task</title>
<sec id="section18-1555343412445054">
<title>Analysis model</title>
<p>In the orientation task, participants had to identify a WP model in the video feeds and mark its accurate location on the map. The dependent variable (score) was multinomial distributed. A score of 0 was given when a WP model was not marked or was marked incorrectly, a score of 1 was given when a WP model was marked incorrectly but deviated from the correct WP by one WP on the planned route, and finally, a score of 2 was given when a WP model was marked correctly. A cumulative logit model for ordinal responses (<xref ref-type="bibr" rid="bibr1-1555343412445054">Agresti, 2002</xref>) was used for analysis since this model is appropriate for multinomial distributed data. Because of the within-subject design of the experiment, the cumulative logit model was analyzed within the generalized estimation equations (GEE) framework, including the fixed effects and one random effect, which accounted for individual differences among participants.</p>
</sec>
<sec id="section19-1555343412445054">
<title>Scenario 1</title>
<p>The fixed effects included the interface configuration (two: UAV or UAV+ UGV) and SpA scores (three: 1, low; 2, medium; 3, high). In addition, the second-order interaction was included in the model. Applying a backward elimination procedure revealed that only the main effect of interface configuration was statistically significant, Wald χ<sup>2</sup>(1) = 23.027, <italic>p</italic> &lt; .01. The estimated probabilities to orient a WP—that is, scores 0, 1, and 2—were 0.4, 0.2, and 0.4, respectively, for the UAV feed alone and 0.2, 0.2, and 0.6 for the combined condition. Thus, contrary to the initial hypothesis, the combined configuration produced more accurate orientation than did the single UAV video feed. Importantly, the likelihood to associate the WP model correctly with the map was 50% higher (from 40% to 60%) in the combined configuration. Note also that in <xref ref-type="bibr" rid="bibr29-1555343412445054">Oron-Gilad et al. (2011)</xref>, the estimated probabilities to orient a WP model with UAV feed alone were 0.38, 0.14, and 0.48 for scores 0, 1, and 2, respectively, quite similar to the ones reported here for the UAV alone.</p>
</sec>
<sec id="section20-1555343412445054">
<title>Scenario 2</title>
<p>The fixed effects included the interface configuration (two: UAV or UAV+ UGV), the type of area that the UVs were scanning, that is, terrain characteristics (two: open or closed), and SpA (three). In addition, the second-order interaction Interface Configuration × Terrain Characteristics was included in the model. Applying a backward elimination procedure revealed that only the main effect of terrain characteristics was statistically significant, Wald χ<sup>2</sup>(1) = 29.037, <italic>p</italic> &lt; .01.</p>
<p>The estimated probabilities to orient a WP—that is, scores 0, 1, and 2—were 0.1, 0.15, and 0.75, respectively, for the open area and 0.3, 0.25, and 0.45, respectively, for the closed area. In line with the initial hypothesis, the combined configuration did not facilitate more accurate orientation than did the single UAV feed. The only parameter that influenced the probabilities to associate the WP model correctly was terrain characteristics, and as expected, it was harder to orient the WP model correctly in the dense area (75% vs. 45%). Indeed, in densely built areas where concealment incidents happen, the utility of the UAV was lower than in the open area. The lower availability of “good” UAV data in such areas affected orientation and further enhances the importance of the UAV feed for orienting.</p>
</sec>
</sec>
<sec id="section21-1555343412445054">
<title>Identification of Elements</title>
<sec id="section22-1555343412445054">
<title>Analysis model</title>
<p>In the identification task, participants had to identify either parked vehicles or moving suspects. Given that a participant identified the element, he had to decipher the number of vehicles or suspects present in the scene. As in the former analysis, the dependent variable (score) was multinomial distributed with ordered categories. A score of −1 was given when an element was not identified at all. A score of 0 was given when an element was detected correctly but the number of elements specified deviated from the correct number of elements by more than one. A score of 1 was given when an element was identified but the specified number of elements deviated by one, and finally, a score of 2 was given when a correct identification of the number of suspicious elements was given. As in the previous analysis, we applied a cumulative logit model for ordinal response within the GEE framework using the backwards elimination procedure.</p>
</sec>
<sec id="section23-1555343412445054">
<title>Scenario 1</title>
<p>The fixed effects were the same two variables as in the orientation task, with an additional variable, element type, indicating the type of element (two: vehicles or suspects). All second-order interactions were included in the model except the interactions with SpA. The final model revealed that the main effect of interface configuration and the main effect of element type were both significant, Wald χ<sup>2</sup>(1) = 197.712, <italic>p</italic> &lt; .01, and Wald χ<sup>2</sup>(1) = 60.837, <italic>p</italic> &lt; .01, respectively. <xref ref-type="table" rid="table2-1555343412445054">Table 2</xref> presents the estimated probabilities to identify suspicious elements by the two main effects. As <xref ref-type="table" rid="table2-1555343412445054">Table 2</xref> demonstrates, the estimated probability to identify suspicious elements correctly (column 5, score 2) was higher in the combined configuration, and vehicles were identified more easily than suspects for both configurations. The latter result is not surprising because vehicles are larger elements.</p>
<table-wrap id="table2-1555343412445054" position="float">
<label>Table 2:</label>
<caption><p>Estimated Probabilities in the Identification Task of Scenario 1</p></caption>
<graphic alternate-form-of="table2-1555343412445054" xlink:href="10.1177_1555343412445054-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Main Effect</th>
<th align="center">−1</th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">2</th>
</tr>
</thead>
<tbody>
<tr>
<td>UAV</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Suspects</td>
<td>.84</td>
<td>.10</td>
<td>.04</td>
<td>.02</td>
</tr>
<tr>
<td> Vehicles</td>
<td>.56</td>
<td>.23</td>
<td>.13</td>
<td>.08</td>
</tr>
<tr>
<td>UAV+UGV</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td> Suspects</td>
<td>.17</td>
<td>.20</td>
<td>.28</td>
<td>.34</td>
</tr>
<tr>
<td> Vehicles</td>
<td>.05</td>
<td>.08</td>
<td>.19</td>
<td>.69</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1555343412445054">
<p><italic>Note</italic>. The values represent the estimated probabilities for each condition according to the final ordinal regression model. Scoring: –1 = unidentified; 0 = element identified but number of elements not correctly specified; 1 = element identified but number of elements specified was lower or higher by 1 than correct number of elements; 2 = correctly identified. UAV = unmanned aerial vehicle; UGV = unmanned ground vehicle.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section24-1555343412445054">
<title>Scenario 2</title>
<p>As in Scenario 1, the independent effects included were the same as in the orientation task, with the additional effect element type. All second-order interactions were included into the model except the interactions with SpA. The final model revealed that the main effect of interface configuration and all second-order interactions were significant (<xref ref-type="table" rid="table3-1555343412445054">Table 3</xref>). <xref ref-type="fig" rid="fig7-1555343412445054">Figure 7</xref> presents the estimated probabilities to identify suspicious elements (suspects or vehicles) by the two main effects, interface configuration and terrain characteristics. As <xref ref-type="fig" rid="fig7-1555343412445054">Figure 7</xref> demonstrates, the estimated probability to identify suspects correctly was higher in the combined configuration, and the estimated likelihood to identify them correctly was identical regardless of terrain characteristics (74%). On the other hand, the likelihood to identify suspicious vehicles correctly was higher for the combined configuration and much higher in the open area than in the closed area (75% and 48%, respectively). Interestingly, in the combined configuration, the estimated probability to identify moving suspects in the closed area was higher than the likelihood to identify the number of parked vehicles (75% and 48%, respectively). Although this finding may seem a bit surprising (since parked vehicles are bigger and supposedly easier to identify), it is possible that because of incidents of concealment, it may have been difficult to identify the correct number of vehicles in the cluster. Furthermore, in the closed area, the UGV provided a relatively narrow angle of view because of obstructions from the walls of near buildings; as such, it may have not facilitated a good view of all vehicles in the cluster as clearly as in the open area, as demonstrated in <xref ref-type="fig" rid="fig8-1555343412445054">Figure 8</xref>.</p>
<table-wrap id="table3-1555343412445054" position="float">
<label>Table 3:</label>
<caption><p>Fixed Effects Found in the Identification Task of Scenario 2</p></caption>
<graphic alternate-form-of="table3-1555343412445054" xlink:href="10.1177_1555343412445054-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Main Effect</th>
<th align="center">Wald χ<sup>2</sup>(<italic>df</italic>)</th>
<th align="center"><italic>p</italic> value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Interface configuration (IC)</td>
<td>188.2(1)</td>
<td>.001</td>
</tr>
<tr>
<td>Terrain characteristics (TC)</td>
<td>0.1(1)</td>
<td>.900</td>
</tr>
<tr>
<td>Element type (ET)</td>
<td>1.5(1)</td>
<td>.230</td>
</tr>
<tr>
<td>IC*TC</td>
<td>8.3(1)</td>
<td>.004</td>
</tr>
<tr>
<td>IC*ET</td>
<td>21.9(1)</td>
<td>.001</td>
</tr>
<tr>
<td>TC*ET</td>
<td>14.3(1)</td>
<td>.001</td>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="fig7-1555343412445054" position="float">
<label>Figure 7.</label>
<caption><p>Estimated probabilities to identify suspicious elements in Scenario 2 (top: suspects; bottom: vehicles), with regard to interface configuration and terrain characteristics (left: closed, densely built area; right: open urban area).</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig7.tif"/>
</fig>
<fig id="fig8-1555343412445054" position="float">
<label>Figure 8.</label>
<caption><p>An example of parked cars as seen in the unmanned aerial vehicle (left) and unmanned ground vehicle (UGV; right) feed. Top: Closed area. The unmanned vehicles (UVs) are following a walking suspect. Note that the buildings obscure the UGV’s field of view of the parked vehicles. Bottom: Open urban area. The UVs are following a blue vehicle, which is driving the suspect. Parked vehicles appear to its left and are not obscured.</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig8.tif"/>
</fig>
</sec>
</sec>
<sec id="section25-1555343412445054">
<title>False Alarms</title>
<sec id="section26-1555343412445054">
<title>Analysis model</title>
<p>This variable measured the number of times participants erroneously pressed one of the buttons (“Vehicles,” “Suspects,” “WP,” or “?”) when no apparent element appeared in the scene at the time of press. Alternatively, the object that appeared in the scene was not defined as a suspicious element that needed to be registered (e.g., passing camels). Since the dependent variable expresses the number of independent, nonrelated events occurring within a fixed period, we decided to use a Poisson regression model with fixed effects and participants as the random effect to account for the individual differences among participants.</p>
</sec>
<sec id="section27-1555343412445054">
<title>Scenario 1</title>
<p>The fixed effects included the interface configuration and SpA. Applying a backward elimination procedure revealed that only the interface configuration was statistically significant, Wald χ<sup>2</sup>(1) = 11.458, <italic>p</italic> &lt; .01. The average number of false alarms with the UAV feed alone was 3.8 (<italic>SE</italic> = 0.6), whereas the average number of false alarms in the combined condition was lower, 2.3 (<italic>SE</italic> = 0.4).</p>
</sec>
<sec id="section28-1555343412445054">
<title>Scenario 2</title>
<p>The fixed effects included the interface configuration and SpA. Applying a backward elimination procedure revealed a significant main effect for interface configuration, Wald χ<sup>2</sup>(1) = 8.516, <italic>p</italic> &lt; .004. The average number of false alarms for the single UAV feed was 2.7 (<italic>SE</italic> = 0.4), whereas the average number of false alarms in the combined configuration was 1.5 (<italic>SE</italic> = 0.3). Thus, in both scenarios, the addition of UGV feed decreased the number of false reports. Although Scenario 2 was 1 min longer and more diverse in the type of terrain characteristics covered, the average number of false alarms was smaller for both the single and combined configurations than in Scenario 1. This finding most likely stems from the differences in the video quality of the two scenarios and reflects the overall sensitivity of performance to video feed quality.</p>
</sec>
</sec>
<sec id="section29-1555343412445054">
<title>Workload Analysis</title>
<sec id="section30-1555343412445054">
<title>Analysis model</title>
<p>Mental workload was calculated as a global RTLX score and for each separate scale, all on a rating scale of 1 to 9. Since the dependent variable was consecutive, a linear mix model (LMM) with random effect of participants and interface configuration as the independent variable was used. Bonferroni corrections were made to account for the multiple comparisons among scales.</p>
</sec>
<sec id="section31-1555343412445054">
<title>Scenario 1</title>
<p>Interface configuration was statistically significant, <italic>F</italic>(1, 28) = 5.65, <italic>p</italic> &lt; .024, for the global score. The UAV-only configuration resulted in higher workload ratings, with means of 5.6 (<italic>SE</italic> = 0.2) and 5.2 (<italic>SE</italic> = 0.2), respectively, for the single and combined configurations. Separate analysis for each of the six NASA-TLX items revealed that interface configuration was statistically significant only with regard to the own-performance assessment (<italic>p</italic> &lt; .004). In the single configuration, participants rated their own performance as 35% lower (3.5, <italic>SE</italic> = 0.3, vs. 4.7, <italic>SE</italic> = 0.3, respectively). Thus, participants felt that they performed significantly better in the combined configuration, although both ratings were far from perfect performance (38% and 52%), reflecting that in general, participants were aware of their difficulties in performing the mission successfully.</p>
</sec>
<sec id="section32-1555343412445054">
<title>Scenario 2</title>
<p>Interface configuration was statistically significant, <italic>F</italic>(1, 21) = 5.25, <italic>p</italic> &lt; .01, for the global score. As in Scenario 1, the single configuration resulted in higher workload ratings (<italic>M</italic> = 5.3, <italic>SE</italic> = 0.2) than did the combined configuration (<italic>M</italic> = 4.9, <italic>SE</italic> = 0.2). Separate analysis of each item alone revealed that the difference between the two configurations was statistically significant only for mental demand (<italic>p</italic> &lt; .006). Participants felt that the UAV-only configuration was more mentally demanding (<italic>M</italic> = 7.5, <italic>SE</italic> = 0.2) than the combined configuration (<italic>M</italic> = 6.9, <italic>SE</italic> = 0.2).</p>
</sec>
</sec>
<sec id="section33-1555343412445054">
<title>Subjective Task-Related Questionnaire</title>
<sec id="section34-1555343412445054">
<title>Analysis model</title>
<p>Participants were asked to rate the difficulty of each task type (identification of suspects and parked vehicles, orientation) and the difficulty of the entire mission in general on a scale of 1 to 10, with 10 being <italic>very difficult</italic>. An LMM was used for the analysis with a .05 significance level. The fixed effect was the interface configuration. The random effect of participants was included to account for the variance between individual participants.</p>
</sec>
<sec id="section35-1555343412445054">
<title>Scenario 1</title>
<p>The main effect for interface configuration was statistically significant for all three task types and the entire mission ratings (as shown in <xref ref-type="table" rid="table4-1555343412445054">Table 4</xref>). Participants’ subjective evaluation of task difficulty corresponded with their operational performance; thus performing the tasks in the UAV-only configuration was perceived as significantly harder.</p>
<table-wrap id="table4-1555343412445054" position="float">
<label>Table 4:</label>
<caption><p>Rate of Difficulty (1-10) for Each Task in Scenario 1</p></caption>
<graphic alternate-form-of="table4-1555343412445054" xlink:href="10.1177_1555343412445054-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Task</th>
<th align="center">Single Configuration</th>
<th align="center">Dual Configuration</th>
<th align="center">Difference <bold><italic>SE</italic></bold></th>
<th align="center"><bold><italic>p</italic></bold> value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Suspect identification</td>
<td>8.8</td>
<td>5.7</td>
<td>0.3</td>
<td>.01</td>
</tr>
<tr>
<td>Vehicle identification</td>
<td>7.2</td>
<td>4.1</td>
<td>0.3</td>
<td>.01</td>
</tr>
<tr>
<td>Orientation</td>
<td>6.5</td>
<td>5.7</td>
<td>0.3</td>
<td>.04</td>
</tr>
<tr>
<td>Entire mission</td>
<td>7.2</td>
<td>6.4</td>
<td>0.2</td>
<td>.02</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section36-1555343412445054">
<title>Scenario 2</title>
<p>The main effect for interface configuration was statistically significant only for the identification task (vehicles identification: single configuration, 5.8; dual configuration, 3.6; difference, <italic>SE</italic> = 0.4, <italic>p</italic> &lt; .01; suspects identification: single configuration, 7.2; dual configuration, 4.8; difference, <italic>SE</italic> = 0.4, <italic>p</italic> &lt; .01). Orientation task ratings did not differ between the two configurations. These findings correlate with the objective operational results, which showed that the combined configuration facilitated performance in the identification tasks but not in the orientation task.</p>
</sec>
</sec>
</sec>
<sec id="section37-1555343412445054">
<title>Attention Allocation (Eye-Tracking) Analysis</title>
<p>Eye movement analysis was used to investigate how participants allocated their attention while engaged in the operational tasks in both the single and dual configurations. To achieve this goal, we calculated several eye movement measures, and three AOIs were defined: the aerial (MAP) window, the UAV window, and the UGV window were defined as rectangles surrounding the areas where these videos (or images) were located on the display. All eye movement measures were based on fixations that were calculated according to a dispersion methodology applied by <xref ref-type="bibr" rid="bibr18-1555343412445054">Gitelman (2002)</xref> via ILAB. The dispersion algorithm has three parameters: minimum fixation duration (in milliseconds), minimum dispersion considered a fixation (in degrees), and maximum consecutive sample loss, set to 100 ms, 1 visual degree, and infinity (default), respectively. Each fixation location was assigned to one of the three AOIs or to a fourth AOI that represented all other locations not falling within the three predefined AOIs.</p>
<p>This section focuses on the analysis of Scenario 2, which was the more complex and abundant scenario in terms of terrain characteristics. Findings regarding Scenario 1 are detailed in <xref ref-type="bibr" rid="bibr28-1555343412445054">Ophir et al. (2011)</xref>. In addition, Scenario 1 was subjectively evaluated by participants as less clear than Scenario 2; thus, participants’ attention allocation patterns were expected to be more homogenous in Scenario 2.</p>
<sec id="section38-1555343412445054">
<title>Mean Dwell Duration (MDD)</title>
<p>The first eye movement measure (dependent variable) is based on dwells (i.e., the duration of a sequence of consecutive fixations falling inside an AOI) on each AOI. MDD is the average duration of dwells falling inside an AOI. Since it is not normally distributed, it was log transformed. Then, LMM including a backward elimination procedure was used. The independent variables included AOI type (three: UAV, UGV, or MAP) and terrain characteristics (two: open or closed). Participants were included as a random effect to account for individual differences among participants. All main effects and the interaction between AOI type and terrain characteristics were included in the model. To compare participants’ performance on a single-channel configuration as a baseline versus a dual-channel configuration, we conducted the same analysis for each configuration separately.</p>
<p>Applying backward elimination on the single-channel configuration revealed main effects for AOI type, <italic>F</italic>(1, 3276) = 9.24, <italic>p</italic> &lt; .001, and terrain characteristics, <italic>F</italic>(1, 3276) = 222.14, <italic>p</italic> &lt; .002. In addition, the interaction between terrain characteristics and AOI type was significant, <italic>F</italic>(1, 3276) = 7.35, <italic>p</italic> &lt; .007. The significant interaction warranted further analysis. Post hoc analysis of the interaction using the Bonferroni correction revealed that the MDD on the UAV was not significantly different for the open and closed areas (MDD = 753, confidence interval [CI] = ±82 ms, and MDD = 760, CI = ±89 ms, respectively), but the MDD on the MAP was significantly shorter in the open area than in the closed area (MDD = 445, CI = ±52 ms, and MDD = 528, CI = ±67 ms, respectively), <italic>p</italic> &lt; .001.</p>
<p>Applying a backward elimination procedure on the dual channel revealed that the main effect of AOI type, <italic>F</italic>(2, 5974) = 5.58, <italic>p</italic> &lt; .004, and the interaction between terrain characteristics and AOI type were statistically significant, <italic>F</italic>(2, 5974) = 9, <italic>p</italic> &lt; .001. Post hoc analysis on the interaction using the Bonferroni correction revealed that the MDD on the UAV was not significantly different between the open and closed areas (MDD = 519 ms, CI [±46] and MDD = 517 ms [±49], respectively). The interaction between terrain characteristics and AOI type stems from MDD differences between MAP and UGV. That is, the MDD allocated to the UGV and MAP differed for open and closed areas (UGV open: MDD = 513 ms [±46], UGV closed: MDD = 459 ms [±45]; <italic>p</italic> &lt; .002; MAP open: MDD = 458 ms [±47], MAP closed: MDD = 507 ms [±51]; <italic>p</italic> &lt; .004). <xref ref-type="fig" rid="fig9-1555343412445054">Figure 9</xref> presents the distribution of estimated MDDs across AOIs within both types of terrain areas.</p>
<fig id="fig9-1555343412445054" position="float">
<label>Figure 9.</label>
<caption><p>Distribution of estimated mean dwell durations across areas of interest in closed and open urban areas in Scenario 2.</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig9.tif"/>
</fig>
<p>In <xref ref-type="fig" rid="fig9-1555343412445054">Figure 9</xref>, one may notice that although the MDD on the UAV does not change from open to closed areas (and vice versa), there is a trade-off pattern between MAP and UGV. That is, the MDD on the UGV was longer in the open area than in the closed area, and the opposite pattern was found for the MAP.</p>
</sec>
<sec id="section39-1555343412445054">
<title>Total Dwell Duration (TDD)</title>
<p>Next, we calculated the TDD that each participant allocated to each of the three AOIs in both closed and open areas. Since the open sections endured longer than the closed ones, each TDD was normalized according to its terrain characteristics. The independent variables included the AOI type and the terrain characteristics. All main effects and the second-order interaction between terrain characteristics and AOI type were included in the model. Participants were included as a random effect. As in the previous analysis, the TDD was log transformed to achieve normality. Applying LMM on the transformed TDD including a backward elimination procedure revealed that the main effect of AOI type, <italic>F</italic>(2, 102) =2.96, <italic>p</italic> &lt; .056, and the interaction between terrain characteristics and AOI type, <italic>F</italic>(2, 102) = 4.68, <italic>p</italic> &lt; .011, were significant. Post hoc analysis of the interaction using the Bonferroni correction revealed that the TDD on the UAV was not significantly different between open and closed areas (TDD = 7,435 ms [±4,581] and TDD = 7,519 ms [±5,264], respectively). The TDD for UGV and MAP, however, did show significant differences between closed and open areas. In fact, TDD was longer in open areas than in closed areas for UGV (TDD = 8,425 ms [±5,196] and TDD = 6,223 ms [±4,366], respectively, <italic>p</italic> &lt; .044), and shorter in open areas than in closed areas for MAP (TDD = 5,211 ms [±3,155] and TDD = 6,967 [±4,879], respectively, <italic>p</italic> &lt; .005). <xref ref-type="fig" rid="fig10-1555343412445054">Figure 10</xref> presents the distribution of estimated TDDs across AOIs within both types of terrain areas. According to <xref ref-type="fig" rid="fig10-1555343412445054">Figure 10</xref>, it appears that the trade-off pattern for MAP and UGV shown in the MDD was consistent for the TDD measure as well.</p>
<fig id="fig10-1555343412445054" position="float">
<label>Figure 10.</label>
<caption><p>The estimated total dwell durations across areas of interest in closed and open urban areas in Scenario two.</p></caption>
<graphic xlink:href="10.1177_1555343412445054-fig10.tif"/>
</fig>
</sec>
<sec id="section40-1555343412445054">
<title>Mean Number of Fixations in Each Dwell</title>
<p>Next, we examined whether the number of fixations composing each dwell varies as a function of terrain characteristics and AOI type. Since the dependent variable expresses the number of independent events occurring within a fixed period of time, we decided to use a Poisson regression model. The independent variables included AOI type and terrain characteristics. All main effects and the second-order interaction between terrain characteristics and AOI type were included in the model. Participants were included as random effect.</p>
<p>Applying a backward elimination procedure on the single configuration (UAV and MAP) revealed that the main effect for AOI type, Wald χ<sup>2</sup>(2) = 75.09, <italic>p</italic> &lt; .001, and the interaction between terrain characteristics and AOI type, Wald χ<sup>2</sup>(2) = 6.737, <italic>p</italic> &lt; .009, were statistically significant. Post hoc analysis of the interaction using the Bonferroni correction revealed that the mean number of fixations in each dwell on the UAV was not significantly different between open and closed areas (<italic>M</italic> = 6.7, <italic>SE</italic> = 0.8, and <italic>M</italic> = 6.2, <italic>SE</italic> = 0.7, respectively). The mean number of fixations on MAP, on the other hand, was different in open and closed areas (<italic>M</italic> = 3.5, <italic>SE</italic> = 0.2, and <italic>M</italic> = 3.9, <italic>SE</italic> = 0.3, respectively, <italic>p</italic> &lt; .001).</p>
<p>Applying a backward elimination procedure on the combined configuration (UAV, UGV and MAP) revealed a statistically significant interaction between terrain characteristics and AOI type, Wald χ<sup>2</sup>(2) = 31.785, <italic>p</italic> &lt; .01. Post hoc analysis of the interaction using the Bonferroni correction revealed that the mean number of fixations in each dwell on the UAV was not significantly different between open and closed areas (<italic>M</italic> = 3.8, <italic>SE</italic> = 0.2, and <italic>M</italic> = 3.9, <italic>SE</italic> = 0.2, respectively). Likewise, the mean number of fixations in each dwell on the UGV was not significantly different between open and closed areas (<italic>M</italic> = 4.5, <italic>SE</italic> = 0.3, and <italic>M</italic> = 3.9, <italic>SE</italic> = 0.3, respectively). Analyzing fixations pattern for MAP revealed that the mean number of fixations in open areas was significantly smaller than in closed areas (<italic>M</italic> = 3.6, <italic>SE</italic> = 0.3, and <italic>M</italic> = 4.1, <italic>SE</italic> = 0.4, respectively; <italic>p</italic> &lt; .003). An additional pairwise comparison between the UGV and MAP in open areas revealed that the mean number of fixations on the UGV channel was higher than on the MAP channel (<italic>p</italic> &lt; .021).</p>
</sec>
<sec id="section41-1555343412445054">
<title>Transition Among AOIs</title>
<p>Finally, we analyzed the number of transitions that each participant made from each AOI to another separately for closed and open areas (see <xref ref-type="table" rid="table5-1555343412445054">Table 5</xref>). We analyzed the number of transitions using LMM. The independent variables included transition direction (e.g., from UAV to UGV) and terrain characteristics. All main effects and the second-order interaction between transition direction and terrain characteristics were included into the model. Applying a LMM with backward elimination procedure revealed that the main effect of transition direction was marginally significant, <italic>F</italic>(5, 137) = 2.17, <italic>p</italic> &lt; .061. Applying post hoc using the Bonferroni correction revealed that the number of transitions from UGV to UAV was marginally larger than the number of transitions from UGV to MAP (44 vs. 29 transitions, <italic>p</italic> &lt; .08).</p>
<table-wrap id="table5-1555343412445054" position="float">
<label>Table 5.</label>
<caption><p>Average Number of Transitions During Scenario 2 From Each Area of Interest to Another</p></caption>
<graphic alternate-form-of="table5-1555343412445054" xlink:href="10.1177_1555343412445054-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">From → To</th>
<th align="center">Number of Transitions</th>
</tr>
</thead>
<tbody>
<tr>
<td>UGV → UAV</td>
<td>44</td>
</tr>
<tr>
<td>UAV → UGV</td>
<td>40</td>
</tr>
<tr>
<td>UAV → MAP</td>
<td>36</td>
</tr>
<tr>
<td>MAP → UAV</td>
<td>32</td>
</tr>
<tr>
<td>MAP → UGV</td>
<td>32</td>
</tr>
<tr>
<td>UGV → MAP</td>
<td>29</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1555343412445054">
<p><italic>Note</italic>. UGV = unmanned ground vehicle window; UAV = unmanned aerial vehicle window; MAP = aerial window.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section42-1555343412445054" sec-type="discussion">
<title>Discussion</title>
<p>In this study, we examined the added value of using two video feeds from both UAV and UGV simultaneously relative to using a single UAV feed. Two scenarios in which there could be potential added value for the addition of the UGV feed were developed on the basis of interviews with SMEs. Scenario 1 was a situation in which the UAV was flying a reconnaissance route at a relatively high altitude while not zooming in on any specific target along the way. The role of the UGV was then to focus on specific targets along the route and to gather more information. Scenario 2 was more complex and consisted of two types of urban areas: a more open urban area with wider streets and a closed, more densely built urban area with narrow alleys. In the closed area, the UAV experienced many incidents of concealment, and the role of the UGV was to provide the information that could not be retrieved well from the UAV.</p>
<sec id="section43-1555343412445054">
<title>Mission Performance, False Reports, and Mental Workload Assessment</title>
<p>In Scenario 1, performance scores in both the identification and orientation tasks were superior in the combined interface compared with the UAV feed alone, supporting the use of the additional UGV. These results are in line with Hypothesis 1 but contrary to Hypothesis 2. Surprisingly, the superiority of the combined display was evident even for the orientation task, although the literature predicts that it is the UAV that facilitates better orientation because of the similarity between the aerial maps and the “global picture” it provides (<xref ref-type="bibr" rid="bibr5-1555343412445054">Chadwick, 2008</xref>). On the other hand, these results are in line with previous findings of <xref ref-type="bibr" rid="bibr29-1555343412445054">Oron-Gilad et al. (2011)</xref>, which showed performance superiority for the UGV video feed (compared with UAV) in a similar scenario also for the orientation task. Furthermore, the number of false reports decreased when the UGV feed was present. Perceived workload was not increased by adding the UGV feed, and on the contrary, participants (correctly) felt that their own performance benefited from the second point of view. In line with Hypothesis 1 and Hypothesis 2, in Scenario 2, advantages for adding the UGV were observed only in the identification task. Performance scores in the orientation task did not gain from the additional UGV feed in the combined interface; however, the addition of the UGV feed did not harm performance in the orientation task. Thus, the combined view generated benefits with regard to the identification tasks, perceived mental demand, and reduction of false reports without having any apparent cost on participants. Subjective estimates of the task-related questionnaire on which participants were asked to rate the difficulty of each task type corresponded well with the objective findings, indicating that participants were well aware of the benefits provided by the additional UGV feed in both scenarios. <xref ref-type="table" rid="table6-1555343412445054">Table 6</xref> provides a summary of results from the perspective of the interface configuration.</p>
<table-wrap id="table6-1555343412445054" position="float">
<label>Table 6:</label>
<caption><p>Results From the Interface Configuration Perspective (i.e., Single vs. Combined) in Scenarios 1 and 2</p></caption>
<graphic alternate-form-of="table6-1555343412445054" xlink:href="10.1177_1555343412445054-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="center"/>
<col align="center"/>
</colgroup>
<thead>
<tr>
<th align="left">Variable</th>
<th align="center">Scenario 1 (No Zooming)</th>
<th align="center">Scenario 2 (Concealments)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID of suspects</td>
<td>√ Combined</td>
<td>√ Combined</td>
</tr>
<tr>
<td>ID of parked vehicles</td>
<td>√ Combined</td>
<td>√ Combined</td>
</tr>
<tr>
<td>Orientation</td>
<td>√ Combined</td>
<td>Not significant</td>
</tr>
<tr>
<td>False reports</td>
<td>√ Combined</td>
<td>√ Combined</td>
</tr>
<tr>
<td>Workload (RTLX)</td>
<td>√ Combined</td>
<td>√ Combined</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-1555343412445054">
<p><italic>Note</italic>. Significant differences between the single and combined configurations are marked with a check and the superior configuration is specified. RTLX = raw NASA Task Load Index (<xref ref-type="bibr" rid="bibr20-1555343412445054">Hart &amp; Staveland, 1988</xref>).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>The major differences found with regard to the orientation task between the two scenarios reinforces the importance of the quality of the UAV feed and of the sensitivity of operators to the conditions in which this feed was taken. Similarly, in <xref ref-type="bibr" rid="bibr24-1555343412445054">Minkov et al. (2010)</xref>, we have shown significant difference in mission performance between two types of UAV video feed. Nevertheless, on the basis of results obtained from both scenarios, one can presume that adding a UGV perspective to an existing UAV one is always beneficial.</p>
<p>Furthermore, with regard to Scenario 2, which was more complex and consisted of two types of urban areas, interesting patterns emerged regarding the utility of the additional UGV feed relative to the UAV feed alone. In line with the initial hypothesis, the combined configuration did not facilitate more accurate orientation than did the single UAV feed. The factor that most influenced the probabilities to associate a WP model correctly was terrain characteristics, and as expected, it was harder to orient correctly in the densely built area, where the UAV experienced more frequent incidents of concealment. Additionally, the identification task was also influenced by terrain characteristics. As expected, identification of elements in the closed, dense area was harder than in the open urban area. Nevertheless, in the closed area, it was easier to identify suspects than vehicles, since the UGV provided a relatively narrow angle of view because of the narrow alleys and buildings on the sides blocking its field of view.</p>
<p>A result worth noting that does not correspond with findings of others in the literature is that the SpA was not a significant predictor of performance in either orientation or identification tasks. Since most of the recent studies (e.g., <xref ref-type="bibr" rid="bibr9-1555343412445054">Chen et al., 2008</xref>; <xref ref-type="bibr" rid="bibr8-1555343412445054">Chen, Barnes, &amp; Qu, 2010</xref>; <xref ref-type="bibr" rid="bibr7-1555343412445054">Chen &amp; Clark, 2008</xref>) that found the SpA to be a good predictor of performance on target search tasks were conducted with operators of robotic assets and not with dismounted soldiers (i.e., passive information consumers), it is possible that other skill-related tests should be examined for dismounted soldiers.</p>
</sec>
<sec id="section44-1555343412445054">
<title>Attention Allocation (Eye Movement Analysis)</title>
<p>Analyzing participants’ eye movements revealed several attention-related patterns that provide complementary evidence to the performance and subjective analyses. We will first discuss the findings in detail with regard to Scenario 2 and then briefly discuss findings related to Scenario 1 (see also <xref ref-type="bibr" rid="bibr28-1555343412445054">Ophir et al., 2011</xref>). Finally, we will discuss the commonalities and differences among the two scenarios and their interpretation. Note that we cannot be certain that participants were indeed focusing their attention at the point of fixation; however, given stimulus duration and the relative size of the AOIs, it is likely that their attention direction and gaze are highly correlated.</p>
<p>With regard to Scenario 2, it seems that the orientation information gained from the UAV channel has reached its upper limit in both types of interface configurations. This conclusion is based on two major findings: (a) the fact that the amount of attention allocated to the UAV (in terms of either MDD, cumulative dwell duration, or number of fixations) was similar for open and closed areas and (b) that in the closed area (which, according to the performance results, produced lower orientation scores), participants allocated more attention to the aerial map at the expense of the UGV feed. The latter finding also suggests that the orientation task was difficult, because even when participants attempted to allocate more attention to the map, their performance did not improve in either one of the interface configurations. Second, it appears that participants gained more information from adding the UGV when the terrain included wide roads rather than narrow alleys. This claim is based on the trade-off found between the UGV and MAP in open and closed areas (recall that attention allocated to the UAV remained constant across terrains). That is, participants allocated more attention (on the basis of eye movement measures previously mentioned) to the UGV in open areas than in closed areas, whereas they allocated more attention to the map when the terrain included closed areas rather than open areas. In other words, when the terrain included closed areas, participants gained less information from attending to the UGV (probably because of the limited field of view it provided), and thus they chose to allocate more attention to the map in an attempt to improve their orientation performance. This pattern of behavior and the poorer objective results in the closed, densely built area are supported in part by findings of <xref ref-type="bibr" rid="bibr11-1555343412445054">Croft, Pittman, and Scialfa (2007)</xref>, who found that hit rates (in target detection and identification tasks) were higher when observers made many small-amplitude eye movements, that is, when ﬁxations were longer but not widely spaced (generating fewer transitions between areas). Thus, when the terrain included open areas, participants allocated more attention to the UGV, a strategy that improved both orientation and identification performance. Finally, the transition direction patterns among the various information channels revealed that after dwelling on the UGV, participants preferred to return to the UAV for confirmation rather than to attend to the map. This pattern emphasizes the complementary relationship between the two channels.</p>
<p>With regard to Scenario 2, it is important to note that as reflected in the objective performance data and in participants’ subjective estimates and comments, findings from Scenario 2 and Scenario 2 differed from each other with regard to the benefit of the UGV to the orientation task and did not show a consistent pattern. One possible explanation is that in Scenario 1, the information that the UAV provided was less coherent. In terms of attention allocation (see <xref ref-type="bibr" rid="bibr28-1555343412445054">Ophir et al., 2011</xref>), participants allocated more attention to the UGV (~44% of the time) than to the UAV (~31%) or MAP (~25%). The preference for the UGV may be explained by the similarity of the angle of view of the UGV feed to the common viewing angle soldiers have of the world and by their lack of experience with aerial views. Since this pattern differed from the one found in Scenario 2 (whereby equal attention was allocated to the UGV and the UAV), it may reflect the fact that the UAV was not providing sufficient information (probably because it was flying at a higher altitude without zooming in on elements of interest in the scenario), which caused participants to rely less on the UAV feed.</p>
<p>In sum, the difference in the attention allocation patterns between the two scenarios stems from the quantity and quality of information each UV feed was able to provide. As might be expected from the literature (<xref ref-type="bibr" rid="bibr30-1555343412445054">Owsley, Ball, Sloane, Roenker, &amp; Bruni, 1991</xref>), hit rates of searched-from-the-air ground targets were higher when both central and peripheral visual functions were good. Eye-tracking results demonstrate that attention was spread between the two UV feeds (as expected in such missions, less attention was allocated to the map), and unlike previous reports (e.g., <xref ref-type="bibr" rid="bibr37-1555343412445054">Thomas &amp; Wickens, 2000</xref>), in both scenarios, a cognitive tunneling effect, whereby participants focus only on a single AOI, was not observed. This finding reinforces that information provided by both feeds was complementary.</p>
</sec>
<sec id="section45-1555343412445054">
<title>Limitations</title>
<p>In this study, we examined two specific operational scenarios, yet the number of possible operational scenarios to evaluate is infinite. Although the two scenarios differed in image quality, flight pattern (reflecting the quantity of information), and terrain characteristics, consistent patterns emerged regarding the benefits of the combined configuration. Additional limitations are related to the experimental method and the conditions in which the study was conducted. Interface configurations were fixed and participants’ baseline was the single view of the UAV feed. It is difficult to assume that the significant amount of improvement observed for the combined condition resulted from practice effects, but given the design of the study, it is impossible to rule out the fact that at least some of the effect did occur because of improvements from familiarity with the experimental tasks. In addition, the task did not require participants to operate or communicate with the UV operators, which may have reduced their workload demands. Furthermore, the study was conducted in a quiet and controlled laboratory environment with adequate lighting and a 15-in. display. Those are improved conditions compared with the conditions dismounted soldiers will have in the field. In <xref ref-type="bibr" rid="bibr29-1555343412445054">Oron-Gilad et al. (2011)</xref>, we have shown that a 6- to 7-in. screen was sufficient to retrieve the necessary information from the map and UAV feed (i.e., a window of 3 to 4 in. is sufficient for a single video feed). Thus, the addition of another channel (UGV) will require a larger display than 6 to 7 in. but not necessarily 15 in.</p>
<p>Finally, one can argue about the practicality of operating UGVs in the operational context presented in both scenarios. In a hostile environment, it is less likely that the UGV will be able to follow a route or a suspicious vehicle or person without being harmed by hostile forces. In practice, it may be less realistic to have a UGV move in narrow alleys, and indeed, as shown in Scenario 2, its utility in such narrow areas is more limited. Nevertheless, since we have demonstrated that a ground view contributes to mission performance, it may be more practical to consider locating the UGV or other ground sensors at specific key locations in the combat environment, where its safety can be ensured. Such scenarios should be further investigated.</p>
</sec>
<sec id="section46-1555343412445054">
<title>Design Implications and Conclusions</title>
<p>In this study, we examined the added value of using video feeds from UAV and UGV simultaneously relative to using a single UAV feed. The strength of our findings are that regardless of the scenario, adding a UGV perspective to an existing UAV one was always beneficial in terms of mission performance, false alarm rates, and workload. Terrain characteristics may play a significant role in the decision whether to incorporate more video feed views. Specifically, as shown in Scenario 2, in which two types of terrain characteristics were examined, there was a trade-off between MAP and UGV feed as a function of the terrain characteristics; participants allocated more attention to the UGV AOI in open urban areas than in closed, densely built areas; and vice versa for the MAP AOI. Thus, in densely built areas, participants gained less from attending to the UGV feed (probably because of the limited field of view it provided). Furthermore, eye-tracking patterns indicated that participants related to information from both UAV and UGV feeds.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We would like to acknowledge the support of Lt. Colonel Einav Kiperman and Major Yaniv Minkov from the Israeli Defense Force (IDF) and the entire IDF Battle Lab team.</p>
</ack>
<fn-group>
<fn fn-type="supported-by">
<p>This work was supported by the U.S. Army Research Laboratory through the Micro-Analysis and Design CTA Grant DAAD19-01C0065, Task Order 113 (Michael Barnes, technical monitor). The views expressed in this work are those of the authors and do not reflect official army policy.</p>
</fn>
</fn-group>
<bio>
<p>Ronny Ophir-Arbelle is a human factors engineer. She holds a BSc and an MSc from Ben-Gurion University of the Negev. Currently, she researches human-machine aspects in military complex systems, specializing in unmanned aerial systems.</p>
<p>Tal Oron-Gilad is a senior lecturer in the Department of Industrial Engineering and Management at Ben-Gurion University of the Negev and the head of the Human Factors program. She holds a PhD in Human Factors Engineering from Ben-Gurion University (2003). Previously, she was a research associate at the University of Central Florida. She is the winner of the 2004 George E. Briggs Best Dissertation Award (American Psychological Association Division 21). Some of her current experimental work concerns the evaluation of tools and displays for dismounted soldiers and unmanned aerial vehicle operators, tactile information presentation, hazard perception, and situation awareness enhancement.</p>
<p>Avinoam Borowsky is currently a post doctoral researcher in the Department of Mechanical and Industrial Engineering at the University of Massachusetts Amherst, as well as in the Liberty Mutual Research Institute. He received his PhD in Human Factors Engineering from Ben-Gurion University (2011). During his PhD, Avinoam specialized in skill acquisition, hazard perception, and eye movements in driving. Some of his current research interest domains are driving expertise, drivers’ distractions, and eye movements.</p>
<p>Yisrael Parmet is a senior lecturer at the Department of Industrial Engineering and Management. He holds a BA in Economics and Statistics and MSc and PhD degrees in Statistics from Tel-Aviv University. He specializes in area design of experiments and statistical modeling. During his studies he served as a research assistant at the statistical laboratory at the Department of Statistics and Tel Aviv University which granted him experience in practical data analysis. In 2007-2008 Dr. Parmet was a visiting professor at the Department of Dermatology and Cutaneous Surgery in the UM Miller School of Medicine.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Agresti</surname><given-names>A.</given-names></name>
</person-group> (<year>2002</year>). <source>Categorical data analysis</source> (<edition>2nd ed.</edition>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley-Interscience</publisher-name>.</citation>
</ref>
<ref id="bibr2-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cassenti</surname><given-names>D.</given-names></name>
<name><surname>Kelley</surname><given-names>T.</given-names></name>
<name><surname>Swoboda</surname><given-names>J.</given-names></name>
<name><surname>Patton</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The effects of communication style on robot navigation performance</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 53rd Annual Meeting</source> (pp. <fpage>359</fpage>–<lpage>363</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr3-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chadwick</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The impacts of multiple robots and display views: An urban search and rescue simulation</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 49th Annual Meeting</source> (pp. <fpage>387</fpage>–<lpage>391</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr4-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chadwick</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Operating multiple semi-autonomous robots: Monitoring, responding, detecting</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 50th Annual Meeting</source> (pp. <fpage>329</fpage>–<lpage>333</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr5-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chadwick</surname><given-names>R. A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Considerations for use of aerial views in remote unmanned ground vehicle operations</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 52nd Annual Meeting</source> (pp. <fpage>252</fpage>–<lpage>256</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr6-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chappelle</surname><given-names>W. L.</given-names></name>
<name><surname>Novy</surname><given-names>P. L.</given-names></name>
<name><surname>Randall</surname><given-names>B.</given-names></name>
<name><surname>McDonald</surname><given-names>K.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Critical psychological attributes of U.S. Air Force (USAF) Predator and Reaper sensor operators according to subject matter experts</article-title>. <source>Aviation, Space, and Environmental Medicine</source>, <volume>81</volume>, <fpage>253</fpage>.</citation>
</ref>
<ref id="bibr7-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J. Y. C.</given-names></name>
<name><surname>Clark</surname><given-names>B. R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>UAV-guided navigation for ground robot operations</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 49th Annual Meeting</source> (pp. <fpage>1412</fpage>–<lpage>1416</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr8-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J. Y. C.</given-names></name>
<name><surname>Barnes</surname><given-names>M. J.</given-names></name>
<name><surname>Qu</surname><given-names>Z.</given-names></name>
</person-group> (<year>2010</year>). <article-title>RoboLeader: An agent for supervisory control of multiple robots</article-title>. In <source>Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction</source> (Vol. <volume>5</volume>, pp. <fpage>81</fpage>–<lpage>82</lpage>). <publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>IEEE</publisher-name>.</citation>
</ref>
<ref id="bibr9-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chen</surname><given-names>J. Y. C.</given-names></name>
<name><surname>Durlach</surname><given-names>P. J.</given-names></name>
<name><surname>Sloan</surname><given-names>J. A.</given-names></name>
<name><surname>Bowens</surname><given-names>L. D.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Human-robot interface in the context of simulated route reconnaissance missions</article-title>. <source>Military Psychology</source>, <volume>20</volume>(<issue>3</issue>), <fpage>135</fpage>–<lpage>149</lpage>.</citation>
</ref>
<ref id="bibr10-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cook</surname><given-names>M.</given-names></name>
<name><surname>Smallman</surname><given-names>H.</given-names></name>
<name><surname>Lacson</surname><given-names>F.</given-names></name>
<name><surname>Manes</surname><given-names>D.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Situation displays for dynamic UAV replanning: Intuitions and performance for display formats</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 54th Annual Meeting</source> (pp. <fpage>492</fpage>–<lpage>496</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr11-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Croft</surname><given-names>J. L.</given-names></name>
<name><surname>Pittman</surname><given-names>D. J.</given-names></name>
<name><surname>Scialfa</surname><given-names>C. T.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Gaze behavior of spotters during an air-to-ground search</article-title>. <source>Human Factors</source>, <volume>49</volume>, <fpage>671</fpage>–<lpage>678</lpage>.</citation>
</ref>
<ref id="bibr12-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Diaz</surname><given-names>D.</given-names></name>
<name><surname>Sims</surname><given-names>V.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Augmenting virtual environments: The influence of spatial ability on learning from integrated displays</article-title>. <source>Journal of High Ability Studies</source>, <volume>14</volume>, <fpage>191</fpage>–<lpage>212</lpage>.</citation>
</ref>
<ref id="bibr13-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dixon</surname><given-names>S. R.</given-names></name>
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
<name><surname>Chang</surname><given-names>D.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Comparing quantitative model predictions to experimental data in multiple-UAV flight control</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 47th Annual Meeting</source> (pp. 104−108). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr14-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ekstrom</surname><given-names>R. B.</given-names></name>
<name><surname>French</surname><given-names>J. W.</given-names></name>
<name><surname>Harman</surname><given-names>H. H.</given-names></name>
</person-group> (<year>1976</year>). <source>Kit of factor-referenced cognitive tests</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr15-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fincannon</surname><given-names>T.</given-names></name>
<name><surname>Evans</surname><given-names>A. W.</given-names></name>
<name><surname>Jentsch</surname><given-names>F.</given-names></name>
<name><surname>Keebler</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Dimensions of spatial ability and their influence on performance with unmanned systems</article-title>. In <person-group person-group-type="editor">
<name><surname>Andrews</surname><given-names>D. H.</given-names></name>
<name><surname>Hull</surname><given-names>T.</given-names></name>
</person-group> (Eds.), <source>Human factors in defense: Human factors in combat identification</source> (pp. <fpage>67</fpage>–<lpage>81</lpage>). <publisher-loc>Burlington, VT</publisher-loc>: <publisher-name>Ashgate</publisher-name>.</citation>
</ref>
<ref id="bibr16-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fincannon</surname><given-names>T.</given-names></name>
<name><surname>Keebler</surname><given-names>R.</given-names></name>
<name><surname>Jentsch</surname><given-names>F.</given-names></name>
<name><surname>Phillips</surname><given-names>E.</given-names></name>
<name><surname>William</surname><given-names>A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Team size, team role, communication modality, and team coordination in the distributed operation of multiple heterogeneous unmanned vehicles</article-title>. <source>Journal of Cognitive Engineering and Decision Making</source>, <volume>5</volume>, <fpage>106</fpage>–<lpage>131</lpage>.</citation>
</ref>
<ref id="bibr17-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fincannon</surname><given-names>T.</given-names></name>
<name><surname>Ososky</surname><given-names>S.</given-names></name>
<name><surname>Jentsch</surname><given-names>F.</given-names></name>
<name><surname>Keebler</surname><given-names>J.</given-names></name>
<name><surname>Phillips</surname><given-names>E.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Some good and bad with spatial ability in three person teams that operate multiple unmanned vehicles</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 54th Annual Meeting</source> (pp. <fpage>1615</fpage>–<lpage>1619</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr18-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gitelman</surname><given-names>D. R.</given-names></name>
</person-group> (<year>2002</year>). <article-title>ILAB: A program for postexperimental eye movement analysis</article-title>. <source>Behavior Research Methods, Instruments and Computers</source>, <volume>34</volume>, <fpage>605</fpage>–<lpage>612</lpage>.</citation>
</ref>
<ref id="bibr19-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goodrich</surname><given-names>M.</given-names></name>
<name><surname>Morse</surname><given-names>B.</given-names></name>
<name><surname>Gerhardt</surname><given-names>D.</given-names></name>
<name><surname>Cooper</surname><given-names>J.</given-names></name>
<name><surname>Quigley</surname><given-names>M.</given-names></name>
<name><surname>Adams</surname><given-names>J.</given-names></name>
<name><surname>Humphrey</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Supporting wilderness search and rescue using a camera-equipped mini UAV</article-title>. <source>Journal of Field Robotics</source>, <volume>25</volume>, <fpage>89</fpage>–<lpage>110</lpage>.</citation>
</ref>
<ref id="bibr20-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hart</surname><given-names>S. G.</given-names></name>
<name><surname>Staveland</surname><given-names>L. E.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Development of a multi-dimensional workload rating scale: Results of empirical and theoretical research</article-title>. In <person-group person-group-type="editor">
<name><surname>Hancock</surname><given-names>P. A.</given-names></name>
<name><surname>Meshkati</surname><given-names>N.</given-names></name>
</person-group> (Eds.), <source>Human mental workload</source> (pp. <fpage>139</fpage>–<lpage>183</lpage>). <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>Elsevier Science</publisher-name>.</citation>
</ref>
<ref id="bibr21-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lathan</surname><given-names>C.</given-names></name>
<name><surname>Tracey</surname><given-names>M.</given-names></name>
</person-group> (<year>2002</year>). <article-title>The effects of operator spatial perception and sensory feed-back on human-robot teleoperation performance</article-title>. <source>Presence</source>, <volume>11</volume>, <fpage>368</fpage>–<lpage>377</lpage>.</citation>
</ref>
<ref id="bibr22-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McDermott</surname><given-names>P.</given-names></name>
<name><surname>Luck</surname><given-names>J.</given-names></name>
<name><surname>Allender</surname><given-names>L.</given-names></name>
<name><surname>Fisher</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Effective human to human communication of information provided by an unmanned vehicle</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 49th Annual Meeting</source> (pp. <fpage>402</fpage>–<lpage>406</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr23-1555343412445054">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Menchaca-Brandan</surname><given-names>M.</given-names></name>
<name><surname>Liu</surname><given-names>A.</given-names></name>
<name><surname>Oman</surname><given-names>C.</given-names></name>
<name><surname>Natapoff</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Influence of perspective-taking and mental rotation abilities in space teleoperation</article-title>. In <conf-name>Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction</conf-name> (Vol. <volume>2</volume>, pp. <fpage>271</fpage>–<lpage>278</lpage>). <conf-loc>Washington, DC: ACM</conf-loc>.</citation>
</ref>
<ref id="bibr24-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Minkov</surname><given-names>Y.</given-names></name>
<name><surname>Lerner</surname><given-names>Y.</given-names></name>
<name><surname>Oron-Gilad</surname><given-names>T.</given-names></name>
<name><surname>Ophir</surname><given-names>R.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Display type effects in military operational tasks using UAV video images: Comparison between two types of UAV feeds</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 54th Annual Meeting</source> (pp. <fpage>71</fpage>–<lpage>75</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr25-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Moray</surname><given-names>N.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Monitoring behavior and supervisory control</article-title>. In <person-group person-group-type="editor">
<name><surname>Boff</surname><given-names>K. R.</given-names></name>
<name><surname>Kaufman</surname><given-names>L.</given-names></name>
<name><surname>Thomas</surname><given-names>J. P.</given-names></name>
</person-group> (Eds.), <source>Handbook of perception and performance</source> (Vol. <volume>2</volume>, pp. 40-1–40-51). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr26-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mouloua</surname><given-names>M.</given-names></name>
<name><surname>Gilson</surname><given-names>R.</given-names></name>
<name><surname>Daskarolis-Kring</surname><given-names>E.</given-names></name>
<name><surname>Kring</surname><given-names>J.</given-names></name>
<name><surname>Hancock</surname><given-names>P. A.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Ergonomics of UAV/UCAV mission success: Considerations for link, control and display issues</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 45th Annual Meeting</source> (pp. <fpage>144</fpage>–<lpage>148</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr27-1555343412445054">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Murphy</surname><given-names>R.</given-names></name>
</person-group> (<year>2009</year>). [<article-title>Communication retrieved from the DoD Robotics Rodeo 2009, Fort Hood, TX</article-title>]. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://science.dodlive.mil/2009/09/09/podast-33-the-first-robotics-rodeo/">http://science.dodlive.mil/2009/09/09/podast-33-the-first-robotics-rodeo/</ext-link></citation>
</ref>
<ref id="bibr28-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ophir</surname><given-names>R.</given-names></name>
<name><surname>Oron-Gilad</surname><given-names>T.</given-names></name>
<name><surname>Borowsky</surname><given-names>A.</given-names></name>
<name><surname>Parmet</surname><given-names>Y.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Single versus dual video feed displays for dismounted soldiers: Performance and attention allocation (eye tracking)</article-title>. In <source>Proceedings of the Human Factors and Ergonomics Society 55th Annual Meeting</source> (pp. <fpage>2064</fpage>–<lpage>2068</lpage>). <publisher-loc>Santa Monica, CA</publisher-loc>: <publisher-name>Human Factors and Ergonomics Society</publisher-name>.</citation>
</ref>
<ref id="bibr29-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Oron-Gilad</surname><given-names>T.</given-names></name>
<name><surname>Redden</surname><given-names>E. S.</given-names></name>
<name><surname>Minkov</surname><given-names>Y.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Robotic displays for dismounted warfighter situation awareness of remote locations: A field study</article-title>. <source>Journal of Cognitive Ergonomics and Decision Making</source>, <volume>5</volume>, <fpage>29</fpage>–<lpage>54</lpage>.</citation>
</ref>
<ref id="bibr30-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Owsley</surname><given-names>C.</given-names></name>
<name><surname>Ball</surname><given-names>K.</given-names></name>
<name><surname>Sloane</surname><given-names>M.</given-names></name>
<name><surname>Roenker</surname><given-names>D.</given-names></name>
<name><surname>Bruni</surname><given-names>J.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Visual/cognitive correlates of vehicle accidents in older drivers</article-title>. <source>Psychology and Aging</source>, <volume>6</volume>, <fpage>403</fpage>–<lpage>415</lpage>.</citation>
</ref>
<ref id="bibr31-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pazuchanics</surname><given-names>S. L.</given-names></name>
<name><surname>Chadwick</surname><given-names>R. A.</given-names></name>
<name><surname>Sapp</surname><given-names>M. V.</given-names></name>
<name><surname>Gillan</surname><given-names>D. J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Robots in space and time: The role of object, motion, and spatial perception in the control and monitoring of UGVs</article-title>. In <person-group person-group-type="editor">
<name><surname>Barnes</surname><given-names>M.</given-names></name>
<name><surname>Jentsch</surname><given-names>F.</given-names></name>
</person-group> (Eds.), <source>Human-robot interactions in future military actions</source> (pp. 83−102). <publisher-loc>Farnham, UK</publisher-loc>: <publisher-name>Ashgate</publisher-name>.</citation>
</ref>
<ref id="bibr32-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Redden</surname><given-names>E. S.</given-names></name>
<name><surname>Elliott</surname><given-names>R.</given-names></name>
<name><surname>Pettitt</surname><given-names>A.</given-names></name>
<name><surname>Carstens</surname><given-names>B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Scaling robotic systems for dismounted warfighters</article-title>. <source>Journal of Cognitive Engineering and Decision Making</source>, <volume>5</volume>, <fpage>156</fpage>–<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr33-1555343412445054">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Salzman</surname><given-names>M.</given-names></name>
<name><surname>Dede</surname><given-names>C.</given-names></name>
<name><surname>Loftin</surname><given-names>R. B.</given-names></name>
<name><surname>Ash</surname><given-names>K.</given-names></name>
</person-group> (<year>1998</year>). <article-title>VR’s frames of reference: A visualization technique for mastering abstract information spaces</article-title>. In <conf-name>Proceedings of the Third International Conference on Learning Sciences</conf-name> (pp. <fpage>249</fpage>–<lpage>255</lpage>). <conf-loc>Charlottesville, VA: Association for the Advancement of Computers in Education</conf-loc>.</citation>
</ref>
<ref id="bibr34-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Senders</surname><given-names>J.</given-names></name>
</person-group> (<year>1964</year>). <article-title>The human operator as a monitor and controller of multidegree of freedom systems</article-title>. <source>IEEE Transactions on Human Factors in Electronics, HFE-5</source>, <fpage>2</fpage>–<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr35-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Senders</surname><given-names>J. W.</given-names></name>
</person-group> (<year>1983</year>). <source>Visual scanning processes</source>. <publisher-loc>Tilburg, Netherlands</publisher-loc>: <publisher-name>University of Tilburg Press</publisher-name>.</citation>
</ref>
<ref id="bibr36-1555343412445054">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sheridan</surname><given-names>T. B.</given-names></name>
</person-group> (<year>1970</year>). <article-title>On how often the supervisor should sample</article-title>. <source>IEEE Transactions on Systems Science and Cybernetics</source>, <volume>SSC-6</volume>, <fpage>140</fpage>–<lpage>145</lpage>.</citation>
</ref>
<ref id="bibr37-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thomas</surname><given-names>L. C.</given-names></name>
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
</person-group> (<year>2000</year>). <source>Effects of display frames of reference on spatial judgments and change detectio</source>n (Tech. Rep. ARL-00-14). <publisher-loc>Urbana-Champaign</publisher-loc>: <publisher-name>University of Illinois</publisher-name>.</citation>
</ref>
<ref id="bibr38-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
<name><surname>Helleberg</surname><given-names>J.</given-names></name>
<name><surname>Goh</surname><given-names>J.</given-names></name>
<name><surname>Xu</surname><given-names>X.</given-names></name>
<name><surname>Horrey</surname><given-names>W. J.</given-names></name>
</person-group> (<year>2001</year>). <source>Pilot task management: Testing an attentional expected value model of visual scanning</source> (ARL-01-14/NASA-01-7). <publisher-loc>Savoy</publisher-loc>: <publisher-name>University of Illinois, Aviation Research Lab</publisher-name>.</citation>
</ref>
<ref id="bibr39-1555343412445054">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wickens</surname><given-names>C. D.</given-names></name>
<name><surname>Vincow</surname><given-names>M.</given-names></name>
<name><surname>Yeh</surname><given-names>M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Design applications of visual spatial thinking: The importance of frame of reference</article-title>. In <person-group person-group-type="editor">
<name><surname>Shah</surname><given-names>P.</given-names></name>
<name><surname>Miyake</surname><given-names>A.</given-names></name>
</person-group> (Eds.), <source>The Cambridge handbook of visuospatial thinking</source> (pp. <fpage>383</fpage>–<lpage>424</lpage>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>