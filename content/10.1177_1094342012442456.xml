<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="other">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342012442456</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342012442456</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Preface</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Operating systems and runtime environments on supercomputers</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Hoefler</surname>
<given-names>Torsten</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342012442456"/>
<xref ref-type="corresp" rid="corresp1-1094342012442456"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Iskra</surname>
<given-names>Kamil</given-names>
</name>
<aff id="aff1-1094342012442456">University of Illinois (UIUC)</aff>
</contrib>
<bio>
<title>Author biographies</title>
<p>
<bold>Torsten Hoefler</bold> leads the modeling and simulation efforts of parallel petascale applications for the NSF-funded Blue Waters project. He is also one of the representatives of the University of Illinois in the Message Passing Interface (MPI) Forum where he chairs the ‘Collective Operations and Topologies’ working group. He received his PhD in Computer Science from Indiana University. He co-authored over 40 peer-reviewed scientific conference and journal articles and chapters of the MPI-2.2 standard. His research interests round the central topic of ‘Performance-centric software development’ and deal with scalable networks, parallel programming techniques, and performance modeling.<fig id="fig1-1094342012442456" position="float">
<graphic xlink:href="10.1177_1094342012442456-fig1.tif"/>
</fig>
</p>
<p>
<bold>Kamil Iskra</bold> received his MS in computer science from AGH University of Science and Technology in Cracow, Poland in 1999. From 1999 to 2000 he was a scientific programmer at the University of Amsterdam, Netherlands, where he worked on task migration for parallel applications. He got his PhD in computer science from the University of Amsterdam in 2005, in the area of parallel discrete event simulation. In 2005 he joined Argonne National Laboratory in the US, first as a postdoctoral researcher, and, since 2008, as an assistant computer scientist. He is also a fellow at the University of Chicago/Argonne Computation Institute. He works on operating systems and I/O forwarding infrastructure for massively parallel machines.<fig id="fig2-1094342012442456" position="float">
<graphic xlink:href="10.1177_1094342012442456-fig2.tif"/>
</fig>
</p>
</bio>
</contrib-group>
<author-notes>
<corresp id="corresp1-1094342012442456">Sean Torsten Hoefler, University of Illinois (UIUC), 1205 W Clark Street, Urbana, IL, 61801</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>26</volume>
<issue>2</issue>
<issue-title>Issues in Large Scale Computing Environments: Heterogeneous Computing and Operating Systems - two subjects, one special issue</issue-title>
<fpage>93</fpage>
<lpage>94</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p>New architectures of multi- and many-core chips in large-scale supercomputers pose several challenges to designers of operating systems and runtime environments. The International Workshop on Runtime and Operating Systems for Supercomputers (ROSS) provides a forum for researchers to exchange ideas and discuss research questions that are relevant to upcoming supercomputers.</p>
<p>Operating systems and runtime environments on supercomputers have similar goals: both seek to provide an environment for executing applications in a scalable and high-performing way. Achieving this goal often requires minimizing the layers of indirection between the application and the architecture. Ron Brightwell argued in his 2011 keynote titled ‘Why nobody should care about operating systems for exascale’ that the operating system should be replaced with a new, specialized runtime system for the next generation of supercomputers.</p>
<p>This special issue presents the best papers of the ROSS’11 workshop in the critical areas of systems research on supercomputers. The contributions range from systems that emulate large-scale architectures and enable researchers to test their ideas in realistic settings to a complete, virtualized kernel for the Blue Gene/P (BG/P) supercomputer.</p>
<p>The first paper, by Bridges et al., describes a testbed for experimentation with operating system and runtime stacks on simulated nodes of a large-scale system. The authors show how to emulate the target system on a smaller machine using virtualization, simulation, time dilation, and slack simulation to provide an accurate estimate. Stoess et al. develop a virtual machine monitor that enables the execution of multiple virtualized applications on a BG/P system. This enables the usage of a full-featured operating system stack on the BG/P supercomputer. Jones addresses the important issue of operating system noise. The author presents a new kernel scheduling algorithm that implements co-scheduling principles for Linux; he then demonstrates impressive benefits of this approach on a large-scale Cray XT-5. Olivier et al. tackle the multi- and many-core challenge and propose a hierarchical scheduler for OpenMP tasks that matches the NUMA hierarchy of current architectures.</p>
<p>We anticipate that many of the principles and techniques presented in this special issue will play an important role in the design of the next successful parallel computing architecture.</p>
</body>
</article>