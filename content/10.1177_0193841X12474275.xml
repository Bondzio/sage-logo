<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><front><journal-meta><journal-id journal-id-type="publisher-id">ERX</journal-id><journal-id journal-id-type="hwp">sperx</journal-id><journal-id journal-id-type="nlm-ta">Eval Rev</journal-id><journal-title>Evaluation Review</journal-title><issn pub-type="ppub">0193-841X</issn><issn pub-type="epub">1552-3926</issn><publisher><publisher-name>SAGE Publications</publisher-name><publisher-loc>Sage CA: Los Angeles, CA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1177/0193841X12474275</article-id><article-id pub-id-type="publisher-id">10.1177_0193841X12474275</article-id><article-categories><subj-group subj-group-type="heading"><subject>Articles</subject></subj-group></article-categories><title-group><article-title>When is a Program Ready for Rigorous Impact Evaluation? The Role of a Falsifiable Logic Model</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Epstein</surname><given-names>Diana</given-names></name><xref ref-type="aff" rid="aff1-0193841X12474275">1</xref><xref ref-type="corresp" rid="corresp1-0193841X12474275"/></contrib><contrib contrib-type="author"><name><surname>Klerman</surname><given-names>Jacob Alex</given-names></name><xref ref-type="aff" rid="aff2-0193841X12474275">2</xref></contrib><bio><title>Author Biographies</title><p><bold>Diana Epstein</bold> is a Researcher at the American Institutes for Research (AIR). She previously worked at Abt Associates, where she began writing this paper. Epstein's research focuses on federal and state education policy and national and community service programs. She holds a PhD from the Pardee RAND Graduate School and served for two years in the AmeriCorps National Civilian Community Corps (NCCC).</p><p><bold>Jacob Alex Klerman</bold> is a Principal Associate and Senior Abt Fellow with Abt Associates in Cambridge, Massachusetts, as well as Co-Director of Abt's Evaluation Methods Center. Klerman's research combines impact evaluations for a range of federal departments and agencies with methodological work on the role, design, and conduct of such impact evaluations.</p></bio></contrib-group><aff id="aff1-0193841X12474275"><label>1</label>American Institutes for Research, San Mateo, CA, USA</aff><aff id="aff2-0193841X12474275"><label>2</label>Abt Associates, Cambridge, MA, USA</aff><author-notes><corresp id="corresp1-0193841X12474275">Diana Epstein, 2800 Campus Drive, Suite 200, San Mateo, CA 94403. Email: <email>diana.epstein@gmail.com</email></corresp></author-notes><pub-date pub-type="epub-ppub"><month>10</month><year>2012</year></pub-date><volume>36</volume><issue>5</issue><fpage>375</fpage><lpage>401</lpage><permissions><copyright-statement>© The Author(s) 2013</copyright-statement><copyright-year>2013</copyright-year><copyright-holder content-type="sage">SAGE Publications</copyright-holder></permissions><abstract><sec><title>Background:</title><p> Recent reviews suggest that many plausible programs are found to have at best small impacts not commensurate with their cost, and often have no detectable positive impacts at all. Even programs with initial rigorous impact evaluation (RIE) that show them to be effective often fail a second test with an expanded population or at multiple sites.</p></sec> <sec><title>Objective:</title><p> This article argues that more rapid movement to RIE is a partial cause of the low success rate of RIE and proposes a constructive response: process evaluations that compare program intermediate outcomes—in the treatment group, during the operation of the program—against a more falsifiable extension of the conventional logic model.</p></sec> <sec><title>Conclusion:</title><p> Our examples suggest that such process evaluations would allow funders to deem many programs unlikely to show impacts and therefore not ready for random assignment evaluation—without the high cost and long time lines of an RIE. The article then develops the broader implications of such a process analysis step for broader evaluation strategy.</p></sec></abstract><kwd-group><kwd>design and evaluation of programs and policies</kwd><kwd>outcome evaluation</kwd><kwd>program design and development</kwd></kwd-group></article-meta></front><body><disp-quote><p>[G]overnment should be seeking out creative, results-oriented programs like the ones here today and helping them replicate their efforts across America.</p><p>President Barack <xref ref-type="bibr" rid="bibr25-0193841X12474275">Obama (2009</xref>)</p></disp-quote><p>Suppose that, like President Obama, one’s goal was to identify new social programs to be rolled out nationwide to address pressing national problems. What process would you design for identifying those programs? This is the stated goal of the Obama Administration’s “innovations funds”: the Corporation for Nation and Community Service’s Social Innovation Fund, the Education Department’s i3/Investing in Innovation Fund, and the Department of Labor’s Workforce Innovation Fund.<sup><xref ref-type="fn" rid="fn1-0193841X12474275">1</xref></sup></p><p>In response to conventional practice that sometimes goes directly from social problem and plausible program ideas to broad-scale implementation, external observers have urged—and government programs have moved toward—inserting two rounds of rigorous, but costly in time and money, impact evaluation “tollgates” (see <xref ref-type="fig" rid="fig1-0193841X12474275">Figure 1</xref>). The first round is an “efficacy evaluation” under ideal conditions and the second round is an “effectiveness evaluation” under more realistic conditions.<sup><xref ref-type="fn" rid="fn2-0193841X12474275">2</xref></sup> With this approach, only programs that pass these double tollgates proceed to broad-scale rollout.<sup><xref ref-type="fn" rid="fn3-0193841X12474275">3</xref></sup> Proponents of these dual tollgates, sometimes called <italic>randomistas</italic>,<sup><xref ref-type="fn" rid="fn4-0193841X12474275">4</xref></sup> argue that the only way to know if a program “works” is through rigorous impact evaluation (hereafter RIE) such as random control trials.<sup><xref ref-type="fn" rid="fn5-0193841X12474275">5</xref></sup></p><fig id="fig1-0193841X12474275" position="float"><label>Figure 1.</label><caption><p>Rigorous impact evaluation before broad rollout.</p></caption><graphic xlink:href="10.1177_0193841X12474275-fig1.tif"/></fig><p>When implemented, such RIEs have a very high rate of negative results; that is, they find no clear evidence of positive program impacts.<sup><xref ref-type="fn" rid="fn6-0193841X12474275">6</xref></sup> This high prevalence of negative findings both emphasizes the necessity of such RIEs and suggests the need to modify the process of moving from program idea to broad-scale rollout. This article contributes ideas toward developing processes for identifying programs to be evaluated and moving those programs through a sequential evaluation process.</p><p>Our emphasis on the process of program evaluation follows from our sense as evaluators that RIE is unavoidable. Like drug trials based on a combination of solid biology and animal trials, many plausible social programs simply will not be effective. Without RIE, current understanding will not distinguish program models that will be effective from program models that will not be. RIE is needed to distinguish plausible programs that work from plausible programs that do not work. Here, we propose a complementary approach to RIE—one that we project will have lower cost, because programs that are unlikely to be found effective will not proceed to the expensive RIE phase.</p><p>Given that only RIE can establish whether a program works, how can we increase the rate of positive findings? Building on discussions by earlier evaluation theorists—in particular, <xref ref-type="bibr" rid="bibr43-0193841X12474275">Wholey (1994</xref>) and <xref ref-type="bibr" rid="bibr41-0193841X12474275">Weiss (1997</xref>)—we argue that what we call a “falsifiable logic model,” combined with process evaluation methods carefully applied to the problem at hand, can help identify programs that are unlikely to be found effective by an RIE. Building on this earlier literature, our contribution is twofold. First, we explain how these ideas can be incorporated into the tollgate framework that is increasingly being used in the evaluation of new federal programs and, more broadly, into government procurement strategies for evaluation. Second, we enumerate five specific forms of logic model failure. For each of those five specific forms, we show by example that applying our proposed approach plausibly would have screened out programs that failed RIE.</p><p><xref ref-type="fig" rid="fig2-0193841X12474275">Figure 2</xref> summarizes our proposed approach, which augments the RIE phase of <xref ref-type="fig" rid="fig1-0193841X12474275">Figure 1</xref> with an iterative sequence of formative evaluation and process evaluation. In this augmented model, a process evaluation serves as an additional tollgate: Only a program that passes its own logic model as measured by the process evaluation proceeds to the impact evaluation stage. Sometimes a promising program model needs several iterations before a workable approach can be found. If we evaluate too early, we will deem a failure a program model that after another round (or rounds) of development might be deemed a success. Sometimes even after several iterations, however, a promising program will still not pass this evaluability tollgate.</p><fig id="fig2-0193841X12474275" position="float"><label>Figure 2.</label><caption><p>Logic model approach to evaluability.</p></caption><graphic xlink:href="10.1177_0193841X12474275-fig2.tif"/></fig><p>We believe that our approach serves two important purposes: first, the falsifiable logic model (FLM) we propose has a winnowing function, that is, our proposed approach helps to narrow the list of programs that proceed to RIE. Second, it has a program-improving function; that is, our proposed approach helps a program to strengthen its implementation and eventually be more likely to be deemed successful by RIE. Because some programs would never be subject to RIE and because other programs would only be subject to RIE after program refinement, more of the programs subjected to RIE would ultimately be deemed successful.</p><p>The balance of this article proceeds as follows. The next section situates our ideas in earlier evaluation theory and the second section describes in detail how we suggest applying these ideas to the randomistas’ tollgates. These ideas are only useful if our proposed “logic model approach to evaluability” would actually screen out programs as not ready for RIE; that is, if it can identify programs that would fail RIE before RIE commences. To show that our logic model approach can screen out programs, the third section provides detailed examples of five forms of logic model failure that could have been identified by a process analysis. Having shown that, at least in some cases, our logic model approach to evaluability would have screened out programs, the fourth section considers how to incorporate the logic model tollgate into the evaluation process. The fifth second considers some broader implications of our approach for evaluation strategy. The final section considers the applicability of the approach.</p><sec id="section1-0193841X12474275"><title>Earlier Discussions of “Evaluability”</title><p>This article aims to resuscitate ideas first proposed by some of the founding scholars of program evaluation and situate them in the modern movement toward RIE. Our argument builds on Campbell’s “contagious cross-validation model,” in which multiple program options would first be tested with a less rigorous screen, and only the most successful would then proceed to RIE (<xref ref-type="bibr" rid="bibr6-0193841X12474275">Campbell 1984</xref>).</p><p>Our approach further builds on the work of Joseph Wholey and what he termed <italic>evaluability assessment</italic>.<sup><xref ref-type="fn" rid="fn7-0193841X12474275">7</xref></sup> In Wholey’s words, evaluability assessment “is a process for clarifying program designs, exploring program reality, and—if necessary—helping redesign programs to ensure that … program goals, objectives, important side effects, and priority information needs are well defined, program goals and objectives are plausible, relevant performance data can be obtained, and the intended users of the evaluation results have agreed on how they will use the information” (<xref ref-type="bibr" rid="bibr43-0193841X12474275">Wholey 1994</xref>).</p><p>Our focus on the intermediate outcomes specified by the program’s theory is closely related to the “theory of change” approach to evaluation proposed by <xref ref-type="bibr" rid="bibr41-0193841X12474275">Weiss (1997</xref>, see Chapter 3, and especially p. 60<sup>8</sup>). She argues: “They, or better still, theories, direct the evaluator’s attention to likely types of near-term and longer-term effects … provid(ing) early indications of program effectiveness. It [the evaluation] need not wait until final outcomes appear (or fail to appear).” Furthermore, this information can have a constructive effect, pointing to how the program might be improved: “If breakdown occurs in the premises of implementation, the implications are immediate and direct: Fix the way the program is being run.” The balance of this article operationalizes this idea and proposes how it might be embedded into the tollgate approach.<sup><xref ref-type="fn" rid="fn9-0193841X12474275">9</xref></sup></p><p>Specifically, we refine and provide examples of how logic models<sup><xref ref-type="fn" rid="fn10-0193841X12474275">10</xref></sup> can help us to decide what to evaluate. Previous use of logic models has helped program developers to get inside the black box of what a program did or did not do (e.g., <xref ref-type="bibr" rid="bibr31-0193841X12474275">Rogers et al. 2000</xref>; Pawson and Tilly 1997; <xref ref-type="bibr" rid="bibr17-0193841X12474275">Leviton and Gutman 2010</xref>); we extend this idea to program evaluation. While our approach relies heavily on a program’s logic model, our approach is perhaps less theoretical. We posit that, like management consulting, formative evaluations and process evaluations proceed from an implicit model of how programs should operate. Given this implicit model, formative evaluators and management consultants infer how program implementation should change in order to increase program impact.</p><p>As empiricists, we place less confidence in implicit models and we are therefore reluctant to use program theory in this way. However, the essence of our argument is that a program’s logic model implicitly embodies a program theory, and a necessary condition for the program to be effective is that it succeeds according to its own program theory. Our approach therefore requires a program to make its program theory explicit in the form of what we call an FLM. Program’s that do not satisfy the requirements of their own FLM can be screened out from more expensive RIE.</p></sec><sec id="section2-0193841X12474275"><title>The Logic Model Approach to Evaluability</title><p>Specifically, our approach expands the role of the logic model. Conventionally, the logic model is a tool for thinking through causal pathways; in our approach, the logic model becomes a tool for evaluating whether a program is satisfying its own stated approach. The foundation of our approach is a requirement that an expanded logic model specify detailed—and falsifiable—goals for one of the components of a conventional logic model—intermediate outcomes that must be realized by members of the treatment group in order for the program to succeed. As will be clear in what follows, our FLM is a major extension of conventional logic models. Conventional logic models specify a sequence of steps; our FLM includes considerably more implementation detail, often including specific quantitative benchmarks for intermediate outcomes.</p><p>Such intermediate outcome goals could be both quantitative and qualitative.<sup><xref ref-type="fn" rid="fn11-0193841X12474275">11</xref></sup> Thus, a training program’s logic model might specify <italic>intermediate</italic> benchmarks such as: (i) space is secured and a curriculum developed; (ii) a specified number of instructors complete a specified number of hours of training; (iii) classes of a specified size are recruited; (iv) instructors teach with fidelity to the curriculum (and fidelity has a precise operational definition); (v) students attend all or most of the classes (with a quantitative standard of what fraction of students will attend what fraction of classes); (vi) students master material, as measured by gain of <italic>X</italic> percent on a pretest/posttest; and end impacts such as (vii) students find jobs that use acquired competencies; and (viii) students are retained in those jobs for at least <italic>X</italic> months.</p><p>Asking program developers to specify their logic models in the level of detail required for an FLM is valuable for four reasons. The first reason underlies the conventional role for a logic model: developing a logic model at this level of explicitness and detail helps program developers to refine their program vision. A program model is more than an “idea” or an “insight.” In developing a program, it is crucial to think realistically. Programs are rarely experienced in their ideal form. Not everyone will attend all of the sessions; not everyone will pass the final exam. What are realistic expectations?</p><p>As important, a program model should specify what intermediate outcomes are needed in order for the program to have an impact. Can the program be cost-effective if classes are not full? Can the program have the projected impact if only two thirds (half?) of the students attend all (at least three quarters?) of the sessions? The process of thinking through the details of program operation should identify issues, lead to improved program design, and thereby result in better program outcomes. This is the conventional argument for urging new programs to develop a logic model. Requiring a logic model at this level of detail seems reasonable before giving a program funds for a pilot program.</p><p>The second reason for asking program developers to specify their logic model in detail is more subtle. When trying to “sell” the program, program proponents have a strong incentive to overpromise. In the absence of a strong evidence base, setting target outcomes may be somewhat arbitrary. A program that promises to yield large impacts for many people is inherently more attractive than one for which only small impacts are promised.<sup><xref ref-type="fn" rid="fn12-0193841X12474275">12</xref></sup> In contrast, at the evaluation stage, program proponents have an incentive to lowball their estimates. If they overestimate or even give their best guess, it is possible that the program will not meet the stated performance goal at RIE and will perhaps be cancelled.<sup><xref ref-type="fn" rid="fn13-0193841X12474275">13</xref></sup> These conflicting pressures could operate to induce what game theory calls “truth telling”; that is, providing program proponents a stronger incentive to give more realistic estimates.<sup><xref ref-type="fn" rid="fn14-0193841X12474275">14</xref></sup> The strength of these two opposing pressures will vary from case to case. They will rarely exactly cancel out, such as to induce complete “truth telling.” Nevertheless, the knowledge that stated benchmarks will be used in evaluation should improve the realism of claims. As a result, the stated benchmarks for intermediate outcomes in our proposed FLM could therefore help in selecting which new programs to fund for the first time and which existing programs to continue funding.</p><p>The third reason for asking program developers to specify their logic model in detail is that such falsifiable goals are crucial to our proposed approach for determining when a program is ready for RIE. In the pilot phase, our approach compares the specified benchmarks for a successful program and for the follow-on RIE against observed intermediate outcomes.</p><p>The fourth reason for asking program developers to specify their logic model in detail concerns evaluation. In as much as there is an RIE tollgate, it must be possible to conduct an RIE, and RIE designs have certain implicit assumptions about program implementation that can be checked at the pilot. For example, random assignment with the standard equal assignments to treatment and control groups requires twice as many applicants as program slots. Many evaluations fail for insufficient applicants, and the implicit assumption of having more applicants than could be served could be checked in a pilot.</p><p>We acknowledge that in current practice, logic models often lack the specificity required by our approach.<sup><xref ref-type="fn" rid="fn15-0193841X12474275">15</xref></sup> For example, the IES evaluation report on the Even Start literacy program states that the children had very different amounts of exposure to the program. This is not (necessarily) a result of failed implementation, rather it was the case that “Even Start guidelines do not specify an expected level of exposure for children or parents, and the hours of instruction offered by local projects vary widely” (Judkins et al. <xref ref-type="bibr" rid="bibr16-0193841X12474275">2008</xref>). As this example illustrates, it is our premise that falsifiable goals are a reasonable degree of specificity for a program seeking substantial funds.</p><p>We conclude this section by acknowledging that satisfying the benchmarks for program operation and outputs and outcomes in the treatment group specified in such an FLM will often not be sufficient to achieve the desired end impacts. Even if there is improvement in outcomes for the treatment group such that benchmarks are satisfied, there may not be any impact. Control groups often also show improvement—sometimes due to regression to the mean, sometimes due to other similar programs in the community. Thus, even a program that satisfies its own logic model for outputs and outcomes for the treatment group (e.g., pre/post progress on a standardized test) may fail an RIE for impact on long-term outcomes, that is, outcomes for the treatment group relative to outcomes for the control group (e.g., earnings).<sup><xref ref-type="fn" rid="fn16-0193841X12474275">16</xref></sup> Nevertheless, a program that cannot achieve the intermediate goals specified by its own logic model will not, according to its own logic model, have the desired (usually longer term) impacts and therefore should not proceed to RIE.<sup><xref ref-type="fn" rid="fn17-0193841X12474275">17</xref></sup></p><p>Crucially, note that this determination of whether a program achieves the intermediate outcomes specified by its own logic model can often be made using conventional process evaluation methods, that is, careful observation of program operation, <italic>without random assignment and without a comparison group</italic>. In addition, in most cases and by design, this determination should be possible at low cost. Our approach is attractive exactly because it is inexpensive—it only requires program operating records (e.g., enrollment and attendance) and end-of-program tests. Thus, to be useful, an FLM needs to specify benchmarks for intermediate outcomes that can be measured without a follow-up survey (given its expensive tracking of former program participants) and without a need to locate and survey nonparticipants (a control group or comparison group).</p></sec><sec id="section3-0193841X12474275"><title>Five Forms of Logic Model Failure</title><p>If most evaluated programs could pass this additional tollgate—their own logic models—then this logic model approach to evaluability would not be useful; it would not screen out programs. If the logic model approach does not screen out programs, then it does not aid the broader RIE process.</p><p>The opposite is true. Many evaluated programs fall short of (the currently implicit) expectations on these intermediate outcome tests, thus failing their own logic models. Other program models may not fail their FLM, but they do fail to meet the conditions required for the follow-on RIE. This section specifies and provides examples of five common forms of failure of programs to satisfy their own logic models: (i) failure to secure required inputs; (ii) low program enrollment; (iii) low program completion rates; (iv) low fidelity; and (v) lack of pre/post improvement. Then, for each form of failure, we discuss how it could have been detected by a process evaluation.</p><p>The sources for these examples are our own experiences and observations, as well as discussions with experts and years of reading the evaluation literature. We recognize that similar lists exist elsewhere in the program development and evaluation literature. Furthermore, we do not intend for this to be a checklist of midpoint accomplishments that all programs must pass. Rather, we hope that identifying these common sources of logic model failure will promote additional critical thinking during program development and implementation. Specific FLMs will need to specify their own benchmarks—sometimes applying the broad categories listed here, at other times adding new categories. In as much as these ideas are applied, it seems almost certain that this list should be expanded and refined. Here it serves primarily as an organizing device for our proofs of concept; that is, examples of benchmarks that could have been prespecified and were not achieved.</p><sec id="section4-0193841X12474275"><title>Failure to Secure Required Inputs</title><p>One way in which a programs fails its own logic model concerns the inputs into the program. Program models often implicitly posit the ability to establish interorganization partnerships and to recruit and retain certain types of staff. Sometimes those partnerships never materialize or the staff cannot be recruited or retained. For example, the Employment Retention and Advancement (ERA) program in Salem, Oregon struggled with both high turnover among case manager staff and a difference in philosophies between staff recruited from welfare agencies and those from community colleges. These implementation challenges affected service delivery and hence the benefits that participants were able to obtain from the program (<xref ref-type="bibr" rid="bibr23-0193841X12474275">Molina, Cheng, and Hendra 2008</xref>). Thus, an FLM should specify the partnerships to be established, the qualifications of the staff to be hired, and the projected retention rate of those staff.</p><p>Relatedly, sometimes the way the program operates will not support RIE. An example comes from an Abt Associates evaluation of evidence-based practice in probation that aimed to see whether caseload reductions among probation officers reduced criminal recidivism and increased revocations for technical violations (<xref ref-type="bibr" rid="bibr14-0193841X12474275">Jalbert and Rhodes 2010</xref>). An agency in Oklahoma City agreed to implement a randomized controlled trial and probation officers volunteered either to be assigned to a reduced caseload or to maintain a regular caseload. However, the experiment degenerated because a large number of officers with regular caseloads either left the agency or accepted a different assignment. This disrupted the equivalency of the random assignment groups and reduced the study’s power such that the evaluators had to switch to a difference-in-difference study design. Implicitly the evaluation design assumed no turnover among officers, but a pilot program would have detected this problem.</p><p>Each of these intermediate outcomes—partnerships to be established, qualifications of staff to be hired, and staff retention rates—could have been checked using conventional process analysis methods.</p></sec><sec id="section5-0193841X12474275"><title>Low Program Enrollment</title><p>A second way that a program fails is that it does not attract the target number of clients/participants. Programs are only worth running if there is a demand. Program models implicitly assume that there is a need for the program—and that people will enroll. In practice, that often is not the case. Underenrollment relative to plan is a fundamental logic model failure and quite common.</p><p>The best document examples of underenrollment arise in RIE. For random assignment to be feasible, a program needs to have a surplus of clients (usually double). The ideal situation for RIE is therefore an existing program with a long waiting list. When such a long waiting list exists, random assignment can often be viewed as the most ethical approach to deciding who will be served.</p><p>When there is not currently a waiting list, sometimes a program can attract additional clients through advertising and recruiting. For a program that is already achieving its target enrollment, resources expended on recruiting and advertising are arguably wasted, but moderate additional expenditures on advertising and recruiting will yield the larger number of applicants required to implement random assignment. However, a program that cannot even attract the target size of the treatment group (or is expending considerable resources to do so) is not ready to recruit sufficient program applicants to implement random assignment.</p><p>In practice, many RIEs have trouble recruiting applicants at the target rate. Underenrollment or extended enrollment periods to achieve target sample sizes are not the exception, they are the rule. Sometimes the RIE is cancelled, as was the case in the Portland Career Builders ERA program which recruited a random assignment sample only a third of the target size (<xref ref-type="bibr" rid="bibr2-0193841X12474275">Azurdia and Barnes 2008</xref>). Other times the RIE limps forward on less than the target number of applicants. In either case, an earlier process study could have detected recruitment challenges and would have indicated that the program was not ready for an RIE.</p><p>Of course, a successful program can usually expect even more demand. An existing program develops a referral network. Claimed impacts and individual success stories help build demand. Nevertheless, it is exactly our premise that we should not be moving to RIE until there is a fully rolled out program considerably larger than the pilot program, even in the pilot site. In that case, at least moderate excess demand (i.e., a waiting list) is a reasonable requirement before proceeding to—and a requirement for successful implementation of—RIE.</p></sec><sec id="section6-0193841X12474275"><title>Low Program Completion Rates</title><p>A third way that a program fails is that sometimes participants initially enroll in the program but do not complete the expected treatment. We know this is a failure of the logic model because, ex post, reports of rigorous impact studies point to failure to complete the treatment as the reason for null results. For example, the report on the rural welfare to work strategies evaluation attributes the lack of substantial impacts to the fact that only about two fifths of the target clients received substantial Future Steps services (<xref ref-type="bibr" rid="bibr21-0193841X12474275">Meckstroth et al. 2006</xref>). In the South Carolina Moving Up ERA program, only half of the program group was actually engaged in the program’s services during the year after they entered the study. Of those who were engaged, the intensity of engagement varied such that some were only minimally involved in the program (<xref ref-type="bibr" rid="bibr34-0193841X12474275">Scrivener, Azurdia, and Page 2005</xref>). Another example is the Cleveland Achieve ERA program, where participation varied widely such that overall the intensity was less than the program designers had envisioned (<xref ref-type="bibr" rid="bibr22-0193841X12474275">Miller, Martin, and Hamilton 2008</xref>).</p><p>Similarly, Building Strong Families’ logic model asserted that multiple group sessions on relationship skills, combined with other support services to unmarried couples around the time of their child’s birth, could improve relationship skills and thereby increase marriage rates and decrease divorce rates (<xref ref-type="bibr" rid="bibr44-0193841X12474275">Wood et al. 2010</xref>). However, in all but one of the sites, less than 10% of the couples received at least 80% of the curriculum. Only in the one site where 45% of the couples received at least 80% of the curriculum was there an impact on measures of relationship quality.</p><p>It is our reading of the literature that there is often a presumption that most (perhaps nearly all) of the sessions must be attended in order for the program to have its full effect. This is clearly true in a program that gives a certificate or a degree. In other programs such a presumption is signaled by the fact that final sessions often include some special capstone or wrap-up activity. A presumption that most sessions need to be completed is sometimes implied by the fact that we fund all of the sessions, that is, if we thought the later sessions were not needed, we would not have funded them.</p><p>Universal attendance at every program session is not realistic. Nevertheless, below some threshold, impacts seem extremely unlikely. Thus, an FLM should specify what defines “enrollment” and what fraction of those enrollees need to attend what number of sessions in order for there to be a measurable (perhaps cost-effective) impact. Then, actual attendance can and should be measured against the FLM’s stated benchmarks.</p></sec><sec id="section7-0193841X12474275"><title>Low Fidelity</title><p>A fourth way in which a program fails is that sometimes clients attend, but the program as implemented falls short of what was envisioned in the logic model. Thus, for example, across each of the four supplemental reading comprehension programs, a Mathematica random assignment evaluation found no evidence of consistent impact. However, the study also found evidence of far from complete implementation of the curricula themselves. Only about three quarters of the targeted practices were actually implemented (James-Burdumy et al. 2010).</p><p>Similarly, Abt’s random assignment evaluation of a national student mentoring program found no consistent pattern of impact (<xref ref-type="bibr" rid="bibr3-0193841X12474275">Bernstein et al. 2009</xref>). In explaining that null result, the report noted that the average amount of mentoring received was lower than the benchmark provided by model community-based mentoring programs.</p><p>Thus, an FLM should specify what constitutes (sufficient) fidelity of implementation. That specification of implementation with fidelity needs to be specific enough to be falsifiable. What defines how the program would be implemented ideally? How will fidelity be measured? What amount of deviation from that ideal implementation would constitute failure?</p><p>This step has important implications for program development as well. Once program developers have defined implementation with fidelity, they should go back to their plan for training and supervising staff. Do those training materials make clear how it is expected that the program will be implemented? Have the instructors been tested to assure that they have mastered the techniques and learned the expectations? Does the program model include sufficient supervisory resources and a supervision scheme that can reasonably be expected to lead to implementation of the program with fidelity? This part of the development of the logic model will lead to additional falsifiable outcomes for the first way that programs fail (insufficient inputs, e.g., staff recruited and trained) and additional falsifiable outcomes for the fourth way that programs fail (low fidelity, e.g., was the supervisory plan implemented—perhaps determined by checking supervisor reports).</p></sec><sec id="section8-0193841X12474275"><title>Lack of Pre/Post Improvement</title><p>The fifth way in which programs fail is that sometimes clients show minimal or no progress on pre/post measures of the intermediate outcome the program was intended to affect. Pre/post measures are subject to well-known biases—in particular, history and maturation; this is why most pre/post designs are not considered RIEs. Nevertheless, it is often true that pre/post provides a plausible lower bound. For example, we often expect improvement in the control group as well, such that pre/post is a lower bound on impact. History might cause pre/post not to be a lower bound; for example, a weakening economy pulls post outcomes for the treatment group below pre outcomes, but perhaps the pre/post decline would have been even larger in a control group. Whether this critique will be valid will vary from evaluation to evaluation and will depend on how significant the changes were in the external environment (e.g., economic conditions). In general, choosing outcomes that are more under the control of the program—for example, test scores on the material taught rather than labor market outcomes—will lessen the salience of the history critique.</p><p>Thus, in an evaluation of National Evaluation of Welfare to Work Strategies (NEWWS), clients randomized to the Human Capital Development (HCD) component showed no progress on objective achievement tests (<xref ref-type="bibr" rid="bibr4-0193841X12474275">Bos et al. 2002</xref>). In as much as the logic model for these HCD programs implied that earnings would rise because clients learned basic skills in reading, writing, and mathematics, the program was a failure.</p><p>NEWWS HCD programs did increase the number of people who received a General Educational Development (GED) diploma, absolutely and relative to the control group. Here and in general, the specifics of the logic model matter. If the program’s logic model had posited that earnings will rise because clients get a GED, even if they do not learn anything, then it might be reasonable to proceed with RIE. However, the HCD program’s logic model had specified actual learning. In this sense, we propose to hold programs to <italic>their own</italic> logic models and, conversely, not to let programs define achieved outcomes as success ex post (after they see the results; in this case, GEDs but no improvement in test scores).</p><p>Thus, an FLM should specify pre/post changes in outcomes for the treatment group, for example, skill attainment, progress on standardized tests, graduation rates, receipt of certificates. To be a useful screening device for programs that should be subjected to RIE, these measurements must be inexpensive. Not all enrollees will still be around at the end of the program. Thus, we would not want a standard in terms of true outcomes for all enrollees; we cannot easily observe outcomes for initial enrollees who do not complete the program (for whatever reason). We also do not want measures conditional on completion (the people we observe). Rather, we want standards in terms of the incoming class, for example, half of the entering class will get a certificate through the program. People who complete but do not get a certificate and people who leave the program before completion would count toward the denominator (program enrollees), but not toward the numerator (those receiving a certificate). The program might like to claim credit for those who get a certificate outside the program, but those certificates are not easily measured (and are probably not due to the program), so it is probably better to define the standard to not include such certificates received by those who initially enrolled in the program, but who did not complete the program and get their certificate through the program.</p><p>As in all performance management systems, in such FLMs it will be crucial to carefully define “enrollees.” The Department of Labor’s Workforce Investment Act training programs sometimes delay official “enrollment” until the program is relatively sure that a trainee will complete the program. For most purposes, a more appropriate definition will be people who receive any program services.</p><p>Finally, note also that such performance standards give program operators strong incentives for “cream skimming,” that is, only enrolling trainees who are likely to meet the standard. Program funders need to be aware of those incentives. If trainees most likely to meet the standard are not the target population, then there is another standard for the second way in which programs fail (insufficient enrollment). Enrollment standards need to specify not only the number of enrollees but also enough about their characteristics to assure that the target population is being enrolled.</p><p>Conversations with researchers and research sponsors suggest that each of these logic model failures is common. These failures reflect gaps in program planning and development. They are also embarrassing and therefore rarely end up in the formal literature. Sometimes as a result of these logic model failures the study is cancelled and therefore no report is produced. More frequently, the program limps through the RIE: a different partnership is attempted; staff standards are relaxed; the intake period is held open much longer than expected. These changes from the original design are mentioned only in passing, if at all. Finally, sometimes the problems—poor attendance at sessions or minimal progress even in the treatment group—are not noticed until the project’s final report, where they are not mentioned at all or used to explain why the program did not find impacts.</p><p>Crucially for our argument, note the common pattern of these examples. Each of these programs was subjected to expensive and time-consuming RIE. Those RIEs found no impact, overall or at most program sites. The RIEs were, however, unnecessary. Through site visits and analysis of program records, a process evaluation could have collected information on partnerships and staffing, initial enrollment, attendance at sessions, and pre/post progress on the target intermediate outcomes. That information could have been compared against a more detailed version of the conventional logic model. Those comparisons would have clearly demonstrated that, according to the program’s own logic model, the program was unlikely to have impacts and therefore was not ready for RIE.</p></sec></sec><sec id="section9-0193841X12474275"><title>Fitting the Logic Model Step Into an Evaluation Process</title><p>The previous section has argued that through a process evaluation it is possible to screen out many programs as not ready for RIE. In this section, we point out three other direct implications of this negative screen.</p><p>First, programs that fail their own logic models do not necessarily need to simply be discarded without further consideration. The premise of technical assistance (management consulting, in the for-profit world) is that, through observing a program as it currently operates, an experienced outsider can suggest ways to improve the program’s operation. Perhaps with those improvements, the program will meet its own logic model. That such technical assistance—or even just another cycle of program operation—will help is more plausible for relatively new programs. In initial cycles of program operation steps often clearly fail, but the experience of failure suggests ways to improve those steps. Unfortunately, however, federal demonstration programs sometimes subject brand new programs to RIE. Thus, United States Department of Agriculture’s (USDA) Health Incentive Pilot was designed and implemented with a random assignment evaluation beginning from the first month of operation. Similarly, USDA’s Summer Electronic Benefit Transfer (EBT) for Children program was established and received a pilot (i.e., small sample) random assignment evaluation in its first year of operation and a full and quite large random assignment evaluation in its second year of operation. Similarly, federal training programs are often funded by short-term contracts, with an RIE requirement. But, this means that we apply RIE to a very early implementation. It seems plausible that lessons learned in the first year or two of operation might lead to program refinements, improved program implementation, and improved client outcomes. The possibility of revising programs such that they will meet their own logic models suggests building such a “formative evaluation” or “technical assistance” step into evaluations—either once the program fails its own logic model, or even before proceeding to the process evaluation.</p><p>However, a caveat is in order. Among the activities of formative evaluations is to help a program refine its logic model. However, the FLM should be set <italic>before</italic> the program is implemented. Once the program is implemented, formative evaluations and technical assistance should focus on changing details of program implementation to satisfy the benchmark intermediate outcomes in the program’s FLM as stated initially. After seeing the result, simply lowering the quantitative benchmarks—how many sessions a client needs to attend, how much progress the client must make on an achievement test—is too easy. Instead, a program with a significantly different logic model, or even substantially different quantitative goals, should often be viewed as a totally new program—and be required to recompete for funding with other new or existing programs.</p><p>Second, even if <italic>sometimes</italic> we can refine a program such that with the refinements it will meet the benchmarks in its FLM, it does not follow that we should (always) fund such program refinement efforts. Some program models that fall short of expectations in their first (or second, or nth) iteration <italic>will</italic> fulfill their own logic models with another cycle through formative evaluation and then process evaluation. Other program models should simply be terminated: these programs were initially promising, they have been tried, and they have been deemed ineffective (i.e., they failed to achieve their own logic models’ intermediate outcomes). Limited resources should be transferred from these failed programs to other promising program models.</p><p>The challenge, of course, is deciding which programs to iterate and which programs to simply terminate. Unfortunately, we have no clear guidance on when to continue investing and when to move on. Foundation program officers, social venture capitalists, and government employees currently make these decisions. As of now, our only guidance is to carefully consider the trade-off (in time and resources) between continuing to invest in a program that can be saved versus scrapping it entirely and instead beginning to explore some other program that offers a different, and perhaps better, approach. Perhaps making the trade-offs of each choice explicit will improve decision making. Additional insights into this choice would clearly be useful.</p><p><xref ref-type="fig" rid="fig2-0193841X12474275">Figure 2</xref> suggests the third implication of our negative screen. Current evaluation strategy for new government programs often includes some formative evaluation/technical assistance and even some process evaluation. However, those steps are often funded simultaneously with, and as part of the same contract as, the impact evaluation. Formative evaluation/technical assistance is provided immediately before and as part of proceeding to random assignment. Such a single contract shrinks the necessarily long interval from program idea to broad-scale rollout.</p><p>The disadvantage of this approach is that a single-contract approach implicitly assumes that most programs will go from program idea through formative evaluation and process evaluation to impact evaluation. However, the thrust of our argument is that many (perhaps most) programs will fail at the process evaluation stage and therefore should <italic>not</italic> proceed (at least immediately) to RIE.<sup><xref ref-type="fn" rid="fn18-0193841X12474275">18</xref></sup></p><p>That many (perhaps most) programs should not proceed immediately from process evaluation to RIE is an additional reason not to issue one contract for both “program development” and for RIE. Instead, this line of argument supports issuing a first contract for “program development,” that is, technical assistance and formative evaluation. If the program and the formative evaluator decide the program is ready, <italic>then</italic> proceed to competition for a second contract for a process evaluation that would compare intermediate outcomes to the benchmarks specified in the program’s own FLM. If the program and the formative evaluator instead decide that the program is not ready for the process evaluation, then the program should apply—competitively—for another round of technical assistance and formative evaluation. For programs that proceed to the process evaluation phase, at the end of that phase the evaluator would prepare a report and the funder would choose between three options: (i) proceed to a third competition for a new contract to conduct the RIE; (ii) proceed to a competition for another round of program development and process evaluation; or (iii) terminate funding for the program.<sup><xref ref-type="fn" rid="fn19-0193841X12474275">19</xref></sup></p><p>While it is true that some things just cannot be rushed, separate contracts will often be an extreme solution. Separate contracts have costs—in time and in coordination. Short of separate contracts, contractual terms might be specified cognizant of the likelihood that it will often turn out not to be worthwhile to proceed from formative evaluation to process evaluation and from process evaluation to impact evaluation. Thus, contracts might be structured in two parts, with a clear implication that the second part is not certain. A formal, and perhaps external, review step could be introduced. To counteract the incumbent evaluator’s bias toward proceeding to RIE (additional funding), the reviewers could be instructed to carefully examine the evaluator’s process evidence and to lean against proceeding.</p></sec><sec id="section10-0193841X12474275"><title>Some Broader Implications for Evaluation Strategy</title><p>The logic model approach to evaluability and program development described in this article embodies a strong implicit assumption about program motivation; this approach assumes that a program’s ultimate goal is to grow from program idea to broad program rollout. It further assumes that the only way to do so is by satisfying the various evaluation tollgates.</p><p>In practice, neither of those assumptions is universally correct. First, with respect to the desire to move promptly through evaluation to broad program rollout, this might be plausible if: (i) the primary goal of program developers is to get their programs rolled out nationally as quickly as possible and (ii) program developers have complete faith in the evaluation process.</p><p>Neither of those conditions is likely to be satisfied. Some program operators would be content to run small local programs. In some cases, this is because their vision truly is local. In other cases, there is a fear of RIE (<xref ref-type="bibr" rid="bibr5-0193841X12474275">Campbell 1969</xref>). Such program operators believe that they are achieving their desired results; they try to avoid RIE because they fear the program might—in their view, incorrectly—be deemed ineffective. Other program operators do not believe that their program can be meaningfully evaluated with a random assignment approach, perhaps because important benefits are not (easily) measurable.<sup><xref ref-type="fn" rid="fn20-0193841X12474275">20</xref></sup> Still others feel that limited resources should be used to serve clients and should not be diverted toward RIE. For each of these groups of program operators, prolonged “program development” will often be the ideal outcome.</p><p>Second, with respect to the necessity of program evaluation in order to proceed to broad-scale rollout, some recent initiatives are consistent with this perspective. The evidence-based Nurse-Family Partnership ($1.5 billion over 5 years) and the evidence-based Teen Pregnancy Prevention Program ($110 million in FY2010) each provided substantial funding for the broad rollout of programs that have passed RIE at both the efficacy and effectiveness level (Orszag 2009a). As Orszag explained: “[This approach] will also create the right incentives for the future. Organizations will know that to be considered for funding, they must provide credible evaluation results that show promise, and be ready to subject their models to analysis.” From this perspective, it appears that the Education Department’s Investing in Innovation Fund (i3), for example, takes the right approach. Rather than funding only program pilots or formative evaluations, i3 awarded the largest amounts of money to promising existing programs and gave preference to programs that are supported by stronger evaluation evidence.</p><p>However, this approach is the exception rather than the rule. As Orszag (2009a, 2009b) and others acknowledge, many programs without RIE evidence and even some programs with negative RIE evidence continue to be funded, often at high levels. Given this reality, avoiding RIE may also be a viable strategy for program developers to pursue.</p><p>These two factors imply that evaluators will often need to induce programs to participate in RIE. Once evaluators need to induce programs to participate in RIE, it is not clear that they can insist that programs develop FLMs and participate in the long time line and onerous sequence of evaluation steps described here.</p><p>Our guidance is simple: this sequence of evaluation steps should be a requirement (or at least a major plus factor) of funding for pilot programs and for proceeding to broad-scale rollout of existing programs. We understand that the reality diverges from that simple guidance. That divergence will make it more difficult to implement the sequence of evaluation steps described in this article. More consideration of these issues is needed.</p></sec><sec id="section11-0193841X12474275"><title>Closing Thoughts and Next Steps</title><p>This article has argued that some programs are undergoing an RIE too early and that more resources should be devoted to determining whether a program is ready for RIE. A more detailed FLM combined with a careful process evaluation would frequently detect programs that failed, for example, to: (i) complete partnerships or hire the desired staff; (ii) recruit sufficient qualified program participants; (iii) induce program participants to engage with the complete program; (iv) implement the program with sufficient fidelity; and (v) improve program participants’ outcomes relative to the program’s own goals. Having failed to satisfy the intermediate outcomes of the program’s own FLM, this program is unlikely to have positive long-term impacts and therefore should not proceed to RIE.</p><p>From this insight—that some programs can be rejected based on the results of benchmarks for the treatment group during (or shortly after) the end of the program—emerges an answer to the question with which we began this article: What process would you design for identifying programs worthy of broad-scale rollout? Our suggested process was depicted in <xref ref-type="fig" rid="fig2-0193841X12474275">Figure 2</xref>: (i) require the program to specify an FLM as part of its application for funding; (ii) fund a pilot with a corresponding formative evaluation; (iii) if repetition of the formative evaluation step is needed, decide whether to fund it, or alternatively to abandon the program model; (iv) proceed to a process evaluation that verifies the satisfaction of the program’s own FLM; (v) if repetition of the process evaluation step is indicated, decide whether to fund it, or alternatively to abandon the program model; (vi) proceed to an RIE efficacy trial; (vii) if the program passes the efficacy trail, proceed to replication; (viii) if the program does not pass the efficacy trial, (perhaps) repeat the formative evaluation and the process evaluations steps (ii–v); (ix) proceed to an RIE effectiveness trial; and (x) if the program passes the effectiveness trial, proceed to broad rollout.</p><p>This article’s advocacy of the use of formative evaluation and process evaluation should not be used as an excuse to delay RIE. We would urge a bias toward either proceeding to the process evaluation and then RIE, or terminating the program. Partially to encourage programs to move on to RIE when appropriate, another round of formative evaluation and process evaluation should be far from automatic. Forcing programs that fail their own logic models to reapply for funding for formative evaluation—alongside other programs that have not had even one round of formative evaluation—is one possible strategy.</p><p>We conclude by acknowledging that the approach described here is unlikely to be adopted completely. Our approach would convert a process that already often takes nearly a decade into a process that will often take considerably more than a decade. We have argued that our proposed approach is consistent with good science and the reality that most programs subjected to RIE do not work.</p><p>Nevertheless, further lengthening of evaluation cycles is, for at least two reasons, potentially problematic. First, our proposed approach is inconsistent with the political cycle. Policy makers and politicians face strong pressures to be seen as “doing something”—and soon! With this attitude, evaluation resources for any given program area may not be available for a long time and the approach proposed here is therefore infeasible. Second, it is sometimes plausible to argue that the effectiveness of the program is itself changing. Society is evolving quickly; an approach that worked 10 years ago may no longer work 10 years from now.</p><p>Even within our framework, several strategies exist to shrink any increase in time lines. First, conduct the process analyses relatively quickly and expect prompt reporting of results. Second, issue one conditional contract (see the earlier discussion about how to minimize the bias of doing so). Third, lean against repeated cycles of formative evaluation and process evaluation.</p><p>We acknowledge that the more salient either of these two reasons (i.e., the political cycle and shifting impact), the less attractive will be the lengthening of evaluation time lines implied by our approach. Those crafting research strategies will need to weigh our proposed approach’s longer time lines against its possible advantages: finding more programs that work by helping allocate scarce evaluation resources only toward those programs that are truly ready for RIE.</p></sec></body><back><ack><title>Acknowledgments</title><p>The authors thank numerous people who suggested the examples. This paper only represents the position of the authors, not of those who suggested examples, or of Abt Associates, the American Institutes for Research, or their research clients.</p></ack><fn-group><fn fn-type="conflict" id="fn21-0193841X12474275"><label>Declaration of Conflicting Interests</label><p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn><fn fn-type="financial-disclosure" id="fn22-0193841X12474275"><label>Funding</label><p>The authors disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: Jacob Alex Klerman's time on this paper was funded with internal Abt Associates research funds.</p></fn></fn-group><notes><title>Notes</title><fn-group><fn fn-type="other" id="fn1-0193841X12474275"><label>1.</label><p>See <ext-link ext-link-type="uri" xlink:href="http://www.whitehouse.gov/administration/eop/sicp/initiatives/innovation-funds">http://www.whitehouse.gov/administration/eop/sicp/initiatives/innovation-funds</ext-link>. In fact, the ideas in this article were partially stimulated by efforts related to those programs and their discussions about “evidence standards” (see e.g., Shah and Jolin 2012).</p></fn><fn fn-type="other" id="fn2-0193841X12474275"><label>2.</label><p>This division into “efficacy evaluation” and “impact evaluation” is the pure case (e.g., <xref ref-type="bibr" rid="bibr37-0193841X12474275">Society for Prevention Research 2004</xref>). It is useful in that initial implementations and impact evaluations often occur under the most favorable conditions and with the explicit intent of demonstrating that the program can work. Thus, such initial implementations often benefit from higher levels of funding than would be expected with broad rollout, often because of donated time, (e.g., from the program developer); available, excited, and committed staff; and some ideal initial siting (often the program model is developed to address problems at this site and to exploit resources at this site). When this is true, then evidence that the program works at that initial site is not evidence that it will work in the other sites which would adopt the program with broad rollout. One would therefore want a second rigorous impact evaluation under more realistic conditions. More broadly, evidence of impact in one site is often not replicated in a second site, so even when “real world” conditions are not present, a second rigorous impact evaluation (RIE) at a new site will usually be appropriate.</p></fn><fn fn-type="other" id="fn3-0193841X12474275"><label>3.</label><p>On the need for RIE, see <xref ref-type="bibr" rid="bibr37-0193841X12474275">Society for Prevention Research (2004</xref>), Ioannidis et al. (2001); <xref ref-type="bibr" rid="bibr1-0193841X12474275">Anderson (2010</xref>), GiveWell (n.d.), and <xref ref-type="bibr" rid="bibr33-0193841X12474275">Sawhill and Baron (2010</xref>). On the role of a second round of rigorous impact evaluation under more realistic conditions, see <xref ref-type="bibr" rid="bibr37-0193841X12474275">Society for Prevention Research (2004</xref>) and <xref ref-type="bibr" rid="bibr18-0193841X12474275">McDonald (2009</xref>, <xref ref-type="bibr" rid="bibr19-0193841X12474275">2010</xref>).</p></fn><fn fn-type="other" id="fn4-0193841X12474275"><label>4.</label><p>On the term <italic>randomistas</italic>, see <xref ref-type="bibr" rid="bibr30-0193841X12474275">Ravallion (2008</xref>) in the developing country evaluation context. Our critique is very different from that of Ravallion.</p></fn><fn fn-type="other" id="fn5-0193841X12474275"><label>5.</label><p>This article is not the place to define “rigorous impact evaluation.” <xref ref-type="bibr" rid="bibr35-0193841X12474275">Shadish, Cook, and Campbell (2001</xref>) is a conventional and thorough discussion. As Berk (2011) emphasizes, the right approach to impact evaluation will vary with the details of the program to be evaluated. Random assignment often has a privileged place in the hierarchy of rigorous evaluation methods, but it is not always appropriate (at least as often implemented) and often other methods—for example, regression discontinuity, difference in differences, propensity score matching—are sufficient or even preferable.</p></fn><fn fn-type="other" id="fn6-0193841X12474275"><label>6.</label><p>See for example the discussion of specific examples and overall patterns at <xref ref-type="bibr" rid="bibr7-0193841X12474275">Coalition for Evidence Based Policy (2009</xref>).</p></fn><fn fn-type="other" id="fn7-0193841X12474275"><label>7.</label><p>Carol Weiss has expressed related ideas (e.g., <xref ref-type="bibr" rid="bibr41-0193841X12474275">Weiss 1997</xref>).</p></fn><fn fn-type="other" id="fn8-0193841X12474275"><label>8.</label><p>See also how Connell and Kubisch (1998) explain how the theory of change approach can aid in evaluating comprehensive community initiatives.</p></fn><fn fn-type="other" id="fn9-0193841X12474275"><label>9.</label><p><xref ref-type="bibr" rid="bibr41-0193841X12474275">Weiss (1997</xref>) also argues for theory in a more scientific sense; that is, that we can use evaluation to learn fundamental things about human behavior and successful programs. We are sympathetic to that basic premise, but that is not our point in this article. Like randomistas, we would argue that that type of theory-based learning requires RIE.</p></fn><fn fn-type="other" id="fn10-0193841X12474275"><label>10.</label><p><xref ref-type="bibr" rid="bibr9-0193841X12474275">Conrad et al. 1999</xref>; <xref ref-type="bibr" rid="bibr20-0193841X12474275">McLaughlin and Jordan 1999</xref>; <xref ref-type="bibr" rid="bibr32-0193841X12474275">Rogers 2005</xref>; <xref ref-type="bibr" rid="bibr40-0193841X12474275">Valley of the Sun United Way 2008</xref>; <xref ref-type="bibr" rid="bibr45-0193841X12474275">W. K. Kellogg Foundation 2004</xref>.</p></fn><fn fn-type="other" id="fn11-0193841X12474275"><label>11.</label><p>Quantitative goals for long-term impacts would themselves be useful at the rigorous impact stage. They would help to power (i.e., choose sample size) such studies and they would help in interpreting the results of those studies.</p></fn><fn fn-type="other" id="fn12-0193841X12474275"><label>12.</label><p>The importance of the social problem and the degree of difficulty of the solution are also important considerations.</p></fn><fn fn-type="other" id="fn13-0193841X12474275"><label>13.</label><p>On this, see <xref ref-type="bibr" rid="bibr35-0193841X12474275">Shadish, Cook, and Campbell (2001</xref>, 52–53): “However, specifying such an effect size is a political act, because a reference point is then created against which an innovation can be evaluated. Thus, even if an innovation has a partial effect, it may not be given credit for this if the promised effect size has not been achieved. Hence, managers of education programs learn to assert: ‘We want to increase achievement,’ rather than stating, ‘We want to increase achievement by two years for every year of teaching.’” We are arguing that funders should require program developers to be more specific, as a condition for funding.</p></fn><fn fn-type="other" id="fn14-0193841X12474275"><label>14.</label><p>The relative intensity of these two motives (overpromise and lowball) are likely to differ across cases, with overselling needed to overcome resistance in some instances but not others. We recognize that the two motives may not always balance.</p></fn><fn fn-type="other" id="fn15-0193841X12474275"><label>15.</label><p>See the quote from <xref ref-type="bibr" rid="bibr35-0193841X12474275">Shadish, Cook, and Campbell (2001</xref>) in the previous footnote. We note that some programs and government agencies are already following our suggested approach. For example, the recently released United States Agency for International Development Evaluation Policy notes that “Compared to evaluations of projects with weak or vague causal maps and articulation of aims, we can expect to learn much more from evaluations of projects that are designed from the outset with clear development hypotheses, realistic expectations of the value and scale of results, and clear understanding of implementation risks” (United States Agency for International Development [USAID] 2011).</p></fn><fn fn-type="other" id="fn16-0193841X12474275"><label>16.</label><p>We have implicitly ruled out the possibility of “sleeper effects,” programs that fail to show short-term effects nevertheless showing long-term effects.</p></fn><fn fn-type="other" id="fn17-0193841X12474275"><label>17.</label><p>The caveat “its own logic model” is crucial. Sometimes programs have long-term impacts without satisfying their own logic model. In that case, the program must have had some impact through some unanticipated pathway.</p></fn><fn fn-type="other" id="fn18-0193841X12474275"><label>18.</label><p>Relative to the ideal process evaluation, funding a single contract probably raises the likelihood of proceeding through to the RIE. The likelihood of proceeding through is raised because relations between the program and the evaluator create a form of capture. Successful technical assistance benefits from a strong rapport between the program and the technical assistance provider. Once that strong rapport and the attendant personal relationships are formed, it becomes much harder for the formative evaluation/technical assistance team—now in the process evaluator role—to state that the program has failed to meet its own logic model. It is not just such friendly relations that induce a bias toward proceeding to RIE. Once a single contract is issued for both steps, the evaluator has a strong financial interest in making the program (appear to) work. If the program does not work, then there is no second phase and hence no RIE. Contractors already face more than enough pressure to please the client (Klerman 2010; Brown and Klerman 2012); more pressure is not needed.</p></fn><fn fn-type="other" id="fn19-0193841X12474275"><label>19.</label><p>Considerations of conflict of interest on the part of the evaluator suggest that barring contractors at one phase from bidding on the next phase would be an even stronger procedural protection against a bias to proceed.</p></fn><fn fn-type="other" id="fn20-0193841X12474275"><label>20.</label><p>For example, comprehensive community initiatives.</p></fn></fn-group></notes><ref-list><title>References</title><ref id="bibr1-0193841X12474275"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>D.</given-names></name></person-group> <year>2010</year>. <article-title>“Proven Programs are the Exception, not the Rule [Web log post].”</article-title> <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://blog.givewell.org/2008/12/18/guest-post-proven-programs-are-the-exception-not-the-rule/">http://blog.givewell.org/2008/12/18/guest-post-proven-programs-are-the-exception-not-the-rule/</ext-link></citation></ref><ref id="bibr2-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Azurdia</surname><given-names>G.</given-names></name><name><surname>Barnes</surname><given-names>Z.</given-names></name></person-group>. <year>2008</year>. <source>The Employment Retention and Advancement Projects: Impacts for Portland’s Career Builders Program</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Manpower Demonstration Research Corporation</publisher-name>.</citation></ref><ref id="bibr2a-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Berk</surname><given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>Evidence-Based Versus Junk-Based Evaluation Research: Some Lessons from 35 Years of the Evaluation Review</article-title>. <source>Evaluation Review</source>, <volume>35</volume>(<issue>3</issue>), <fpage>191</fpage>–<lpage>203</lpage>.</citation></ref><ref id="bibr3-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>L.</given-names></name><name><surname>Dun Rappaport</surname><given-names>C.</given-names></name><name><surname>Olsho</surname><given-names>L.</given-names></name><name><surname>Hunt</surname><given-names>D.</given-names></name><name><surname>Levin</surname><given-names>M.</given-names></name></person-group>. <year>2009</year>. <article-title>“Impact Evaluation of the U.S. Department of Education’s Student Mentoring Program”</article-title> <comment>NCEE 2009-4047, National Center for Education Evaluation and Regional Assistance, Institute of Education Sciences, U.S. Department of Education, Washington, DC</comment>.</citation></ref><ref id="bibr4-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Bos</surname><given-names>J. M.</given-names></name><name><surname>Scrivener</surname><given-names>S.</given-names></name><name><surname>Snipes</surname><given-names>J.</given-names></name><name><surname>Hamilton</surname><given-names>G.</given-names></name></person-group>. <year>2002</year>. <source>Improving Basic Skills: The Effects of Adult Education in Welfare-to-Work Programs</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Manpower Demonstration Research Corporation</publisher-name>.</citation></ref><ref id="bibr4a-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>A.</given-names></name><name><surname>Klerman</surname><given-names>J. A.</given-names></name></person-group>. <year>2012</year>. <article-title>“Insights from Public Accounting for Contracting for Independent Evaluation”</article-title>. <source>Evaluation Review</source> <volume>36</volume>:<issue>4</issue>(June):<fpage>185</fpage>–<lpage>2197</lpage>. <comment>doi 10.1177/0193841X12450163</comment></citation></ref><ref id="bibr5-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>D. T.</given-names></name></person-group> <year>1969</year>. <article-title>“Reforms as Experiments.”</article-title> <source>American Psychologist</source> <volume>24</volume>:<fpage>409</fpage>–<lpage>29</lpage>.</citation></ref><ref id="bibr6-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>D. T.</given-names></name></person-group> <year>1984</year>. <article-title>“Can we be Scientific in Applied Social Science?”</article-title> <source>Evaluation Studies Review Annual</source> <volume>9</volume>:<fpage>26</fpage>–<lpage>48</lpage>.</citation></ref><ref id="bibr7-0193841X12474275"><citation citation-type="web"><collab collab-type="author">Coalition for Evidence Based Policy</collab>. <year>2009</year>. <article-title>“The Coalition is Pleased with GAO’s Confirmation of the Top Tier Initiative’s Adherence to Rigorous Standards and Overall Transparency.”</article-title> Accessed January 3, 2013, <ext-link ext-link-type="uri" xlink:href="http://coalition4evidence.org/wordpress/?page_id=410">http://coalition4evidence.org/wordpress/?page_id=410</ext-link>. </citation></ref><ref id="bibr8-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Connell</surname><given-names>J. P.</given-names></name><name><surname>Kubisch</surname><given-names>A. C.</given-names></name></person-group>. <year>1998</year>. <article-title>“Applying a Theory of Change Approach to the Evaluation of Comprehensive Community Initiatives: Progress, Prospects, and Problems.”</article-title> <source>The Aspen Institute</source>. <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="https://communities.usaidallnet.gov/fa/system/files/Applying+Theory+of+Change+Approach.pdf">https://communities.usaidallnet.gov/fa/system/files/Applying+Theory+of+Change+Approach.pdf</ext-link></citation></ref><ref id="bibr9-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Conrad</surname><given-names>K. J.</given-names></name><name><surname>Randolph</surname><given-names>F.</given-names></name><name><surname>Kirby</surname><given-names>M.</given-names><suffix>Jr</suffix></name><name><surname>Bebout</surname><given-names>R. R.</given-names></name></person-group> <year>1999</year>. <article-title>“Creating and using Logic Models—Four Perspectives.”</article-title> <source>Alcoholism Treatment Quarterly</source> <volume>17</volume>:<fpage>17</fpage>–<lpage>31</lpage>.</citation></ref><ref id="bibr10-0193841X12474275"><citation citation-type="web"><collab collab-type="author">GiveWell</collab>. (<year>n.d</year>.). <article-title>“Social Programs that Just don’t Work.”</article-title> <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.givewell.org/giving101/Social-Programs-That-Just-Dont-Work">http://www.givewell.org/giving101/Social-Programs-That-Just-Dont-Work</ext-link></citation></ref><ref id="bibr13-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>J. P. A.</given-names></name><name><surname>Haidich</surname><given-names>A. B.</given-names></name><name><surname>Pappa</surname><given-names>M.</given-names></name><name><surname>Pantazis</surname><given-names>N.</given-names></name><name><surname>Kokori</surname><given-names>S. I.</given-names></name><name><surname>Tektonidou</surname><given-names>M. G.</given-names></name><name><surname>Contopoulos-Ionnidis</surname><given-names>D. G.</given-names></name><name><surname>Lau</surname><given-names>J.</given-names></name></person-group>. <year>2001</year>. <article-title>“Comparison of Evidence of Treatment Effects in Randomized and Nonrandomized Studies.”</article-title> <source>Journal of the American Medical Association</source> <volume>286</volume>:<fpage>821</fpage>–<lpage>30</lpage>.</citation></ref><ref id="bibr14-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Jalbert</surname><given-names>S.</given-names></name><name><surname>Rhodes</surname><given-names>W.</given-names></name></person-group>. <year>2010</year>. <source>Reduced Caseloads Improve Probation Outcomes in Oklahoma City</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Abt Associates</publisher-name>.</citation></ref><ref id="bibr15-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>James-Burdumy</surname><given-names>S.</given-names></name><name><surname>Deke</surname><given-names>J.</given-names></name><name><surname>Lugo-Gil</surname><given-names>J.</given-names></name><name><surname>Crey</surname><given-names>N.</given-names></name><name><surname>Hershey</surname><given-names>A.</given-names></name><name><surname>Gersten</surname><given-names>R.</given-names></name><name><surname>Newman-Gonchar</surname><given-names>R.</given-names></name><name><surname>Dimino</surname><given-names>J.</given-names></name><name><surname>Haymond</surname><given-names>K.</given-names></name><name><surname>Faddis</surname><given-names>B.</given-names></name></person-group>. <year>2010</year>. <article-title>“Effectiveness of Selected Supplemental Reading Comprehension Interventions: Findings from Two Student Cohorts.”</article-title> <comment>NCEE 2010-4015, National Center for Education Evaluation and Regional Assistance, Institute of Education Sciences, U.S. Department of Education, Washington, DC</comment>.</citation></ref><ref id="bibr16-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Judkins</surname><given-names>D.</given-names></name><name><surname>St.Pierre</surname><given-names>R. B.</given-names></name><name><surname>Goodson</surname> <given-names>B.</given-names></name><name><surname>Gutmann</surname><given-names>B.</given-names></name><name><surname>von Glatz</surname><given-names>A.</given-names></name><name><surname>Hamilton</surname><given-names>J.</given-names></name><name><surname>Webber</surname><given-names>A.</given-names></name><name><surname>Troppe</surname><given-names>P.</given-names></name><name><surname>Rimdzius</surname><given-names>T.</given-names></name></person-group> <year>2008</year>. <article-title>“A Study of Classroom Literacy Interventions and Outcomes in Even Start.”</article-title> <comment>NCEE 20084028, National Center for Education Evaluation and Regional Assistance, Institute of Education Sciences, U.S. Department of Education, Washington, DC</comment>.</citation></ref><ref id="bibr16a-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Klerman</surname><given-names>J. A.</given-names></name></person-group> <year>2010</year>. <article-title>“Contracting for Independent Evaluation: Approaches to an Inherent Tension.”</article-title> <source>Evaluation Review</source> <volume>34</volume>(<issue>4, August</issue>):<fpage>299</fpage>–<lpage>333</lpage>.</citation></ref><ref id="bibr17-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Leviton</surname><given-names>L. C.</given-names></name><name><surname>Gutman</surname><given-names>M. A.</given-names></name></person-group>. <year>2010</year>. <article-title>“Overview and Rationale for the Systematic Screening and Assessment Method.”</article-title> In <person-group person-group-type="editor"><name><surname>Leviton</surname><given-names>L. C.</given-names></name><name><surname>Kettel Khan</surname><given-names>L.</given-names></name><name><surname>Dawkins</surname><given-names>N.</given-names></name></person-group> (Eds.), <source>The Systematic Screening and Assessment Method: Finding Innovations Worth Evaluating. New Directions for Evaluation</source> <volume>125</volume>:<fpage>7</fpage>–<lpage>31</lpage>.</citation></ref><ref id="bibr18-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>McDonald</surname><given-names>S.-K.</given-names></name></person-group> <year>2009</year>. <article-title>“Scale-up as a Framework for Intervention, Program, and Policy Evaluation Research.”</article-title> In <source>Handbook of Education Policy Research</source> (pp. <fpage>191</fpage>–<lpage>208</lpage>), edited by <person-group person-group-type="editor"><name><surname>Sykes</surname><given-names>G.</given-names></name><name><surname>Schneider</surname><given-names>B.</given-names></name><name><surname>Plank</surname><given-names>D. N.</given-names></name></person-group>. <publisher-loc>New York</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation></ref><ref id="bibr19-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>McDonald</surname><given-names>S.-K.</given-names></name></person-group> <year>2010</year>. <article-title>“Developmental Stages for Evaluating Scale.”</article-title> <source>The Evaluation Exchange</source> <volume>15</volume>. <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.hfrp.org/evaluation/the-evaluation-exchange/issue-archive/current-issue-scaling-impact/developmental-stages-for-evaluating-scale">http://www.hfrp.org/evaluation/the-evaluation-exchange/issue-archive/current-issue-scaling-impact/developmental-stages-for-evaluating-scale</ext-link></citation></ref><ref id="bibr20-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>McLaughlin</surname><given-names>J. A.</given-names></name><name><surname>Jordan</surname><given-names>G. B.</given-names></name></person-group>. <year>1999</year>. <article-title>“Logic Models: A Tool for Telling your Program’s Performance Story.”</article-title> <source>Evaluation and Program Planning</source> <volume>22</volume>:<fpage>65</fpage>–<lpage>72</lpage>.</citation></ref><ref id="bibr21-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Meckstroth</surname><given-names>A.</given-names></name><name><surname>Burwick</surname><given-names>A.</given-names></name><name><surname>Ponza</surname><given-names>M.</given-names></name><name><surname>Marsh</surname><given-names>S.</given-names></name><name><surname>Novak</surname><given-names>T.</given-names></name><name><surname>Phillips</surname><given-names>S.</given-names></name><name><surname>Diaz-Tena</surname><given-names>N. </given-names></name><name><surname>Ng</surname><given-names>J.</given-names></name></person-group>. <year>2006</year>. <article-title>“Paths to Work in Rural Places: Key Findings and Lessons from the Impact Evaluation of the Future Steps Rural Welfare-to-Work Program.”</article-title> <comment>Final report MPR No. 8762-192, 202. Mathematica Policy Research, Princeton, NJ</comment>.</citation></ref><ref id="bibr22-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>C.</given-names></name><name><surname>Martin</surname><given-names>V.</given-names></name><name><surname>Hamilton</surname><given-names>G.</given-names></name></person-group>. <year>2008</year>. <source>The Employment Retention and Advancement Projects: Findings for the Cleveland Achieve Model: Implementation and Early Impacts of an Employer-Based Approach to Encourage Employment Retention among Low-Wage Workers</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Manpower Demonstration Research Corporation</publisher-name>.</citation></ref><ref id="bibr23-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Molina</surname><given-names>F.</given-names></name><name><surname>Cheng</surname><given-names>W.-L.</given-names></name><name><surname>Hendra</surname><given-names>R.</given-names></name></person-group>. <year>2008</year>. <source>The Employment Retention and Advancement Project: Results from the Valuing Individual Success and Increasing Opportunities Now (VISION) Program in Salem, Oregon</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Manpower Demonstration Research Corporation</publisher-name>.</citation></ref><ref id="bibr25-0193841X12474275"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Obama</surname><given-names>B.</given-names></name></person-group> <year>2009</year>. <article-title>“Remarks of President Obama on Community Solutions Agenda.”</article-title> <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.nationalservice.gov/about/newsroom/statements_detail.asp?tbl_pr_id=1828">http://www.nationalservice.gov/about/newsroom/statements_detail.asp?tbl_pr_id=1828</ext-link></citation></ref><ref id="bibr27-0193841X12474275"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Orszag</surname><given-names>P. R.</given-names></name></person-group> <year>2009a</year> <article-title>“Building Rigorous Evidence to Drive Policy [Web log post].”</article-title> <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.whitehouse.gov/omb/blog/09/06/08/BuildingRigorousEvidencetoDrivePolicy">http://www.whitehouse.gov/omb/blog/09/06/08/BuildingRigorousEvidencetoDrivePolicy</ext-link></citation></ref><ref id="bibr28-0193841X12474275"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Orszag</surname><given-names>P. R.</given-names></name></person-group> <year>2009b</year>. <article-title>“Increased Emphasis on Program Evaluation [Memorandum].”</article-title> <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.whitehouse.gov/omb/assets/memoranda_2010/m10-01.pdf">http://www.whitehouse.gov/omb/assets/memoranda_2010/m10-01.pdf</ext-link> </citation></ref><ref id="bibr29-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Pawson</surname><given-names>R.</given-names></name><name><surname>Tilley</surname><given-names>N.</given-names></name></person-group>. <year>1997</year>. <source>Realistic Evaluation</source>. <publisher-loc>London, England</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation></ref><ref id="bibr30-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ravallion</surname><given-names>M.</given-names></name></person-group> <year>2008</year>. <article-title>“Should the Randomistas Rule?” <italic>The Economists’</italic></article-title> <source>Voice</source> <comment>6: article 6</comment>.</citation></ref><ref id="bibr31-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>P.</given-names></name><name><surname>Petrosino</surname><given-names>A.</given-names></name><name><surname>Huebner</surname><given-names>T.</given-names></name><name><surname>Hacsi</surname><given-names>T.</given-names></name></person-group>. <year>2000</year>. <article-title>“Program Theory Evaluation: Practice, Promise, and Problems.”</article-title> <source>New Directions in Evaluation</source> <volume>87</volume>:<fpage>5</fpage>–<lpage>13</lpage>.</citation></ref><ref id="bibr32-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>P. J.</given-names></name></person-group> <year>2005</year>. <article-title>“Logic Models.”</article-title> In <source>Encyclopedia of Evaluation</source>, edited by <person-group person-group-type="editor"><name><surname>Mathison</surname><given-names>S.</given-names></name></person-group>, <fpage>232</fpage>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation></ref><ref id="bibr33-0193841X12474275"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sawhill</surname><given-names>I. V.</given-names></name><name><surname>Baron</surname><given-names>J.</given-names></name></person-group>. <year>2010</year>. <article-title>“We Need a New Start for Head Start.”</article-title> <source>Education Week</source> <volume>29</volume>:<fpage>22</fpage>–<lpage>23</lpage>.</citation></ref><ref id="bibr34-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Scrivener</surname><given-names>S.</given-names></name><name><surname>Azurdia</surname><given-names>G.</given-names></name><name><surname>Page</surname><given-names>J.</given-names></name></person-group>. <year>2005</year>. <source>The Employment Retention and Advancement Projects: Results from the South Carolina ERA site</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Manpower Demonstration Research Corporation</publisher-name>.</citation></ref><ref id="bibr35-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Shadish</surname><given-names>W. R.</given-names></name><name><surname>Cook</surname><given-names>T. D.</given-names></name><name><surname>Campbell</surname><given-names>D. T.</given-names></name></person-group>. <year>2001</year>. <source>Experimental and Quasi-Experimental Designs for Generalized Causal Inference</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Houghton Mifflin</publisher-name>.</citation></ref><ref id="bibr36-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Shah, Shivam</surname><given-names>Mallick</given-names></name><name><surname>Jolin</surname><given-names>Michele</given-names></name></person-group> <year>2012</year>. <article-title>“Social Sector Innovation Funds: Lessons Learned and Recommendations.”</article-title> <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.americanprogress.org/wp-content/uploads/2012/11/JolinInnovationFunds-2.pdf">http://www.americanprogress.org/wp-content/uploads/2012/11/JolinInnovationFunds-2.pdf</ext-link>. <publisher-name>America Achieves and the Center for American Progress</publisher-name>.</citation></ref><ref id="bibr37-0193841X12474275"><citation citation-type="book"><collab collab-type="author">Society for Prevention Research</collab>. <year>2004</year>. <source>Standards of Evidence: Criteria for Efficacy, Effectiveness and Dissemination</source>. <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.preventionresearch.org/sofetext.php">http://www.preventionresearch.org/sofetext.php</ext-link></citation></ref><ref id="bibr39-0193841X12474275"><citation citation-type="book"><collab collab-type="author">United States Agency for International Development</collab>. <year>2011</year>. <source>USAID Evaluation Policy. Bureau for Policy, Planning, and Learning</source>. <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.usaid.gov/evaluation/">http://www.usaid.gov/evaluation/</ext-link></citation></ref><ref id="bibr40-0193841X12474275"><citation citation-type="book"><collab collab-type="author">Valley of the Sun United Way</collab>. <year>2008</year>. <source>Logic Model Handbook 2008</source>. <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://vsuw.org/file/logic_model_handbook_updated_2008.pdf">http://vsuw.org/file/logic_model_handbook_updated_2008.pdf</ext-link></citation></ref><ref id="bibr41-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>C.</given-names></name></person-group> <year>1997</year>. <source>Evaluation</source>, <edition>2nd ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation></ref><ref id="bibr43-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Wholey</surname><given-names>J.</given-names></name></person-group> <year>1994</year>. <article-title>“Assessing the Feasibility and Likely Usefulness of Evaluation.”</article-title> In <source>Handbook of Practical Program Evaluation</source>, edited by <person-group person-group-type="editor"><name><surname>Hatry</surname><given-names>H. P.</given-names></name><name><surname>Wholey</surname><given-names>J. S.</given-names></name><name><surname>Newcomer</surname><given-names>K. E.</given-names></name></person-group>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation></ref><ref id="bibr44-0193841X12474275"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Wood</surname><given-names>R. G.</given-names></name><name><surname>McConnell</surname><given-names>S.</given-names></name><name><surname>Moore</surname><given-names>Q.</given-names></name><name><surname>Clarkwest</surname><given-names>A.</given-names></name><name><surname>Hseuh</surname><given-names>J.</given-names></name></person-group>. <year>2010</year>. <article-title>“The Building Strong Families Project: Strengthening Unmarried Parents’</article-title> <comment>Relationships: The Early Impacts of Building Strong Families: Executive Summary.” MPR No. 08935.155</comment>. <publisher-name>Mathematica Policy Research</publisher-name>, <publisher-loc>Princeton, NJ</publisher-loc>.</citation></ref><ref id="bibr45-0193841X12474275"><citation citation-type="book"><collab collab-type="author">W. K. Kellogg Foundation</collab>. <year>2004</year>. <source>Using Logic Models to Bring Together Planning, Evaluation and Action: Logic Model Development Guide</source>. <publisher-loc>Battle Creek, MI</publisher-loc>: <publisher-name>W.W. Kellogg Foundation</publisher-name>. <comment>Accessed January 3, 2013</comment>, <ext-link ext-link-type="uri" xlink:href="http://www.ncga.state.nc.us/PED/Resources/documents/LogicModelGuide.pdf">http://www.ncga.state.nc.us/PED/Resources/documents/LogicModelGuide.pdf</ext-link></citation></ref></ref-list></back></article>