<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EDM</journal-id>
<journal-id journal-id-type="hwp">spedm</journal-id>
<journal-title>Journal of Cognitive Engineering and Decision Making</journal-title>
<issn pub-type="ppub">1555-3434</issn>
<issn pub-type="epub">XXXX-XXXX</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1555343412445474</article-id>
<article-id pub-id-type="publisher-id">10.1177_1555343412445474</article-id>
<title-group>
<article-title>System Dynamics Modeling of Sensory-Driven Decision Priming</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Schmorrow</surname><given-names>Dylan D.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Bolstad</surname><given-names>Cheryl A.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>May</surname><given-names>Katrina A.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Cuevas</surname><given-names>Haydee M.</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Patterson</surname><given-names>Robert Earl</given-names></name>
<aff id="aff1-1555343412445474">Air Force Research Laboratory</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Fournier</surname><given-names>Lisa R.</given-names></name>
<aff id="aff2-1555343412445474">Washington State University</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Williams</surname><given-names>Logan</given-names></name>
<aff id="aff3-1555343412445474">Air Force Research Laboratory</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Amann</surname><given-names>Ryan</given-names></name>
<aff id="aff4-1555343412445474">L-3 Communications</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Tripp</surname><given-names>Lisa M.</given-names></name>
<aff id="aff5-1555343412445474">Washington State University</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Pierce</surname><given-names>Byron J.</given-names></name>
<aff id="aff6-1555343412445474">Air Force Research Laboratory</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-1555343412445474">Robert Patterson, 711 HPW/RHXM, 2215 First Street, Bldg 33, Wright-Patterson AFB, OH 45433, <email>Robert.Patterson@wpafb.af.mil</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>7</volume>
<issue>1</issue>
<issue-title>Special Section: Exploring Cognitive Readiness in Complex Operational Environments: Advances in Theory and Practice, Part III</issue-title>
<fpage>3</fpage>
<lpage>25</lpage>
<permissions>
<copyright-statement>© 2012, Human Factors and Ergonomics Society.</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<p>The authors present an empirical investigation and a system dynamics model of human <italic>decision priming</italic>. Decision priming occurs when initial information creates the expectation that a given decision is appropriate, which speeds up or slows down decision making. A conjunction benefits-and-costs paradigm was used to collect the empirical data, whereas system dynamics techniques were used to create a computational model of decision priming. Decision priming occurred with simulated naturalistic stimuli (i.e., models of military tanks in a desert scene presented in perspective view), the results of which were modeled in a parallel-channels coactive architecture. Simulation revealed that the basic features of decision priming in humans could be simulated with this architecture. Decision priming likely occurs in naturalistic settings. Predictions derived from the model could provide useful information for the design of multimodal or multichannel displays.</p>
</abstract>
<kwd-group>
<kwd>decision making</kwd>
<kwd>decision priming</kwd>
<kwd>system dynamics modeling</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1555343412445474" sec-type="intro">
<title>Introduction</title>
<p>Decision making can be difficult and subject to errors, especially in stressful conditions or in conditions of divided attention. For example, making a quick decision about an upcoming exit ramp on a highway can be difficult while driving at night or driving in the fog. As another example, a military pilot may need to make a quick decision during a mission while being presented with visual sensor information or auditory information that conflicts with an out-the-window view.</p>
<p>Decision processing in these kinds of real-world situations may involve having critical information about the attributes of different options distributed in time, with some information occurring earlier in time and other information occurring later (i.e., the timing of information about different attributes may be staggered). Moreover, the decision to be made may involve deciding whether to perform an action, depending on the available evidence at hand, which may be time dependent.</p>
<p>This article presents an experimental study and computational model of <italic>decision priming</italic>, a pattern recognition–based decision process that is time dependent. Decision priming occurs when an observer is presented with multiple sources of information that are distributed in time, and the information that is processed first creates an expectation that a certain decision is appropriate. If the expectation is correct, then the decision-making process may be accelerated; however, if the expectation is wrong, then the decision-making process may be delayed. In either case, the expectation primes or biases the decision-making process toward a given state, which is appropriate in the former case and inappropriate in the latter case and therefore must be countermanded. In the cognitive psychology literature, these two outcomes owing to decision priming have been termed <italic>conjunction benefits</italic> (decision making is accelerated) and <italic>conjunction costs</italic> (decision making is delayed), which we now discuss.</p>
<sec id="section2-1555343412445474">
<title>Conjunction Benefits and Conjunction Costs</title>
<p>Conjunction benefits can occur, for example, when an individual, such as an image analyst, has to render a split-second decision about the presence or absence of a certain vehicle (i.e., the target) in a scene displayed on a video monitor on the basis of two or more features of the vehicle, such as its color and shape. If one feature, such as color, is visually processed faster than the other feature, such as shape, then information about color can create an expectation that the target vehicle is present, which will speed up the decision to respond <italic>present</italic> if the target vehicle is actually present. In this case, the feature for which processing is the fastest primes the decision made about the target object as a whole (which is conceptualized as a conjunction of the target features). This effect is called <italic>conjunction benefits</italic>.</p>
<p>Evidence for conjunction benefits comes from laboratory research showing that when two stimulus features differ in their processing speed, decisions that the two features are present together are typically faster than decisions made about the presence of the single feature for which processing is the slowest (<xref ref-type="bibr" rid="bibr11-1555343412445474">Fournier, Bowd, &amp; Herbert, 2000</xref>; <xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier, Eriksen, &amp; Bowd, 1998</xref>; <xref ref-type="bibr" rid="bibr13-1555343412445474">Fournier, Herbert, &amp; Farris, 2004</xref>; <xref ref-type="bibr" rid="bibr14-1555343412445474">Fournier, Patterson, Dyre, Wiediger, &amp; Winters, 2007</xref>; <xref ref-type="bibr" rid="bibr15-1555343412445474">Fournier, Scheffers, Coles, Adamson, &amp; Vila, 2000</xref>). It is thought that the accumulating information indicating the presence of the feature for which processing is fastest can partially activate (prime) a central decision process that the feature conjunction is present before information arrives about the feature for which processing is the slowest; hence the conjunction shows a “benefit” relative to the slowest feature. Subsequently, the accumulating information indicating the presence of the latter feature will also contribute to the decision made about the presence of the conjunction (<xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier et al., 1998</xref>, <xref ref-type="bibr" rid="bibr13-1555343412445474">2004</xref>, <xref ref-type="bibr" rid="bibr14-1555343412445474">2007</xref>; <xref ref-type="bibr" rid="bibr11-1555343412445474">Fournier, Bowd, et al., 2000</xref>; <xref ref-type="bibr" rid="bibr15-1555343412445474">Fournier, Scheffers, et al., 2000</xref>).</p>
<p>However, conjunction costs can occur in this example when information about color creates an expectation that the target vehicle is present, which ends up delaying the decision to respond <italic>absent</italic> if the target vehicle is actually absent (e.g., it was a different vehicle of the same color). Again, the feature for which processing is the fastest primes the decision made about the target object as a whole, which in this case is incorrect, and the priming must be countermanded. This effect is called <italic>conjunction costs</italic>.</p>
<p>Evidence for conjunction costs comes from laboratory research showing that when two stimulus features differ in their processing speed, decisions that a feature conjunction is absent are typically slower when the feature for which processing is the fastest is present than when neither feature is present (<xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier et al., 1998</xref>, <xref ref-type="bibr" rid="bibr13-1555343412445474">2004</xref>, <xref ref-type="bibr" rid="bibr14-1555343412445474">2007</xref>; <xref ref-type="bibr" rid="bibr11-1555343412445474">Fournier, Bowd, et al., 2000</xref>; <xref ref-type="bibr" rid="bibr15-1555343412445474">Fournier, Scheffers, et al., 2000</xref>). It is thought that the feature for which processing is the fastest incorrectly primes a central decision process that the feature conjunction is present. Delayed activation resulting from the feature for which processing is the slowest must override this primed decision, which takes additional processing time (<xref ref-type="bibr" rid="bibr7-1555343412445474">Eriksen &amp; Schultz, 1979</xref>; see also <xref ref-type="bibr" rid="bibr20-1555343412445474">Logan &amp; Cowan, 1984</xref>; <xref ref-type="bibr" rid="bibr31-1555343412445474">Schall, 2001</xref>, <xref ref-type="bibr" rid="bibr32-1555343412445474">2003</xref>); hence the conjunction shows a “cost.”</p>
<p>Decision priming would be expected to occur in those tasks for which individuals must make a speeded response on the basis of the presence of a visual target whose features are processed at different rates and the processing of which is critical to rendering a decision. For instance, in some Air Force situations involving unmanned aerial vehicles, two individuals control the vehicle: a pilot who controls flight of the aircraft and a sensor operator who controls the movements of the sensors. Two other individuals, who may be positioned at a location different from the pilot and sensor operator, provide analysis: an “eyes-on” person who views the video feeds coming from the sensors and a “screener” who communicates back to the pilot and sensor operator. The role of the eyes-on person is to look at the video imagery and to call out to the screener any activity of interest so that the screener can pass on that information, typically via typing on a keyboard, to the pilot and sensor operator so that they know where to move the sensor next.</p>
<p>When the eyes-on person scans the video feed for the presence of a particular target, and that target comprises features whose mental processing is staggered in time, the feature that is processed the fastest (e.g., direction of movement of the target) may lead to decision priming. This priming, in turn, may create a situation in which the eyes-on person initially calls out the presence of a target to the screener, and the latter begins to type in the message to the pilot and sensor operator, which then must be countermanded by the eyes-on person when he or she realizes that the target was not actually present in the video. Such a situation could lead to confusion and delay in the communication between the screener and the pilot and sensor operator. In a sense, the presence of multiple humans in the chain of communication may amplify the effects of decision priming.</p>
<p>It should be noted that decision priming may be related to several phenomena in the literature. First, decision priming may be analogous to the framing effects in judgment and decision making, wherein different linguistic descriptors of equivalent outcomes can lead to different choices (<xref ref-type="bibr" rid="bibr18-1555343412445474">Kahneman &amp; Tversky, 1979</xref>), which could be taken to be a form of linguistic priming. Decision priming may also be related to the multidimensional search literature, in which decisions made about feature conjunctions require attentional processing (<xref ref-type="bibr" rid="bibr38-1555343412445474">Treisman &amp; Gelade, 1980</xref>). Finally, decision priming may seem related to the development of automaticity, because the mapping between stimuli and responses is consistent (<xref ref-type="bibr" rid="bibr33-1555343412445474">Shiffrin &amp; Schneider, 1977</xref>); however, this comparison is not likely to be accurate, because decision priming involves decisions made about feature conjunctions that require attention (<xref ref-type="bibr" rid="bibr38-1555343412445474">Treisman &amp; Gelade, 1980</xref>), as noted earlier.</p>
<p>In the present study, we sought to create a computational model of decision priming that could be used in the future to explore various simulated outcomes and to identify how best to present information for rapid decision making when multimodal or multichannel displays are employed. For present purposes, we needed to conduct an empirical investigation to calibrate our model to one set of parameters and to demonstrate its feasibility. Accordingly, we report in this article the results of an initial investigation into documenting and modeling decision priming.</p>
<p>Because we sought to create an initial model of the dynamics of a pattern recognition–based decision process, we focused only on the perceptual recognition process and a simple decision mechanism. Accordingly, we set up our paradigm to focus on simulated real-world stimuli and ignored other issues that might occur within a more developed simulated real-world context, such as the distraction of attention, the performance of difficult sensory motor tasks, or the presence of high-stakes consequences when decision errors are made. Thus, we used models of vehicles presented within a simulated naturalistic scene, which is a relevant approach because recent studies have shown that individuals can extract the meaning from naturalistic scenes within a few hundred milliseconds (<xref ref-type="bibr" rid="bibr17-1555343412445474">Grill-Spector &amp; Kanwisher, 2005</xref>; <xref ref-type="bibr" rid="bibr26-1555343412445474">Oliva &amp; Torralba, 2006</xref>; <xref ref-type="bibr" rid="bibr35-1555343412445474">Thorpe, Fize, &amp; Marlot, 1996</xref>).</p>
<p>In the present article, we report the results of a decision-priming study in which we examined conjunction benefits and conjunction costs using briefly exposed stimuli composed of models of military tanks positioned on a desert terrain seen in perspective view. To anticipate, we found that decision priming in the form of conjunction costs did occur with our naturalistic stimuli. We then modeled these results within a computational framework using system dynamics modeling techniques (e.g., <xref ref-type="bibr" rid="bibr9-1555343412445474">Forrester, 1961</xref>, <xref ref-type="bibr" rid="bibr10-1555343412445474">1968</xref>; <xref ref-type="bibr" rid="bibr34-1555343412445474">Sterman, 2000</xref>).</p>
</sec></sec>
<sec id="section3-1555343412445474">
<title>Empirical Study</title>
<p>In creating this study, we imagined a hypothetical scenario in which participants would be required to make judgments about the presence or absence of target features of two types of military tanks (e.g., on joint maneuvers) viewed in a desert scene presented in perspective view. We chose tank models from America and Russia because these two tanks were not easily discriminated when positioned on the desert terrain. In doing so, it was important to keep recognition performance below 100% accuracy and thus avoid a ceiling-effect confound but also to keep recognition performance above 50% accuracy (which was chance performance) and thus avoid a floor-effect confound. We determined from preliminary experiments that these two types of tank models met these requirements.</p>
<p>Accordingly, the two target features of the tanks were (a) a particular color, either green or brown camouflage, and/or (b) a particular shape, either an American shape or a Russian shape (see top picture of <xref ref-type="fig" rid="fig1-1555343412445474">Figure 1</xref>). Each individual had to render a judgment about the presence or absence of a single feature (single-feature trials) or of the conjunction of two target features (feature conjunction trials) in the briefly-exposed stimulus. Across blocks of the single-feature trials, each feature (i.e., green, brown, American shape, Russian shape) was assigned to be the target equally often, the order of which was determined randomly for each participant. Across blocks of the feature conjunction trials, each combination of features (i.e., green American, brown American, green Russian, or brown Russian) was assigned to be the target conjunction equally often, the order of which was determined randomly for each participant.</p>
<fig id="fig1-1555343412445474" position="float">
<label>Figure 1.</label>
<caption>
<p>Top: Photograph showing an up-close view of the four feature conjunction objects positioned on the desert scene in perspective view. From left to right, the objects are a green American tank, a green Russian tank, a brown American tank, and a brown Russian tank (the text outlined in white shown above the objects in the photograph is given here only for information purposes; the text was not present during the actual study). Bottom: Photograph showing the display of a green American tank (positioned in the middle of the picture) on the desert terrain as viewed by a participant.</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig1.tif"/></fig>
<p>In conducting this study, we needed to control two potential biases during the feature conjunction trials. First, for the situation in which the four feature combinations (i.e., green American shape, brown American shape, green Russian shape, brown Russian shape) occurred equally often across trials, a tendency was created for an <italic>absent</italic> response bias, because the target conjunction would occur in only 25% of the trials and the nontarget conjunctions collectively would occur in 75% of the trials. The way to control for this <italic>absent</italic> response bias would be to present the target conjunction in 50% of the trials and the nontarget conjunctions collectively in 50% of the trials. However, for the latter situation, in which the target conjunction occurred in half the trials, a tendency was created for a target conjunction familiarity bias (<xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier et al., 1998</xref>; <xref ref-type="bibr" rid="bibr24-1555343412445474">Nickerson, 1972</xref>, <xref ref-type="bibr" rid="bibr25-1555343412445474">1973</xref>), because the target conjunction would occur more often than each of the three nontarget conjunctions. The way to control for this target conjunction familiarity bias would be to present the four feature combinations equally often, which could lead to an <italic>absent</italic> response bias, as stated earlier.</p>
<p>Thus, the way to control for both potential biases would be to run separate blocks of trials with (a) the four feature combinations occurring equally often, which controls for the target conjunction familiarity bias, and (b) the target conjunction occurring in half of the trials and the nontarget conjunctions collectively occurring in half of the trials, which controls for an <italic>absent</italic> response bias. Accordingly, there were two blocks of data collection for the feature conjunction trials: the 25% block, in which the target conjunction appeared in 25% of the trials and the three nontarget conjunctions collectively appeared in 75% of the trials, and the 50% block, in which the target conjunction appeared in 50% of the trials and the other three nontarget conjunctions collectively appeared in 50% of the trials (i.e., each nontarget conjunction appeared on 16.67% of the trials on average).</p>
<sec id="section4-1555343412445474">
<title>Participants</title>
<p>Eight individuals (6 male, 2 female; ages 21 to 57 years) participated in the study, all of whom possessed normal or corrected-to-normal acuity in both eyes (tested with the Optek 2000 test), and all gave documented informed consent. Two of the 8 participants are authors of this article (LW and RA). At time of testing, they were naive as to the test hypotheses, and their data showed trends that were the same as those shown by the remaining 6 participants.</p>
</sec>
<sec id="section5-1555343412445474">
<title>Stimuli and Apparatus</title>
<p>A model of a military tank was positioned in the middle of a perspective desert scene presented on a 61-in. Samsung DLP HD projection TV (Model HL61A750). The pixel resolution of the TV was 1,920 × 1,080. At a viewing distance of 1 m, the TV screen subtended 68.13° (horizontal) × 41.40° (vertical) at the eye of the participant.</p>
<p>There were four possible stimuli: The tank model was either an American M1A1 tank (43 mm × 13 mm; angular size, 2.46° × 0.74° at the eye) with a green camouflage pattern, an American M1A1 tank with a brown camouflage pattern, a Russian T62 tank (40 mm × 12 mm; angular size, 2.29° × 0.69°) with a green camouflage pattern, or a Russian T62 tank with a brown camouflage pattern. The overall luminance of the display was 14.26 cd/m<sup>2</sup>, as measured with a Minolta Luminance Meter, Model LS-100. (Note that we did not need to control the relative contrast of the two colors because we were concerned with creating stimuli whose properties led to sufficient reaction time [RT] differences in the single-feature trials so that decision priming could be investigated.)</p>
<p>The desert scene and tank were generated via MetaVR Virtual Reality Scene Generator (VRSG) Version 5.5, and the experimental trials were controlled via Expert Common Immersive Theatre Environment (XCITE) software, Version 3.0. A Logitech Extreme 3D Pro Joystick was used to collect participant responses and advance through the trials.</p>
</sec>
<sec id="section6-1555343412445474">
<title>Procedure</title>
<p>At the beginning of each trial, a message appeared on the display (e.g., <italic>Green; Brown American tank</italic>) that indicated the target single feature or target feature conjunction to be judged on that trial. Next, the stimulus for that trial—one of the tanks positioned in the middle of the desert scene—was exposed for a duration of 200 ms. The participant’s task was to determine whether the tank possessed the single target feature (single-feature trials), or a conjunction of two target features (feature conjunction trials) and to respond <italic>present</italic> or <italic>absent</italic>. RT and recognition accuracy were recorded.</p>
<p>Each participant completed 96 trials for each for the following four blocks: (a) color-only judgments and (b) shape-only judgments (both of which were single-feature trials), wherein 50% of the time the target color or target shape was present and 50% of the time the target color or target shape was absent; (c) color and shape judgments (feature conjunction trials) wherein 25% of the time the target conjunction was present and 75% of the time the target conjunction was absent, which defined the “25% block”; and (4) color and shape judgments (feature conjunction trials) wherein 50% of the time the target conjunction was present and 50% of the time the target conjunction was absent, which defined the “50% block.” Employing a within-subjects design, the order of presentation of the four blocks to individual participants was determined by a Latin square procedure.</p>
<p>In each of the four blocks just described, all four stimuli (i.e., green American tank, brown American tank, green Russian tank, brown Russian tank) were presented, but the particular kind of judgment required of the participant depended on whether the trials were single-feature trials or feature conjunction trials. The order of presentation of the four stimuli in each block was determined randomly for each participant. Auditory feedback informed the participant about the correctness of his or her response following each trial. Each participant was trained up to asymptotic performance before formal data collection began. During training, the stimuli were randomly presented to the participants in the same way as in the main study.</p>
<p>For statistical analyses, the single-feature trials and the 25% or 50% feature conjunction trials were combined into a variable called “feature condition,” with three levels: color, shape, and color-plus-shape. Next, this feature condition factor was crossed with a factor called “response type,” which had two levels, present versus absent, to create a 3 × 2 factorial design. (Note that the factor of response type reflected how the participant responded, <italic>present</italic> versus <italic>absent</italic>, with respect to the target color, target shape, or target conjunction of color-plus-shape, regardless of the accuracy of the response; the issue of accuracy is discussed later.) In this 3 × 2 design, the color-present condition and the shape-present condition entailed single-feature trials. Moreover, the single-feature trials that involved the absence of the target color or the target shape—that is, the color-absent and shape-absent single-feature trials—were ignored because they have no conceptual meaning for the phenomenon of priming. The other four conditions of the 3 × 2 design, namely, the color-plus-shape-present condition, color-absent condition, shape-absent condition, and the both-color-and-shape-absent condition, entailed 25% or 50% feature conjunction trials. Thus, the color-absent condition was really the color-absent, shape-present feature conjunction condition, and the shape-absent condition was really the color-present, shape-absent feature conjunction condition. Thus, a different 3 × 2 design was created by combining the 25% block or the 50% block together with the single-feature blocks. (In the literature, this method is typically used for analyzing conjunction benefits and conjunction costs data.)</p>
<p>For purposes of presentation in figures, however, the six conditions composing the 3 × 2 design were relabeled as six feature conditions. In this case, the first two conditions are single-feature trials: C (color present) and S (shape present). The latter four conditions are feature conjunction trials: C, S (color and shape present); C, nS (color present, shape absent); nC, S (color absent, shape present); and nC, nS (both color and shape absent).</p>
</sec></sec>
<sec id="section7-1555343412445474" sec-type="results">
<title>Results</title>
<sec id="section8-1555343412445474">
<title>The 25% Block</title>
<p>The top panel of <xref ref-type="fig" rid="fig2-1555343412445474">Figure 2</xref> shows RTs for the six feature conditions in the 25% block. The first two histogram bars from the left correspond to the single-feature trials, and the remaining four histogram bars correspond to the feature conjunction trials. A 2 × 3 analysis of variance for within-subjects designs, with Greenhouse-Geisser correction, revealed that there was a significant main effect of feature condition, <italic>F</italic>(1.1, 7.6) = 17.6, <italic>p</italic> &lt; .01. In a post hoc analysis, however, Tukey’s HSD test showed that none of the pairwise comparisons was significantly different from one another (with <italic>p</italic> &gt; .05). The analysis also revealed that there was no significant main effect of response type, <italic>F</italic>(1.0, 7.0) = 0.680, <italic>p</italic> &gt; .05. Finally, the analysis showed that the interaction between feature condition and response type was significant, <italic>F</italic>(1.8, 12.3) = 7.4, <italic>p</italic> &lt; .01.</p>
<fig id="fig2-1555343412445474" position="float">
<label>Figure 2.</label>
<caption>
<p>Empirical reaction time (upper graph) and accuracy (lower graph) shown for six feature conditions for the 25% block of trials (i.e., in which the target feature conjunction occurred in 25% of the trials). For the six feature conditions, C = target color only; S = target shape only; C, S = target color and target shape conjunction; C, nS = target color but not target shape; nC, S = target shape but not target color; nC, nS = neither target color nor target shape. Each mean is an average of eight observers. Error bars equal ±1 standard error of the mean.</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig2.tif"/></fig>
<p>This significant interaction was probed with a test of simple main effects wherein a one-way ANOVA (Greenhouse-Geisser correction) was computed on the <italic>present</italic> responses and on the <italic>absent</italic> responses, respectively. For the <italic>present</italic> responses, the analysis revealed that the effect of feature condition was significant, <italic>F</italic>(1.4, 9.6) = 11.8, <italic>p</italic> &lt; .01. Tukey’s HSD test showed that <italic>present</italic> responses were faster with the single feature of color (the C condition) than with the single feature of shape (the S condition), <italic>p</italic> &lt; .01. (However, there was no significant difference between present responses to the target conjunction of color and shape [the C, S condition] versus responses to the single feature of shape [the S condition], <italic>p</italic> &gt; .05.) For <italic>absent</italic> responses, the analysis showed that the effect of feature condition was significant, <italic>F</italic>(1.0, 7.3) = 17.2, <italic>p</italic> &lt; .01. Tukey’s HSD test revealed that responding was slower when color was present and shape was absent (the C, nS condition) versus when color was absent (and shape present; the nC, S condition) or when both color and shape were absent (the nC, nS condition), with both comparisons <italic>p</italic> &lt; .05.</p>
<p>The fact that responding <italic>present</italic> to the target conjunction (color and shape) was not significantly faster than responding to the single target feature for which responding was the slowest (shape) means that evidence for conjunction benefits was not obtained. However, the fact that responding <italic>absent</italic> to the absence of the target conjunction was significantly slower when the single target feature for which responding in the single feature case was fastest (color) was present relative to when that feature was absent reveals that conjunction costs were obtained in the 25% block.</p>
<p>With regard to accuracy, the bottom panel of <xref ref-type="fig" rid="fig2-1555343412445474">Figure 2</xref> shows recognition accuracy across the six feature conditions. If there was a speed–accuracy trade-off present in the data, the trend found in the accuracy data would follow closely the trend revealed in the RT data. That is, RT and accuracy would be positively correlated: As RT increased and responding became slower, accuracy would increase because the participants would be sacrificing speed of responding for accuracy; or as RT decreased and responding became faster, accuracy would decrease because the participants would be sacrificing accuracy for speed of responding. However, neither instance of a speed–accuracy trade-off occurred. There was a tendency for RT and accuracy to be negatively correlated, which indicated that in certain conditions, RT decreased and responding became faster as accuracy increased, and in other conditions, RT increased and responding became slower as accuracy decreased. The correlation between RT and accuracy was –.68.</p>
</sec>
<sec id="section9-1555343412445474">
<title>The 50% Block</title>
<p>The top panel of <xref ref-type="fig" rid="fig3-1555343412445474">Figure 3</xref> shows RTs for the six feature conditions in the 50% block. As before, the first two histogram bars from the left correspond to the single-feature trials, and the remaining four histogram bars correspond to the feature conjunction trials. A 2 × 3 analysis of variance for within-subjects designs, with Greenhouse-Geisser correction, revealed that there was a significant main effect of feature condition, <italic>F</italic>(1.1, 7.7) = 21.2, <italic>p</italic> &lt; .01. In a post hoc analysis, Tukey’s HSD test showed that responding was overall faster with the conjunction of color and shape than with the single feature of shape, <italic>p</italic> &lt; .05. The analysis also revealed that there was no significant main effect of response type, <italic>F</italic>(1.0, 7.0) = 4.9, <italic>p</italic> &gt; .05, and no significant interaction between feature condition and response type, <italic>F</italic>(1.4, 9.8) = 1.8, <italic>p</italic> &gt; .05.</p>
<fig id="fig3-1555343412445474" position="float">
<label>Figure 3.</label>
<caption>
<p>Empirical reaction time (upper graph) and accuracy (lower graph) shown for six feature conditions (abscissa) for the 50% block of trials (i.e., in which the target feature conjunction occurred in 50% of the trials). For the six feature conditions, C = target color only; S = target shape only; C, S = target color and target shape conjunction; C, nS = target color but not target shape; nC, S = target shape but not target color; nC, nS = neither target color nor target shape. Each mean is an average of eight observers. Error bars equal ±1 standard error of the mean.</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig3.tif"/></fig>
<p>Despite the lack of a significant interaction, we still computed separate one-way ANOVAs (Greenhouse-Geisser correction) on the <italic>present</italic> responses and on the <italic>absent</italic> responses, respectively, to probe for priming effects. For the <italic>present</italic> responses, the analysis revealed that the effect of feature condition was significant, <italic>F</italic>(1.3, 9.4) = 7.0, <italic>p</italic> &lt; .05. However, Tukey’s HSD test showed that none of the pairwise comparisons was significantly different from one another (with <italic>p</italic> &gt; .05). (Again, there was no significant difference between <italic>present</italic> responses to the target conjunction of color and shape [the C, S condition] versus responses to the single feature of shape [the S condition], <italic>p</italic> &gt; .05.) For <italic>absent</italic> responses, the analysis showed that the effect of feature condition was significant, <italic>F</italic>(1.3, 8.8) = 23.8, <italic>p</italic> &lt; .01. Tukey’s HSD test revealed that responding was slower when color was present and shape was absent (the C, nS condition) versus when color was absent (and shape present; the nC, S condition) or when both color and shape were absent (the nC, nS condition), both <italic>p</italic> &lt; .01.</p>
<p>The fact that responding <italic>present</italic> to the target conjunction (color and shape) was not significantly faster than responding to the single target feature for which responding was the slowest (shape) means that evidence for conjunction benefits was not obtained. However, the fact that responding <italic>absent</italic> to the absence of the target conjunction was significantly slower when the target feature for which responding in the single feature case was fastest (color) was present relative to when that feature was absent reveals that conjunction costs were obtained in the 50% block.</p>
<p>With regard to accuracy, the bottom panel of <xref ref-type="fig" rid="fig3-1555343412445474">Figure 3</xref> shows recognition accuracy across the six feature conditions. Consistent with the 25% block, in this 50% block, there was no speed–accuracy trade-off. There was a strong tendency for RT and accuracy to be negatively correlated, with the correlation between RT and accuracy being –.95.</p>
<p>The principal result here is that reliable conjunction costs were produced in both the 25% and 50% blocks.</p>
</sec></sec>
<sec id="section10-1555343412445474">
<title>Modeling</title>
<sec id="section11-1555343412445474">
<title>System Dynamics Modeling</title>
<p>To gain insight into the dynamics of decision priming, we modeled priming in our study using methods from system dynamics (e.g., <xref ref-type="bibr" rid="bibr9-1555343412445474">Forrester, 1961</xref>, <xref ref-type="bibr" rid="bibr10-1555343412445474">1968</xref>; <xref ref-type="bibr" rid="bibr34-1555343412445474">Sterman, 2000</xref>). For an overview of system dynamics modeling, see <xref ref-type="bibr" rid="bibr27-1555343412445474">Patterson, Fournier, Pierce, Winterbottom, and Tripp (2009)</xref>. The justification for this approach is that it allowed us to precisely model and predict the timing of decision priming, which is, after all, a dynamical phenomenon. The system dynamics approach can be contrasted to other modeling approaches, such as the adaptive control of thought–rational (ACT-R), which is a modeling environment based on the LISP programming language that models human cognition as a semantic network connected to procedural memory (<xref ref-type="bibr" rid="bibr1-1555343412445474">Anderson, Bothell, &amp; Byrne, 2004</xref>), or Bayesian approaches (<xref ref-type="bibr" rid="bibr23-1555343412445474">Mueller, 2009</xref>). Whereas these modeling approaches can represent simple dynamics, such as memory retrieval latencies, the present approach can represent the complicated dynamics found in complex feedback systems in the form of systems of differential equations.</p>
<p>Our model of decision priming is shown in <xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>. The model is composed of three levels of processing: (a) feature integration by feature- integration mechanisms, labeled <italic>green infor-mation, brown information, American shape information</italic>, and <italic>Russian shape information</italic>, shown on the left side of the figure; (b) individual recognition processes, labeled <italic>green recognition, brown recognition, American shape recognition</italic>, and <italic>Russian shape recognition</italic>, followed by <italic>color comparison</italic> and <italic>shape comparison</italic> mechanisms, depicted in the middle of the figure; and (c) a central decision process rendering a <italic>target present</italic> or <italic>target absent</italic> decision, shown on the right side of the figure. This model is discussed in more detail next.</p>
<fig id="fig4-1555343412445474" position="float">
<label>Figure 4.</label>
<caption>
<p>A coactive parallel-channels model (with nested decisional operators) of human decision making that simulates decision priming. Information processing proceeds from left to right in the figure. On the left, each of four feature information integration mechanisms integrates one of four types of information. Each of two integration mechanisms integrates a given color, green or brown, and each of two other mechanisms integrates information about a given shape of tank, American shape or Russian shape. In the middle of the figure, four feature recognition processes receive information from the integration mechanisms on the left. Green and brown recognition processes project to a color comparison mechanism; American shape and Russian shape recognition processes project to a shape comparison mechanism. The color and shape comparison mechanisms project to a coactive central decision process, shown on the right side of the figure, which makes a final decision about the presence or absence of the target.</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig4.tif"/></fig>
<p>Before proceeding to the details of the model, it should be noted that cognitive processes, such as feature integration mechanisms, feature recognition processes, and a central decision process, are included in the model because they have been hypothesized to exist in the decision priming literature for some time now (<xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier et al., 1998</xref>, <xref ref-type="bibr" rid="bibr13-1555343412445474">2004</xref>, <xref ref-type="bibr" rid="bibr14-1555343412445474">2007</xref>; <xref ref-type="bibr" rid="bibr11-1555343412445474">Fournier, Bowd, et al., 2000</xref>; <xref ref-type="bibr" rid="bibr15-1555343412445474">Fournier, Scheffers, et al., 2000</xref>). Moreover, these integration, recognition, and decision processes are modeled within a parallel channels–coactive architecture because it is well recognized in the stochastic modeling literature that such an architecture is one reasonable approach for modeling interactions between signals (<xref ref-type="bibr" rid="bibr36-1555343412445474">Townsend &amp; Ashby, 1983</xref>).</p>
<sec id="section12-1555343412445474">
<title>First level of processing</title>
<p>The <italic>feature information integration mechanisms</italic> integrate perceptual information about one of four types of features in parallel: green color, brown color, American shape, and Russian shape. This perceptual integration of feature information follows the well-known <italic>S</italic>-shaped profile described by a logistic function (<xref ref-type="bibr" rid="bibr34-1555343412445474">Sterman, 2000</xref>). In a logistic formulation that entails self-limiting growth, the accumulation of the initial information grows exponentially because cognitive capacity is high relative to the amount of information coming in to the system. Finally, as information being accumulated approaches cognitive capacity, the rate of information accumulation slows down until capacity is reached.</p>
<p>Logistic functions, or sigmoid curves, are widely used to model various phenomena undergoing growth within capacitated systems, such as growth in ecology, neural networks, chemistry, biology, and economics (<xref ref-type="bibr" rid="bibr16-1555343412445474">Gershenfeld, 1999</xref>, p. 150; <xref ref-type="bibr" rid="bibr21-1555343412445474">Lotka, 1956</xref>; <xref ref-type="bibr" rid="bibr28-1555343412445474">Perez, 2002</xref>). In the fields of perception and psychophysics, curves relating performance to stimulus intensity or time follow a sigmoid function (<xref ref-type="bibr" rid="bibr2-1555343412445474">Blake &amp; Sekuler, 2005</xref>), which justifies our use of the logistic function in our model (note that probit curves involve binary response models; thus the logistic function would be more appropriate here). As an example, for the color green we have</p>
<p>
<disp-formula id="disp-formula1-1555343412445474">
<mml:math display="block" id="math1-1555343412445474">
<mml:mrow>
<mml:mtext>Net</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>growth</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>rate</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>of</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>green</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>information</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mi>d</mml:mi>
<mml:mtext>GI</mml:mtext>
<mml:mo>/</mml:mo>
<mml:mi>d</mml:mi>
<mml:mi>t</mml:mi>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mn>1</mml:mn>
<mml:mi>|</mml:mi>
<mml:mtext>GI</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo> <mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mi>k</mml:mi>
<mml:mn>1</mml:mn>
<mml:mtext>GI</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>GI</mml:mtext>
<mml:mo>/</mml:mo>
<mml:mtext>C</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow> <mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-1555343412445474" xlink:href="10.1177_1555343412445474-eq1.tif"/>
</disp-formula>
</p>
<p>where GI = green information, <italic>t</italic> = time, <italic>k1</italic> = growth–decay fraction, and <italic>C</italic> = capacity. The logistic function is a capacity-limited growth function that, in the field of system dynamics, can be modeled as a set of coupled positive and negative feedback loops (<xref ref-type="bibr" rid="bibr34-1555343412445474">Sterman, 2000</xref>), with (<italic>k1</italic>GI) defining the positive loop and [(<italic>k1</italic>GI)(GI/<italic>C</italic>)] defining the negative loop (<xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>), that establish a dynamic equilibrium at 100% as the information is accumulated.</p>
<p>The response of each feature information integration mechanism can range from 0 to 100 units, with the units denoting the percentage of information that can be integrated within a given period (capacity is set at 100% of information that can be integrated). To activate one or more of the feature integration mechanisms, the simulation is started with the relevant feature integration mechanism having an initial value of 10 units, for example, GI(0) = 10, which is compounded over time by the logistic process. (The simulation can easily be recalibrated to accommodate a different starting value, and there is no potential for bifurcation.) To deactivate a given mechanism, the simulation is started with the relevant mechanism having an initial value of 0 units. The activation of various combinations of feature information integration mechanisms is how the model represents the presentation of different stimuli across different simulated trials.</p>
<p>A threshold mechanism follows each feature information integration mechanism, which represents a given amount of perceptual information that must be exceeded before each of the subsequent recognition processes can respond:</p>
<p>
<disp-formula id="disp-formula2-1555343412445474">
<mml:math display="block" id="math2-1555343412445474">
<mml:mrow>
<mml:mtext>IF</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Green</mml:mtext>
<mml:mo>&gt;</mml:mo>
<mml:mn>50</mml:mn>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>THEN</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Green</mml:mtext>
<mml:mo>.</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>ELSE</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mn>0</mml:mn>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-1555343412445474" xlink:href="10.1177_1555343412445474-eq2.tif"/>
</disp-formula>
</p>
<p>A threshold setting of 50 units (i.e., 50% of information that can be accumulated, or half of the capacity) means that more than half of the feature information must be accumulated before the recognition process can respond, which is consistent with the traditional definition of <italic>threshold</italic> (<xref ref-type="bibr" rid="bibr2-1555343412445474">Blake &amp; Sekuler, 2005</xref>). Together with the logistic function described earlier, the threshold mechanism represents threshold effects in perceptual recognition (<xref ref-type="bibr" rid="bibr6-1555343412445474">Dodwell, 1971</xref>).</p>
</sec>
<sec id="section13-1555343412445474">
<title>Second level of processing</title>
<p>The <italic>recognition processes</italic> integrate the evidence, derived from the feature information integration mechanisms, that a given type of feature is present to render a recognition response about that feature. The net growth rate of recognition response for each recognition process is given by the following expression, using the color green as an example:</p>
<p>
<disp-formula id="disp-formula3-1555343412445474">
<mml:math display="block" id="math3-1555343412445474">
<mml:mrow>
<mml:mtext>dGR</mml:mtext>
<mml:mo>/</mml:mo>
<mml:mtext>dt</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mrow>
<mml:mo>{</mml:mo> <mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>k</mml:mtext>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>GI</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>[</mml:mo> <mml:mrow>
<mml:mn>1</mml:mn>
<mml:mo>−</mml:mo>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>GI</mml:mtext>
<mml:mo>/</mml:mo>
<mml:mtext>C</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow> <mml:mo>]</mml:mo>
</mml:mrow>
</mml:mrow> <mml:mo>}</mml:mo>
</mml:mrow>
<mml:mo>−</mml:mo>
<mml:mrow>
<mml:mo>[</mml:mo> <mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>k</mml:mtext>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>GR</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula3-1555343412445474" xlink:href="10.1177_1555343412445474-eq3.tif"/>
</disp-formula>
</p>
<p>where GR = green recognition, <italic>t</italic> = time, <italic>k1</italic> = growth–decay fraction, GI = green information, and <italic>C</italic> = capacity.</p>
<p>This function is a capacity-limited growth function that also can be modeled as a set of coupled positive and negative feedback loops, with {(<italic>k1</italic>)(GI)[1 – (GI/<italic>C</italic>)]} defining the positive loop and – [(<italic>k1</italic>)(GR)] defining the negative loop. Defining the net growth rate of the recognition response in this way produces a transient (approximately bell-shaped) pulse, which decays to zero, in the output of this process (provided that <italic>k1</italic> is not zero). Defining the net growth rate of the recognition process according to <xref ref-type="disp-formula" rid="disp-formula3-1555343412445474">Equation 3</xref> enables the contribution of each recognition process (i.e., a given color or a given shape) to be limited to one half of the final response of the central decision process.</p>
<p>Note that <italic>k1</italic> in <xref ref-type="disp-formula" rid="disp-formula1-1555343412445474">Equations 1</xref> and <xref ref-type="disp-formula" rid="disp-formula3-1555343412445474">3</xref> and <italic>k1</italic> and <italic>k2</italic> in <xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref> represent the multiplicative inverse of the time constants of the capacitated exponential growth functions (e.g., the logistic function) that define the rate of information accumulation in the feature information integration mechanism and the feature recognition process. For priming to occur, the <italic>k1</italic> and <italic>k2</italic> values of the respective color and shape pathways must be sufficiently different so that temporal misalignment in the corresponding responses is produced.</p>
<p>Output from the recognition processes project to mechanisms called “color comparison” and “shape comparison,” each of which compares one type of feature with a given target feature to determine whether the output of the recognition processes is consistent. These mechanisms compare the color (or shape) information that is being output by the recognition processes and determine whether a sign change is warranted. This process is captured in the following logic (assume in this example that green is part of a target feature conjunction):</p>
<p>
<disp-formula id="disp-formula4-1555343412445474">
<mml:math display="block" id="math4-1555343412445474">
<mml:mrow>
<mml:mtext>IF</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Green</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Recognition</mml:mtext>
<mml:mo>&gt;</mml:mo>
<mml:mtext>Brown</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Recogniton</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mtext>THEN</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Green</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Recognition</mml:mtext>
<mml:mo>*</mml:mo>
<mml:mi>β</mml:mi>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mtext>ELSE</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Brown</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Recognition</mml:mtext>
<mml:mo>*</mml:mo>
<mml:mo>−</mml:mo>
<mml:mi>β</mml:mi>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula4-1555343412445474" xlink:href="10.1177_1555343412445474-eq4.tif"/>
</disp-formula>
</p>
<p>where β equals some value that serves as a scaling factor that keeps information emanating from the color comparison and the shape comparison mechanisms balanced (so that each type of signal contributes to one half of the total response of the central decision process). The output from these two comparison mechanisms, which can be positive- or negative-signed pulses, project to the central decision process for rendering a final decision.</p>
</sec>
<sec id="section14-1555343412445474">
<title>Third level of processing</title>
<p>The target decision is rendered by the central decision process, which integrates information from the two comparison mechanisms and determines whether a target (single feature or feature conjunction) is present. The central decision process is composed of a logic statement followed by two interconnected decision reservoirs. The logic statement, labeled <italic>decision</italic> in <xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>, compares the two streams of information from the color and shape comparison mechanisms, respectively, to determine which of the four possible combinations is present:</p>
<p>
<disp-formula id="disp-formula5-1555343412445474">
<mml:math display="block" id="math5-1555343412445474">
<mml:mrow>
<mml:mtext>IF</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Color</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
<mml:mo>&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mtext>AND</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Shape</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
<mml:mo>&lt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mtext>OR</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Shape</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mi>C</mml:mi>
<mml:mtext>omparison</mml:mtext>
<mml:mo>&gt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mtext>AND</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Color</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
<mml:mo>&lt;</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mtext>THEN</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>[</mml:mo> <mml:mrow>
<mml:mtext>MAX</mml:mtext>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Color</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>Shape</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>*</mml:mo>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow> <mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>+</mml:mo>
<mml:mtext>MIN</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Color</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mspace width="0.25em"/>
<mml:mtext>Shape</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mspace width="0.25em"/>
<mml:mtext>ELSE</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Color</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
<mml:mo>+</mml:mo>
<mml:mtext>Shape</mml:mtext>
<mml:mspace width="0.25em"/>
<mml:mtext>Comparison</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula5-1555343412445474" xlink:href="10.1177_1555343412445474-eq5.tif"/>
</disp-formula>
</p>
<p>Note that <xref ref-type="disp-formula" rid="disp-formula5-1555343412445474">Equation 5</xref> is a conditional expression whose action depends on whether the specified Boolean condition evaluates to true or false. This expression is in contrast to a Bayesian decision rule that maximizes the posterior expected value of a utility function or one that minimizes the posterior expected value of a loss function. <xref ref-type="disp-formula" rid="disp-formula5-1555343412445474">Equation 5</xref> is also different from a fuzzy logic formulation that would entail multivalued logical expressions.</p>
<p>According to <xref ref-type="disp-formula" rid="disp-formula5-1555343412445474">Equation 5</xref>, if output from one comparison mechanism is positive and output from the other comparison mechanism is negative, then the output from the positive mechanism (i.e., the MAX function) is multiplied by −1 and added to the output of the negative mechanism (the MIN function) to yield a negative flow rate, affecting the central decision process (flow in the direction of a <italic>target absent</italic> decision). If the output from both comparison mechanisms is negative, then processing goes to the ELSE statement, which simply adds two negative flow rates to yield a negative flow rate, affecting the central decision process. Finally, if the output from the two comparison mechanisms is positive, then processing again goes to the ELSE statement, which adds two positive flow rates to yield a positive flow rate, affecting the central decision process (flow in the direction of a <italic>target present</italic> decision).</p>
<p>This logic also allows for the possibility of decision making when the target is defined by only one feature (single-feature trials), because such conditions would lead to the processing of the “ELSE (Color Comparison + Shape Comparison)” statement, given that the output from one comparison mechanism would be zero.</p>
<p>A second mechanism of the central decision process is conceptualized as being composed of two interconnected decision reservoirs, a <italic>target present</italic> decision reservoir and a <italic>target absent</italic> decision reservoir. The level in each decision reservoir symbolizes the percentage of decision commitment to each decision. Each run of the simulation began with decision ambiguity (50% in each of the two reservoirs). Over time, the amount of decision commitment flowed from one reservoir into the other, depending on incoming evidence from the color and shape comparison mechanisms. The rate of flow into or out of one or the other decision reservoir was determined by the sum of the activation levels of the comparison mechanisms. For the <italic>target present</italic> reservoir,</p>
<p>
<disp-formula id="disp-formula6-1555343412445474">
<mml:math display="block" id="math6-1555343412445474">
<mml:mrow>
<mml:mtext>TP</mml:mtext>
<mml:mo>=</mml:mo>
<mml:mstyle displaystyle="true">
<mml:mrow>
<mml:mo>∫</mml:mo>
<mml:mrow>
<mml:mtext>CDGR</mml:mtext>
<mml:mi>d</mml:mi>
<mml:mi>t</mml:mi>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mtext>T</mml:mtext>
<mml:mtext>P</mml:mtext>
</mml:msub>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula6-1555343412445474" xlink:href="10.1177_1555343412445474-eq6.tif"/>
</disp-formula>
</p>
<p>where T<sub>P</sub> is the <italic>target present</italic> decision; CDGR is the central decision growth rate, which equals Color Comparison + Shape Comparison; and T<sub>P</sub>(0) is the initial value of T<sub>P</sub>, which was 50%.</p>
<p><xref ref-type="disp-formula" rid="disp-formula6-1555343412445474">Equation 6</xref> indicates that activation of the color and shape comparison mechanisms were added and the sum projected to the central decision process as a rate. This rate was integrated by the central decision process to become an increasing or decreasing level of decision commitment for a given option. For each simulated decision, one reservoir asymptoted to a level of 100%, representing complete commitment to the corresponding option, and the other reservoir asymptoted to a level of 0%, representing a complete abandonment of the opposing option. Thus, the total (unsigned) response of the central decision process will be between 50% and 100%. For example, in the case in which early information starts the decision commitment toward one option, which is then counteracted by subsequent information that makes the decision commitment swing to the opposite option (i.e., a conjunction-cost effect), the percentage commitment starts out at 50% for a given option, increases toward that option for a brief period, goes back to 50%, and then decreases for that option while at the same time increasing for the opposite option, ending at 100% commitment to the opposite option.</p>
</sec>
<sec id="section15-1555343412445474">
<title>Summary and assumptions</title>
<p>Our model entails a framework based on parallel channels with nested decisional operators (PCNDO). The channels feed into a final coactive decision stage. One type of operator is an XOR gate (i.e., exclusive OR, or an exclusive disjunction operator), which would exist for each of two pairs of lower-level channels. Each channel signals a given feature, and each pair of channels signals a target feature match or nonmatch. For example, in our model, two members of a pair signal the colors green and brown, respectively, and—assuming that the target feature is the color green—that pair of channels would signal a target feature match if the green channel is activated and a nonmatch if the brown channel is activated. (The two members of the other pair of channels signal American shape and Russian shape, respectively.) The output from the two pairs of channels would be combined with an AND gate, which would lead to a final decision (e.g., target feature conjunction present or absent). An analogous type of model has been described by <xref ref-type="bibr" rid="bibr37-1555343412445474">Townsend and Wenger (2004)</xref> and by <xref ref-type="bibr" rid="bibr27-1555343412445474">Patterson et al. (2009)</xref>.</p>
<p>Several assumptions of our model are as follows:</p>
<list id="list1-1555343412445474" list-type="order"><list-item><p>Attended features appearing in one spatial location are selected for the decision process in a parallel (e.g., <xref ref-type="bibr" rid="bibr8-1555343412445474">Eriksen &amp; St. James, 1986</xref>; <xref ref-type="bibr" rid="bibr19-1555343412445474">Lavie, 1995</xref>; <xref ref-type="bibr" rid="bibr38-1555343412445474">Treisman &amp; Gelade, 1980</xref>), continuous fashion, reflecting whether each stimulus feature is present or absent in the target conjunction.</p></list-item>
<list-item><p>Information about a particular feature, leading to a feature recognition response, accumulated in a time-variant fashion (i.e., rate of accumulation varied with time). Such time-variant processes are important for maximizing rewards (i.e., successful action taken) when passage of time may lead to a changing situation that negates the decision-making process (see <xref ref-type="bibr" rid="bibr5-1555343412445474">Ditterich, 2006</xref>).</p></list-item>
<list-item><p>Two pairs of channels, which represented two sets of independent processing streams with different time courses, operated in parallel to produce decision priming. This approach was different from a classic random walk or diffusion model, in which evidence is accumulated as a single signed total representing differences between the evidence accumulating for different decision alternatives before the decision stage is reached (<xref ref-type="bibr" rid="bibr4-1555343412445474">Busemeyer &amp; Townsend, 1993</xref>; <xref ref-type="bibr" rid="bibr29-1555343412445474">Ratcliff, 1978</xref>, <xref ref-type="bibr" rid="bibr30-1555343412445474">2001</xref>; <xref ref-type="bibr" rid="bibr39-1555343412445474">Usher &amp; McClelland, 2001</xref>).</p></list-item>
<list-item><p>Feature recognition was a discrete, two-stage process that required a threshold to be reached before the next processing stage began. <xref ref-type="bibr" rid="bibr22-1555343412445474">Meyer, Yantis, Osman, and Smith (1985)</xref> suggested that such a framework was appropriate for simple binary decisions when stimuli are mapped to responses in a compatible fashion. A multistage process was also required for model implementation, in which output from the feature recognition stage was a time-limited response that was integrated by the central decision process.</p></list-item>
<list-item><p>The time course of decision making depended on system parameters growth–decay factor <italic>k</italic>, capacity <italic>C</italic>, and scaling factor β. (The β was a weighting factor for the types of information evaluated in the cognitive process, and it was adjusted to ensure balance among sources in the simulation trials.) The values of these parameters were determined empirically by trial and error so that the best fit of the model output to our experimental data was obtained; this procedure, however, did not guarantee a close fit, only what was probably the best fit given our model architecture.</p></list-item>
<list-item><p>Our model was deterministic and lacked stochasticity, similar to the ballistic accumulator model of <xref ref-type="bibr" rid="bibr3-1555343412445474">Brown and Heathcote (2005)</xref>.</p></list-item></list>
<p>It is important to note that our model could not fail in terms of accuracy, but it could fail in terms of timing. That is, our model was structured to always give a correct answer on each simulated trial to mimic the high levels of accuracy typically found in the empirical literature (<xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier et al., 1998</xref>, <xref ref-type="bibr" rid="bibr13-1555343412445474">2004</xref>, <xref ref-type="bibr" rid="bibr14-1555343412445474">2007</xref>; <xref ref-type="bibr" rid="bibr11-1555343412445474">Fournier, Bowd, et al., 2000</xref>; <xref ref-type="bibr" rid="bibr15-1555343412445474">Fournier, Scheffers, et al., 2000</xref>). However, the model still could have failed in the sense that the simulated timing (i.e., decision priming) would not have corresponded with the empirical data. As discussed in the next section, the simulated timing did correspond well with the empirical data.</p>
</sec>
</sec>
<sec id="section16-1555343412445474">
<title>Modeling Results and Analysis</title>
<p>The model previously described was implemented in the software Stella, Version 9.2 (ISEE Systems, Waltham, MA). The parameters used in the simulation are discussed next. The Euler method of numerical integration was used with <italic>DT</italic> = 0.01. <italic>DT</italic> refers to the time step used in the simulation (e.g., the period during which the integration occurs in the modeling). The recommended size of <italic>DT</italic> is between one tenth and one sixth of the shortest time constant in the simulation (<xref ref-type="bibr" rid="bibr34-1555343412445474">Sterman, 2000</xref>).</p>
<p>In the simulation, color was chosen to be easier to recognize than shape, which was consistent with the empirical data (<xref ref-type="fig" rid="fig2-1555343412445474">Figures 2</xref> and <xref ref-type="fig" rid="fig3-1555343412445474">3</xref>). The target conjunction was chosen to be a green American tank, although other target combinations would have yielded exactly the same results. (In the human literature, RT found for one or another stimulus dimension typically depend on stimulus parameters such as display contrast, display brightness, and feature similarity, so there is a range of values reported across the literature involving different dimensions; see <xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier et al., 1998</xref>, <xref ref-type="bibr" rid="bibr13-1555343412445474">2004</xref>, <xref ref-type="bibr" rid="bibr14-1555343412445474">2007</xref>; <xref ref-type="bibr" rid="bibr11-1555343412445474">Fournier, Bowd, et al., 2000</xref>; <xref ref-type="bibr" rid="bibr15-1555343412445474">Fournier, Scheffers, et al., 2000</xref>. The important point is that for decision priming to occur, one stimulus dimension must be processed faster than another dimension.)</p>
<p>Next, the values of the growth and decay fractions of the feature information integration mechanisms (<italic>k1</italic> and <italic>k2</italic> in <xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>; see also <xref ref-type="disp-formula" rid="disp-formula1-1555343412445474">Equation 1</xref>) that would make the net growth rate of the color dimension faster than that of the shape dimension were established empirically. These values were also used for the growth and decay fractions of the corresponding recognition processes (<xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref> and <xref ref-type="disp-formula" rid="disp-formula3-1555343412445474">Equation 3</xref>). As the growth and decay fractions were established, the value of the β scaling factor of the color and shape comparison mechanisms (<xref ref-type="disp-formula" rid="disp-formula4-1555343412445474">Equation 4</xref>) was adjusted to keep information emanating from the two mechanisms balanced so that each type of signal contributed to one half of the final response of the central decision process.</p>
<p>Across simulations, the values of the growth and decay fractions, and of β, were established such that the simulated RTs for single-feature judgments (i.e., RT in Condition C and in Condition S) matched closely the empirical values shown in <xref ref-type="fig" rid="fig2-1555343412445474">Figures 2</xref> and <xref ref-type="fig" rid="fig3-1555343412445474">3</xref>. The growth and decay fraction for the color dimension (i.e., <italic>k1</italic>) was 12 and β was ±6.1945. The growth and decay fraction for the shape dimension (i.e., <italic>k2</italic>) was 11.4963 and β was ±5.9461.</p>
<p>Thus, these values were determined empirically by trial and error so that our model output provided the best fit to the empirical data obtained from the single-feature judgments only. After these values were chosen, it remained to be seen whether the model output fit the conjunction or whole-object judgments for which the parameter values were not changed. Also note that although these parameters (i.e., <italic>k, C</italic>, and β) were inherent to our model, they can be estimated by measuring the time course of the accumulation of perceptual information about color and shape.</p>
<p>These values resulted in a simulated RT of 700 ms for recognizing color (for the 25% and 50% blocks, the empirical RT for recognizing color was 694 ms and 705 ms, respectively) and a simulated RT of 790 ms for recognizing shape (for the 25% and 50% blocks, the empirical RT for recognizing shape was 779 ms and 799 ms, respectively). These simulated RTs provided close fits to the empirical data. See <xref ref-type="fig" rid="fig5-1555343412445474">Figure 5</xref> for a depiction of the dynamics of mechanism responding and the first two histogram bars on the left side of <xref ref-type="fig" rid="fig9-1555343412445474">Figures 9</xref> and <xref ref-type="fig" rid="fig10-1555343412445474">10</xref> for the fit to empirical data. The <italic>k1, k2</italic>, and β parameters were fixed, and then simulations of system responding were run for various feature combinations: green and American shape, brown and American shape, green and Russian shape, and brown and Russian shape.</p>
<fig id="fig5-1555343412445474" position="float">
<label>Figure 5.</label>
<caption>
<p>Simulation of the coactive parallel-channels decision-making model (<xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>) showing the time course of processing of green information only (top panel) and American shape information only (bottom panel). Ordinates depict percentage activation or percentage commitment to a present decision. Simulated reaction time for green-only target present decision was 700 ms; for American shape-only target present decision, it was 790 ms.</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig5.tif"/></fig>
<p>The simulated results showed that RT for a <italic>target present</italic> decision was 740 ms for the conjunction green–American shape, the dynamics of which are depicted in <xref ref-type="fig" rid="fig6-1555343412445474">Figure 6</xref>. For the 25% and 50% blocks, respectively, the empirical data were 725 ms and 700 ms; see the third histogram bar from the left in <xref ref-type="fig" rid="fig9-1555343412445474">Figures 9</xref> and <xref ref-type="fig" rid="fig10-1555343412445474">10</xref>. In the simulation, this result represents a gain (i.e., a conjunction benefit) of 55% for the conjunction relative to the slowest feature across the interval between the slowest and fastest features. The average of the gains in the 25% and the 50% blocks was 87%. A representative value of conjunction gain reported in the behavioral literature is a gain of 54% (<xref ref-type="bibr" rid="bibr14-1555343412445474">Fournier et al., 2007</xref>).</p>
<fig id="fig6-1555343412445474" position="float">
<label>Figure 6.</label>
<caption>
<p>Simulation of the coactive parallel-channels decision-making model (<xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>) showing the time course of processing of both the green information and the American shape information together leading to a target conjunction present decision. Ordinate depicts percentage activation or percentage commitment to a present decision. Simulated reaction time for the green–American shape target conjunction present decision was 740 ms. This pattern of simulated responding is consistent with decision priming (i.e., conjunction benefits; compare with <xref ref-type="fig" rid="fig5-1555343412445474">Figure 5</xref>).</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig6.tif"/></fig>
<p>In the simulation, the ordering of the conditions—fastest feature (green = 700 ms) &lt; both features (green–American shape = 740 ms) &lt; slowest feature (American shape = 790 ms)—is exactly analogous to that reported in the literature and reflects conjunction benefits: The response to the presence of the conjunction is faster than the response to the single feature for which responding is slowest (recall that we did not find this effect in our empirical data reported earlier). This effect represents decision priming in which the feature for which responding is the fastest primes the decision made about the conjunction. Thus, the model produced this form of decision priming even though we did not find evidence for it in the present study.</p>
<p>Simulated RT for a <italic>target absent</italic> decision was 740 ms for American shape but not green, 740 ms when neither feature was present, and 800 ms for green but not American shape, the dynamics of which are shown in <xref ref-type="fig" rid="fig7-1555343412445474">Figures 7</xref> and <xref ref-type="fig" rid="fig8-1555343412445474">8</xref>. The empirical values obtained in the 25% block were 699 ms, 704 ms, and 831 ms, respectively; and for the 50% blocks, they were 727 ms, 749 ms, and 888 ms, respectively (see the three histogram bars from the right side in <xref ref-type="fig" rid="fig9-1555343412445474">Figures 9</xref> and <xref ref-type="fig" rid="fig10-1555343412445474">10</xref>).</p>
<fig id="fig7-1555343412445474" position="float">
<label>Figure 7.</label>
<caption>
<p>Simulation of the coactive parallel-channels decision-making model (<xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>) showing the time course of processing of (upper panel) green information and Russian shape information, which leads to a target conjunction absent decision (shown as 0% commitment to a present decision). Ordinate depicts percentage activation or percentage commitment to a present decision. Simulated reaction time for the green information plus Russian shape information was 800 ms. This pattern of results is consistent with decision priming (i.e., conjunction costs) as found in the present study (compare with <xref ref-type="fig" rid="fig8-1555343412445474">Figure 8</xref>).</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig7.tif"/></fig>
<fig id="fig8-1555343412445474" position="float">
<label>Figure 8.</label>
<caption>
<p>Simulation of the coactive parallel-channels decision-making model (<xref ref-type="fig" rid="fig4-1555343412445474">Figure 4</xref>) showing the time course of processing of brown information together with Russian shape information, which leads to a target conjunction absent decision (shown as 0% commitment to a present decision). Ordinate depicts percentage activation or percentage commitment to a present decision. Simulated reaction time for the brown information plus Russian shape information was 740 ms.</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig8.tif"/></fig>
<fig id="fig9-1555343412445474" position="float">
<label>Figure 9.</label>
<caption>
<p>Empirical reaction time (solid bars) and simulated reaction time (open bars) shown for six feature conditions for the 25% block (i.e., in which the target feature conjunction occurred in 25% of the trials). For the six feature conditions, C = target color only; S = target shape only; C, S = target color and target shape conjunction; C, nS = target color but not target shape; nC, S = target shape but not target color; nC, nS = neither target color nor target shape. Each mean is an average of eight observers. Error bars equal ±1 standard error of the mean. (The empirical data are reproduced here from <xref ref-type="fig" rid="fig2-1555343412445474">Figure 2</xref>.)</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig9.tif"/></fig>
<fig id="fig10-1555343412445474" position="float">
<label>Figure 10.</label>
<caption>
<p>Empirical reaction time (solid bars) and simulated reaction time (open bars) shown for six feature conditions (abscissa) for the 50% block (i.e., in which the target feature conjunction occurred in 50% of the trials). For the six feature conditions, C = target color only; S = target shape only; C, S = target color and target shape conjunction; C, nS = target color but not target shape; nC, S = target shape but not target color; nC, nS = neither target color nor target shape. Each mean is an average of eight observers. Error bars equal ±1 standard error of the mean. (The empirical data are reproduced here from <xref ref-type="fig" rid="fig3-1555343412445474">Figure 3</xref>.)</p></caption>
<graphic xlink:href="10.1177_1555343412445474-fig10.tif"/></fig>
<p>In the simulation, the ordering of the RTs—neither target feature present (brown Russian shape = 740 ms) ≤ slowest target feature present (brown American shape = 740 ms) &lt; fastest target feature present (green Russian shape = 800 ms)—is analogous to that reported in the literature and reflects conjunction costs: The absence of the conjunction is responded to fastest when neither target feature is present than when the feature to which responding is fastest is present (recall that we did find this effect in our empirical data reported earlier). This effect represents decision priming in which the feature for which responding is the fastest primes the decision that the target conjunction is present, which must then be countermanded when the other (nontarget) feature reaches the central decision process. The model produced this form of decision priming, consistent with our empirical data.</p>
<p>Inspection of <xref ref-type="fig" rid="fig9-1555343412445474">Figures 9</xref> and <xref ref-type="fig" rid="fig10-1555343412445474">10</xref> reveals that all of the simulated RTs across all conditions fall within one standard error of the corresponding empirical mean RTs, except for the C, nS condition of the 50% block (i.e., target color but not target shape, which produced the longest RT). But even for this condition, whose standard error was ±61.5 ms, the simulated RT of 800 ms was 1.4 standard errors below the empirical mean RT of 888 ms. Thus, none of our simulated RTs would be considered significantly different from the corresponding empirical RTs in either a one-tailed or a two-tailed statistical test. We conclude that our simulated RTs provide a reasonable fit to the empirical RT data.</p>
</sec>
</sec>
<sec id="section17-1555343412445474" sec-type="discussion">
<title>Discussion</title>
<p>The results of our empirical study reveal that decision priming, in the form of conjunction costs, is obtained with simulated naturalistic stimuli. In our study, vehicles (i.e., military tanks) within a naturalistic desert scene were presented in a perspective view, and a staggering of the timing that was needed for participants to recognize different features of the vehicles produced decision priming. Specifically, in the case in which one feature matched the target conjunction but a second feature did not, decision making was delayed (conjunction costs; see, e.g., <xref ref-type="bibr" rid="bibr12-1555343412445474">Fournier et al., 1998</xref>, <xref ref-type="bibr" rid="bibr13-1555343412445474">2004</xref>, <xref ref-type="bibr" rid="bibr14-1555343412445474">2007</xref>; <xref ref-type="bibr" rid="bibr11-1555343412445474">Fournier, Bowd, et al., 2000</xref>; <xref ref-type="bibr" rid="bibr15-1555343412445474">Fournier, Scheffers, et al., 2000</xref>). These results were used to calibrate our model.</p>
<p>In both the 25% and 50% blocks, we found a negative correlation between RT and accuracy. Although this finding may seem, at first glance, counterintuitive (i.e., how can decision accuracy improve for shorter RTs?), we point out that such a situation may occur when a given task is easier to perform in one condition relative to another condition; for example, individuals may perform the easier task faster and with a higher degree of accuracy than they do the more difficult task.</p>
<p>More specifically, we point out that the trends in our accuracy data suggest that accuracy decreases in those conditions for which decision priming and conjunction costs occur, which is indicated by the negative correlations reported earlier (i.e., accuracy decreases as RT increases). This finding is important because accuracy may have greater significance than does latency in some work situations. Thus, even though we have focused on interpreting our results in terms of RT because decision priming is a temporal phenomenon, it is important that the expectations that engender priming affect both the speed and accuracy of responding.</p>
<p>In our empirical study, decision competition resulted in a delay in final decision making of about 140 ms, or an increase in RT of about 19%. In real-world operational situations, multiple stimuli may be present, attention may be distracted, difficult sensory motor tasks may be performed, and reaction to threats may be a life-or-death proposition. In such operational conditions, the increase in RT (i.e., delay in responding) caused by decision competition could be more than 19%, and in such conditions, such a delay could be fatal (e.g., in the military, a delay in firing at an enemy before being fired upon).</p>
<p>For example, recall the unmanned aerial vehicle scenario discussed in the Introduction. In that scenario, decision priming occurring in the eyes-on person when he or she visually processes the video information could lead to confusion and delay in transmitting information (via the screener) to the pilot and sensor operator. If such information were to involve support for a combat operation, then the delay in responding—which would likely entail seconds rather than milliseconds, given the presence of multiple humans in the chain of communication, which could amplify the effects of decision priming—could involve a life-or-death situation.</p>
<p>We modeled our empirical results within a computational framework that entailed a coactive PCNDO architecture using system dynamics techniques (e.g., <xref ref-type="bibr" rid="bibr9-1555343412445474">Forrester, 1961</xref>, <xref ref-type="bibr" rid="bibr10-1555343412445474">1968</xref>; <xref ref-type="bibr" rid="bibr34-1555343412445474">Sterman, 2000</xref>). We found that our model generated simulated RTs that provided a reasonable fit to our empirical RT data through manipulation of the model growth, decay, and information-weighting parameters. A critical feature of the model was the existence of a threshold that, together with a difference in the net growth rate of feature information integration, created a relative time delay. This relative time delay turned the model into a race structure in which the fastest response to a given feature primed a central decision stage into initiating a decision. In realistic terms, thresholds are considered a stochastic process (<xref ref-type="bibr" rid="bibr2-1555343412445474">Blake &amp; Sekuler, 2005</xref>). Thus, further refinements of the computational architecture would add stochastic processes to our model; however, the general conclusions would remain unchanged (i.e., the present model yields “average” effects).</p>
<p>Recall that in the present study, we sought to create a computational model of decision priming that could be used in the future for exploring various simulated outcomes and to identify how best to present information for rapid decision making when multimodal or multichannel displays are used. As an initial example of how to present information for rapid decision making, we ran simulations that involved systematically manipulating the threshold value of the shape-processing stream to determine how it affected system responding, with the goal of eliminating decision priming and conjunction costs. We found that when the corresponding threshold value of <xref ref-type="disp-formula" rid="disp-formula2-1555343412445474">Equation 2</xref> (for shape) is decreased from 50% to 48.2%, conjunction costs are eliminated. An analogous effect would occur if the corresponding threshold value of the color-processing stream was increased by an equivalent amount. Either manipulation brings the color- and shape-processing streams into temporal alignment, which is consistent with empirical evidence showing that when decisions about individual features are aligned, conjunction benefits and costs disappear (<xref ref-type="bibr" rid="bibr13-1555343412445474">Fournier et al., 2004</xref>).</p>
<p>In real-world operational situations, analogous manipulations would involve the temporal alignment of quickly processed information that comes from different sources, especially information that could introduce potentially ambiguous or misleading initial cues for decision making. Such information could come from a sensor display, head-worn display, head-up display, or an auditory display and would involve separable stimulus dimensions wherein different display attributes are initially processed independently of one another, such as the color and shape of an object (for integral dimensions, such as the height and width of an object, the two dimensions are processed holistically, and the possibility that one dimension will contribute to the priming of decisions made about a conjunction formed from multiple dimensions would be unlikely).</p>
<p>It is likely that the temporal alignment of information from such sources could be implemented by manipulating the physical characteristics of the displays, such as their contrast, brightness, position relative to the operator, or loudness, or by providing attentional markers. Analogous effects would be expected in manipulations that caused stimulus dimensions to be processed at the same time (or processed at very different times, depending on design goal). We predict that such temporal alignment would eliminate decision priming in the real world. Thus, our model can be used to assist in testing different display designs when the dynamics of processing individual display features are known.</p>
<p>However, in the example of the unmanned aerial vehicle scenario discussed previously, wherein the eyes-on person is viewing a sensor feed, it may be difficult or impossible to manipulate factors that temporally align the mental processing of various cues in the display. Nonetheless, it would still be important to acknowledge the potential for decision priming in such situations so that action could be taken to remedy the potential problem, such as colocating the eyes-on person and screener together with the pilot and sensor operator.</p>
<p>Furthermore, we discovered an interesting result when simulating the situation that produces decision competition, that is, when simulating the presence of the target color together with a nontarget shape (i.e., the C, nS condition). We found that only a very slight shift in decision commitment was necessary to produce the decision competition and the conjunction-costs priming effect. Specifically, at the beginning of the simulation, the level of decision commitment for the <italic>target present</italic> response started out at 50% (as designed). As the simulation proceeded, this decision commitment slightly increased to 50.19%, owing to the incoming target color information. Next, the decision commitment for the <italic>target present</italic> response began its decline toward 0%, owing to the subsequent incoming nontarget shape information (i.e., ending with a <italic>target absent</italic> response). Thus, the bias in decision commitment underlying the priming effect amounted to a temporary increase in simulated commitment of only 0.19%.</p>
<p>Thus, a shift in decision commitment toward a <italic>target present</italic> response of less than one fifth of a percentage point was sufficient for biasing the process so that the final decision (<italic>target absent</italic> response) was significantly delayed, and conjunction costs were produced. This is a surprising result that shows that large changes in end states can arise from very subtle changes in initial conditions, which is consistent with a nonlinear system. Accordingly, we predict that human decision making in the real world is susceptible to subtle biases, which can produce potentially large decision-priming effects. To minimize the effects of decision priming, efforts could be directed toward designing work tools to support resilience in the face of such sensitivity to initial conditions.</p>
<p>Our computational model of decision priming can be recalibrated for use in different operational contexts (e.g., decision making by a sensor operator in a remotely piloted aircraft situation) and tested to determine whether its simulated dynamics can be useful for predicting decision making in those situations. Our ability to do so will be founded on the use of system dynamics techniques for modeling complex dynamic cognitive systems.</p>
<p>Despite our efforts at exploring several simulated hypothetical outcomes, discussed earlier, it remains to be determined to what degree our model and simulation results will actually generalize to more complex operational displays and environments. Accordingly, directions for future research would include an examination of decision priming phenomena in conditions that more closely represent real-world military decision tasks, such as those involving diverted attention, performance of difficult sensory motor tasks, or high-stakes outcomes.</p>
<p>Accordingly, future directions for the model would include adding components that model priming among multiple cues and the addition of a stochastic random-walk component to each channel (<xref ref-type="bibr" rid="bibr29-1555343412445474">Ratcliff, 1978</xref>, <xref ref-type="bibr" rid="bibr30-1555343412445474">2001</xref>). Toward that end, we have just completed data collection for a three-cue decision-priming study for which the cues were target color, shape, and orientation (the visual processing of all three of which were staggered in time). Initial inspection of the data suggests that we obtained decision priming in terms of conjunction benefits as well as conjunction costs, with the latter being stronger when two of the cues primed the decision (and the third cue was absent) relative to when only one cue primed the decision (and two cues were absent). Initial attempts at modeling these three-cue data seem to suggest that simulations of our system dynamics model may follow the data fairly closely. In the near future, we will statistically analyze the data and formally model them, the results of which will be reported in a subsequent manuscript.</p>
</sec>
</body>
<back>
<ack>
<p>This manuscript has been cleared for public release.</p>
</ack>
<fn-group>
<fn fn-type="supported-by"><p>This work was supported by U.S. Air Force Contract FA8650-05-D-6502, Task Order 0037, to Link Simulation and Training, L-3 Communications.</p></fn>
</fn-group>
<bio>
<p>Robert Earl Patterson is a branch technical advisor for the Anticipate and Influence Behavior Research Division, 711 Human Performance Wing, of the Air Force Research Laboratory at Wright-Patterson AFB, Ohio. Previously he was a tenured full professor of psychology and neuroscience at Washington State University. His research focuses on applied cognition, cognitive engineering, applied vision, and system dynamics modeling. He received his PhD from Vanderbilt University in 1984 and was a postdoctoral research fellow at Northwestern University from 1985 to 1987.</p>
<p>Lisa R. Fournier is an associate professor and director of experimental training in the psychology department at Washington State University. Her research focuses on visual attention, perception, memory, and action. Most recently, she has been investigating how perception-based actions (both internally and externally guided action) are represented. She received her PhD in experimental psychology from the University of Illinois at Urbana-Champaign in 1993. She also was a postdoctoral trainee in the cognitive psychophysiology laboratory at the University of Illinois at Urbana-Champaign from 1993 to 1995 and received a Summer Research Fellowship from the Air Force Office of Scientific Research in 1995 to assess workload in a real-world, multitask environment using psychophysiological and behavioral measures.</p>
<p>Logan Williams is a research engineer at the Air Force Research Laboratory, Warfighter Readiness Research Division, at Wright-Patterson AFB, Ohio. He has previously served as the lead systems engineer for the F-16, A-10, and KC-135 aircrew training systems and has more than a decade of experience in networked control systems, optical system design, and physics-based modeling and simulation. He has earned BS degrees in electrical engineering and physics as well as an MS in electrical engineering from the University of Utah. He is currently pursuing a PhD in electro-optical engineering at the University of Dayton.</p>
<p>Ryan Amann was a research scientist at the Air Force Research Laboratory in Mesa, Arizona, from 2007 to 2011. His research interests are in the areas of applied cognition and applied perception. He received a BS degree in applied psychology from Arizona State University in 2007.</p>
<p>Lisa M. Tripp is a recent graduate of Washington State University, where she earned a doctorate in experimental psychology (2011) and MS degrees in experimental psychology (2009) and applied mathematics (2007). She was a participant in the Department of Defense’s SMART Scholarship Program (2010-2011). Her research focuses on applied cognition, cognitive engineering, and system dynamics modeling. She is currently working as a research psychologist for the Warfighter Readiness Research Division, 711th Human Performance Wing, of the Air Force Research Laboratory at Wright-Patterson AFB, Ohio.</p>
<p>Byron J. Pierce is a program manager of training systems analysis at Renaissance Sciences Cor-poration in Chandler, Arizona. He is also a Volunteer Emeritus Corps scientist for the Warfighter Readiness Research Division, Human Effectiveness Directorate, 711th Human Perfor mance Wing, at Wright Patterson AFB, Ohio. From 1995 to 2007, he led the Visual Science and Technology Team at the division, and from 2007 to 2011, he was program lead for Robust Immersive Decision Environments research. He received his doctorate in experimental psychology at Arizona State University in 1989 and MS degree at University of Illinois in 1978.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>J. R.</given-names></name>
<name><surname>Bothell</surname><given-names>D.</given-names></name>
<name><surname>Byrne</surname><given-names>M. D.</given-names></name>
</person-group> (<year>2004</year>). <article-title>An integrated theory of the mind</article-title>. <source>Psychological Review</source>, <volume>111</volume>, <fpage>1036</fpage>–<lpage>1060</lpage>.</citation>
</ref>
<ref id="bibr2-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Blake</surname><given-names>R.</given-names></name>
<name><surname>Sekuler</surname><given-names>R.</given-names></name>
</person-group> (<year>2005</year>). <source>Perception</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr3-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>S.</given-names></name>
<name><surname>Heathcote</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>A ballistic model of choice response time</article-title>. <source>Psychological Review</source>, <volume>112</volume>, <fpage>117</fpage>–<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr4-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Busemeyer</surname><given-names>J. R.</given-names></name>
<name><surname>Townsend</surname><given-names>J. T.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Decision field theory: A dynamic-cognitive approach to decision making in an uncertain environment</article-title>. <source>Psychological Review</source>, <volume>100</volume>, <fpage>432</fpage>–<lpage>459</lpage>.</citation>
</ref>
<ref id="bibr5-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ditterich</surname><given-names>J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Evidence for time-variant decision making</article-title>. <source>European Journal of Neuroscience</source>, <volume>24</volume>, <fpage>3628</fpage>–<lpage>3641</lpage>.</citation>
</ref>
<ref id="bibr6-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dodwell</surname><given-names>P. C.</given-names></name>
</person-group> (<year>1971</year>). <article-title>On perceptual clarity</article-title>. <source>Psychological Review</source>, <volume>78</volume>, <fpage>275</fpage>–<lpage>289</lpage>.</citation>
</ref>
<ref id="bibr7-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eriksen</surname><given-names>C. W.</given-names></name>
<name><surname>Schultz</surname><given-names>D. W.</given-names></name>
</person-group> (<year>1979</year>). <article-title>Information processing in visual search: A continuous flow conception and experimental results</article-title>. <source>Perception and Psychophysics</source>, <volume>25</volume>, <fpage>249</fpage>–<lpage>263</lpage>.</citation>
</ref>
<ref id="bibr8-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eriksen</surname><given-names>C. W.</given-names></name>
<name><surname>St. James</surname><given-names>J. D.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Visual attention within and around the field of focal attention: A zoon-lens model</article-title>. <source>Perception and Psychophysics</source>, <volume>40</volume>, <fpage>225</fpage>–<lpage>240</lpage>.</citation>
</ref>
<ref id="bibr9-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Forrester</surname><given-names>J. W.</given-names></name>
</person-group> (<year>1961</year>). <source>Industrial dynamics</source>. <publisher-loc>Waltham, MA</publisher-loc>: <publisher-name>Pegasus Communications</publisher-name>.</citation>
</ref>
<ref id="bibr10-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Forrester</surname><given-names>J. W.</given-names></name>
</person-group> (<year>1968</year>). <source>Principles of systems</source>. <publisher-loc>Waltham, MA</publisher-loc>: <publisher-name>Pegasus Communications</publisher-name>.</citation>
</ref>
<ref id="bibr11-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fournier</surname><given-names>L. R.</given-names></name>
<name><surname>Bowd</surname><given-names>C.</given-names></name>
<name><surname>Herbert</surname><given-names>R. J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Interference from multi-dimensional objects during feature and conjunction discriminations</article-title>. <source>Quarterly Journal of Experimental Psychology</source>, <volume>53A</volume>, <fpage>191</fpage>–<lpage>209</lpage>.</citation>
</ref>
<ref id="bibr12-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fournier</surname><given-names>L. R.</given-names></name>
<name><surname>Eriksen</surname><given-names>C. W.</given-names></name>
<name><surname>Bowd</surname><given-names>C.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Multiple feature discrimination faster than single feature discrimination within the same object?</article-title> <source>Perception and Psychophysics</source>, <volume>60</volume>, <fpage>1384</fpage>–<lpage>1405</lpage>.</citation>
</ref>
<ref id="bibr13-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fournier</surname><given-names>L. R.</given-names></name>
<name><surname>Herbert</surname><given-names>R. J.</given-names></name>
<name><surname>Farris</surname><given-names>C.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Demands on attention and the role of response priming in visual discrimination of feature conjunctions</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>30</volume>, <fpage>836</fpage>–<lpage>852</lpage>.</citation>
</ref>
<ref id="bibr14-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fournier</surname><given-names>L. R.</given-names></name>
<name><surname>Patterson</surname><given-names>R.</given-names></name>
<name><surname>Dyre</surname><given-names>B. P.</given-names></name>
<name><surname>Wiediger</surname><given-names>M.</given-names></name>
<name><surname>Winters</surname><given-names>R.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Conjunction benefits and costs reveal decision priming for first-order and second-order features</article-title>. <source>Perception and Psychophysics</source>, <volume>69</volume>, <fpage>1409</fpage>–<lpage>1421</lpage>.</citation>
</ref>
<ref id="bibr15-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fournier</surname><given-names>L. R.</given-names></name>
<name><surname>Scheffers</surname><given-names>M. K.</given-names></name>
<name><surname>Coles</surname><given-names>M. G. H.</given-names></name>
<name><surname>Adamson</surname><given-names>A.</given-names></name>
<name><surname>Vila</surname><given-names>E.</given-names></name>
</person-group> (<year>2000</year>). <article-title>When complexity helps: An electrophysiological analysis of multiple feature benefits in object perception</article-title>. <source>Acta Psychologica</source>, <volume>104</volume>, <fpage>119</fpage>–<lpage>142</lpage>.</citation>
</ref>
<ref id="bibr16-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gershenfeld</surname><given-names>N. A.</given-names></name>
</person-group> (<year>1999</year>). <source>The nature of mathematical modeling</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grill-Spector</surname><given-names>K.</given-names></name>
<name><surname>Kanwisher</surname><given-names>N.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Visual recognition: As soon as you know it is there, you know what it is</article-title>. <source>Psychological Science</source>, <volume>16</volume>, <fpage>152</fpage>–<lpage>160</lpage>.</citation>
</ref>
<ref id="bibr18-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kahneman</surname><given-names>D.</given-names></name>
<name><surname>Tversky</surname><given-names>A.</given-names></name>
</person-group> (<year>1979</year>). <article-title>Prospect theory: An analysis of decision under risk</article-title>. <source>Econometrics</source>, <volume>47</volume>, <fpage>263</fpage>–<lpage>291</lpage>.</citation>
</ref>
<ref id="bibr19-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lavie</surname><given-names>N.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Perceptual load as a necessary condition for selective attention</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>21</volume>, <fpage>451</fpage>–<lpage>468</lpage>.</citation>
</ref>
<ref id="bibr20-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Logan</surname><given-names>G. D.</given-names></name>
<name><surname>Cowan</surname><given-names>W. B.</given-names></name>
</person-group> (<year>1984</year>). <article-title>On the ability to inhibit thought and action: A theory of an act of control</article-title>. <source>Psychological Review</source>, <volume>91</volume>, <fpage>295</fpage>–<lpage>327</lpage>.</citation>
</ref>
<ref id="bibr21-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lotka</surname><given-names>A. J.</given-names></name>
</person-group> (<year>1956</year>). <source>Elements of mathematical biology</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Dover</publisher-name>.</citation>
</ref>
<ref id="bibr22-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Meyer</surname><given-names>D. E.</given-names></name>
<name><surname>Yantis</surname><given-names>S.</given-names></name>
<name><surname>Osman</surname><given-names>A. M.</given-names></name>
<name><surname>Smith</surname><given-names>J. E.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Temporal properties of human information processing: Tests of discrete versus continuous models</article-title>. <source>Cognitive Psychology</source>, <volume>17</volume>, <fpage>445</fpage>–<lpage>518</lpage>.</citation>
</ref>
<ref id="bibr23-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mueller</surname><given-names>S. T.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A Bayesian recognitional decision model</article-title>. <source>Journal of Cognitive Engineering and Decision Making</source>, <volume>3</volume>, <fpage>111</fpage>–<lpage>130</lpage>.</citation>
</ref>
<ref id="bibr24-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nickerson</surname><given-names>R. S.</given-names></name>
</person-group> (<year>1972</year>). <article-title>Binary-classification reaction time: A review of some studies of human information-processing capabilities</article-title>. <source>Psychonomic Monograph Supplements</source>, <volume>4</volume>(<issue>17</issue>, Whole No. 65), <fpage>275</fpage>–<lpage>317</lpage>.</citation>
</ref>
<ref id="bibr25-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nickerson</surname><given-names>R. S.</given-names></name>
</person-group> (<year>1973</year>). <article-title>Frequency, recency, and repetition effects on same and different response times</article-title>. <source>Journal of Experimental Psychology</source>, <volume>101</volume>, <fpage>330</fpage>–<lpage>336</lpage>.</citation>
</ref>
<ref id="bibr26-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Oliva</surname><given-names>A.</given-names></name>
<name><surname>Torralba</surname><given-names>A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Building the gist of a scene: The role of global image features in recognition</article-title>. <source>Progress in Brain Research: Visual Perception</source>, <volume>155</volume>, <fpage>23</fpage>–<lpage>36</lpage>.</citation>
</ref>
<ref id="bibr27-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Patterson</surname><given-names>R.</given-names></name>
<name><surname>Fournier</surname><given-names>L.</given-names></name>
<name><surname>Pierce</surname><given-names>B.</given-names></name>
<name><surname>Winterbottom</surname><given-names>M.</given-names></name>
<name><surname>Tripp</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>System dynamics modeling of the recognition-primed decision model</article-title>. <source>Journal of Cognitive Engineering and Decision Making</source>, <volume>3</volume>, <fpage>250</fpage>–<lpage>276</lpage>.</citation>
</ref>
<ref id="bibr28-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Perez</surname><given-names>C.</given-names></name>
</person-group> (<year>2002</year>). <source>Technological revolution and financial capital: The dynamics of bubbles and golden ages</source>. <publisher-loc>Cheltenham, UK</publisher-loc>: <publisher-name>Edward Elgar Publishing</publisher-name>.</citation>
</ref>
<ref id="bibr29-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ratcliff</surname><given-names>R.</given-names></name>
</person-group> (<year>1978</year>). <article-title>A theory of memory retrieval</article-title>. <source>Psychological Review</source>, <volume>85</volume>, <fpage>59</fpage>–<lpage>108</lpage>.</citation>
</ref>
<ref id="bibr30-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ratcliff</surname><given-names>R.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Diffusion and random walk processes</article-title>. In <person-group person-group-type="editor">
<name><surname>Smelser</surname><given-names>N. J.</given-names></name>
<name><surname>Baltes</surname><given-names>P. B.</given-names></name>
</person-group> (Eds.), <source>International encyclopedia of the social and behavioral sciences</source> (pp. <fpage>3668</fpage>–<lpage>3673</lpage>). <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr31-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schall</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Neural basis of deciding, choosing and acting</article-title>. <source>Nature Reviews</source>, <volume>2</volume>, <fpage>33</fpage>–<lpage>42</lpage>.</citation>
</ref>
<ref id="bibr32-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schall</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Neural correlates of decision processes: neural and mental chronometry</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>13</volume>, <fpage>182</fpage>–<lpage>186</lpage>.</citation>
</ref>
<ref id="bibr33-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shiffrin</surname><given-names>R. M.</given-names></name>
<name><surname>Schneider</surname><given-names>W.</given-names></name>
</person-group> (<year>1977</year>). <article-title>Controlled and automatic human information processing II. Perceptual learning, automatic attending, and a general theory</article-title>. <source>Psychological Review</source>, <volume>84</volume>, <fpage>127</fpage>–<lpage>190</lpage>.</citation>
</ref>
<ref id="bibr34-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Sterman</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2000</year>). <source>Business dynamics: Systems thinking and modeling for a complex world</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>.</citation>
</ref>
<ref id="bibr35-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thorpe</surname><given-names>S.</given-names></name>
<name><surname>Fize</surname><given-names>D.</given-names></name>
<name><surname>Marlot</surname><given-names>C.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source>, <volume>381</volume>, <fpage>520</fpage>–<lpage>522</lpage>.</citation>
</ref>
<ref id="bibr36-1555343412445474">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Townsend</surname><given-names>J. T.</given-names></name>
<name><surname>Ashby</surname><given-names>F. G.</given-names></name>
</person-group> (<year>1983</year>). <source>The stochastic modeling of elementary psychological processes</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr37-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Townsend</surname><given-names>J. T.</given-names></name>
<name><surname>Wenger</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2004</year>). <article-title>A theory of interactive parallel processing: New capacity measures and predictions for a response time inequality series</article-title>. <source>Psychological Review</source>, <volume>111</volume>, <fpage>1003</fpage>–<lpage>1035</lpage>.</citation>
</ref>
<ref id="bibr38-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treisman</surname><given-names>A. M.</given-names></name>
<name><surname>Gelade</surname><given-names>A.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Feature integration theory of attention</article-title>. <source>Cognitive Psychology</source>, <volume>12</volume>, <fpage>97</fpage>–<lpage>136</lpage>.</citation>
</ref>
<ref id="bibr39-1555343412445474">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Usher</surname><given-names>M.</given-names></name>
<name><surname>McClelland</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The time course of perceptual choice: The leaky, competing accumulator model</article-title>. <source>Psychological Review</source>, <volume>108</volume>, <fpage>550</fpage>–<lpage>592</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>