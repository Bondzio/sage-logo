<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ULT</journal-id>
<journal-id journal-id-type="hwp">spult</journal-id>
<journal-title>Ultrasound</journal-title>
<issn pub-type="ppub">1742-271X</issn>
<issn pub-type="epub">1743-1344</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1742271X13493088</article-id>
<article-id pub-id-type="publisher-id">10.1177_1742271X13493088</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Original Research</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Is there agreement on what makes a good ultrasound image?</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Keeble</surname><given-names>C</given-names></name>
<xref ref-type="aff" rid="aff1-1742271X13493088">1</xref>
<xref ref-type="corresp" rid="corresp1-1742271X13493088"/>
</contrib>
<contrib contrib-type="author">
<name><surname>Wolstenhulme</surname><given-names>S</given-names></name>
<xref ref-type="aff" rid="aff1-1742271X13493088">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Davies</surname><given-names>AG</given-names></name>
<xref ref-type="aff" rid="aff1-1742271X13493088">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Evans</surname><given-names>JA</given-names></name>
<xref ref-type="aff" rid="aff1-1742271X13493088">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-1742271X13493088"><label>1</label>Centre for Epidemiology &amp; Biostatistics, Worsley Building, University of Leeds, Leeds, UK; <sup>2</sup>School of Healthcare, Baines Wing, University of Leeds, Leeds, UK; <sup>3</sup>Division of Medical Physics, University of Leeds, The General Infirmary, Leeds, UK</aff>
<author-notes>
<corresp id="corresp1-1742271X13493088">Claire Keeble. Email: <email>mm07cmk@leeds.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2013</year>
</pub-date>
<volume>21</volume>
<issue>3</issue>
<fpage>118</fpage>
<lpage>123</lpage>
<permissions>
<copyright-statement>© The British Medical Ultrasound Society 2013 Reprints and permissions: sagepub.co.uk/journalsPermissions.nav</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">The British Medical Ultrasound Society</copyright-holder>
</permissions>
<abstract>
<p>With the introduction of national guidelines for ultrasound screening it might be assumed that there is agreement in the key features required in assessing image quality. The aim of this study was to determine whether it is possible that personal preferences may influence what is regarded as an acceptable image. Two image sets, one vascular and one obstetric, were taken to the British Medical Ultrasound Society ‘Ultrasound 2012’ conference for delegates to rate whether images were acceptable or not. Data collection took place on days one and two, with the results presented on day three of the conference. Images from a variety of ultrasound machines were used and ranked in order of the odds of producing an acceptable image using logistic regression. Agreement between observers was investigated and three images per person were repeated to look at agreement within observers. Audience feedback was used to record the reasons why images were regarded as acceptable or not. Eighty-two participants reviewed each of the two image sets. The machine rankings revealed that some machines were up to 12 times more likely to produce an acceptable image than other machines. Agreement amongst experts or non-experts was found, but disagreement between the subgroups of experts and non-experts. Agreement within observers was around 80% and similar results were found in each of the image sets. Despite image quality assessment, personal preferences and expertise may still affect judgement, and guidelines may not ensure agreement.</p>
</abstract>
<kwd-group>
<kwd>Quality</kwd>
<kwd>diagnostic</kwd>
<kwd>guidelines</kwd>
<kwd>agreement</kwd>
<kwd>ultrasound</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="sec1-1742271X13493088" sec-type="intro"><title>Introduction</title>
<p>There is a long-standing problem in the lack of agreement between users’ perception of ultrasound image quality and objective test data.<sup><xref ref-type="bibr" rid="bibr1-1742271X13493088">1</xref>,<xref ref-type="bibr" rid="bibr2-1742271X13493088">2</xref></sup> This is exacerbated by the constant change and innovation, which makes it difficult for testing techniques to keep pace.<sup><xref ref-type="bibr" rid="bibr3-1742271X13493088">3</xref><xref ref-type="bibr" rid="bibr4-1742271X13493088"/><xref ref-type="bibr" rid="bibr5-1742271X13493088"/><xref ref-type="bibr" rid="bibr6-1742271X13493088"/><xref ref-type="bibr" rid="bibr7-1742271X13493088"/>–<xref ref-type="bibr" rid="bibr8-1742271X13493088">8</xref></sup> The UK National Screening Committee have implemented programmes, which use ultrasound imaging for screening purposes, such as the Fetal Anomaly Screening Programme (FASP)<sup><xref ref-type="bibr" rid="bibr9-1742271X13493088">9</xref></sup> and the National Abdominal Aortic Aneurysm Screening Programme (NAAASP).<sup><xref ref-type="bibr" rid="bibr10-1742271X13493088">10</xref></sup> The FASP and NAAASP programmes require horizontal crown-rump length (CRL) and axial maximum, external antero-posterior, abdominal aortic diameter measurements, respectively. The standard operating procedures for these programmes specify features that should be demonstrated in the ultrasound image, for example, whether anatomical landmarks and measurement end-points can be seen.<sup><xref ref-type="bibr" rid="bibr9-1742271X13493088">9</xref>,<xref ref-type="bibr" rid="bibr10-1742271X13493088">10</xref></sup> There is general agreement that the clinical quality of ultrasound images should be monitored and maintained but there is currently no consensus as to how to achieve this. In this context, it is important to know the extent to which observers agree with each other in their subjective evaluation of images. It is possible that personal preferences may dictate what is regarded as an acceptable image. The British Medical Ultrasound Society (BMUS) ‘Ultrasound 2012’ conference provided an opportunity to gather evidence, which might help to clarify this issue by conducting live research, including the collection of data from many experienced specialists. We aimed to investigate if the ultrasound machine model, or the expertise of the observer, influenced subjective preference for ultrasound images. We also examined the intra-observer agreement and inter-observer agreement, within and between expert and non-expert observers.</p>
</sec>
<sec id="sec2-1742271X13493088" sec-type="methods"><title>Methods</title>
<p>Two sets of images, one vascular and one obstetric were taken to the BMUS ‘Ultrasound 2012’ conference in Telford, 10th—12th December 2012. The ‘What Makes a Good Image?’ stall was constructed in the exhibition hall and delegates were encouraged to participate in the data collection. The observer evaluations took place in a covered booth to replicate the conditions used to view ultrasound images in practice, such as dimmed ambient lighting, and to maintain privacy. Viewing was performed on a personal computer using dedicated software written in MATLAB (MATLAB, R2012a, Mathworks Inc, Natick, MA). The software was written after consultation with a statistician to ensure relevant information was collected for analysis. The computer screen size and resolution were specifically chosen to be as close as possible to those used in practice. The same screen was used for all observers to minimise bias. Volunteer observers were assigned a unique identification number by the viewing software, and were first asked to choose between two image sets: CRL or transverse abdominal aortic aneurysm (AAA) screening images. Next, observers were asked if they use the selected types of clinical images in their daily work, which was later used to categorise the observers as ‘expert’ or ‘non-expert’. Finally, the observers were shown, one at a time, images from the selected set and asked to choose between ‘Yes’ and ‘No’ when asked whether the image was of acceptable diagnostic quality. No reference to the national screening guidelines was made in the question, so as not to discourage delegates from participating, but it was assumed that those who reviewed similar images in their daily work would be familiar with the guidelines and use them to guide their judgement. Those observers who asked for clarification were referred to the national guidelines. The viewing order was randomised for each observer. All observer response data were recorded into a file for subsequent statistical analysis.</p>
<p>The images used were selected from those acquired during two studies approved and funded by the National Screening Committee. The CRL images were collected during a study to determine the equipment specification for ultrasound machines suitable for use in the FASP. The AAA images were collected during a similar study prior to the introduction of the NAAASP. All images were anonymised at the point of collection and hence the patient identity was blinded from all observers.</p>
<p>For both the CRL and AAA images, the original image set was collected from all patients arriving routinely. A subset of the acquired images was selected for use at the BMUS scoring session. The subset was selected according to the following constraints:
<list id="list1-1742271X13493088" list-type="order">
<list-item><p>Three images were included from each different machine.</p></list-item>
<list-item><p>The total number of images to be viewed by each observer was limited to around 30 in order to avoid observer fatigue.</p></list-item>
<list-item><p>The image set included three repeat images to evaluate observer self-reproducibility.</p></list-item>
</list>For the CRL images, we concentrated on those images which showed the majority of the body and head section. When selecting the AAA images, we avoided those in which the aorta was superficial (&lt;7 cm) as we argued these would lack challenge. We also excluded those with very deep aortas (&gt;12 cm) as they could be unreasonably challenging. One objective was to rank the machines by their odds of producing an acceptable image. Since the machine type is categorical and the outcome is binary, sample size calculations are not possible and therefore the required sample size cannot be calculated. Instead, data were gathered from as many delegates as possible and the number of images used was designed to be as large as possible, while considering the conference environment.</p>
<p>The AAA image set contained 27 different images plus three repeated images, totalling 30 images for the observer to assess, while the CRL image set consisted of 30 different images plus three repeated images, totalling 33 images. The AAA image set contained three images from each of nine different machines, while the CRL image set contained three images from each of 10 machines. The machines, from which the images were taken, are shown in <xref ref-type="table" rid="table1-1742271X13493088">Table 1</xref>. The survey took around three minutes for each observer to complete. Observers were allowed to participate in the data collection for both image sets, but forbidden from participating in either image set more than once. The data collection took place on the first two days of the conference, 10th—11th December 2012, with the results being presented on the third day of the conference, 12th December 2012.
<table-wrap id="table1-1742271X13493088" position="float"><label>Table 1</label><caption><p>The machines from which the images used in the survey were taken</p></caption>
<graphic alternate-form-of="table1-1742271X13493088" xlink:href="10.1177_1742271X13493088-table1.tif"/>
<table frame="hsides"><thead align="left">
<tr><th>CRL machines</th>
<th>AAA machines</th>
</tr></thead>
<tbody align="left">
<tr>
<td>Aloka 5000</td>
<td>GE LogiQ-E</td>
</tr>
<tr>
<td>Aloka Prosound_V6</td>
<td>GE LogiQ Book XP</td>
</tr>
<tr>
<td>Aloka SSD3500</td>
<td>Philips CX50</td>
</tr>
<tr>
<td>Hitachi EUB6500</td>
<td>SIUI CTS-900 (c/o MIS)</td>
</tr>
<tr>
<td>Philips HD11</td>
<td>Sonosite M-Turbo</td>
</tr>
<tr>
<td>Philips IU22</td>
<td>Sonosite Micromaxx</td>
</tr>
<tr>
<td>Prosound Alpha 10</td>
<td>Sonosite Nanomaxx</td>
</tr>
<tr>
<td>Siemens Acuson X300</td>
<td>Toshiba Viamo</td>
</tr>
<tr>
<td>Siemens Anatares</td>
<td>Zonare Z-one</td>
</tr>
<tr>
<td>Toshiba Nemio</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1742271X13493088">
<p>CRL: crown-rump length; AAA: abdominal aortic aneurysm</p></fn></table-wrap-foot>
</table-wrap></p>
<p>The statistical analysis was carried out between the second and third day of the conference and used the statistical software R.<sup><xref ref-type="bibr" rid="bibr11-1742271X13493088">11</xref></sup> Logistic regression was used to generate odds ratios, with 95% confidence intervals, to rank the machines in order of their odds of producing an acceptable image and investigate the agreement between the observers. The logistic regression model was generated with the survey result as the outcome (Yes/No) and took into account the observer identification number and machine type to allow for the differences between observers and between machines. The odds ratios were generated by taking the exponential of the logistic regression model coefficient estimate for a given machine. The odds ratio then shows the odds of the given machine producing an acceptable image. The odds ratio for each machine was compared with the least preferred machine, which was assigned a reference odds ratio of one. Agreement between observers was investigated using the variation in survey responses from one observer to another. The three repeated images were used to assess agreement within observers by calculating the number of times the observers gave the same or different responses for a given image. Subgroups of the data, experts and non-experts, were also analysed. During the presentation of the results at the conference, audience feedback was used to record the reasons why images were regarded as acceptable or not.</p>
</sec>
<sec id="sec3-1742271X13493088" sec-type="results"><title>Results</title>
<p>A total of 82 observers participated in each of the two image sets, which were analysed separately. This resulted in 164 observers, each analysing at least 30 images, giving 5166 responses in total. We believe that statistically, this is a sufficient sample to draw conclusions. It was found that 63 of the 82 CRL delegates (77%) and 58 of the 82 AAA observers (71%) stated they reviewed similar images daily in their work and hence were termed as ‘experts’, with the remainder being termed as ‘non-experts’. The percentage of ‘yes’ responses was 33.0% and 41.5% in the AAA and CRL data sets, respectively.</p>
<p>Agreement between the observers regarding whether or not an image was acceptable was tested. It was expected that the image acceptability rating should be determined more by the machine type used to generate the image than the observer. It was found that, when all observers are considered for a single data set, the machine type used is only half as important as the observer in determining the image rating. The observer is therefore more influential than the machine type. This means that there is more variability between observers than between machine types. More variability between observers means less agreement between observers, as a range of different responses were given. However, when the data sets are split by subgroups of experts and non-experts, the machine type is more influential than the observer. Therefore, there is less variability between observers, suggesting similarities between those in each of the subgroups and hence agreement in the responses given. The most extreme case is for the AAA data set in the expert subgroup, where the machine type is 65 times more influential in determining the image rating than the observer, derived using the variance between machines types and between observers. The non-expert group for the AAA data set had a value of three times more, while the expert and non-expert CRL groups had values of four and seven, respectively. This could suggest that within the subgroups, observers generally agree whether an image is acceptable or not, but there is disagreement between the subgroups. There is less variability in the survey results when considering just the expert or non-expert subgroup, as similar ‘Yes/No’ responses were given in the survey, but when experts and non-experts are considered together there is more disagreement as to whether an image is acceptable, hence more variability in the survey results.</p>
<p><xref ref-type="table" rid="table2-1742271X13493088">Table 2</xref> shows the ordering of the machines, when ranked by their odds of producing an acceptable image, compared with the least successful machine. The most successful machine was almost 12 times more likely to produce an acceptable image than the least successful machine in the AAA data set (<xref ref-type="fig" rid="fig1-1742271X13493088">Figure 1</xref>). In the CRL data set (<xref ref-type="fig" rid="fig2-1742271X13493088">Figure 2</xref>), the most successful machine was over four times more likely to produce an acceptable image, compared with the least successful machine. This suggests that for the images used, there are differences between the machine types, which contribute towards whether an image of acceptable quality for diagnostic purposes is produced. <xref ref-type="table" rid="table2-1742271X13493088">Table 2</xref> was generated using the results from the data sets including all participating delegates, but similar results were found when the data sets were split by subgroups of experts and non-experts.
<fig id="fig1-1742271X13493088" position="float"><label>Figure 1</label><caption><p>Odds ratios and 95% confidence intervals for each AAA machine</p>
<p>AAA: abdominal aortic aneurysm</p></caption><graphic xlink:href="10.1177_1742271X13493088-fig1.tif"/>
</fig>
<fig id="fig2-1742271X13493088" position="float"><label>Figure 2</label><caption><p>Odds ratios and 95% confidence intervals for each CRL machine</p>
<p>CRL: crown-rump length</p></caption><graphic xlink:href="10.1177_1742271X13493088-fig2.tif"/>
</fig>
<table-wrap id="table2-1742271X13493088" position="float"><label>Table 2</label><caption><p>The odds of producing an acceptable image by machine type with 95% confidence interval</p></caption>
<graphic alternate-form-of="table2-1742271X13493088" xlink:href="10.1177_1742271X13493088-table2.tif"/>
<table frame="hsides"><thead align="left">
<tr><th>CRL machine number</th>
<th>Estimate</th>
<th>AAA machine number</th>
<th>Estimate</th>
</tr></thead>
<tbody align="left">
<tr>
<td>1</td>
<td>1.00 (1.00, 1.00)</td>
<td>1</td>
<td>1.00 (1.00, 1.00)</td>
</tr>
<tr>
<td>2</td>
<td>1.03 (0.69, 1.53)</td>
<td>2</td>
<td>2.23 (1.33, 3.72)</td>
</tr>
<tr>
<td>3</td>
<td>1.24 (0.84, 1.83)</td>
<td>3</td>
<td>3.32 (2.01, 5.46)</td>
</tr>
<tr>
<td>4</td>
<td>1.72 (1.18, 2.53)</td>
<td>4</td>
<td>5.03 (3.08, 8.21)</td>
</tr>
<tr>
<td>5</td>
<td>2.33 (1.60, 3.41)</td>
<td>5</td>
<td>5.73 (3.52, 9.34)</td>
</tr>
<tr>
<td>6</td>
<td>2.49 (1.70, 3.64)</td>
<td>6</td>
<td>6.03 (3.70, 9.84)</td>
</tr>
<tr>
<td>7</td>
<td>3.25 (2.23, 4.74)</td>
<td>7</td>
<td>6.59 (4.05, 10.72)</td>
</tr>
<tr>
<td>8</td>
<td>3.29 (2.25, 4.81)</td>
<td>8</td>
<td>11.72 (7.23, 19.02)</td>
</tr>
<tr>
<td>9</td>
<td>4.36 (2.98, 6.40)</td>
<td>9</td>
<td>11.99 (7.38, 19.46)</td>
</tr>
<tr>
<td>10</td>
<td>4.37 (3.00, 6.39)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-1742271X13493088">
<p>CRL: crown-rump length; AAA: abdominal aortic aneurysm</p></fn></table-wrap-foot>
</table-wrap></p>
<p>The most and least preferred images from each data set were chosen using the highest and lowest number of ‘Yes’ votes, respectively. The most preferred CRL image (<xref ref-type="fig" rid="fig3-1742271X13493088">Figure 3</xref>) received a total of 82 ‘Yes’ votes. However, this machine was only ranked fourth overall in the 10 CRL machines. The most preferred AAA image (<xref ref-type="fig" rid="fig4-1742271X13493088">Figure 4</xref>) received 70 ‘Yes’ votes and was taken from the machine ranked third overall in the nine AAA machines. The least preferred CRL image (<xref ref-type="fig" rid="fig5-1742271X13493088">Figure 5</xref>) only received two ‘Yes’ votes; the corresponding machine was also ranked bottom of the CRL machines. The least preferred AAA image (<xref ref-type="fig" rid="fig6-1742271X13493088">Figure 6</xref>) only received one ‘Yes’ vote, but was from the same machine which also produced the best AAA image. The most and least preferred images were generated using the entire data set, but similar results were found when the data sets were split into subgroups of experts and non-experts.
<fig id="fig3-1742271X13493088" position="float"><label>Figure 3</label><caption><p>The most preferred CRL image</p>
<p>CRL: crown-rump length</p></caption><graphic xlink:href="10.1177_1742271X13493088-fig3.tif"/>
</fig>
<fig id="fig4-1742271X13493088" position="float"><label>Figure 4</label><caption><p>The most preferred AAA image</p>
<p>AAA: abdominal aortic aneurysm</p></caption><graphic xlink:href="10.1177_1742271X13493088-fig4.tif"/>
</fig>
<fig id="fig5-1742271X13493088" position="float"><label>Figure 5</label><caption><p>The least preferred CRL image</p>
<p>CRL: crown-rump length</p></caption><graphic xlink:href="10.1177_1742271X13493088-fig5.tif"/>
</fig>
<fig id="fig6-1742271X13493088" position="float"><label>Figure 6</label><caption><p>The least preferred AAA image</p>
<p>AAA: abdominal aortic aneurysm</p></caption><graphic xlink:href="10.1177_1742271X13493088-fig6.tif"/>
</fig></p>
<p>Three repeated images for each observer were used to assess whether observers agreed with themselves. It is expected that, whether the national screening guidelines are adhered to or not, observers should rate an image consistently. It was found from the CRL data set that 17% of the time, observers provided a different rating the first and second time they viewed the same image. This was true for 23% of the time in the AAA data set. Therefore, for around a fifth of the repeated images, the observers were not consistent in the rating they gave.</p>
<p>These results were presented on the third day of the conference to the delegates, many of whom had participated in the data collection. A discussion took place afterwards and comments were given on how the most and least preferred images had been selected. The audience feedback generally confirmed previous thoughts that those classed as experts were considering different features of the image to those classed as non-experts. For example, the experts raised issues such as whether the image met all the screening guidelines and whether anatomical landmarks and measurement end-points could be seen. In comparison, the non-experts made comments regarding the contrast, spatial resolution or magnification of the image. General comments were also given that showed delegates in practice undertake a dynamic ultrasound examination to obtain the sections required, then freeze the image rather than use just one static image.</p>
</sec>
<sec id="sec4-1742271X13493088" sec-type="discussion"><title>Discussion</title>
<p>The large number of images regarded as unacceptable supports the audience feedback that the general quality of the images was low, although all images had been regarded as acceptable by the sonographer when they were taken. The image quality overall should not affect the findings of the study, since it was focussed on agreement rather than if the images were of high quality. However, the quality of the image sets may have caused the observers to alter their opinion of ‘acceptable’ part way through the survey.</p>
<p>Recall in the analysis, we considered how important the machine type was, compared to the observer, in determining the survey response. The low variation in responses and hence agreement in the subgroups, caused the machine types to be from 3 to 65 times more important in determining the survey response than the observer. The high variation in responses overall and hence disagreement between subgroups of the observers caused the machine types to be only half as important as the observer in determining the survey response, possibly suggesting that experts are focussing on different areas to non-experts when rating images. It may be that experts are adhering to the national screening guidelines, while non-experts are considering aesthetic features such as contrast and focus.</p>
<p>The images used may not be representative of the machines they were taken from, hence the ordering of the machines was not intended to highlight the ‘best’ and ‘worst’ machines, but to investigate whether there are differences in image quality between machines, or whether machines have similar odds of producing an image which is regarded as acceptable. For this reason, we chose to anonymise the machines after analysis, although the images were randomly selected to minimise bias. We accept that the machine rankings could be more accurate, if a larger sample of images from each machine were used. However, even with the current sample size, there are statistically significant differences between machines in terms of their odds of producing an acceptable image. Our reason for only selecting three images per machine was to encourage delegate participation in the limited conference time. Machine ordering may also have been affected by personal preferences. It is important to note that the preference for one machine in this study does not necessarily imply better performance or adequacy for use in the AAA or CRL screening programmes.</p>
<p>Conclusions could be drawn from the results, as delegate participation was good for each of the data sets and comparison between the data sets was aided by the equal number of delegates in each data set. The findings were also similar in each of the two data sets, suggesting similar issues occur in different areas of ultrasound screening.</p>
<p>The strengths of the study include the involvement of many ultrasound experts under the same conditions. The weaknesses include the relatively small sample size due to time constraints at the conference. It would also have been preferable to have a series of images or a video clip, rather than one static image. There may be other unknown factors contributing to image acceptability, such as the characteristics of the patient. There could also be difficulties in obtaining an acceptable image if the sonographer is unfamiliar with the machine. We may have drawn different conclusions if the survey question had referred to the national guidelines, although the audience feedback session implied that many of the observers had used the national guidelines when assessing the images in the survey.</p>
</sec>
<sec id="sec5-1742271X13493088" sec-type="conclusions"><title>Conclusion</title>
<p>National guidelines for the screening programmes are intended to ensure high repeatability or reproducibility of measures, which in turn should improve agreement between and within observers when gauging if an image is adequate for its intended purpose. However, the large amount of variability in these findings suggests that personal preferences and expertise may still affect judgement. There were similar levels of agreement regarding image adequacy within expert and non-expert observers, but there was poor agreement between the two groups. Machine model was found to be a significant factor in image adequacy. The variation between and within observers in their image preferences should be considered when planning any quality assessment programme.</p>
</sec>
</body>
<back><ack>
<title>Acknowledgements</title>
<p>We would like to thank BMUS for providing a stall at the ‘Ultrasound 2012’ conference and the delegates who participated in the data collection. We would also like to thank Toshiba for donating a raffle prize for participants. We are pleased to acknowledge the support of the FASP and the NAAASP in funding the original studies which allowed the images used to be collected.</p></ack>
<sec id="sec6-1742271X13493088"><title>Declarations</title>
<sec id="sec7-1742271X13493088"><title>Competing interests</title>
<p>JAE is a past president of BMUS (1992–1994) and currently an editorial board member of Ultrasound. SW is currently the deputy editor of Ultrasound and a member of the Scientific &amp; Education committee for BMUS.</p>
</sec>
<sec id="sec8-1742271X13493088"><title>Funding</title>
<p>CK is a PhD student funded by an MRC Capacity Building Studentship.</p>
</sec>
<sec id="sec9-1742271X13493088"><title>Ethical approval</title>
<p>Ethical approval was not required as the research used previously collected, non-identifiable information.</p>
</sec>
<sec id="sec10-1742271X13493088"><title>Guarantor</title>
<p>CK.</p>
</sec>
<sec id="sec11-1742271X13493088"><title>Contributorship</title>
<p>JAE and SW researched literature, obtained ultrasound images, assisted the study design and supervised the study. CK assisted in the study design and carried out the data collection and the data analysis. AD designed and produced the survey used for data collection. CK wrote the first draft of the manuscript. All authors reviewed and edited the manuscript and approved the final version of the manuscript.</p>
</sec>
</sec>
<ref-list>
<title>References</title>
<ref id="bibr1-1742271X13493088"><label>1</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Metcalfe</surname><given-names>SC</given-names></name><name><surname>Evans</surname><given-names>JA</given-names></name></person-group>. <article-title>A study of the relationship between routine ultrasound quality assurance parameters and subjective image assessment</article-title>. <source>Br J Radiol</source> <year>1992</year>; <volume>65</volume>: <fpage>570</fpage>–<lpage>5</lpage>.</citation></ref>
<ref id="bibr2-1742271X13493088"><label>2</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Donofrio</surname><given-names>NM</given-names></name><name><surname>Hanson</surname><given-names>JA</given-names></name><name><surname>Hirsch</surname><given-names>JH</given-names></name><etal/></person-group>. <article-title>Investigating the efficacy of current quality assurance performance tests in diagnostic ultrasound</article-title>. <source>J Clin Ultrasound</source> <year>1984</year>; <volume>12</volume>: <fpage>251</fpage>–<lpage>60</lpage>.</citation></ref>
<ref id="bibr3-1742271X13493088"><label>3</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>EL</given-names></name><name><surname>Frank</surname><given-names>GR</given-names></name><name><surname>Dong</surname><given-names>F</given-names></name></person-group>. <article-title>Liquid or solid ultrasonically tissue-mimicking materials with very low scatter</article-title>. <source>Ultrasound Med Biol</source> <year>1998</year>; <volume>24</volume>: <fpage>535</fpage>–<lpage>42</lpage>.</citation></ref>
<ref id="bibr4-1742271X13493088"><label>4</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Dudley</surname><given-names>NJ</given-names></name><name><surname>Gibson</surname><given-names>NM</given-names></name><name><surname>Fleckney</surname><given-names>MF</given-names></name><etal/></person-group>. <article-title>The effect of speed of sound in ultrasound test objects on lateral resolution</article-title>. <source>Ultrasound Med Biol</source> <year>2002</year>; <volume>28</volume>: <fpage>1561</fpage>–<lpage>4</lpage>.</citation></ref>
<ref id="bibr5-1742271X13493088"><label>5</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Thijssen</surname><given-names>JM</given-names></name><name><surname>van der Wijk</surname><given-names>MC</given-names></name><name><surname>Cuypers</surname><given-names>MHM</given-names></name></person-group>. <article-title>Performance testing of medical echo/doppler equipment</article-title>. <source>Eur J Ultrasound</source> <year>2002</year>; <volume>15</volume>: <fpage>151</fpage>–<lpage>164</lpage>.</citation></ref>
<ref id="bibr6-1742271X13493088"><label>6</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Browne</surname><given-names>JE</given-names></name><name><surname>Ramnarine</surname><given-names>KV</given-names></name><name><surname>Watson</surname><given-names>AJ</given-names></name><etal/></person-group>. <article-title>Assessment of the acoustic properties of common tissue mimicking test phantoms</article-title>. <source>Ultrasound Med Biol</source> <year>2003</year>; <volume>29</volume>: <fpage>1053</fpage>–<lpage>60</lpage>.</citation></ref>
<ref id="bibr7-1742271X13493088"><label>7</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Zagzebski</surname><given-names>JA</given-names></name></person-group>. <article-title>Simulation study of the effects of speed of sound and attenuation on ultrasound lateral resolution</article-title>. <source>Ultrasound Med Biol</source> <year>2004</year>; <volume>30</volume>: <fpage>1297</fpage>–<lpage>306</lpage>.</citation></ref>
<ref id="bibr8-1742271X13493088"><label>8</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Goldstein</surname><given-names>A</given-names></name></person-group>. <article-title>Beam width measurements in low acoustic velocity phantoms</article-title>. <source>Ultrasound Med Biol</source> <year>2004</year>; <volume>30</volume>: <fpage>1297</fpage>–<lpage>306</lpage>.</citation></ref>
<ref id="bibr9-1742271X13493088"><label>9</label><citation citation-type="other"><comment>Fetal Anomaly Screening Programme. See <ext-link ext-link-type="uri" xlink:href="http://fetalanomaly.screening.nhs.uk/">http://fetalanomaly.screening.nhs.uk/</ext-link> (last checked 18 April 2013)</comment>.</citation></ref>
<ref id="bibr10-1742271X13493088"><label>10</label><citation citation-type="other"><comment>National Screening Programme Standard Operating Procedures and Workbook. See <ext-link ext-link-type="uri" xlink:href="http://aaa.screening.nhs.uk">http://aaa.screening.nhs.uk</ext-link> (last checked 18 April 2013)</comment>.</citation></ref>
<ref id="bibr11-1742271X13493088"><label>11</label><citation citation-type="other"><comment>R Development Core Team. <italic>R: A Language and Environment for Statistical Computing</italic>. R Foundation for Statistical Computing, Vienna, Austria, 2012. ISBN 3-900051-07-0. See <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org">http://www.R-project.org</ext-link></comment>.</citation></ref>
</ref-list>
</back>
</article>