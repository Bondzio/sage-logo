<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LAS</journal-id>
<journal-id journal-id-type="hwp">splas</journal-id>
<journal-title>Language and Speech</journal-title>
<issn pub-type="ppub">0023-8309</issn>
<issn pub-type="epub">1756-6053</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0023830911434098</article-id>
<article-id pub-id-type="publisher-id">10.1177_0023830911434098</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Motor Equivalent Strategies in the Production of German /∫/ under Perturbation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Brunner</surname><given-names>Jana</given-names></name>
<aff id="aff1-0023830911434098">Zentrum für Allgemeine Sprachwissenschaft, Germany</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Hoole</surname><given-names>Phil</given-names></name>
<aff id="aff2-0023830911434098">Ludwig-Maximilians-Universität München, Germany</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0023830911434098">Jana Brunner, Universität Potsdam, Karl-Liebknecht-Str. 24-25, 14476 Potsdam, Germany Email: <email>jana.brunner@uni-potsdam.de</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>55</volume>
<issue>4</issue>
<fpage>457</fpage>
<lpage>476</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The German sibilant /∫/ is produced with a constriction in the postalveolar region and often with protruded lips. By covarying horizontal lip and tongue position speakers can keep a similar acoustic output even if the articulation varies. This study investigates whether during two weeks of adaptation to an artificial palate speakers covary these two articulatory parameters, whether tactile landmarks have an influence on the covariation and to what extent speakers can foresee the acoustic result of the covariation without auditory feedback. Six German speakers were recorded with EMA. Four of them showed a covariation of lip and tongue, which is consistent with the motor equivalence hypothesis. The acoustic output, however, does not stay entirely constant but varies with the tongue position. The role of tactile landmarks is negligible. To a certain extent, speakers are able to adapt even without auditory feedback.</p>
</abstract>
<kwd-group>
<kwd>fricative</kwd>
<kwd>motor equivalence</kwd>
<kwd>perturbation</kwd>
<kwd>sibilant</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0023830911434098" sec-type="intro">
<title>1 Introduction</title>
<sec id="section2-0023830911434098">
<title>1.1 Variation in the production of /∫/</title>
<p>Producing /∫/ requires a very precise articulation. At least the following factors are important in articulating this sound:</p>
<list id="list1-0023830911434098" list-type="bullet">
<list-item><p>The tongue has to form a constriction in the postalveolar region (<xref ref-type="bibr" rid="bibr39-0023830911434098">Shadle, 1985</xref>).</p></list-item>
<list-item><p>In some languages such as German the lips may be protruded.</p></list-item>
<list-item><p>The jaw has to be in a rather high position (<xref ref-type="bibr" rid="bibr40-0023830911434098">Shadle, 1990</xref>; <xref ref-type="bibr" rid="bibr32-0023830911434098">Mooshammer, Hoole, &amp; Geumann, 2007</xref>) so that the air jet is directed against the incisors.</p></list-item>
<list-item><p>In contrast to fricatives such as /f/ and /x/ the tongue has to be grooved (e.g., <xref ref-type="bibr" rid="bibr40-0023830911434098">Shadle, 1990</xref>; <xref ref-type="bibr" rid="bibr41-0023830911434098">Shadle, Berezina, Proctor, &amp; Iskarous, 2008</xref>) within the constriction, but also at the more posterior region of the tongue behind the constriction (the “inlet”).</p></list-item></list>
<p>A variation in the third and fourth parameters leads to changes in the higher frequency ranges of the spectrum beyond 4 kHz (<xref ref-type="bibr" rid="bibr40-0023830911434098">Shadle, 1990</xref>; <xref ref-type="bibr" rid="bibr41-0023830911434098">Shadle et al., 2008</xref>). Both a change in lip protrusion and a change in constriction position have an influence on the length of the front cavity (region between constriction and lip opening), which in turn influences the frequency of the spectral peak (usually at about 3 kHz). For a constant constriction position, more lip protrusion lengthens the front cavity and consequently lowers the frequency of the spectral peak. Less lip protrusion makes the cavity shorter and raises the spectral peak. For constant lip positions a more advanced constriction position raises the frequency of the spectral peak and a more retracted constriction position lowers it (<xref ref-type="bibr" rid="bibr39-0023830911434098">Shadle, 1985</xref>).</p>
<p>Speakers can thus influence the length of their front cavity by two parameters: lip protrusion and constriction position. Speakers who have a rather fronted constriction position have to use more lip protrusion than speakers who have a rather retracted constriction position in order to produce similar acoustic results. Since there is a many-to-one relationship between articulation and acoustics (cf. e.g., <xref ref-type="bibr" rid="bibr10-0023830911434098">Guenther, 1994</xref>; <xref ref-type="bibr" rid="bibr22-0023830911434098">Jordan, 1996</xref>; <xref ref-type="bibr" rid="bibr24-0023830911434098">Laboissière, 1992</xref>), even a single speaker can use several articulatory configurations while keeping the acoustic result constant. This study aims at investigating this kind of <italic>motor equivalence</italic> (i.e., different articulations but similar acoustic outputs) in single speakers.</p>
</sec>
<sec id="section3-0023830911434098">
<title>1.2 Motor equivalence</title>
<p>Within the speech field, the term <italic>motor equivalence</italic> is used to denote different articulatory strategies to produce similar acoustic outputs, for example by using different contributions of several articulators to a vocal tract shape or by using different vocal tract shapes resulting in similar acoustic outputs.</p>
<p>A very well-known example for motor equivalence is bite block speech. Bite block studies have shown that, if the jaw movement was blocked the tongue managed to compensate for that and the speaker could still produce nearly normal speech by forming constriction sizes that are similar to the ones formed in unperturbed speech (<xref ref-type="bibr" rid="bibr26-0023830911434098">Lindblom, Lubker, &amp; Gay, 1979</xref>; <xref ref-type="bibr" rid="bibr23-0023830911434098">Kelso &amp; Tuller, 1983</xref>). Some studies, however, found that there were small acoustic differences between the bite block and the normal condition (<xref ref-type="bibr" rid="bibr31-0023830911434098">McFarland &amp; Baum, 1995</xref>) in vowels and consonants.</p>
<p>Other studies suggesting the existence of motor equivalent strategies are the lip perturbation studies by <xref ref-type="bibr" rid="bibr7-0023830911434098">Folkins and Zimmermann (1982)</xref> and <xref ref-type="bibr" rid="bibr9-0023830911434098">Gracco and Abbs (1985)</xref>. <xref ref-type="bibr" rid="bibr7-0023830911434098">Folkins and Zimmermann (1982)</xref> carried out a perturbation experiment where speakers’ lower lips were pushed down unexpectedly by an electrical stimulation. The three speakers investigated in the study showed active compensatory behavior in bilabial stop production by moving the upper lip further down and by moving the jaw up. <xref ref-type="bibr" rid="bibr9-0023830911434098">Gracco and Abbs (1985)</xref> perturbed the movement of the lower lip by pushing it down with a paddle unexpectedly during bilabial stop production. Speakers compensated via an increase in movement amplitude, velocity and movement time of both upper and lower lip in order to produce the closure they intended to produce without the perturbation. In all these studies speakers were able to produce a similar vocal tract shape while the contributions of single articulators (jaw, lips, tongue) varied.</p>
<p>However, motor equivalence has also been reported in unperturbed speech, for example in American English /u/. This sound can be produced for example (1) with protruded lips and a constriction in the velar region, or (2) with open lips and a constriction in the velo-pharyngeal region. <xref ref-type="bibr" rid="bibr35-0023830911434098">Perkell, Matthies, Svirsky, and Jordan (1993)</xref> investigated motor equivalence patterns between lip rounding and tongue body raising and -retraction in American English /u/. Both strategies lead to a lower F2 and could therefore be covaried while keeping the acoustic output constant. This EMA study discussed productions of four speakers each producing /u/ 300 times in different contexts. Three of the four subjects showed weak correlations thereby giving some support to the motor equivalence hypothesis.</p>
<p>Another study investigating different articulations of an /u/-phoneme, this time in French, is the one presented in <xref ref-type="bibr" rid="bibr36-0023830911434098">Savariaux, Perrier, and Orliaguet (1995)</xref> and <xref ref-type="bibr" rid="bibr37-0023830911434098">Savariaux, Perrier, Orliaguet, and Schwartz (1999)</xref>. In this study speakers’ lip movement was perturbed by a 2.5 cm diameter tube which held the lips open. The opening that was enforced by this lip tube is considerably larger than the one observed in unperturbed productions. <xref ref-type="bibr" rid="bibr35-0023830911434098">Perkell et al. (1993)</xref> report a maximal opening of 0.86 cm<sup>2</sup> for their speakers which would, for a perfectly circular lip opening, correspond to a diameter of slightly more than 1 cm. Savariaux et al.’s speakers were asked to produce /u/ in this condition. The nomograms by <xref ref-type="bibr" rid="bibr6-0023830911434098">Fant (1960)</xref> suggest that a way to compensate for this perturbation is to produce the constriction at a more posterior place. In fact, the X-ray data recorded from the French subjects showed that some of the speakers retracted the tongue further than in the unperturbed condition, with one of them producing a constriction in the velo-pharyngeal region resulting in similar formant frequencies of the produced sound as compared to the productions with protruded lips and a velar constriction.</p>
<p><xref ref-type="bibr" rid="bibr33-0023830911434098">Perkell, Guenther, Lane, Matthies, Perrier, Vick, Wilhelms-Tricarico, and Zandipour (2000)</xref> investigated motor equivalence in American English /∫/. The acoustic characteristics of this sound are determined by the length of the front cavity (from the constriction to the lip opening). This length of the front cavity, and thereby the acoustic output, can be kept similar by covarying lip protrusion and constriction position. Motor equivalence for lip rounding vs. tongue tip fronting was investigated for eight speakers. The results were mixed in that some speakers showed motor equivalence (i.e., covariation between the two gestures) but others did not.</p>
<p>Another very well-known case of motor equivalence is found in the various articulations of American English /r/. This sound can be produced by several markedly different articulatory configurations, namely with a bunched tongue or with a retroflexed tongue (<xref ref-type="bibr" rid="bibr4-0023830911434098">Delattre &amp; Freeman, 1968</xref>). In both cases a constriction is created which leads to a low F3, a very salient acoustic characteristic of this sound. <xref ref-type="bibr" rid="bibr46-0023830911434098">Westbury, Hashi, and Lindstrom (1998)</xref> found for a sample of 53 speakers that the two production types, bunched and retroflex, rather form the edges of a continuum and productions are often somewhere between the two extremes. The production type furthermore depends not only on the speaker but also on the context. <xref ref-type="bibr" rid="bibr48-0023830911434098">Zhou, Espy-Wilson, Tiede, and Boyce (2007)</xref> and <xref ref-type="bibr" rid="bibr49-0023830911434098">Zhou, Espy-Wilson, Boyce, Tiede, Holland, and Choe (2008)</xref> found small acoustic differences between the two production types. The retroflexed variant has a larger difference between F4 and F5 than the bunched variant. The authors explain this with the fact that the resonances come from different cavities in the two production types. This finding shows that the vocal tract shapes of the two sounds indeed differ, although the acoustic output, at least for the lower formants, is similar.</p>
<p>There is some evidence that motor equivalence is a phenomenon that is language specific. <xref ref-type="bibr" rid="bibr25-0023830911434098">Lawson, Scobbie, and Stuart-Smith (2011)</xref> investigated the /r/-productions of working class and middle class Scottish speakers and found that working class speakers tended to use variants with the tongue tip up or the tongue front up whereas middle class speakers tended to have bunched variants. Furthermore, acoustic differences were found: middle class speakers produced audibly rhotic variants of the sounds whereas the productions of working class speakers were often r-less or weakly rhotic. The authors note that the acoustic difference is merely due to a different timing (raising gesture delayed beyond the offset of voicing for the working class speakers), however, the results suggest that for these Scottish speakers the two articulations are not equivalent since they are used in sociolectal variation.</p>
<p>It is also possible to see a link between motor equivalence and cue trading.<sup><xref ref-type="fn" rid="fn1-0023830911434098">1</xref></sup> In trading relations also, different articulations can lead to a similar <italic>perceptual</italic> result. <xref ref-type="bibr" rid="bibr5-0023830911434098">Diehl and Kingston (1991)</xref>, for example, show that [+voice] judgments in stops increase not only for longer voicing durations but also for low F1 and low F0 of the following vowel. Diehl and Kingston therefore suggest that these three parameters are merged into the perception of only one characteristic.<sup><xref ref-type="fn" rid="fn2-0023830911434098">2</xref></sup> So, maybe the concept of motor equivalence should be broadened to produce not only something acoustically similar but rather something perceptually similar.</p>
<p>One could wonder why speakers should use motor equivalent strategies if they could just as well use a single articulatory strategy. There are several possibilities. One possibility is reduction of articulatory effort. In some contexts it might be easier to use bunched /r/ than in others, where the production of retroflex /r/ might involve less effort (cf. discussion in <xref ref-type="bibr" rid="bibr1-0023830911434098">Brunner, Ghosh, Hoole, Matthies, Tiede, &amp; Perkell, 2011a</xref>). A similar example would be utterances of the same sound in the same context but with differences in emphasis on the word (Scobbie, personal communication). A syllable bearing emphasis can be expected to have a slightly different articulation than the same syllable not bearing emphasis. As a result of this the production of a particular sound in that syllable might be easier using a different articulatory strategy in the emphasis case than in the no-emphasis case.</p>
<p>In perturbed speech motor equivalent strategies are often used because the speakers are blocked from using their usual strategies (e.g., bite block). Another reason for using motor equivalence strategies could be to find either the most efficient articulatory strategy or the one giving the best acoustic result by trying out several strategies, for example when adapting to a dental device.</p>
<p>What seems to be clear is that speakers only use motor equivalent strategies if there is some change in some domain (e.g., context, emphasis, perturbation). If speakers produce the same utterance in the same situation over and over again they will use the same articulatory strategy over and over again. Consequently, in all motor equivalence studies speakers were brought to produce a lot of articulatory variability, for example by perturbing their speech (e.g., <xref ref-type="bibr" rid="bibr36-0023830911434098">Savariaux et al., 1995</xref>) or by producing sounds in different contexts (e.g., <xref ref-type="bibr" rid="bibr46-0023830911434098">Westbury et al., 1998</xref>). Inducing sources of variability is essential for these studies since, if speakers are producing the same sound several times under exactly the same conditions articulatory strategies will be so similar that motor equivalence cannot be observed.</p>
<p>The present study uses a perturbation in order to increase articulatory variability and thus the use of motor equivalent strategies. If speech is perturbed speakers have to vary their articulation to adapt. During this process they are likely to try out different articulatory strategies in order to find an efficient way to produce the sound under perturbation.</p>
</sec>
<sec id="section4-0023830911434098">
<title>1.3 Auditory and tactile feedback in normal and perturbed speech</title>
<p>A number of studies have shown that during speech acquisition, but also when adapting to a perturbation speakers use auditory and tactile feedback. Within the framework of their speech production model DIVA <xref ref-type="bibr" rid="bibr13-0023830911434098">Guenther, Hampson, and Johnson (1998)</xref> and Guenther (<xref ref-type="bibr" rid="bibr11-0023830911434098">1995</xref>, <xref ref-type="bibr" rid="bibr12-0023830911434098">2003</xref>) propose that during speech learning speakers set up various mappings between orosensory, articulatory and auditory characteristics of sounds. Once set up these mappings can be used in feedforward control to find an articulatory configuration for a desired acoustic output.</p>
<p>Within the framework of this model, as long as the vocal tract does not change, it is not necessary to have auditory or tactile feedback available in order to speak. This assumption is in agreement with the observation that postlingually deaf speakers are able to maintain intelligible speech for years (e.g., <xref ref-type="bibr" rid="bibr44-0023830911434098">Waldstein, 1990</xref>), possibly due to the mapping between orosensory targets onto speech sounds. Further support regarding the role of tactile feedback after speech learning comes from <xref ref-type="bibr" rid="bibr27-0023830911434098">Linke (1980)</xref> who discusses a speaker suffering from loss of oral sensibility. Although this subject had problems controlling many actions involving the oral cavity (for example eating and smoking), he did not experience problems during speech, even with auditory feedback masked, possibly due to the mapping from articulatory actions to acoustic outputs. However, when his speech was disturbed (stimulation of the M. orbicularis oris), he had problems adapting. Similarly, <xref ref-type="bibr" rid="bibr18-0023830911434098">Hoole (1987)</xref> discusses adaptation to a bite block by a speaker suffering from loss of oral sensibility with and without auditory feedback available. This speaker did not have problems speaking without perturbation. In the absence of auditory feedback the speaker was not able to compensate sufficiently although he showed clear compensatory behavior when auditory feedback was available.</p>
<p>The examples show that in unperturbed speech feedback is not necessary. However, when speech is perturbed feedback becomes essential in order to carry out remappings between orosensory, articulatory and auditory information. An example for such a reset using auditory feedback is described in a study by <xref ref-type="bibr" rid="bibr21-0023830911434098">Jones and Munhall (2003)</xref>. Jones and Munhall extended the upper incisors of speakers and asked them to produce /s/, at first with auditory feedback masked and afterwards with auditory feedback available. The extension increased the size of the front cavity (from the constriction to the mouth opening). Acoustically, this resulted in a lower center of gravity. As long as there was no auditory feedback available the speakers did not adapt and possibly used their learned orosensory-auditory mappings. When auditory feedback became available speakers adapted the tongue position so that the acoustic output became more similar to that of the unperturbed condition.</p>
<p><xref ref-type="bibr" rid="bibr16-0023830911434098">Honda, Fukino, and Kaburagi (2002)</xref> also investigated the role of auditory feedback in adaptation. In this study speech was perturbed by a palatal prosthesis with an inflatable balloon at the alveolar ridge. The speakers were asked to produce the sounds /∫a/ and /t∫a/. On some trials the balloon was blown up. Auditory feedback was temporally masked. Identification scores in the subsequent perception experiment were in general high except for the first trial after inflation. The productions without auditory feedback were more often misidentified than the productions with auditory feedback available, showing that auditory feedback is used in adaptation.</p>
<p>In a follow-up study (<xref ref-type="bibr" rid="bibr17-0023830911434098">Honda &amp; Murano, 2003</xref>) adaptation to the inflatable palate was investigated in four different feedback conditions, (1) with tactile and auditory feedback available, (2) with auditory feedback masked, (3) with tactile feedback masked via an anesthetic, and (4) with both auditory and tactile feedback masked. The results show that speakers adapt best with both kinds of feedback available and worst with both kinds of feedback masked. When only one kind of feedback was available the error scores for the perturbed trials were comparable. These results suggest that both kinds of feedback are used in adaptation.</p>
</sec>
<sec id="section5-0023830911434098">
<title>1.4 Aims of the study and hypotheses</title>
<p>Whereas some studies have found that some speakers use motor equivalent strategies in /r/ and /u/, so far no clear results have been found for /∫/. A possible reason for that could be that /∫/ has a very stable articulation due to the great amount of linguo-palatal contact. This might reduce the articulatory variability in this sound so that motor equivalence is simply not found because there is not enough variability. If the production of /∫/ was perturbed this should lead to a change in articulation so that motor equivalence could possibly be observed. The primary aim of this study was to investigate the use of motor equivalence strategies in adaptation. In order to do this, speakers’ speech was perturbed by a palatal prosthesis. The first hypothesis, related to this first aim, is:</p>
<list id="list2-0023830911434098" list-type="bullet">
<list-item><p><italic>Motor equivalence hypothesis</italic>. If speakers use motor equivalence strategies there should be a positive correlation between the horizontal position of the tongue tip and the lip position: the more retracted the tongue, the less lip protrusion. Furthermore, there should be no correlation between an acoustic parameter such as the center of gravity and either one of the articulatory parameters because the aim of motor equivalence strategies is to keep the acoustic output constant. Alternatively, if there were no motor equivalence strategies but just random variation in articulation, each articulatory parameter should correlate with the acoustic parameter, and the two articulatory parameters should not correlate.</p></list-item></list>
<p>The second aim of the study was to investigate whether a certain kind of palate shape has an influence on the amount of articulatory variability (and possibly motor equivalence) observed in perturbed speech. Some studies have shown that tactile feedback is used in adaptation (cf. e.g., the sessions with auditory feedback masking in <xref ref-type="bibr" rid="bibr16-0023830911434098">Honda et al., 2002</xref>; <xref ref-type="bibr" rid="bibr17-0023830911434098">Honda &amp; Murano, 2003</xref>). Related to that, it is possible that speakers use certain landmarks such as a pronounced alveolar ridge as points of orientation for the tongue so that the articulatory variability (and possibly motor equivalence) is reduced. In contrast to that, it is possible that a very flat palate increases the articulatory variability because there is no landmark and the tongue can slide along the palate. In order to investigate this question two different artificial palates were used, one with a pronounced alveolar ridge (“alveolar palate”) and a flat one (“central palate”). Related to this aim the second hypothesis is set up:</p>
<list id="list3-0023830911434098" list-type="bullet">
<list-item><p><italic>Articulatory landmark hypothesis</italic>. When speech is perturbed with a palatal prosthesis, speakers will probably lower their tongue in order to prevent a closure. Furthermore, since the prosthesis changes not only the palatal height but also the palatal contour, they should try to find a new place of articulation for /∫/. While doing this speakers should make use of auditory feedback, but they could also use tactile feedback of articulatory landmarks, such as the alveolar ridge. Following on from this, speakers with an alveolar prosthesis (with a pronounced alveolar ridge), should produce /∫/ at a certain point behind the alveolar ridge. In contrast to that, speakers with a central palate should find it more difficult to find the new place of articulation because there is no landmark. As a result of this, within the perturbed productions, there should be more variability in tongue position (and, provided that the motor equivalence hypothesis is correct, more motor equivalence) for the speakers with a central palate as opposed to the speakers with an alveolar palate.</p></list-item>
</list>
<p>The third aim of the study was to investigate how far speakers can adapt to an artificial palate while using tactile feedback only. <xref ref-type="bibr" rid="bibr16-0023830911434098">Honda et al. (2002)</xref> suggest that adaptation with tactile feedback only is possible to some extent. In the present study this will be further investigated. We are interested in the question of whether speakers, if they adapt without auditory feedback, use a motor equivalent strategy. Therefore, speakers’ auditory feedback was masked at perturbation onset. Related to this aim the following hypothesis was set up:</p>
<list id="list4-0023830911434098" list-type="bullet">
<list-item><p><italic>Auditory feedback hypothesis</italic>. If auditory feedback is absolutely essential in adaptation speakers should not be able to adapt when their auditory feedback is masked. The acoustic result of their productions should differ from the productions with auditory feedback available. Furthermore, the productions without auditory feedback should not “fit in” with the covariation of tongue and lip (if there is such covariation). Alternatively, if auditory feedback is not essential in adaptation there should be no clear acoustic difference in the productions with auditory feedback masked and auditory feedback available and the productions without auditory feedback should fit in with the covariation between tongue and lip.</p></list-item>
</list>
</sec></sec>
<sec id="section6-0023830911434098" sec-type="methods">
<title>2 Methods</title>
<sec id="section7-0023830911434098">
<title>2.1 Speakers</title>
<p>Six volunteers took part in the study, two males and four females. They were between 25 and 40 years old. None of the speakers reported any history of speech or hearing problems, although hearing was not screened prior to the experiment. All speakers spoke Standard German with some regional influence. All the subjects reported in this study had worn dental devices for 1–3 years.<sup><xref ref-type="fn" rid="fn3-0023830911434098">3</xref></sup> For each person a dental cast was made by a dental technician in order to be able to exclude speakers with malocclusions and to make sure that the speakers had a rather domed palate so that there was enough space for the insertion of the prosthesis. As a result of this procedure, one speaker was excluded because her palate was too flat, and another speaker was excluded because of malocclusion and a very small palate.</p>
</sec>
<sec id="section8-0023830911434098">
<title>2.2 Perturbation</title>
<p>A two-week perturbation experiment was carried out. The speakers’ articulation was perturbed by a palatal prosthesis. There were two types of custom-made prostheses, the first one moved the alveolar ridge posteriorly (“alveolar palate”), the second one made the palate flatter and lower by filling out the palatal arch (“central palate”). The palates had a maximal thickness of 1 cm. They were thus considerably thicker than a standard EPG palate. An example of each palate type is shown in <xref ref-type="fig" rid="fig1-0023830911434098">Figure 1</xref>. The plots show the natural palatal contour (bold solid line) and the artificial palate (bold dashed line). The tongue contour of one production of /∫/ is shown as a linear interpolation between three EMA sensors on the tongue tip, tongue dorsum and tongue back (see “Experimental setup”). The position of the upper lip sensor is shown as well.</p>
<fig id="fig1-0023830911434098" position="float">
<label>Figure 1.</label>
<caption>
<p>Examples for an alveolar prosthesis (left subplot) and a central prosthesis (right subplot), midsagittal view. Front is left. Bold solid line: natural palatal contour. Bold dashed line: artificial palatal contour. Thin solid line: estimation of unperturbed tongue contour (linear interpolation between sensor positions) during a production of /∫/. Thin dashed line: estimation of tongue contour under perturbation when tongue is lowered without further adaptation. Asterisks: sensor positions.</p>
</caption>
<graphic xlink:href="10.1177_0023830911434098-fig1.tif"/></fig>
<p>The prosthesis in each case lowers the palate so that one can expect a closure during /∫/ if speakers do not adapt. If speakers compensate just by lowering the tongue, this will, assuming a rotational movement of the jaw, lead to a postlaminal constriction (thin dashed lines in <xref ref-type="fig" rid="fig1-0023830911434098">Figure 1</xref>). The change will be much more severe for speakers with the alveolar palate than for the ones with the central palate. Since it is difficult to form the medial groove postlaminally one can expect the speakers to adapt further, for example by a retraction of the tongue. This will lead to a change in the length of the front cavity which has to be compensated for by less lip protrusion.</p>
<p>If the speakers with the alveolar prosthesis have found a way to produce /∫/ with the prosthesis, it should be easy for them to remember the tongue position and use it over and over again because of the alveolar ridge, which can be used as a point of orientation. Thus, for speakers with an alveolar palate the strategy should change at perturbation onset and perturbation offset, but stay rather constant in between.</p>
<p>For the speakers with a central palate there should be less retraction of the tongue because the changes are less severe. However, since for these speakers the palate is completely flat, it should be difficult to keep a constant place of articulation since the tactile feedback is similar over a wide range of articulatory positions. In contrast to the alveolar palate the central palate might therefore cause articulation to vary over the time of adaptation.</p>
<p>Three speakers (AM1, AM2 and AF1) were recorded with an alveolar prosthesis and three (CF1, CF2 and CF3) with a central prosthesis. Allocation of the speakers to one of these groups was done according to the shape of their natural palates. Speakers without a pronounced alveolar ridge were provided with an alveolar palate, the others were provided with a central palate. This procedure unfortunately led to a biased gender distribution. The central palate group consisted of females only, whereas there were two men in the alveolar palate group. Speakers were asked to wear the prosthesis all day for two weeks and to make a serious effort to improve their speech.</p>
</sec>
<sec id="section9-0023830911434098">
<title>2.3 Experimental setup</title>
<p>Speakers’ articulator movements were recorded via electromagnetic articulography. Sensors were placed midsagittally, one at approximately 1 cm behind the tongue tip (<italic>tongue tip sensor</italic>), one as much retracted as possible (<italic>tongue back sensor</italic>) and one in between the two (<italic>tongue mid sensor</italic>), one below the lower incisors (<italic>jaw sensor</italic>) and one on each lip. Reference sensors were placed on the bridge of the nose, and above the upper incisors. After the recordings, the articulatory data were corrected for head movements and rotated to the occlusal plane. For the present purpose the data of the tongue tip and the upper lip sensor were analyzed. The upper lip was analyzed rather than the lower lip in order to avoid the difficulties involved in decomposing the lower-lip sensor signal into its jaw-related and intrinsic lip components (cf. <xref ref-type="bibr" rid="bibr47-0023830911434098">Westbury, Lindstrom, &amp; McClean, 2002</xref>).</p>
<p>Acoustic recordings were carried out with a DAT recorder and a Sennheiser MKH 20 P48 microphone. The distance between the microphone and the speaker’s lips was about 30 cm. The acoustic signal was downsampled to 24 kHz.</p>
<p>The experiment consisted of several recording sessions which are summarized in <xref ref-type="table" rid="table1-0023830911434098">Table 1</xref>. On the first day of the experiment three different sessions were recorded. First, speakers were recorded without the prosthesis (session <italic>1np</italic>, meaning “first session, no perturbation”). In the second session, the artificial palate was inserted and speakers’ auditory feedback was masked with white noise (session <italic>2wp</italic>, “second session, white noise – perturbed”). In the third session speakers were recorded with auditory feedback available (session <italic>3pe</italic>, “third session, perturbed”). The subjects were instructed to wear the prosthesis all day and practice speaking. They were asked to read aloud an exercise sheet once a day and to write down the number of hours they had worn the prosthesis each day. All speakers reported to have worn the prosthesis between 12 and 18 hours per day. After one week adaptation time speakers were recorded with the prosthesis in place (session <italic>4pe</italic>, “fourth session, perturbed”). A final perturbed session was recorded after two weeks (session <italic>5pe</italic>, “fifth session, perturbed”). Then speakers removed the prosthesis and were recorded without the perturbation (session <italic>6np</italic>, “sixth session, no perturbation”).</p>
<table-wrap id="table1-0023830911434098" position="float">
<label>Table 1.</label>
<caption>
<p>Recording sessions.</p>
</caption>
<graphic alternate-form-of="table1-0023830911434098" xlink:href="10.1177_0023830911434098-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Recording name</th>
<th align="left">Adaptation time</th>
<th align="left">Auditory feedback masking</th>
<th align="left">Perturbation</th>
</tr>
</thead>
<tbody>
<tr>
<td>1np (“no perturbation”)</td>
<td>0</td>
<td>no</td>
<td>no</td>
</tr>
<tr>
<td>2wp (“white noise, perturbed”)</td>
<td>0</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr>
<td>3pe (“perturbed”)</td>
<td>duration of session 2wp</td>
<td>no</td>
<td>yes</td>
</tr>
<tr>
<td>4pe (“perturbed”)</td>
<td>1 week</td>
<td>no</td>
<td>yes</td>
</tr>
<tr>
<td>5pe (“perturbed”)</td>
<td>2 weeks</td>
<td>no</td>
<td>yes</td>
</tr>
<tr>
<td>6np (“no perturbation”)</td>
<td>2 weeks</td>
<td>no</td>
<td>no</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section10-0023830911434098">
<title>2.4 Speech material</title>
<p>The target sound /∫/ was recorded in the nonsense word /’∫axa/ spoken in a carrier phrase: <italic>Ich sah Schacha an</italic> (“I looked at /’∫axa/”). There were 20 repetitions in each session, randomized with 10 other CVCV sequences including all lingual sounds of German, each repeated 20 times in the same carrier phrase giving 220 sentences per session and a total of 120 repetitions of /’∫axa/ per speaker for the six sessions. The recording time of each of the six sessions was about 20 minutes.</p>
</sec>
<sec id="section11-0023830911434098">
<title>2.5 Auditory feedback masking</title>
<p>In order to investigate how speakers adapt when no auditory feedback is available their auditory feedback was masked in the first perturbed session with white noise (100 Hz–10 kHz) presented over headphones.</p>
</sec>
<sec id="section12-0023830911434098">
<title>2.6 Acoustic analysis</title>
<sec id="section13-0023830911434098">
<title>2.6.1 Segmentation</title>
<p>The consonant /∫/ was acoustically segmented (friction onset to friction offset) in each utterance. The segmentation was carried out in PRAAT. All further analyses, articulatory and acoustic, were carried out at the temporal midpoint of the segments.</p>
</sec>
<sec id="section14-0023830911434098">
<title>2.6.2 Band-pass filtering</title>
<p>As has been shown in previous palate perturbation studies, in fricatives speakers adapt not only the position of the constriction (influencing the frequency of the spectral peak) but also the tongue shape (e.g., <xref ref-type="bibr" rid="bibr15-0023830911434098">Hamlet &amp; Stone, 1978</xref>; <xref ref-type="bibr" rid="bibr2-0023830911434098">Brunner, Hoole, &amp; Perrier, 2011b</xref> for /s/). This influences the amplitudes of the higher frequencies (<xref ref-type="bibr" rid="bibr41-0023830911434098">Shadle et al., 2008</xref>). Visual analyses of our spectra confirmed this. During session 2wp there was less energy in the higher frequencies than in the unperturbed session. For both prosthesis types, only with time did speakers manage to produce tongue shapes which resulted in the production of high frequency noise. Furthermore, although /∫/ is phonologically unvoiced, for some speakers there were remnants of voicing. Both the changes in high frequency energy and the voicing would have been problematic for the calculation of spectral parameters since we were primarily interested in the frequency of the main spectral peak at around 3 kHz because this is the acoustic characteristic which is influenced by the position of the constriction and lip protrusion.</p>
<p>In order to get information about the frequency of this peak without much influence of other characteristics of the spectrum, the data were band-pass filtered with cut-off frequencies of 700 Hz and 6 kHz. The lower cut-off frequency was chosen so that voicing was excluded from the analysis. The higher cut-off frequency was chosen so that the influence of tongue shape changes during adaptation was excluded from the analysis.</p>
</sec>
<sec id="section15-0023830911434098">
<title>2.6.3 Calculation of spectral parameters</title>
<p>As shown in a modeling study by <xref ref-type="bibr" rid="bibr39-0023830911434098">Shadle (1985)</xref> a lengthening of the front cavity leads to a downwards shift of the main concentration of energy of the spectrum. This lengthening can be reached either by retracting the constriction or by using more lip protrusion. Three parameters which have been used for the description of shifts in the main concentration of spectral energy before, that is, center of gravity, skewness and the second coefficient of a discrete cosine transform (<xref ref-type="bibr" rid="bibr45-0023830911434098">Watson &amp; Harrington, 1999</xref>; <xref ref-type="bibr" rid="bibr14-0023830911434098">Guzik &amp; Harrington, 2007</xref>) have been tested for their suitability to describe the relation between acoustics and a change in front cavity length (cf. Appendix). For a longer front cavity, the center of gravity should become lower whereas the skewness value and the DCT-coefficient 2 should become higher. Similarly, for a shorter front cavity due to either less lip protrusion or a more advanced constriction position, leading to an upwards shift of the main energy concentration in the spectrum, the center of gravity should become higher, whereas the skewness and DCT-coefficient 2 should become lower. As is shown in the Appendix, the DCT-coefficient 2 described this relation best and was therefore selected for the following analyses.</p>
</sec></sec>
<sec id="section16-0023830911434098">
<title>2.7 Articulatory analysis</title>
<sec id="section17-0023830911434098">
<title>2.7.1 Data exclusion</title>
<p>On each recording day the sensors had to be glued to the tongue anew. We tried to position the sensor at exactly the same location in each recording session. To do so, photos of the tongue were taken for each session and anatomical landmarks on the tongue were noted in order to be able to position the sensor at the same place in the following sessions. In order to be able to judge the comparability of the sensor positions the speaker was asked in each session to put his/her tongue against the palate in rest position. Comparisons of the photos and the rest position recordings showed that in session <italic>4pe</italic> of speakers AM2 and CF2 the tongue tip sensor had been glued to a slightly different location. These sessions were removed from the data. Unfortunately, it is not possible to completely exclude the possibility that there were small differences in sensor positioning in the other sessions as well, but they should be in the range of a millimeter maximum. Also, the first three sessions and the last two were recorded with the same sensor gluing. Comparisons of positions with different and same gluings showed no greater differences in sensor positions for sessions with different gluings than for sessions with the same gluing.</p>
<p>For the sessions of the first day of speaker CF1 no data were available because of technical problems with the upper lip sensor. Also, some of the acoustic measurements for speakers AM1, CF1 and CF3 resulted in outliers (below or beyond the mean ±2 standard deviations). These data were removed as well.</p>
</sec>
<sec id="section18-0023830911434098">
<title>2.7.2 Lip position</title>
<p>Lip protrusion was estimated as the horizontal position of the upper lip sensor. As a consequence of the experimental arrangement lower values mean more lip protrusion.</p>
</sec>
<sec id="section19-0023830911434098">
<title>2.7.3 Tongue tip position</title>
<p>The constriction position was estimated as the horizontal position of the tongue tip sensor. The higher this value, the more retracted is the tongue.</p>
</sec></sec>
<sec id="section20-0023830911434098">
<title>2.8 Relationships between articulatory and acoustic parameters</title>
<p>According to the motor equivalence hypothesis, speakers should covary the horizontal upper lip position and the horizontal tongue tip position. Therefore, Pearson correlations were calculated for these two articulatory parameters, pooling data from all sessions.</p>
<p>Furthermore, correlations between the acoustic parameter DCT-coefficient 2 and each of the articulatory parameters were calculated. According to the motor equivalence hypothesis, there should be no correlations between the acoustic parameter and any of the articulatory parameters. Statistical analyses were carried out in R.</p>
</sec></sec>
<sec id="section21-0023830911434098" sec-type="results">
<title>3 Results</title>
<p>At the beginning of this section the measurement results are presented briefly. Afterwards they are discussed in relation to the three hypotheses.</p>
<sec id="section22-0023830911434098">
<title>3.1 Relationship between tongue tip position and lip position</title>
<p><xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref> shows the relations between the two articulatory parameters (horizontal tongue tip position and horizontal upper lip position). Each subplot shows the results for one speaker. The speakers with an alveolar palate are shown on the left, the speakers with a central palate are shown on the right. The numbers in the plots refer to single productions and give the session in which these productions were recorded (1: 1np, 2: 2wp …). Numbers are given in different grey shades in order to make sessions visually better distinguishable. Regression lines are also given. In each subplot lip position is shown on the abscissa and tongue position is given on the ordinate. Low values on either axis mean that an articulator was at a more anterior position, high values mean that it was at a more posterior position. <xref ref-type="table" rid="table2-0023830911434098">Table 2</xref> (second column) gives the correlation coefficients and <italic>p</italic>-values for the correlations between tongue tip position and lip position. A positive correlation between tongue tip position and lip position would be in agreement with the motor equivalence hypothesis since protruding or retracting both tongue and lip at the same time would keep the front cavity about the same size.</p>
<fig id="fig2-0023830911434098" position="float">
<label>Figure 2.</label>
<caption>
<p>Lip position versus tongue position. If speakers use motor equivalent strategies there should be a positive correlation. Each subplot shows the results for one speaker. The abscissa gives results for horizontal upper lip position in cm, the ordinate gives results of the horizontal tongue tip position in cm. Lower values refer to more advanced positions. Numbers in different grey shades refer to the session in which the production was recorded (1: session 1np, no perturbation, no masking, 2: session 2wp, perturbed, with masking, 3: session 3pe, perturbed, no masking, 4: session 4pe, perturbed, no masking, 5: session 5pe, perturbed, no masking, 6: session 6np, no perturbation, no masking). Straight lines show regression.</p>
</caption>
<graphic xlink:href="10.1177_0023830911434098-fig2.tif"/></fig>
<table-wrap id="table2-0023830911434098" position="float">
<label>Table 2.</label>
<caption>
<p>Correlation between articulatory and acoustic parameters.</p>
</caption>
<graphic alternate-form-of="table2-0023830911434098" xlink:href="10.1177_0023830911434098-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Speaker</th>
<th align="left">tongue – lip</th>
<th align="left">lip – coeff2</th>
<th align="left">tongue – coeff2</th>
<th align="left"><italic>N</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>AM1</td>
<td>−0.40 (.000)</td>
<td>−0.509 (.000)</td>
<td>0.386 (.000)</td>
<td>119</td>
</tr>
<tr>
<td>AM2</td>
<td>−0.08 (.439)</td>
<td>−0.200 (.046)</td>
<td>0.648 (.000)</td>
<td>100</td>
</tr>
<tr>
<td>AF1</td>
<td>0.27 (.003)</td>
<td>0.143 (.121)</td>
<td>0.821 (.000)</td>
<td>120</td>
</tr>
<tr>
<td>CF1</td>
<td>0.69 (.000)</td>
<td>−0.090 (.505)</td>
<td>0.181 (.174)</td>
<td>58</td>
</tr>
<tr>
<td>CF2</td>
<td>0.83 (.000)</td>
<td>0.336 (.001)</td>
<td>0.682 (.000)</td>
<td>100</td>
</tr>
<tr>
<td>CF3</td>
<td>0.74 (.000)</td>
<td>0.620 (.000)</td>
<td>0.787 (.000)</td>
<td>118</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0023830911434098"><p>First column: speaker, second column: correlation coefficients and <italic>p</italic>-values for relation between tongue position and lip position, third column: relation between lip position and coefficient 2, fourth column: relation between tongue position and coefficient 2, last column: number of repetitions taken into account.</p></fn>
</table-wrap-foot></table-wrap>
</sec>
<sec id="section23-0023830911434098">
<title>3.2 Relationship between lip and DCT-coefficient 2</title>
<p><xref ref-type="fig" rid="fig3-0023830911434098">Figure 3</xref> shows the lip position on the abscissa and the DCT-coefficient 2 on the ordinate. <xref ref-type="table" rid="table2-0023830911434098">Table 2</xref> (third column) gives the results for the correlation between lip position and coefficient 2. No correlation between these two parameters would be in line with the motor equivalence hypothesis. A negative correlation would mean that there was more energy in the lower frequencies if there was more lip protrusion (and thus a longer front cavity). This would suggest that there is no motor equivalence. A positive correlation would mean that there was more energy in the lower frequencies if there was less protrusion. This is only possible if, at the same time, the tongue was retracted.</p>
<fig id="fig3-0023830911434098" position="float">
<label>Figure 3.</label>
<caption>
<p>As <xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref> but for lip position vs. DCT-coefficient 2.</p>
</caption>
<graphic xlink:href="10.1177_0023830911434098-fig3.tif"/></fig>
</sec>
<sec id="section24-0023830911434098">
<title>3.3 Relationship between tongue position and DCT-coefficient 2</title>
<p><xref ref-type="fig" rid="fig4-0023830911434098">Figure 4</xref> and the fourth column in <xref ref-type="table" rid="table2-0023830911434098">Table 2</xref> give the results of the correlation between tongue position and DCT-coefficient 2. No correlation between the two parameters suggests that there was motor equivalence. A positive correlation means that there was more energy in the lower frequencies when the tongue was retracted and the lip did not compensate (sufficiently) for it. A negative correlation would mean that there was more energy in the lower frequencies when the tongue was more advanced. This is only possible if the lip overcompensates.</p>
<fig id="fig4-0023830911434098" position="float">
<label>Figure 4.</label>
<caption>
<p>As <xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref> but for tongue tip position vs. DCT-coefficient 2.</p>
</caption>
<graphic xlink:href="10.1177_0023830911434098-fig4.tif"/></fig>
</sec>
<sec id="section25-0023830911434098">
<title>3.4 Motor equivalence hypothesis</title>
<p>According to the motor equivalence hypothesis there should be a positive correlation between tongue tip position and lip position. Furthermore, there should be no correlation between either one of the articulatory parameters and DCT-coefficient 2. As can be seen in <xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref> and <xref ref-type="table" rid="table2-0023830911434098">Table 2</xref> (second column), there is a significant positive correlation between tongue tip position and lip position for four speakers (AF1, CF1, CF2 and CF3), that is, the tongue was more advanced when the lip was protruded and more retracted when the lip was retracted. Speaker AM1 has a significant negative correlation, speaker AM2 has a non-significant negative correlation, suggesting that these two speakers do not use motor equivalence strategies in adaption.</p>
<p>The second part of the motor equivalence hypothesis concerns the relation between the articulatory parameters tongue tip position and lip position and the acoustic parameter DCT-coefficient 2. According to the hypothesis, if speakers are using motor equivalence strategies, the acoustic output should stay similar. Thus, there should be no correlation between either one of the articulatory parameters and DCT-coefficient 2. Speaker CF1, who had a positive correlation between tongue and lip position, has no correlation between articulatory and acoustic parameters (cf. <xref ref-type="table" rid="table2-0023830911434098">Table 2</xref>, columns 3 and 4, <xref ref-type="fig" rid="fig3-0023830911434098">Figures 3</xref> and <xref ref-type="fig" rid="fig4-0023830911434098">4</xref>), which is perfectly in agreement with the motor equivalence hypothesis. Two of the speakers with a positive correlation between tongue position and lip position, CF2 and CF3, have a significant positive correlation between lip position and coefficient 2 meaning that there was more energy in the higher frequency bands when there was more lip protrusion (i.e., a longer front cavity). This is only possible with a positive correlation between tongue tip and DCT-coefficient 2, which in fact, was found (cf. <xref ref-type="table" rid="table2-0023830911434098">Table 2</xref>, fourth column, <xref ref-type="fig" rid="fig4-0023830911434098">Figure 4</xref>). These two speakers, CF2 and CF3 thus have insufficient adaptation via the lip for their great variability in tongue position.</p>
<p>The fourth speaker with a positive correlation between lip and tongue, AF1, does not have a significant correlation between lip position and coefficient 2, but a positive correlation between tongue position and coefficient 2. Similar to speakers CF2 and CF3, this speaker thus also has more variability in tongue than in lip position, but to a lesser degree than speakers CF2 and CF3.</p>
<p>The remaining two speakers, AM1 and AM2, who do not have positive correlations between tongue and lip, have negative correlations between lip and coefficient 2 and positive correlations between tongue and coefficient 2, as would have been expected for no motor equivalence.</p>
<p>Summarizing the results, the speakers can be classified as follows:</p>
<list id="list5-0023830911434098" list-type="simple">
<list-item><p>(1) No motor equivalence: speakers AM1 and AM2, no positive correlation between articulatory parameters but correlations between articulatory and acoustic parameters.</p></list-item>
<list-item><p>(2) Perfect motor equivalence: speaker CF1, positive correlation between tongue and lip, no correlation between articulatory and acoustic parameters.</p></list-item>
<list-item><p>(3) Partial motor equivalence: speakers CF2, CF3 and AF1, positive correlation between tongue and lip, but more variability in tongue than in lip position leading to a positive correlation between tongue position and coefficient 2.</p></list-item></list>
</sec>
<sec id="section26-0023830911434098">
<title>3.5 Articulatory landmark hypothesis</title>
<p>According to the articulatory landmark hypothesis there should be more variability in tongue position for speakers with a central palate than for speakers with an alveolar palate. As a consequence there should also be more motor equivalence in those speakers.</p>
<p>The two speakers for whom there is no motor equivalence are speakers with an alveolar prosthesis. In addition to that, the correlation is not very strong for the third speaker with this type of prosthesis. At first sight it seems that this is in line with the articulatory landmark hypothesis. However, according to this hypothesis the landmark should reduce the articulatory variability of the tongue tip during the perturbed sessions and this should be the reason for a reduction of the motor equivalence. Looking at the variability in tongue tip positions in the perturbed sessions (2wp, 3pe, 4pe, 5pe), this does not seem to be true. For speakers AM1 and AF1 the tongue tip position varied about 15 mm and for speaker AM2 about 8 mm (<xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref>). The speakers with the central palates had between 5 and 10 mm variability. So, although there is more motor equivalence for speakers with a central palate than for speakers with an alveolar palate this cannot be attributed to the presence or absence of an articulatory landmark.</p>
</sec>
<sec id="section27-0023830911434098">
<title>3.6 Auditory feedback hypothesis</title>
<p>According to the auditory feedback hypothesis, if auditory feedback is absolutely essential in adaptation, speakers should not be able to adapt when their auditory feedback is masked. The acoustic result of their productions should differ from the productions with auditory feedback available. Also, the productions without auditory feedback should not “fit in” with the covariation of tongue and lip. Comparing session 2wp to all other sessions, this session usually does not fall out of the correlation between tongue tip and lip (if there is one, cf. <xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref>). Some speakers had a higher DCT-coefficient in this session than in the other sessions (CF2, AF1, CF3, cf. <xref ref-type="fig" rid="fig3-0023830911434098">Figures 3</xref> and <xref ref-type="fig" rid="fig4-0023830911434098">4</xref>). Except for speaker AF1, this acoustic difference between sessions 1np and 2wp was, however, quite small, especially if one compares it to the overall acoustic variability in all sessions together. This suggests that adaptation is to a high degree possible even without auditory feedback.</p>
<p>For three speakers (AM2, CF2, CF3), the productions in session 2wp are very similar to the ones in 3pe. So, in contrast to the auditory feedback hypothesis, adaptation is not impossible without auditory feedback even if it is mainly restricted to keeping the articulatory strategy unchanged as much as possible. Generally, it is not possible to say whether changes between session 2wp and 3pe are solely due to the lack vs. presence of auditory feedback or whether they are at least in part due to practice gained in the course of session 2wp.</p>
</sec></sec>
<sec id="section28-0023830911434098" sec-type="discussion">
<title>4 Discussion</title>
<p>To summarize the results, in line with the hypothesis the majority of the speakers in this study showed the expected covariation of tongue and lip (cf. correlation of tongue and lip sensors in <xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref>). Contrary to the hypothesis, however, three of these speakers had more tongue movement than lip movement resulting in a correlation between lip movement and DCT2 (cf. <xref ref-type="fig" rid="fig3-0023830911434098">Figure 3</xref>) so that the acoustic output was not entirely independent of the two articulatory parameters but depended on the tongue position. So the motor equivalence hypothesis might be reformulated: Speakers covary their articulatory positions not in order to keep the acoustic output entirely stable, but to reduce variability in the acoustic output. This is also in line with studies such as <xref ref-type="bibr" rid="bibr49-0023830911434098">Zhou et al. (2008)</xref> showing that in motor equivalence in American English /r/ there is acoustic variability in the higher formants. The acoustic variability is kept within a range which makes it possible for a potential listener to recognize the sound.</p>
<p>The lip compensated only partly for changes in tongue position. This led to some acoustic variability which, given that the degree of compensation did not increase any further towards the end of the adaptation phase, seemed to be tolerated by the speakers. Speaker CF3 shows this very clearly. She had a clear correlation between tongue and lip position, moreover a positive correlation between tongue position and coefficient 2 as well as between lip position and coefficient 2. The tongue position varied by about 8 mm whereas the lip position varied by 4 mm only. Thus, if the tongue was fronted the lip was fronted as well but not as much as would be necessary to keep the cavity the same length.</p>
<p>Especially for the alveolar prosthesis one could have expected that the speakers produce the constriction at a more posterior place of articulation. This would have resulted in a very clear contrast between the postalveolar and the alveolar fricative. However, the data show that at least some of the speakers produce a more advanced constriction position for the postalveolar which is compensated for by more lip protrusion by some of the speakers. This result is in line with the assumption that speakers do not simply aim at maximizing the contrast between phonemes but that fine phonetic detail matters and the speakers consequently try to produce a fricative which they regard as typical for a German speaker.</p>
<p>A further observation was that motor equivalence was clearer for speakers with a central artificial palate than for speakers with an alveolar artificial palate. There was only one speaker with an alveolar palate showing a positive correlation between the two articulatory parameters, and this speaker actually had the weakest of the significant positive correlations between the articulatory parameters. The articulatory landmark hypothesis suggested that speakers with an alveolar palate would use the alveolar ridge as a landmark, which would reduce the variation in tongue position in the perturbed sessions. This was not observed. The lack of motor equivalence for these speakers can therefore not be ascribed to too little variability in tongue position due to the use of the alveolar ridge as a landmark, possibly because the speakers noticed that the alveolar ridge was at a more posterior place and therefore did not regard it as a useful landmark.</p>
<p>Looking at the results of session 2wp in <xref ref-type="fig" rid="fig2-0023830911434098">Figures 2</xref> to <xref ref-type="fig" rid="fig4-0023830911434098">4</xref> it seems that, except for speaker AF1, missing auditory feedback did not have much of an influence on the relationship between tongue and lip position or between either of the articulatory parameters and the acoustics. There was a usually small change in tongue position when no auditory feedback was available which was not compensated for by a change in lip position. However, the speakers did not compensate with auditory feedback either (session 3pe) suggesting that the acoustic change induced by the tongue position change was negligible. Thus, although the results do not show that speakers use motor equivalence strategies without auditory feedback, they suggest that speakers can adapt up to a certain degree even without auditory feedback. This is in line with a finding in <xref ref-type="bibr" rid="bibr34-0023830911434098">Perkell, Guenther, Lane, Matthies, Stockmann, Tiede, and Zandipour (2004)</xref> suggesting that the goals for sibilants are to a high extent somatosensory rather than auditory.</p>
<p>Although the results for single sessions are not given here, it is evident from <xref ref-type="fig" rid="fig2-0023830911434098">Figure 2</xref> that there are no correlations within a session except for some few exceptions (e.g., sessions 5pe and 6np of speaker CF2). One could wonder why, if speakers were using motor equivalence to a certain degree, they did not do this within a session. We can think of two reasons for that, a methodological one and one concerning speech motor learning. The first reason for not finding correlations between the two articulatory parameters could be that there are too few repetitions within a session. As demonstrated by <xref ref-type="bibr" rid="bibr35-0023830911434098">Perkell et al. (1993)</xref> for motor equivalent strategies in American English /u/ in unperturbed speech about 300 repetitions are needed until a correlation between lip protrusion and tongue body retraction can be found. In the present study, only 20 repetitions per session were available. A second possibility would be that speakers optimize their articulatory patterns over the adaptation process (<xref ref-type="bibr" rid="bibr38-0023830911434098">Schulz, Stein, &amp; Micallef, 2001</xref>; <xref ref-type="bibr" rid="bibr42-0023830911434098">Tiede, Mooshammer, Goldstein, Shattuck-Hufnagel, Perkell, &amp; Matthies, 2009</xref>). An optimization process would necessarily reduce the variability so that no correlation will be found. As can be seen in our data, the development over sessions is not linear, that is, speakers do not for example move from a configuration with little lip protrusion and a retracted tongue towards one with much lip protrusion and an advanced tongue. Rather, they seem to try out another strategy on each adaptation day. It is possible that they have already optimized the strategy they are using in each session, so that the articulatory variability within the session is reduced.</p>
<p>The results are partly in agreement with studies investigating adaptation to electropalatography palates, and they can expand the knowledge gained by these earlier studies. A study by <xref ref-type="bibr" rid="bibr29-0023830911434098">McAuliffe, Lin, Robb, and Murdoch (2008)</xref> into the effect of wearing an EPG palate showed that not all adult speakers are able to adapt. Within three hours, two of the three subjects were able to adapt /s/, /∫/ and /t/ perceptually, but the third subject was not. In a similar study <xref ref-type="bibr" rid="bibr30-0023830911434098">McAuliffe, Robb, and Murdoch (2007)</xref> found that speakers were able to adapt within 45 min to 3 hours. Vowel durations and vowel formant frequencies were found to be virtually unaffected by the palate whereas especially /s/ (but not so much /∫/) was severely affected immediately after the insertion of the palate. All speakers had adapted to some extent after 3 hours, most of them earlier (at 45 min).</p>
<p>The results of the present study are in agreement with these earlier studies by showing that there are speaker specific differences. Some speakers adapt better than others. As we suggest in a follow-up study to the present one (<xref ref-type="bibr" rid="bibr1-0023830911434098">Brunner et al., 2011a</xref>), this could be due to differences in the auditory acuity of these speakers.</p>
<p>Although our palatal prostheses were much thicker than standard EPG palates, the acoustic measurements show that speakers adapted quite quickly (in less than 45 minutes). This difference in adaptation time is probably due to the fact that we focused on the location of the main spectral peak and ignored the higher frequencies. As our results for /s/ from a previous perturbation study (<xref ref-type="bibr" rid="bibr2-0023830911434098">Brunner et al., 2011b</xref>) suggest, the adaptation of the higher frequencies might take up to two weeks for thick palates.</p>
<p>Our results are in line with findings from some other earlier studies. <xref ref-type="bibr" rid="bibr43-0023830911434098">Timmins, Cleland, Wood, Hardcastle, and Wishart (2009)</xref>, for example, investigated the production of /∫/ in young people with Down’s syndrome. They show that many of those speakers, who differ from typically developing children in their palate shape, managed to produce perceptually acceptable productions of the fricative even if the articulation differed as compared to that of typically developing children and adults. The speakers often produced a retracted groove. Our results are in line with the idea that these speakers might have succeeded in producing these sounds by changing the degree of lip protrusion. In a similar study <xref ref-type="bibr" rid="bibr3-0023830911434098">Cleland, Timmins, Wood, Hardcastle, and Wishart (2009)</xref> report two young people with Down’s syndrome who were able to produce perceptually adequate productions of /∫/ with fronted articulations. Our results strengthen the assumption that these speakers use more lip protrusion.</p>
<p>The measurement for front cavity size (horizontal difference between tongue tip and upper lip sensor) is fairly rudimentary in the present study. Future studies could use additional techniques, for example MRI, to provide more exact information on front cavity dimensions.</p>
</sec>
</body>
<back>
<app-group>
<table-wrap id="table3-0023830911434098" position="float">
<label>Table A.1.</label>
<caption>
<p>Correlations between length of front cavity and the acoustic parameters.</p>
</caption>
<graphic alternate-form-of="table3-0023830911434098" xlink:href="10.1177_0023830911434098-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Speaker</th>
<th align="left">COG</th>
<th align="left">Skewness</th>
<th align="left">Coeff2</th>
<th align="left"><italic>N</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>AM1</td>
<td>−.384 (.000)***</td>
<td>−.095 (.304)</td>
<td><bold>0.520 (.000)</bold> ***</td>
<td>119</td>
</tr>
<tr>
<td>AM2</td>
<td><bold>-.686 (.000)</bold> ***</td>
<td>.494 (.000) ***</td>
<td>0.623 (.000) ***</td>
<td>100</td>
</tr>
<tr>
<td>AF1</td>
<td>−.641 (.000) ***</td>
<td>.162 (.077)</td>
<td><bold>0.781 (.000)</bold> ***</td>
<td>120</td>
</tr>
<tr>
<td>CF1</td>
<td><bold>-.492 (.000)</bold> ***</td>
<td>.426 (.001) *</td>
<td>0.338 (.010) *</td>
<td>58</td>
</tr>
<tr>
<td>CF2</td>
<td>−.237 (.017) *</td>
<td>−0.243 (.015) *</td>
<td><bold>0.443 (.000)</bold> ***</td>
<td>100</td>
</tr>
<tr>
<td>CF3</td>
<td>−.538 (.000) ***</td>
<td>.282 (.002) **</td>
<td><bold>0.699 (.000)</bold> ***</td>
<td>118</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0023830911434098"><p>First column: speaker, second to fourth columns: correlation coefficients and significances (in parentheses) of the correlations between front cavity length and center of gravity, skewness and coefficient 2, respectively, fifth column: number of productions. Highest correlation coefficients for a speaker are in bold.</p></fn>
</table-wrap-foot>
</table-wrap>
</app-group>
<ack><p>This study was supported by grants from the Deutsche Forschungsgemeinschaft (PO 334/4-1 to Bernd Pompino-Marschall, HO 3271/1-1 to Phil Hoole), by a grant from the Ministère délégué à l’enseignement superieur et à la recherche scientifique for a cotutelle de thèse, and a grant for postdoctoral research from the Deutscher Akademischer Austauschdienst to Jana Brunner. We thank Jörg Dreyer for carrying out the 2D EMA recordings and Olesya Rauch, Vivien Hein and Susanne Waltl for acoustic segmentation. Many thanks to Tine Mooshammer, Bernd Pompino-Marschall, Melanie Weirich, editor Jim Scobbie and reviewers Maria-Josep Solé and Megan McAuliffe for comments on earlier versions. Thanks also to Mark Tiede for providing scripts for the calculation of spectra and spectral moments.</p></ack>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0023830911434098">
<label>1</label>
<p>We are grateful to editor Jim Scobbie for drawing this to our attention.</p></fn>
<fn fn-type="other" id="fn2-0023830911434098">
<label>2</label>
<p>But see <xref ref-type="bibr" rid="bibr28-0023830911434098">Löfqvist, Baer, McGarr, &amp; Story (1989)</xref> and <xref ref-type="bibr" rid="bibr19-0023830911434098">Hoole &amp; Honda (2011)</xref> for alternative approaches to the relationship between voicing and F0.</p></fn>
<fn fn-type="other" id="fn3-0023830911434098">
<label>3</label>
<p>We were actually interested in recruiting subjects without any experience in wearing dental prostheses. However, it was found that there are hardly any young people in Germany without such experience. In order to have a homogeneous group of subjects, we therefore decided to recruit speakers with such experience.</p></fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brunner</surname><given-names>J.</given-names></name>
<name><surname>Ghosh</surname><given-names>S. S.</given-names></name>
<name><surname>Hoole</surname><given-names>P.</given-names></name>
<name><surname>Matthies</surname><given-names>M.</given-names></name>
<name><surname>Tiede</surname><given-names>M.</given-names></name>
<name><surname>Perkell</surname><given-names>J.</given-names></name>
</person-group> (<year>2011a</year>). <article-title>The influence of auditory acuity on acoustic variability and the use of motor equivalence during adaptation to a perturbation</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>, <volume>54</volume>, <fpage>727</fpage>–<lpage>739</lpage>.</citation>
</ref>
<ref id="bibr2-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brunner</surname><given-names>J.</given-names></name>
<name><surname>Hoole</surname><given-names>P.</given-names></name>
<name><surname>Perrier</surname><given-names>P.</given-names></name>
</person-group> (<year>2011b</year>). <article-title>Adaptation strategies in perturbed /s/</article-title>. <source>Clinical Linguistics and Phonetics</source>, <volume>25</volume>(<issue>8</issue>), <fpage>705</fpage>–<lpage>724</lpage>.</citation>
</ref>
<ref id="bibr3-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cleland</surname><given-names>J.</given-names></name>
<name><surname>Timmins</surname><given-names>C.</given-names></name>
<name><surname>Wood</surname><given-names>S. E.</given-names></name>
<name><surname>Hardcastle</surname><given-names>W. J.</given-names></name>
<name><surname>Wishart</surname><given-names>J. G.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Electropalatographic therapy for children and young people with Down’s syndrome</article-title>. <source>Clinical Linguistics and Phonetics</source>, <volume>23</volume>(<issue>12</issue>), <fpage>926</fpage>–<lpage>939</lpage>.</citation>
</ref>
<ref id="bibr4-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Delattre</surname><given-names>P.</given-names></name>
<name><surname>Freeman</surname><given-names>D.</given-names></name>
</person-group> (<year>1968</year>). <article-title>A dialect study of American R’s by x-ray motion picture</article-title>. <source>Linguistics</source>, <volume>44</volume>, <fpage>29</fpage>–<lpage>68</lpage>.</citation>
</ref>
<ref id="bibr5-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Diehl</surname><given-names>R. L.</given-names></name>
<name><surname>Kingston</surname><given-names>J.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Phonetic covariation as auditory enhancement: The case of the [+voice]/[-voice] distinction</article-title>. In <person-group person-group-type="editor">
<name><surname>Engstrand</surname><given-names>O.</given-names></name>
<name><surname>Kylander</surname><given-names>C.</given-names></name>
</person-group> (Eds.), <source>Current phonetic research paradigms: Implications for speech motor control</source>, PERILUS, volume <volume>14</volume>, <publisher-name>University of Stockholm</publisher-name>, <fpage>139</fpage>–<lpage>143</lpage>.</citation>
</ref>
<ref id="bibr6-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fant</surname><given-names>G.</given-names></name>
</person-group> (<year>1960</year>). <source>Acoustic theory of speech production</source>. <publisher-loc>The Hague</publisher-loc>: <publisher-name>Mouton</publisher-name>.</citation>
</ref>
<ref id="bibr7-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Folkins</surname><given-names>J. W.</given-names></name>
<name><surname>Zimmermann</surname><given-names>G. N.</given-names></name>
</person-group> (<year>1982</year>). <article-title>Lip and jaw interaction during speech: Responses to perturbation of lower-lip movement prior to bilabial closure</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>71</volume>(<issue>5</issue>), <fpage>1225</fpage>–<lpage>1233</lpage>.</citation>
</ref>
<ref id="bibr8-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Forrest</surname><given-names>K.</given-names></name>
<name><surname>Weismer</surname><given-names>G.</given-names></name>
<name><surname>Milencovic</surname><given-names>P.</given-names></name>
<name><surname>Dougall</surname><given-names>R. N.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Statistical analysis of word-initial voiceless obstruents: Preliminary data</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>84</volume>(<issue>1</issue>), <fpage>115</fpage>–<lpage>123</lpage>.</citation>
</ref>
<ref id="bibr9-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gracco</surname><given-names>V. L.</given-names></name>
<name><surname>Abbs</surname><given-names>J. H.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Dynamic control of the perioral system during speech: Kinematic analyses of autogenic and nonautogenic sensorimotor processes</article-title>. <source>Journal of Neurophysiology</source>, <volume>54</volume>(<issue>2</issue>), <fpage>418</fpage>–<lpage>432</lpage>.</citation>
</ref>
<ref id="bibr10-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guenther</surname><given-names>F. H.</given-names></name>
</person-group> (<year>1994</year>). <article-title>A neural network model of speech acquisition and motor equivalent speech production</article-title>. <source>Biological Cybernetics</source>, <volume>72</volume>, <fpage>43</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr11-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guenther</surname><given-names>F. H.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Speech sound acquisition, coarticulation, and rate effects in a neural network model of speech production</article-title>. <source>Psychological Review</source>, <volume>102</volume>, <fpage>594</fpage>–<lpage>621</lpage>.</citation>
</ref>
<ref id="bibr12-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Guenther</surname><given-names>F. H.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Neural control of speech movements</article-title>. In <person-group person-group-type="editor">
<name><surname>Meyer</surname><given-names>A.</given-names></name>
<name><surname>Schiller</surname><given-names>N.</given-names></name>
</person-group> (Eds.), <source>Phonetics and phonology in language comprehension and production: Differences and similarities</source> (pp. <fpage>209</fpage>–<lpage>240</lpage>). <publisher-loc>Berlin</publisher-loc>: <publisher-name>Mouton de Gruyter</publisher-name>.</citation>
</ref>
<ref id="bibr13-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guenther</surname><given-names>F. H.</given-names></name>
<name><surname>Hampson</surname><given-names>M.</given-names></name>
<name><surname>Johnson</surname><given-names>D.</given-names></name>
</person-group> (<year>1998</year>). <article-title>A theoretical investigation of reference frames for the planning of speech movements</article-title>. <source>Psychological Review</source>, <volume>105</volume>, <fpage>611</fpage>–<lpage>633</lpage>.</citation>
</ref>
<ref id="bibr14-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guzik</surname><given-names>K.</given-names></name>
<name><surname>Harrington</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The quantification of place of articulation assimilation in electropalatographic data using the similarity index (SI)</article-title>. <source>Advances in Speech-Language Pathology</source>, <volume>9</volume>, <fpage>109</fpage>–<lpage>119</lpage>.</citation>
</ref>
<ref id="bibr15-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hamlet</surname><given-names>S. L.</given-names></name>
<name><surname>Stone</surname><given-names>M.</given-names></name>
</person-group> (<year>1978</year>). <article-title>Compensatory alveolar consonant production induced by wearing a dental prosthesis</article-title>. <source>Journal of Phonetics</source>, <volume>6</volume>, <fpage>227</fpage>–<lpage>248</lpage>.</citation>
</ref>
<ref id="bibr16-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Honda</surname><given-names>M.</given-names></name>
<name><surname>Fukino</surname><given-names>A.</given-names></name>
<name><surname>Kaburagi</surname><given-names>T.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Compensatory responses of articulators to unexpected perturbation of the palatal shape</article-title>. <source>Journal of Phonetics</source>, <volume>30</volume>(<issue>3</issue>), <fpage>281</fpage>–<lpage>302</lpage>.</citation>
</ref>
<ref id="bibr17-0023830911434098">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Honda</surname><given-names>M.</given-names></name>
<name><surname>Murano</surname><given-names>E.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Effects of tactile and auditory feedback on compensatory articulatory response to an unexpected palatal perturbation</article-title>. In <person-group person-group-type="editor">
<name><surname>Palethorpe</surname><given-names>S.</given-names></name>
<name><surname>Tabain</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <conf-name>Proceedings of the 6th Speech Production Seminar</conf-name>, <conf-loc>Sydney</conf-loc>, <fpage>97</fpage>–<lpage>100</lpage>.</citation>
</ref>
<ref id="bibr18-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hoole</surname><given-names>P.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Bite-block speech in the absence of oral sensibility</article-title>. <source>Proceedings of the 11th International Congress of Phonetic Sciences</source>, <volume>4</volume>, <fpage>16</fpage>–<lpage>19</lpage>.</citation>
</ref>
<ref id="bibr19-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hoole</surname><given-names>P.</given-names></name>
<name><surname>Honda</surname><given-names>K.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Automaticity vs. feature-enhancement in the control of segmental F0</article-title>. In <person-group person-group-type="editor">
<name><surname>Clements</surname><given-names>N.</given-names></name>
<name><surname>Ridouane</surname><given-names>R.</given-names></name>
</person-group> (Eds.), <source>Where do phonological contrasts come from? Cognitive, physical and developmental bases of phonological features</source> (pp.<fpage>131</fpage>–<lpage>171</lpage>). <series>Series Language Faculty and Beyond (LFAB): Internal and External Variation in Linguistics</series>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>John Benjamins</publisher-name>.</citation>
</ref>
<ref id="bibr20-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jain</surname><given-names>A. K.</given-names></name>
</person-group> (<year>1989</year>). <source>Fundamentals of digital image processing</source>. <publisher-loc>Englewood Cliffs, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</citation>
</ref>
<ref id="bibr21-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jones</surname><given-names>J. A.</given-names></name>
<name><surname>Munhall</surname><given-names>K. G.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Learning to produce speech with an altered vocal tract: The role of auditory feedback</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>113</volume>(<issue>1</issue>), <fpage>532</fpage>–<lpage>543</lpage>.</citation>
</ref>
<ref id="bibr22-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jordan</surname><given-names>M. I.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Computational aspects of motor control and motor learning</article-title>. In <person-group person-group-type="editor">
<name><surname>Heuer</surname><given-names>H.</given-names></name>
<name><surname>Keele</surname><given-names>S.</given-names></name>
</person-group> (Eds.), <source>Handbook of perception and action: Motor skills</source> (pp. <fpage>71</fpage>–<lpage>120</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr23-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kelso</surname><given-names>J. A. S.</given-names></name>
<name><surname>Tuller</surname><given-names>B.</given-names></name>
</person-group> (<year>1983</year>). <article-title>“Compensatory articulation” under conditions of reduced afferent information: A dynamic formulation</article-title>. <source>Journal of Speech and Hearing Research</source>, <volume>26</volume>, <fpage>217</fpage>–<lpage>224</lpage>.</citation>
</ref>
<ref id="bibr24-0023830911434098">
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Laboissière</surname><given-names>R.</given-names></name>
</person-group> (<year>1992</year>). <source>Préliminaires pour une robotique de la communication parlée: inversion et contrôle d’un modèle articulatoire du conduit vocal</source> (Doctoral dissertation). <publisher-name>Institut National Polytechnique de Grenoble</publisher-name>.</citation>
</ref>
<ref id="bibr25-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lawson</surname><given-names>E.</given-names></name>
<name><surname>Scobbie</surname><given-names>J. M.</given-names></name>
<name><surname>Stuart-Smith</surname><given-names>J.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The social stratification of tongue shape for postvocalic /r/ in Scottish English</article-title>. <source>Journal of Sociolinguistics</source>, <volume>15</volume>, <fpage>256</fpage>–<lpage>268</lpage>.</citation>
</ref>
<ref id="bibr26-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lindblom</surname><given-names>B.</given-names></name>
<name><surname>Lubker</surname><given-names>J.</given-names></name>
<name><surname>Gay</surname><given-names>T.</given-names></name>
</person-group> (<year>1979</year>). <article-title>Formant frequencies of some fixed-mandible vowels and a model of speech motor programming by predictive simulation</article-title>. <source>Journal of Phonetics</source>, <volume>7</volume>, <fpage>147</fpage>–<lpage>161</lpage>.</citation>
</ref>
<ref id="bibr27-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Linke</surname><given-names>D.</given-names></name>
</person-group> (<year>1980</year>). <article-title>Vorprogrammierung und Rückkopplung bei der Sprache</article-title>. In <person-group person-group-type="editor">
<name><surname>Spreng</surname><given-names>M.</given-names></name>
</person-group> (Ed.), <source>Interaktion zwischen Artikulation und akustischer Perzeption</source> (pp. <fpage>50</fpage>–<lpage>57</lpage>). <publisher-loc>Stuttgart</publisher-loc>: <publisher-name>Thieme</publisher-name>.</citation>
</ref>
<ref id="bibr28-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Löfqvist</surname><given-names>A.</given-names></name>
<name><surname>Baer</surname><given-names>T.</given-names></name>
<name><surname>McGarr</surname><given-names>N. S.</given-names></name>
<name><surname>Story</surname><given-names>R. S.</given-names></name>
</person-group> (<year>1989</year>). <article-title>The cricothyroid muscle in voicing control</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>85</volume>, <fpage>1314</fpage>–<lpage>1321</lpage>.</citation>
</ref>
<ref id="bibr29-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McAuliffe</surname><given-names>M. K.</given-names></name>
<name><surname>Lin</surname><given-names>E.</given-names></name>
<name><surname>Robb</surname><given-names>M. P.</given-names></name>
<name><surname>Murdoch</surname><given-names>B. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Influence of a standard electropalaography artificial palate upon articulation: A preliminary study</article-title>. <source>Folia Phoniatrica Logop</source>, <volume>60</volume>(<issue>1</issue>), <fpage>45</fpage>–<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr30-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McAuliffe</surname><given-names>M. J.</given-names></name>
<name><surname>Robb</surname><given-names>M. P.</given-names></name>
<name><surname>Murdoch</surname><given-names>B. E.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Acoustic and perceptual analysis of speech adaptation to an artificial palate</article-title>. <source>Clinical Linguistics and Phonetics</source>, <volume>21</volume>, <fpage>885</fpage>–<lpage>894</lpage>.</citation>
</ref>
<ref id="bibr31-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McFarland</surname><given-names>D. H.</given-names></name>
<name><surname>Baum</surname><given-names>S. R.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Incomplete compensation to articulatory perturbation</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>97</volume>(<issue>3</issue>), <fpage>1865</fpage>–<lpage>1873</lpage>.</citation>
</ref>
<ref id="bibr32-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mooshammer</surname><given-names>C.</given-names></name>
<name><surname>Hoole</surname><given-names>P.</given-names></name>
<name><surname>Geumann</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Jaw and order</article-title>. <source>Language and Speech</source>, <volume>50</volume>(<issue>2</issue>), <fpage>145</fpage>–<lpage>176</lpage>.</citation>
</ref>
<ref id="bibr33-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Perkell</surname><given-names>J.</given-names></name>
<name><surname>Guenther</surname><given-names>F.</given-names></name>
<name><surname>Lane</surname><given-names>H.</given-names></name>
<name><surname>Matthies</surname><given-names>M.</given-names></name>
<name><surname>Perrier</surname><given-names>P.</given-names></name>
<name><surname>Vick</surname><given-names>J.</given-names></name>
<name><surname>Wilhelms-Tricarico</surname><given-names>R.</given-names></name>
<name><surname>Zandipour</surname><given-names>M.</given-names></name>
</person-group> (<year>2000</year>). <article-title>A theory of speech motor control and supporting data from speakers with normal hearing and with profound hearing loss</article-title>. <source>Journal of Phonetics</source>, <volume>28</volume>(<issue>3</issue>), <fpage>233</fpage>–<lpage>272</lpage>.</citation>
</ref>
<ref id="bibr34-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Perkell</surname><given-names>J. S.</given-names></name>
<name><surname>Guenther</surname><given-names>F. H.</given-names></name>
<name><surname>Lane</surname><given-names>H.</given-names></name>
<name><surname>Matthies</surname><given-names>M. L.</given-names></name>
<name><surname>Stockmann</surname><given-names>E.</given-names></name>
<name><surname>Tiede</surname><given-names>M.</given-names></name>
<name><surname>Zandipour</surname><given-names>M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The distinctness of speakers’ productions of vowel contrasts is related to their discrimination of the contrasts</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>116</volume>, <fpage>2338</fpage>–<lpage>2344</lpage>.</citation>
</ref>
<ref id="bibr35-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Perkell</surname><given-names>J. S.</given-names></name>
<name><surname>Matthies</surname><given-names>M. L.</given-names></name>
<name><surname>Svirsky</surname><given-names>M. A.</given-names></name>
<name><surname>Jordan</surname><given-names>M. I.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Trading relations between tongue-body raising and lip rounding in production of the vowel /u/: A pilot “motor equivalence” study</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>93</volume>(<issue>5</issue>), <fpage>2948</fpage>–<lpage>2961</lpage>.</citation>
</ref>
<ref id="bibr36-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Savariaux</surname><given-names>C.</given-names></name>
<name><surname>Perrier</surname><given-names>P.</given-names></name>
<name><surname>Orliaguet</surname><given-names>J.-P.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Compensation strategies for the perturbation of the rounded vowel [u] using a lip tube: A study of the control space in speech production</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>98</volume>(<issue>5</issue>), <fpage>2428</fpage>–<lpage>2442</lpage>.</citation>
</ref>
<ref id="bibr37-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Savariaux</surname><given-names>C.</given-names></name>
<name><surname>Perrier</surname><given-names>P.</given-names></name>
<name><surname>Orliaguet</surname><given-names>J.-P.</given-names></name>
<name><surname>Schwartz</surname><given-names>J.-L.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Compensation strategies for the perturbation of French [u] using a lip tube II: Perceptual analysis</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>106</volume>(<issue>1</issue>), <fpage>381</fpage>–<lpage>393</lpage>.</citation>
</ref>
<ref id="bibr38-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schulz</surname><given-names>G. M.</given-names></name>
<name><surname>Stein</surname><given-names>L.</given-names></name>
<name><surname>Micallef</surname><given-names>R.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Speech motor learning: Preliminary data</article-title>. <source>Clinical Linguistics and Phonetics</source>, <volume>15</volume>(<issue>1&amp;2</issue>), <fpage>157</fpage>–<lpage>161</lpage>.</citation>
</ref>
<ref id="bibr39-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shadle</surname><given-names>C.</given-names></name>
</person-group> (<year>1985</year>). <article-title>The acoustics of fricative consonants</article-title>. <source>Technical Report</source> <volume>506</volume>, <publisher-name>Research Laboratory of Electronics</publisher-name>, <publisher-loc>MIT Cambridge, MA</publisher-loc>.</citation>
</ref>
<ref id="bibr40-0023830911434098">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Shadle</surname><given-names>C.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Articulatory-acoustic relationships in fricative consonants</article-title>. In <person-group person-group-type="editor">
<name><surname>Hardcastle</surname><given-names>W. J.</given-names></name>
<name><surname>Marchal</surname><given-names>A.</given-names></name>
</person-group> (Eds.), <source>Speech production and speech modelling</source> (pp. <fpage>187</fpage>–<lpage>209</lpage>). <publisher-loc>Dordrecht</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>.</citation>
</ref>
<ref id="bibr41-0023830911434098">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Shadle</surname><given-names>C.</given-names></name>
<name><surname>Berezina</surname><given-names>M.</given-names></name>
<name><surname>Proctor</surname><given-names>M.</given-names></name>
<name><surname>Iskarous</surname><given-names>K.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Mechanical models of fricatives based on MRI-derived vocal tract shapes</article-title>. <conf-name>Proceedings of the 8th International Seminar on Speech Production</conf-name>, <conf-loc>Strasbourg</conf-loc>, <fpage>417</fpage>–<lpage>420</lpage>.</citation>
</ref>
<ref id="bibr42-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tiede</surname><given-names>M.</given-names></name>
<name><surname>Mooshammer</surname><given-names>C.</given-names></name>
<name><surname>Goldstein</surname><given-names>L.</given-names></name>
<name><surname>Shattuck-Hufnagel</surname><given-names>S.</given-names></name>
<name><surname>Perkell</surname><given-names>J.</given-names></name>
<name><surname>Matthies</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Optimization of articulator trajectories in producing learned nonsense words</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>125</volume>(<issue>4</issue>), <fpage>2499</fpage>.</citation>
</ref>
<ref id="bibr43-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Timmins</surname><given-names>C.</given-names></name>
<name><surname>Cleland</surname><given-names>J.</given-names></name>
<name><surname>Wood</surname><given-names>S. E.</given-names></name>
<name><surname>Hardcastle</surname><given-names>W. J.</given-names></name>
<name><surname>Wishart</surname><given-names>J. G.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A perceptual and electropalatographic study of /S/ in young people with Down’s syndrome</article-title>. <source>Clinical Linguistics and Phonetics</source>, <volume>23</volume>(<issue>12</issue>), <fpage>911</fpage>–<lpage>925</lpage>.</citation>
</ref>
<ref id="bibr44-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Waldstein</surname><given-names>R. S.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Effects of postlingual deafness on speech production: Implications for the role of auditory feedback</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>88</volume>, <fpage>2099</fpage>–<lpage>2114</lpage>.</citation>
</ref>
<ref id="bibr45-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Watson</surname><given-names>C.</given-names></name>
<name><surname>Harrington</surname><given-names>J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Acoustic evidence for dynamic formant trajectories in Australian English vowels</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>106</volume>(<issue>1</issue>), <fpage>458</fpage>–<lpage>468</lpage>.</citation>
</ref>
<ref id="bibr46-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Westbury</surname><given-names>J.</given-names></name>
<name><surname>Hashi</surname><given-names>M.</given-names></name>
<name><surname>Lindstrom</surname><given-names>M.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Differences among speakers in lingual articulation of American English /r/</article-title>. <source>Speech Communication</source>, <volume>26</volume>, <fpage>203</fpage>–<lpage>226</lpage>.</citation>
</ref>
<ref id="bibr47-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Westbury</surname><given-names>J. R.</given-names></name>
<name><surname>Lindstrom</surname><given-names>M. J.</given-names></name>
<name><surname>McClean</surname><given-names>M. D.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Tongues and lips without jaws: A comparison of methods for decoupling speech movements</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>, <volume>45</volume>, <fpage>651</fpage>–<lpage>662</lpage>.</citation>
</ref>
<ref id="bibr48-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>X.</given-names></name>
<name><surname>Espy-Wilson</surname><given-names>C.</given-names></name>
<name><surname>Tiede</surname><given-names>M.</given-names></name>
<name><surname>Boyce</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Acoustic cues of “retroflex” and “bunched” American English rhotic sound</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>121</volume>(<issue>5</issue>), <fpage>3168</fpage>.</citation>
</ref>
<ref id="bibr49-0023830911434098">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zhou</surname><given-names>X.</given-names></name>
<name><surname>Espy-Wilson</surname><given-names>C.</given-names></name>
<name><surname>Boyce</surname><given-names>S.</given-names></name>
<name><surname>Tiede</surname><given-names>M.</given-names></name>
<name><surname>Holland</surname><given-names>C.</given-names></name>
<name><surname>Choe</surname><given-names>A.</given-names></name>
</person-group> (<year>2008</year>). <article-title>A magnetic resonance imaging-based articulatory and acoustic study of “retroflex” and “bunched” American English /r/</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>123</volume>(<issue>6</issue>), <fpage>4466</fpage>–<lpage>4481</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>