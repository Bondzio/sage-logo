<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ALH</journal-id>
<journal-id journal-id-type="hwp">spalh</journal-id>
<journal-title>Active Learning in Higher Education</journal-title>
<issn pub-type="ppub">1469-7874</issn>
<issn pub-type="epub">1741-2625</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1469787412441285</article-id>
<article-id pub-id-type="publisher-id">10.1177_1469787412441285</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Mind the gap: An analysis of how quality assurance processes influence programme assessment patterns</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Jessop</surname><given-names>Tansy</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>McNab</surname><given-names>Nicole</given-names></name>
<aff id="aff1-1469787412441285">University of Winchester, UK</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Gubby</surname><given-names>Laura</given-names></name>
<aff id="aff2-1469787412441285">Canterbury Christ Church University, UK</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-1469787412441285">Tansy Jessop, Learning and Teaching Development Unit, University of Winchester, West Hill, Winchester SO22 4NR, UK Email: <email>Tansy.Jessop@winchester.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2012</year>
</pub-date>
<volume>13</volume>
<issue>2</issue>
<fpage>143</fpage>
<lpage>154</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>This article explores the relationship between the lack of visible attention to formative assessment in degree specifications and its marginalization in practice. Degree specification documents form part of the quality apparatus emphasizing the accountability and certification duties of assessment. Ironically, a framework designed to assure quality may work to the exclusion of a pedagogic duty to students. This study draws on interview and documentary evidence from 14 programmes at a single UK university, supported by data from a national research project. The authors found that institutional quality frameworks focused programme leaders’ attention on summative assessment, usually atomized to the modular unit. The invisibility of formative assessment in documentation reinforced the tendency of modular programmes to have high summative demands, with optional, fragmented and infrequent formative assessment. Heavy workloads, modularity and pedagogic uncertainties compounded the problem. The article concludes with reflections about facilitating a more pervasive culture of formative assessment to improve student learning.</p>
</abstract>
<kwd-group>
<kwd>academic structures</kwd>
<kwd>formative assessment</kwd>
<kwd>modularization</kwd>
<kwd>programme assessment</kwd>
<kwd>quality assurance</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1469787412441285">
<title>The context of declining formative assessment patterns</title>
<p>The centrality of formative assessment as a pedagogic principle is well established. Conventionally, it is defined as low-risk, unmarked assessment with feedback, through which students gradually master the skills of a discipline, and are encouraged to take risks and be creative without it impacting on their marks (and thus their degree classification). It contrasts with summative assessment, defined as the final, marked judgements of performance which determine a student’s award. Formative assessment is a process of nurturing learning, while summative is the outcome of that process, namely a grade.</p>
<p>Formative assessment processes are the cornerstone of student learning, with formative feedback described as ‘the most powerful single influence enhancing achievement’ (<xref ref-type="bibr" rid="bibr13-1469787412441285">Hattie, 2009</xref>: 12). The largest study of formative assessment ever written, based on data from 250 studies ranging from infant schools through to universities, found conclusively that formative assessment does improve learning: ‘The gains in achievement appear to be quite considerable, and amongst the largest ever reported for educational interventions’ (<xref ref-type="bibr" rid="bibr2-1469787412441285">Black and William, 1998</xref>: 61). Formative assessment has wide support as a significant condition of learning in higher education (<xref ref-type="bibr" rid="bibr4-1469787412441285">Boud, 2000</xref>; <xref ref-type="bibr" rid="bibr11-1469787412441285">Gibbs and Simpson, 2004</xref>; <xref ref-type="bibr" rid="bibr15-1469787412441285">Knight, 2000</xref>; <xref ref-type="bibr" rid="bibr18-1469787412441285">Nicol and McFarlane-Dick, 2006</xref>). It provides a ‘fine tuning mechanism for how and what we learn’ (<xref ref-type="bibr" rid="bibr4-1469787412441285">Boud, 2000</xref>: 156), particularly by helping students to self-regulate and monitor their own learning. In summary, formative assessment is designed to <italic>develop</italic> capacity rather than to <italic>measure</italic> it (<xref ref-type="bibr" rid="bibr12-1469787412441285">Gipps, 1999</xref>).</p>
<p>While recognizing that students do derive some feedback from summative judgements, in this study we have defined summative and formative tasks and feedback as distinctive. A number of factors constrain summative feedback from both being developmental and providing a mark. One is evidence that feedback on summative tasks is too readily dismissed or neglected when there is a grade which counts towards a degree award (<xref ref-type="bibr" rid="bibr2-1469787412441285">Black and William, 1998</xref>: 18; <xref ref-type="bibr" rid="bibr19-1469787412441285">Orrell, 2006</xref>: 453; <xref ref-type="bibr" rid="bibr24-1469787412441285">Taras, 2002</xref>, <xref ref-type="bibr" rid="bibr25-1469787412441285">2008</xref>). Another is that summative feedback often arrives too late to be acted on, because of time-consuming reliability checks such as moderation and second marking. A further reason is that many summative tasks are ‘one-off’, with little consideration in the assessment design of how students could act on or apply feedback to the next task, usually on a new unit of study, as <xref ref-type="bibr" rid="bibr19-1469787412441285">Orrell (2006</xref>: 449) found in her Australian study. The academic structure of many new degrees works against the formative use of feedback from summative tasks, across courses and modules.</p>
<p>In the UK context, <xref ref-type="bibr" rid="bibr17-1469787412441285">Knight and Yorke (2003)</xref> maintain that one of the main consequences of the modular degree system has normally been the depletion of formative assessment opportunities (see also <xref ref-type="bibr" rid="bibr21-1469787412441285">Rust, 2000</xref>; <xref ref-type="bibr" rid="bibr29-1469787412441285">Yorke, 1998</xref>). The modular system has normally divided course credits into semester-long units, in order to make credits and awards transferable across universities in the UK, and within countries represented by the 47 signatories of the European Bologna process. A modular design of courses is also commonplace in Australia and other Western higher education systems. As a consequence of modular degree systems, students study discrete units of study, in compressed time, with the need to measure and accredit their achievements being the main drivers for a largely summative assessment diet.</p>
<p>The recent resurgence in research interest in programme-wide assessment reflects anxieties that degree coherence, progression and ‘slow learning’ are at risk in modular degrees (<xref ref-type="bibr" rid="bibr3-1469787412441285">Bloxham and Boyd, 2007</xref>; <xref ref-type="bibr" rid="bibr6-1469787412441285">Claxton, 1998</xref>; <xref ref-type="bibr" rid="bibr9-1469787412441285">Gibbs and Dunbar-Goddet, 2007</xref>, <xref ref-type="bibr" rid="bibr10-1469787412441285">2009</xref>; <xref ref-type="bibr" rid="bibr15-1469787412441285">Knight, 2000</xref>; <xref ref-type="bibr" rid="bibr17-1469787412441285">Knight and Yorke, 2003</xref>; <xref ref-type="bibr" rid="bibr21-1469787412441285">Rust, 2000</xref>). In the literature, programme assessment strategies are described as having learning outcomes to reflect both the whole programme philosophy and the complexity of outcomes not routinely contained in a 12-week module. Some strategies imply de-coupling assessment from modular credit schemes to allow for fewer and more wide-ranging summative assessment tasks spanning modules across a whole degree programme. A programme approach aims to create opportunities for more formative assessment, leading to enhanced student learning.</p>
<p>Strategies to make assessment more programmatic in character wrestle with complex modular degree structures and the quality assurance regimes which underpin them. Within mass higher education systems, such as those in the UK and Australia, assessment procedures and practices are standardized and stipulated in programme documentation, with the development of ‘a strong trend towards fine-grained prescription, atomised assessment, the accumulation of little “credits” like grains of sand’ (<xref ref-type="bibr" rid="bibr22-1469787412441285">Sadler, 2007</xref>: 392). The volume, length and format of assessment are specified in relation to credit weightings, module by module. Documents containing the programme specification are described as definitive documents because they contain the non-negotiable elements or defining features of a degree course. In most definitive documents, summative assessment dominates, while formative assessment is virtually invisible.</p>
<p>Procedures to assure the quality of higher education institutions have been formalized across all UK universities since the establishment of a single quality assurance agency in 1997 (<xref ref-type="bibr" rid="bibr20-1469787412441285">QAA, 2008</xref>). Quality assurance mechanisms are designed to serve two main purposes. The first is to measure student learning in relation to rules and regulations about credits and module pathways for certification (the degree award), and the second is to assure and validate the programme of study as credible and comparable with other degrees in the higher education sector. Both of these purposes are about measurable, public indicators of quality, rather than the more messy business of learning and teaching. They are product indicators, built on rational and linear presuppositions about ‘outcomes’, rather than process indicators, which reflect subtle and complex learning encounters that make up the curriculum (<xref ref-type="bibr" rid="bibr16-1469787412441285">Knight, 2001</xref>). Inevitably, within the systems of programme documentation, summative assessment dominates as a measurable product, an outcome leading to certification, for which institutions are held accountable.</p>
<p>While this research comes out of the UK modular degree context, the findings may have resonance for other systems and places. Becker observed a similar phenomenon in his study of student life at Kansas University more than 40 years ago: ‘grades had become the administrative device that actively diverted students from really learning anything’ (<xref ref-type="bibr" rid="bibr1-1469787412441285">Becker, 1968</xref>: xiii). Administrative systems have subtle, unintended consequences for how the curriculum is developed and pedagogy is practised. In some cases, this may lead to ‘the displacement of learning by procedural compliance’ (<xref ref-type="bibr" rid="bibr28-1469787412441285">Torrance, 2007</xref>: 293).</p>
<p>This article examines the implications of official discourses of assessment found in programme validation documents, in relation to their interpretation in practice. It seeks to find out whether, and if so how, the prescribed institutional version of assessment might differ from the practice of assessment as recounted by programme leaders at a local level, exploring whether, or to what extent, ‘the context of national and local requirements for certification and accountability exert a powerful influence on its practice’ (<xref ref-type="bibr" rid="bibr2-1469787412441285">Black and William, 1998</xref>: 20).</p>
</sec>
<sec id="section2-1469787412441285" sec-type="methods">
<title>Methodology</title>
<p>The researchers collected interview data from 14 programme leaders of undergraduate degree programmes across four faculties in one UK university in April 2009. Sampling of programmes was based on a combination of convenience and quota sampling (<xref ref-type="bibr" rid="bibr5-1469787412441285">Bryman, 2001</xref>: 97, 99). Members of the university programme leaders’ forum were invited to participate, and thereafter email approaches were directed to programme leaders in under-represented faculties. The final sample included foundation degrees, arts, humanities and social science programmes, and professional degrees from all three faculties. Alongside the interview process, the researchers conducted a fine-grained analysis of programme validation documents to elicit key data on the assessment of each programme.</p>
<p>Respondents participated in 60–90-minute semi-structured interviews, which focused on their perceptions of the assessment diet across the whole degree programme. They gave informed, signed consent, following an explanation of the purpose and remit of the project. The ethical dimensions of the project included the usual conditions of confidentiality, anonymity and the right to withdraw at any stage, as well as adherence to data protection laws. All interviews were recorded, professionally transcribed, and analysed using the qualitative software, <italic>Atlas.ti</italic>.</p>
<p>The research is an example of multiple case study analysis (<xref ref-type="bibr" rid="bibr23-1469787412441285">Stake, 2006</xref>), exploring 14 whole degree programmes, focusing particularly on assessment patterns within the cases, and triangulating data from programme leaders and degree documents. Multiple case study analysis places more emphasis on distilling similarities and differences across cases than on the rich contextual detail of each case. In this small-scale study, the data were collected from programme leaders and documents, and did not draw on student voice or the experience of programme team members.</p>
<p>Data analysis was conducted according to qualitative research traditions; valuing the researcher as interpreter, and using the tools and techniques of coding, identifying themes and theory generation (<xref ref-type="bibr" rid="bibr7-1469787412441285">Cousin, 2008</xref>; <xref ref-type="bibr" rid="bibr8-1469787412441285">Ely, 1991</xref>; <xref ref-type="bibr" rid="bibr26-1469787412441285">Tesch, 1990</xref>). The researchers developed a coding system in response to ideas generated in the data, and drawing both from the literature on assessment and from evidence gathered in the field, particularly from participation in the ‘Transforming the experience of students through assessment’ (<xref ref-type="bibr" rid="bibr27-1469787412441285">TESTA, 2009–12</xref>) National Teaching Fellowship project. The qualitative software <italic>Atlas.ti</italic> allowed the researchers to cluster codes into themes more easily, visualize relationships between codes and enumerate the frequency of segments of conversation, leading to more focused theorizing about key issues in the data.</p>
<p>The analysis discussed in this article is a multiple case study of assessment patterns on 14 degree programmes in a single institution, but it reflects findings from a wider analysis of 20 programme cases from various subject disciplines in eight different universities. These additional cases have arisen through participation in the TESTA research project, and have had a student and team member dimension, but are as yet unpublished.</p>
</sec>
<sec id="section3-1469787412441285" sec-type="results">
<title>Findings</title>
<p>Three key themes arise from the data, which are applicable to the institution under study but may have resonance beyond it:</p>
<list id="list1-1469787412441285" list-type="order">
<list-item><p>written statements in public programme documents for quality assurance purposes send an implicit message to academics about what matters in assessment,</p></list-item>
<list-item><p>programme leaders feel constrained in their freedom to develop assessment at the local level in ways which impact on the inclusion of formative opportunities,</p></list-item>
<list-item><p>institutional conditions influence the practice of formative assessment.</p></list-item>
</list>
<sec id="section4-1469787412441285">
<title>Implicit messages from public programme documents</title>
<p>Programme documentation on the 14 programmes under study supports the view that ‘the grading function is over-emphasized and the learning function under-emphasized’ (<xref ref-type="bibr" rid="bibr2-1469787412441285">Black and William, 1998</xref>: 18). <xref ref-type="table" rid="table1-1469787412441285">Table 1</xref> shows the balance of summative to formative assessment represented in the definitive documents of programmes in this study. The headings ‘Volume of summative assessment’ and ‘Volume of formative assessment’ represent the number of assessment events that a student would expect to encounter over the course of a three-year degree.</p>
<table-wrap id="table1-1469787412441285" position="float">
<label>Table 1.</label>
<caption>
<p>Recorded summative-to-formative ratios (Source: validation documents)</p>
</caption>
<graphic alternate-form-of="table1-1469787412441285" xlink:href="10.1177_1469787412441285-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Programmes of study</th>
<th align="left">Volume of summative assessment</th>
<th align="left">Volume of formative assessment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Creative Writing</td>
<td>45</td>
<td>0</td>
</tr>
<tr>
<td>Childhood, Youth and Community</td>
<td>40</td>
<td>0</td>
</tr>
<tr>
<td>Social Care</td>
<td>41</td>
<td>0</td>
</tr>
<tr>
<td>BA Primary</td>
<td>33</td>
<td>0</td>
</tr>
<tr>
<td>Education Studies</td>
<td>36</td>
<td>0</td>
</tr>
<tr>
<td>History</td>
<td>45</td>
<td>0</td>
</tr>
<tr>
<td>Sport Studies</td>
<td>43</td>
<td>0</td>
</tr>
<tr>
<td>Social Work</td>
<td>32</td>
<td>0</td>
</tr>
<tr>
<td>Journalism</td>
<td>46</td>
<td>0</td>
</tr>
<tr>
<td>Law</td>
<td>33</td>
<td>0</td>
</tr>
<tr>
<td>American Studies</td>
<td>52</td>
<td>0</td>
</tr>
<tr>
<td>Media Studies</td>
<td>34</td>
<td>1</td>
</tr>
<tr>
<td>Business Management</td>
<td>47</td>
<td>0</td>
</tr>
<tr>
<td>Foundation Childhood Studies</td>
<td>39</td>
<td>0</td>
</tr>
<tr>
<td><bold>AVERAGE</bold></td>
<td><bold>40</bold></td>
<td><bold>0</bold></td>
</tr>
</tbody>
</table></table-wrap>
<p><xref ref-type="table" rid="table1-1469787412441285">Table 1</xref> demonstrates the extent of summative assessment and the absence of specific formative assessment occasions, with the ratio of summative to formative averaging 40:0. From our experience of working on the TESTA project, programme documents in at least eight other UK institutions contain similar statements about patterns of assessment, with rare and usually modular-level exceptions, which state formative requirements in the documentation.</p>
<p>The implicit message of the documentation is that lecturers and students should focus on the summative tasks that count towards their final degree classification:<disp-quote>
<p>In my experience, where we offer formative feedback or assessment, the uptake is really low . . . as a programme everything is based around summative assessment, so I think it is really difficult for students to see what the point of formative is. (Programme Leader A)</p>
</disp-quote></p>
<p>Among the 14 programme leaders interviewed, several had included formative assessment and feedback in the normal course of teaching, but most were wary of making it a course requirement, both for philosophical reasons of wanting students to be self-motivated and independent learners, and for more practical reasons related to enforcing non-marked elements of the course. As a result, formative assessment opportunities were voluntary and taken up only by a minority of students:<disp-quote>
<p>Students would do weekly tasks and we would then reflect on those and there would be a forum and they would send the material or ideas and we’d reflect on that. But that wasn’t officially part of the assessment and I found the consequence of it not being officially part of the diet being that a hard core did it and no more. (Programme Leader B)</p>
</disp-quote></p>
<p>The marking of summative assessment impacts how much or how little formative assessment is designed into the course, given that more summative tasks mean higher workloads. One programme leader quantified in detail the marking and moderating load of tutors on a course which attracts 60 students a year and contains nearly 50 summative tasks over the period of the degree:<disp-quote>
<p>Typically a member of staff who is maybe teaching five modules a semester will have to read at any particular assessment point . . . something between 300,000 and 0.5 million words. (Programme Leader C)</p>
</disp-quote></p>
<p>Given the high number of summative tasks, both academics and students are not keen on adding in more assessment to the mix, albeit formative. Programme leaders complained of over-assessment:<disp-quote>
<p>You end up assessing for assessment’s sake rather than thinking about what the assessment is for. (Programme Leader D)</p>
<p>They’re over-assessed. It’s not just here. I think they’re over-assessed generally . . . there’s just too much of it. (Programme Leader B)</p>
</disp-quote></p>
<p>One implication of the requirement that appears to suggest that we should specify <italic>only</italic> summative assessment in definitive documents is that formative assessment becomes less visible. It is seen as something discretionary, something which may or may not be included depending on the programme leader, module tutors, philosophy, workloads, student culture, and perceived content load of the programme. Summative assessment is re-inscribed as the thing that really matters, the most important part of taking a degree. With some reluctance, programme leaders regard formative assessment as an ‘extra’, which they cannot easily fit into their workloads and into the planned curriculum:<disp-quote>
<p>I have to say we’re finding it [formative assessment] more difficult as tutors, as the numbers grow on the courses, you know, and a lot of us now are thinking I can’t do this, I just cannot do this because it’s just so much extra time . . . (Programme Leader E)</p>
</disp-quote></p>
</sec>
<sec id="section5-1469787412441285">
<title>Factors constraining programme leaders in assessment design</title>
<p>Programme documents are intended to summarize the global philosophy, aims, selection of knowledge and curriculum content, pedagogy and assessment patterns across whole degree programmes. As such, these documents are the defensible symbols of validated programmes for quality assurance purposes. A tacit dimension of programme documentation is its capacity to be open enough for teaching teams to develop the curriculum and pedagogy responsively and independently. This approach holds in tension the controls of quality and the freedom of practitioners, acting as a reasonable means of ‘balancing institutional standardisation with departmental discretion’ (<xref ref-type="bibr" rid="bibr29-1469787412441285">Yorke, 1998</xref>: 102).</p>
<p>Universities, and academics within them, defend the right to be minimalist in what is prescribed about pedagogy and assessment, so that programme specifications are loose enough for development to occur, while also enshrining the principle of programme autonomy, as indicated in this response:<disp-quote>
<p>Policy and processes are kept at a fairly minimum level . . . there’s standardisation at a basic level but actually the programmes have quite a lot of control at the end of the day. (Programme Leader F)</p>
</disp-quote></p>
<p>In practice, however, there are differences between the ‘espoused theories’ of the university, and the ‘theories in use’ by academics, who find that ‘resolving problems of providing adequate formative assessment is beyond their power . . . it requires action at institutional level, and even political level’ (<xref ref-type="bibr" rid="bibr17-1469787412441285">Knight and Yorke, 2003</xref>: 44). In explaining the persistence of high summative assessment loads, one programme leader indicated that academics were cautious about making changes without official sanction:<disp-quote>
<p>We certainly haven’t had a message here to crank assessment up since I’ve been in post, and if anything, it’s been the opposite message. But I think to an extent tradition, maybe a sense of nervousness about whether people really mean it when they say ‘Stop assessing so much’, and maybe habit [keep us from changing it]. (Programme Leader C)</p>
</disp-quote></p>
<p>Another speculated that a role which has been described in the literature as ‘largely ill-defined and unsupported within higher education’ (<xref ref-type="bibr" rid="bibr14-1469787412441285">Johnston and Westwood, 2007</xref>: 6) contributes to perpetuating the status quo:<disp-quote>
<p>One can’t actually follow it through in a systematic, ordered way, because the authority isn’t there to actually do it. (Programme Leader G)</p>
</disp-quote></p>
<p>The discretion of programme teams to address the summative–formative imbalance is also constrained by under-conceptualization of assessment in theory and practice. In their comprehensive review of formative assessment, Black and William found that it ‘was not well understood by teachers and is weak in practice’(<xref ref-type="bibr" rid="bibr2-1469787412441285">1998</xref>: 20). Within the higher education context, Knight and Yorke found that ‘assessment design and instrumentation are often ad hoc and lack a theoretical base’(<xref ref-type="bibr" rid="bibr17-1469787412441285">2003</xref>: 38). Data from our study resonate with these findings in demonstrating a lack of shared understanding of what constitutes formative assessment, exemplified in the following quotes:<disp-quote>
<p>I don’t think it’s explicitly defined anywhere. (Programme Leader H)</p>
<p>Every assignment has a formative element to it . . . so it doesn’t matter what the mark, they will have formative feedback. (Programme Leader I)</p>
<p>That mark doesn’t count, that’s the only time when it is a genuinely formative assessment and doesn’t contribute to their module mark. (Programme Leader J)</p>
<p>As far as I’m concerned Yr 1 is formative because it doesn’t count to the degree outcome. (Programme Leader K)</p>
<p>It’s formative in the sense that it may be a narrower, more detailed study of an aspect of the module. (Programme Leader L)</p>
<p>When we have formative assessment, it’s a formative stage in a summative process. (Programme Leader A)</p>
</disp-quote></p>
<p>These differences in perception were reflected in varied approaches to giving formative feedback on drafts, in spite of university-wide approval of the draft–feedback–redraft cycle in final-year projects, widely regarded as pedagogically sound. Several programme leaders were advocates of commenting on drafts while others felt it compromised degree standards. For some, comments on drafts are regarded as a normal part of pedagogy:<disp-quote>
<p>We will build a formative period in week four or week five . . . we will actually do a ‘Yeah, submit your draft’ and we will spend half a session actually looking at drafts, get the students to look at each other’s drafts, talk about it, do a presentation, have the chat . . . (Programme Leader F)</p>
</disp-quote></p>
<p>For others, marking drafts was unacceptable for ethical reasons, compromising the original efforts of the student, and potentially, leading to claims of unfair advantage:<disp-quote>
<p>What we try not to do is have people sending in the whole assignment two weeks before and saying read through that, because essentially they’re asking you to pre-mark it and we don’t do that. So you’ve kind of got an ethical ‘where’s the line?’ question. (Programme Leader E)</p>
</disp-quote></p>
<p>Some students were similarly nervous about this, with an eye on unfair advantage in grades, rather than the process of learning:<disp-quote>
<p>One which surprised me fantastically over the last two weeks was that giving drafts in was cheating because you’re getting extra help, and this is from a second year. They hadn’t picked up a culture at all of the fact that it was helping them to dialogue in a critical way. (Programme Leader M)</p>
</disp-quote></p>
</sec>
<sec id="section6-1469787412441285">
<title>Institutional conditions inhibiting formative assessment</title>
<p>In the literature, there is growing concern that the modular system has fragmented degree coherence, increasing summative while decreasing formative assessment, and escalating marker workloads. In this study programme leaders’ comments corroborated the view that greater unitization has led to summative assessment at short intervals, thereby reducing opportunities for formative assessment (<xref ref-type="bibr" rid="bibr17-1469787412441285">Knight and Yorke, 2003</xref>: 48). One programme leader criticized the modular structure as anti-developmental:<disp-quote>
<p>I don’t think there is any benefit of the modular system . . . the benefit from assessment should be development and the modular system works against development. (Programme Leader M)</p>
</disp-quote></p>
<p>Some felt that modules led students to compartmentalize learning, reducing their capacity to make connections across modules:<disp-quote>
<p>It’s almost like we work in silos at the moment. (Programme Leader E)</p>
<p>I’m not comfortable with the modular system because the students tick off modules. You can’t have a professional who hasn’t actually put them all together. (Programme Leader G)</p>
<p>I find that certain modules just don’t promote learning. You learn a topic, and it’s quite narrow and specific, and then you really do just move on. (Programme Leader M)</p>
</disp-quote></p>
<p>Others described the deleterious effects of modularity on deep learning:<disp-quote>
<p>I am concerned about the depth of a 15 credit module. (Programme Leader D)</p>
<p>The modular system encourages students to read for assessment rather than to read for their disciplines. That’s the big loss. (Programme Leader C)</p>
</disp-quote></p>
<p>Several programme leaders indicated that current levels of summative assessment were unsustainable.</p>
<p><disp-quote>
<p>We cannot keep assessing the way we have done when the numbers are going to be so big for some points of the university. (Programme Leader E)</p>
</disp-quote></p>
<p>There was one voice of dissent about ‘over-assessment’, critiquing the perception that this was carved in institutional stone, and underlining the responsibility programme leaders have for determining assessment volumes:<disp-quote>
<p>Students may feel there’s too much of it and it’s bunched. I don’t believe that’s the case . . . You don’t have to have two assignments per module if you don’t want them, if they don’t suit your programme. There’s nothing in the current regulations that makes you do that. (Programme Leader L)</p>
</disp-quote></p>
<p>Contrastingly, another programme leader commented that workloads inhibited innovation:<disp-quote>
<p>A very significant chunk of staff across the university are very engaged and interested in new forms of assessment, but I think one of the reasons other staff aren’t is that they’re just overwhelmed and just don’t feel they have the time to invest in exploring new ways of doing things. (Programme Leader C)</p>
</disp-quote></p>
<p>In the view of many, the combination of the modular structure and, if not designed otherwise, burdensome summative assessment which fails to address students’ learning needs is a call to a new structure which provides more opportunities for reflection, and for both deep and ‘slow learning’ (<xref ref-type="bibr" rid="bibr6-1469787412441285">Claxton, 1998</xref>):<disp-quote>
<p>Within bigger modules [we need to] begin to build in more space for students to think and reflect with slightly fewer assessment points, rather than simply doubling up fifteen credit assessments in a thirty credit package. To think a bit smarter and to open up a bit more space, which maybe would allow staff to deepen the feedback and at the same time also deepen the learning experience for the student. (Programme Leader C)</p>
</disp-quote></p>
</sec></sec>
<sec id="section7-1469787412441285" sec-type="discussion|conclusions">
<title>Discussion and conclusion</title>
<p>Assessment literature underlines the value of formative assessment (<xref ref-type="bibr" rid="bibr2-1469787412441285">Black and William, 1998</xref>; <xref ref-type="bibr" rid="bibr4-1469787412441285">Boud, 2000</xref>; <xref ref-type="bibr" rid="bibr11-1469787412441285">Gibbs and Simpson, 2004</xref>; <xref ref-type="bibr" rid="bibr18-1469787412441285">Nicol and McFarlane-Dick, 2006</xref>) but it rarely engages with the systemic, structural and institutional frameworks which constrain the practice of formative assessment. This research has sought to show the interplay between official discourses of assessment, mainly enshrined in programme documentation and quality assurance procedures, and programme leaders’ interpretations of them – in one case study institution. It has examined the implications of the official discourses of assessment in relation to their interpretation in practice, identifying three main explanations for why programme leaders persist with high-summative–low-formative patterns of assessment. These are: firstly, the influence of programme documents on assessment design, particularly the invisibility of required formative assessment; secondly, workload pressures, pedagogic uncertainties and anxieties about the remit of the programme leader’s role; and finally, the influence of the modular structure, with its compressed time structure, on assessment patterns.</p>
<p>The distinctive question that the study has explored is the extent to which programme documents constrain the practice of formative assessment, and bolster high ratios of summative assessment. Evidence from the study showed in many instances that the statement of intent to complete an average of 40 summative tasks and zero formative tasks played out in practice. This was not because teachers were negligent, passive, lacking in commitment and passion, or completely unaware of the value of formative assessment. In part, it was an acceptance of the power of the ‘system’, the validation framework, in conveying an implicit message that summative assessment is pre-eminent. In spite of official verbal messages from management at both faculty/department level and above, programme leaders in this study have, with few exceptions, seemed reluctant to change the system. The finding that very little formative assessment occurs, and when it occurs is often not taken up by students, has been verified across a wide range of programmes in different kinds of UK universities participating in the expansion of the TESTA National Teaching Fellowship Project.</p>
<p>A second theme in the data provided further explanation for the apparent lack of formative assessment on many programmes in relation to validation documentation. The freedom afforded to programmes to steer the pedagogy locally makes the assumption that the theory and practice of assessment design is within the gift of programme leaders. In this study, a combination of an open agenda for developing pedagogy, uncertainties about how and why to design programmes containing formative assessment, academic workloads and anxiety about programme leaders’ authority all contributed to the overall dearth of formative assessment. The findings demonstrated diverse understandings among programme leaders about the role and value of formative assessment. For some, the benefits of promoting more formative assessment across the whole programme were simply not clear enough, and the effort of getting instrumental, marks-driven students to undertake it, and persuading their colleagues of its virtue, were sufficient deterrents. The perception and reality of high summative marking workloads depleted the energy and creativity of programme leaders and their teams, preventing them from improving the assessment design on programmes. The programme documents conspired with these complex contextual factors by signalling that satisfactory completion of summative assessment points was ‘good enough’.</p>
<p>A final explanatory theme suggested that the modular degree structure, with compressed 12-week units, and a linked credit system, raised the number of summative assessments, and diminished opportunities for formative assessment, both within and across modules. Summative assessment’s necessary emphasis on reliability – second or double marking, moderation and external examining – exacerbated workloads, impeding the timely return of feedback. Within this study, there was little deviation from the formulaic pattern of two summative assessment tasks per module, with some programme leaders hoping that the first set of feedback might play a notionally formative function. If and when the first assessment task and its feedback is returned in time to provide developmental feedback for the second task, the grade has a significant chance of distracting the student from paying attention to the feedback. In some instances, the summative task may be so specific as to constitute a ‘one-off’ from which students struggle to transfer the learning.</p>
<p>All research has limitations. This is an exploratory small-scale study based on 14 degree programme cases in one university in the UK. The purpose of a case study was to elucidate the complex set of relationships and social behaviours in one particular setting, here located as the assessment environment on degree programmes at one university. The paradox of case study research is that, while not being generalizable, it does convey implicit inferences, often illuminating contexts beyond the case study in question. This case study does not make grand claims, but it does pose questions about the extent to which quality assurance processes influence academic leaders, and the impact of this on assessment patterns, and, by extension, student learning. One sampling limitation in this multiple case analysis is that the data were collected from official documents and programme leaders only, without drawing on the student or programme team perspective. Future research might explore the extent to which programme leaders and/or their teams in other institutions are influenced by quality assurance regimes in ways that impact assessment patterns, particularly the balance of formative to summative assessment. There is also a rich vein of research to be mined in understanding the relative value that students assign to graded summative assessment, and to formative assessment and feedback.</p>
<p>The challenge confronting the pedagogically mindful, and universities interested in redressing the apparent imbalance between summative and formative assessment, is how to embed and assign more value to formative assessment on degree programmes. Developing closer synergies and working relationships between the pedagogic enhancement functions in a university and those within universities who assure the quality of degrees would enable more dialogue about the potential impact of quality procedures on assessment practice. Altering programme documentation to reflect an institutional commitment to formative assessment may be a potential lever of change. But transforming assessment design to include more genuine formative assessment that students take up will need multi-level strategies to address the academic structure of degrees, high summative and low formative assessment ratios, academic workloads, and gaps in awareness of assessment theory and practice among disciplinary academics and within programme teams. More substantially, it will require a re-conceptualization among programme leaders and their teams to think more programmatically about whole-degree assessment patterns, and to devise strategies to tilt the balance in favour of more assessment <italic>for</italic> learning.</p>
</sec>
</body>
<back>
<ack><p>Thanks to Graham Gibbs for encouraging us to publish on this topic, and to Alan Penny for his probing and thoughtful comments on an earlier draft. Thanks also to the reviewers for their comments which have sharpened the paper, and to Lynne P. Baldwin, editor of the journal, for her extremely helpful suggestions about addressing the reviewers’ comments.</p></ack>
<bio>
<title>Biographical notes</title>
<p>Tansy Jessop is a Senior Fellow in Learning and Teaching at the University of Winchester. She is the leader of the Higher Education Academy funded Transforming the Experience of Students through Assessment (TESTA) National Teaching Fellowship Project addressing programme-wide assessment patterns in four universities. She has previously published on social research methods, teacher development, learning spaces and the minority student experience in higher education. <italic>Address</italic>: Learning and Teaching Development Unit, University of Winchester, West Hill, Winchester SO22 4NR, UK. [email: <email>Tansy.Jessop@winchester.ac.uk</email>]</p>
<p>Nicole McNab is a researcher in the Learning and Teaching Development Unit at Winchester. She has conducted student experience research on programme assessment, plagiarism and anonymous marking. She manages all the statistical data processing and analysis for the TESTA project. Her research interests are in academic misconduct, marker variation, and student reflection and achievement. <italic>Address</italic>: Learning and Teaching Development Unit, University of Winchester, West Hill, Winchester SO22 4NR, UK. [email: <email>Nicole.McNab@winchester.ac.uk</email>]</p>
<p>Laura Gubby is a University Instructor in the Department of Sport Science, Tourism and Leisure at Canterbury Christ Church University. She has had an active part in the TESTA project from the start, having worked part-time as a Research Assistant in the Learning and Teaching Development Unit at the University of Winchester. Her research interests include the use and perceptions of learning spaces, and assessment. <italic>Address</italic>: Canterbury Christ Church University, North Holmes Road, Canterbury, Kent CT1 1QU, UK. [email: <email>Laura.Gubby@canterbury.ac.uk</email>]</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Becker</surname><given-names>H</given-names></name>
</person-group> (<comment>with B Geer and EC Hughes</comment>) (<year>1968</year>) <source>Making the Grade: The Academic Side of College Life</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley. Second edition with new introduction</publisher-name> (<year>1995</year>).</citation>
</ref>
<ref id="bibr2-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Black</surname><given-names>P</given-names></name>
<name><surname>William</surname><given-names>D</given-names></name>
</person-group> (<year>1998</year>) <article-title>Assessment and classroom learning</article-title>. <source>Assessment in Education: Principles, Policy and Practice</source> <volume>5</volume>(<issue>1</issue>): <fpage>7</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr3-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bloxham</surname><given-names>S</given-names></name>
<name><surname>Boyd</surname><given-names>P</given-names></name>
</person-group> (<year>2007</year>) <article-title>Planning a programme assessment strategy</article-title>. In: <source>Developing Effective Assessment in Higher Education</source>. <publisher-loc>Berkshire</publisher-loc>: <publisher-name>Open University Press, pp</publisher-name>. <fpage>157</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr4-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Boud</surname><given-names>D</given-names></name>
</person-group> (<year>2000</year>) <article-title>Sustainable assessment: Rethinking assessment for the learning society</article-title>. <source>Studies in Continuing Education</source> <volume>22</volume>(<issue>2</issue>): <fpage>151</fpage>–<lpage>67</lpage>.</citation>
</ref>
<ref id="bibr5-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bryman</surname><given-names>A</given-names></name>
</person-group> (<year>2001</year>) <source>Social Research Methods</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Claxton</surname><given-names>G</given-names></name>
</person-group> (<year>1998</year>) <source>Hare Brain, Tortoise Mind</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Fourth Estate</publisher-name>.</citation>
</ref>
<ref id="bibr7-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cousin</surname><given-names>G</given-names></name>
</person-group> (<year>2008</year>) <source>Researching Learning in Higher Education</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr8-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ely</surname><given-names>M</given-names></name>
</person-group> (<year>1991</year>) <source>Doing Qualitative Research: Circles within Circles</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Falmer</publisher-name>.</citation>
</ref>
<ref id="bibr9-1469787412441285">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Gibbs</surname><given-names>G</given-names></name>
<name><surname>Dunbar-Goddet</surname><given-names>H</given-names></name>
</person-group> (<year>2007</year>) <article-title>The effects of programme assessment environments on student learning</article-title>. <comment>Higher Education Academy. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.heacademy.ac.uk/assets/York/documents/ourwork/research/gibbs_0506.pdf">http://www.heacademy.ac.uk/assets/York/documents/ourwork/research/gibbs_0506.pdf</ext-link></comment> (<access-date>accessed 19 April 2011</access-date>).</citation>
</ref>
<ref id="bibr10-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gibbs</surname><given-names>G</given-names></name>
<name><surname>Dunbar-Goddet</surname><given-names>H</given-names></name>
</person-group> (<year>2009</year>) <article-title>Characterising programme-level assessment environments that support learning</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>34</volume>(<issue>4</issue>): <fpage>481</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr11-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gibbs</surname><given-names>G</given-names></name>
<name><surname>Simpson</surname><given-names>C</given-names></name>
</person-group> (<year>2004</year>) <article-title>Conditions under which assessment supports students' learning</article-title>. <source>Learning and Teaching in Higher Education</source> <volume>1</volume>(<issue>1</issue>): <fpage>3</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr12-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gipps</surname><given-names>C</given-names></name>
</person-group> (<year>1999</year>) <article-title>Socio-cultural aspects of assessment</article-title>. <source>Review of Research in Education</source> <volume>24</volume>: <fpage>355</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr13-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hattie</surname><given-names>J</given-names></name>
</person-group> (<year>2009</year>) <source>Visible Learning: A Synthesis of Over 800 Meta-Analyses Relating to Achievement</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr14-1469787412441285">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Johnston</surname><given-names>V</given-names></name>
<name><surname>Westwood</surname><given-names>J</given-names></name>
</person-group> (<year>2007</year>) <article-title>Developing a framework for the professional development of programme leaders</article-title>. <comment>Report for the Higher Education Academy. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.heacademy.ac.uk/assets/York/documents/johnston_final_report.pdf">http://www.heacademy.ac.uk/assets/York/documents/johnston_final_report.pdf</ext-link></comment> (<access-date>accessed 18 April 2011</access-date>).</citation>
</ref>
<ref id="bibr15-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Knight</surname><given-names>PT</given-names></name>
</person-group> (<year>2000</year>) <article-title>The value of a programme-wide approach to assessment</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>25</volume>(<issue>3</issue>): <fpage>237</fpage>–<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr16-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Knight</surname><given-names>PT</given-names></name>
</person-group> (<year>2001</year>) <article-title>Complexity and curriculum: A process approach to curriculum-making</article-title>. <source>Teaching in Higher Education</source> <volume>6</volume>(<issue>3</issue>): <fpage>369</fpage>–<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr17-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Knight</surname><given-names>PT</given-names></name>
<name><surname>Yorke</surname><given-names>M</given-names></name>
</person-group> (<year>2003</year>) <source>Assessment, Learning and Employability</source>. <publisher-loc>Maidenhead</publisher-loc>: <publisher-name>Open University Press</publisher-name>.</citation>
</ref>
<ref id="bibr18-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Nicol</surname><given-names>DJ</given-names></name>
<name><surname>McFarlane-Dick</surname><given-names>D</given-names></name>
</person-group> (<year>2006</year>) <article-title>Formative assessment and self-regulated learning: A model and seven principles of good feedback practice</article-title>. <source>Studies in Higher Education</source>. <volume>31</volume>(<issue>2</issue>): <fpage>199</fpage>–<lpage>218</lpage>.</citation>
</ref>
<ref id="bibr19-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Orrell</surname><given-names>J</given-names></name>
</person-group> (<year>2006</year>) <article-title>Feedback on learning achievement: Rhetoric and reality</article-title>. <source>Teaching in Higher Education</source> <volume>11</volume>(<issue>4</issue>): <fpage>441</fpage>–<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr20-1469787412441285">
<citation citation-type="web">
<collab>QAA</collab> (<year>2008</year>) <article-title>The history of QAA and evaluation in UK higher education</article-title>. In: <source>QAA Self Evaluation</source>. <publisher-name>Quality Assurance Agency for Higher Education</publisher-name>, pp. <fpage>17</fpage>–<lpage>19</lpage>. <comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://qaa.ac.uk/international/ENQA/SED08/default.asp#p5">https://qaa.ac.uk/international/ENQA/SED08/default.asp#p5</ext-link></comment> (<access-date>accessed 2 March 2011</access-date>).</citation>
</ref>
<ref id="bibr21-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rust</surname><given-names>C</given-names></name>
</person-group> (<year>2000</year>) <article-title>Opinion piece: A possible student-centred assessment solution to some of the current problems of modular degree programmes</article-title>. <source>Active Learning in Higher Education</source> <volume>1</volume>: <fpage>126</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr22-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sadler</surname><given-names>DR</given-names></name>
</person-group> (<year>2007</year>) <article-title>Perils in the meticulous specification of goals and assessment criteria</article-title>. <source>Assessment in Education: Principles, Policy and Practice</source> <volume>14</volume>(<issue>3</issue>): <fpage>387</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr23-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stake</surname><given-names>R</given-names></name>
</person-group> (<year>2006</year>) <source>Multiple Case Study Analysis</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr24-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taras</surname><given-names>M</given-names></name>
</person-group> (<year>2002</year>) <article-title>Using assessment for learning and learning from assessment</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>27</volume>(<issue>6</issue>): <fpage>501</fpage>–<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr25-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Taras</surname><given-names>M</given-names></name>
</person-group> (<year>2008</year>) <article-title>Summative and formative assessment: Perceptions and realities</article-title>. <source>Active Learning in Higher Education</source> <volume>9</volume>(<issue>2</issue>): <fpage>172</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr26-1469787412441285">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tesch</surname><given-names>R</given-names></name>
</person-group> (<year>1990</year>) <source>Qualitative Research: Analysis Types and Software Tools</source>. <publisher-loc>Basingstoke</publisher-loc>: <publisher-name>Falmer</publisher-name>.</citation>
</ref>
<ref id="bibr27-1469787412441285">
<citation citation-type="web">
<collab>TESTA</collab> (<year>2009–12</year>) <article-title>Transforming the experience of students through assessment</article-title>. <comment>Higher Education Academy National Teaching Fellowship Project. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.testa.ac.uk">www.testa.ac.uk</ext-link></comment> (<access-date>accessed 15 April 2011</access-date>).</citation>
</ref>
<ref id="bibr28-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Torrance</surname><given-names>H</given-names></name>
</person-group> (<year>2007</year>) <article-title>Assessment <italic>as</italic> learning? How the use of explicit learning objectives, assessment criteria and feedback in post-secondary education and training can come to dominate learning</article-title>. <source>Assessment in Education</source> <volume>14</volume>(<issue>3</issue>): <fpage>281</fpage>–<lpage>94</lpage>.</citation>
</ref>
<ref id="bibr29-1469787412441285">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yorke</surname><given-names>M</given-names></name>
</person-group> (<year>1998</year>) <article-title>The management of assessment in higher education</article-title>. <source>Assessment and Evaluation in Higher Education</source> <volume>23</volume>(<issue>2</issue>): <fpage>101</fpage>–<lpage>16</lpage>.</citation>
</ref></ref-list>
</back>
</article>