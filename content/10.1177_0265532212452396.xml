<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">LTJ</journal-id>
<journal-id journal-id-type="hwp">spltj</journal-id>
<journal-title>Language Testing</journal-title>
<issn pub-type="ppub">0265-5322</issn>
<issn pub-type="epub">1477-0946</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0265532212452396</article-id>
<article-id pub-id-type="publisher-id">10.1177_0265532212452396</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Scoring with the computer: Alternative procedures for improving the reliability of holistic essay scoring</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Attali</surname><given-names>Yigal</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Lewis</surname><given-names>Will</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Steier</surname><given-names>Michael</given-names></name>
</contrib>
<aff id="aff1-0265532212452396">Educational Testing Service, USA</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0265532212452396">Yigal Attali, Educational Testing Service, Rosedale Rd., MS-10-R, Princeton, NJ 08541, USA. Email: <email>yattali@ets.org</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>1</issue>
<fpage>125</fpage>
<lpage>141</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Automated essay scoring can produce reliable scores that are highly correlated with human scores, but is limited in its evaluation of content and other higher-order aspects of writing. The increased use of automated essay scoring in high-stakes testing underscores the need for human scoring that is focused on higher-order aspects of writing. This study experimentally evaluated several alternative procedures for eliciting distinct human scores and improving their reliability. Essays written in response to the argument and issue tasks of the Analytical Writing measure of the GRE General Test were scored by experienced raters under different conditions. Criteria for evaluation included inter-rater agreement, agreement with machine scores, and cross-task reliability. First, the use of a modified scoring rubric that focused on higher-order writing skills increased the reliability for one type of task but decreased it for another. Second, scoring in batches of similar length essays did not have any effect on scores. Third, scoring with available automated essay scores increased reliability of human scores, but also increased their similarity with automated scores. Finally, the use of a more refined 18-point scoring scale significantly increased reliability.</p>
</abstract>
<kwd-group>
<kwd>Automated scoring</kwd>
<kwd>essay writing assessment</kwd>
<kwd>reliability</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Performance assessments necessarily involve subjective judgments. This fact accords a central place to the rater and the rating process, and nowhere more so than for direct writing assessments. This inherent subjectivity can be at odds with the concept of psychometric reliability (<xref ref-type="bibr" rid="bibr26-0265532212452396">Moss, 1994</xref>). In the context of large-scale, high-stakes writing assessments in particular, a primary goal is to ensure that raters think similarly enough about what constitutes a high- or low-quality student response to achieve reasonable consistency of scores across ratings.</p>
<p>Achieving this goal is a continuous challenge, as numerous studies of rater behavior have shown substantial differences in the way in which raters interpret scoring criteria (see, e.g., <xref ref-type="bibr" rid="bibr17-0265532212452396">Engelhard, 1994</xref>; <xref ref-type="bibr" rid="bibr7-0265532212452396">Bachman, Lynch &amp; Mason, 1995</xref>; <xref ref-type="bibr" rid="bibr25-0265532212452396">Lumley &amp; McNamara, 1995</xref>; <xref ref-type="bibr" rid="bibr32-0265532212452396">Weigle, 1998</xref>; <xref ref-type="bibr" rid="bibr18-0265532212452396">Engelhard &amp; Myford, 2003</xref>, <xref ref-type="bibr" rid="bibr14-0265532212452396">Eckes, 2008</xref>). Moreover, rater training has not been able to eliminate completely these differences (<xref ref-type="bibr" rid="bibr25-0265532212452396">Lumley &amp; McNamara, 1995</xref>; <xref ref-type="bibr" rid="bibr32-0265532212452396">Weigle, 1998</xref>, <xref ref-type="bibr" rid="bibr33-0265532212452396">1999</xref>; <xref ref-type="bibr" rid="bibr9-0265532212452396">Barrett, 2001</xref>; <xref ref-type="bibr" rid="bibr15-0265532212452396">Elder, Knoch, Barkhuizen, &amp; von Randow, 2005</xref>).</p>
<p>As a result, reliability of large-scale essay writing assessments is mediocre relative to the time required to write essays, typically 30–45 minutes. <xref ref-type="bibr" rid="bibr11-0265532212452396">Breland, Camp, Jones, Morris, &amp; Rock (1987)</xref> conducted an extensive reliability study of essay ratings. Examinees wrote six essays in three modes of writing, and each essay was scored by three experienced raters. The reliability estimate for a test composed of two essays (in a single mode) was .59 when each essay was scored by one rater, and .70 when each essay was scored by two raters. <xref ref-type="bibr" rid="bibr10-0265532212452396">Breland, Bridgeman, and Fowles (1999)</xref> summarized the findings of reliability studies conducted before the <xref ref-type="bibr" rid="bibr11-0265532212452396">Breland et al. (1987)</xref> study, and found that the mean reliability estimates for two double-rated essay examinations was .71. They also report on unpublished results from the writing assessment of the Graduate Management Admissions Test (GMAT), which consist of two tasks. The mean reliability estimate for a two-essay, double-rated assessment was .71.</p>
<p>The relatively low reliability and the high cost of human essay rating have led to a growing interest in the application of automated natural language processing techniques for the development of automated essay scoring (AES) as a supplement to human scoring of essays. In recent years, several high-stakes graduate admissions programs have implemented AES in the operational scoring of their writing assessments. The first of these programs was the GMAT®, which started to use e-rater (<xref ref-type="bibr" rid="bibr12-0265532212452396">Burstein, Kukich, Wolff, Lu, &amp; Chodorow, 1998</xref>) in 1999. In 2006, The GMAT transitioned to Intellimetric (<xref ref-type="bibr" rid="bibr16-0265532212452396">Elliot, 2001</xref>), when the GMAT contract changed from ETS to ACT. Since October of 2008, a version of e-rater based on e-rater V.2 (<xref ref-type="bibr" rid="bibr6-0265532212452396">Attali &amp; Burstein, 2006</xref>) is used operationally as part of the scoring process for the GRE Issue and Argument prompts. E-rater has also been used for scoring TOEFL independent prompts since July of 2009, and for TOEFL integrated prompts since October of 2010. Finally, the Intelligent Essay Assessor™ (IEA) by Knowledge Analysis Technologies™ (<xref ref-type="bibr" rid="bibr23-0265532212452396">Landauer, Laham, &amp; Foltz, 2003</xref>) is deployed in the Pearson Test of English since 2009.</p>
<p>The operational implementation of AES was performed in all cases by replacing one human rater with AES, without changing the scoring rubrics of human raters. In other words, the automated score is seen as interchangeable with the human score. There is indeed empirical support for the close resemblance between human and automated scores. Based on a sample of 2000 sixth- to twelfth-grade students, each of whom wrote two essays, <xref ref-type="bibr" rid="bibr6-0265532212452396">Attali and Burstein (2006)</xref> estimated the true-score correlations between e-rater and human essay scores to be .97. In other words, the alternate-form correlations between human and machine scores (e.g. correlation between human score of essay 1 and e-rater score of essay 2) were almost the same as the alternate-form reliability of the human scores (correlation between human score of essay 1 and human score of essay 2). Similarly, based on 5000 computer-based TOEFL (the previous version of the current iBT TOEFL) examinees who repeated the test within a few weeks, <xref ref-type="bibr" rid="bibr2-0265532212452396">Attali (2007)</xref> estimated the true-score correlations between e-rater and human essay scores to be again .97.</p>
<p>Nevertheless, it is clear that AES is limited in its evaluation of higher-order aspects of writing, such as the quality of ideas, their development, and organization. For example, AES evaluates content and ideas mostly with ‘bag-of-words’ approaches (<xref ref-type="bibr" rid="bibr22-0265532212452396">Landauer, Foltz, &amp; Laham, 1998</xref>; <xref ref-type="bibr" rid="bibr6-0265532212452396">Attali &amp; Burstein, 2006</xref>; <xref ref-type="bibr" rid="bibr3-0265532212452396">Attali, 2011a</xref>). Humans can assess these higher-order aspects more easily simply because they can comprehend the text. The major premise of this study is that it is not reasonable to expect and strive for AES that is indistinguishable from human scoring (<xref ref-type="bibr" rid="bibr5-0265532212452396">Attali, 2012</xref>). The advances in AES and its operational implementation underscore the need for a ‘division of labor’ between human and machine assessment that takes into account the relative strengths of both methods. By encouraging raters to focus on higher-order aspects of writing it might be possible to achieve (a) higher reliability of human scoring, (b) lower correlations between human and machine scores, and consequently (c) higher reliability of combined human and machine scores.</p>
<p>This paper explores the effects of several methods intended to encourage raters to focus their attention on higher-order aspects of writing. The first method was the use of a modified scoring rubric that emphasizes higher-order aspects of the original rubric. Although this approach seems straightforward, past research suggests that raters find it difficult to focus on a certain trait while ignoring others (<xref ref-type="bibr" rid="bibr21-0265532212452396">Huot, 1990</xref>). For example, <xref ref-type="bibr" rid="bibr24-0265532212452396">Lee, Gentile, and Kantor (2008)</xref> found very high correlations (in the 80s) between trait scores (organization, development, vocabulary, sentence variety, grammar/usage, and mechanics), higher than their estimated reliabilities. Therefore, to help raters ignore lower-order aspects of the essay, the information that raters were asked to ignore was emphasized or revealed in different ways. First, the order of essays was manipulated to purposefully group together essays that are similar in shallow characteristics. In particular, raters were asked to score essays in batches that were composed of similar-length essays. Essay length is a strong predictor of human scores (e.g. <xref ref-type="bibr" rid="bibr24-0265532212452396">Lee et al., 2008</xref>, report correlations of .90 between holistic scores and essay length). This result may be a natural side effect of the need to provide supporting detail (<xref ref-type="bibr" rid="bibr29-0265532212452396">Powers, 2005</xref>), but the usual random ordering of essays for scoring may make it harder for raters to take into account the differences in quality of essays that are similar in length. Second, in an even more explicit effort to help raters ignore lower order aspects of writing, the e-rater score of the essay was shown to the raters. In both cases, the purpose of the manipulations was explained to the raters as part of the goal of focusing on higher-order aspects of writing.</p>
<p>This study evaluated an additional technical modification to the rating process, one that directly addresses the reliability of ratings. Essay ratings typically use a small number of categories that correspond to the descriptor levels in the scoring rubrics. Many large-scale assessment programs use a six-point scale (<xref ref-type="bibr" rid="bibr34-0265532212452396">Weigle, 2002</xref>, p. 123). <xref ref-type="bibr" rid="bibr1-0265532212452396">Alderson, Clapham, and Wall (1995)</xref> recommend using scales ‘with no more than about seven points, as it is difficult to make much finer distinctions’ (p. 111). However, to our knowledge there is no research on the limits of rater distinctions in essay ratings. Peculiarly, during training raters use both benchmarks – clear examples of a score point, and rangefinders – responses exemplifying the range of each score point (<xref ref-type="bibr" rid="bibr8-0265532212452396">Baldwin, Fowles, &amp; Livingston, 2005</xref>, p. 13). That is, rangefinders are essays that may be ‘low’ or ‘high’ for a specific score category. If raters are able to distinguish between at least three levels of a score point (low, typical, and high), using a finer grained score scale would increase the reliability of scores. In this study, a six-point score scale was augmented to 18 score points by defining a low and high level for each category.</p>
<p>An experimental design was used in this study to investigate the different manipulations – higher-order versus regular rubric, ordering of essays by essay length versus random ordering, and availability of e-rater scores. Professional raters, highly familiar with the rubrics and type of tasks they were asked to score, rated essays in different conditions. Examinees wrote two essays each, and each rater scored both essays of each examinee. The effects of the different scoring conditions were evaluated with respect to the following outcomes: relation between ratings of the same essay (inter-rater agreement), relation between human and e-rater scores, and relation between scores of different essays written by the same student. In addition, the effect of using the augmented score scale was investigated by comparing original (augmented) scores with rounded scores with respect to inter-rater agreement and alternate-form reliability.</p>
<sec id="section1-0265532212452396" sec-type="methods">
<title>Method</title>
<sec id="section2-0265532212452396">
<title>Writing tasks</title>
<p>The Analytical Writing measure of the GRE General Test comprises two essay writing tasks. In the issue task, the student is asked to discuss and express his or her perspective on a topic of general interest. In the argument task, a brief passage is presented in which the author makes a case for some course of action or interpretation of events by presenting claims backed by reasons and evidence. The student’s task is to discuss the logical soundness of the author’s case by critically examining the line of reasoning and the use of evidence.</p>
<p>The two tasks are scored by human raters using a holistic scoring rubric with six points, and since October of 2008, e-rater is used operationally as part of the scoring process of both tasks. E-rater does not, however, directly contribute to essay scores. Instead, one human rating is obtained for the essay. If the difference between this rating and the e-rater essay score does not exceed a .5 threshold, the final essay score is equal to the human score. If this threshold is exceeded, a second human rater provides an additional rating, and the final essay score is the average of the two human ratings. Finally, the scores for the two tasks are averaged and only one writing score is reported.</p>
</sec>
<sec id="section3-0265532212452396">
<title>Scoring rubrics</title>
<p>The GRE scoring rubrics emphasize ideas, development and organization, word choice, sentence fluency, and conventions, as the following description of a typical high-scored ‘outstanding’ issue response shows (<xref ref-type="bibr" rid="bibr19-0265532212452396">GRE, 2011</xref>):</p>
<list id="list1-0265532212452396" list-type="bullet">
<list-item><p>articulates a clear and insightful position on the issue in accordance with the assigned task;</p></list-item>
<list-item><p>develops the position fully with compelling reasons and/or persuasive examples;</p></list-item>
<list-item><p>sustains a well-focused, well-organized analysis, connecting ideas logically;</p></list-item>
<list-item><p>conveys ideas fluently and precisely, using effective vocabulary and sentence variety;</p></list-item>
<list-item><p>demonstrates facility with the conventions of standard written English (i.e. grammar, usage and mechanics), but may have minor errors.</p></list-item></list>
<p>A typical ‘outstanding’ argument essay is described as:</p>
<list id="list2-0265532212452396" list-type="bullet">
<list-item><p>clearly identifies important features of the argument and analyzes them insightfully;</p></list-item>
<list-item><p>develops ideas cogently, organizes them logically, and connects them with clear transitions;</p></list-item>
<list-item><p>effectively supports the main points of the critique;</p></list-item>
<list-item><p>demonstrates control of language, including appropriate word choice and sentence variety;</p></list-item>
<list-item><p>demonstrates facility with the conventions (i.e. grammar, usage, and mechanics) of standard written English but may have minor errors.</p></list-item>
</list>
<p>The modified rubric that emphasized higher-order aspects of writing included only the first three bullets of the preceding description. In addition, each prompt (in each rubric) is accompanied by prompt notes and benchmark essays that exemplify each score point.</p>
</sec>
<sec id="section4-0265532212452396">
<title>Design: First stage</title>
<p>The following design factors were used in the first stage of the study.</p>
<sec id="section5-0265532212452396">
<title>Task</title>
<p>Each examinee wrote two essays in response to an argument and issue prompts. Each participating rater (without being aware of this fact) scored both the argument and issue essays written by examinees.</p>
</sec>
<sec id="section6-0265532212452396">
<title>Prompt pair</title>
<p>Two pairs of argument and issue prompts were used in the study. Each rater scored only one of the pairs.</p>
</sec>
<sec id="section7-0265532212452396">
<title>Examinee group</title>
<p>For each prompt pair, the essays of 200 examinees were used in the study. They were separated into two equivalent groups (labeled A and B), matched on their GRE verbal scores. Each rater scored the essays of both groups of examinees, but in different scoring conditions.</p>
</sec>
<sec id="section8-0265532212452396">
<title>Scoring rubric</title>
<p>In addition to the regular GRE scoring rubric, a special scoring rubric was prepared for each task. This special rubric emphasized higher-order writing skills. Each rater used (and was aware of) only one scoring rubric.</p>
</sec>
<sec id="section9-0265532212452396">
<title>Essay order</title>
<p>Scoring was performed separately for each set of 100 essays written by one of the examinee groups (A or B) in response to one of the tasks (argument or issue). Each such set of essays was scored in one of two orders: according to essay length from shortest to longest or in a specific random order. Each rater was presented with both orders, one for essays written by examinee group A and the other for essays written by examinee group B.</p>
</sec>
<sec id="section10-0265532212452396">
<title>Task order</title>
<p>Participating raters were assigned to one of four task order conditions, defined by which examinee group is scored in random order, and which order is scored first. The task orders of the four conditions are presented in <xref ref-type="table" rid="table1-0265532212452396">Table 1</xref>.</p>
<table-wrap id="table1-0265532212452396" position="float">
<label>Table 1.</label>
<caption><p>Task order conditions (Task/Examinee group/Essay order)</p></caption>
<graphic alternate-form-of="table1-0265532212452396" xlink:href="10.1177_0265532212452396-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Order</th>
<th align="left">1st Set</th>
<th align="left">2nd Set</th>
<th align="left">3rd Set</th>
<th align="left">4th Set</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Issue/A/Random</td>
<td>Argument /A/Random</td>
<td>Issue/B/Length</td>
<td>Argument /B/Length</td>
</tr>
<tr>
<td>2</td>
<td>Issue/B/Random</td>
<td>Argument /B/Random</td>
<td>Issue/A/Length</td>
<td>Argument /A/Length</td>
</tr>
<tr>
<td>3</td>
<td>Issue/B/Length</td>
<td>Argument /B/Length</td>
<td>Issue/A/Random</td>
<td>Argument /A/Random</td>
</tr>
<tr>
<td>4</td>
<td>Issue/A/Length</td>
<td>Argument/A/Length</td>
<td>Issue/B/Random</td>
<td>Argument/B/Random</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In summary, participant raters used (and were aware of) only one scoring rubric, either the regular or higher order. They also scored essays written to only one pair of argument and issue prompts. Each rater scored both argument and issue essays written by 200 examinees, for a total of 400 ratings. Each rater scored two sets of 100 essays in random order and two other sets in essay length order, from shortest to longest. Finally, there were four different task order conditions.</p>
<p>A total of 32 professional GRE raters participated in the study. For each of the two prompt pairs, four raters were randomly assigned to each task order condition, for a total of 32 raters (4 raters × 4 order conditions × 2 pairs of prompts). Participants were paid an honorarium of $450. All scoring was performed in a special website for the study. The score scale for the study was augmented beyond the standard 1–6 GRE scale to include a + and – for each score point (e.g. to the score of 1 were added the scores 1− and 1+), for a total of 18 score points.</p>
<p>After completing the tasks, raters were asked to share their thoughts about the method of essay ordering by length, about the augmented score scale, and about the special higher-order rubric for those who used it.</p>
</sec>
</sec>
<sec id="section11-0265532212452396">
<title>Design: Second stage</title>
<p>The purpose of the second stage of the study was to evaluate the effect of presenting the e-rater score to raters during their assessment of essays. This stage took place about two months after the first stage, and only the 16 raters who used the higher-order rubric participated in it. In this stage, the raters scored the 400 essays for the prompt pair they were not exposed to in the first stage. Only random essay ordering was used and only one task order: Issue/A, Argument/A, Issue/B, Argument/B. After completing this task, raters were asked again to share their thoughts about scoring with e-rater scores present.</p>
</sec>
<sec id="section12-0265532212452396">
<title>Analyses</title>
<p>All the following analyses interpreted the ‘low’ and ‘high’ levels of each score point as one third below or above the rounded score. The main purpose of the analyses was to compare the effects of the different essay orders and rubrics on study scores. Essays were presented in two orders, random and according to essay length (shortest to longest). In addition, three scoring conditions were used: the regular GRE rubric, the higher-order (HO) rubric, and the HO rubric in conjunction with the presentation of e-rater scores (termed HOE, for higher order + E-rater).</p>
<p>Several types of possible effects of order and condition were evaluated. First, inter-rater correlations were computed for each set of 100 essays between all pairs of raters scoring under the same condition and order. Second, correlations between human and e-rater scores were computed for each set of 100 essays. Third, inter-rater partial correlations controlling for e-rater scores were computed for each set of 100 essays. And fourth, cross-task correlations between the two essays each student wrote were computed for each set of 100 essays and each rater across the different scoring conditions. To interpret these four types of analyses, two-level analyses of variance were performed on the Fisher transformation of the sample correlations, with prompt and rater as random effects (or only prompt for inter-rater correlations), and task, order, and condition as independent variables. The main interest was in possible order and condition main effects or interactions.</p>
</sec>
</sec>
<sec id="section13-0265532212452396" sec-type="results">
<title>Results</title>
<sec id="section14-0265532212452396">
<title>Descriptive statistics</title>
<p><xref ref-type="table" rid="table2-0265532212452396">Table 2</xref> presents mean study scores across conditions. The table shows small differences across the different essay orders, as well as across sets (1 or 2 versus 3 or 4). The scores for the issue task are higher than the argument task by about half a point, and the scores for the regular rubric are lower than other rubrics by about .20–.25 points. Finally, the scores for the second argument prompt were higher than the first by .22 points overall, and the scores for the second issue prompt were higher than the first by .09 points overall.</p>
<table-wrap id="table2-0265532212452396" position="float">
<label>Table 2.</label>
<caption><p>Means (and <italic>SD</italic>s) of scores</p></caption>
<graphic alternate-form-of="table2-0265532212452396" xlink:href="10.1177_0265532212452396-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">HOE rubric</th>
<th align="left" colspan="2">Higher-order rubric<hr/></th>
<th align="left" colspan="2">Regular rubric<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Argument prompt 1</td>
</tr>
<tr>
<td>Set 2</td>
<td>3.42 (1.08)</td>
<td>3.13 (1.09)</td>
<td>3.38 (0.98)</td>
<td>3.28 (1.05)</td>
<td>3.04 (0.89)</td>
</tr>
<tr>
<td>Set 4</td>
<td>3.45 (1.01)</td>
<td>3.53 (1.06)</td>
<td>3.22 (1.00)</td>
<td>2.99 (0.85)</td>
<td>3.31 (1.04)</td>
</tr>
<tr>
<td colspan="6">Argument prompt 2</td>
</tr>
<tr>
<td>Set 2</td>
<td>3.60 (1.00)</td>
<td>3.64 (1.03)</td>
<td>3.70 (1.03)</td>
<td>3.61 (0.96)</td>
<td>3.19 (0.95)</td>
</tr>
<tr>
<td>Set 4</td>
<td>3.51 (0.99)</td>
<td>3.74 (1.03)</td>
<td>3.52 (0.99)</td>
<td>3.28 (0.96)</td>
<td>3.37 (0.93)</td>
</tr>
<tr>
<td colspan="6">Issue prompt 1</td>
</tr>
<tr>
<td>Set 1</td>
<td>3.88 (0.84)</td>
<td>3.77 (0.87)</td>
<td>3.85 (0.78)</td>
<td>3.75 (0.90)</td>
<td>3.51 (0.76)</td>
</tr>
<tr>
<td>Set 3</td>
<td>3.95 (0.74)</td>
<td>4.06 (0.84)</td>
<td>3.86 (0.80)</td>
<td>3.57 (0.78)</td>
<td>3.75 (0.89)</td>
</tr>
<tr>
<td colspan="6">Issue prompt 2</td>
</tr>
<tr>
<td>Set 1</td>
<td>3.93 (0.88)</td>
<td>3.71 (0.80)</td>
<td>4.01 (0.79)</td>
<td>3.90 (0.74)</td>
<td>3.71 (0.86)</td>
</tr>
<tr>
<td>Set 3</td>
<td>4.00 (0.77)</td>
<td>4.26 (0.82)</td>
<td>3.79 (0.83)</td>
<td>3.69 (0.75)</td>
<td>3.77 (0.78)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0265532212452396">
<p><italic>Note:</italic> HOE = Higher-order rubric + e-rater scores. Each number is based on 100 essays, each rated by four raters, except for HOE, where each essay is rated by eight raters.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section15-0265532212452396">
<title>Inter-rater correlations</title>
<p>To compare levels of agreement between raters across study conditions, all 100-essay pair-wise correlations between raters were computed. For each set of 100 essays, either 8 raters (for the HOE condition) or 4 raters (for the two other rubrics) provided scores, resulting in 28 or 6 pair-wise correlations, respectively. <xref ref-type="table" rid="table3-0265532212452396">Table 3</xref> presents the mean (and <italic>SD</italic>) of these correlations for each prompt, rubric, and essay order. Each cell in the table is based on all pair-wise correlations for each of the two examinee groups.</p>
<table-wrap id="table3-0265532212452396" position="float">
<label>Table 3.</label>
<caption><p>Means (and <italic>SD</italic>s) of 100-essay inter-rater correlations for study scores</p></caption>
<graphic alternate-form-of="table3-0265532212452396" xlink:href="10.1177_0265532212452396-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">HOE rubric</th>
<th align="left" colspan="2">Higher-order rubric<hr/></th>
<th align="left" colspan="2">Regular rubric<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Argument</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>.85 (.06)</td>
<td>.82 (.06)</td>
<td>.84 (.04)</td>
<td>.78 (.10)</td>
<td>.84 (.04)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>.86 (.04)</td>
<td>.86 (.04)</td>
<td>.84 (.04)</td>
<td>.81 (.03)</td>
<td>.81 (.04)</td>
</tr>
<tr>
<td colspan="6">Issue</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>.84 (.05)</td>
<td>.71 (.07)</td>
<td>.69 (.07)</td>
<td>.84 (.06)</td>
<td>.82 (.05)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>.81 (.08)</td>
<td>.74 (.06)</td>
<td>.70 (.12)</td>
<td>.75 (.05)</td>
<td>.76 (.08)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0265532212452396">
<p><italic>Note:</italic> HOE = Higher-order rubric + e-rater scores. Each number is based on 28 (for HOE) or six correlations for each of two examinee groups, with a total of 56 or 12 correlations.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>To interpret these results, a two-level analysis of variance was conducted on the Fisher transformation of the sample correlations, with prompt as a random effect, and task, order, and rubric as independent variables. The main interest was in possible order and rubric main effects or interactions.</p>
<p>Initial results showed no main effect or interaction for order, so a modified analysis was conducted without the order factor.<sup><xref ref-type="fn" rid="fn1-0265532212452396">1</xref></sup> No significant main effect for task was found (<italic>F</italic>[1, 2.1] = 10.5, <italic>p</italic> = .08), with an average correlation of .84 for argument and .79 for issue. Significant effects were found for both the rubric effect (<italic>F</italic>[2, 408] = 41.7, <italic>p</italic> &lt; .01) and the rubric-task interaction (<italic>F</italic>[2, 408] = 18.2, <italic>p</italic> &lt; .01). Post-hoc comparisons found significant differences (<italic>p</italic> &lt; .05) between all three rubrics in both tasks. For argument, the average correlations in the HOE, HO, and regular rubrics were .86, .84, and .81, respectively. For issue, the average correlations in the HOE, HO, and regular rubrics were .83, .71, and .79, respectively. That is, the HOE correlations were highest in both tasks, and the HO correlations for issue were especially low.</p>
</sec>
<sec id="section16-0265532212452396">
<title>Correlations with e-rater scores</title>
<p>For each rater and 100-essay set, the correlation between e-rater and human scores was computed. <xref ref-type="table" rid="table4-0265532212452396">Table 4</xref> presents the mean (and <italic>SD</italic>) of these correlations for each prompt, rubric, and essay order. A two-level analysis of variance was conducted on the Fisher transformation of the sample correlations, with prompt and rater as random effects, and task, order, and rubric as independent variables. The main interest was in possible order and rubric main effects or interactions.</p>
<table-wrap id="table4-0265532212452396" position="float">
<label>Table 4.</label>
<caption><p>Means (and <italic>SD</italic>s) of 100-essay correlations between e-rater and study scores</p></caption>
<graphic alternate-form-of="table4-0265532212452396" xlink:href="10.1177_0265532212452396-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">HOE rubric</th>
<th align="left" colspan="2">Higher-order rubric<hr/></th>
<th align="left" colspan="2">Regular rubric<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Argument</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>.81 (.08)</td>
<td>.74 (.08)</td>
<td>.71 (.05)</td>
<td>.69 (.09)</td>
<td>.70 (.03)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>.85 (.06)</td>
<td>.75 (.05)</td>
<td>.77 (.05)</td>
<td>.75 (.04)</td>
<td>.76 (.04)</td>
</tr>
<tr>
<td colspan="6">Issue</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>.89 (.05)</td>
<td>.78 (.06)</td>
<td>.77 (.05)</td>
<td>.79 (.05)</td>
<td>.80 (.07)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>.88 (.07)</td>
<td>.80 (.06)</td>
<td>.76 (.09)</td>
<td>.75 (.07)</td>
<td>.77 (.07)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0265532212452396">
<p><italic>Note:</italic> HOE = Higher-order rubric + e-rater scores.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Initial results showed no main effect or interaction for order, so a modified analysis was conducted without the order factor. No significant main effect for task was found, (<italic>F</italic>[1, 2.0] = 6.6, <italic>p</italic> = .12), with an average correlation of .77 for argument and .81 for issue. A significant effect was found only for the rubric effect (<italic>F</italic>[2, 184] = 62.2, <italic>p</italic> &lt; .01) but not the rubric-task interaction (<italic>F</italic>[2, 184] = .9, <italic>p</italic> = .41). Post-hoc comparisons found significant differences (<italic>p</italic> &lt; .05) between the HOE rubric (average correlation of .86) and both the HO (.76) and regular (.79) rubrics. That is, the HOE correlations with e-rater scores were highest in both tasks.</p>
</sec>
<sec id="section17-0265532212452396">
<title>Inter-rater partial correlations, controlled for e-rater scores</title>
<p>To compare levels of agreement between raters across study conditions after controlling for e-rater scores, all 100-essay pair-wise partial correlations between raters were computed. <xref ref-type="table" rid="table5-0265532212452396">Table 5</xref> presents the mean (and <italic>SD</italic>) of these partial correlations for each prompt, rubric, and essay order. A two-level analysis of variance was conducted on the Fisher transformation of the sample correlations, with prompt as random effect, and task, order, and rubric as independent variables.</p>
<table-wrap id="table5-0265532212452396" position="float">
<label>Table 5.</label>
<caption><p>Means (and <italic>SD</italic>s) of 100-essay inter-rater partial correlations for study scores</p></caption>
<graphic alternate-form-of="table5-0265532212452396" xlink:href="10.1177_0265532212452396-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Task</th>
<th align="left">HOE rubric</th>
<th align="left" colspan="2">Higher-order rubric<hr/></th>
<th align="left" colspan="2">Regular rubric<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Argument</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>.52 (.30)</td>
<td>.60 (.13)</td>
<td>.68 (.10)</td>
<td>.59 (.18)</td>
<td>.68 (.06)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>.49 (.20)</td>
<td>.67 (.08)</td>
<td>.61 (.16)</td>
<td>.57 (.09)</td>
<td>.54 (.12)</td>
</tr>
<tr>
<td colspan="6">Issue</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>.27 (.15)</td>
<td>.25 (.14)</td>
<td>.24 (.16)</td>
<td>.59 (.11)</td>
<td>.53 (.08)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>.20 (.12)</td>
<td>.30 (.08)</td>
<td>.33 (.15)</td>
<td>.44 (.12)</td>
<td>.43 (.09)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0265532212452396">
<p><italic>Note:</italic> HOE = Higher-order rubric + e-rater scores. Each number is based on 28 (for HOE) or six correlations for each of two examinee groups, with a total of 56 or 12 correlations.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Initial results showed no main effect or interaction for order, so a modified analysis was conducted without the order factor. A significant main effect for task was found, (<italic>F</italic>[1, 2.1] = 49.4, <italic>p</italic> = .02), with an average partial correlation of .56 for argument and .31 for issue. Significant effects were found for both the rubric effect (<italic>F</italic>[2, 408] = 31.2, <italic>p</italic> &lt; .01) and the rubric-task interaction (<italic>F</italic>[2, 408] = 14.0, <italic>p</italic> &lt; .01). For the argument task, post-hoc comparisons found significant differences (<italic>p</italic> &lt; .05) between the HOE rubric (average partial correlation of .51) and both the HO (.64) and regular (.61) rubrics. For the issue task, post-hoc comparisons found significant differences (<italic>p</italic> &lt; .05) between the regular rubric (average partial correlation of .50) and both the HOE (.23) and HO (.28) rubrics. That is, the HOE partial correlations were lowest in both tasks, and the HO partial correlations were lower than regular partial correlations for issue.</p>
</sec>
<sec id="section18-0265532212452396">
<title>Cross-task correlations</title>
<p>Cross-task correlations between argument and issue examinee scores awarded by the same raters were calculated for each 100-examinee group. Both human scores and the average of human and e-rater scores were correlated. <xref ref-type="table" rid="table6-0265532212452396">Table 6</xref> presents the mean (and <italic>SD</italic>) of these cross-task correlations for each prompt, rubric, and essay order. A two-level analysis of variance was conducted on the Fisher transformation of the sample correlations, with prompt and rater as random effects, and order and rubric as independent variables.</p>
<table-wrap id="table6-0265532212452396" position="float">
<label>Table 6.</label>
<caption><p>Means (and <italic>SD</italic>s) of 100-essay cross-task correlations</p></caption>
<graphic alternate-form-of="table6-0265532212452396" xlink:href="10.1177_0265532212452396-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Prompt</th>
<th align="left">HOE rubric</th>
<th align="left" colspan="2">Higher-order rubric<hr/></th>
<th align="left" colspan="2">Regular rubric<hr/></th>
</tr>
<tr>
<th/>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
<th align="left">Length order</th>
<th align="left">Random order</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="6">Human scores</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>0.62 (0.08)</td>
<td>0.51 (0.06)</td>
<td>0.52 (0.06)</td>
<td>0.55 (0.08)</td>
<td>0.57 (0.05)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>0.63 (0.07)</td>
<td>0.58 (0.07)</td>
<td>0.60 (0.08)</td>
<td>0.63 (0.06)</td>
<td>0.58 (0.10)</td>
</tr>
<tr>
<td colspan="6">Average of human and e-rater scores</td>
</tr>
<tr>
<td>Prompt 1</td>
<td>0.75 (0.03)</td>
<td>0.72 (0.04)</td>
<td>0.73 (0.03)</td>
<td>0.74 (0.03)</td>
<td>0.74 (0.02)</td>
</tr>
<tr>
<td>Prompt 2</td>
<td>0.74 (0.03)</td>
<td>0.73 (0.04)</td>
<td>0.74 (0.03)</td>
<td>0.76 (0.02)</td>
<td>0.73 (0.04)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0265532212452396">
<p><italic>Note:</italic> HOE = Higher-order rubric + e-rater scores. Each number is based on two examinee groups for each of eight (for HOE) or four raters.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Initial results showed no main effect or interaction for order, so a modified analysis was conducted without the order factor. For human scores only, a significant effect for rubric was found, (<italic>F</italic>[2, 55.2] = 11.2, <italic>p</italic> &lt; .01). Post-hoc comparisons found significant differences (<italic>p</italic> &lt; .05) between the HOE rubric (average correlation of .62) and both the HO (.56) and regular (.58) rubrics. That is, HOE correlations were higher than the other two rubrics.</p>
<p>For the average of human and e-rater scores, the effect for rubric was not significant, (<italic>F</italic>[2, 65.0] = 1.3, <italic>p</italic> = .28), and therefore no significant differences were found between the HOE rubric (average correlation of .74), the HO rubric (.73) and regular rubric (.73).</p>
</sec>
<sec id="section19-0265532212452396">
<title>Comparison of augmented and round scores</title>
<p><xref ref-type="fig" rid="fig1-0265532212452396">Figure 1</xref> shows the distributions of study scores for each task, limited to the regular rubric (other rubrics have similar distributions). The figure shows that the most common score is 3 for argument and 4 for issue. Overall, a significant proportion of ratings, 27% for each task, used either the high (+) or low (−) levels of score points. In most cases, the mid-level score is used more often than either the low or high levels, with some exceptions, such as score category 1, categories 2 for issue, and 5 for argument.</p>
<fig id="fig1-0265532212452396" position="float">
<label>Figure 1.</label>
<caption><p>Histogram of augmented scores for regular rubric</p></caption>
<graphic xlink:href="10.1177_0265532212452396-fig1.tif"/>
</fig>
<p>To estimate the effect of using the augmented scores on reliability, G-theory analyses were conducted on the original augmented scores and on the rounded scores. For each rubric and pair of prompts, the random effects variance components of a fully crossed two-facet (person-by-item-by-rater) design were estimated. For example, the following variances were estimated for augmented scores of prompt pair A of the regular rubric:</p>
<p><disp-formula id="disp-formula1-0265532212452396">
<mml:math display="block" id="math1-0265532212452396">
<mml:mrow>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>p</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>43</mml:mn>
<mml:mo>,</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>i</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>12</mml:mn>
<mml:mo>,</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>r</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>03</mml:mn>
<mml:mo>,</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mrow>
<mml:mi>p</mml:mi>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>20</mml:mn>
<mml:mo>,</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mrow>
<mml:mi>p</mml:mi>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>01</mml:mn>
<mml:mo>,</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>01</mml:mn>
<mml:mo>,</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mrow>
<mml:mi>p</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>r</mml:mi>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>=</mml:mo>
<mml:mo>.</mml:mo>
<mml:mn>15</mml:mn>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0265532212452396" xlink:href="10.1177_0265532212452396-eq1.tif"/>
</disp-formula></p>
<p><xref ref-type="fig" rid="fig2-0265532212452396">Figure 2</xref> presents the G-coefficients for different numbers of raters and items (essays) for augmented and rounded scores. Results are shown for the regular rubric (results for other rubrics are similar) and G-coefficients were averaged across the two prompt pairs (results are almost identical across pairs). The figure shows that augmented scores have consistently higher reliabilities than rounded scores. The size of the effect is equivalent to around 50% of the number of raters. For example, for one essay the reliability of rounded scores increases from .51 with one rater to .59 with two raters, but it is .55 for augmented scores and one rater. Similarly, the reliability of rounded scores is .62 with three raters, but it is also .62 for augmented scores and two raters. Finally, the reliability of rounded scores is .64 with four raters, but it is also .64 (and slightly higher) for augmented scores and three raters. The same picture emerges for two essays.</p>
<fig id="fig2-0265532212452396" position="float">
<label>Figure 2.</label>
<caption><p>G-coefficients for regular rubric</p></caption>
<graphic xlink:href="10.1177_0265532212452396-fig2.tif"/>
</fig>
</sec>
<sec id="section20-0265532212452396">
<title>Rater reactions</title>
<p>Participants were asked to share their thoughts about the study manipulations. Comments were received from 26 participants in the first stage and eight in the second stage. Overall, participants liked the experience and thought these manipulations were interesting. With respect to the ordering of essays according to length, opinions were mixed. Eight raters expressed a positive attitude mainly because it makes scoring easier. Scoring by length ‘facilitated in determining subtleties in scoring’, ‘helped me to focus the finer points of an essay of a particular length’, and ‘makes scoring the top and the bottom essays easier – that is the decisions one makes between ones and twos or fives and sixes.’ On the other hand, eight other raters did not like this ordering. One reason given was that it made scoring harder, especially batches of longer essays: ‘scoring the groups of long responses was quite exhausting’, ‘It can be very refreshing to work with and within the entire score range, not only for variety, but for clarity’, and ‘it did get tedious when there were whole folders of long essays to score.’ More importantly, some raters felt that contrary to intention, length might become an even more important factor in scoring: ‘It’s almost as if you can become conditioned to expect a certain score after looking at ones of similar size over time’ and it ‘might unduly influence a rater’s thinking to even subconsciously tend towards “longer must be better”.’</p>
<p>With respect to the higher-order rubric, opinions were also mixed. Three raters thought it important to focus on these aspects: ‘When focusing on content, you truly understand a person’s critical analysis, argumentative skills. Sometimes, this can be overlooked when reading for too many things’ and ‘it means the scorer has to take even a badly-ESL-impaired paper seriously.’ However, other raters had negative or mixed opinions. Some doubted whether it is possible to ignore or separate language control from content and organization: ‘Ultimately, I doubt that I can judge whether an example is relevant without somehow taking into account whether it is clear’ or ‘one cannot entirely separate development and the quality of thought from the use of language’ or ‘the language problems made it difficult to determine the adequacy or complexity of ideas.’ However, some of the same raters did find the special rubric useful, especially for certain types of essays: ‘That said, there are always those responses that make me think – If only this person had a slightly better command of the language’ and ‘The real difference then was that many responses that I would have scored a 3 under the current scoring guide received a score in the 4− to 4+ range in this study.’</p>
<p>Surprisingly, raters did not have strong opinions about the availability of e-rater scores. They thought it was interesting, even ‘fascinating,’ to compare their own scores with those of the machine, and they noted that in most cases of large discrepancies, e-rater awarded higher scores because it did not take into account problems in content or organization. Some raters felt it helped them to focus on higher-order aspects of writing, but others thought e-rater scores could confuse less confident raters.</p>
<p>Finally, participants expressed overwhelming support (with 22 of 26 positive opinions) for the augmented score scale. They especially liked the ability to provide a more precise score: ‘I LOVE the freedom of having a finer, and more accurate, scale’, ‘The finer scale was remarkable. It made me, as a scorer, really think about the lines between scores’, and ‘I think it is nice to have more options because the range of a score point can vary so much.’ The few negative opinions focused on the fact that the augmented scale made scoring more complex. A somewhat related comment by some raters noted that the study did not include scoring guides to help define the distinctions between low, solid, and high levels, which will be necessary if this scale would be used operationally. One rater also noted, somewhat humorously, that the augmented scale does not make it easier to discern the difference between a 3+ and a 4−; ‘That breaking point will remain as arduous a threshold as ever’.</p>
</sec>
</sec>
<sec id="section21-0265532212452396" sec-type="discussion">
<title>Discussion</title>
<p>The purpose of this study was to explore several procedural changes to the process of rating essays in the wake of the adoption of AES. Their purpose was to increase the reliability of human scoring, but even more importantly, to increase the reliability of the combined human and machine scores. This goal could be achieved by decreasing the overlap between the aspects of writing that human raters and e-rater are considering. However, it could also be achieved simply by increasing the reliability of human scores. Both approaches were explored in this study. A direct approach for decreasing the overlap was attempted by asking raters to use a rubric that focuses on higher-order aspects of writing (that are not well covered by AES). An indirect approach was attempted by making certain kinds of information, essay length or the e-rater score, more salient in the hope that raters will find it easier to ignore it.</p>
<p>Although the main purpose of the attempts to reduce the overlap between human and automated scores was to decrease the correlation between them, they could also have direct effects on the reliability of scores. It might be easier for raters to provide consistent scores that are based on higher-order aspects of writing. It might also be easier to discern differences in essay quality when a batch of essays of similar lengths is presented for evaluation, and a similar argument could be made for the availability of e-rater scores.</p>
<p>However, the results of this study did not confirm these expectations. Remarkably, the essay order manipulation did not have any effect on scores, their levels of agreement across raters, nor their agreement with e-rater scores. As some of the raters remarked, the order of essays did not seem to make a difference on the rating process.</p>
<p>The use of the higher-order (HO) rubric did have measurable effects, but not always in the expected direction. The HO rubric inter-rater agreement was higher (relative to the regular rubric) for the argument task, but lower for the issue task. In addition, the HO rubric did not have an effect on correlations with e-rater scores. Finally, as a result of the contradicting results for the two tasks, the cross-task correlations for HO were similar to those of the regular rubric. These results suggest fundamental differences between the two tasks. Whereas raters were able to take advantage of a focus on content and organization to increase reliability of the argument task, this same focus created more confusion for the issue task. This implies that the quality of ideas and organization is more important for argument, and conversely, that the issue task lends itself more easily to assessment of language control. These implications are supported by an examination of the scoring rubrics for the two tasks, as well as the lower performance of e-rater on the argument task. However, the results of this study provide more direct evidence of how the design of writing tasks can affect the types of evaluations that they support.</p>
<p>Surprising results were also obtained for the HOE condition. The availability of e-rater scores significantly increased inter-rater agreement for both tasks. However, contrary to expectation, it also increased agreement with e-rater scores. As a result, the cross-task reliability of HOE scores was significantly higher than the other two scoring conditions, but the difference disappeared when combined human and e-rater scores were compared. These results suggest that raters were not able to discount the information that was provided to them, but rather assimilated it into their own judgments.</p>
<p>From a cognitive perspective, the order manipulation and e-rater score availability are similar in that they aim to change the context of the rating process. Several studies investigated contextual effects in essay ratings by asking whether raters are influenced from the quality of previous responses they judged. In particular, a contrast effect was found by several authors (<xref ref-type="bibr" rid="bibr13-0265532212452396">Daly &amp; Dickson-Markman, 1982</xref>; <xref ref-type="bibr" rid="bibr20-0265532212452396">Hales &amp; Tokar, 1975</xref>; <xref ref-type="bibr" rid="bibr31-0265532212452396">Spear, 1997</xref>), whereby a previous response of lower quality makes the current response look better, and vice versa. However, <xref ref-type="bibr" rid="bibr4-0265532212452396">Attali (2011b)</xref> found that professional essay raters of a large-scale standardized testing program produced ratings that were slightly drawn towards previous ratings, creating an assimilation effect. <xref ref-type="bibr" rid="bibr4-0265532212452396">Attali (2011b)</xref> explained these differences in results by relating them to the larger psychological literature on assimilation and contrast in judgments. Perceived similarity between target and context or anchor (<xref ref-type="bibr" rid="bibr27-0265532212452396">Mussweiler, 2003</xref>; <xref ref-type="bibr" rid="bibr30-0265532212452396">Sherif, Taub, &amp; Hovland, 1958</xref>) and confidence and certainty in one’s own views (<xref ref-type="bibr" rid="bibr28-0265532212452396">Pelham &amp; Wachsmuth, 1995</xref>) are factors that may favor the assimilation of the context into the judgment of a target stimulus. In contrast to previous research that used inexperienced raters, a very small number of ratings, and artificial contexts, raters in Attali’s study were highly experienced and rated thousands of essays each.</p>
<p>For the present study, the lack of effects for the order manipulation is consistent with the very weak effects that <xref ref-type="bibr" rid="bibr4-0265532212452396">Attali (2011b)</xref> found for professional raters. The conclusions of both studies suggest that experienced raters do not pay much attention to previous rated essays. However, the availability of another rating for the <italic>same</italic> essay is a different matter, even if this is a machine score. The assimilation effect found in this study is reminiscent of the results of the classic experiment by <xref ref-type="bibr" rid="bibr30-0265532212452396">Sherif, Taub, and Hovland (1958)</xref>, who found that the availability of an anchor weight that was similar to a target weight resulted in weight judgments that were drawn towards the anchor.</p>
<p>It is somewhat ironic that the simplest manipulation in this study had the most consistent and striking results. Specifically, allowing raters to express their evaluations with an augmented scale that includes three levels of performance within each score category had a significant beneficial effect on reliability. Their enthusiasm with the augmented scale suggests that they find these distinctions relatively easy to make, contrary to previous suggestions in the literature on essay assessment. It is important to note that the raters did not have any formal training or supporting materials for the augmented scale, but nevertheless successfully used the entire scale. This suggests that highly experienced raters might even be able to use a finer grained scale.</p>
</sec>
</body>
<back>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0265532212452396">
<label>1.</label>
<p>Because HOE was presented in only one order, adjusted means for post-hoc comparisons are not estimable when order is included as an effect.</p></fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Alderson</surname><given-names>J. C.</given-names></name>
<name><surname>Clapham</surname><given-names>C.</given-names></name>
<name><surname>Wall</surname><given-names>D.</given-names></name>
</person-group> (<year>1995</year>). <source>Language test construction and evaluation</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Attali</surname><given-names>Y.</given-names></name>
</person-group> (<year>2007</year>). <source>Construct validity of e-rater in scoring TOEFL essays</source> (ETS RR-07-21). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr3-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Attali</surname><given-names>Y.</given-names></name>
</person-group> (<year>2011a</year>). <source>A differential word use measure for content analysis in automated essay scoring</source> (ETS RR-11-36). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr4-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Attali</surname><given-names>Y.</given-names></name>
</person-group> (<year>2011b</year>). <article-title>Sequential effects in essay ratings</article-title>. <source>Educational and Psychological Measurement</source>, <volume>71</volume>, <fpage>68</fpage>–<lpage>79</lpage>.</citation>
</ref>
<ref id="bibr5-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Attali</surname><given-names>Y.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Validity and reliability of automated essay scoring</article-title>. In <person-group person-group-type="editor">
<name><surname>Shermis</surname><given-names>M.D.</given-names></name>
<name><surname>Burstein</surname><given-names>J.C.</given-names></name>
</person-group> (Eds.), <source>Handbook on automated essay evaluation: Current applications and new directions</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr6-0265532212452396">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Attali</surname><given-names>Y.</given-names></name>
<name><surname>Burstein</surname><given-names>J.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Automated essay scoring with e-rater® V.2</article-title>. <source>Journal of Technology, Learning, and Assessment</source>, <volume>4</volume>(<issue>3</issue>). Available from <ext-link ext-link-type="uri" xlink:href="http://ejournals.bc.edu/ojs/index.php/jtla/">http://ejournals.bc.edu/ojs/index.php/jtla/</ext-link></citation>
</ref>
<ref id="bibr7-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bachman</surname><given-names>L. F.</given-names></name>
<name><surname>Lynch</surname><given-names>B. K.</given-names></name>
<name><surname>Mason</surname><given-names>M.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Investigating variability in tasks and rater judgments in a performance test of foreign language speaking</article-title>. <source>Language Testing</source>, <volume>12</volume>, <fpage>238</fpage>–<lpage>257</lpage>.</citation>
</ref>
<ref id="bibr8-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Baldwin</surname><given-names>D.</given-names></name>
<name><surname>Fowles</surname><given-names>M.</given-names></name>
<name><surname>Livingston</surname><given-names>S.</given-names></name>
</person-group> (<year>2005</year>). <source>Guidelines for constructed-response and other performance assessments</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr9-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Barrett</surname><given-names>S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The impact of training on rater variability</article-title>. <source>International Education Journal</source>, <volume>2</volume>, <fpage>49</fpage>–<lpage>58</lpage>.</citation>
</ref>
<ref id="bibr10-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Breland</surname><given-names>H. M.</given-names></name>
<name><surname>Bridgeman</surname><given-names>B.</given-names></name>
<name><surname>Fowles</surname><given-names>M. E.</given-names></name>
</person-group> (<year>1999</year>). <source>Writing assessment in admission to higher education: Review and framework</source> (College Board Report No. 99-3). <publisher-loc>New York</publisher-loc>: <publisher-name>College Entrance Examination Board</publisher-name>.</citation>
</ref>
<ref id="bibr11-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Breland</surname><given-names>H. M.</given-names></name>
<name><surname>Camp</surname><given-names>R.</given-names></name>
<name><surname>Jones</surname><given-names>R. J.</given-names></name>
<name><surname>Morris</surname><given-names>M. M.</given-names></name>
<name><surname>Rock</surname><given-names>D. A.</given-names></name>
</person-group> (<year>1987</year>). <source>Assessing writing skill</source>. (Research Monograph No. 11). <publisher-loc>New York</publisher-loc>: <publisher-name>College Entrance Examination Board</publisher-name>.</citation>
</ref>
<ref id="bibr12-0265532212452396">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Burstein</surname><given-names>J. C.</given-names></name>
<name><surname>Kukich</surname><given-names>K.</given-names></name>
<name><surname>Wolff</surname><given-names>S.</given-names></name>
<name><surname>Lu</surname><given-names>C.</given-names></name>
<name><surname>Chodorow</surname><given-names>M.</given-names></name>
</person-group> (<year>1998</year>). <source>Computer analysis of essays</source>. <conf-name>Paper presented at the annual meeting of the National Council of Measurement in Education</conf-name>, <conf-loc>San Diego, CA</conf-loc>.</citation>
</ref>
<ref id="bibr13-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Daly</surname><given-names>J. A.</given-names></name>
<name><surname>Dickson-Markman</surname><given-names>F.</given-names></name>
</person-group> (<year>1982</year>). <article-title>Contrast effects in evaluating essays</article-title>. <source>Journal of Educational Measurement</source>, <volume>19</volume>, <fpage>309</fpage>–<lpage>316</lpage>.</citation>
</ref>
<ref id="bibr14-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eckes</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Rater types in writing performance assessments: A classification approach to rater variability</article-title>. <source>Language Testing</source>, <volume>25</volume>, <fpage>155</fpage>–<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr15-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Elder</surname><given-names>C.</given-names></name>
<name><surname>Knoch</surname><given-names>U.</given-names></name>
<name><surname>Barkhuizen</surname><given-names>G.</given-names></name>
<name><surname>von Randow</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Individual feedback to enhance rater training: Does it work?</article-title> <source>Language Assessment Quarterly</source>, <volume>2</volume>, <fpage>175</fpage>–<lpage>196</lpage>.</citation>
</ref>
<ref id="bibr16-0265532212452396">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Elliot</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2001</year>). <source>IntelliMetric: From here to validity</source>. <conf-name>Paper presented at the annual meeting of the American Educational Research Association</conf-name>, <conf-loc>Seattle, WA</conf-loc>.</citation>
</ref>
<ref id="bibr17-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Engelhard</surname><given-names>G.</given-names><suffix>Jr.</suffix></name>
</person-group> (<year>1994</year>). <article-title>Examining rater errors in the assessment of written composition with a many-faceted Rasch model</article-title>. <source>Journal of Educational Measurement</source>, <volume>31</volume>, <fpage>93</fpage>–<lpage>112</lpage>.</citation>
</ref>
<ref id="bibr18-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Engelhard</surname><given-names>G.</given-names><suffix>Jr.</suffix></name>
<name><surname>Myford</surname><given-names>C. M.</given-names></name>
</person-group> (<year>2003</year>). <source>Monitoring faculty consultant performance in the Advanced Placement English Literature and Composition Program with a many-faceted Rasch model</source> (College Board Research Report No. 2003–1). <publisher-loc>New York</publisher-loc>: <publisher-name>College Entrance Examination Board</publisher-name>.</citation>
</ref>
<ref id="bibr19-0265532212452396">
<citation citation-type="web">
<collab>GRE</collab> (<year>2011</year>). <source>Scoring guide for the issue task</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.ets.org/gre/revised_general/prepare/analytical_writing/issue/scoring_guide">www.ets.org/gre/revised_general/prepare/analytical_writing/issue/scoring_guide</ext-link></citation>
</ref>
<ref id="bibr20-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hales</surname><given-names>L. W.</given-names></name>
<name><surname>Tokar</surname><given-names>E.</given-names></name>
</person-group> (<year>1975</year>). <article-title>The effect of the quality of preceding responses on the grades assigned to subsequent responses to an essay question</article-title>. <source>Journal of Educational Measurement</source>, <volume>12</volume>, <fpage>115</fpage>–<lpage>117</lpage>.</citation>
</ref>
<ref id="bibr21-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Huot</surname><given-names>B.</given-names></name>
</person-group> (<year>1990</year>). <article-title>The literature of direct writing assessment: Major concerns and prevailing trends</article-title>. <source>Review of Educational Research</source>, <volume>60</volume>, <fpage>237</fpage>–<lpage>263</lpage>.</citation>
</ref>
<ref id="bibr22-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Landauer</surname><given-names>T. K.</given-names></name>
<name><surname>Foltz</surname><given-names>P. W.</given-names></name>
<name><surname>Laham</surname><given-names>D.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Introduction to latent semantic analysis</article-title>. <source>Discourse Processes</source>, <volume>25</volume>, <fpage>259</fpage>–<lpage>284</lpage>.</citation>
</ref>
<ref id="bibr23-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Landauer</surname><given-names>T. K.</given-names></name>
<name><surname>Laham</surname><given-names>D.</given-names></name>
<name><surname>Foltz</surname><given-names>P. W.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Automated scoring and annotation of essays with the Intelligent Essay Assessor</article-title>. In <person-group person-group-type="editor">
<name><surname>Shermis</surname><given-names>M. D.</given-names></name>
<name><surname>Burstein</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Automated essay scoring: A cross-disciplinary perspective</source> (pp. <fpage>87</fpage>–<lpage>112</lpage>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr24-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lee</surname><given-names>Y.</given-names></name>
<name><surname>Gentile</surname><given-names>C.</given-names></name>
<name><surname>Kantor</surname><given-names>R.</given-names></name>
</person-group> (<year>2008</year>). <source>Analytic scoring of TOEFL® CBT essays: Scores from humans and e-rater</source> (ETS RR-08-01). <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr25-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lumley</surname><given-names>T.</given-names></name>
<name><surname>McNamara</surname><given-names>T. F.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Rater characteristics and rater bias: Implications for training</article-title>. <source>Language Testing</source>, <volume>12</volume>, <fpage>54</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr26-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moss</surname><given-names>P. A.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Can there be validity without reliability?</article-title> <source>Educational Researcher</source>, <volume>23</volume>, <fpage>5</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr27-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mussweiler</surname><given-names>T.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Comparison processes in social judgment: Mechanisms and consequences</article-title>. <source>Psychological Review</source>, <volume>110</volume>, <fpage>472</fpage>–<lpage>489</lpage>.</citation>
</ref>
<ref id="bibr28-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pelham</surname><given-names>B. W.</given-names></name>
<name><surname>Wachsmuth</surname><given-names>J. O.</given-names></name>
</person-group> (<year>1995</year>). <article-title>The waxing and waning of the social self: Assimilation and contrast in social comparison</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>69</volume>, <fpage>825</fpage>–<lpage>838</lpage>.</citation>
</ref>
<ref id="bibr29-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Powers</surname><given-names>D.</given-names></name>
</person-group> (<year>2005</year>). <source>‘Wordiness’: A selective review of its influence, and suggestions for investigating its relevance in tests requiring extended written responses (ETS RM-04-08)</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Educational Testing Service</publisher-name>.</citation>
</ref>
<ref id="bibr30-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sherif</surname><given-names>M.</given-names></name>
<name><surname>Taub</surname><given-names>D.</given-names></name>
<name><surname>Hovland</surname><given-names>C. I.</given-names></name>
</person-group> (<year>1958</year>). <article-title>Assimilation and contrast effects of anchoring stimuli on judgments</article-title>. <source>Journal of Experimental Psychology</source>, <volume>55</volume>, <fpage>150</fpage>–<lpage>155</lpage>.</citation>
</ref>
<ref id="bibr31-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spear</surname><given-names>M.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The influence of contrast effects upon teachers’ marks</article-title>. <source>Educational Research</source>, <volume>39</volume>, <fpage>229</fpage>–<lpage>233</lpage>.</citation>
</ref>
<ref id="bibr32-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weigle</surname><given-names>S. C.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Using FACETS to model rater training effects</article-title>. <source>Language Testing</source>, <volume>15</volume>, <fpage>263</fpage>–<lpage>287</lpage>.</citation>
</ref>
<ref id="bibr33-0265532212452396">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weigle</surname><given-names>S. C.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Investigating rater/prompt interactions in writing assessment: Quantitative and qualitative approaches</article-title>. <source>Assessing Writing</source>, <volume>6</volume>, <fpage>145</fpage>–<lpage>178</lpage>.</citation>
</ref>
<ref id="bibr34-0265532212452396">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Weigle</surname><given-names>S. C.</given-names></name>
</person-group> (<year>2002</year>). <source>Assessing writing</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>