<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">WCX</journal-id>
<journal-id journal-id-type="hwp">spwcx</journal-id>
<journal-title>Written Communication</journal-title>
<issn pub-type="ppub">0741-0883</issn>
<issn pub-type="epub">1552-8472</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0741088313488075</article-id>
<article-id pub-id-type="publisher-id">10.1177_0741088313488075</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Online Survey Design and Development: A Janus-Faced Approach</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lauer</surname><given-names>Claire</given-names></name>
<xref ref-type="aff" rid="aff1-0741088313488075">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>McLeod</surname><given-names>Michael</given-names></name>
<xref ref-type="aff" rid="aff2-0741088313488075">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Blythe</surname><given-names>Stuart</given-names></name>
<xref ref-type="aff" rid="aff2-0741088313488075">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0741088313488075"><label>1</label>Arizona State University, Mesa, AZ, USA</aff>
<aff id="aff2-0741088313488075"><label>2</label>Michigan State University, East Lansing, MI, USA</aff>
<author-notes>
<corresp id="corresp1-0741088313488075">Claire Lauer, Technical Communication Program, Arizona State University, 7271 E. Sonoran Arroyo Mall, Mesa, AZ 85212, USA. Email: <email>claire.lauer@asu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>30</volume>
<issue>3</issue>
<issue-title>Special Issue on New Methods for the Study of Written Communication</issue-title>
<fpage>330</fpage>
<lpage>357</lpage>
<permissions>
<copyright-statement>© 2013 SAGE Publications</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>In this article we propose a <italic>Janus-faced</italic> approach to survey design—an approach that encourages researchers to consider how they can design and implement surveys more effectively using the latest web and database tools. Specifically, this approach encourages researchers to look two ways at once; attending to both the survey interface (client side; what users see) and the database design (server side; what researchers collect) so that researchers can pursue the most dynamic and layered data collection possible while ensuring greater participation and completion rates from respondents. We illustrate the potentials of a Janus-faced approach using a successfully designed and implemented nationwide survey on the writing lives of professional writing alumni. We offer up a series of questions that a researcher will want to consider during each stage of survey development.</p>
</abstract>
<kwd-group>
<kwd>interface</kwd>
<kwd>database</kwd>
<kwd>client</kwd>
<kwd>server</kwd>
<kwd>web</kwd>
<kwd>data</kwd>
<kwd>research</kwd>
<kwd>information architecture</kwd>
<kwd>interface design</kwd>
<kwd>information design</kwd>
<kwd>collaboration</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Professionals encounter a steady stream of requests to participate in online surveys. On the WPA List in the first two months of 2013, for example, there were seven requests for participants in online surveys, with three additional requests appearing during the first week of March 2013. Given the rapid rise in popularity of online surveys—which utilize advancements in web and programming technologies to design, administer, and process surveys in new ways—one would expect to see an equally rapid rise in the development of effective online survey theories and techniques for writing studies researchers. However, what appears to have happened instead is a rise in the use of prepackaged survey platforms (e.g., SurveyMonkey, Zoomerang, Qualtrics, and many others) that have removed the perceived need for both programming skills and for any revisiting of theories informing survey construction. We believe that such a revisiting is necessary because, although survey software has made the process easier, it hasn’t necessarily made it more effective. As just about everyone who has participated in an online survey knows, it is all too common for surveys to present participants with long lists of questions with little indication of the effort required to complete the survey or even its overall purpose. Thus, we argue that writing researchers need to take a more active role in constructing surveys outside of the prepackaged options because, while some surveys may be smaller in scale and intended for internal use to improve an individual course or program, other surveys are quite large and sponsored by large educational organizations (like Educational Testing Service), and the results of these kinds of surveys exert wide-ranging and potentially dramatic influence on the work we do in the field and on our ability to advocate for our work to others outside of the field.</p>
<sec id="section1-0741088313488075" sec-type="methods">
<title>Survey Research and Methodology in Writing Studies</title>
<p>Survey research has, of course, been used for decades in writing studies as a valuable tool for collecting data and uncovering trends about perceptions, practices, and experiences that can in turn be used to inform our teaching and research. Surveys are valuable instruments to employ when a researcher needs to obtain descriptive information from a large population (<xref ref-type="bibr" rid="bibr25-0741088313488075">Lauer &amp; Asher, 1988</xref>, p. 55). Surveys of large-scale sets of writers may be useful for gauging genre use and technology use and for detecting correlations between socioeconomic factors and writing. In addition, surveys remain important because they allow researchers to seek trends that might not otherwise appear in smaller data sets. Does a trend observed in a classroom (of perhaps 25 students) hold true across a program of 3,000 students? Does a trend observed on one campus hold true when analyzed across programs around the country?</p>
<p>In the professional writing community, for example, scholars have surveyed students, alumni, teachers, professionals, and managers about their perceptions of professional writing coursework (<xref ref-type="bibr" rid="bibr11-0741088313488075">Coon &amp; Scanlon, 1997</xref>; <xref ref-type="bibr" rid="bibr14-0741088313488075">Cox, 1976</xref>). Others have used surveys to identify and assess the kinds of job skills needed by professional writers (<xref ref-type="bibr" rid="bibr5-0741088313488075">Bednar &amp; Olney, 1987</xref>; <xref ref-type="bibr" rid="bibr21-0741088313488075">Halpern, 1981</xref>; <xref ref-type="bibr" rid="bibr33-0741088313488075">Sapp &amp; Zhang, 2009</xref>; <xref ref-type="bibr" rid="bibr45-0741088313488075">Whiteside, 2003</xref>), and still others have used surveys to gather data about current workplace practices (<xref ref-type="bibr" rid="bibr7-0741088313488075">Brumberger, 2007</xref>; <xref ref-type="bibr" rid="bibr15-0741088313488075">Dawley &amp; Anthony, 2003</xref>; <xref ref-type="bibr" rid="bibr16-0741088313488075">Dayton &amp; Hopper, 2010</xref>; <xref ref-type="bibr" rid="bibr30-0741088313488075">Moss, 1995</xref>). In the composition community, survey research has been used to discover trends in topics such as program assessment (<xref ref-type="bibr" rid="bibr38-0741088313488075">Stitt-Bergh &amp; Hilgers, 2009</xref>; <xref ref-type="bibr" rid="bibr44-0741088313488075">Weiser, 1999</xref>), satisfaction with university writing centers (<xref ref-type="bibr" rid="bibr40-0741088313488075">Thompson et al., 2009</xref>), and instructor perception of multimodal writing (<xref ref-type="bibr" rid="bibr2-0741088313488075">D. Anderson et al., 2006</xref>). Although these surveys have produced valuable data for our field, only the most recent were conducted online, and those that were appear to have used prepackaged survey software rather than taking advantage of some of the capabilities that web and database technologies now provide. So although their results may continue to be relevant, the methodologies they employed can no longer serve as a guiding example for how to design and implement a modern survey using the latest technologies.</p>
<p>Until recently, scholars had engaged in theorizing the development of effective survey instruments in writing studies (<xref ref-type="bibr" rid="bibr3-0741088313488075">P. Anderson, 1985</xref>; <xref ref-type="bibr" rid="bibr8-0741088313488075">Chou, 1997</xref>; <xref ref-type="bibr" rid="bibr34-0741088313488075">Sherblom, Sullivan, &amp; Sherblom, 1993</xref>; among others). <xref ref-type="bibr" rid="bibr3-0741088313488075">P. Anderson (1985)</xref> continues to serve as a primer for writing studies researchers on the theory and practice of effective survey development, discussing both the elements of a survey and concepts like operational definitions, levels of measurement, reliability, validity, and others. <xref ref-type="bibr" rid="bibr34-0741088313488075">Sherblom et al. (1993)</xref> also introduce important perspectives to consider in survey development, in response to what they perceived as an overall lack of attention to careful and systematic development, construction, and analysis of surveys in business communication (p. 58). <xref ref-type="bibr" rid="bibr8-0741088313488075">Chou (1997)</xref> introduces researchers to networked, email, and web-based surveys, but his relevance is limited by the fact that the web technologies he refers to were only in their infancy back then and did not include more sophisticated visual design or back-end processing considerations.</p>
<p>Substantial innovations have emerged in web-based survey development since <xref ref-type="bibr" rid="bibr8-0741088313488075">Chou (1997)</xref>, yet recent special issues of journals devoted to research methods—including <italic>Technical Communication</italic> (2008), <italic>Written Communication</italic> (2008), and <italic>College Composition and Communication</italic> (2012)—include no discussion of survey methodology. No books appear to include this discussion either. <xref ref-type="bibr" rid="bibr25-0741088313488075">Lauer and Asher (1988)</xref> write about sampling and surveys, but the subject has either disappeared or been submerged in more recent volumes (see, e.g., <xref ref-type="bibr" rid="bibr4-0741088313488075">Bazerman &amp; Prior, 2003</xref>; <xref ref-type="bibr" rid="bibr23-0741088313488075">Kirsch &amp; Sullivan, 1992</xref>; <xref ref-type="bibr" rid="bibr28-0741088313488075">McKee &amp; DeVoss, 2008</xref>; <xref ref-type="bibr" rid="bibr31-0741088313488075">Nickoson &amp; Sheridan, 2012</xref>). Survey research is included in <xref ref-type="bibr" rid="bibr22-0741088313488075">Hughes and Hayhoe’s (2007)</xref> <italic>A Research Primer for Technical Communication</italic>, but it is largely intended for an audience of research novices. This is problematic because, while surveys (both large and small, local and national) fill a particular need, our field is paying little attention to the challenges of developing complex, larger-scale data collection despite the new functionality available that would make surveys more effective than ever, both in terms of user completion and in terms of the collection of complex data. To borrow a distinction made by <xref ref-type="bibr" rid="bibr24-0741088313488075">Lanham (1993)</xref>, writing studies often looks through surveys, but seldom looks at them (pp. 80-81).</p>
</sec>
<sec id="section2-0741088313488075">
<title>A Janus-Faced Approach to Survey Design</title>
<p>We propose a Janus-faced approach to looking <italic>at</italic> survey design—an approach that encourages researchers to look two ways at once—specifically, to develop surveys with an eye toward the participant’s experience while also planning for usable outputs for researchers. This dichotomy can also be understood in terms of attending to both the front-end survey interface (how participants interact with questions and produce data) and back-end data collection (how participant responses are stored and utilized) so that researchers can pursue the most dynamic and layered data collection possible while ensuring greater participation and completion rates from participants.</p>
<p>This approach encourages researchers to consider how they can implement surveys more effectively using the latest tools to develop both the front-end interface and the back-end data collection in their surveys. At this point those latest tools are web-based programming and database tools. However, a Janus-faced approach accommodates changes in technologies and can be adapted to whatever future technologies or capabilities that may develop.</p>
<p>A Janus-faced approach arises out of the reality that the possibilities available to researchers are much more extensive and promising than what simple survey packages are able to offer. Continued advancements in technology that enable more sophisticated user interaction (technologies like HTML, CSS, and JavaScript libraries like jQuery) paired with back-end, server-side technologies for processing and storing user data (programming languages like PHP and Ruby combined with databases like MySQL) enable efficient and dynamic data collection and analysis that extend beyond prepackaged survey software. Although such prepackaged software has made surveys more accessible and easier to develop and distribute, their predefined structures have also had the effect of making researchers bend to their limitations.</p>
<fig id="fig1-0741088313488075" position="float">
<label>Figure 1.</label>
<caption>
<p>Illustration of Janus coin face adapted from one found at <ext-link ext-link-type="uri" xlink:href="http://mythopoetry.com">mythopoetry.com</ext-link>.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig1.tif"/>
</fig>
<p>As part of a Janus-faced approach, we offer a set of productive questions designed to help researchers as they develop surveys appropriate to their particular contexts. To illustrate the potentials of this approach, we refer often to a nationwide survey that we constructed on the writing lives of professional writing alumni. We designed this survey to meet the needs of participants and researchers through question development, interaction design, usability testing, and data analysis. In the following sections we offer a series of developmental issues and the questions that a researcher should consider.</p>
</sec>
<sec id="section3-0741088313488075">
<title>Determining Research Objectives</title>
<p><italic>Key questions: What questions do I want answered? What data do I want to collect? What do I want to be able to do with my data? What technologies do I have available to me to solicit and process data?</italic></p>
<p>A typical starting point for researchers is to identify research questions and the kinds of information that can be gathered to address those questions. But we would add to this that it is imperative for researchers to formulate their questions with an eye toward the technological tools that will enable researchers to collect data in a way that is both engaging for the user and easily processed by the researcher. Researchers should envision participant responses not just as “answers” but also as “data” to accommodate both the collection of participant content and the consideration of how such content should be stored and processed.</p>
<p>Previous to our current online, database-driven technological context, survey researchers would often store and process their data using spreadsheets. Because of the ubiquity of spreadsheet software like Microsoft Excel and the ability of survey packages to easily export results into such software, researchers may default into envisioning data collection in terms of what information they can imagine storing in rows in columns. This perspective can limit not only the ways that participants might be asked to respond to questions, but also the amount of data researchers can decide to collect and the questions researchers may decide to ask of the data.</p>
<p>In the study we’re referencing here, we knew that we wanted to learn about the writing lives of professional and technical communication (PTC) graduates. We didn’t want to limit ourselves to hearing from graduates from just a single program or graduating class. We wanted to be able to hear from as many PTC graduates as possible and find out from them a variety of things, including these:</p>
<list id="list1-0741088313488075" list-type="bullet">
<list-item><p>What genres did alumni write?</p></list-item>
<list-item><p>What genres did they value?</p></list-item>
<list-item><p>For whom did they write?</p></list-item>
<list-item><p>With whom did they write?</p></list-item>
<list-item><p>Where did they write?</p></list-item>
<list-item><p>What technologies did they use?</p></list-item>
</list>
<p>As we were deciding what we wanted to know, we also had to imagine how we wanted to analyze and report the data we were going to collect. Though our questions seemed relatively contained at the onset, we wanted to collect additional data about each question that would enable us to chart trends among all respondents while simultaneously constructing a picture of each individual respondent. After these questions were suitably fleshed out and added to contextual questions about participant demographics and professional experience, our five initial questions had turned into at least 75 data points, or more, depending on how each participant responded. Storing this amount of data as a single row in a spreadsheet, as most prepackaged survey tools would provide, is incredibly limiting for both researchers and participants, as we’ll discuss later (see <xref ref-type="bibr" rid="bibr6-0741088313488075">Blythe, Lauer, &amp; Curran, forthcoming</xref>, for survey results).</p>
</sec>
<sec id="section4-0741088313488075">
<title>Identifying Collaborators</title>
<p><italic>Key questions: Should I make the survey myself? Should I collaborate with others? Who might those others be? Where can I find them? What might persuade them to work with me?</italic></p>
<p>If researchers are going to be able to move beyond spreadsheets and utilize more advanced interface design and data storage databases, they are going to need help. Most researchers will acknowledge their limitations when it comes to mastering—single-handedly and quickly—the complex array of web-based tools currently available. Rather than succumbing to the technical limitations of a prepackaged survey tool—or taking the time to master web design and development, database programming, and statistical analysis skills—researchers should take seriously the possibility of collaborating with skilled colleagues to design customized survey mechanisms that will produce the most useful methods possible for data collection and analysis. While some argue that researchers should develop skills that would allow them to code their own mechanisms (<xref ref-type="bibr" rid="bibr27-0741088313488075">Lockett et al., 2012</xref>), the skills required to code an effective and reliable mechanism are difficult to master and require time to develop. Seeking out programmers and statisticians who can collaborate with researchers in this component of the work makes building custom mechanisms more feasible for most researchers.</p>
<p>Identifying where potential collaborators might be found on campus and identifying how they might benefit from the collaboration are good first steps. Larger universities may have departments, colleges, or schools within their universities that sponsor computing environments and that hire experienced graduate students, or that network with experienced researchers across a variety of departments. Similarly, universities may have university-wide training or resource labs that provide these services. Universities may also have research assistance networks that help researchers secure important resources. Programs or departments that work heavily in computing or statistics (like psychology and education) may have faculty who are willing to trade services, or donate their time in exchange for a name on any future publications. One of the authors has worked with two different statisticians, both of whom gave of their expertise in return for a coauthor credit.</p>
<p>Other options include putting the word out to graduate students and undergraduate students in these fields, who can often be hired for less than professionals and/or are willing to work for internship credit. Sometimes departments will provide funding to hire these sorts of students. Another author has worked with both graduate students and a professor in the neighboring psychology department who processed statistics in return for small stipends that were funded by the author’s department (sometimes available out of already-approved travel funds). In addition, funding can be secured on a small scale through grants from organizations like the Council of Writing Program Administrators, the Council for Programs in Technical and Scientific Communication, the National Council of Teachers of English, the International Writing Centers Association, and others who would benefit from the research. One thing to remember is that what may seem terrifically difficult for a researcher in writing studies (e.g., programming a web interface to interact with a database, running statistics on the data once it is collected) are quite easy tasks for those who do them often, which may make their interest in participating more likely than expected.</p>
<p>Securing collaborators is one challenge, but another is recognizing their valuable technological expertise but limited rhetorical expertise. This means that a researcher in rhetoric or writing-related fields should come to the table with a basic understanding of information architecture and interaction design, both of which are rhetorical work despite not being traditional tasks of rhetoric researchers. While we are not advocating that researchers be masters of the technology they will be using, we argue that understanding information architecture allows a researcher to clearly articulate to others what data must be collected—what is valuable, how it will be collected through the survey, how it will be stored in a database, and how it will be utilized in reports. Understanding interaction design helps researchers craft paths through the survey for participants, identifying what the optimal question sequence is, where questions can be hidden unless needed, and how to communicate progress to participants.</p>
<p>In the software industry, the products of information architecture and interaction design are often called “requirements documentation,” documents that enable dialogue first between designers and stakeholders to ensure that all needs are met, and later between designers and developers to find the best possible solution for constructing those designs. This latter dialogue is particularly important as it helps to establish the time and cost for building a design, and developers can often propose compromises that will accomplish the same goal but do so in simpler ways that save on development costs. Researchers need to start with the assumption that their collaborators will not necessarily understand the objectives or have the same goals. The design and “requirements gathering” phase of a project, which is intensely rhetorical and a natural fit for writing researchers, helps collaborators to align goals and determine needs while moving forward with the larger objective of a collaborative research project.</p>
</sec>
<sec id="section5-0741088313488075">
<title>Viewing Survey Participation as User Experience</title>
<p><italic>Key questions: How can I create an interface that encourages potential respondents to take the survey? Once they decide to participate, how do I keep them engaged long enough to finish?</italic></p>
<p>The design of web surveys can involve two competing goals. The first is to collect a large amount of data that can be cross-referenced to produce complex, interconnected results (as we discussed in the section on determining research objectives). The second goal is to design an interface that will engage users long enough to complete the survey. “Survey fatigue” is a well-documented, often-discussed problem in the survey literature (<xref ref-type="bibr" rid="bibr1-0741088313488075">Adams &amp; Umbach, 2012</xref>; <xref ref-type="bibr" rid="bibr26-0741088313488075">Lipka, 2011</xref>; <xref ref-type="bibr" rid="bibr32-0741088313488075">Rogelberg &amp; Stanton, 2007</xref>; <xref ref-type="bibr" rid="bibr36-0741088313488075">Sinickas, 2007</xref>). <xref ref-type="bibr" rid="bibr12-0741088313488075">Couper and Miller (2008</xref>, p. 833) report that the problem has become so significant that the completion rate for opt-in surveys has dropped to below 10%. The design of our own survey proceeded with this challenge in mind. What follows in this section is a description of decisions that need to be made in order to raise the completion rate. In our case, we believe that the decisions we describe here led to the 68.5% completion rate of our survey. In other words, of the 375 participants who began the survey, 257 (<italic>N</italic> = 257) completed it.</p>
<p>The participant side of a web survey is a means through which a researcher and participant engage in symbolic activity. It is, as <xref ref-type="bibr" rid="bibr46-0741088313488075">Zimmerman and Schultz (2000</xref>, p. 177) note, a unique form of communication between researchers and participants. Surveying others has traditionally involved deciding how to word questions to ensure that responses are of the kind that would further a research agenda. However, web technologies can now fundamentally change the way we solicit responses from participants, not only by the options participants have in responding and by the visual arrangement and interfaces that accompany the questions, but also by the ways in which researchers can store responses in databases for future analysis (see also <xref ref-type="bibr" rid="bibr9-0741088313488075">Christian &amp; Dillman, 2004</xref>; <xref ref-type="bibr" rid="bibr13-0741088313488075">Couper, Traugott, &amp; Lamias, 2001</xref>; <xref ref-type="bibr" rid="bibr18-0741088313488075">Dillman, Smyth, &amp; Christian, 2009</xref>; <xref ref-type="bibr" rid="bibr41-0741088313488075">Tourangeau, Couper, &amp; Conrad, 2004</xref>).</p>
<p>Researchers need to assess what tools and options they have at their disposal, and to think in terms of not only origins (Where are we coming from in our research?) but also ends: How are we going to ensure that people answer the questions and finish the survey? How do we want people to be able to respond? What do we want to be able to do with the data, and thus how should we ask for them and then store them? Put another way, when soliciting responses from a participant, researchers need to think not just about what language (words) to use, but also how to present those words and/or visuals on a screen, how they can be grouped, contextualized, introduced, and arranged and how participants will be given the option of responding to them.</p>
<p>Survey research shows that surveys up to 20 minutes have a much higher completion rate than do surveys of any length beyond that (<xref ref-type="bibr" rid="bibr20-0741088313488075">Galesic &amp; Bosnjak, 2009</xref>). Not surprisingly, the shorter a survey, the more likely a respondent will complete it. However, because shorter surveys are not generally in the best interest of researchers’ ability to collect meaningful data, researchers should take advantage of the web’s capabilities to maintain a respondent’s interest throughout a longer survey. For respondents to our survey, a commitment to a 20-minute length was made up front. We incorporated “the fine print,” including institutional review board–mandated language about participation and contact information, but in smaller text off to the side that could be scrolled (see <xref ref-type="fig" rid="fig2-0741088313488075">Figure 2</xref>). In our usability testing of the survey (something we’ll talk about throughout these sections), not one of our participants actually read the preliminary text, so we regarded subordinating it to the friendlier, plain language as a usability improvement.</p>
<fig id="fig2-0741088313488075" position="float">
<label>Figure 2.</label>
<caption>
<p>Welcome screen of survey showing IRB language and commitment to 20-minute time limit.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig2.tif"/>
</fig>
<p>In addition to honoring a 20-minute time limit, the typical way that we tried to avoid frustrating respondents was to create interfaces that adapted to a respondent’s input. We knew that the grouping of questions into clearly identified sections can assure a respondent’s persistence with a survey (<xref ref-type="bibr" rid="bibr10-0741088313488075">Christian, Dillman, &amp; Smyth, 2007</xref>; <xref ref-type="bibr" rid="bibr37-0741088313488075">Smyth, Dillman, Christian, &amp; Stern, 2006</xref>), but our original list of survey questions, compiled into a Microsoft Word document, was simply unapproachable and unwieldy. By grouping related questions and naming each section clearly, we gave respondents a better sense of what they were being asked and why it was important. (We see this as a move akin to the need to group a set of tasks in a tutorial to no more than 10 steps [<xref ref-type="bibr" rid="bibr39-0741088313488075">Sun Microsystems, 2009</xref>, p. 123].) Each section also had a <italic>very</italic> brief description of what we were asking for (see <xref ref-type="fig" rid="fig3-0741088313488075">Figure 3</xref>). These descriptions were made more succinct after usability participants indicated that they simply did not read our longer explanations.</p>
<fig id="fig3-0741088313488075" position="float">
<label>Figure 3.</label>
<caption>
<p>Section header with brief description of questions to be answered in that section.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig3.tif"/>
</fig>
<p>For example, Section 1 of the survey was intended to give us insight into the careers of professional writers after they had completed their degrees, including their job titles, fields in which they had been employed, and their continued education. We realized that exposing all of the possible questions we might want participants to answer would be long, unwieldy, and intimidating. To avoid that, we were able to identify the questions we wanted all participants to answer and the remainder could be hidden from a participant’s view until the participant responded in particular ways. As shown in <xref ref-type="fig" rid="fig4-0741088313488075">Figures 4</xref> and <xref ref-type="fig" rid="fig5-0741088313488075">5</xref>, hiding options for users until applicable made the survey seem more “customized” toward each individual participant (<xref ref-type="bibr" rid="bibr42-0741088313488075">Tuten, 2010</xref>) and resulted in an initial set of four questions that could fit on one screen, which was far more approachable and, depending on participant responses, could be completed very quickly.</p>
<fig id="fig4-0741088313488075" position="float">
<label>Figure 4.</label>
<caption>
<p>Section 1 of the survey when the user first lands on it; all questions immediately scannable, short, not intimidating.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig4.tif"/>
</fig>
<fig id="fig5-0741088313488075" position="float">
<label>Figure 5.</label>
<caption>
<p>Section 1 with all possible questions displayed based on user response to the initial questions; long, intimidating, scary.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig5.tif"/>
</fig>
<p>By hiding these extra choices unless the participants indicated they needed them, we were able to make that step of the survey appear shorter, requiring less screen reading and making it easier for the participants to move through it quickly. Developing such “skip patterns” greatly improves researchers’ ability to guide a user seamlessly through a survey (<xref ref-type="bibr" rid="bibr17-0741088313488075">Dillman, Caldwell, &amp; Gansemer, 2000</xref>; <xref ref-type="bibr" rid="bibr19-0741088313488075">Fleming &amp; Bowden, 2009</xref>; <xref ref-type="bibr" rid="bibr35-0741088313488075">Shropshire, Hawdon, &amp; Witte, 2009</xref>).</p>
<p>A status bar was another commitment we made to helping respondents’ persist through the survey (see <xref ref-type="fig" rid="fig6-0741088313488075">Figure 6</xref>). The status bar showed participants exactly how many sections of the survey there were, which section of the survey they were currently responding to, and exactly how far they had gone in the completion of that step.</p>
<fig id="fig6-0741088313488075" position="float">
<label>Figure 6.</label>
<caption>
<p>Progress indicator showing stages of the survey that a user will need to complete.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig6.tif"/>
</fig>
<p>As <xref ref-type="fig" rid="fig7-0741088313488075">Figure 7</xref> illustrates, when participants completed a section, they were notified of that fact by a green checkmark indicator and then advanced to the next section.</p>
<fig id="fig7-0741088313488075" position="float">
<label>Figure 7.</label>
<caption>
<p>Progress indicator showing already completed stages and current completion rate.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig7.tif"/>
</fig>
<p>Survey research supports the inclusion of progress indicators (<xref ref-type="bibr" rid="bibr43-0741088313488075">Vehovar, Batagelj, Manfreda, &amp; Zaletel, 2002</xref>, who cite many others), as did our usability tests. Before adding these displays (known also as “wayfinding” in user experience design language), participants in our usability tests expressed frustration about not knowing how far along in the survey they were or how much was left. While these features do not themselves save respondents time, we noticed through additional usability testing that adding them had an appreciable effect in reducing participant frustration in the process of completing the survey.</p>
<p>One more way we increased participant persistence was by using techniques that made moving and selecting data easy, perhaps even engaging, for respondents. Our usability testing consistently identified Sections 3 and 4 of the survey to be the most problematic in terms of survey fatigue. Once participants had built their initial list of all writing genres they had ever created (they were required to indicate at least five), they were asked to create two lists, one of the five genres they wrote most frequently (Section 3) and one of the five they valued most (Section 4). To make this process as efficient and speedy as possible, participants were presented with the list of writing types they had indicated in the previous question and given an interface that allowed them to simply drag and drop the appropriate genre into an ordered list. The “top 5” box would glow green when participants successfully added a type to the list, and they could remove items and rearrange as necessary (see <xref ref-type="fig" rid="fig8-0741088313488075">Figure 8</xref>). The button to advance to the next step was disabled until participants finished the list, giving them first a condition (“Build Your Top-5 List”) and then an option (“Go to Question 3”) when the step was complete. The novelty of being able to build an ordered list by dragging existing options into a new area of the screen (rather than having to retype those options) and the use of color to highlight the completion of a task are some of the visual cues and tools that reduced cognitive complexity (<xref ref-type="bibr" rid="bibr17-0741088313488075">Dillman et al., 2000</xref>) and we believe resulted in the 68.5% completion rate of our opt-in survey.</p>
<fig id="fig8-0741088313488075" position="float">
<label>Figure 8.</label>
<caption>
<p>To build the lists of writing they do most often and writing they value most, participants are asked to choose from the list of writing they generated in question 1 and drag those types to their top 5 list. Items in the top five can be rearranged as needed.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig8.tif"/>
</fig>
</sec>
<sec id="section6-0741088313488075">
<title>Balancing Restriction and Flexibility</title>
<p><italic>Key questions: When should respondents be asked to select from a list, and when should they be able to respond in their own words? Where can compromises be made between gathering needed data and optimizing the experience for participants?</italic></p>
<p>In a Janus-faced approach to survey design, it is important to focus on user experience and researcher needs simultaneously. This often involves deciding between restricted and open-ended responses. In our survey, for example, prepopulating the list of genres and auto-suggesting existing genres for participants to choose from was about more than optimizing the time they spent responding to the survey. We chose to store writing types in a database, making it easy to display randomized lists (thus bypassing concerns over biasing based on list placement), but also making it easy to reuse this information across participants. If users had to enter types of writing manually, not only would there be the nightmare task of having to account for spelling mistakes or inconsistencies (“blogs” vs. “blog posts” vs. “bligs”), but also then there would be no way to consistently track a particular writing type across multiple participants. Consider “email,” for example: if every participant had a unique response for “email” (e.g., email, e-mail, Gmail, MS Outlook), it would be difficult to query the data for all participants who identified that (or a version of that) as a response. By saving that type in a database table, and giving users a checkbox rather than having them enter their own wording, that type became trackable, and we were able to see how all participants, or individual participants, responded with that genre. In this way, prepopulating and auto-suggesting had the effect of normalizing much of the survey data and reducing the amount of work researchers would need to do after the initial data collection cycle had ended.</p>
<p>As illustrated in <xref ref-type="fig" rid="fig9-0741088313488075">Figure 9</xref>, one place where we balanced the competing needs for restriction and flexibility was in Section 2, where we began with a prepopulated list of genres that participants could then add to.</p>
<fig id="fig9-0741088313488075" position="float">
<label>Figure 9.</label>
<caption>
<p>Section 2, Question 1 of the survey, in which students were asked to select the genres they’ve produced since completing their degrees. The genres were stored in the database and randomized for each participant so as not to bias the responses.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig9.tif"/>
</fig>
<p>While we wanted to make it as easy as possible for participants to select the most common types of writing they might have used, we also wanted to enable them to add types we hadn’t accounted for. To that end we added an “Add More Types” option that would give participants an interface where they could add custom types to their list (see <xref ref-type="fig" rid="fig10-0741088313488075">Figures 10</xref> and <xref ref-type="fig" rid="fig11-0741088313488075">11</xref>). When a participant began entering the name of a new writing type, the survey would display a list of similar types that had been entered previously, or that were already on the list but that perhaps the participant had overlooked.</p>
<fig id="fig10-0741088313488075" position="float">
<label>Figure 10.</label>
<caption>
<p>The button participants would click if there were types of writing they wanted to add to the list but had not been included in the default list.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig10.tif"/>
</fig>
<fig id="fig11-0741088313488075" position="float">
<label>Figure 11.</label>
<caption>
<p>The “Add Type” box in which participants would enter the type of writing they wanted to add, with the survey providing suggestions as they typed.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig11.tif"/>
</fig>
<p>Though this option made it easier for participants to accurately name what they wrote, it also made the data collection process cleaner and resulted in less work for researchers on the database side of the survey.</p>
<sec id="section7-0741088313488075">
<title>Collecting and Using Metadata</title>
<p><italic>Key questions: What can survey data tell us about how participants experience the survey? How can those data help researchers design better tools and get better participant completion rates?</italic></p>
<p>One of the many advantages of a web-based survey is that it’s possible to collect data about how all participants interact with the survey mechanism. A great deal can be learned by recording how participants move through the survey,including when they start and when they finish, and, if they didn’t finish, where they abandoned the survey. Database tables can hold more information than just what participants specifically input in response to researcher questions; data about usage, response times, and paths the participant takes through the questions can also be collected and used to gain further insight into a participant’s experience of the survey. For example, the <italic>participant</italic> table in our database was designed to store not only participant demographic data but also metadata describing how the participant engaged with the survey. The following is a list of fields added to the <italic>participant</italic> table that didn’t answer any research questions but were essential in understanding and optimizing the participant’s experience:</p>
<list id="list2-0741088313488075" list-type="bullet">
<list-item><p><italic>Timestart</italic>—The time stamp corresponding to the date and time the user began taking the survey.</p></list-item>
<list-item><p><italic>Timeend</italic>—The time stamp corresponding to when the user completed the last step of the survey.</p></list-item>
<list-item><p><italic>Laststep</italic>—The field that was updated as the participant progressed through the survey, recording each time he or she advanced to another step.</p></list-item>
<list-item><p><italic>Completed</italic>—The field containing a simple binary (0 or 1, yes or no) indicating whether or not the user had completed the survey. This field made it very easy to differentiate between complete and incomplete surveys and calculate completion rates.</p></list-item>
</list>
<p>While the demographic data stored in this table were particularly important for our research, the survey metadata this table also stored allowed us to understand and test participants’ experience. For example, by calculating the difference between <italic>timestart</italic> and <italic>timeend</italic>, we were able to see if participants were spending more than the 20 minutes we promised them it would take to complete the survey. After extensive testing we were able to get the average completion rate down to 824.9 seconds, or 13.8 minutes, well under the time bargain we made with participants in the consent form.</p>
<p>The metadata labeled <italic>Laststep</italic> were particularly important for usability testing because they allowed us to identify where participants were abandoning the survey, determine what might be causing trouble at that particular point, and modify the design to alleviate that trouble. During usability testing, <italic>Laststep</italic> data showed that most of those who abandoned the survey did so at Section 3, the second to last section; we were able to address that during testing by adjusting the design (see <xref ref-type="fig" rid="fig15-0741088313488075">Figure 15</xref>, discussed later in the article), which dramatically improved our completion rate. While these data show our final completion rate at 68.5%, they also reveal that 47 of the 128 participants who abandoned the survey (36.7%) did so at Section 4, the “Most Valued” section, where participants began ordering the list of valued genres. If the survey were to be run again, we would focus our usability efforts on reducing the user frustration in that section.</p>
</sec>
<sec id="section8-0741088313488075">
<title>Researchers as Information Designers</title>
<p><italic>Key questions: How do the data collected need to be used? How can they be utilized during the survey to guide or assist participants? How can they be utilized in the form of reports and exports for researchers during and after the survey?</italic></p>
<p>In addition to designing surveys to collect metadata, researchers can make choices regarding the design of their database tables that greatly improve the ways in which data can be used during and after the survey, while also ensuring that the values of the researchers are represented. When thinking about database tables overall, researchers should remember that the order in which a participant completes a survey should not coincide with how a researcher decides to build tables to store the data. For example, if we present the participant with four sections of a survey, we would not, then, set up our database tables to store data from each corresponding section in a linear fashion. Rather, researchers should base their construction of tables on factors such as whether information needs to be stored as a single response or multiple responses, or whether data will need to be reused throughout various sections of a survey.</p>
<p>In our survey, for example, the <italic>participant</italic> table stored demographic data that did not have multiple possible responses. This worked well for questions like “birth year” where we expected users to have a single response. However, we built a second table (<italic>demographic_responses</italic>) to store responses to the demographic questions where the participant could choose multiple responses. Without the ability to let participants respond with multiple answers, and without the means to store those responses as equally valid, researchers force participants into uncomfortable political territory. The question of gender, for example, is an issue we took very seriously. If we had only the <italic>participants</italic> table to store single responses, participants would have had to answer one way or another (see <xref ref-type="fig" rid="fig12-0741088313488075">Figure 12</xref>). However, using the <italic>demographic_responses</italic> table, we could store two separate responses to the same question, allowing participants who identify with more than one gender to respond how they wish (see <xref ref-type="fig" rid="fig13-0741088313488075">Figure 13</xref>).</p>
<fig id="fig12-0741088313488075" position="float">
<label>Figure 12.</label>
<caption>
<p>Using radio buttons limits user selection to one option. Storing a participant’s response to this question requires only one field in a database table. Allowing participants to select only one response sends a clear message about the bias of the survey designers.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig12.tif"/>
</fig>
<fig id="fig13-0741088313488075" position="float">
<label>Figure 13.</label>
<caption>
<p>Using checkboxes allows participants to submit multiple responses. This type of interaction allows more freedom for participants to identify with multiple genders, but storing multiple responses requires a different information architecture.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig13.tif"/>
</fig>
<p>While the decision to use checkboxes over radio buttons to collect responses to the question of gender respects participants’ right to identify with multiple genders, it also allows researchers to easily query participants’ responses in a number of ways; for example, researchers can identify</p>
<list id="list3-0741088313488075" list-type="bullet">
<list-item><p>How many participants selected multiple genders, a specific gender, or didn’t respond</p></list-item>
<list-item><p>How many genders a specific participant selected</p></list-item>
</list>
<p>While on the surface the question of radio buttons versus checkboxes may seem small, that choice embodies both serious rhetorical implications as well as technical implications for how to store and retrieve multiple responses to the same question.</p>
<p>From an information design perspective, perhaps the most significant challenge we faced was in the design of the key questions for this study—questions about which types of writing participants do most often and which they value most. These responses needed to be stored as two lists of five prioritized genres identified by each participant (see <xref ref-type="fig" rid="fig8-0741088313488075">Figure 8</xref>). These sequences were our essential data points that would guide how each participant responded to the rest of the survey, so they needed to stored separately from the other data, like demographics, or like the inventoried list of all the writing that participants had ever done (see <xref ref-type="fig" rid="fig9-0741088313488075">Figure 9</xref>). To accomplish this, we designed a database table called <italic>sequences</italic> (see <xref ref-type="fig" rid="fig14-0741088313488075">Figure 14</xref>) that would store one record for each genre in a participant’s lists, resulting in 10 records per participant (2 lists per participant × 5 genres per list).</p>
<fig id="fig14-0741088313488075" position="float">
<label>Figure 14.</label>
<caption>
<p>When participants submit their ordered list of genres, their responses are parsed by the survey and each of the five types is stored as a separate record in the database table named “sequences.”</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig14.tif"/>
</fig>
<p>Once processed by the survey, a participant’s responses would be stored in the <italic>sequences</italic> table (see <xref ref-type="table" rid="table1-0741088313488075">Table 1</xref>).</p>
<table-wrap id="table1-0741088313488075" position="float">
<label>Table 1.</label>
<caption>
<p>Sequences.</p>
</caption>
<graphic alternate-form-of="table1-0741088313488075" xlink:href="10.1177_0741088313488075-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="left"/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">id</th>
<th align="center">usercode</th>
<th align="center">genreid</th>
<th align="center">question</th>
<th align="center">sequence</th>
</tr>
</thead>
<tbody>
<tr>
<td>10900</td>
<td>78920810</td>
<td>045 [abstracts]</td>
<td>often</td>
<td>1</td>
</tr>
<tr>
<td>10901</td>
<td>78920810</td>
<td>018 [brand materials]</td>
<td>often</td>
<td>2</td>
</tr>
<tr>
<td>10902</td>
<td>78920810</td>
<td>009 [database content]</td>
<td>often</td>
<td>3</td>
</tr>
<tr>
<td>10903</td>
<td>78920810</td>
<td>101 [cover letters]</td>
<td>often</td>
<td>4</td>
</tr>
<tr>
<td>10904</td>
<td>78920810</td>
<td>037 [press releases]</td>
<td>often</td>
<td>5</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>This table and its data are essential to the survey and emblematic of the challenge of storing sequenced data, so we would like to explain the table in greater detail to illustrate the larger question of how tables like this should be thought about. The <bold>id</bold> field is simply a unique identifier for each record, the value of which is automatically set by the database, so that each record could be used individually if and when necessary. The value stored in the <bold>usercode</bold> field corresponds to the unique id of each participant, allowing the survey to connect each response to that participant’s data as stored in every other table in the survey (see the <xref ref-type="fig" rid="fig17-0741088313488075">appendix</xref> for a complete breakdown of all the data tables we constructed for the survey and how they are related). The <bold>genreid</bold> field stores a number that points to a record in the <italic>genres</italic> table that stores information about each genre selected by the participant (in <xref ref-type="table" rid="table1-0741088313488075">Table 1</xref> the genre name is also included in brackets, which we’ve included simply as a reading aid). The value in the <bold>question</bold> field is the same for each record because they are all responding to the “often” question (a writing type done most often by the participant), and the value in the <bold>sequence</bold> field indicates what order the participants arranged their chosen genres in response to that question, allowing us to reconstruct their lists at a later time.</p>
<p>It is in the reassembling of the data for various purposes that the value in storing data in this way becomes apparent. For example, immediately following the steps in which participants create their two lists of five genres, they are asked to provide additional detail about each genre (see <xref ref-type="fig" rid="fig15-0741088313488075">Figure 15</xref>).</p>
<fig id="fig15-0741088313488075" position="float">
<label>Figure 15.</label>
<caption>
<p>Partial screenshot of Section 3 of the survey, asking participants for more data about each of the genres in their two lists.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig15.tif"/>
</fig>
<p>Section 3 of the survey is built around the two lists users created in Section 2 and that we stored in the <italic>sequences</italic> table. <xref ref-type="fig" rid="fig15-0741088313488075">Figure 15</xref> shows the step in which respondents add descriptive data about each of the genres in their lists. The interface makes use of the participant’s previous responses in two ways. First, the chosen genres serve as a progress indicator, displayed along the left side of the page, marking the genres the user has successfully supplied details for. Second, the item for which the participant is currently providing detail is bolded in red at the top of the section, indicating to the participant which genre he or she is currently adding detail for. If the participant had included the genre in both lists, it would be displayed in both navigation lists to the left and he or she would need to respond only once.</p>
<p>While this display uses the participant’s data to assist him or her in both adding more detail about each genre and navigating the survey, we can use those same data to assist the researchers in their analysis of the data. <xref ref-type="fig" rid="fig16-0741088313488075">Figure 16</xref> illustrates two of many possible views researchers might create using a single source of data (participant’s sequences).</p>
<fig id="fig16-0741088313488075" position="float">
<label>Figure 16.</label>
<caption>
<p>Illustration of how the data submitted to the sequences table by participants can be used to create multiple reports for researchers.</p>
</caption>
<graphic xlink:href="10.1177_0741088313488075-fig16.tif"/>
</fig>
<p>First, the researcher can generate a report about each participant, including lists of most valued and most frequently used genres. Using the same data, the researcher can then generate a report about each genre to determine how often a genre was used in either list, and where that genre was placed on each list (how often participants prioritized it as number 1 versus number 5, for example). These are only two examples of how these data might be recombined or analyzed, but because of the flexibility of how participant responses are stored, options for exploring data and repurposing them for different audiences with different needs are almost unlimited.</p>
</sec>
</sec>
<sec id="section9-0741088313488075" sec-type="conclusions">
<title>Implications and Conclusions</title>
<p>Survey research in writing studies has a great deal of potential for providing the field with important information about trends in skills, experiences, and perceptions related to the teaching and composing practices of instructors, students, practitioners, administrators, and others. This potential has been neglected as of late because researchers have not taken advantage of our current technological advancements, which now include tools that make engaging participants and storing complex arrangements of participant data more accessible than ever before. We call this approach to survey design a <italic>Janus-faced</italic> approach because it asks researchers to attend to both user and researcher needs simultaneously in the design, development, and analysis of surveys and survey data. A Janus-faced approach also advocates for researchers’ collaboration with programmers and statisticians who can help facilitate the kind of broad, detailed data collection that will ultimately benefit the field and better legitimize our work to those outside of writing studies.</p>
<p>But this work would be largely impossible if researchers were to rely on prepackaged survey platforms that allow for little more than building simple lists of questions and exporting the responses to those questions to a spreadsheet for limited analysis. Prepackaged survey platforms (like SurveyMonkey) usually limit researchers to the development of simple questions that lack interdependency and nuance, resulting in data with the same problems. By relying on these “user-friendly” tools, we lose the ability to assist users with custom progress displays, encourage participants to give us deeper responses about their choices, or generate a range of reports to suit researcher needs. For the survey we discuss throughout this article, collecting and using our data in this way allowed us to collect <italic>more</italic> data (and thus generate more useful reports) by improving the user experience and helping users through to completion. Designing in this way is a challenge, but it is not new; Barbara <xref ref-type="bibr" rid="bibr29-0741088313488075">Mirel (1996)</xref> discusses the rhetorical challenges of designing useful data reports with database content in the workplace, but few of our survey tools have risen to meet those challenges.</p>
<p>We would argue, finally, that while a <italic>Janus-faced</italic> approach does not advocate for researchers to <italic>master</italic> the programming and statistical skills necessary for this kind of research, at the very least researchers who wish to engage in survey design should develop a passing familiarity with a core skill set that is rhetorical in its very nature and includes</p>
<list id="list4-0741088313488075" list-type="bullet">
<list-item><p>user experience design</p></list-item>
<list-item><p>interface design</p></list-item>
<list-item><p>information architecture</p></list-item>
<list-item><p>information design</p></list-item>
</list>
<p>While each of these topic areas is a field unto itself, a basic knowledge of them and their practices is empowering. These practices are the rhetoric behind effective surveys: understanding your needs as a researcher, the needs of participants, best practices for asking questions, best practices for user interaction, and effective displays of data gathered from effective database design. Attention to all of these empowers researchers to design surveys that will not only gather more data but make them more useful and more revealing.</p>
</sec>
</body>
<back>
<app-group>
<title>Appendix</title>
<fig id="fig17-0741088313488075" position="float">
<graphic xlink:href="10.1177_0741088313488075-fig17.tif"/>
</fig>
</app-group>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The authors received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<bio>
<title>Author Biographies</title>
<p><bold>Claire Lauer</bold> is an assistant professor in the Technical Communication program at Arizona State University. Her research interests include investigating the teaching and learning of visual design principles in professional writing, and examining the ways in which new kinds of composing practices are defined and discussed in writing studies.</p>
<p><bold>Michael McLeod</bold> is a faculty member in the Department of Writing, Rhetoric, and American Cultures at Michigan State University, where he teaches in both the Professional Writing and Digital Rhetoric &amp; Professional Writing graduate programs. He teaches courses in digital rhetoric, web design, and user experience design. He is also co-founder of the educational technology company Drawbridge and the co-inventor of the writing platform Eli Review.</p>
<p><bold>Stuart Blythe</bold> is an associate professor in the Department of Writing, Rhetoric, and American Cultures at Michigan State University. His primary research interests include the epistemological and ethical challenges of communications between lay and expert groups and the possibilities of rhetorical agency within complex organizations.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Adams</surname><given-names>M.</given-names></name>
<name><surname>Umbach</surname><given-names>P.</given-names></name>
</person-group> (<year>2012</year>). <article-title>Nonresponse and online student evaluations of teaching: Understanding the influence of salience, fatigue, and academic environments</article-title>. <source>Research in Higher Education</source>, <volume>53</volume>, <fpage>576</fpage>-<lpage>591</lpage>.</citation>
</ref>
<ref id="bibr2-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>D.</given-names></name>
<name><surname>Atkins</surname><given-names>A.</given-names></name>
<name><surname>Ball</surname><given-names>C.</given-names></name>
<name><surname>Millar</surname><given-names>K.</given-names></name>
<name><surname>Selfe</surname><given-names>C.</given-names></name>
<name><surname>Selfe</surname><given-names>R.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Integrating multimodality into composition curricula: Survey methodology and results from a CCCC research grant</article-title>. <source>Composition Studies</source>, <volume>34</volume>, <fpage>59</fpage>-<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr3-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Anderson</surname><given-names>P.</given-names></name>
</person-group> (<year>1985</year>). <article-title>Survey methodology</article-title>. In <person-group person-group-type="editor">
<name><surname>Odell</surname><given-names>L.</given-names></name>
<name><surname>Goswami</surname><given-names>D.</given-names></name>
</person-group> (Eds.), <source>Writing in nonacademic settings</source> (pp. <fpage>453</fpage>-<lpage>501</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford</publisher-name>.</citation>
</ref>
<ref id="bibr4-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bazerman</surname><given-names>C.</given-names></name>
<name><surname>Prior</surname><given-names>P.</given-names></name>
</person-group> (<year>2003</year>). <source>What writing does and how it does it: An introduction to analyzing texts and textual practices</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr5-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bednar</surname><given-names>A. S.</given-names></name>
<name><surname>Olney</surname><given-names>R. J.</given-names></name>
</person-group> (<year>1987</year>). <article-title>Communication needs of recent graduates</article-title>. <source>Bulletin of the Association for Business Communication</source>, <volume>50</volume>, <fpage>22</fpage>-<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr6-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Blythe</surname><given-names>S.</given-names></name>
<name><surname>Lauer</surname><given-names>C.</given-names></name>
<name><surname>Curran</surname><given-names>P</given-names></name>
</person-group>, (forthcoming). <article-title>Professional &amp; technical communication in a Web 2.0 world: A report on a nationwide survey</article-title>. <source>Technical Communication Quarterly</source>.</citation>
</ref>
<ref id="bibr7-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brumberger</surname><given-names>E.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Visual communication in the workplace: A survey of practice</article-title>. <source>Technical Communication Quarterly</source>, <volume>16</volume>, <fpage>369</fpage>-<lpage>395</lpage>.</citation>
</ref>
<ref id="bibr8-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chou</surname><given-names>C.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Computer networks in communication survey research</article-title>. <source>IEEE Transactions on Professional Communication</source>, <volume>40</volume>(<issue>3</issue>), <fpage>197</fpage>-<lpage>208</lpage>.</citation>
</ref>
<ref id="bibr9-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christian</surname><given-names>L.</given-names></name>
<name><surname>Dillman</surname><given-names>D.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The influence of graphical and symbolic language manipulations on responses to self-administered questions</article-title>. <source>Public Opinion Quarterly</source>, <volume>68</volume>, <fpage>57</fpage>-<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr10-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Christian</surname><given-names>L.</given-names></name>
<name><surname>Dillman</surname><given-names>D.</given-names></name>
<name><surname>Smyth</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Helping respondents get it right the first time: The influence of words, symbols, and graphics in web surveys</article-title>. <source>Public Opinion Quarterly</source>, <volume>71</volume>, <fpage>113</fpage>-<lpage>125</lpage>.</citation>
</ref>
<ref id="bibr11-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Coon</surname><given-names>A. C.</given-names></name>
<name><surname>Scanlon</surname><given-names>P. M.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Does the curriculum fit the career? Some conclusions from a survey of graduates of a degree program in professional and technical communication</article-title>. <source>Journal of Technical Writing and Communication</source>, <volume>27</volume>, <fpage>391</fpage>-<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr12-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Couper</surname><given-names>M. P.</given-names></name>
<name><surname>Miller</surname><given-names>P. V.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Web survey methods: Introduction</article-title>. <source>Public Opinion Quarterly</source>, <volume>72</volume>, <fpage>831</fpage>-<lpage>835</lpage>.</citation>
</ref>
<ref id="bibr13-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Couper</surname><given-names>M. P.</given-names></name>
<name><surname>Traugott</surname><given-names>M.</given-names></name>
<name><surname>Lamias</surname><given-names>M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Web survey design and administration</article-title>. <source>Public Opinion Quarterly</source>, <volume>65</volume>, <fpage>230</fpage>-<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr14-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cox</surname><given-names>H.</given-names></name>
</person-group> (<year>1976</year>). <article-title>The voices of experience: The business communication alumnus reports</article-title>. <source>Journal of Business Communication</source>, <volume>13</volume>, <fpage>35</fpage>-<lpage>46</lpage>.</citation>
</ref>
<ref id="bibr15-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dawley</surname><given-names>D. D.</given-names></name>
<name><surname>Anthony</surname><given-names>W. P.</given-names></name>
</person-group> (<year>2003</year>). <article-title>User perceptions of e-mail at work</article-title>. <source>Journal of Business and Technical Communication</source>, <volume>17</volume>, <fpage>170</fpage>-<lpage>200</lpage>.</citation>
</ref>
<ref id="bibr16-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dayton</surname><given-names>D.</given-names></name>
<name><surname>Hopper</surname><given-names>K. B.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Single sourcing and content management: A survey of STC members</article-title>. <source>Technical Communication</source>, <volume>57</volume>, <fpage>375</fpage>-<lpage>397</lpage>.</citation>
</ref>
<ref id="bibr17-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dillman</surname><given-names>D.</given-names></name>
<name><surname>Caldwell</surname><given-names>S.</given-names></name>
<name><surname>Gansemer</surname><given-names>M.</given-names></name>
</person-group> (<year>2000</year>). <source>Visual design effects on item non-response to a question about work satisfaction that precedes the Q-12 agree-disagree items</source> (Research Report 1-10). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Gallup</publisher-name>.</citation>
</ref>
<ref id="bibr18-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dillman</surname><given-names>D. A.</given-names></name>
<name><surname>Smyth</surname><given-names>J. D.</given-names></name>
<name><surname>Christian</surname><given-names>L. M.</given-names></name>
</person-group> (<year>2009</year>). <source>Internet, mail, and mixed mode surveys: The tailored design method</source> (<edition>3rd ed.</edition>). <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr19-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fleming</surname><given-names>C.</given-names></name>
<name><surname>Bowden</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Web-based surveys as an alternative to traditional mail methods</article-title>. <source>Journal of Environmental Management</source>, <volume>90</volume>, <fpage>284</fpage>-<lpage>292</lpage>.</citation>
</ref>
<ref id="bibr20-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Galesic</surname><given-names>M.</given-names></name>
<name><surname>Bosnjak</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Effects of questionnaire length on participation and indicator of response quality in a web survey</article-title>. <source>Public Opinion Quarterly</source>, <volume>73</volume>, <fpage>349</fpage>-<lpage>360</lpage>.</citation>
</ref>
<ref id="bibr21-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Halpern</surname><given-names>J. W.</given-names></name>
</person-group> (<year>1981</year>). <article-title>What should we be teaching students in business writing?</article-title> <source>Journal of Business Communication</source>, <volume>18</volume>, <fpage>39</fpage>-<lpage>53</lpage>.</citation>
</ref>
<ref id="bibr22-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hughes</surname><given-names>M.</given-names></name>
<name><surname>Hayhoe</surname><given-names>G.</given-names></name>
</person-group> (<year>2007</year>). <source>A research primer for technical communication: Methods, exemplars, and analyses</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr23-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kirsch</surname><given-names>G.</given-names></name>
<name><surname>Sullivan</surname><given-names>P.</given-names></name>
</person-group> (<year>1992</year>). <source>Methods and methodology in composition research</source>. <publisher-loc>Carbondale</publisher-loc>: <publisher-name>Southern Illinois University Press</publisher-name>.</citation>
</ref>
<ref id="bibr24-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lanham</surname><given-names>R.</given-names></name>
</person-group> (<year>1993</year>). <source>The electronic word: Democracy, technology, and the arts</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr25-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lauer</surname><given-names>J.</given-names></name>
<name><surname>Asher</surname><given-names>J. W.</given-names></name>
</person-group> (<year>1988</year>). <source>Composition research: Empirical designs</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr26-0741088313488075">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lipka</surname><given-names>S.</given-names></name>
</person-group> (<year>2011</year>, <month>August</month> <day>7</day>). <article-title>Want data? Ask students. Again and again</article-title>. <source>Chronicle of Higher Education</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://chronicle.com/article/Want-Data-Ask-Students-Again/128537/">http://chronicle.com/article/Want-Data-Ask-Students-Again/128537/</ext-link></citation>
</ref>
<ref id="bibr27-0741088313488075">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lockett</surname><given-names>A.</given-names></name>
<name><surname>Losh</surname><given-names>E.</given-names></name>
<name><surname>Rieder</surname><given-names>M.</given-names></name>
<name><surname>Sample</surname><given-names>M.</given-names></name>
<name><surname>Stolley</surname><given-names>K.</given-names></name>
<name><surname>Vee</surname><given-names>A.</given-names></name>
</person-group> (<year>2012</year>). <article-title>The role of computational literacy in computers and writing</article-title>. <source>Enculturation: A Journal of Rhetoric, Writing, and Culture</source>, <volume>14</volume>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://enculturation.net/computational-literacy">http://enculturation.net/computational-literacy</ext-link></citation>
</ref>
<ref id="bibr28-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>McKee</surname><given-names>H.</given-names></name>
<name><surname>DeVoss</surname><given-names>D.</given-names></name>
</person-group> (Eds.). (<year>2008</year>). <source>Digital writing research: Technologies, methodologies, and ethical issues</source>. <publisher-loc>Cresskill, NJ</publisher-loc>: <publisher-name>Hampton Press</publisher-name>.</citation>
</ref>
<ref id="bibr29-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mirel</surname><given-names>B.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Writing and database technology: Extending the definition of writing in the workplace</article-title>. In <person-group person-group-type="editor">
<name><surname>Sullivan</surname><given-names>P.</given-names></name>
<name><surname>Dautermann</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Electronic literacies in the workplace: Technologies of writing</source> (pp. <fpage>91</fpage>-<lpage>114</lpage>). <publisher-loc>Urbana, IL</publisher-loc>: <publisher-name>National Council of Teachers of English</publisher-name>.</citation>
</ref>
<ref id="bibr30-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moss</surname><given-names>F. K.</given-names></name>
</person-group> (<year>1995</year>). <article-title>Perceptions of communication in the corporate community</article-title>. <source>Journal of Business and Technical Communication</source>, <volume>9</volume>, <fpage>63</fpage>-<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr31-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Nickoson</surname><given-names>L.</given-names></name>
<name><surname>Sheridan</surname><given-names>M.</given-names></name>
</person-group> (Eds.). (<year>2012</year>). <source>Writing studies research in practice: Methods and methodologies</source>. <publisher-loc>Carbondale</publisher-loc>: <publisher-name>Southern Illinois University Press</publisher-name>.</citation>
</ref>
<ref id="bibr32-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rogelberg</surname><given-names>S. G.</given-names></name>
<name><surname>Stanton</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Understanding and dealing with organizational survey nonresponse</article-title>. <source>Organizational Research Methods</source>, <volume>10</volume>, <fpage>195</fpage>-<lpage>209</lpage>.</citation>
</ref>
<ref id="bibr33-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sapp</surname><given-names>D. A.</given-names></name>
<name><surname>Zhang</surname><given-names>Q.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Trends in industry supervisors’ feedback on business communication internships</article-title>. <source>Business Communication Quarterly</source>, <volume>72</volume>, <fpage>274</fpage>-<lpage>288</lpage>.</citation>
</ref>
<ref id="bibr34-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sherblom</surname><given-names>J. C.</given-names></name>
<name><surname>Sullivan</surname><given-names>C. F.</given-names></name>
<name><surname>Sherblom</surname><given-names>E. C.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The what, the whom, and the hows of survey research</article-title>. <source>Business Communication Quarterly</source>, <volume>56</volume>, <fpage>58</fpage>-<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr35-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shropshire</surname><given-names>K.</given-names></name>
<name><surname>Hawdon</surname><given-names>J.</given-names></name>
<name><surname>Witte</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Web survey design: Balancing measurement, response, and topical interest</article-title>. <source>Sociological Methods &amp; Research</source>, <volume>37</volume>, <fpage>344</fpage>-<lpage>370</lpage>.</citation>
</ref>
<ref id="bibr36-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sinickas</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Finding a cure for survey fatigue</article-title>. <source>Strategic Communication Management</source>, <volume>11</volume>, <fpage>11</fpage>.</citation>
</ref>
<ref id="bibr37-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Smyth</surname><given-names>J.</given-names></name>
<name><surname>Dillman</surname><given-names>D.</given-names></name>
<name><surname>Christian</surname><given-names>L.</given-names></name>
<name><surname>Stern</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Effects of using visual design principles to group response options in web surveys</article-title>. <source>International Journal of Internet Science</source>, <volume>1</volume>, <fpage>6</fpage>-<lpage>16</lpage>.</citation>
</ref>
<ref id="bibr38-0741088313488075">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Stitt-Bergh</surname><given-names>M.</given-names></name>
<name><surname>Hilgers</surname><given-names>T.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Program assessment: Processes, propagation, and culture change</article-title>. <source>Across the Disciplines</source>, <volume>6</volume>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://wac.colostate.edu/atd/assessment/stittbergh_hilgers.cfm">http://wac.colostate.edu/atd/assessment/stittbergh_hilgers.cfm</ext-link></citation>
</ref>
<ref id="bibr39-0741088313488075">
<citation citation-type="book">
<collab>Sun Microsystems</collab>. (<year>2009</year>). <source>Read me first! A style guide for the computer industry</source> (<edition>3rd ed.</edition>). <publisher-loc>Santa Clara, CA</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr40-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Thompson</surname><given-names>I.</given-names></name>
<name><surname>Whyte</surname><given-names>A.</given-names></name>
<name><surname>Shannon</surname><given-names>D.</given-names></name>
<name><surname>Muse</surname><given-names>A.</given-names></name>
<name><surname>Miller</surname><given-names>K.</given-names></name>
<name><surname>Chappell</surname><given-names>M.</given-names></name>
<name><surname>Whigham</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Examining our lore: A survey of students’ and tutors’ satisfaction with writing center conferences</article-title>. <source>Writing Center Journal</source>, <volume>29</volume>, <fpage>78</fpage>-<lpage>105</lpage>.</citation>
</ref>
<ref id="bibr41-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tourangeau</surname><given-names>R.</given-names></name>
<name><surname>Couper</surname><given-names>M.</given-names></name>
<name><surname>Conrad</surname><given-names>F.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Spacing, position, and order: Interpretive heuristics for visual features of survey questions</article-title>. <source>Public Opinion Quarterly</source>, <volume>68</volume>, <fpage>368</fpage>-<lpage>393</lpage>.</citation>
</ref>
<ref id="bibr42-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Tuten</surname><given-names>T.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Conducting online surveys</article-title>. In <person-group person-group-type="editor">
<name><surname>Gosling</surname><given-names>S.</given-names></name>
<name><surname>Johnson</surname><given-names>J.</given-names></name>
</person-group> (Eds.), <source>Advanced methods for conducting online behavioral research</source> (pp. <fpage>179</fpage>-<lpage>192</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Psychological Association</publisher-name>.</citation>
</ref>
<ref id="bibr43-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Vehovar</surname><given-names>V.</given-names></name>
<name><surname>Batagelj</surname><given-names>Z.</given-names></name>
<name><surname>Manfreda</surname><given-names>K. L.</given-names></name>
<name><surname>Zaletel</surname><given-names>M.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Nonresponse in web surveys</article-title>. In <person-group person-group-type="editor">
<name><surname>Groves</surname><given-names>R. M.</given-names></name>
<name><surname>Dillman</surname><given-names>D. A.</given-names></name>
<name><surname>Eltinge</surname><given-names>J. L.</given-names></name>
<name><surname>Little</surname><given-names>R. J. A.</given-names></name>
</person-group> (Eds.), <source>Survey nonresponse</source> (pp. <fpage>229</fpage>-<lpage>242</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr44-0741088313488075">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Weiser</surname><given-names>I.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Local research and curriculum development: Using surveys to learn about writing assignments in the disciplines</article-title>. In <person-group person-group-type="editor">
<name><surname>Rose</surname><given-names>S.</given-names></name>
<name><surname>Weiser</surname><given-names>I.</given-names></name>
</person-group> (Eds.), <source>The writing program administrator as researcher</source> (pp. <fpage>95</fpage>-<lpage>106</lpage>). <publisher-loc>Portsmouth, NH</publisher-loc>: <publisher-name>Boynton/Cook Heinemann</publisher-name>.</citation>
</ref>
<ref id="bibr45-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Whiteside</surname><given-names>A. L.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The skills that technical communicators need: An investigation of technical communication graduates, managers, and curricula</article-title>. <source>Journal of Technical Writing and Communication</source>, <volume>33</volume>, <fpage>303</fpage>-<lpage>318</lpage>.</citation>
</ref>
<ref id="bibr46-0741088313488075">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zimmerman</surname><given-names>B.</given-names></name>
<name><surname>Schultz</surname><given-names>J.</given-names></name>
</person-group> (<year>2000</year>). <article-title>A study of the effectiveness of information design principles applied to clinical research questionnaires</article-title>. <source>Technical Communication</source>, <volume>47</volume>, <fpage>177</fpage>-<lpage>194</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>