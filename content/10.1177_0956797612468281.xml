<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612468281</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612468281</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Females Scan More Than Males</article-title>
<subtitle>A Potential Mechanism for Sex Differences in Recognition Memory</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Heisz</surname><given-names>Jennifer J.</given-names></name>
<xref ref-type="aff" rid="aff1-0956797612468281">1</xref>
<xref ref-type="aff" rid="aff2-0956797612468281">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Pottruff</surname><given-names>Molly M.</given-names></name>
<xref ref-type="aff" rid="aff3-0956797612468281">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Shore</surname><given-names>David I.</given-names></name>
<xref ref-type="aff" rid="aff3-0956797612468281">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-0956797612468281"><label>1</label>Rotman Research Institute at Baycrest, Toronto, Ontario, Canada</aff>
<aff id="aff2-0956797612468281"><label>2</label>Department of Kinesiology, McMaster University</aff>
<aff id="aff3-0956797612468281"><label>3</label>Department of Psychology, Neuroscience &amp; Behaviour, McMaster University</aff>
<author-notes>
<corresp id="corresp1-0956797612468281">Jennifer J. Heisz, Department of Kinesiology, McMaster University, 1280 Main St. West, Hamilton, Ontario L8S 4K1, Canada E-mail: <email>heiszjj@mcmaster.ca</email></corresp>
<corresp id="corresp2-0956797612468281">David I. Shore, Department of Psychology, Neuroscience &amp; Behaviour, McMaster University, 1280 Main St. West, Hamilton, Ontario L8S 4K1, Canada E-mail: <email>dshore@mcmaster.ca</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>24</volume>
<issue>7</issue>
<fpage>1157</fpage>
<lpage>1163</lpage>
<history>
<date date-type="received">
<day>14</day>
<month>7</month>
<year>2010</year>
</date>
<date date-type="accepted">
<day>24</day>
<month>10</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>Recognition-memory tests reveal individual differences in episodic memory; however, by themselves, these tests provide little information regarding the stage (or stages) in memory processing at which differences are manifested. We used eye-tracking technology, together with a recognition paradigm, to achieve a more detailed analysis of visual processing during encoding and retrieval. Although this approach may be useful for assessing differences in memory across many different populations, we focused on sex differences in face memory. Females outperformed males on recognition-memory tests, and this advantage was directly related to females’ scanning behavior at encoding. Moreover, additional exposures to the faces reduced sex differences in face recognition, which suggests that males may be able to improve their recognition memory by extracting more information at encoding through increased scanning. A strategy of increased scanning at encoding may prove to be a simple way to enhance memory performance in other populations with memory impairment.</p>
</abstract>
<kwd-group>
<kwd>individual differences</kwd>
<kwd>faces</kwd>
<kwd>memory</kwd>
<kwd>recognition</kwd>
<kwd>eye movements</kwd>
<kwd>sex differences</kwd>
<kwd>face perception</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Recollection of personally experienced events (i.e., mental time travel) represents one of our greatest cognitive capabilities (<xref ref-type="bibr" rid="bibr21-0956797612468281">Tulving, 2002</xref>); the information gained through experience often guides current and future behavior. Reexperienced events are not replicas of the past but reconstructions based on past instances and the context in which those instances were stored (<xref ref-type="bibr" rid="bibr2-0956797612468281">Bartlett, 1932</xref>; <xref ref-type="bibr" rid="bibr9-0956797612468281">Jacoby &amp; Brooks, 1984</xref>). These recollections reflect particulars of the individual’s memory system at the time of encoding (<xref ref-type="bibr" rid="bibr20-0956797612468281">Tulving, 1984</xref>). The present study used eye-tracking technology to assess individual differences in episodic memory, focusing specifically on sex differences.</p>
<p>Females have richer autobiographical experiences than do males (<xref ref-type="bibr" rid="bibr1-0956797612468281">Andreano &amp; Cahill, 2009</xref>), recounting autobiographical events that are more detailed (<xref ref-type="bibr" rid="bibr16-0956797612468281">Pillemer, Wink, DiDonato, &amp; Sanborn, 2003</xref>) and dating those events with greater precision (<xref ref-type="bibr" rid="bibr18-0956797612468281">Skowronski, Betz, Thompson, &amp; Shannon, 1991</xref>). Even when unprompted, females provide descriptions of personally experienced events that are more elaborate than those provided by males (<xref ref-type="bibr" rid="bibr4-0956797612468281">Friedman &amp; Pines, 1991</xref>). Although these subjective reports are compelling, they are difficult to assess objectively.</p>
<p>In the laboratory, we can create highly controlled events and examine a participant’s memory for those events using a recognition test (<xref ref-type="bibr" rid="bibr19-0956797612468281">Tulving, 1983</xref>). Tests of recognition examine the participant’s ability to judge whether a stimulus (e.g., a face) was presented previously (<xref ref-type="bibr" rid="bibr10-0956797612468281">Kelly &amp; Jacoby, 2000</xref>; <xref ref-type="bibr" rid="bibr23-0956797612468281">Yonelinas, 2002</xref>). Females show superior recognition memory compared with males (<xref ref-type="bibr" rid="bibr7-0956797612468281">Herlitz, Nilsson, &amp; Bäckman, 1997</xref>; <xref ref-type="bibr" rid="bibr8-0956797612468281">Herlitz &amp; Rehnman, 2008</xref>; <xref ref-type="bibr" rid="bibr17-0956797612468281">Shapiro &amp; Penrod, 1986</xref>; but see <xref ref-type="bibr" rid="bibr12-0956797612468281">Maccoby &amp; Jacklin, 1974</xref>). However, one problem with a standard recognition-memory test is the lack of specificity concerning the processing stage during which these sex differences originate (i.e., during encoding, retention, or retrieval).</p>
<p>Eye-tracking technology provides a tool for exploring whether sex differences observed in recognition memory originate during encoding or retrieval. Compared with males, females may extract more information at encoding, thus producing a richer memory representation, or they may process the stimulus in a way that maximizes the relevant information.</p>
<p>To characterize potential sex differences in face encoding, we recorded participants’ eye movements while they were introduced to new faces by name. We then examined whether sex differences in visual processing at encoding could explain typical sex effects in memory at test. To do this, we measured subsequent memory performance using a recognition test and determined whether the typical sex effects in memory could be eliminated when eye movement behavior at encoding was included as a covariate. If observed, this result would provide strong support for the hypothesis that the differences in encoding account for the typical sex difference observed in memory performance at test.</p>
<p>We also were interested in potential sex differences in the ability to integrate past related instances into a coherent representation, which could be used to support subsequent processing. We expected participants to show a progressive decrease in the total number of fixations for repeatedly presented faces, in line with previous research (<xref ref-type="bibr" rid="bibr5-0956797612468281">Heisz &amp; Ryan, 2011</xref>; <xref ref-type="bibr" rid="bibr6-0956797612468281">Heisz &amp; Shore, 2008</xref>). Observing a similar behavioral pattern across the sexes would suggest that this aspect of memory is comparable. Moreover, most previous studies have assessed sex differences after a single exposure within a single day; providing multiple opportunities to establish a memory representation may boost males’ memory performance and, thus, minimize typical sex effects.</p>
<sec id="section1-0956797612468281" sec-type="methods">
<title>Method</title>
<sec id="section2-0956797612468281">
<title>Participants</title>
<p>In total, 40 females (mean age = 19.5 years, <italic>SD</italic> = 1.6) and 40 males (mean age = 19.7 years, <italic>SD</italic> = 1.8) from the McMaster University community participated in this study. Twenty participants (10 males, 10 females) participated in a 4-day experiment and received either 40 Canadian (Can) dollars or Can$20 in addition to course credit. The remaining participants took part in a 1-day experiment in return for either Can$15 or course credit. All participants reported normal or corrected-to-normal vision. Informed consent was obtained from each participant. The data of 10 females and 1 male who participated in the 4-day experiment have been published previously in <xref ref-type="bibr" rid="bibr6-0956797612468281">Heisz and Shore (2008)</xref>.</p>
</sec>
<sec id="section3-0956797612468281">
<title>Apparatus and stimuli</title>
<p>A Dell computer was used to collect eye movement data using the EyeLink II system (Version 1.1). Stimuli were presented on a ViewSonic Professional Series P220f monitor.</p>
<p>Stimuli consisted of 120 face photographs (60 female, 60 male), randomly selected for each participant from a larger set (<xref ref-type="bibr" rid="bibr15-0956797612468281">Mondloch, Geldart, Maurer, &amp; Le Grand, 2003</xref>). All photographs were black-and-white and portrayed Caucasian faces with neutral expressions. An elliptical mask was used to isolate each face from the middle of the forehead to the lower chin (including the eyebrows and the outer margins of the eyes). The faces were centrally presented at a constant viewing distance of 80 cm and measured approximately 7.9° of visual angle in height and 5.7° of visual angle in width.</p>
<p>Female and male names were selected from the U.S. Census Bureau’s Documentation and Methodology for Frequently Occurring Names in the U.S., circa 1990 (<xref ref-type="bibr" rid="bibr22-0956797612468281">U.S. Census Bureau, 2011</xref>); the first 30 names were used.</p>
</sec>
<sec id="section4-0956797612468281">
<title>Procedure</title>
<sec id="section5-0956797612468281">
<title>Four-day experiment</title>
<p>The 4-day experiment was conducted over 4 consecutive days (see <xref ref-type="bibr" rid="bibr6-0956797612468281">Heisz &amp; Shore, 2008</xref>, for further details). Sixty faces (all female) were divided randomly into four sets. Sets A, B, and C consisted of 10 faces, each randomly assigned 1 of the 30 names; the remaining 30 faces made up Set D. The experiment consisted of initial encoding trials on Days 1, 2, and 3; cued-recall learning trials on Days 1, 2, 3, and 4; and recognition-test trials on Day 4. Each phase began with calibration and validation of the eye tracker, and each trial was initiated on the participant’s fixation of the central fixation dot, which corrected for any drift in gaze position.</p>
<sec id="section6-0956797612468281">
<title>Initial encoding</title>
<p>Ten novel faces were introduced consecutively by name. Each face was presented for 5 s, and participants were instructed to learn it by name. Sets A, B, and C were introduced on Days 1, 2, and 3, respectively. Faces were classified according to the number of prior exposures.</p>
</sec>
<sec id="section7-0956797612468281">
<title>Cued-recall learning</title>
<p>On each day of the experiment, all previously introduced faces were represented in random order and participants were asked to name each face. Once a participant responded, the face was removed and the correct name was presented aurally. The number of trials per learning session increased according to the number of faces that previously had been introduced in the experiment (details about the cued-recall tests are presented in the Supplemental Material available online).</p>
</sec>
<sec id="section8-0956797612468281">
<title>Recognition</title>
<p>On Day 4, the 30 previously introduced faces (Sets A, B, and C) plus 30 novel faces (Set D) were individually presented in random order. Participants were asked to indicate whether each face had been introduced previously by pressing one of two response keys (counterbalanced across participants). Once the participant responded (or after 3 s of presentation), the face was removed from the display. No response feedback was provided.</p>
</sec>
</sec>
<sec id="section9-0956797612468281">
<title>One-day experiment</title>
<p>The design of the 1-day experiment was similar to that of the 4-day experiment, with the following exceptions: The entire experiment was conducted during a single day, only one learning session (instead of two) was conducted for each new face set, and memory was tested for both female and male faces, in different sets of observers.</p>
<p>It is important to note that new faces were introduced throughout both experiments and grouped according to the total number of prior exposures. As noted in the Initial Encoding section, for the 4-day experiment, this procedure meant that 10 new faces were introduced on each of the first 3 days of testing. For the 1-day experiment, the three sets of 10 faces were introduced on the same day, spaced throughout the session. Faces were introduced throughout the experiments to control for effects of habituation and fatigue on eye movement behavior. However, this procedure created a difference between experiments with respect to the temporal interval at which faces were learned and tested, which may have affected performance. The between-subjects design allowed us to check for effects of protocol change (4-day experiment vs. 1-day experiment) by comparing face memory across experiments.</p>
</sec>
</sec>
<sec id="section10-0956797612468281">
<title>Data analysis</title>
<sec id="section11-0956797612468281">
<title>Behavioral data</title>
<p>Recognition data were statistically analyzed using signal detection (<xref ref-type="bibr" rid="bibr13-0956797612468281">MacMillan &amp; Creelman, 2005</xref>): <italic>d</italic>′ = <italic>z</italic>(hits) – <italic>z</italic>(false alarms). Following guidelines outlined in <xref ref-type="bibr" rid="bibr13-0956797612468281">MacMillan and Creelman (2005)</xref>, we corrected for infinite values by converting 1 to .98 [1 – (0.5/<italic>N</italic>)] and 0 to .02 (0.5/<italic>N</italic>), respectively, where <italic>N</italic> is the number of trials for old versus new conditions (<italic>N</italic> = 30). The mean proportions of hits and false alarms for each experiment are reported in Table S1 in the Supplemental Material.</p>
</sec>
<sec id="section12-0956797612468281">
<title>Eye movements</title>
<p>Three percent of trials were excluded because of a calibration error. Eye movement analysis included correct and incorrect trials; both types of trials yielded similar results when analyzed separately. The total presentation time for initial encoding was 5 s. To control for variable viewing times across memory trials of the 4-day experiment, we limited analysis to the 1st second of recognition-test trials (0.4% of trials were &lt; 1 s). For the 1-day experiment, we used a fixed window for eye movement analyses to avoid these limitations; during test trials, faces were presented for 2 s, after which participants responded.</p>
<p>Eye movement measures were analyzed with respect to the individual faces and to the inner features (eyes, nose, and mouth) within each face. Fixations to the outside of the facial image were excluded from analysis (0.9% of all fixations). To accommodate different placement of features, three nonoverlapping feature templates were used. The medium feature template was used for 61% of the stimuli, and the high and low feature templates were used for 14% and 25% of the stimuli, respectively. All three templates covered the same area (<xref ref-type="bibr" rid="bibr6-0956797612468281">Heisz &amp; Shore, 2008</xref>). From the eye movement data, we extracted the mean number of fixations made to each face and the mean proportion of the total number of fixations made to the eyes, nose, and mouth for each face.</p>
</sec>
</sec>
<sec id="section13-0956797612468281">
<title>Statistical analysis</title>
<p>Omnibus analysis of variance (ANOVA) tests were conducted separately for the 4-day and 1-day experiments. We assessed sex differences in visual processing at encoding using ANOVAs conducted on the total number of fixations at encoding and the proportion of fixations to each of the inner features. Each analysis had a between-subjects factor of participants’ sex (female or male); proportion analyses had an additional within-subjects factor of feature (eyes, nose, or mouth). We also assessed sex differences in both memory performance and visual processing during the recognition test using ANOVAs conducted on <italic>d</italic>′ values and the number of fixations; each analysis had a between-subjects factor of participants’ sex (female or male) and a within-subjects factor of number of exposures—4-day experiment: two, four, or six; 1-day experiment: two, three, or four. All analyses for the 1-day experiment included an additional between-subjects factor of face sex (female or male), which did not significantly contribute to any of the effects.</p>
</sec>
</sec>
<sec id="section14-0956797612468281" sec-type="results">
<title>Results</title>
<sec id="section15-0956797612468281">
<title>Typical sex differences in memory are modulated by learning and test conditions</title>
<p>As a starting point, we examined whether the typical female advantage in recognition memory was affected by repeated exposure to the faces or by the context under which the new material was learned (4-day experiment vs. 1-day experiment). We observed the typical female recognition advantage over males in the 1-day experiment (<xref ref-type="fig" rid="fig1-0956797612468281">Fig. 1</xref>), which was revealed by a significant main effect of participants’ sex, <italic>F</italic>(1, 56) = 4.90, <italic>p</italic> &lt; .05, η<sub><italic>p</italic></sub><sup>2</sup> = .08. Repeated exposures increased memory performance in both experiments, as evidenced by a significant linear contrast of exposure in recognition—4-day experiment: <italic>F</italic>(1, 18) = 90.40, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .83; 1-day experiment: <italic>F</italic>(1, 56) = 23.71, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .29. For the 4-day experiment, in which faces were repeated across multiple days, the typical female advantage was observed only for faces with the least amount of prior exposure (i.e., two prior exposures), <italic>t</italic>(18) = 1.81 (one-tailed), <italic>p</italic> &lt; .05, <italic>d</italic> = 0.85.</p>
<fig id="fig1-0956797612468281" position="float">
<label>Fig. 1.</label>
<caption>
<p>Recognition-test performance for repeatedly presented faces as a function of the number of prior exposures and participants’ sex. Results are shown separately for the (a) 4-day experiment and (b) 1-day experiment. Error bars represent standard errors of the mean. Asterisks indicate significant differences between males and females (*<italic>p</italic> &lt; .05).</p>
</caption>
<graphic xlink:href="10.1177_0956797612468281-fig1.tif"/>
</fig>
</sec>
<sec id="section16-0956797612468281">
<title>Females make more fixations at encoding, and these increased fixations produce the memory difference</title>
<p>During initial encoding, females made more fixations than males (see <xref ref-type="fig" rid="fig2-0956797612468281">Fig. 2</xref>)—4-day experiment: <italic>F</italic>(1, 18) = 15.99, <italic>p</italic> &lt; .01, η<sub><italic>p</italic></sub><sup>2</sup> = .47; 1-day experiment: <italic>F</italic>(1, 56) = 4.14, <italic>p</italic> &lt; .05, η<sub><italic>p</italic></sub><sup>2</sup> = .07. There were no sex differences in the distribution of fixations across the inner features of the faces during initial encoding (<xref ref-type="table" rid="table1-0956797612468281">Table 1</xref>), all <italic>F</italic>s &lt; 1. However, across repeated exposures, females directed a greater proportion of fixations to the eyes of female faces compared with the eyes of male faces; this pattern was not observed for male or female faces among male participants (see Fig. S1 in the Supplemental Material), which suggests that females quickly learn to process other female faces efficiently by directing fixations to highly informative face regions—a process that may ultimately result in superior recollection.</p>
<fig id="fig2-0956797612468281" position="float">
<label>Fig. 2.</label>
<caption>
<p>Mean number of fixations during the initial encoding of previously unfamiliar faces as a function of experiment and participants’ sex. Error bars represent standard errors of the mean. Asterisks indicate significant differences between males and females (*<italic>p</italic> &lt; .05, **<italic>p</italic> &lt; .001).</p>
</caption>
<graphic xlink:href="10.1177_0956797612468281-fig2.tif"/>
</fig>
<table-wrap id="table1-0956797612468281" position="float">
<label>Table 1.</label>
<caption>
<p>Proportion of Fixations Made to the Eyes, Nose, and Mouth During the Initial Encoding of Previously Unfamiliar Faces</p>
</caption>
<graphic alternate-form-of="table1-0956797612468281" xlink:href="10.1177_0956797612468281-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">Fixation<hr/></th>
</tr>
<tr>
<th align="left">Experiment</th>
<th align="center">Eyes</th>
<th align="center">Nose</th>
<th align="center">Mouth</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="4">4-day experiment</td>
</tr>
<tr>
<td> Females</td>
<td>.56 (.03)</td>
<td>.19 (.02)</td>
<td>.17 (.02)</td>
</tr>
<tr>
<td> Males</td>
<td>.57 (.03)</td>
<td>.21 (.03)</td>
<td>.10 (.01)</td>
</tr>
<tr>
<td colspan="4">1-day experiment</td>
</tr>
<tr>
<td> Females</td>
<td>.54 (.02)</td>
<td>.22 (.02)</td>
<td>.15 (.01)</td>
</tr>
<tr>
<td> Males</td>
<td>.53 (.02)</td>
<td>.26 (.02)</td>
<td>.12 (.01)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0956797612468281">
<p>Note: Standard errors of the mean are shown in parentheses.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>To examine the relation between the number of fixations made at encoding and subsequent recognition performance, we first conducted an ANOVA on mean recognition performance with a between-subjects factor of participants’ sex (collapsed across exposure and experiment) and observed a main effect, which demonstrated the typical female advantage, <italic>F</italic>(1, 78) = 5.67, <italic>p</italic> &lt; .05, η<sub><italic>p</italic></sub><sup>2</sup> = .07. We then included the number of fixations made at encoding as a covariate. Removing the influence of number of fixations at encoding eliminated the female advantage in recognition memory, <italic>F</italic>(1, 77) = 2.28, n.s.</p>
</sec>
<sec id="section17-0956797612468281">
<title>The relation between fixations at encoding and memory is a general individual difference</title>
<p>Ignoring the factor of sex, we tested whether the observed relation between scanning during encoding and subsequent recognition memory reflected a general individual difference. Pearson product–moment correlation tests were conducted on mean number of fixations at encoding and subsequent recognition performance at test, collapsing across the factor of exposure (see <xref ref-type="fig" rid="fig3-0956797612468281">Fig. 3</xref>). We observed a significant correlation between fixations at encoding and recognition-memory performance at test for the 4-day experiment, <italic>r</italic>(18) = .49, <italic>p</italic> &lt; .05, <italic>r</italic><sup>2</sup> = .23, and the 1-day experiment, <italic>r</italic>(58) = .38, <italic>p</italic> &lt; .01, <italic>r</italic><sup>2</sup> = .14.</p>
<fig id="fig3-0956797612468281" position="float">
<label>Fig. 3.</label>
<caption>
<p>Scatter plots (with best-fitting regression lines) showing recognition memory at test as a function of mean number of fixations during encoding. Results are shown separately for the (a) 4-day experiment and (b) 1-day experiment.</p>
</caption>
<graphic xlink:href="10.1177_0956797612468281-fig3.tif"/>
</fig>
</sec>
<sec id="section18-0956797612468281">
<title>Eye movements did not reveal sex differences in the effect of multiple exposures</title>
<p>During the recognition test, the number of previous exposures affected the number of fixations equally for females and males. Both females and males showed a progressive decrease in fixations to the faces across repeated exposures (see <xref ref-type="table" rid="table2-0956797612468281">Table 2</xref>), as evidenced by the significant linear contrast of exposure—4-day experiment: <italic>F</italic>(1, 18) = 19.05, <italic>p</italic> &lt; .001, η<sub><italic>p</italic></sub><sup>2</sup> = .51; 1-day experiment: <italic>F</italic>(1, 56) = 6.43, <italic>p</italic> &lt; .05, η<sub><italic>p</italic></sub><sup>2</sup> = .10.</p>
<table-wrap id="table2-0956797612468281" position="float">
<label>Table 2.</label>
<caption>
<p>Total Number of Fixations Made to Faces During the Recognition Test for Faces With Varying Numbers of Past Exposures</p>
</caption>
<graphic alternate-form-of="table2-0956797612468281" xlink:href="10.1177_0956797612468281-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Experiment and number of exposures</th>
<th align="center">Females</th>
<th align="center">Males</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">4-day experiment</td>
</tr>
<tr>
<td> 2 exposures</td>
<td>3.28 (.15)</td>
<td>3.07 (.14)</td>
</tr>
<tr>
<td> 4 exposures</td>
<td>2.82 (.27)</td>
<td>2.84 (.16)</td>
</tr>
<tr>
<td> 6 exposures</td>
<td>2.64 (.27)</td>
<td>2.77 (.23)</td>
</tr>
<tr>
<td colspan="3">1-day experiment</td>
</tr>
<tr>
<td> 2 exposures</td>
<td>5.94 (.19)</td>
<td>5.55 (.17)</td>
</tr>
<tr>
<td> 3 exposures</td>
<td>5.84 (.20)</td>
<td>5.56 (.18)</td>
</tr>
<tr>
<td> 4 exposures</td>
<td>5.67 (.20)</td>
<td>5.50 (.18)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0956797612468281">
<p>Note: Standard errors of the mean are shown in parentheses.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
<sec id="section19-0956797612468281" sec-type="discussion">
<title>Discussion</title>
<p>Across various types of episodic stimuli, females tend to outperform males on recognition-memory tests (<xref ref-type="bibr" rid="bibr7-0956797612468281">Herlitz et al., 1997</xref>; <xref ref-type="bibr" rid="bibr8-0956797612468281">Herlitz &amp; Rehnman, 2008</xref>; <xref ref-type="bibr" rid="bibr17-0956797612468281">Shapiro &amp; Penrod, 1986</xref>). However, given the nature of such tests, the stage of processing at which these differences emerge has been unknown. In the present study, we used eye-tracking technology to achieve a more detailed analysis of sex differences during recognition. Sex differences were observed in the visual processing of faces at encoding, and these differences contributed to subsequent sex differences in recognition memory at test.</p>
<p>When males initially processed the faces, they made fewer fixations than females did, which suggests that they may have gathered less perceptual information to establish a memory representation for the newly learned faces. At test, this lower number of fixations translated into the typical sex difference in recognition memory, with females showing superior performance compared with males. Critically, when we controlled for the number of fixations made during encoding, the sex difference in recognition memory was no longer observed. These results demonstrate a direct link between visual processing at encoding and the subsequent recognition of repeatedly presented materials at test, providing a mechanism for sex differences in recognition memory.</p>
<p>It is interesting to consider whether sex differences in visual processing at encoding could sufficiently explain sex differences in recognition memory. Evidence in support of this idea comes from the similar changes observed for both sexes in eye movement behavior and recognition performance across repetition. Repeatedly presenting faces created multiple opportunities to establish a memory representation. Both sexes evinced decreased fixations and improved recognition performance for repeated faces such that the sex effect ultimately was eliminated for the faces that were seen most often.</p>
<p>The decrease in fixations observed for repeated faces suggests more efficient scanning of familiar faces by both males and females. According to this account, a previously viewed face cues the retrieval of associated memories, including a stored representation of the face image itself. The particular details of the stored representation may guide current visual processing, allowing observers to seek out new information or information that potentially was not well encoded during past exposures. Repeatedly viewing the same face gradually increases the amount of stored information available for that face and, in turn, reduces the amount of visual processing needed to encode it. The absence of a sex difference in the effect of exposure on recognition memory provides indirect support for the claim that sex differences in recognition memory originate at encoding, given that males appeared to eventually reach the same recognition accuracy level as their female counterparts. Critically, this result suggests that males’ recognition memory could be improved by implementing a strategy that encourages more eye movements during initial encoding.</p>
<p>Although increased scanning at encoding provides a clear mechanism for sex differences in face recognition, a similar visual-processing explanation may account for other group differences in memory performance. Evidence in support of this explanation comes from our correlation analyses, in which we ignored the factor of sex and continued to observe a relation between visual processing at encoding and subsequent recognition at test (see <xref ref-type="fig" rid="fig3-0956797612468281">Fig. 3</xref>). These results highlight the utility of eye-tracking technology as a means of parsing traditional memory paradigms into finer memory-processing stages to pinpoint the origin of a memory difference.</p>
<p>It is important to note that the current results apply to recognition memory only. Performance on the embedded cued-recall tests was much more complicated and did not show the straightforward pattern of more scanning at encoding producing better memory performance (see the Supplemental Material). This result is not surprising, given that recall and recognition tasks appear to tap into different mnemonic processes (<xref ref-type="bibr" rid="bibr3-0956797612468281">Enns &amp; Shore, 1997</xref>; <xref ref-type="bibr" rid="bibr23-0956797612468281">Yonelinas, 2002</xref>). Specifically, recognition can be based on familiarity, whereas cued-recall involves recollection processes (<xref ref-type="bibr" rid="bibr10-0956797612468281">Kelly &amp; Jacoby, 2000</xref>; <xref ref-type="bibr" rid="bibr23-0956797612468281">Yonelinas, 2002</xref>). Heightened visual scanning during the initial encoding of a face may improve memory for the face image and, thus, directly affect face familiarity. In contrast, memory binding between the name and the face is required for the cued-recall test, and males seem to have difficulty making this association. This difficulty may be due to a deficit in binding associative information (i.e., the associative hookup; <xref ref-type="bibr" rid="bibr14-0956797612468281">McGuire, 1961</xref>). Alternatively, the observed sex differences in cued-recall memory may reflect sex differences favoring females in verbal processing of the names (<xref ref-type="bibr" rid="bibr11-0956797612468281">Lewin, Wolgers, &amp; Herlitz, 2001</xref>). Regardless of the specific underlying process, sex differences driving the cued-recall effect cannot be explained fully by the number of eye movements at encoding.</p>
<p>In conclusion, we observed sex differences in the visual processing of faces during initial encoding. Compared with males, females made more fixations across the face images during initial encoding, and this tendency was related to better memory performance at test. Repeatedly presenting the faces provided additional opportunities for visual processing and minimized sex differences.</p>
</sec>
</body>
<back>
<ack>
<p>The authors thank Hervé Abdi, Bruce Milliken, and the cognition group in the Department of Psychology, Neuroscience &amp; Behaviour at McMaster University for many thoughtful discussions. The authors also thank Craig Wilson for programming the experiments and Sarah Lade for her assistance in running the 1-day experiment.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research was supported by the Natural Sciences and Engineering Research Council of Canada—Canada Graduate Scholarships (NSERC-CGS) to J. J. H. and M. M. P., as well as a Discovery Grant to D. I. S. Further support was supplied through a Premier’s Research Excellence Award and a New Opportunities Award from the Canada Foundation for Innovation and the Ontario Innovation Trust to D. I. S.</p>
</fn>
<fn fn-type="supplementary-material">
<label>Supplemental Material</label>
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Andreano</surname><given-names>J. M.</given-names></name>
<name><surname>Cahill</surname><given-names>L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Sex influences on the neurobiology of learning and memory</article-title>. <source>Learning &amp; Memory</source>, <volume>16</volume>, <fpage>248</fpage>–<lpage>266</lpage>.</citation>
</ref>
<ref id="bibr2-0956797612468281">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bartlett</surname><given-names>F. C.</given-names></name>
</person-group> (<year>1932</year>). <source>Remembering: A study in experimental and social psychology</source>. <publisher-loc>Cambridge University Press</publisher-loc>: <publisher-name>Cambridge, England</publisher-name>.</citation>
</ref>
<ref id="bibr3-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Enns</surname><given-names>J. T.</given-names></name>
<name><surname>Shore</surname><given-names>D. I.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Separate influences of orientation and lighting in the inverted-face effect</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>59</volume>, <fpage>23</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr4-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Friedman</surname><given-names>A.</given-names></name>
<name><surname>Pines</surname><given-names>A.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Sex differences in gender-related childhood memories</article-title>. <source>Sex Roles</source>, <volume>25</volume>, <fpage>25</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Heisz</surname><given-names>J. J.</given-names></name>
<name><surname>Ryan</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The effects of prior exposure of face processing in younger and older adults</article-title>. <source>Frontiers in Aging Neuroscience</source>, <volume>3</volume>, <fpage>15</fpage>.</citation>
</ref>
<ref id="bibr6-0956797612468281">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Heisz</surname><given-names>J. J.</given-names></name>
<name><surname>Shore</surname><given-names>D. I.</given-names></name>
</person-group> (<year>2008</year>). <article-title>More efficient scanning for familiar faces</article-title>. <source>Journal of Vision</source>, <volume>8</volume>(<issue>1</issue>), Article <fpage>9</fpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/8/1/9">http://www.journalofvision.org/content/8/1/9</ext-link></citation>
</ref>
<ref id="bibr7-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Herlitz</surname><given-names>A.</given-names></name>
<name><surname>Nilsson</surname><given-names>L. G.</given-names></name>
<name><surname>Bäckman</surname><given-names>L.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Gender differences in episodic memory</article-title>. <source>Memory &amp; Cognition</source>, <volume>25</volume>, <fpage>801</fpage>–<lpage>811</lpage>.</citation>
</ref>
<ref id="bibr8-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Herlitz</surname><given-names>A.</given-names></name>
<name><surname>Rehnman</surname><given-names>J.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Sex differences in episodic memory</article-title>. <source>Current Directions in Psychological Science</source>, <volume>17</volume>, <fpage>52</fpage>–<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612468281">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Jacoby</surname><given-names>L. L.</given-names></name>
<name><surname>Brooks</surname><given-names>L. R.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Nonanalytic cognition: Memory, perception and concept formation</article-title>. In <person-group person-group-type="editor">
<name><surname>Bower</surname><given-names>G.</given-names></name>
</person-group> (Ed.), <source>The psychology of learning and motivation: Advances in research and theory</source> (<volume>Vol. 18</volume>, pp. <fpage>1</fpage>–<lpage>47</lpage>). <publisher-loc>Orlando, FL</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-0956797612468281">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kelly</surname><given-names>C. M.</given-names></name>
<name><surname>Jacoby</surname><given-names>L. L.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Recollection and familiarity: Process-dissociation</article-title>. In <person-group person-group-type="editor">
<name><surname>Tulving</surname><given-names>E.</given-names></name>
<name><surname>Craik</surname><given-names>F. I. M.</given-names></name>
</person-group> (Eds.), <source>The Oxford handbook of memory</source> (pp. <fpage>215</fpage>–<lpage>228</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lewin</surname><given-names>C.</given-names></name>
<name><surname>Wolgers</surname><given-names>G.</given-names></name>
<name><surname>Herlitz</surname><given-names>A.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Sex differences favoring women in verbal but not in visuospatial episodic memory</article-title>. <source>Neuropsychology</source>, <volume>15</volume>, <fpage>165</fpage>–<lpage>173</lpage>.</citation>
</ref>
<ref id="bibr12-0956797612468281">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Maccoby</surname><given-names>E. E.</given-names></name>
<name><surname>Jacklin</surname><given-names>C. N.</given-names></name>
</person-group> (<year>1974</year>). <source>The psychology of sex differences</source>. <publisher-loc>Stanford, CA</publisher-loc>: <publisher-name>Stanford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr13-0956797612468281">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>MacMillan</surname><given-names>N. A.</given-names></name>
<name><surname>Creelman</surname><given-names>C. D.</given-names></name>
</person-group> (<year>2005</year>). <source>Detection theory: A user’s guide</source> (<edition>2nd ed.</edition>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr14-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McGuire</surname><given-names>W. J.</given-names></name>
</person-group> (<year>1961</year>). <article-title>A multiprocess model for paired-associate learning</article-title>. <source>Journal of Experimental Psychology</source>, <volume>63</volume>, <fpage>81</fpage>–<lpage>99</lpage>.</citation>
</ref>
<ref id="bibr15-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mondloch</surname><given-names>C. J.</given-names></name>
<name><surname>Geldart</surname><given-names>S.</given-names></name>
<name><surname>Maurer</surname><given-names>D.</given-names></name>
<name><surname>Le Grand</surname><given-names>R.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Developmental changes in face processing skills</article-title>. <source>Journal of Experimental Child Psychology</source>, <volume>86</volume>, <fpage>67</fpage>–<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pillemer</surname><given-names>D.</given-names></name>
<name><surname>Wink</surname><given-names>P.</given-names></name>
<name><surname>DiDonato</surname><given-names>T.</given-names></name>
<name><surname>Sanborn</surname><given-names>R.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Gender differences in autobiographical memory styles of older adults</article-title>. <source>Memory</source>, <volume>11</volume>, <fpage>525</fpage>–<lpage>532</lpage>.</citation>
</ref>
<ref id="bibr17-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shapiro</surname><given-names>P. N.</given-names></name>
<name><surname>Penrod</surname><given-names>S.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Meta-analysis of facial identification studies</article-title>. <source>Psychological Bulletin</source>, <volume>100</volume>, <fpage>139</fpage>–<lpage>156</lpage>.</citation>
</ref>
<ref id="bibr18-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Skowronski</surname><given-names>J. J.</given-names></name>
<name><surname>Betz</surname><given-names>A. L.</given-names></name>
<name><surname>Thompson</surname><given-names>C. P.</given-names></name>
<name><surname>Shannon</surname><given-names>L.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Social memory in everyday life: Recall of self-events and other-events</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>60</volume>, <fpage>831</fpage>–<lpage>843</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tulving</surname><given-names>E.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Ecphoric processes in episodic memory</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>, <volume>302</volume>, <fpage>361</fpage>–<lpage>370</lpage>.</citation>
</ref>
<ref id="bibr20-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tulving</surname><given-names>E.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Précis of elements of episodic memory</article-title>. <source>Behavioral &amp; Brain Sciences</source>, <volume>7</volume>, <fpage>223</fpage>–<lpage>268</lpage>.</citation>
</ref>
<ref id="bibr21-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tulving</surname><given-names>E.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Episodic memory: From mind to brain</article-title>. <source>Annual Review of Psychology</source>, <volume>53</volume>, <fpage>1</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr22-0956797612468281">
<citation citation-type="gov">
<collab>U.S. Census Bureau</collab>. (<year>2011</year>). <source>Frequently occurring surnames from the 1990 census</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.census.gov/genealogy/www/data/1990surnames/index.html">http://www.census.gov/genealogy/www/data/1990surnames/index.html</ext-link></citation></ref>
<ref id="bibr23-0956797612468281">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yonelinas</surname><given-names>A. P.</given-names></name>
</person-group> (<year>2002</year>). <article-title>The nature of recollection and familiarity: A review of 30 years of research</article-title>. <source>Journal of Memory and Language</source>, <volume>46</volume>, <fpage>441</fpage>–<lpage>517</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>