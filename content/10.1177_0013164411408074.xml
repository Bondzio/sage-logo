<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPM</journal-id>
<journal-id journal-id-type="hwp">spepm</journal-id>
<journal-title>Educational and Psychological Measurement</journal-title>
<issn pub-type="ppub">0013-1644</issn>
<issn pub-type="epub">1552-3888</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0013164411408074</article-id>
<article-id pub-id-type="publisher-id">10.1177_0013164411408074</article-id>
<title-group>
<article-title>Multigroup Generalizability Analysis of Verbal, Quantitative, and Nonverbal Ability Tests for Culturally and Linguistically Diverse Students</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lakin</surname><given-names>Joni M.</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411408074">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Lai</surname><given-names>Emily R.</given-names></name>
<xref ref-type="aff" rid="aff2-0013164411408074">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0013164411408074"><label>1</label>Auburn University, Auburn, AL, USA</aff>
<aff id="aff2-0013164411408074"><label>2</label>Pearson, Iowa City, IA, USA</aff>
<author-notes>
<corresp id="corresp1-0013164411408074">Joni Lakin, Department of Educational Foundations, Leadership, and Technology, 4036 Haley Center, Auburn University, Auburn, AL 36849-5221, USA Email: <email>jml0035@auburn.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2012</year>
</pub-date>
<volume>72</volume>
<issue>1</issue>
<fpage>139</fpage>
<lpage>158</lpage>
<permissions>
<copyright-statement>© 2012 SAGE Publications</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>For educators seeking to differentiate instruction, cognitive ability tests sampling multiple content domains, including verbal, quantitative, and nonverbal reasoning, provide superior information about student strengths and weaknesses compared with unidimensional reasoning measures. However, these ability tests have not been fully evaluated with respect to fairness and validity for English-language learners (ELL). In particular, reliability is an important aspect of validity that has not been sufficiently evaluated. In this study, multivariate generalizability methodologies were used to explore the differential reliability of the Cognitive Abilities Test across ELL and non-ELL students in two schools with large Hispanic populations. Results suggest that verbal and quantitative reasoning skills are measured less precisely for ELL students than for non-ELL students. However, the composite score of the three batteries showed strong reliability in both groups. We conclude that multidimensional tests provide reliable information about the academic strengths of ELL and non-ELL students, though further research is needed.</p>
</abstract>
<kwd-group>
<kwd>English-language learners</kwd>
<kwd>ability tests</kwd>
<kwd>reliability</kwd>
<kwd>generalizability</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Ability tests play an important role in helping teachers provide differentiated instruction to students (e.g., <xref ref-type="bibr" rid="bibr30-0013164411408074">Lohman &amp; Hagen, 2001b</xref>; <xref ref-type="bibr" rid="bibr38-0013164411408074">Reschly &amp; Hosp, 2004</xref>). However, they are often criticized for not providing fair and valid assessments of students who are English-language learners (ELL), because the strong language demands of the verbal components of many ability tests do not permit ELL students to fully engage in the task and demonstrate their abilities (<xref ref-type="bibr" rid="bibr13-0013164411408074">Ford, Grantham, &amp; Whiting, 2008</xref>). Similar charges have been made with respect to achievement tests, particularly mathematics tests where the language demands are seen as extraneous to the construct of interest (<xref ref-type="bibr" rid="bibr1-0013164411408074">Abedi &amp; Lord, 2001</xref>; <xref ref-type="bibr" rid="bibr24-0013164411408074">Li &amp; Brennan, 2007</xref>).</p>
<p>Validity research exploring the utility of such tests for ELL students is imperative given the rapidly increasing population of such students in the U.S. school system (<xref ref-type="bibr" rid="bibr2-0013164411408074">American Federation of Teachers, 2004</xref>; <xref ref-type="bibr" rid="bibr17-0013164411408074">Harris, Rapp, Martinez, &amp; Plucker, 2007</xref>). There is also continued interest in creating talent development programs that better serve culturally and linguistically diverse students (<xref ref-type="bibr" rid="bibr8-0013164411408074">Callahan, 2005</xref>; <xref ref-type="bibr" rid="bibr13-0013164411408074">Ford et al., 2008</xref>; <xref ref-type="bibr" rid="bibr17-0013164411408074">Harris et al., 2007</xref>). Developing such programs requires adopting assessments that provide valid and useful information about the abilities of these students. Previous research has explored aspects of predictive and construct validity of tests when used with culturally and linguistically diverse populations (<xref ref-type="bibr" rid="bibr19-0013164411408074">Keith, 1999</xref>; <xref ref-type="bibr" rid="bibr20-0013164411408074">Kvist &amp; Gustafsson, 2008</xref>; <xref ref-type="bibr" rid="bibr32-0013164411408074">Lohman, Korb, &amp; Lakin, 2008</xref>). The purpose of this study was to explore a less commonly considered aspect of fairness: differences in measurement precision between ELL and non-ELL students. In particular, we investigated the precision of a test measuring reasoning in verbal, quantitative, and nonverbal/figural content domains for a sample of Hispanic ELL students compared with a sample of primarily White and Hispanic non-ELL students.</p>
<sec id="section1-0013164411408074">
<title>The Benefits of Multidimensional Tests</title>
<p>Researchers of cognitive abilities have recognized that tests with varied content are better indicators of ability than unidimensional measures (<xref ref-type="bibr" rid="bibr12-0013164411408074">Floyd, Shands, Rafael, Bergeron, &amp; McGrew, 2009</xref>; <xref ref-type="bibr" rid="bibr41-0013164411408074">Süß &amp; Beauducel, 2005</xref>). In particular, researchers have demonstrated that multidimensional reasoning tests have greater construct representation because they sample more aspects of what it means to reason well (<xref ref-type="bibr" rid="bibr9-0013164411408074">Carroll, 1993</xref>). For example, although figure analogies are excellent measures of general reasoning skills, they require the examinee to manipulate figures and shapes. These figural reasoning skills do not necessarily translate into effective reasoning with words or numbers. Thus, a test consisting only of figural analogies taps into only one element of what it means to reason well. A test measuring multiple, broad content domains, such as verbal, quantitative, and nonverbal symbol systems, would better represent the construct of reasoning than a test of nonverbal reasoning alone.</p>
<p>In an educational context, multidimensional tests are also more useful and valid because they better align with the criterion of interest—academic success. Research has confirmed that ability tests sampling a range of school-relevant reasoning abilities (particularly in language and math) are important in predicting future academic achievement, because those tests are more reflective of the criterion of interest—future achievement across a variety of academic domains (<xref ref-type="bibr" rid="bibr14-0013164411408074">Gustafsson &amp; Balke, 1993</xref>; <xref ref-type="bibr" rid="bibr44-0013164411408074">Wilhelm, 2005</xref>). Importantly, other research has indicated that the types of specific reasoning abilities that are important to academic success are the same for the largest U.S. ethnic groups (<xref ref-type="bibr" rid="bibr19-0013164411408074">Keith, 1999</xref>).</p>
<p>From a teacher’s viewpoint, multidimensional ability tests might also provide more helpful information for how the teacher might differentiate instruction, because the multiple scores reported by these tests provide a richer description of the cognitive strengths and weaknesses that can form aptitudes for learning in the classroom (<xref ref-type="bibr" rid="bibr39-0013164411408074">Snow, 1992</xref>). Scores on multidimensional tests such as the Cognitive Abilities Test (CogAT; <xref ref-type="bibr" rid="bibr29-0013164411408074">Lohman &amp; Hagen, 2001a</xref>) provide at least two pieces of useful information. One is the level of ability the student demonstrates across batteries. Level of ability (or elevation) provides teachers with a useful guide for providing instruction at a pace and level of complexity appropriate to a student’s skill (<xref ref-type="bibr" rid="bibr10-0013164411408074">Cronbach &amp; Gleser, 1953</xref>). The second useful piece of information is the shape of the profile, which reveals the relative strengths and weaknesses for each student, which the teacher can then use to adapt instruction (<xref ref-type="bibr" rid="bibr30-0013164411408074">Lohman &amp; Hagen, 2001b</xref>). The profiles provided by the CogAT have been found to provide reliable and useful information about student abilities (<xref ref-type="bibr" rid="bibr28-0013164411408074">Lohman, Gambrell, &amp; Lakin, 2008</xref>).</p>
</sec>
<sec id="section2-0013164411408074">
<title>Current Practice in Assessing ELL Students</title>
<p>English language learners are students who enter U.S. schools with limited English proficiency and who are acquiring English while also continuing to learn academic content, usually instructed in English. Because ELL students acquire English at different paces and because new ELL students enter schools each year, this group of students is highly variable in terms of their current proficiency and academic knowledge. Because of these students’ variable English proficiency, current practice in assessing the cognitive abilities of ELL students often advocates the use of nonverbal assessments (<xref ref-type="bibr" rid="bibr23-0013164411408074">Lewis, 2001</xref>; <xref ref-type="bibr" rid="bibr36-0013164411408074">Pierce et al., 2007</xref>; <xref ref-type="bibr" rid="bibr37-0013164411408074">Powers &amp; Barkan, 1986</xref>). Nonverbal tests are preferred because they reduce the language load of the test, which is expected to increase fairness for ELL students. However, the ELL student population varies greatly in their English proficiency, with some nearing full proficiency. And development of English proficiency is itself an important academic outcome for ELL students. Therefore, measuring a broader range of cognitive abilities may be desirable. The nonverbal tests most frequently used yield single indicators of general ability that lend themselves only to general instructional modifications (<xref ref-type="bibr" rid="bibr25-0013164411408074">Lohman, 2005</xref>). Such tests provide limited information on how teachers might adapt instruction to strengths and weaknesses, particularly with respect to verbal domains.</p>
<p>Some existing nonverbal tests do acknowledge the important role that multiple-item formats play in sampling a wider range of cognitive processes, including the Comprehensive Test of Nonverbal Intelligence (cTONI; <xref ref-type="bibr" rid="bibr15-0013164411408074">Hammill, Pearson, &amp; Widerholdt, 1996</xref>) and Universal Nonverbal Intelligence Test (UNIT; <xref ref-type="bibr" rid="bibr4-0013164411408074">Bracken &amp; McCallum, 1998</xref>). Multiple-format nonverbal tests arguably increase the construct coverage over single-format nonverbal tests and cancel out irrelevant task-specific sources of variance (<xref ref-type="bibr" rid="bibr33-0013164411408074">McCallum, Bracken, &amp; Wasserman, 2001</xref>). However, as long as the tests do not tap into the ability to reason with language and mathematical symbols, they will always underrepresent the construct of interest to educators (<xref ref-type="bibr" rid="bibr35-0013164411408074">Ortiz &amp; Ochoa, 2005</xref>) and fail to provide useful information for differentiating instruction (<xref ref-type="bibr" rid="bibr25-0013164411408074">Lohman, 2005</xref>, <xref ref-type="bibr" rid="bibr27-0013164411408074">2009</xref>).</p>
<p>The limitations of nonverbal tests are revealed in predictive validity studies. Strong relationships between ability and achievement test scores are an important component to an ability test’s validity argument, as having strong cognitive reasoning skills should correlate with academic accomplishment in the long term (<xref ref-type="bibr" rid="bibr42-0013164411408074">Thorndike, 1982</xref>). However, whereas verbal and quantitative measures have strong predictive validity for academic outcomes, nonverbal tests have sometimes yielded disappointingly low predictive validities. For example, <xref ref-type="bibr" rid="bibr3-0013164411408074">Borghese (2009)</xref> found that the UNIT full-scale scores correlated .51 with math achievement, but only .28 with reading as measured by the Stanford Achievement Test (SAT-10),<sup><xref ref-type="fn" rid="fn1-0013164411408074">1</xref></sup> which is much lower than what is typically observed for verbal tests (about .80; <xref ref-type="bibr" rid="bibr22-0013164411408074">Lakin &amp; Lohman, 2011</xref>). Likewise, <xref ref-type="bibr" rid="bibr18-0013164411408074">Jones (2006)</xref> found correlations below .10 between UNIT scores in first grade and reading achievement on the Texas Assessment of Knowledge and Skills in third grade for both ELL and non-ELL students.</p>
<p>Even when the nonverbal tests perform well, they are rarely more effective at predicting academic success than multidimensional tests with language and math components, even among ELL students. Although <xref ref-type="bibr" rid="bibr21-0013164411408074">Lakin and Lohman (2009)</xref> found moderate relationships between the Nonverbal Battery of the CogAT and the Arizona Instrument to Measure Standards achievement test in reading and math (<italic>r</italic> = .41 and .57, respectively), the relationships for the Verbal and Quantitative Batteries were significantly stronger: <italic>r</italic> = .60 between Verbal and Reading scores and .68 between Quantitative and Mathematics scores in their sample of Hispanic ELL students. Such research suggests that the field of psychological assessment underestimates the viability and value of assessing cognitive abilities in ELL students beyond figural/nonverbal reasoning.</p>
<p><xref ref-type="bibr" rid="bibr21-0013164411408074">Lakin and Lohman (2009)</xref> concluded that verbal and quantitative reasoning tests could provide valuable information about the abilities of ELL students, and they also cautioned that the tests showed some undesirable psychometric properties, including substantial range restriction in the battery scores of ELL students. This range restriction accounted for a large part of the lower predictive validity for ELL students compared with non-ELL students, but the reason for the narrowed range of scores was unclear. One potential explanation for the range restriction is that the items are more difficult for ELL students due to their language load. A mismatch of item difficulty may also affect the precision with which the construct is measured for ELL students (<xref ref-type="bibr" rid="bibr42-0013164411408074">Thorndike, 1982</xref>).</p>
<p>Other researchers concerned with the assessment of ELL students have called for greater attention to measurement precision as a contributor of test bias. <xref ref-type="bibr" rid="bibr45-0013164411408074">Young (2009)</xref> called attention to the dearth of validity research focusing on the assessment of ELL students, especially evaluations of test reliability. <xref ref-type="bibr" rid="bibr40-0013164411408074">Solano-Flores and Li (2006)</xref> called for more research incorporating generalizability theory analyses to explore the role of language on achievement tests. Differences in reliability are an often overlooked, but potentially important, contributor of bias because unreliability can depress other aspects of validity, including predictive validity.</p>
<p>To investigate the reliability and sources of error for multidimensional test scores for ELL students, this study addressed the following questions:</p>
<list id="list1-0013164411408074" list-type="order">
<list-item><p>Are measures of verbal, quantitative, and nonverbal ability equally reliable for ELL and non-ELL students?</p></list-item>
<list-item><p>What does a generalizability-like coefficient indicate about the probable reliability of examinee profiles? Is the probable reliability of a randomly selected ELL student profile different from that of a randomly selected non-ELL student profile?</p></list-item>
<list-item><p>What is the impact of manipulating domain sampling attributes on measurement precision? That is, if the table of specifications were altered so all 152 items came from <italic>just one content domain</italic> (verbal, quantitative, or nonverbal), what would be the effect on measurement error?</p></list-item>
</list>
</sec>
<sec id="section3-0013164411408074" sec-type="methods">
<title>Method</title>
<p>CogAT Form 6 (<xref ref-type="bibr" rid="bibr29-0013164411408074">Lohman &amp; Hagen, 2001a</xref>) was administered to a sample of 145 ELL and 236 non-ELL students in third and fourth grade. Students receiving any kind of ELL services were classified as ELL by the school. In this district, ELL services are determined on the basis of number of years in the school and performance on the Stanford English Language Proficiency test (<xref ref-type="bibr" rid="bibr16-0013164411408074">Harcourt Educational Assessment, 2003</xref>). The data for this study were collected as part of Project Bright Horizons, a study developed by a team of researchers and school administrators (described in more detail in <xref ref-type="bibr" rid="bibr32-0013164411408074">Lohman, Korb, et al., 2008</xref>). Two schools in a Southwestern school district in the United States participated in the study. The district has a large population of Hispanic students: 50% of the non-ELL students and 95% of the ELL students were Hispanic.</p>
<p>The district also has a large proportion of students receiving free or reduced-price lunch: 95% of the Hispanic students, 91% of students from other minority groups, and 53% of the White students were eligible. Additional demographic information is provided in <xref ref-type="table" rid="table1-0013164411408074">Table 1</xref>.</p>
<table-wrap id="table1-0013164411408074" position="float">
<label>Table 1.</label>
<caption>
<p>Breakdown of Sample by Demographic Category</p>
</caption>
<graphic alternate-form-of="table1-0013164411408074" xlink:href="10.1177_0013164411408074-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Grade</th>
<th align="center" colspan="2">Sex</th>
<th align="center" colspan="3">Ethnicity</th>
<th align="center" colspan="2">Free or reduced-price lunch eligibility</th>
</tr>
<tr>
<th/>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">Male</th>
<th align="center">Female</th>
<th align="center">White</th>
<th align="center">Hispanic</th>
<th align="center">Other</th>
<th align="center">Yes</th>
<th align="center">No</th>
</tr>
</thead>
<tbody>
<tr>
<td>Non-ELL</td>
<td>97</td>
<td>139</td>
<td>122</td>
<td>114</td>
<td>67</td>
<td>125</td>
<td>44</td>
<td>195</td>
<td>41</td>
</tr>
<tr>
<td>ELL</td>
<td>80</td>
<td>65</td>
<td>73</td>
<td>72</td>
<td>1</td>
<td>138</td>
<td>6</td>
<td>142</td>
<td>3</td>
</tr>
<tr>
<td>Total</td>
<td>177</td>
<td>204</td>
<td>195</td>
<td>186</td>
<td>68</td>
<td>263</td>
<td>50</td>
<td>337</td>
<td>44</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013164411408074">
<p><italic>Note</italic>. ELL = English-language learners.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<sec id="section4-0013164411408074">
<title>Measures Used</title>
<p>The CogAT is a measure of cognitive abilities comprising Verbal, Quantitative, and Nonverbal Batteries.<sup><xref ref-type="fn" rid="fn2-0013164411408074">2</xref></sup> Each battery consists of three unique item formats. The Verbal Battery uses verbal analogies, verbal classification, and sentence completion item formats. The Quantitative Battery consists of number series, equation building, and quantitative relations tasks. Finally, the Nonverbal Battery consists of figure analogies, figure classification, and paper folding tasks. The CogAT shows strong convergent and discriminant validity with other measures of abilities (<xref ref-type="bibr" rid="bibr31-0013164411408074">Lohman &amp; Hagen, 2002</xref>).</p>
<p>CogAT was designed to measure general fluid reasoning (Gf) by tapping into reasoning abilities in verbal, quantitative, and nonverbal/figural symbol systems (<xref ref-type="bibr" rid="bibr31-0013164411408074">Lohman &amp; Hagen, 2002</xref>). These symbol systems correspond to the three CogAT batteries, which were chosen because they are important psychologically and instructionally (<xref ref-type="bibr" rid="bibr9-0013164411408074">Carroll, 1993</xref>). Users are encouraged to rely on individual battery scores (verbal, quantitative, or nonverbal) rather than the composite score to make criterion-referenced instructional decisions, such as gifted and talented identification, as each battery provides unique information about a student’s aptitude for success in different academic domains. For differentiating instruction, users are encouraged to use the detailed profiles of battery scores provided, which specify level, shape, and scatter of scores—as these profiles align with specific instructional recommendations (<xref ref-type="bibr" rid="bibr30-0013164411408074">Lohman &amp; Hagen, 2001b</xref>).</p>
<p>All tests on the CogAT begin with directions that are read aloud by the teacher and also presented as text to the students. In this study, directions were read in Spanish when teachers found it to be more appropriate for their students. However, all three sections of the Verbal Battery and one section of the Quantitative Battery require some reading in English. On the Verbal Battery, students must read either individual words (verbal classification and verbal analogies) or short sentences (sentence completion). On the quantitative relations section of the Quantitative Battery, students read individual words (e.g., <italic>foot</italic> or <italic>gallon</italic>). The other Quantitative Battery sections and all the Nonverbal Battery tests do not require reading.</p>
<p>CogAT Form 6 tests have substantial overlap (around 80%) across grade levels. For each section of a battery, this means that only 3 to 5 items are unique to each level of the test. The overlap is systematic: at each level, the easiest 3 to 5 items are dropped from the beginning of each section and new, more difficult items are added at the end. Therefore, the third- and fourth-grade students in this study took 152 common test items across the three batteries. To simplify the model in this study, only these overlapping items were used in the analyses. At the battery level, the common items included 52 verbal items, 48 quantitative items, and 52 nonverbal items. The data for the other 38 nonoverlapping items at each level were discarded.<sup><xref ref-type="fn" rid="fn3-0013164411408074">3</xref></sup></p>
</sec>
<sec id="section5-0013164411408074">
<title>Data Analysis</title>
<p>This study used the framework of generalizability theory (G-theory), which essentially marries classical test theory with factorial analysis of variance (ANOVA) methods to decompose observed scores into their constituent parts: universe-score variance (which is akin to true-score variance), variance due to the facets of the design (which are similar conditions of the measurement procedure in which the investigator is interested), and error. Generalizability studies (G-studies) specify a universe of admissible observations, which identifies the conditions of measurement in which the researcher is interested. For example, if students were tested on a sample of items, which were then scored by raters, the researcher might be interested in knowing how much variability in student performance was attributable to the specific items represented on the test (the items facet), to the raters scoring the tasks (raters facet), and to the interaction of these two facets. G-studies use factorial ANOVA approaches to estimate variance components corresponding to the facets of the design, facilitating comparisons of the relative importance of various sources of error. Decision studies (studies), on the other hand, define a universe of generalization, which characterizes what constitutes a replication of the measurement procedure. D-studies permit estimation of variance components necessary for supporting decisions about a given measurement procedure, such as the minimum number of prompts, raters, or other facets needed to achieve certain levels of reliability. Univariate G-theory has been extended to accommodate multivariate designs, in which each person is assumed to have multiple universe scores—one score corresponding to each of several, correlated domains (<xref ref-type="bibr" rid="bibr6-0013164411408074">Brennan, 2001b</xref>).</p>
<p>In this study, we performed multivariate G- and D-studies. For each of these analyses, we used the fully crossed <italic>p</italic><sup>•</sup> × <italic>i</italic><sup>◦</sup> “table of specifications” design, in which each battery (verbal, quantitative, and nonverbal) featured a unique, nonoverlapping set of items.<sup><xref ref-type="fn" rid="fn4-0013164411408074">4</xref></sup> In this design, we treated both persons and items as random in the universe of generalization. As in the achievement domain, items on ability tests represent merely a sample from the universe of possible items. Thus, in making inferences about examinee ability, it is necessary to generalize beyond the specific items represented on a given instantiation of the test to encompass all the possible items that could have been represented. In the <italic>p</italic><sup>•</sup> × <italic>i</italic><sup>◦</sup> design, all persons respond to all items, and items are nested within three levels of the fixed facet (i.e., the three batteries of the test). The closed circle corresponding to the persons facet represents the fact that persons are crossed with all levels of the fixed facet. The open circle corresponding to the items facet represents the fact that items are nested within levels. We treat the three batteries of the CogAT as fixed because they do not change across subsequent revisions, even though the items themselves are replaced every 8 to 10 years. Note that this design entails estimation of covariance components corresponding to the linked facets (i.e., those with closed circles). Thus, in this case, we estimate only the covariance between universe scores on the various levels of the fixed facet (i.e., scores on the separate batteries), since different items appear on each battery.</p>
<p>We performed multivariate G- and D-studies on the combined group of students as well as separately for each language group (ELL and non-ELL). However, results from the global analysis of the total group were almost identical to results for the non-ELL group of students, so we report only separate group-level results in the current article. The purpose for conducting separate analyses—one each with the separate language groups—was to contrast results obtained from the different modeling approaches. <xref ref-type="bibr" rid="bibr6-0013164411408074">Brennan (2001b)</xref> recommends using this general strategy when the objects of measurement are stratified according to some characteristic.<sup><xref ref-type="fn" rid="fn5-0013164411408074">5</xref></sup> Furthermore, <xref ref-type="bibr" rid="bibr24-0013164411408074">Li and Brennan (2007)</xref> point out that reliability estimates computed on the total group of students may misrepresent the reliability of a measure for a specific subgroup of students, because the different facets of the measurement procedure can contribute in different ways to the variability of student performance. Thus, performing separate analyses for each group permits a comparison of the relative contribution of each facet across groups.</p>
<p>We performed D-studies to estimate how many more verbal, quantitative, and nonverbal items would be needed to obtain estimates of ELL ability that were comparable in reliability with those from non-ELL examinees and to explore the effect of manipulating domain sampling attributes on error. In addition, because one of the recommended uses of the CogAT is to support instructional differentiation based on a student’s profile, we explored the error associated with examinee profiles and pairwise difference scores. We conducted all analyses in <italic>mGENOVA</italic> using raw scores as input (<xref ref-type="bibr" rid="bibr5-0013164411408074">Brennan, 2001a</xref>).</p>
</sec>
</sec>
<sec id="section6-0013164411408074" sec-type="results">
<title>Results</title>
<p><xref ref-type="table" rid="table2-0013164411408074">Table 2</xref> reports basic distribution information in the raw-score metric, including effect sizes in Cohen’s <italic>d</italic> metric and variance ratios (the ratio of variance for non-ELL students to variance for ELL students) for all three batteries. As can be seen by examining the values for Cohen’s <italic>d</italic>, non-ELL examinees outperformed the ELL examinees on all three batteries, although as expected, the Verbal Battery exhibited the most dramatic difference. Variance ratios indicated significantly narrower score distributions for the ELL students on the Quantitative and Verbal Batteries. A value of 1.97 for the Quantitative Battery indicates that non-ELL students were 97% more variable than ELL students (<xref ref-type="bibr" rid="bibr11-0013164411408074">Feingold, 1992</xref>).</p>
<table-wrap id="table2-0013164411408074" position="float">
<label>Table 2.</label>
<caption>
<p>Descriptive Statistics (Raw Scores)</p>
</caption>
<graphic alternate-form-of="table2-0013164411408074" xlink:href="10.1177_0013164411408074-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="6">Battery</th>
</tr>
<tr>
<th/>
<th align="center" colspan="2">Verbal</th>
<th align="center" colspan="2">Quantitative</th>
<th align="center" colspan="2">Nonverbal</th>
</tr>
<tr>
<th align="left">Descriptive statistic</th>
<th align="center">ELL</th>
<th align="center">Non-ELL</th>
<th align="center">ELL</th>
<th align="center">Non-ELL</th>
<th align="center">ELL</th>
<th align="center">Non-ELL</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean</td>
<td>17.32</td>
<td>30.37</td>
<td>19.94</td>
<td>28.54</td>
<td>21.90</td>
<td>31.24</td>
</tr>
<tr>
<td><italic>SD</italic></td>
<td>7.67</td>
<td>14.37</td>
<td>9.09</td>
<td>12.75</td>
<td>13.18</td>
<td>14.16</td>
</tr>
<tr>
<td>Number of items</td>
<td align="center" colspan="2">52</td>
<td align="center" colspan="2">48</td>
<td align="center" colspan="2">52</td>
</tr>
<tr>
<td>Cohen’s <italic>d</italic></td>
<td align="center" colspan="2">1.13</td>
<td align="center" colspan="2">0.78</td>
<td align="center" colspan="2">0.68</td>
</tr>
<tr>
<td>Variance ratio</td>
<td align="center" colspan="2">3.51</td>
<td align="center" colspan="2">1.97</td>
<td align="center" colspan="2">1.15</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013164411408074">
<p><italic>Note.</italic> ELL = English-language learners. Cohen’s <italic>d</italic> greater than 0 indicates higher mean scores for non-ELL students. Variance ratios of greater than 1 indicate that non-ELL students had greater variability.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p><xref ref-type="table" rid="table3-0013164411408074">Table 3</xref> reports estimated G-study variance and covariance components for the <italic>p</italic><sup>•</sup> × <italic>i</italic><sup>◦</sup> design for ELL and non-ELL students separately. It should be noted that these results represent item scores for single persons, in contrast to the results presented in <xref ref-type="table" rid="table2-0013164411408074">Table 2</xref>, which were expressed as total scores. Examining first the results for the non-ELL group, the magnitude of estimated disattenuated correlations<sup><xref ref-type="fn" rid="fn6-0013164411408074">6</xref></sup> among universe scores from the various batteries (reported in italics in the upper diagonals in the first block) suggests the test is measuring related, though distinct, constructs. The single largest variance component across all three batteries is clearly the persons by items interaction, which alone accounts for around 65% of the total variance in scores. Conceptually, this result suggests that most of the variance in scores is due to the fact that students perform differentially well on different items. The second largest variance component is the person effect (around 30%), which indicates that individual differences in the underlying construct contribute strongly to test performance. The distribution of variance components across the three batteries is remarkably similar, which suggests that the relative contribution of each facet to performance on batteries measuring verbal, quantitative, and nonverbal reasoning skills is essentially the same.</p>
<table-wrap id="table3-0013164411408074" position="float">
<label>Table 3.</label>
<caption>
<p>G-Study Estimated Variance/Covariance Components</p>
</caption>
<graphic alternate-form-of="table3-0013164411408074" xlink:href="10.1177_0013164411408074-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">ELL</th>
<th align="center" colspan="3">Non-ELL</th>
</tr>
<tr>
<th align="left">Source</th>
<th align="center">Verbal</th>
<th align="center">Quantitative</th>
<th align="center">Nonverbal</th>
<th align="center">Verbal</th>
<th align="center">Quantitative</th>
<th align="center">Nonverbal</th>
</tr>
</thead>
<tbody>
<tr>
<td><italic>p</italic></td>
<td>0.01820 (8%)</td>
<td><italic>0.43956</italic></td>
<td><italic>0.45419</italic></td>
<td>0.07393 (30%)</td>
<td><italic>0.68106</italic></td>
<td><italic>0.64848</italic></td>
</tr>
<tr>
<td/>
<td>0.01047</td>
<td>0.03117 (13%)</td>
<td><italic>0.65916</italic></td>
<td>0.04836</td>
<td>0.06819 (28%)</td>
<td><italic>0.65675</italic></td>
</tr>
<tr>
<td/>
<td>0.01495</td>
<td>0.0284</td>
<td>0.05956 (24%)</td>
<td>0.04715</td>
<td>0.04586</td>
<td>0.07150 (30%)</td>
</tr>
<tr>
<td><italic>i</italic></td>
<td>0.02106 (9%)</td>
<td/>
<td/>
<td>0.01473 (6%)</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>0.01312 (5%)</td>
<td/>
<td/>
<td>0.01389 (6%)</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>0.00837 (3%)</td>
<td/>
<td/>
<td>0.01278 (5%)</td>
</tr>
<tr>
<td><italic>pi</italic></td>
<td>0.18293 (82%)</td>
<td/>
<td/>
<td>0.15521 (64%)</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>0.19829 (82%)</td>
<td/>
<td/>
<td>0.16156 (66%)</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>0.17544 (72%)</td>
<td/>
<td/>
<td>0.15705 (65%)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0013164411408074">
<p><italic>Note</italic>. ELL = English-language learners. Estimated disattenuated correlations are reported in italics in the upper diagonals. Estimated covariances are reported in the lower diagonals. Numbers in parentheses represent the percentage of the total variance attributable to that variance component.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>In contrast, results for ELL students differ in several ways from the non-ELL results. First, estimated disattenuated correlations indicate that the relationships between performance on the Verbal Battery and performance on the other two batteries were noticeably weaker for ELL students than they were for non-ELL students. Whereas the correlations for non-ELL students were around .65 among all the batteries, for ELL students, the correlations with the Verbal Battery were around .45 for quantitative and nonverbal. On the other hand, the correlation between quantitative and nonverbal reasoning was equally strong for ELL and non-ELL students. Together, these results suggest there is less overlap among estimates of verbal, nonverbal, and quantitative abilities for ELL students than for non-ELL students.</p>
<p>Second, the relative magnitudes of the variance components (reported in parentheses) are quite different across the three batteries for ELL students. In particular, for ELL students, the Nonverbal Battery showed much more variance attributable to persons and less variance attributable to the person–item interaction compared with the other two batteries. In contrast, for non-ELL students, the distribution of variance components across the three batteries was relatively similar. Finally, the absolute magnitudes of the variance components differed for ELL students versus non-ELL students. In particular, the <italic>pi</italic> component, representing the interaction between persons and items as well as all unmeasured sources of variance, was uniformly larger for ELL students than for non-ELL students. This latter result suggests that language status does matter with respect to measurement precision for Verbal and Quantitative Batteries.</p>
<p><xref ref-type="table" rid="table4-0013164411408074">Table 4</xref> reports the estimated D-study universe score and error matrices for the <italic>p</italic><sup>•</sup> × <italic>i</italic><sup>◦</sup> design for the same analyses. Turning first to the error matrices, both relative and absolute errors were larger for ELL students than for non-ELL students. Accordingly, when we analyzed ELL students separately, we obtained somewhat lower reliability estimates, particularly for the Verbal and Quantitative Batteries. When we combined the three individual battery scores to form a composite, reliability estimates were high (in the mid .90 range) even for ELL students.</p>
<table-wrap id="table4-0013164411408074" position="float">
<label>Table 4.</label>
<caption>
<p>D-Study Estimated Universe-Score and Error Variance Matrices</p>
</caption>
<graphic alternate-form-of="table4-0013164411408074" xlink:href="10.1177_0013164411408074-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="3">ELL</th>
<th align="center" colspan="3">Non-ELL</th>
</tr>
<tr>
<th align="left">Source</th>
<th align="center">Verbal</th>
<th align="center">Quantitative</th>
<th align="center">Nonverbal</th>
<th align="center">Verbal</th>
<th align="center">Quantitative</th>
<th align="center">Nonverbal</th>
</tr>
</thead>
<tbody>
<tr>
<td>∑<sub>T</sub></td>
<td>0.01820</td>
<td><italic>0.43956</italic></td>
<td><italic>0.45419</italic></td>
<td>0.07393</td>
<td><italic>0.68106</italic></td>
<td><italic>0.64848</italic></td>
</tr>
<tr>
<td/>
<td>0.01047</td>
<td>0.03117</td>
<td><italic>0.65916</italic></td>
<td>0.04836</td>
<td>0.06819</td>
<td><italic>0.65675</italic></td>
</tr>
<tr>
<td/>
<td>0.01495</td>
<td>0.02840</td>
<td>0.05956</td>
<td>0.04715</td>
<td>0.04586</td>
<td>0.07150</td>
</tr>
<tr>
<td>∑<sub>δ</sub></td>
<td>0.00352</td>
<td/>
<td/>
<td>0.00298</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>0.00413</td>
<td/>
<td/>
<td>0.00337</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>0.00337</td>
<td/>
<td/>
<td>0.00302</td>
</tr>
<tr>
<td>∑<sub>Δ</sub></td>
<td>0.00392</td>
<td/>
<td/>
<td>0.00327</td>
<td/>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>0.00440</td>
<td/>
<td/>
<td>0.00366</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td/>
<td>0.00353</td>
<td/>
<td/>
<td>0.00327</td>
</tr>
<tr>
<td>Φ</td>
<td>0.82264</td>
<td>0.87620</td>
<td>0.94397</td>
<td>0.95767</td>
<td>0.94912</td>
<td>0.95631</td>
</tr>
<tr>
<td><italic>P</italic><sup>2</sup></td>
<td>0.83798</td>
<td>0.88298</td>
<td>0.94639</td>
<td>0.96119</td>
<td>0.95296</td>
<td>0.95947</td>
</tr>
<tr>
<td colspan="7">Composite-score results</td>
</tr>
<tr>
<td> Nominal weights</td>
<td>0.34211</td>
<td>0.31579</td>
<td>0.34211</td>
<td>0.34211</td>
<td>0.31579</td>
<td>0.34211</td>
</tr>
<tr>
<td> Effective weights</td>
<td>0.20780</td>
<td>0.30310</td>
<td>0.48900</td>
<td>0.35130</td>
<td>0.30750</td>
<td>0.34120</td>
</tr>
<tr>
<td> Φ</td>
<td align="center" colspan="3">0.94838</td>
<td align="center" colspan="3">0.97996</td>
</tr>
<tr>
<td> <italic>P</italic><sup>2</sup></td>
<td align="center" colspan="3">0.95188</td>
<td align="center" colspan="3">0.98154</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn4-0013164411408074">
<p><italic>Note</italic>. ELL = English-language learners. Estimated disattenuated correlations are reported in italics in the upper diagonals. Estimated covariances are reported in the lower diagonals. Φ represents the index of dependability. <italic>P</italic><sup>2</sup> represents the generalizability coefficient.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>One can also compare nominal and effective weights across groups. Whereas nominal weights are proportional to the number of items in each battery, effective weights are proportional to the contribution each battery makes to composite universe-score variance. Thus, an observed disparity between nominal and effective weights suggests whether certain levels of the fixed facet are contributing more to composite universe-score variance than would be expected given test length. Comparing the relative size of the nominal and effective weights within each of the two groups, one can see that they are quite different. Unlike non-ELL students, for whom nominal and effective weights were comparable across the three batteries, effective weights within the ELL group suggest that almost half of the universe score variance for ELL students is attributable to their performance on the Nonverbal Battery alone.</p>
<p><xref ref-type="fig" rid="fig1-0013164411408074">Figure 1</xref> displays the results of additional D-studies, which investigated the number of verbal and quantitative items necessary to obtain reliability estimates for ELL students that would equal those for non-ELL students. In particular, <xref ref-type="fig" rid="fig1-0013164411408074">Figure 1</xref> illustrates how many more verbal and quantitative items ELL students would have to respond to in order to obtain reliability estimates of .96 and .95, respectively, for verbal and quantitative scores. These analyses suggest that ELL students would need to respond to more than twice as many quantitative and more than three times as many verbal items (of similar quality) in order to obtain comparable estimates of reliability. Although increasing the length of these batteries is not a practical solution to the problem of lower reliability of verbal and quantitative scores for ELL students, these results communicate the magnitude of the disparities in estimated reliability coefficients for ELL and non-ELL students.</p>
<fig id="fig1-0013164411408074" position="float">
<label>Figure 1.</label>
<caption>
<p>Number of items and generalizability for English-language learner students</p>
</caption>
<graphic xlink:href="10.1177_0013164411408074-fig1.tif"/>
</fig>
<p>A reasonable question to ask is whether taking all 152 items from the battery that is the most reliable for ELL students (the Nonverbal Battery) would improve measurement precision relative to the composite score formed from all three batteries. This is not a trivial question, given the suggestion in the literature that single- and multiple-format nonverbal measures of reasoning abilities are the best representations of ELL student ability (<xref ref-type="bibr" rid="bibr36-0013164411408074">Pierce et al., 2007</xref>). Holding the overall test length constant (keeping the same number of total items on the test), but sampling exclusively from the nonverbal reasoning domain, provides a rigorous test of this hypothesis. Results from a D-study in which the nominal weights implied by the universe of generalization were held constant but the estimation weights were manipulated so that all 152 items were drawn from the nonverbal test alone provided interesting results. Specifically, administering a unidimensional test of 152 nonverbal items to ELL students would result in an estimate of mean squared relative error (which provides an index of the amount of error involved in using the observed mean on the Nonverbal Battery alone as an estimate of composite universe score variance) that is nearly 15 times larger than relative error variance of the multidimensional composite. Thus, to the extent that verbal, nonverbal, and quantitative ability are all part of the construct of reasoning, using nonverbal measures alone diminishes both reliability and validity. That is, using only nonverbal measures both underrepresents the construct and reduces the precision of estimated ability. Accordingly, administering all three batteries yields more precise estimates of examinee ability than administering a long version of even the battery that is <italic>most reliable</italic> for ELL students.</p>
<p>Because one of the recommended uses of the CogAT is instructional adaptation based on examinee ability profiles across the three batteries, we investigated the amount of error associated with profiles. <xref ref-type="table" rid="table5-0013164411408074">Table 5</xref> reports estimated profile variances, including observed-score profile variability <italic>V<sub>x</sub></italic>, universe-score profile variability, <italic>V</italic><sub>µ</sub>, and both relative and absolute error variability (<italic>V</italic><sub>δ</sub> and <italic>V</italic><sub>Δ</sub>, respectively). These variance components are similar to those estimated for individual battery scores in the context of a D-study, except that they represent a decomposition of the observed variability of <italic>profiles</italic> formed by scores on the three batteries.<sup><xref ref-type="fn" rid="fn7-0013164411408074">7</xref></sup> The bottom row of <xref ref-type="table" rid="table5-0013164411408074">Table 5</xref> reports estimates of <italic>G</italic>, which is the proportion of variance in the profile of observed scores that is explained by the variance in the profile of universe scores. In other words, <italic>G</italic> represents a type of generalizability coefficient for the profile of battery scores for a randomly selected person from each group (<xref ref-type="bibr" rid="bibr6-0013164411408074">Brennan, 2001b</xref>). This index provides some indication of the probable reliability of an individual examinee profile. For example, for the typical native English speaker, 88% of the variance in observed mean scores on the three batteries is attributable to variance in universe scores. Similarly, for the typical ELL, around 85% of the variance in observed mean scores is attributable to variance in universe scores. Thus, whereas results from individual subtest scores were differentially reliable for ELL versus non-ELL students, the score profiles of a randomly selected student from each group would likely be similarly reliable.</p>
<table-wrap id="table5-0013164411408074" position="float">
<label>Table 5.</label>
<caption>
<p>Expected Within-Object-of-Measurement Variability</p>
</caption>
<graphic alternate-form-of="table5-0013164411408074" xlink:href="10.1177_0013164411408074-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="center">Source</th>
<th align="center">ELL</th>
<th align="center">Non-ELL</th>
</tr>
</thead>
<tbody>
<tr>
<td><italic>V<sub>x</sub></italic></td>
<td>0.01607</td>
<td>0.01810</td>
</tr>
<tr>
<td><italic>V</italic><sub>µ</sub></td>
<td>0.01364</td>
<td>0.01603</td>
</tr>
<tr>
<td><italic>V</italic><sub>δ</sub></td>
<td>0.00245</td>
<td>0.00208</td>
</tr>
<tr>
<td><italic>V</italic><sub>Δ</sub></td>
<td>0.00264</td>
<td>0.00226</td>
</tr>
<tr>
<td><italic>G</italic></td>
<td>0.84860</td>
<td>0.88545</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn5-0013164411408074">
<p><italic>Note</italic>. ELL = English-language learners.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>A classroom teacher using individual student performance profiles to support particular instructional decisions at the student level would need some idea of the amount of error associated with various difference scores. Thus, we conducted an additional series of D-studies investigating the reliability of pairwise differences separately for ELL and non-ELL students. <xref ref-type="table" rid="table6-0013164411408074">Table 6</xref> reports the generalizability coefficients associated with each difference score. The coefficients are relatively similar and uniformly high across the three battery contrasts for non-ELL students. However, the verbal/quantitative difference score appears to be somewhat less reliable for ELL students. Thus, the inference of a relative strength or weakness between verbal and quantitative reasoning will be less stable for ELL than for non-ELL students.</p>
<table-wrap id="table6-0013164411408074" position="float">
<label>Table 6.</label>
<caption>
<p>Reliability of Difference Scores</p>
</caption>
<graphic alternate-form-of="table6-0013164411408074" xlink:href="10.1177_0013164411408074-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Difference score</th>
<th align="center">ELL</th>
<th align="center">Non-ELL</th>
</tr>
</thead>
<tbody>
<tr>
<td>V-Q</td>
<td>0.78800</td>
<td>0.87730</td>
</tr>
<tr>
<td>V-NV</td>
<td>0.87410</td>
<td>0.89490</td>
</tr>
<tr>
<td>Q-NV</td>
<td>0.81885</td>
<td>0.88252</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn6-0013164411408074">
<p><italic>Note</italic>. ELL = English-language learners.</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section7-0013164411408074" sec-type="discussion">
<title>Discussion</title>
<p>On a multidimensional measure of abilities comprising verbal, nonverbal, and quantitative reasoning skills, non-ELL students outperformed their ELL peers, with the most dramatic group differences exhibited on the Verbal Battery and smaller differences on the Quantitative and Nonverbal Batteries. ELL student scores, in turn, exhibited range restriction compared with non-ELL student scores. Results from this study shed some light on potential consequences of this range restriction. Specifically, results suggest that verbal and quantitative reasoning skills are measured less precisely for ELL students than they are for non-ELL students. ELL students’ verbal reasoning scores also correlated less strongly with their scores from the Nonverbal and Quantitative Batteries compared with non-ELL students. In addition, the generalizability coefficients for the Verbal and Quantitative Batteries were substantially lower for ELL students than they were for non-ELLs, for whom all three batteries performed uniformly well. In fact, ELL students would need to respond to more than twice as many quantitative items and more than three times as many verbal items in order to achieve comparable reliability to non-ELL students. These results are consistent with <xref ref-type="bibr" rid="bibr24-0013164411408074">Li and Brennan (2007)</xref>, who also found substantially lower precision for ELL students taking a reading achievement test. They estimated that their reading test would need to double in length to match precision for ELL and non-ELL students. The fact that G- and D-study results differed across the two groups of students in our study suggests that language status does, in fact, matter for measurement precision.</p>
<p>Despite these limitations in the measurement precision for ELL students, results also suggest that the Verbal and Quantitative Batteries of this multidimensional ability test may contribute reliable information about the abilities of these students. Although the Verbal and Quantitative Batteries showed lower reliability than the Nonverbal Battery, the coefficients (.84 and .88) are comparable with the reliabilities of many subtest scores on individually administered ability tests (<xref ref-type="bibr" rid="bibr27-0013164411408074">Lohman, 2009</xref>). Moreover, simply administering a much longer version of the Nonverbal Battery to ELL students underrepresents the construct and diminishes measurement precision, to the extent that verbal, nonverbal, and quantitative reasoning all form part of the ability construct. Combining verbal, quantitative, and nonverbal reasoning scores produces composite scores that are almost as reliable for ELL students as they are for non-ELL students. Furthermore, a reliability-like coefficient for individual student profiles was quite similar for both groups of students.</p>
<sec id="section8-0013164411408074">
<title>Implications</title>
<p>Researchers of cognitive abilities have long recognized that tests with varied content are better indicators of ability than unidimensional tests. Despite this, researchers concerned with the assessment of culturally and linguistically diverse students tend to prefer nonverbal reasoning tasks for ELL populations, believing they provide the only reliable means of measuring these students’ cognitive abilities. This study found evidence that the concern over the use of verbal and quantitative content is only partially supported. Certainly, the differences in reliability for the Verbal and Quantitative Batteries across language groups warrant further research into developing tests that better measure the full range of reasoning abilities in ELL students. However, this study clearly showed that administering a measure of cognitive abilities that draws on a range of content areas provides superior reliability to a test that consists of only nonverbal items. Indeed, we showed that creating a nonverbal test with the same number of items as the CogAT would not achieve equivalent reliability to a test with verbal, quantitative, and nonverbal items, even for ELL students.</p>
<p>We found that the reliability of composite scores and profiles for ELL and non-ELL students was surprisingly similar. This result provides support for the use of these tests in making instructional decisions, as they provide reliable information about the reasoning skills of students regardless of language status. However, the results of this study do not address larger issues about the validity of the interpretations that can be made from test scores. In fact, given the clear differences in opportunity to learn between ELL and non-ELL students, certain adjustments to the interpretations of scores are warranted for the use of either cut-scores for identification purposes or profiles for differentiating instruction (<xref ref-type="bibr" rid="bibr26-0013164411408074">Lohman, 2006</xref>; <xref ref-type="bibr" rid="bibr43-0013164411408074">Weiss, Saklofske, Prifitera, &amp; Holdnack, 2006</xref>).</p>
<p>If ability profiles are used, specific instructional implications must be developed for ELL students. Even with the same ability profile, ELL students will differ from non-ELL students in their instructional needs, including pace, structure, and linguistic support (<xref ref-type="bibr" rid="bibr34-0013164411408074">Olsen, 2010</xref>). Specific instructional recommendations have been developed for non-ELL students (<xref ref-type="bibr" rid="bibr30-0013164411408074">Lohman &amp; Hagen, 2001b</xref>), but a new line of research is needed to develop appropriate ideas for curricular differentiation for ELL students.</p>
</sec>
<sec id="section9-0013164411408074">
<title>Limitations and Future Directions for Research</title>
<p>Several aspects of the study design may limit the extent to which these results generalize to other contexts. First, one cannot assume that the conclusions supported here will apply to other ability tests, even those that conceptualize reasoning abilities as a multidimensional construct. Thus, future studies should investigate whether differences in the precision with which the abilities of ELL and non-ELL students are measured materialize on other multidimensional ability tests. Indeed, we noted that evaluation of reliability coefficients for examinee subgroups (rather than the overall population) may be informative to test developers in a number of contexts who must make decisions about test composition and minimum test length. Second, analyses reported here were based on relatively small samples of ELL and non-ELL students. As such, estimated variance and covariance components are subject to sampling error and should be interpreted cautiously. Third, the ELL sample used in this study consisted almost entirely of Hispanic ELL students. As such, it is possible that the conclusions reached in this study would not apply to non-Hispanic ELL students. Both ELL and non-ELL samples also consisted of large numbers of low SES students, with a slightly greater proportion of ELL students being eligible for free or reduced-price lunch. Thus, future studies should replicate this study using larger and more heterogeneous samples of ELL students and native English speakers. In addition, future studies might consider finer distinctions of ELL categories because language proficiency varies greatly among ELL students and even among non-ELL students, particularly when there are large numbers of former or reclassified ELL students in that group. Fourth, these analyses were based on raw scores while the CogAT only reports scaled scores at the battery level, which are produced by applying a Rasch model scale transformation to raw scores. If the analysis were to be repeated using scaled scores instead of raw scores, it is possible that results would be different, particularly for ELL students, who disproportionately occupy the lower end of the score distribution where scores are most affected by the scale transformation.</p>
<p>One aspect of multidimensional ability tests that might impact measurement precision, but which was not considered in this study, is item format. In the same way that tests measuring varied content and reflecting multiple dimensions of reasoning are better measures of the construct of general reasoning, it is believed that tests using multiple item formats are more representative of the domain than single-format tests (<xref ref-type="bibr" rid="bibr33-0013164411408074">McCallum et al., 2001</xref>). Although the measure used in this study includes nine different item formats (three unique item formats within each battery), item format was not included as a facet in the design. Thus, it was not possible to test the hypothesis that varied item formats result in better measurement precision for ELL students relative to single-format tests.</p>
<p>Finally, the developmental effects of grade level could be included in future study designs. The students included in our sample were third- and fourth-grade students, but grade level was not factored into the design. Thus, any potential differences in the reliability of third- and fourth-grade student ability would be partially confounded with differences in language status. Specifically, whereas the distribution of third- and fourth-grade students was 55% and 45%, respectively, for ELL students, this distribution was 40% and 60%, respectively, for non-ELL students. This confounding of grade-level with language status would be problematic, for example, if third-grade reasoning abilities were less reliably estimated than fourth-grade reasoning abilities. Such confounding could yield spurious conclusions of differential reliability for ELL and non-ELL students. However, the fact that other research has found similar disparities in the measurement precision of ELL and non-ELL students lends support to our findings (<xref ref-type="bibr" rid="bibr24-0013164411408074">Li &amp; Brennan, 2007</xref>; <xref ref-type="bibr" rid="bibr40-0013164411408074">Solano-Flores &amp; Li, 2006</xref>).</p>
<p>The results of this study only support the conclusion that verbal and quantitative ability are measured less precisely for ELL students. Additional research is needed to identify methods of increasing that precision. For instance, while the D-study analysis indicated that twice or three times as many items might be needed, this analysis assumes the test developer is adding items of similar quality. Adding or substituting items of higher quality with respect to measuring the skills of ELL students would increase precision more rapidly. One possibility is creating item formats that are less sensitive to language background while still measuring important reasoning domains. For example, new picture-based verbal item formats are being developed that may tap into verbal reasoning without tying the test items to a particular administration language.<sup><xref ref-type="fn" rid="fn8-0013164411408074">8</xref></sup> Similar research attempting to reduce the language load of mathematics tests is also being conducted extensively in the achievement domain (e.g., <xref ref-type="bibr" rid="bibr1-0013164411408074">Abedi &amp; Lord, 2001</xref>).</p>
</sec>
<sec id="section10-0013164411408074">
<title>Conclusion</title>
<p>Multidimensional tests can play an important role in helping teachers adapt instruction appropriately to the range of talents of students in their classroom. However, the interpretation of test scores and the instructional recommendations they offer must be validated for culturally and linguistically diverse students. This study explored the role that language status plays in the precision with which multidimensional ability tests measure the reasoning abilities of ELL and non-ELL students. We concluded that both composite and profile scores may provide reliable information about the abilities of these students when interpreted appropriately. However, this study did not show whether these composite and profile scores have the same instructional implications for ELL and non-ELL students. That is, we have only set the foundation for research on the validity of interpretations made from these tests by showing that they provide acceptable measurement precision. Given the potential for such tests to be used for adaptation of instruction for both ELL and non-ELL students, it is important for additional research to explore the educational value of such applications.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We would like to gratefully acknowledge the thoughtful comments provided by Robert Brennan, David Lohman, John Young, Michael Kane, Dan Eignor, and Guillermo Solano-Flores on an earlier draft of this article.</p>
</ack>
<fn-group>
<fn fn-type="other">
<p>Portions of the research described in this article were completed while both authors were employed by The University of Iowa. Any opinions expressed in this paper are those of the authors and not necessarily of Pearson.</p>
</fn>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0013164411408074">
<label>1.</label>
<p>When corrected for range restriction, <xref ref-type="bibr" rid="bibr3-0013164411408074">Borghese (2009)</xref> reported that these uncorrected correlations increased to .64 and .38, respectively.</p>
</fn>
<fn fn-type="other" id="fn2-0013164411408074">
<label>2.</label>
<p>What the CogAT manual refers to as batteries (verbal, quantitative, and nonverbal) might be called subtests in other test programs. Consistent with that terminology, we will use the term <italic>battery</italic> to refer to individual subtests. Also to maintain consistency, we refer to subdivisions within batteries as test sections.</p>
</fn>
<fn fn-type="other" id="fn3-0013164411408074">
<label>3.</label>
<p>The full-length version of Form 6 of the CogAT includes 65 verbal, 60 quantitative, and 65 nonverbal items.</p>
</fn>
<fn fn-type="other" id="fn4-0013164411408074">
<label>4.</label>
<p>We use the notation and terminology developed in <xref ref-type="bibr" rid="bibr6-0013164411408074">Brennan (2001b)</xref>.</p>
</fn>
<fn fn-type="other" id="fn5-0013164411408074">
<label>5.</label>
<p>See <xref ref-type="bibr" rid="bibr7-0013164411408074">Brown (1999)</xref> and <xref ref-type="bibr" rid="bibr24-0013164411408074">Li and Brennan (2007)</xref> for similar approaches to examining the issue of differential reliability across different language groups.</p>
</fn>
<fn fn-type="other" id="fn6-0013164411408074">
<label>6.</label>
<p>Estimated disattenuated correlations are computed using estimated variance and covariance components corresponding to universe scores. Thus, they are corrected for measurement error.</p>
</fn>
<fn fn-type="other" id="fn7-0013164411408074">
<label>7.</label>
<p>For a detailed explanation of within-person profile variability, as well as formulas for computing the relevant variance components, see <xref ref-type="bibr" rid="bibr6-0013164411408074">Brennan (2001b)</xref>, especially pages 320-324.</p>
</fn>
<fn fn-type="other" id="fn8-0013164411408074">
<label>8.</label>
<p>For more information, contact the first author.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Abedi</surname><given-names>J.</given-names></name>
<name><surname>Lord</surname><given-names>C.</given-names></name>
</person-group> (<year>2001</year>). <article-title>The language factor in mathematics tests</article-title>. <source>Applied Measurement in Education</source>, <volume>14</volume>, <fpage>219</fpage>-<lpage>234</lpage>.</citation>
</ref>
<ref id="bibr2-0013164411408074">
<citation citation-type="book">
<collab>American Federation of Teachers</collab>. (<year>2004</year>). <source>Closing the achievement gap: Focus on Latino students</source> (<comment>Policy Brief No. 17</comment>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>. (ERIC Document Service No. ED497878)</citation>
</ref>
<ref id="bibr3-0013164411408074">
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Borghese</surname><given-names>P.</given-names></name>
</person-group> (<year>2009</year>). <source>An analysis of predictive, convergent, and discriminant validity of the Universal Nonverbal Intelligence Test with limited English proficient Mexican-American elementary students</source> <comment>(Doctoral dissertation). Retrieved from ProQuest Dissertations. (AAT 3351828)</comment></citation>
</ref>
<ref id="bibr4-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bracken</surname><given-names>B. A.</given-names></name>
<name><surname>McCallum</surname><given-names>R. S.</given-names></name>
</person-group> (<year>1998</year>). <source>Universal Nonverbal Intelligence Test (UNIT)</source>. <publisher-loc>Itasca, IL</publisher-loc>: <publisher-name>Riverside</publisher-name>.</citation>
</ref>
<ref id="bibr5-0013164411408074">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2001a</year>). <article-title>mGENOVA (Version 2.1)</article-title>. [<comment>Computer software</comment>]. <publisher-loc>Iowa City, IA</publisher-loc>: <publisher-name>Iowa Testing Programs</publisher-name>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.education.uiowa.edu/casma/computer_programs.htm#genova">http://www.education.uiowa.edu/casma/computer_programs.htm#genova</ext-link></comment></citation>
</ref>
<ref id="bibr6-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2001b</year>). <source>Generalizability theory</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr7-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Brown</surname><given-names>J. D.</given-names></name>
</person-group> (<year>1999</year>). <article-title>The relative importance of persons, items, subtests, and languages to TOEFL test variance</article-title>. <source>Language Testing</source>, <volume>16</volume>, <fpage>217</fpage>-<lpage>238</lpage>.</citation>
</ref>
<ref id="bibr8-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Callahan</surname><given-names>C. M.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Identifying gifted students from underrepresented populations</article-title>. <source>Theory into Practice</source>, <volume>44</volume>(<issue>2</issue>), <fpage>98</fpage>-<lpage>104</lpage>.</citation>
</ref>
<ref id="bibr9-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Carroll</surname><given-names>J. B.</given-names></name>
</person-group> (<year>1993</year>). <source>Human cognitive abilities: A survey of factor-analytic studies</source>. <publisher-loc>Cambridge, England</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cronbach</surname><given-names>L. J.</given-names></name>
<name><surname>Gleser</surname><given-names>G. C.</given-names></name>
</person-group> (<year>1953</year>). <article-title>Assessing similarity between profiles</article-title>. <source>Psychological Bulletin</source>, <volume>50</volume>, <fpage>456</fpage>-<lpage>473</lpage>.</citation>
</ref>
<ref id="bibr11-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Feingold</surname><given-names>A.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Sex differences in variability in intellectual abilities: A new look at an old controversy</article-title>. <source>Review of Educational Research</source>, <volume>62</volume>, <fpage>61</fpage>-<lpage>84</lpage>.</citation>
</ref>
<ref id="bibr12-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Floyd</surname><given-names>R. G.</given-names></name>
<name><surname>Shands</surname><given-names>E. I.</given-names></name>
<name><surname>Rafael</surname><given-names>F. A.</given-names></name>
<name><surname>Bergeron</surname><given-names>R.</given-names></name>
<name><surname>McGrew</surname><given-names>K. S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>The dependability of general-factor loadings: The effects of factor-extraction methods, test battery composition, test battery size, and their interactions</article-title>. <source>Intelligence</source>, <volume>37</volume>, <fpage>453</fpage>-<lpage>465</lpage>.</citation>
</ref>
<ref id="bibr13-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ford</surname><given-names>D. Y.</given-names></name>
<name><surname>Grantham</surname><given-names>T. C.</given-names></name>
<name><surname>Whiting</surname><given-names>G. W.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Culturally and linguistically diverse students in gifted education: Recruitment and retention issues</article-title>. <source>Exceptional Children</source>, <volume>74</volume>, <fpage>289</fpage>-<lpage>306</lpage>.</citation>
</ref>
<ref id="bibr14-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gustafsson</surname><given-names>J.-E.</given-names></name>
<name><surname>Balke</surname><given-names>G.</given-names></name>
</person-group> (<year>1993</year>). <article-title>General and specific abilities as predictors of school achievement</article-title>. <source>Multivariate Behavioral Research</source>, <volume>28</volume>, <fpage>407</fpage>-<lpage>434</lpage>.</citation>
</ref>
<ref id="bibr15-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Hammill</surname><given-names>D. D.</given-names></name>
<name><surname>Pearson</surname><given-names>N. A.</given-names></name>
<name><surname>Widerholdt</surname><given-names>J. L.</given-names></name>
</person-group> (<year>1996</year>). <source>Comprehensive Test of Nonverbal Intelligence</source>. <publisher-loc>Austin, TX</publisher-loc>: <publisher-name>PRO-ED</publisher-name>.</citation>
</ref>
<ref id="bibr16-0013164411408074">
<citation citation-type="book">
<collab>Harcourt Educational Assessment</collab>. (<year>2003</year>). <source>Stanford English Language Proficiency Test</source>. <publisher-loc>San Antonio, TX</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr17-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Harris</surname><given-names>B.</given-names></name>
<name><surname>Rapp</surname><given-names>K. E.</given-names></name>
<name><surname>Martinez</surname><given-names>R. S.</given-names></name>
<name><surname>Plucker</surname><given-names>J. A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Identifying English language learners for gifted and talented programs: Current practices and recommendations for improvement</article-title>. <source>Roeper Review</source>, <volume>29</volume>, <fpage>26</fpage>-<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr18-0013164411408074">
<citation citation-type="thesis">
<person-group person-group-type="author">
<name><surname>Jones</surname><given-names>C. K.</given-names></name>
</person-group> (<year>2006</year>). <source>The relationship of language proficiency, general intelligence, and reading achievement with a sample of low performing, limited English proficient students</source> <comment>(Doctoral dissertation). Retrieved from ProQuest Dissertations. (AAT 3296415)</comment></citation>
</ref>
<ref id="bibr19-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Keith</surname><given-names>T. Z.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Effects of general and specific abilities on student achievement: Similarities and differences across ethnic groups</article-title>. <source>School Psychology Quarterly</source>, <volume>14</volume>, <fpage>239</fpage>-<lpage>262</lpage>.</citation>
</ref>
<ref id="bibr20-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kvist</surname><given-names>A. V.</given-names></name>
<name><surname>Gustafsson</surname><given-names>J. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The relation between fluid intelligence and the general factor as a function of cultural background: A test of Cattell’s investment theory</article-title>. <source>Intelligence</source>, <volume>36</volume>, <fpage>422</fpage>-<lpage>436</lpage>.</citation>
</ref>
<ref id="bibr21-0013164411408074">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Lakin</surname><given-names>J. M.</given-names></name>
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
</person-group> (<conf-date>2009, July</conf-date>) <source>Predictive validity of ability tests for academic achievement similar for English-language learners and non-ELL students</source>. <conf-name>Poster presented at the biannual meeting of the International Society for the Study of Individual Differences</conf-name>, <conf-loc>Chicago, IL</conf-loc>.</citation>
</ref>
<ref id="bibr22-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lakin</surname><given-names>J. M.</given-names></name>
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
</person-group> (<year>2011</year>). <article-title>The predictive accuracy of verbal, quantitative, and nonverbal reasoning tests: Consequences for talent identification and program diversity</article-title>. <source>Journal for the Education of the Gifted</source>, <volume>34</volume>, <fpage>595</fpage>-<lpage>623</lpage>.</citation>
</ref>
<ref id="bibr23-0013164411408074">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Lewis</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2001</year>). <source>Language isn’t needed: Nonverbal assessments and gifted learners</source>. <conf-name>Proceedings of the Growing Partnership for Rural Special Education Conference</conf-name>, <conf-loc>San Diego, CA</conf-loc>. (<comment>ERIC Document Reproduction Service No. ED 453026</comment>)</citation>
</ref>
<ref id="bibr24-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Li</surname><given-names>D.</given-names></name>
<name><surname>Brennan</surname><given-names>R. L.</given-names></name>
</person-group> (<year>2007</year>). <source>A multi-group generalizability analysis of a large-scale reading comprehension test</source> (<comment>CASMA Research Report No. 25</comment>). <publisher-loc>Iowa City, IA</publisher-loc>: <publisher-name>Center for Advanced Studies in Measurement and Assessment</publisher-name>.</citation>
</ref>
<ref id="bibr25-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
</person-group> (<year>2005</year>). <article-title>The role of nonverbal ability tests in identifying academically gifted students: An aptitude perspective</article-title>. <source>Gifted Child Quarterly</source>, <volume>49</volume>, <fpage>111</fpage>-<lpage>138</lpage>.</citation>
</ref>
<ref id="bibr26-0013164411408074">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
</person-group> (<year>2006</year>). <source>Practical advice on using the Cognitive Abilities Test as part of a talent identification system</source>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://faculty.education.uiowa.edu/dlohman/">http://faculty.education.uiowa.edu/dlohman/</ext-link></comment></citation>
</ref>
<ref id="bibr27-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Identifying academically talented students: Some general principles, two specific procedures</article-title>. In <person-group person-group-type="editor">
<name><surname>Shavinina</surname><given-names>L.</given-names></name>
</person-group> (Ed.), <source>International handbook on giftedness</source> (pp. <fpage>971</fpage>-<lpage>997</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr28-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
<name><surname>Gambrell</surname><given-names>J.</given-names></name>
<name><surname>Lakin</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The commonality of extreme discrepancies in the ability profiles of academically gifted students</article-title>. <source>Psychology Science Quarterly</source>, <volume>50</volume>, <fpage>269</fpage>-<lpage>282</lpage>.</citation>
</ref>
<ref id="bibr29-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
<name><surname>Hagen</surname><given-names>E. P.</given-names></name>
</person-group> (<year>2001a</year>). <source>Cognitive Abilities Test (Form 6)</source>. <publisher-loc>Itasca, IL</publisher-loc>: <publisher-name>Riverside</publisher-name>.</citation>
</ref>
<ref id="bibr30-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
<name><surname>Hagen</surname><given-names>E. P.</given-names></name>
</person-group> (<year>2001b</year>). <source>CogAT Form 6 interpretive guide for teachers and counselors</source>. <publisher-loc>Itasca, IL</publisher-loc>: <publisher-name>Riverside</publisher-name>.</citation>
</ref>
<ref id="bibr31-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
<name><surname>Hagen</surname><given-names>E. P.</given-names></name>
</person-group> (<year>2002</year>). <source>Cognitive Abilities Test (Form 6): Research handbook</source>. <publisher-loc>Itasca, IL</publisher-loc>: <publisher-name>Riverside</publisher-name>.</citation>
</ref>
<ref id="bibr32-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lohman</surname><given-names>D. F.</given-names></name>
<name><surname>Korb</surname><given-names>K.</given-names></name>
<name><surname>Lakin</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Identifying academically gifted English language learners using nonverbal tests: A comparison of the Raven, NNAT, and CogAT</article-title>. <source>Gifted Child Quarterly</source>, <volume>52</volume>, <fpage>275</fpage>-<lpage>296</lpage>.</citation>
</ref>
<ref id="bibr33-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McCallum</surname><given-names>R. S.</given-names></name>
<name><surname>Bracken</surname><given-names>B. A.</given-names></name>
<name><surname>Wasserman</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2001</year>). <source>Essentials of nonverbal assessment</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr34-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Olsen</surname><given-names>L.</given-names></name>
</person-group> (<year>2010</year>). <source>Reparable harm: Fulfilling the unkept promise of educational opportunity for long term English learners</source>. <publisher-loc>Long Beach, CA</publisher-loc>: <publisher-name>Californians Together</publisher-name>.</citation>
</ref>
<ref id="bibr35-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ortiz</surname><given-names>S. O.</given-names></name>
<name><surname>Ochoa</surname><given-names>S. H.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Advances in cognitive assessment of culturally and linguistically diverse individuals</article-title>. In <person-group person-group-type="editor">
<name><surname>Flanagan</surname><given-names>D. P.</given-names></name>
<name><surname>Harrison</surname><given-names>P. L.</given-names></name>
</person-group> (Eds.), <source>Contemporary intellectual assessment</source> (pp. <fpage>234</fpage>-<lpage>250</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Guilford Press</publisher-name>.</citation>
</ref>
<ref id="bibr36-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pierce</surname><given-names>R. L.</given-names></name>
<name><surname>Adams</surname><given-names>C. M.</given-names></name>
<name><surname>Speirs-Neumeister</surname><given-names>K. L.</given-names></name>
<name><surname>Cassady</surname><given-names>J. C.</given-names></name>
<name><surname>Dixon</surname><given-names>F. A.</given-names></name>
<name><surname>Cross</surname><given-names>T. L.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Development of an identification procedure for a large urban school corporation: Identifying culturally diverse and academically gifted elementary students</article-title>. <source>Roeper Review</source>, <volume>29</volume>, <fpage>113</fpage>-<lpage>118</lpage>.</citation>
</ref>
<ref id="bibr37-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Powers</surname><given-names>S.</given-names></name>
<name><surname>Barkan</surname><given-names>J. H.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Concurrent validity of the Standard Progressive Matrices for Hispanic and non-Hispanic seventh-grade students</article-title>. <source>Psychology in the Schools</source>, <volume>23</volume>, <fpage>333</fpage>-<lpage>336</lpage>.</citation>
</ref>
<ref id="bibr38-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Reschly</surname><given-names>D. J.</given-names></name>
<name><surname>Hosp</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2004</year>). <article-title>State SLD identification policies and practices</article-title>. <source>Learning Disability Quarterly</source>, <volume>27</volume>, <fpage>197</fpage>-<lpage>213</lpage>.</citation>
</ref>
<ref id="bibr39-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Snow</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Aptitude theory: Yesterday, today, and tomorrow</article-title>. <source>Educational Psychologist</source>, <volume>27</volume>, <fpage>1</fpage>-<lpage>5</lpage>.</citation>
</ref>
<ref id="bibr40-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Solano-Flores</surname><given-names>G.</given-names></name>
<name><surname>Li</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <article-title>The use of generalizability (G) theory in the testing of linguistic minorities</article-title>. <source>Educational Measurement: Issues and Practice</source>, <volume>25</volume>, <fpage>13</fpage>-<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr41-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Süß</surname><given-names>H.-M.</given-names></name>
<name><surname>Beauducel</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Faceted models of intelligence</article-title>. In <person-group person-group-type="editor">
<name><surname>Wilhelm</surname><given-names>O.</given-names></name>
<name><surname>Engle</surname><given-names>R. W.</given-names></name>
</person-group> (Eds.), <source>Handbook of understanding and measuring intelligence</source> (pp. <fpage>313</fpage>-<lpage>332</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr42-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Thorndike</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1982</year>). <source>Applied psychometrics</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Houghton Mifflin</publisher-name>.</citation>
</ref>
<ref id="bibr43-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Weiss</surname><given-names>L. G.</given-names></name>
<name><surname>Saklofske</surname><given-names>D. H.</given-names></name>
<name><surname>Prifitera</surname><given-names>A.</given-names></name>
<name><surname>Holdnack</surname><given-names>J. A.</given-names></name>
</person-group> (<year>2006</year>). <source>WISC-IV advanced clinical interpretation</source>. <publisher-loc>Burlington, MA</publisher-loc>: <publisher-name>Elsevier</publisher-name>.</citation>
</ref>
<ref id="bibr44-0013164411408074">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wilhelm</surname><given-names>O.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Measuring reasoning ability</article-title>. In <person-group person-group-type="editor">
<name><surname>Wilhelm</surname><given-names>O.</given-names></name>
<name><surname>Engle</surname><given-names>R. W.</given-names></name>
</person-group> (Eds.), <source>Handbook of understanding and measuring intelligence</source> (pp. <fpage>373</fpage>-<lpage>392</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr45-0013164411408074">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Young</surname><given-names>J. W.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A framework for test validity research on content assessments taken by English language learners</article-title>. <source>Educational Assessment</source>, <volume>14</volume>, <fpage>122</fpage>-<lpage>138</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>