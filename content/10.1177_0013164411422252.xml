<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EPM</journal-id>
<journal-id journal-id-type="hwp">spepm</journal-id>
<journal-title>Educational and Psychological Measurement</journal-title>
<issn pub-type="ppub">0013-1644</issn>
<issn pub-type="epub">1552-3888</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0013164411422252</article-id>
<article-id pub-id-type="publisher-id">10.1177_0013164411422252</article-id>
<title-group>
<article-title>A Proposed Solution to the Problem With Using Completely Random Data to Assess the Number of Factors With Parallel Analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Green</surname><given-names>Samuel B.</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411422252">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Levy</surname><given-names>Roy</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411422252">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Thompson</surname><given-names>Marilyn S.</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411422252">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Lu</surname><given-names>Min</given-names></name>
<xref ref-type="aff" rid="aff1-0013164411422252">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Lo</surname><given-names>Wen-Juo</given-names></name>
<xref ref-type="aff" rid="aff2-0013164411422252">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0013164411422252"><label>1</label>Arizona State University, Tempe, AZ, USA</aff>
<aff id="aff2-0013164411422252"><label>2</label>University of Arkansas, Fayetteville, AR, USA</aff>
<author-notes>
<corresp id="corresp1-0013164411422252">Samuel B. Green, Arizona State University, PO Box 873701, Tempe, AZ 85287-3701, USA Email: <email>samgreen@asu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>6</month>
<year>2012</year>
</pub-date>
<volume>72</volume>
<issue>3</issue>
<fpage>357</fpage>
<lpage>374</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>A number of psychometricians have argued for the use of parallel analysis to determine the number of factors. However, parallel analysis must be viewed at best as a heuristic approach rather than a mathematically rigorous one. The authors suggest a revision to parallel analysis that could improve its accuracy. A Monte Carlo study is conducted to compare revised and traditional parallel analysis approaches. Five dimensions are manipulated in the study: number of observations, number of factors, number of measured variables, size of the factor loadings, and degree of correlation between factors. Based on the results, the revised parallel analysis method, using principal axis factoring and the 95th percentile eigenvalue rule, offers promise.</p>
</abstract>
<kwd-group>
<kwd>factor analysis</kwd>
<kwd>number of factors</kwd>
<kwd>parallel analysis</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Researchers frequently use exploratory factor analysis to establish factors that could explain in a parsimonious and meaningful way the covariation among a set of measures. Empirical criteria are frequently applied to suggest the number of factors that should be extracted. <xref ref-type="bibr" rid="bibr9-0013164411422252">Horn (1965)</xref> and others (e.g., <xref ref-type="bibr" rid="bibr4-0013164411422252">Fabrigar, Wegener, MacCallum, &amp; Strahan, 1999</xref>; <xref ref-type="bibr" rid="bibr13-0013164411422252">Preacher &amp; MacCallum, 2003</xref>) have argued that the popular eigenvalue-greater-than-one criterion (<xref ref-type="bibr" rid="bibr7-0013164411422252">Guttman, 1954</xref>; <xref ref-type="bibr" rid="bibr11-0013164411422252">Kaiser, 1960</xref>) does not perform well, and parallel analysis, perhaps in conjunction with other criteria such as the scree test (<xref ref-type="bibr" rid="bibr2-0013164411422252">Cattell, 1978</xref>), should be used to determine the number of factors. Since <xref ref-type="bibr" rid="bibr9-0013164411422252">Horn (1965)</xref> first presented parallel analysis, a number of researchers have suggested how parallel analysis might be modified to improve its accuracy (e.g., <xref ref-type="bibr" rid="bibr1-0013164411422252">Buja &amp; Eyuboglu, 1992</xref>; <xref ref-type="bibr" rid="bibr6-0013164411422252">Glorfeld, 1995</xref>; <xref ref-type="bibr" rid="bibr10-0013164411422252">Humphreys &amp; Montanelli, 1975</xref>). Nevertheless, no parallel analysis method appears uniformly better than others across a wide range of population conditions (e.g., <xref ref-type="bibr" rid="bibr3-0013164411422252">Crawford et al., 2010</xref>).</p>
<p>All parallel analysis methods include the generation of comparison data sets containing completely random data to assess the number of factors. As suggested by previous researchers (<xref ref-type="bibr" rid="bibr8-0013164411422252">Harshman &amp; Reddon, 1983</xref>; <xref ref-type="bibr" rid="bibr16-0013164411422252">Turner 1998</xref>), the general use of comparison data sets with completely random data is flawed. In this article, we offer an approach that should at least partially correct this problem, particularly with well-defined factor structures, and evaluate this proposed approach using a Monte Carlo study.</p>
<sec id="section1-0013164411422252">
<title>Brief Literature Review of Parallel Analysis</title>
<p>There are multiple approaches for conducting a parallel analysis (<xref ref-type="bibr" rid="bibr6-0013164411422252">Glorfeld, 1995</xref>; <xref ref-type="bibr" rid="bibr9-0013164411422252">Horn, 1965</xref>; <xref ref-type="bibr" rid="bibr19-0013164411422252">Zwick &amp; Velicer, 1986</xref>). One common method involves performing a principal component analysis on an observed correlation matrix. Next, multiple correlation matrices (e.g., 100 matrices) are generated assuming uncorrelated multivariate normally distributed population data with the same number of variables and sample size as the observed data. Principal component analysis is conducted on each of these random correlation matrices. The mean eigenvalues for the sequential components are computed. The assessed number of dimensions is equal to the number of eigenvalues for the observed data that exceed the respective means of eigenvalues for the random data. <xref ref-type="fig" rid="fig1-0013164411422252">Figure 1</xref> is an example of results from a parallel analysis based on principal component analysis with mean eigenvalues; the results indicate three dimensions.</p>
<fig id="fig1-0013164411422252" position="float">
<label>Figure 1.</label>
<caption>
<p>Illustration of traditional parallel analysis based on principal component analysis and the mean eigenvalue criterion</p>
</caption>
<graphic xlink:href="10.1177_0013164411422252-fig1.tif"/>
</fig>
<p>Some methodologists have advocated using common factor methods for conducting parallel analysis rather than principal component analysis (e.g., <xref ref-type="bibr" rid="bibr5-0013164411422252">Ford, MacCallum, &amp; Tait, 1986</xref>; <xref ref-type="bibr" rid="bibr10-0013164411422252">Humphreys &amp; Montanelli, 1975</xref>). They argue that common factor methods should be used to assess the factor structure of educational and psychological scales in that the model underlying these methods allows for measurement error in scales, whereas the model for principal component analysis does not (e.g., <xref ref-type="bibr" rid="bibr4-0013164411422252">Fabrigar et al., 1999</xref>; <xref ref-type="bibr" rid="bibr14-0013164411422252">Snook &amp; Gorsuch, 1989</xref>; <xref ref-type="bibr" rid="bibr18-0013164411422252">Widaman, 1993</xref>). To be consistent, researchers should use common factor analytic methods in conducting their parallel analyses if they plan to apply these methods to determine the underlying structure of their scales. More technically speaking, <xref ref-type="bibr" rid="bibr12-0013164411422252">Mulaik (2010</xref>, p. 195) states in the revision of his classic book on factor analysis that the major failing of the parallel analysis method is that it is “based on eigenvalues of the wrong matrix.” He then points out that the rationale underlying Thurstone’s minimum rank concept is based not on the correlation matrix but the reduced correlation matrix with communalities along the diagonal. Nevertheless, the majority of research on the application of parallel analysis has used principal component analysis (<xref ref-type="bibr" rid="bibr15-0013164411422252">Steger, 2006</xref>; <xref ref-type="bibr" rid="bibr17-0013164411422252">Velicer, Eaton, &amp; Fava, 2000</xref>).</p>
<p><xref ref-type="bibr" rid="bibr19-0013164411422252">Zwick and Velicer (1986)</xref> found parallel analysis showed a slight tendency toward extracting “minor components,” especially with lower factor loadings and smaller sample sizes. This finding has supported the belief that parallel analysis tends to overextract factors and has encouraged the use of a more stringent criterion in the form of the 95th or 99th percentile of the distribution of eigenvalues from the random data, which reduces the number of factors extracted (e.g., <xref ref-type="bibr" rid="bibr1-0013164411422252">Buja &amp; Eyuboglu, 1992</xref>; <xref ref-type="bibr" rid="bibr6-0013164411422252">Glorfeld, 1995</xref>; <xref ref-type="bibr" rid="bibr16-0013164411422252">Turner, 1998</xref>). Potentially, the application of these more stringent criteria could be used with parallel analyses based on common factor analytic or principal component analytic methods.</p>
<p>Recently, <xref ref-type="bibr" rid="bibr3-0013164411422252">Crawford et al. (2010)</xref> compared the performance of parallel analysis using principal component analysis and parallel analysis using principal axis factoring to identify the number of underlying factors. Additionally, the accuracies of the mean eigenvalue and the 95th percentile eigenvalue criteria were examined. The 95th percentile criterion was preferable for assessing the first eigenvalue using either extraction method. In assessing subsequent eigenvalues, parallel analysis using principal component analysis tended to perform as well as or better than parallel analysis with principal axis factoring for models with one factor or multiple, minimally correlated factors, with neither eigenvalue criterion showing uniform superiority. If the factors were more than minimally correlated, parallel analysis based on principal axis factoring using the mean eigenvalue criterion generally performed best. Overall, the results were complex and did not allow for a single recommendation about what particular parallel analysis method or methods should be used in practice.</p>
</sec>
<sec id="section2-0013164411422252">
<title>A Conceptual Flaw With Traditional Parallel Analysis Methods</title>
<p>Under the hypothesis that the data have <italic>k</italic> underlying factors, the rationale for applying a traditional parallel analysis using the 95th percentile rule is that the empirical distribution of the <italic>k</italic>th + 1 eigenvalue from factor or component analyses of comparison data sets constitutes an appropriate reference distribution. The use of the 95th percentile rule embodies a decision rule akin to a hypothesis testing framework with α = .05. Theoretically, this rationale may be legitimate for evaluating the hypothesis that there are no common factors (i.e., <italic>k</italic> = 0) in that the empirical distribution of the 1st eigenvalue (i.e., <italic>k</italic> + 1 = 1) from the analysis of the comparison data sets represents the sampling distribution under the null hypothesis that there are no common factors. However, this rationale for traditional parallel analysis methods is problematic for evaluating the remaining eigenvalues, as the empirical distribution of eigenvalues beyond the first one is conditioned on the presence of zero factors rather than <italic>k</italic> factors (<xref ref-type="bibr" rid="bibr8-0013164411422252">Harshman &amp; Reddon, 1983</xref>; <xref ref-type="bibr" rid="bibr16-0013164411422252">Turner, 1998</xref>). From this perspective, parallel analysis must be viewed as a heuristic approach rather than a mathematically rigorous method resting on a solid conceptual base.</p>
<p><xref ref-type="bibr" rid="bibr16-0013164411422252">Turner (1998)</xref> demonstrated this problem with parallel analysis by generating data sets containing 10 variables with a sample size of either 100 or 1,000. Underlying the 10 variables was either no factors or a single factor that accounted for 30% or 50% of the total variance. The 2nd through 10th eigenvalues for data sets with one underlying factor were, on average, smaller than the eigenvalues for data sets with no underlying factors. In addition, the difference was greater when the sample size was small and the percentage of variance accounted for by the factor was large. Extrapolating from these results, if the number of factors is <italic>k</italic>, then the eigenvalues beyond the <italic>k</italic>th factor for random data in parallel analysis are positively biased and, therefore, could result in underfactoring.</p>
</sec>
<sec id="section3-0013164411422252">
<title>The Revised Parallel Analysis Method</title>
<p>Recognizing this flaw in parallel analysis, <xref ref-type="bibr" rid="bibr16-0013164411422252">Turner (1998)</xref> suggested that the appropriate reference distribution for evaluating the need for the <italic>k</italic>th + 1 factor is one in which the previous <italic>k</italic> factors have been modeled or incorporated. However, no specific procedure for modeling factors has been advanced or investigated based on our review of the literature. We next propose a method and then investigate the validity of the newly proposed approach.</p>
<p>The revised parallel analysis procedure is designed to allow for a more accurate assessment of the eigenvalue for the <italic>k</italic>th + 1 factor by taking into account the existence of <italic>k</italic> prior factors. Ideally, the multiple data sets for a parallel analysis should be generated based on the loadings of the <italic>k</italic> factors for the population from which the data were sampled. Because the population factor loadings are unknown, the sample factor loadings are substituted for the population values in conducting a revised parallel analysis. Results from this analysis should be more accurate to the extent that sample results are more stable and the factor structure is well defined. In other words, revised parallel analysis should yield more satisfactory results with large sample sizes and factor structures with high factor loadings, large number of measures per factor, and low correlation between factors.</p>
<p>We now describe revised parallel analysis in greater detail. As with traditional parallel analysis, a factor analysis is initially conducted on the observed data set, with <italic>N</italic><sub>V</sub> variables and <italic>N</italic><sub>O</sub> subjects. The statistics from the factor analysis required by revised parallel analysis include the eigenvalues and standardized loadings for the unrotated factors. Next, as described below, a 5-step approach is used to generate multiple comparison data sets based on a model with <italic>k</italic> underlying factors. The comparison data sets are factor analyzed and their results, in conjunction with the factor analytic results from the observed data set, are evaluated. This 5-step approach is applied initially using comparison data sets with 0 underlying factors (i.e., <italic>k</italic> = 0), then with 1 underlying factor (i.e., <italic>k</italic> = 1), and so on (i.e., <italic>k</italic> = 2, 3, . . .) until the decision about the inferred number of factors is made. We describe the 5-step approach as follows.</p>
<sec id="section4-0013164411422252">
<title>Step 1</title>
<p>Generate 100 comparison data sets (or more, if one prefers greater precision) with <italic>N</italic><sub>V</sub> variables and <italic>N</italic><sub>O</sub> subjects. The data are generated assuming <italic>k</italic> factors based on the following equation:</p>
<p><disp-formula id="disp-formula1-0013164411422252">
<mml:math display="block" id="math1-0013164411422252">
<mml:mrow>
<mml:mi mathsize="normal" mathvariant="bold">X</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi mathsize="normal" mathvariant="bold">F</mml:mi>
<mml:msup>
<mml:mi>Λ</mml:mi>
<mml:mo>′</mml:mo>
</mml:msup>
<mml:mo>+</mml:mo>
<mml:mi mathsize="normal" mathvariant="bold">E</mml:mi>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0013164411422252" xlink:href="10.1177_0013164411422252-eq1.tif"/>
</disp-formula></p>
<p>where <bold>X</bold> is a <italic>N</italic><sub>O</sub> × <italic>N</italic><sub>V</sub> matrix containing the scores on the variables, <bold>F</bold> is a <italic>N</italic><sub>O</sub> × <italic>N<sub>k</sub></italic> matrix containing generated scores on the factors, Λ is the <italic>N</italic><sub>V</sub> × <italic>N<sub>k</sub></italic> matrix containing the standardized loadings for <italic>k</italic> factors from the analysis based on the observed data set, and <bold>E</bold> is a <italic>N</italic><sub>O</sub> × <italic>N</italic><sub>V</sub> matrix containing generated residual scores for the variables. The factor scores are assumed to be independently and normally distributed with a mean of zero and a variance of 1. The error scores for a variable are assumed to be independently and normally distributed with a mean of zero and a variance equal to 1 minus the sum of the <italic>k</italic> squared loadings for the variable of interest. If <italic>k</italic> = 0, then <bold>X</bold> = <bold>E</bold>.</p>
</sec>
<sec id="section5-0013164411422252">
<title>Step 2</title>
<p>Conduct a factor analysis on each of the 100 comparative data sets generated in Step 1. Determine the eigenvalue for the <italic>k</italic>th + 1 factor for each of these data sets.</p>
</sec>
<sec id="section6-0013164411422252">
<title>Step 3</title>
<p>Compute a summary statistic (e.g., mean or 95th percentile) of the 100 eigenvalues for the <italic>k</italic>th + 1 factor determined in Step 2.</p>
</sec>
<sec id="section7-0013164411422252">
<title>Step 4</title>
<p>If the eigenvalue for the <italic>k</italic>th + 1 factor for the observed data set is less than the summary statistic (e.g., mean or 95th percentile) of the eigenvalues for the <italic>k</italic>th + 1 factor for the comparative data sets, then the parallel analysis is completed, and the number of factors underlying the observed data is inferred to be <italic>k</italic>. Otherwise, the number of factors underlying the sample data is inferred to be at least <italic>k</italic> + 1 factors unless contraindicated by the results of Step 5.</p>
</sec>
<sec id="section8-0013164411422252">
<title>Step 5</title>
<p>This step is required if the parallel analysis is based on principal axis factoring or is optional if it is based on principal component analyses. If the eigenvalue for the <italic>k</italic>th + 1 factor for the observed data set is less than or equal to zero with principal axis factoring or less than one with principal components, then the parallel analysis is completed, and the number of factors underlying the observed data is inferred to be <italic>k</italic>. If the parallel analysis proceeds based on the results of Steps 4 and 5, then <italic>k</italic> is incremented by 1, and Steps 1 through 5 are repeated.</p>
<p>The last step was required to take into account nonpositive eigenvalues based on principal axis factoring with observed data sets. A zero or negative eigenvalue indicates that a factor has zero or negative variance. In practice, no well-trained researcher extracts factors with zero or negative variance. Accordingly, Step 5 reflects good practice. In addition, this step prevents generating comparative data sets based on loadings for factors with no variance or negative variance.</p>
<p>The rationale for Step 5 differs for parallel analysis with principal component analysis. However, an argument can be made for applying Step 5 with principal component analysis and requiring eigenvalues to be at least 1.0 in value. In so doing, the revised method would incorporate the eigenvalue-greater-than-one rule. It is possible that this restriction could worsen the accuracy of the revised method in that the eigenvalue-greater-than-one rule is not strongly supported by research. On the other hand, the literature suggests that the use of the eigenvalue-greater-than-one rule tends to result in overfactoring (e.g., <xref ref-type="bibr" rid="bibr4-0013164411422252">Fabrigar et al., 1999</xref>; <xref ref-type="bibr" rid="bibr13-0013164411422252">Preacher &amp; MacCallum, 2003</xref>). Because the inclusion of this rule with the revised method using principal component analysis is invoked only to limit the number of factors, its use could result in an improvement to the performance of parallel analysis.</p>
</sec>
</sec>
<sec id="section9-0013164411422252">
<title>Objective of the Monte Carlo Study</title>
<p>The primary goal of our Monte Carlo study was to evaluate the relative accuracy of revised and traditional parallel analysis methods. These methods are applied using a number of variations; the 14 variations are shown in <xref ref-type="table" rid="table1-0013164411422252">Table 1</xref>. More specifically, for both traditional and revised parallel analyses, we applied two eigenvalue rules: (a) the eigenvalue for the observed data set must exceed the 95th percentile of the eigenvalues from the comparative data sets (λ<sub>95</sub> rule) and (b) the eigenvalue for the observed data set must exceed the mean of the eigenvalues from the comparative data sets (λ<sub><italic>M</italic></sub> rule). In addition, we considered parallel analyses using principal component analysis and principal axis factoring. Finally, we included or excluded in our parallel analysis methods a cutoff on eigenvalues of 0 for principal axis factoring and 1.0 for principal component analyses. For the revised parallel analysis method based on principal axis factoring, however, we always required eigenvalues to be greater than 0 to avoid the nonsensical step of generating comparative data based on a factor with zero or negative variance.</p>
<table-wrap id="table1-0013164411422252" position="float">
<label>Table 1.</label>
<caption>
<p>Exposition of the Fourteen Parallel Analysis Methods</p>
</caption>
<graphic alternate-form-of="table1-0013164411422252" xlink:href="10.1177_0013164411422252-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="2">Generation of Comparative Data</th>
<th align="center" colspan="2">Principal Components</th>
<th align="center" colspan="2">Principal Axis</th>
</tr>
<tr>
<th align="left">Method</th>
<th align="center">Cutoff</th>
<th align="center">Factors With λ ≥ 1: Not Included in Rule</th>
<th align="center">Factors With λ ≥ 1: Included in Rule</th>
<th align="center">Factors With λ &gt; 0: Not Included in Rule</th>
<th align="center">Factors With λ &gt; 0: Included in Rule</th>
</tr>
</thead>
<tbody>
<tr>
<td>Traditional</td>
<td>λ<sub>M</sub></td>
<td>TC:λ<sub>M</sub></td>
<td>TC:λ<sub>M</sub>λ<sub>≥1</sub></td>
<td>TA:λ<sub>M</sub></td>
<td>TA:λ<sub>M</sub>λ<sub>&gt;0</sub></td>
</tr>
<tr>
<td/>
<td>λ<sub>95</sub></td>
<td>TC:λ<sub>95</sub></td>
<td>TC:λ<sub>95</sub>λ<sub>≥1</sub></td>
<td>TA:λ<sub>95</sub></td>
<td>TA:λ<sub>95</sub>λ<sub>&gt;0</sub></td>
</tr>
<tr>
<td>Revised</td>
<td>λ<sub>M</sub></td>
<td>RC:λ<sub>M</sub></td>
<td>RC:λ<sub>M</sub>λ<sub>≥1</sub></td>
<td>—</td>
<td>RA:λ<sub>M</sub>λ<sub>&gt;0</sub></td>
</tr>
<tr>
<td/>
<td>λ<sub>95</sub></td>
<td>RC:λ<sub>95</sub></td>
<td>RC:λ<sub>95</sub>λ<sub>≥1</sub></td>
<td>—</td>
<td>RA:λ<sub>95</sub>λ<sub>&gt;0</sub></td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section10-0013164411422252" sec-type="methods">
<title>Method</title>
<p>In this section, we describe the variables manipulated in the generation of the data, the method used to generate the sample data and the data for the parallel analyses, and the analyses to assess the number of factors.</p>
<sec id="section11-0013164411422252">
<title>Data Generation Design</title>
<p>The data generation design included 90 conditions as a function of five manipulated variables:</p>
<list id="list1-0013164411422252" list-type="bullet">
<list-item><p><italic>Number of observations</italic> (<italic>N</italic><sub>O</sub>): The number of observations was set at 100, 300, or 500.</p></list-item>
<list-item><p><italic>Number of factors</italic> (<italic>N</italic><sub>F</sub>): Data were generated based on factor models with 0, 1, 2, or 3 factors.</p></list-item>
<list-item><p><italic>Number of variables</italic> (<italic>N</italic><sub>V</sub>): For data generated based on 0 factors or 1 factor, the number of variables was either 6 or 12. For data generated based on a model with 2 or 3 underlying factors, any variable had a nonzero loading on one and only one factor, and the number of variables with nonzero loadings was either 3 or 6 per factor. Accordingly, <italic>N</italic><sub>V</sub> was 6 or 12 for 0, 1, or 2 factor models and 9 or 18 for 3 factor models.</p></list-item>
<list-item><p><italic>Factor loadings</italic>: The nonzero loadings for data generated with 1 or more factors were all .4 or all .7.</p></list-item>
<list-item><p><italic>Factor correlations</italic>: The factor correlations for data generated with 2 or 3 factors were 0, .5, or .8.</p></list-item>
</list>
</sec>
<sec id="section12-0013164411422252">
<title>Data Generation</title>
<p>One thousand data sets representing the sample data were generated for each condition based on <xref ref-type="disp-formula" rid="disp-formula1-0013164411422252">Equation (1)</xref>. For data sets with uncorrelated factors, the scores for each factor in <italic>F</italic> were generated using RANNOR, an SAS normal random number generator, with a mean of zero and a unit variance. For data sets with correlated factors, the scores for factors were generated based on a higher-order factor structure, specified to create the desired correlation among the factors of interest. The scores for these higher-order factors and the disturbances to create the factors of interest were generated using RANNOR. Finally, the error scores in <bold>E</bold> were generated using RANNOR, the errors for the measured variables were specified to be uncorrelated with each other and the factor scores, and the variances of the errors were set such that the variances of the measured variables in <bold>X</bold> were unities.</p>
<p>For traditional parallel analysis, 100 data sets were generated for each of the 1,000 sample data sets. These 100 data sets had the same number of variables and observations as the sample data and were generated using RANNOR. The variables were specified to be normally distributed, with means of zero, unit variances, and zero correlations with each other. For revised parallel analysis, 100 data sets were generated to assess each sequential eigenvalue associated with a sample data set. The generation of the data sets for revised parallel analysis followed the procedures previously described in the section titled “The Revised Parallel Analysis Method.” RANNOR was used to generate the normally distributed scores for the comparison data sets.</p>
</sec>
<sec id="section13-0013164411422252">
<title>Data Analysis</title>
<p>Each of the 1,000 sample data sets and their accompanying comparative data sets were analyzed using the 14 methods shown in <xref ref-type="table" rid="table1-0013164411422252">Table 1</xref>. When principal axis factoring was applied in conducting parallel analyses, the communalities were sample multiple <italic>R</italic><sup>2</sup>s between the variables and all remaining variables in the data set. To abbreviate the otherwise unwieldy names of the methods, the approach to parallel analysis is abbreviated as T (traditional) or R (revised) and the extraction method is abbreviated as C (component analysis) or A (axis factoring). Subscripts following λ indicate the rule used. For example, revised parallel analysis using principal component analysis and implementing the 95th percentile rule with eigenvalues greater than or equal to 1 is abbreviated as RC:λ<sub>95</sub>λ<sub>≥1</sub>.</p>
<p>To assess the quality of a parallel analysis method for a condition, we computed the percentage of replications with the correct number of identified factors. We also assessed whether there was a tendency for a method to assess too few factors (underfactoring) or too many factors (overfactoring). Arbitrarily, we noted a tendency to underfactor if the percentage of replications yielding too few factors was 5% or more, and this percentage was at least 5% greater than the percentage of replications yielding too many factors. Similarly, we indicated a tendency to overfactor if the percentage of replications yielding too many factors was 5% or more, and this percentage was at least 5% greater than the percentage of replications yielding too few factors.</p>
<p>We also evaluated a method’s overall quality by appraising whether its accuracy was well behaved across conditions. More specifically, a method should demonstrate greater accuracy as sample size increases. In addition, accuracy should increase as factors are better defined, that is, as the magnitude of factor loadings increases and as the number of items assessing a factor increases. Finally, a method should show some decrement in accuracy as factors become highly correlated in that they become less distinct.</p>
<p>It should be noted that, ideally, accuracy for methods using the λ<sub>95</sub> rule should approach but not exceed 95%. For data with <italic>k</italic> factors, these methods could potentially be 100% accurate in identifying the presence of the 1st through <italic>k</italic>th factor, but should be at most 95% accurate in identifying the absence of the <italic>k</italic>th + 1 factor.</p>
</sec>
</sec>
<sec id="section14-0013164411422252" sec-type="results">
<title>Results</title>
<p>We initially consider whether the inclusion of an absolute eigenvalue cutoff improves the accuracy of parallel analyses. We then compare revised and traditional parallel analysis methods.</p>
<sec id="section15-0013164411422252">
<title>Assessment of Inclusion of Absolute Eigenvalue Cutoffs in Parallel Analyses</title>
<p>We compared parallel analysis methods with and without the addition of absolute eigenvalue cutoffs of 0 for principal axis factoring (λ &gt; 0 cutoff) and greater than or equal to 1 for principal component analysis (denoted λ ≥ 1 cutoff). Across all conditions, we found no differences in accuracies between the traditional methods with or without absolute eigenvalue cutoffs, regardless of whether principal component analysis or principal axis factoring in combination with λ<sub>M</sub> rule or λ<sub>95</sub> rule was applied; accordingly, we present in tables only those results for traditional methods with the appropriate absolute eigenvalue cutoff.</p>
<p>In contrast, inclusion of the λ ≥ 1 cutoff tended to affect the accuracy of revised parallel analysis using principal component analysis, regardless of whether λ<sub>M</sub> rule or λ<sub>95</sub> rule was applied. In <xref ref-type="table" rid="table2-0013164411422252">Tables 2</xref> and <xref ref-type="table" rid="table3-0013164411422252">3</xref>, we present results for a subset of the conditions to illustrate these effects. Inclusion of the λ ≥ 1 cutoff had no effect with zero underlying factors and a neutral or positive effect with one or more uncorrelated factors. In some conditions, these effects were very strong, with percentages increasing from 0 without the λ ≥ 1 cutoff to 100 with the cutoff. With 2 or 3 moderately correlated (i.e., .5) underlying factors, similar results were found, with only three minor exceptions. In contrast, for conditions with highly correlated factors (i.e., .8), inclusion of the λ ≥ 1 cutoff frequently produced negative effects. In some of these conditions, accuracy decreased with increases in sample size when the λ ≥ 1 absolute cutoff was applied.</p>
<table-wrap id="table2-0013164411422252" position="float">
<label>Table 2.</label>
<caption>
<p>Accuracy Percentages for Revised Parallel Analysis Based on Principal Components for Conditions With Zero or One Underlying Factor</p>
</caption>
<graphic alternate-form-of="table2-0013164411422252" xlink:href="10.1177_0013164411422252-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="4">6 Variables</th>
<th align="center" colspan="4">12 Variables</th>
</tr>
<tr>
<th/>
<th/>
<th align="center" colspan="2">95th Percentile Eigenvalue</th>
<th align="center" colspan="2">Mean Eigenvalue Rule</th>
<th align="center" colspan="2">95th Percentile Eigenvalue</th>
<th align="center" colspan="2">Mean Eigenvalue Rule</th>
</tr>
<tr>
<th align="left">Sample Size</th>
<th align="center">Factor Loadings</th>
<th align="center">RC:λ<sub>95</sub></th>
<th align="center">RC:λ<sub>95</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>M</sub></th>
<th align="center">RC:λ<sub>M</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>95</sub></th>
<th align="center">RC:λ<sub>95</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>M</sub></th>
<th align="center">RC:λ<sub>M</sub>λ<sub>≥1</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="10">Zero underlying factors</td>
</tr>
<tr>
<td> 100</td>
<td>—</td>
<td>94.6<sup>O</sup></td>
<td>94.6<sup>O</sup></td>
<td>50.8<sup>O</sup></td>
<td>50.8<sup>O</sup></td>
<td>93.4<sup>O</sup></td>
<td>93.4<sup>O</sup></td>
<td>52.2<sup>O</sup></td>
<td>52.2<sup>O</sup></td>
</tr>
<tr>
<td> 300</td>
<td>—</td>
<td>95.1<sup>O</sup></td>
<td>95.1</td>
<td>55.5<sup>O</sup></td>
<td>55.5<sup>O</sup></td>
<td>94.7<sup>O</sup></td>
<td>94.7<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
</tr>
<tr>
<td> 500</td>
<td>—</td>
<td>94.2<sup>O</sup></td>
<td>94.2<sup>O</sup></td>
<td>55.2<sup>O</sup></td>
<td>55.2<sup>O</sup></td>
<td>94.9<sup>O</sup></td>
<td>94.9<sup>O</sup></td>
<td>54.8<sup>O</sup></td>
<td>54.8<sup>O</sup></td>
</tr>
<tr>
<td colspan="10">One underlying factor</td>
</tr>
<tr>
<td> 100</td>
<td>.4</td>
<td>58.6<sup>O</sup></td>
<td>58.6<sup>O</sup></td>
<td>2.7<sup>O</sup></td>
<td>13.2<sup>O</sup></td>
<td>81.4<sup>O</sup></td>
<td>81.4<sup>O</sup></td>
<td>16.1<sup>O</sup></td>
<td>16.1<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>78.8<sup>O</sup></td>
<td>100.0</td>
<td>1.3<sup>O</sup></td>
<td>100.0</td>
<td>95.4</td>
<td>99.1</td>
<td>20.5<sup>O</sup></td>
<td>98.6</td>
</tr>
<tr>
<td> 300</td>
<td>.4</td>
<td>7.8<sup>O</sup></td>
<td>67.3<sup>O</sup></td>
<td>0.0<sup>O</sup></td>
<td>66.8<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
<td>3.1<sup>O</sup></td>
<td>3.3<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>14.1<sup>O</sup></td>
<td>100.0</td>
<td>0.0<sup>O</sup></td>
<td>100.0</td>
<td>80.4<sup>O</sup></td>
<td>100.0</td>
<td>2.8<sup>O</sup></td>
<td>100.0</td>
</tr>
<tr>
<td> 500</td>
<td>.4</td>
<td>0.0<sup>O</sup></td>
<td>90.7<sup>O</sup></td>
<td>0.0<sup>O</sup></td>
<td>90.7<sup>O</sup></td>
<td>31.5<sup>O</sup></td>
<td>31.5<sup>O</sup></td>
<td>.3<sup>O</sup></td>
<td>5.3<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>0.2<sup>O</sup></td>
<td>100.0</td>
<td>0.0<sup>O</sup></td>
<td>100.0</td>
<td>62.2<sup>O</sup></td>
<td>100.0</td>
<td>.4<sup>O</sup></td>
<td>100.0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0013164411422252">
<p><italic>Note</italic>. Superscript O denotes the percentage of replications yielding too many factors was 5% or more, and this percentage was at least 5% more than the percentage yielding too few factors.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table3-0013164411422252" position="float">
<label>Table 3.</label>
<caption>
<p>Accuracy Percentages for Revised Parallel Analysis Based on Principal Components for Conditions With Two and Three Underlying Factors</p>
</caption>
<graphic alternate-form-of="table3-0013164411422252" xlink:href="10.1177_0013164411422252-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="4">3 Variables Per Factor</th>
<th align="center" colspan="4">6 Variables Per Factor</th>
</tr>
<tr>
<th/>
<th/>
<th align="center" colspan="2">95th Percentile Eigenvalue</th>
<th align="center" colspan="2">Mean Eigenvalue Rule</th>
<th align="center" colspan="2">95th Percentile Eigenvalue</th>
<th align="center" colspan="2">Mean Eigenvalue Rule</th>
</tr>
<tr>
<th align="left">Sample Size</th>
<th align="center">Factor Loadings</th>
<th align="center">RC:λ<sub>95</sub></th>
<th align="center">RC:λ<sub>95</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>M</sub></th>
<th align="center">RC:λ<sub>M</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>95</sub></th>
<th align="center">RC:λ<sub>95</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>M</sub></th>
<th align="center">RC:λ<sub>M</sub>λ<sub>≥1</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="10">Correlation of 0 between two factors</td>
</tr>
<tr>
<td> 100</td>
<td>.4</td>
<td>24.2<sup>U</sup></td>
<td>30.5<sup>U</sup></td>
<td>0.2<sup>O</sup></td>
<td>42.9<sup>O</sup></td>
<td>71.9<sup>O</sup></td>
<td>71.9<sup>O</sup></td>
<td>12.0<sup>O</sup></td>
<td>12.0<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>99.8</td>
<td>100.0</td>
<td>95.7</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>99.6</td>
<td>99.8</td>
</tr>
<tr>
<td> 300</td>
<td>.4</td>
<td>4.9<sup>O</sup></td>
<td>86.4<sup>O</sup></td>
<td>0.0<sup>O</sup></td>
<td>87.0<sup>O</sup></td>
<td>47.5<sup>O</sup></td>
<td>47.5<sup>O</sup></td>
<td>0.9<sup>O</sup></td>
<td>1.5<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td> 500</td>
<td>.4</td>
<td>0.5<sup>O</sup></td>
<td>98.1</td>
<td>0.0<sup>O</sup></td>
<td>98.1</td>
<td>28.8<sup>O</sup></td>
<td>30.0<sup>O</sup></td>
<td>0.2<sup>O</sup></td>
<td>15.9<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td colspan="10">Correlation of .8 between two factors</td>
</tr>
<tr>
<td> 100</td>
<td>.4</td>
<td>21.6<sup>U</sup></td>
<td>32.2<sup>U</sup></td>
<td>0.0<sup>O</sup></td>
<td>68.6<sup>O</sup></td>
<td>23.1<sup>U</sup></td>
<td>23.1<sup>U</sup></td>
<td>11.2<sup>O</sup></td>
<td>11.2<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>77.3<sup>U</sup></td>
<td>8.9<sup>U</sup></td>
<td>25.3<sup>O</sup></td>
<td>8.9<sup>U</sup></td>
<td>91.9<sup>U</sup></td>
<td>87.0<sup>U</sup></td>
<td>65.4<sup>O</sup></td>
<td>88.4<sup>U</sup></td>
</tr>
<tr>
<td> 300</td>
<td>.4</td>
<td>0.0<sup>O</sup></td>
<td>60.0<sup>U</sup></td>
<td>0.0<sup>O</sup></td>
<td>60.9<sup>U</sup></td>
<td>25.2<sup>O</sup></td>
<td>25.2<sup>O</sup></td>
<td>0.1<sup>O</sup></td>
<td>3.6<sup>U</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>89.6<sup>O</sup></td>
<td>0.2<sup>U</sup></td>
<td>11.9<sup>O</sup></td>
<td>0.2<sup>U</sup></td>
<td>99.7</td>
<td>91.9<sup>U</sup></td>
<td>67.5<sup>O</sup></td>
<td>91.9<sup>U</sup></td>
</tr>
<tr>
<td> 500</td>
<td>.4</td>
<td>0.0<sup>O</sup></td>
<td>33.9<sup>U</sup></td>
<td>0.0<sup>O</sup></td>
<td>33.9<sup>U</sup></td>
<td>9.5<sup>O</sup></td>
<td>24.7<sup>O</sup></td>
<td>0.0<sup>O</sup></td>
<td>25.9<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>82.5<sup>O</sup></td>
<td>0.0<sup>U</sup></td>
<td>6.4<sup>O</sup></td>
<td>0.0<sup>U</sup></td>
<td>99.6</td>
<td>94.6<sup>U</sup></td>
<td>71.0<sup>O</sup></td>
<td>94.6<sup>U</sup></td>
</tr>
<tr>
<td colspan="10">Correlation of 0 among 3 factors</td>
</tr>
<tr>
<td> 100</td>
<td>.4</td>
<td>16.1<sup>U</sup></td>
<td>16.1<sup>U</sup></td>
<td>1.3<sup>O</sup></td>
<td>18.8<sup>O</sup></td>
<td>59.2<sup>U</sup></td>
<td>59.2<sup>U</sup></td>
<td>14.3<sup>O</sup></td>
<td>14.3<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>100.0</td>
<td>99.4</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td> 300</td>
<td>.4</td>
<td>9.1<sup>O</sup></td>
<td>57.3<sup>O</sup></td>
<td>0.0<sup>O</sup></td>
<td>57.6<sup>O</sup></td>
<td>57.9<sup>O</sup></td>
<td>57.9<sup>O</sup></td>
<td>3.5<sup>O</sup></td>
<td>3.5<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td> 500</td>
<td>.4</td>
<td>1.0<sup>O</sup></td>
<td>85.4<sup>O</sup></td>
<td>0.0<sup>O</sup></td>
<td>85.4<sup>O</sup></td>
<td>40.3<sup>O</sup></td>
<td>40.3<sup>O</sup></td>
<td>0.8<sup>O</sup></td>
<td>0.8<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td colspan="10">Correlation of .8 among 3 factors</td>
</tr>
<tr>
<td> 100</td>
<td>.4</td>
<td>6.0<sup>U</sup></td>
<td>6.1<sup>U</sup></td>
<td>2.2<sup>O</sup></td>
<td>51.6<sup>O</sup></td>
<td>3.1<sup>U</sup></td>
<td>3.1<sup>U</sup></td>
<td>15.6<sup>O</sup></td>
<td>15.6<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>9.6<sup>U</sup></td>
<td>1.3<sup>U</sup></td>
<td>61.5<sup>O</sup></td>
<td>1.3<sup>U</sup></td>
<td>43.6<sup>U</sup></td>
<td>43.6<sup>U</sup></td>
<td>81.4</td>
<td>77.4<sup>U</sup></td>
</tr>
<tr>
<td> 300</td>
<td>.4</td>
<td>4.6<sup>O</sup></td>
<td>42.0<sup>U</sup></td>
<td>0.0<sup>O</sup></td>
<td>53.1<sup>U</sup></td>
<td>13.1<sup>U</sup></td>
<td>13.1<sup>U</sup></td>
<td>3.1<sup>O</sup></td>
<td>3.1<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>92.2<sup>U</sup></td>
<td>0.0<sup>U</sup></td>
<td>65.1<sup>O</sup></td>
<td>0.0<sup>U</sup></td>
<td>100.0</td>
<td>80.9<sup>U</sup></td>
<td>94.1<sup>O</sup></td>
<td>80.9<sup>U</sup></td>
</tr>
<tr>
<td> 500</td>
<td>.4</td>
<td>0.0<sup>O</sup></td>
<td>20.9<sup>U</sup></td>
<td>0.0<sup>O</sup></td>
<td>21.6<sup>U</sup></td>
<td>17.9</td>
<td>17.9</td>
<td>1.0<sup>O</sup></td>
<td>0.2<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>99.0</td>
<td>0.0<sup>U</sup></td>
<td>67.6<sup>O</sup></td>
<td>0.0<sup>U</sup></td>
<td>100.0</td>
<td>87.6<sup>U</sup></td>
<td>97.6</td>
<td>87.6<sup>U</sup></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn2-0013164411422252">
<p><italic>Note</italic>. Superscript U denotes the percentage of replications yielding too few factors was 5% or more, and this percentage was at least 5% more than the percentage yielding too many factors. Superscript O denotes the percentage of replications yielding too many factors was 5% or more, and this percentage was at least 5% more than the percentage yielding too few factors.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Overall, inclusion of the λ ≥ 1 absolute cutoff tended to improve performance of the revised parallel analysis method using principal component analysis. More specifically, inclusion of the λ ≥ 1 absolute cutoff produced no improvement for zero underlying factors, marked improvement for many conditions with one underlying factor, and improvement in many conditions with two or three underlying factors, except when factors were highly correlated.</p>
</sec>
<sec id="section16-0013164411422252">
<title>Comparison of Revised Versus Traditional Parallel Analysis Methods</title>
<p>We present the accuracy percentages for assessing the relative quality of the revised and traditional parallel analysis methods for zero and one underlying factor in <xref ref-type="table" rid="table4-0013164411422252">Table 4</xref> and for two and three underlying factors in <xref ref-type="fig" rid="fig2-0013164411422252">Figures 2</xref> and <xref ref-type="fig" rid="fig3-0013164411422252">3</xref>, respectively. In the table and figures, we included the absolute eigenvalue cutoff of 1 as part of the revised methods using principal component analysis because it tended to produce more accurate results, as discussed in the previous section. For simplicity, we labeled the accuracy percentages for the traditional methods as including the absolute eigenvalue cutoffs of 0 or 1, although the same accuracy percentages were found for traditional methods that did not include these cutoffs. We discuss results separately for conditions with 0, 1, and 2 or more underlying factors.</p>
<table-wrap id="table4-0013164411422252" position="float">
<label>Table 4.</label>
<caption>
<p>Accuracy Percentages for Traditional and Revised Parallel Analyses for Conditions With Zero or One Underlying Factor</p>
</caption>
<graphic alternate-form-of="table4-0013164411422252" xlink:href="10.1177_0013164411422252-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th/>
<th align="center" colspan="4">95th Percentile Eigenvalue Rule</th>
<th align="center" colspan="4">Mean Eigenvalue Rule</th>
</tr>
<tr>
<th/>
<th/>
<th align="center" colspan="2">Principal Axis</th>
<th align="center" colspan="2">Principal Components</th>
<th align="center" colspan="2">Principal Axis</th>
<th align="center" colspan="2">Principal Components</th>
</tr>
<tr>
<th align="left">Sample Size</th>
<th align="center">Factor Loadings</th>
<th align="center">TA:λ<sub>95</sub>λ<sub>&gt;0</sub></th>
<th align="center">RA:λ<sub>95</sub>λ<sub>&gt;0</sub></th>
<th align="center">TC:λ<sub>95</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>95</sub>λ<sub>≥1</sub></th>
<th align="center">TA:λ<sub>M</sub>λ<sub>&gt;0</sub></th>
<th align="center">RA:λ<sub>M</sub>λ<sub>&gt;0</sub></th>
<th align="center">TC:λ<sub>M</sub>λ<sub>≥1</sub></th>
<th align="center">RC:λ<sub>M</sub>λ<sub>≥1</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="10">Zero underlying factors—6 variables</td>
</tr>
<tr>
<td> 100</td>
<td>—</td>
<td>95.1</td>
<td>95.1</td>
<td>94.6<sup>O</sup></td>
<td>94.6<sup>O</sup></td>
<td>51.9<sup>O</sup></td>
<td>51.9<sup>O</sup></td>
<td>50.8<sup>O</sup></td>
<td>50.8<sup>O</sup></td>
</tr>
<tr>
<td> 300</td>
<td>—</td>
<td>95.1</td>
<td>95.1</td>
<td>95.1</td>
<td>95.1</td>
<td>55.5<sup>O</sup></td>
<td>55.5<sup>O</sup></td>
<td>55.5<sup>O</sup></td>
<td>55.5<sup>O</sup></td>
</tr>
<tr>
<td> 500</td>
<td>—</td>
<td>94.3<sup>O</sup></td>
<td>94.3<sup>O</sup></td>
<td>94.2<sup>O</sup></td>
<td>94.2<sup>O</sup></td>
<td>55.3<sup>O</sup></td>
<td>55.3<sup>O</sup></td>
<td>55.2<sup>O</sup></td>
<td>55.2<sup>O</sup></td>
</tr>
<tr>
<td colspan="10">Zero underlying factors—12 variables</td>
</tr>
<tr>
<td> 100</td>
<td>—</td>
<td>94.1</td>
<td>94.1</td>
<td>93.4</td>
<td>93.4<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
<td>52.2<sup>O</sup></td>
<td>52.2<sup>O</sup></td>
</tr>
<tr>
<td> 300</td>
<td>—</td>
<td>94.7</td>
<td>94.7</td>
<td>94.7</td>
<td>94.7<sup>O</sup></td>
<td>53.8<sup>O</sup></td>
<td>53.8<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
<td>53.6<sup>O</sup></td>
</tr>
<tr>
<td> 500</td>
<td>—</td>
<td>94.9</td>
<td>94.9</td>
<td>94.9</td>
<td>94.9<sup>O</sup></td>
<td>54.1<sup>O</sup></td>
<td>54.1<sup>O</sup></td>
<td>54.8<sup>O</sup></td>
<td>54.8<sup>O</sup></td>
</tr>
<tr>
<td colspan="10">One underlying factor—6 variables</td>
</tr>
<tr>
<td> 100</td>
<td>.4</td>
<td>92.9</td>
<td>94.1</td>
<td>96.7</td>
<td>58.6<sup>O</sup></td>
<td>63.2<sup>O</sup></td>
<td>57.7<sup>O</sup></td>
<td>86.3<sup>O</sup></td>
<td>13.2<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>96.2</td>
<td>100.0</td>
<td>100.0</td>
<td>98.2</td>
<td>59.5<sup>O</sup></td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td> 300</td>
<td>.4</td>
<td>98.2</td>
<td>97.0</td>
<td>99.8</td>
<td>67.3<sup>O</sup></td>
<td>87.5<sup>O</sup></td>
<td>63.7<sup>O</sup></td>
<td>99.2</td>
<td>66.8<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>96.0</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>59.8<sup>O</sup></td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td> 500</td>
<td>.4</td>
<td>99.9</td>
<td>96.8</td>
<td>100.0</td>
<td>90.7<sup>O</sup></td>
<td>94.2<sup>O</sup></td>
<td>64.3<sup>O</sup></td>
<td>100.0</td>
<td>90.7<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>96.2</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>81.8<sup>O</sup></td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td colspan="10">One underlying factor—12 variables</td>
</tr>
<tr>
<td> 100</td>
<td>.4</td>
<td>94.1<sup>O</sup></td>
<td>94.7<sup>O</sup></td>
<td>99.0</td>
<td>81.4<sup>O</sup></td>
<td>64.8<sup>O</sup></td>
<td>53.8<sup>O</sup></td>
<td>88.8<sup>O</sup></td>
<td>16.1<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>97.1</td>
<td>100.0</td>
<td>99.1</td>
<td>100.0</td>
<td>56.3<sup>O</sup></td>
<td>100.0</td>
<td>98.6</td>
</tr>
<tr>
<td> 300</td>
<td>.4</td>
<td>98.2</td>
<td>95.0<sup>O</sup></td>
<td>100.0</td>
<td>53.6<sup>O</sup></td>
<td>81.1<sup>O</sup></td>
<td>54.8<sup>O</sup></td>
<td>99.6</td>
<td>3.3<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>95.4</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>56.0<sup>O</sup></td>
<td>100.0</td>
<td>100.0</td>
</tr>
<tr>
<td> 500</td>
<td>.4</td>
<td>99.6</td>
<td>96.0</td>
<td>100.0</td>
<td>31.5<sup>O</sup></td>
<td>89.6<sup>O</sup></td>
<td>56.5<sup>O</sup></td>
<td>100.0</td>
<td>5.3<sup>O</sup></td>
</tr>
<tr>
<td/>
<td>.7</td>
<td>100.0</td>
<td>96.3</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>55.8<sup>O</sup></td>
<td>100.0</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0013164411422252">
<p><italic>Note</italic>. Superscript O denotes the percentage of replications yielding too many factors was 5% or more, and this percentage was at least 5% more than the percentage yielding too few factors.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<fig id="fig2-0013164411422252" position="float">
<label>Figure 2.</label>
<caption>
<p>Accuracy of four parallel analysis methods for conditions with two underlying factors. Note that the lines for TC:λ<sub>95</sub>λ<sub>≥1</sub> and TC:λ<sub>M</sub>λ<sub>≥1</sub> are superimposed on each other and evidence near zero accuracy for the next to the last graph in the third column.</p>
</caption>
<graphic xlink:href="10.1177_0013164411422252-fig2.tif"/>
</fig>
<fig id="fig3-0013164411422252" position="float">
<label>Figure 3.</label>
<caption>
<p>Accuracy of four parallel analysis methods for conditions with three underlying factors. Note that the lines for TC:λ<sub>95</sub>λ<sub>≥1</sub> and TC:λ<sub>M</sub>λ<sub>≥1</sub> are superimposed on each other and evidence near zero accuracy for the bottom two graphs in the third column</p>
</caption>
<graphic xlink:href="10.1177_0013164411422252-fig3.tif"/>
</fig>
<sec id="section17-0013164411422252">
<title>No underlying factors</title>
<p>For conditions with 0 underlying factors, as shown in <xref ref-type="table" rid="table4-0013164411422252">Table 4</xref>, the traditional and revised parallel analysis methods produced equivalent results. The major differences in accuracy across all methods occurred between methods that used the λ<sub>95</sub> rule (percentages between 93.4 and 95.1) and those that used the λ<sub>M</sub> rule (percentages between 50.8 and 55.5).</p>
</sec>
<sec id="section18-0013164411422252">
<title>One underlying factor</title>
<p>As presented in <xref ref-type="table" rid="table4-0013164411422252">Table 4</xref> for conditions with one underlying factor, traditional parallel analysis tended to yield similar or higher accuracy percentages in comparison with comparable revised methods. However, the differences between the revised and the traditional approaches were minimal for principal axis factoring with the 95th percentile eigenvalue rule. Overall, the accuracies for parallel analysis methods using the 95th percentile eigenvalue rule were equal to or greater than the accuracies for comparable methods using the mean eigenvalue rule. Of the methods using the 95th percentile eigenvalue rule, the revised method based on principal component analysis tended to perform poorer across conditions when factor loadings were .4. For conditions with one underlying factor and loadings of .7, accuracy always exceeded 95%.</p>
</sec>
<sec id="section19-0013164411422252">
<title>Two or three underlying factors</title>
<p>We present the accuracy percentages for parallel analysis methods to detect two and three underlying factors in <xref ref-type="fig" rid="fig2-0013164411422252">Figures 2</xref> and <xref ref-type="fig" rid="fig3-0013164411422252">3</xref>, respectively. Only four of the parallel analysis methods—TA:λ<sub>95</sub>λ<sub>&gt;0</sub>, RA:λ<sub>95</sub>λ<sub>&gt;0</sub>, TC:λ<sub>95</sub>λ<sub>≥1</sub>, and TC:λ<sub>M</sub>λ<sub>≥1</sub>—were included on the graphs to make them less cluttered and easier to interpret.</p>
<p>The accuracies for the four methods excluded from the graphs—RC:λ<sub>95</sub>λ<sub>≥1</sub>, TA:λ<sub>M</sub>λ<sub>&gt;0</sub>, RA:λ<sub>M</sub>λ<sub>&gt;0</sub>, and RC:λ<sub>M</sub>λ<sub>≥1</sub>—behaved poorly for data with two and three underlying factors. In particular, the accuracies for RC:λ<sub>95</sub>λ<sub>≥1</sub> and RC:λ<sub>M</sub>λ<sub>≥1</sub> often failed to increase with increases in sample size (holding other manipulated dimensions constant). The insensitivity to sample size occurred across a wide variety of conditions: 2 or 3 underlying factors; correlations among factors of 0, .5, or .8; and 3 or 6 variables per factor. In addition, these two methods frequently showed decreases in accuracy with increases in the number of variables per factor when loadings were .4, holding all other manipulated dimensions constant. Finally, the accuracies for RC:λ<sub>95</sub>λ<sub>≥1</sub> and RC:λ<sub>M</sub>λ<sub>≥1</sub> commonly increased in value as the correlation between factors increased from 0 to .5. The accuracies for the other two methods excluded from the graphs (TA:λ<sub>M</sub>λ<sub>&gt;0</sub> or RA:λ<sub>M</sub>λ<sub>&gt;0</sub>) also behaved badly. The accuracies for TA:λ<sub>M</sub>λ<sub>&gt;0</sub> decreased in value with increases in sample size when factors were correlated .8, the number of variables per factor was 3, and the factor loadings were .4. Multiple times the percentages for RA:λ<sub>M</sub>λ<sub>&gt;0</sub> remained relatively homogeneous or decreased in value with increases in sample size across conditions with all other manipulated factors held constant. These results were particularly difficult to understand in that they occurred only across conditions in which factors were relatively well defined, that is, those with 6 variables per factor and factor loadings of .7. Last, the accuracies for both TA:λ<sub>M</sub>λ<sub>&gt;0</sub> and RA:λ<sub>M</sub>λ<sub>&gt;0</sub> often decreased in value with increases in the number of variables per factor.</p>
<p>We now turn our attention to <xref ref-type="fig" rid="fig2-0013164411422252">Figures 2</xref> and <xref ref-type="fig" rid="fig3-0013164411422252">3</xref>. No single method shown in these figures yielded consistently more accurate results across all conditions. All four methods produced very accurate results with high factor loadings and uncorrelated factors (i.e., bottom two graphs on left side in <xref ref-type="fig" rid="fig2-0013164411422252">Figures 2</xref> and <xref ref-type="fig" rid="fig3-0013164411422252">3</xref>) and inaccurate results with low factor loadings and highly correlated factors (i.e., top two graphs on right side in <xref ref-type="fig" rid="fig2-0013164411422252">Figures 2</xref> and <xref ref-type="fig" rid="fig3-0013164411422252">3</xref>).</p>
<p>The parallel analysis methods using principal component analyses tended overall to perform more poorly. Although the accuracies for TC:λ<sub>95</sub>λ<sub>≥1</sub> were similar to those for the other three methods when factors were uncorrelated, they generally had the lowest values when factors were correlated .5 or .8. The accuracy percentages for TC:λ<sub>M</sub>λ<sub>≥1</sub> were highest among the four methods under some conditions (e.g., <italic>N</italic> = 100, 2 factors correlated .5 or .8, factor loadings of .4) and approached zero under others (e.g., 2 factors correlated .8, factor loadings of .7, and 3 variables per factor). In addition, as shown in the top two graphs in the last column of <xref ref-type="fig" rid="fig2-0013164411422252">Figure 2</xref>, the accuracies for TC:λ<sub>M</sub>λ<sub>≥1</sub> were badly behaved when factors were correlated .8 in that the accuracies for TC:λ<sub>M</sub>λ<sub>≥1</sub> decreased with increases in sample size.</p>
<p>The accuracies for TA:λ<sub>95</sub>λ<sub>&gt;0</sub> and RA:λ<sub>95</sub>λ<sub>&gt;0</sub> were generally well behaved. The only exceptions were for conditions with highly correlated factors, weak factor loadings, and three variables per factor. For these conditions, the accuracies for TA:λ<sub>95</sub>λ<sub>&gt;0</sub> were generally low and decreased in value with increases in sample size (from 8.5 to 3.4 and from 1.2 to .1 for 2 and 3 underlying factors, respectively). On the other hand, the accuracies for RA:λ<sub>95</sub>λ<sub>&gt;0</sub> for these conditions properly increased in values (from 5.7 to 10.2) with increases in sample size with 2 underlying factors, but were homogeneous and very low (between .3 and .6) with 3 underlying factors. Across conditions, RA:λ<sub>95</sub>λ<sub>&gt;0</sub> generally was approximately as accurate or more accurate than TA:λ<sub>95</sub>λ<sub>&gt;0</sub>.</p>
</sec>
</sec>
</sec>
<sec id="section20-0013164411422252" sec-type="discussion">
<title>Discussion</title>
<p>We could make the following recommendation based on the results: (a) any of the methods that use the 95th percentile rule are effective in assessing 0 underlying factors; (b) TC:λ<sub>95</sub>λ<sub>≥1</sub> is most effective in evaluating 1 underlying factor; (c) TC:λ<sub>M</sub>λ<sub>≥1</sub> works best with multiple uncorrelated factors with 3 variables per factor; (d) TC:λ<sub>95</sub>λ<sub>≥1</sub> is preferred with multiple uncorrelated factors with 6 variables per factor; (e) RC:λ<sub>M</sub>λ<sub>≥1</sub> yields the best results for moderately correlated factors with 3 variables per factor; (f) TC:λ<sub>M</sub>λ<sub>≥1</sub> produces the best results for moderately correlated factors with 6 variables per factor; (g) RC:λ<sub>M</sub>λ<sub>≥1</sub> had the highest accuracies for highly correlated factors with 3 variables per factor and low factor loadings; (h) RA:λ<sub>M</sub>λ<sub>&gt;0</sub> had the highest accuracies for highly correlated factors with 6 variables per factor and low factor loadings; (i) RA:λ<sub>95</sub>λ<sub>&gt;0</sub> tends to work best across conditions for highly correlated factors with high factor loadings if the sample size is 300 or greater; (j) RC:λ<sub>95</sub> is preferred for two highly correlated factors with high factor loadings if the sample size is 100; and (k) RC:λ<sub>M</sub> yields the best results for three highly correlated factors with high factor loadings if the sample size is 100.</p>
<p>At this point, readers are likely to be wondering what to make of recommendations of seven different methods dependent on conditions of a study. In practice, researchers who apply parallel analysis know the basic design of their study: the number of subjects and the number of variables in their study. However, they do not know the number of factors underlying their measures, the number of variables associated with each factor, the correlation among factors, and the magnitudes of the factor loadings. Accordingly, researchers need a parallel analysis method that is relatively accurate across different factor structures.</p>
<p>On the whole, the revised parallel analysis method based on principal axis factoring using the λ<sub>95</sub> rule (RA:λ<sub>95</sub>λ<sub>&gt;0</sub>) yielded relatively high accuracy percentages in comparison with other methods across various factor structures. With zero or one underlying factor, the accuracy percentages for this method were greater than 94%. For conditions with two or three factors, the accuracies for RA:λ<sub>95</sub>λ<sub>&gt;0</sub> were excellent (i.e., mean of 94%) when factor loadings were high and fair when factor loadings were low as long as samples were at least moderate in size (i.e., mean of 50% with <italic>N</italic> = 300 and mean of 61% with <italic>N</italic> = 500).</p>
<p>The finding that percentages for RA:λ<sub>95</sub>λ<sub>&gt;0</sub> tended to perform better than other parallel analysis methods with well-defined factor structures is consistent with the conceptual underpinning of these methods in two ways. First, the data were generated using a common factor model, and therefore parallel analysis methods using this model, such as principal axis factoring, should produce more orderly results than those based on a principal component model. Second, the revised method uses factor loadings from the observed data to generate the comparison data sets. These factor loadings are likely to yield more stable and accurate results for studies with large sample sizes and factor structures with high factor loadings and many variables per factor. Accordingly, RA:λ<sub>95</sub>λ<sub>&gt;0</sub> should be and was most accurate with a sample size of 500, factor loadings of .7, and 6 variables per factor (accuracies across these conditions of 98% or greater). From a practical viewpoint, the results suggest researchers are more likely to identify correctly a two- or three-factor model using RA:λ<sub>95</sub>λ<sub>&gt;0</sub> if they have well-designed measures and studies. In other words, the use of this method reinforces good research practices.</p>
<p>Our study also indicated that the inclusion of an absolute eigenvalue cutoff of 0 for principal axis factoring or 1 for principal component analysis had no effect on traditional parallel analysis and a positive effect on revised parallel analysis. Based on the results for the traditional parallel analysis methods, the eigenvalue-greater-than-one criterion does not produce fewer factors than traditional parallel analysis methods. This finding is consistent with previous research that suggests that the eigenvalue-greater-than-one criterion overfactors and performs poorly in comparison with parallel analysis methods (e.g., <xref ref-type="bibr" rid="bibr19-0013164411422252">Zwick &amp; Velicer, 1986</xref>).</p>
<p>In summary, the revised parallel analysis method using principal axis factoring and the 95th percentile rule offers promise based on the findings of our investigation. Future research should include additional conditions with more varied factor structures to ensure the generality of our conclusion.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="conflict">
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Buja</surname><given-names>A.</given-names></name>
<name><surname>Eyuboglu</surname><given-names>N.</given-names></name>
</person-group> (<year>1992</year>). <article-title>Remarks on parallel analysis</article-title>. <source>Multivariate Behavioral Research</source>, <volume>27</volume>, <fpage>509</fpage>-<lpage>540</lpage>.</citation>
</ref>
<ref id="bibr2-0013164411422252">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cattell</surname><given-names>R. B.</given-names></name>
</person-group> (<year>1978</year>). <source>The scientific use of factor analysis in behavioral and life sciences</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Plenum</publisher-name>.</citation>
</ref>
<ref id="bibr3-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Crawford</surname><given-names>A.</given-names></name>
<name><surname>Green</surname><given-names>S. B.</given-names></name>
<name><surname>Levy</surname><given-names>R.</given-names></name>
<name><surname>Lo</surname><given-names>W.-J.</given-names></name>
<name><surname>Scott</surname><given-names>L.</given-names></name>
<name><surname>Svetina</surname><given-names>D. S.</given-names></name>
<name><surname>Thompson</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Evaluation of parallel analysis methods for determining the number of factors</article-title>. <source>Educational and Psychological Measurement</source>, <volume>70</volume>, <fpage>885</fpage>-<lpage>901</lpage>.</citation>
</ref>
<ref id="bibr4-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fabrigar</surname><given-names>L. R.</given-names></name>
<name><surname>Wegener</surname><given-names>D. T.</given-names></name>
<name><surname>MacCallum</surname><given-names>R. C.</given-names></name>
<name><surname>Strahan</surname><given-names>E. J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Evaluating the use of exploratory factor analysis in psychological research</article-title>. <source>Psychological Methods</source>, <volume>4</volume>, <fpage>272</fpage>-<lpage>299</lpage>.</citation>
</ref>
<ref id="bibr5-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ford</surname><given-names>J. K.</given-names></name>
<name><surname>MacCallum</surname><given-names>R. C.</given-names></name>
<name><surname>Tait</surname><given-names>M.</given-names></name>
</person-group> (<year>1986</year>). <article-title>The applications of exploratory factor analysis in applied psychology: A critical review and analysis</article-title>. <source>Personnel Psychology</source>, <volume>39</volume>, <fpage>291</fpage>-<lpage>314</lpage>.</citation>
</ref>
<ref id="bibr6-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Glorfeld</surname><given-names>L. W.</given-names></name>
</person-group> (<year>1995</year>). <article-title>An improvement on Horn’s parallel analysis methodology for selecting the correct number of factors to retain</article-title>. <source>Educational and Psychological Measurement</source>, <volume>55</volume>, <fpage>377</fpage>-<lpage>393</lpage>.</citation>
</ref>
<ref id="bibr7-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Guttman</surname><given-names>L.</given-names></name>
</person-group> (<year>1954</year>). <article-title>Some necessary conditions for common factor analysis</article-title>. <source>Psychometrika</source>, <volume>19</volume>, <fpage>149</fpage>-<lpage>161</lpage>.</citation>
</ref>
<ref id="bibr8-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Harshman</surname><given-names>R. A.</given-names></name>
<name><surname>Reddon</surname><given-names>J. R.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Determining the number of factors by comparing real with random data: A serious flaw and some possible corrections</article-title>. <source>Proceedings of the Classification Society of North America at Philadelphia</source>, <fpage>14</fpage>-<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr9-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Horn</surname><given-names>J. L.</given-names></name>
</person-group> (<year>1965</year>). <article-title>A rationale and test for the number of factors in factor analysis</article-title>. <source>Psychometrika</source>, <volume>30</volume>, <fpage>179</fpage>-<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr10-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Humphreys</surname><given-names>L. G.</given-names></name>
<name><surname>Montanelli</surname><given-names>R. G.</given-names></name>
</person-group> (<year>1975</year>). <article-title>An investigation of the parallel analysis criterion for determining the number of common factors</article-title>. <source>Multivariate Behavioral Research</source>, <volume>10</volume>, <fpage>193</fpage>-<lpage>206</lpage>.</citation>
</ref>
<ref id="bibr11-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kaiser</surname><given-names>H. F.</given-names></name>
</person-group> (<year>1960</year>). <article-title>The application of electronic computers to factor analysis</article-title>. <source>Educational and Psychological Measurement</source>, <volume>20</volume>, <fpage>141</fpage>-<lpage>151</lpage>.</citation>
</ref>
<ref id="bibr12-0013164411422252">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mulaik</surname><given-names>S. A.</given-names></name>
</person-group> (<year>2010</year>). <source>Foundations of factor analysis</source> (<edition>2nd ed.</edition>). <publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>Chapman &amp; Hall/CRC</publisher-name>.</citation>
</ref>
<ref id="bibr13-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Preacher</surname><given-names>K. J.</given-names></name>
<name><surname>MacCallum</surname><given-names>R. C.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Repairing Tom Swift’s electric factor analysis machine</article-title>. <source>Understanding Statistics</source>, <volume>2</volume>, <fpage>13</fpage>-<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr14-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Snook</surname><given-names>S. C.</given-names></name>
<name><surname>Gorsuch</surname><given-names>R. L.</given-names></name>
</person-group> (<year>1989</year>). <article-title>Component analysis versus common factor analysis: A Monte Carlo study</article-title>. <source>Psychological Bulletin</source>, <volume>106</volume>, <fpage>148</fpage>-<lpage>154</lpage>.</citation>
</ref>
<ref id="bibr15-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Steger</surname><given-names>M. F.</given-names></name>
</person-group> (<year>2006</year>). <article-title>An illustration of issues in factor extraction and identification of dimensionality in psychological assessment data</article-title>. <source>Journal of Personality Assessment</source>, <volume>86</volume>, <fpage>263</fpage>-<lpage>272</lpage>.</citation>
</ref>
<ref id="bibr16-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Turner</surname><given-names>N. E.</given-names></name>
</person-group> (<year>1998</year>). <article-title>The effect of common variance and structure pattern on random data eigenvalues: Implications for the accuracy of parallel analysis</article-title>. <source>Educational and Psychological Measurement</source>, <volume>58</volume>, <fpage>541</fpage>-<lpage>568</lpage>.</citation>
</ref>
<ref id="bibr17-0013164411422252">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Velicer</surname><given-names>W. F.</given-names></name>
<name><surname>Eaton</surname><given-names>C. A.</given-names></name>
<name><surname>Fava</surname><given-names>J. L.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Construct explication through factor or component analysis: A review and evaluation of alternative procedures for determining the number of factors or components</article-title>. In <person-group person-group-type="editor">
<name><surname>Goffin</surname><given-names>R. D.</given-names></name>
<name><surname>Helmes</surname><given-names>E.</given-names></name>
</person-group> (Eds.), <source>Problems and solutions in human assessment: Honoring Douglas Jackson at seventy</source> (pp. <fpage>41</fpage>-<lpage>71</lpage>). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Kluwer</publisher-name>.</citation>
</ref>
<ref id="bibr18-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Widaman</surname><given-names>K. F.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Common factor analysis versus principal component analysis: Differential bias in representing model parameters?</article-title> <source>Multivariate Behavioral Research</source>, <volume>28</volume>, <fpage>263</fpage>-<lpage>311</lpage>.</citation>
</ref>
<ref id="bibr19-0013164411422252">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Zwick</surname><given-names>W. R.</given-names></name>
<name><surname>Velicer</surname><given-names>W. F.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Comparison of five rules for determining the number of components to retain</article-title>. <source>Psychological Bulletin</source>, <volume>99</volume>, <fpage>432</fpage>-<lpage>442</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>