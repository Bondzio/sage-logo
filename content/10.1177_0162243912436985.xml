<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">STH</journal-id>
<journal-id journal-id-type="hwp">spsth</journal-id>
<journal-title>Science, Technology, &amp; Human Values</journal-title>
<issn pub-type="ppub">0162-2439</issn>
<issn pub-type="epub">1552-8251</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0162243912436985</article-id>
<article-id pub-id-type="publisher-id">10.1177_0162243912436985</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Values Levers</article-title>
<subtitle>Building Ethics into Design</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Shilton</surname>
<given-names>Katie</given-names>
</name>
<xref ref-type="aff" rid="aff1-0162243912436985">1</xref>
<xref ref-type="corresp" rid="corresp1-0162243912436985"/>
</contrib>
<bio>
<title>Author Biography</title>
<p>
<bold>Katie Shilton</bold> is an assistant professor in the College of Information Studies at the University of Maryland, College Park, and a senior research fellow at the University of Maryland’s Information Policy &amp; Access Center. Her research explores ethics and policy for the design of information collections, systems and technologies. Katie received a BA from Oberlin College, a Master of Library and Information Science from UCLA, and a PhD in Information Studies from UCLA.</p>
</bio>
</contrib-group>
<aff id="aff1-0162243912436985">
<label>1</label>College of Information Studies, University of Maryland, College Park, MD, USA</aff>
<author-notes>
<corresp id="corresp1-0162243912436985">Katie Shilton, College of Information Studies, University of Maryland, 4105 Hornbake Bldg, South Wing, College Park, MD 20742, USA. Email: <email>kshilton@umd.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>38</volume>
<issue>3</issue>
<fpage>374</fpage>
<lpage>397</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>As information systems transform our world, computer scientists design affordances that influence the uses and impacts of these technological objects. This article describes how the practices of design affect the social values materialized in emerging technologies, and explores how design practices can encourage ethical reflection and action. The article presents an ethnography of a laboratory that engineered software for mobile phones to track users’ locations, habits, and behaviors. This technical work raised a number of ethical challenges, particularly around questions of data use and surveillance. The ethnography suggests that particular activities within laboratories can help engineers agree on social values as important to design. It characterizes these activities as values levers: practices that open new conversations about social values and encourage consensus around those values as design criteria. Laboratory leaders and advocates can enable and strengthen these levers to encourage ethical reflection and action as an explicit part of design practice.</p>
</abstract>
<kwd-group>
<kwd>values in design</kwd>
<kwd>ethics</kwd>
<kwd>critical technical practice</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0162243912436985">
<title>Introduction</title>
<p>The statement that “technologies have politics” has been a cornerstone of science and technology studies for decades (<xref ref-type="bibr" rid="bibr19-0162243912436985">Pinch and Bijker 1984</xref>; <xref ref-type="bibr" rid="bibr24-0162243912436985">Suchman et al. 1999</xref>; <xref ref-type="bibr" rid="bibr26-0162243912436985">Verbeek 2006</xref>; <xref ref-type="bibr" rid="bibr27-0162243912436985">Winner 1980</xref>). Investigations into the politics of information technologies, including the social values embedded in those technologies, have increasingly been the goal of cross-disciplinary research efforts in engineering ethics (van de <xref ref-type="bibr" rid="bibr25-0162243912436985">Poel and Verbeek 2006</xref>), values-sensitive design (<xref ref-type="bibr" rid="bibr9-0162243912436985">Friedman and Nissenbaum 1997</xref>), critical technical practice (<xref ref-type="bibr" rid="bibr1-0162243912436985">Agre 1997</xref>), reflective design (<xref ref-type="bibr" rid="bibr20-0162243912436985">Sengers et al. 2005</xref>), and values in design (<xref ref-type="bibr" rid="bibr13-0162243912436985">Knobel and Bowker 2011</xref>). These traditions posit that the values held by designers affect how information technologies are imagined; how systems handle data, create categories, and draw inferences; and what affordances are available for user interaction. All of these decisions affect the social consequences of emerging technologies.</p>
<p>This article investigates how, as <xref ref-type="bibr" rid="bibr26-0162243912436985">Verbeek (2006</xref>, 369) describes it, engineers “do ethics by other means.” It reports on a two-year ethnography of a computer science (CS) laboratory: the Center for Embedded Networked Sensing (CENS), a research center based at the University of California, Los Angeles (UCLA). The study drew upon interviews, document analysis, and participant observation to describe how values intersected with design work in this lab. CENS designers were engaged in collecting new kinds of data about people using an increasingly pervasive technology: the mobile phone. Widespread data collection using phones as sensors is referred to as <italic>participatory </italic>or<italic> urban sensing</italic> (<xref ref-type="bibr" rid="bibr2-0162243912436985">Burke et al. 2006</xref>; <xref ref-type="bibr" rid="bibr8-0162243912436985">Eisenman et al. 2006</xref>; <xref ref-type="bibr" rid="bibr16-0162243912436985">Miluzzo et al. 2007</xref>). Participatory sensing projects might ask users to send text messages recording data points (e.g., weight, exercise, mood, or food eaten) throughout the day, and provide users with visualizations to explore patterns in their data. Other applications use participants’ mobile phones to record their location and give feedback such as a daily carbon footprint or exposure to air pollution. To undertake this new kind of data collection, participatory sensing laboratories collect very granular and sometimes sensitive personal data, including location, health information, habits, behaviors, and routines. This study investigated how social values related to pervasive data collection, such as privacy, consent, equity and forgetting, were raised as subjects in design, agreed upon as design criteria, and transformed into concrete technological features. This process relied upon <italic>values levers</italic>: practices that pried open discussions about values in design and helped the team build consensus around social values as design criteria.</p>
</sec>
<sec id="section2-0162243912436985">
<title>Method: Between Ethnographer and Values Worker</title><p>Design is a situated activity, and the sociotechnical network undergirding design practice affects the values considered during design (<xref ref-type="bibr" rid="bibr15-0162243912436985">Mehalik and Gorman 2006</xref>). I observed this situated activity by working alongside engineers as they built software and systems. I joined the Center in 2007 as a graduate student focused on social issues in computing, with a particular interest in technologically mediated memory and forgetting. I was hired to focus on social values, and particularly concerns about privacy voiced by leaders and outside advisers. Because of my role as a CENS team member, I have walked a line between ethnographer of design and advocate for social values within the team. I have written elsewhere about the intricacies of balancing these roles (<xref ref-type="bibr" rid="bibr21-0162243912436985">Shilton 2010</xref>); here, I concentrate on how involvement in both advocacy and ethnography helped illuminate values reflection and decision making in design.</p><p>CENS has hundreds of participants engaged in diverse science and technology research activities. The subgroup of thirty researchers that I observed focused on participatory sensing. Design in this group was a largely virtual activity, with a strong focus on writing, running, and testing code. Decision making about this code was a group activity. As one CENS leader put it: “CENS is like a host of meetings.” Meetings provided opportunities for designers to compare notes and exchange ideas, motivated deadlines for development, and ensured time for laboratory leaders to advise projects. Meetings were the primary space in which the group reached consensus on design criteria and plans for implementation. Observing meetings provided the chance to witness decision making in process. And because systems for participatory sensing are a new design challenge with uncertain social consequences, discussion of social potential and consequences was a reoccurring and visible component of these meetings.</p><p>The laboratory was guided by four faculty leaders, referred to by title: the Director, the Co-Principal Investigator, the Statistics Lead, and the Area Lead.<sup><xref ref-type="fn" rid="fn1-0162243912436985">1</xref></sup> In addition, eight professional staff, two postdoctoral scholars, eleven graduate students, and five undergraduate students (identified with a fictitious first initial throughout this account) worked on participatory sensing projects.</p><p>I spent the first year at CENS working on conceptual investigations to define social values important to participatory sensing. But because my early focus was on advocacy, it took time to figure out what I should be looking for <italic>outside </italic>of my own role. An initial year of interviews and observations, and a shift away from advocacy and toward design, suggested promising objects of study: expressions of ideology, the justifications for those expressions, and the practices that triggered those expressions. To find these ideologies, justifications, and practices, I interviewed all thirty participatory sensing team members, and took over 200 hours of field notes over two years of participant-observation.</p><sec id="section3-0162243912436985"><title>Values and Values Tensions in CENS Design</title><p>From the time I arrived at CENS, lab members expressed concern about the relationship between the data collection devices under construction and the specter of <italic>surveillance</italic>. “Surveillance” is a loaded term, associated with government power and repression, historical police states, and pernicious actors. CENS designers wanted to build tools for data collection, but did not want to build <italic>surveillance</italic> tools. The focus on—or fear of—surveillance led designers to consider a number of specific values to avoid or counteract surveillance. These included discussing values such as privacy, consent and participation, equity, and forgetting.</p><p>Privacy discussions focused on the sharing or withholding of personal information. Privacy came up repeatedly in publications, meetings, and interviews. Students on the project described turning off sensors to hide their location, or feeling pressure to be socially active when participating in sensing research. Consent was another value that surfaced frequently in CENS conversations. Informed consent is a long-standing ethic in human subjects research, but was a new concern to many CENS students, as human subjects research ethics are generally not taught within the discipline of CS (<xref ref-type="bibr" rid="bibr12-0162243912436985">Hollander 2009</xref>). Establishing meaningful informed consent was challenging when collecting granular behavioral data using pervasive devices. Consent was further complicated by a trend toward “opportunistic,” automatic sensing pursued by peer research laboratories (<xref ref-type="bibr" rid="bibr6-0162243912436985">Christin et al. 2011</xref>; <xref ref-type="bibr" rid="bibr3-0162243912436985">Campbell et al. 2006</xref>).</p><p>Antisurveillance concerns also generated conversations about fairness and equity. The team debated who should participate in sensing, and who benefited from sensing. Laboratory leaders expressed hope that sensing itself could level social playing fields and advance social equality by making data more accessible and useable to individuals and community groups. Inspired by citizen science, photovoice, and participatory research (<xref ref-type="bibr" rid="bibr5-0162243912436985">Catalani and Minkler 2010</xref>; <xref ref-type="bibr" rid="bibr4-0162243912436985">Cargo and Mercer 2008</xref>), CENS leaders discussed the possibility that communities could use participatory sensing data to “make a case” for problems or needs (<xref ref-type="bibr" rid="bibr10-0162243912436985">Goldman et al. 2009</xref>). CENS discussions about surveillance also incorporated specters of persistent memory and its opposite, forgetting. These discussions included consideration of how best to collect the minimum amount of data needed for a given project and how to build workable data retention policies.</p><p>Though CENS designers discussed and valued privacy, consent, equity, and forgetting, they also faced significant competing values that sometimes outweighed these antisurveillance principles. Students eager for success as academics faced stringent deadlines and pressure to publish new ideas quickly. The constant pressures of technical innovation made a slower, values-oriented design process unattractive. Designers rationalized their decisions to prioritize expediency over values like privacy and equity in several ways. Some declared that issues such as privacy or abuses of power were not truly pressing or dangerous. Others saw social values as important, but outside of their expertise. As undergraduate F. put it: “[Privacy] really should be in the philosophy department.” Others students indicated that such values simply were not a design priority; a functional system was most important. Features needed to collect personal data (such as capture and upload mechanisms) were implemented much more quickly than those needed to limit collection, restrict sharing, or delete data. Finally, antisurveillance values like forgetting sometimes competed directly with innovation values like new knowledge discovery, or systems values like efficiency and practicality.</p></sec><sec id="section4-0162243912436985"><title>Values Levers in Design</title><p>As I watched concern for privacy, consent, equity and forgetting surface and conflict with other values at CENS, I tried to untangle the practices and activities that encouraged these values to manifest. Sociologists and anthropologists have long been interested in the factors that influence a community or working group’s values or ethics. To understand the relationship between a social setting and ethics, <xref ref-type="bibr" rid="bibr7-0162243912436985">Collier and Lakoff (2005</xref>) describe what they call “regimes of living”: cohesive ethical orientations providing a shared sense of how things should be done. Specifically, regimes of living connect ethics directly to practice:<disp-quote><p>Here the term “ethics” refers not to the adjudication of values but, as Bernard Williams puts it, to the question “How should one live?” Ethical problems, in this sense, involve a certain idea of practice (“how”), a notion of the subject of ethical reflection (“one”), and questions of norms or values (“should”) related to a certain form of life in a given domain of living. This engagement with philosophical discussions helps to frame ethical questions in terms of techniques, practices, and rationality (<xref ref-type="bibr" rid="bibr7-0162243912436985">Collier and Lakoff 2005, 22</xref>).</p></disp-quote></p><p>I began to look for practices that encouraged particular regimes of living—sets of ethical orientations—at CENS. The first activities I focused on were data management practices. Because I believed that many of the surveillance challenges engendered by participatory sensing technologies resided in the personal nature of the data and its associated inferences, I asked students: what are the personal data in your project? And who is in charge of the personal data in your project? But these questions led to puzzled responses. Many students replied like graduate student C.: “Who’s responsible for the data? Might I ask why this is important?”</p><p>A discussion with postdoctoral scholar M. helped clarify the discrepancy between my assumption that collecting, organizing, and protecting personal data were prime responsibilities of system designers, and C.’s implication that “responsibility” for data was unimportant. These questions about personal data were badly framed for CS students. During early-stage design, designers’ intellectual energy is spent on the algorithms and storage that make up systems, rather than the data flowing through the systems. As the Director explained to me: “Understand that there would <italic>be</italic> no data without first having a system.”</p><p>Once I understood the disconnect between the objects of CS research focus (systems) and the objects of surveillance concerns (data), I started to look for practices and people that served to increase designers’ focus on, or awareness of, personal data. Sensing data was largely forgotten until someone, or something, created a reminder. Then, the personal data became a fulcrum for the translation from project work to values thinking. In this way, data functioned as infrastructure at CENS (<xref ref-type="bibr" rid="bibr22-0162243912436985">Star 2010</xref>; <xref ref-type="bibr" rid="bibr23-0162243912436985">Star and Griesemer 1989</xref>). Unearthing this data infrastructure was critical to values considerations. Practices that called attention to that data infrastructure functioned like levers: tools that opened up new discussions and concerns. I began to label these existing practices <italic>values levers</italic>: activities that pried open conversations about antisurveillance values in the midst of the rush toward software products and publications.</p></sec><sec id="section5-0162243912436985"><title>Experiencing Internal Prototype Testing</title><p>Focusing on practices that unearthed personal data revealed a frequent values lever: students reported discovering privacy, consent and equity concerns while testing their applications and those of their colleagues. At CENS, as in many development labs, it was common practice to prototype new systems internally before they were tested with users. Prototype testing of participatory sensing technologies encouraged a focus on personal data, and particularly the inferences that could be drawn with such data. Some examples were straightforward. After participating in a health and wellness data collection pilot, graduate student G. e-mailed the team: “Just browsing the survey questions, I now understand how critical privacy is for such an application.” The kinds of data under request (including location as well as questions about eating, sleeping, and exercise habits) allowed the student to imagine what inferences might be made about his behavior and made the surveillance concerns concrete in a way that his previous design work had not. Graduate student L. provided another example when she shared her recent location traces in a slide presentation to the team. When the map of her global positioning system (GPS) traces appeared, the erratic path revealed that L. had been saving time during a rush hour commute by cutting through parking lots and gas stations. L. was visibly embarrassed, and immediately pulled her data off the screen. Other students teased her about her driving habits for weeks.</p><p>Evidence of the link between internal prototype testing and values concerns pervaded my interviews, as well. Staff member A. described her reaction to seeing her location data:<disp-quote><p>It was a big shock showing my data on GPS. I was totally surprised at the effect of that. … the first time I had to show this map to someone and realized it identified where I lived and where I went to work, I had a moment of, Gasp! That’s the best way I can describe it. I had something that actually tightened up inside of me.</p></disp-quote></p><p>Experiencing internal testing helped designers focus not only on the personal data collected, but the possible inferences that could be drawn from that data. Exploring inferences also led to sensitivity toward values such as equity and power. As L. related in an interview:<disp-quote><p>I think I feel safe with the data we collected, because we already knew [that my colleagues] were not really interested in my location traces. … But if this campaign was something owned by a company, I would feel kind of scared. Because I would not know where [the data] is, actually, and how it’s used.</p></disp-quote></p><p>Experiencing data collection helped the graduate student identify not only that personal data could contribute to sensitive inferences, but that the sensitivity would depend upon power: who collected the data and what they chose to do with that data.</p><p>The prototype testing experiences of students and staff contributed to a group consensus around privacy as a design principle for CENS systems. The team operationalized this design principle in several systems. They built passwords and sharing options into health monitoring software (<xref ref-type="bibr" rid="bibr11-0162243912436985">Hicks et al. 2011</xref>), and experimented with sharing location data at different granularities, substituting neighborhood or zipcode level data for precise latitude and longitude (<xref ref-type="bibr" rid="bibr17-0162243912436985">Mun et al. 2009</xref>). The most prominent manifestation of privacy concerns was a project to develop the Personal Data Vault (PDV). The PDV was protected cloud storage for sensing data controlled by the individual data collector (<xref ref-type="bibr" rid="bibr18-0162243912436985">Mun et al. 2010</xref>). The PDV was intended to increase individuals’ privacy by giving users the ability to collect, aggregate, and interpret their data before sharing with third parties.</p><p>There is nothing novel about the finding that internal prototype testing is important to good design. However, the effects of such testing on designers’ consideration of social values have gone unexplored. Internal testing fostered a focus on personal data that was distinctive within the design process. When CENS students ran their colleagues’ location-tracking programs over the weekend, or answered sensitive survey questions, they gained new respect for privacy and equity as design criteria. A practice meant to check new products for usability and bugs encouraged researchers to reflect on the sensitivity of the personal data in their systems, creating the unanticipated result of making values personal.</p></sec><sec id="section6-0162243912436985"><title>Working on Interdisciplinary Teams</title><p>Working on interdisciplinary teams was another design practice that encouraged a focus on personal data and enabled discussions of privacy, consent, equity, and forgetting. The majority of CENS participatory-sensing designers had undergraduate degrees in CS or electrical engineering. However, a small but vocal number of the design team hailed from statistics, design/media arts, and information studies. Statisticians, for example, attended weekly meetings and were a regular part of design. Their needs were almost as primary to the design process as those of the computer scientists.</p><p>Statisticians’ comments and interests during design meetings frequently referred designers back to issues inherent in the data. An example from a spring 2009 design meeting demonstrated this tendency. The Statistics Lead recommended a statistical software tool to T., a CS graduate student. The features of the tool led to a discussion of data representation, and how T. wished to display the data his system gathered. The pair discussed the virtues of, and contrasts between, bubble plots, heat maps, and other map-based representations. Staff member B. chimed in: “We should present useful, but neutral, information, like time, location, who you’re with: only facts.” To which the Statistics Lead responded, “I have a concern that we’re encouraging people to chase noise.” Here, the Statistics Lead raised concerns about the nature of the project data: whether data like time and location, popular among computer scientists because they were easily documented via mobile phone, were in fact useful and meaningful.</p><p>This refocusing on project data was the (largely unintentional) deployment of a values lever. It allowed for not only statistical discussions but also ethical debate about data representation, sharing, and security. These debates helped students reach consensus about the importance of data legibility, a design principle that stemmed from concerns about equity. Legibility was necessary to help users understand and benefit from their data, and concerns about legibility were reflected in user interface design for health and environmental software applications, as well as interface design for the PDV. The unusually interdisciplinary nature of CENS design positioned the data collected by participatory sensing as a bridge between CS, statistics, design/media arts, and information studies. Being forced to talk across disciplinary boundaries helped the design team articulate and agree upon values like legibility and, more broadly, equity.</p></sec></sec><sec id="section7-0162243912436985"><title>Designing around Constraints</title><p>Values levers that refocused design on personal data were the first I identified at CENS, but they were not by any means the only levers. Practices that helped change the perception of a values-based design process, from slow and cumbersome to creative and fruitful, served as a values lever as well. Values were not only a constraint on design but also a creative set of potential conditions that could be met with new inventions. As the Statistics Lead put it in an interview:<disp-quote><p>Unconstrained design is often boring. If you take a very practical view of it, the privacy constraints pose really good challenges, right? … That’s a kind of sales pitch for the student who doesn’t buy the “Just be respectful or you’re going to feel queasy afterwards” and all that.</p></disp-quote></p><p>Like the Statistics Lead, the Director saw privacy as not just an important social value but also a potential space for creativity and innovation. As she said in an interview:<disp-quote><p>[Privacy] is a first-class design objective to the point that sometimes I feel a little opportunistic about it. In the sense that it is a source of design innovation and gives us something intellectual to work on the CS side. … It’s an interesting computer science problem.</p></disp-quote></p><p>Students at CENS made similar discoveries about the creativity that could arise from the constraints posed by antisurveillance values. As L., a graduate student who became the lead developer of the PDV, said in an interview:<disp-quote><p>At first I wasn’t really interested in [privacy] … I didn’t really know that privacy could mean that the user has control over the data. I feel it's exciting actually … it’s kind of a new concept for me, defining privacy by having control over the data.</p></disp-quote></p><p>When values were seen as a boon to creativity and new innovation, it formed a values lever, opening up space for values to become design criteria. Designing around values-based constraints helped make social values interesting and relevant to designers. The most visible manifestation of the impact of this lever was work on the PDV. Students working on the PDV investigated research challenges such as effective filters for the data, and innovative techniques for identifying, tracking, and auditing the data. Several conference papers and at least one dissertation resulted from work on the PDV.</p><sec id="section8-0162243912436985"><title>Internalizing Leader Advocacy</title><p>Because CENS was an academic research lab, faculty members held a large amount of power to decide which projects students pursued and what issues students faced during design, testing, and implementation. At the head of the lab was the Director, whose leadership was infused with her social ideals. She was explicitly aware, for example, that power and equity were at question in CENS design, saying:<disp-quote><p>I have an intellectual interest in building systems that support a wider range of people’s notions of privacy, because I absolutely believe in uneven power and people with stigmatized activities and behaviors and things like that, so I have an intellectual commitment to that.</p></disp-quote></p><p>The Director’s values were shared by the Area Lead, a professor with a humanities as well as a technical background. The Area Lead invoked his interdisciplinary background when setting the vision and ideology of the lab:<disp-quote><p>One [of the most important things I try to convey to students] is sort of a humanistic perspective, even for students that have a technical background… I'm interested in having a rigorous or at least attentive, having an attentiveness to the humanistic set of values that exist by calling this participatory sensing.</p></disp-quote></p><p>A third active leader in participatory sensing was the Statistics Lead. The Statistics Lead’s values incorporated power, equity, and also parsimony: the idea that as little personal data as possible should be collected to enable privacy and forgetting.<disp-quote><p>There was this impulse to just collect all the possible data. So to have [the students] realize that data are necessarily this incomplete record of reality, so no matter how much you try to collect, you’re not going to really get at the soul… So to stop [students] from viewing the world as an automaton that can go and collect data in one way or another.</p></disp-quote></p><p>Because the CENS leaders were openly concerned about antisurveillance values, I assumed their explicit advocacy would deploy powerful values levers. Students, however, were less reflective about how their mentors’ values affected their design practice. A small number (two interviewees) openly acknowledged this influence, but most denied it. CENS students were adamant about their ability to choose their own research projects and to decide the best ways to go about their work.</p><p>This stated independence did not seem completely honest. I observed advisers influence the direction of research projects using a combination of persuasiveness, authority, and control over funding. In one memorable exchange during a 2010 group meeting, graduate student T. justified his choice of a sensing project focused on campus recycling, rather than something more akin to surveillance:<disp-quote><p>Like if I had total liberty, I don’t know if I would be doing exactly the same collections. … For instance, would [the university] like to be associated with sustainability, or would they like to be associated with some random thing, like … monitoring weirdoes that exist on the [UCLA landmark] Bruin Walk that bother me a lot. What if I asked people to monitor that?</p></disp-quote></p><p>T. implied that it was pressure from the “university” that kept him from undertaking surveillance projects. He then listed funding agencies as another check on his behavior: surely the National Science Foundation (NSF) would never fund a surveillance project. At this point, the Director could not hold back her incredulity. She cut in: “I was talking about another form of pressure.”<disp-quote><p>T: I mean, like my own self-conscience?</p><p>Director: No. There’s something in between you and NSF.</p></disp-quote></p><p>The Director was indicating her own role in defining permissible projects for her advisee. She tried to get T. to admit to the values pressures she put on him. But T. would not take the bait. He continued to insist: “[The NSF] is the source of the pressure. You just transmit it.” The relationship between mentors and students at CENS complicated the role of leaders’ advocacy as a values lever in CENS design.</p><p>Leaders were able to deploy a stronger lever through internal procedures that routinized consideration of antisurveillance values. The leadership created a mandatory online form for designers planning new personal data collections. The form asked questions about the proposed projects, including population, types of data under request, and plans for storage and reuse. It asked what privacy or participation techniques would be used and asked researchers to articulate risks and benefits to participants. Finally, it required students to submit a consent form to leadership for approval. This form was a procedural values lever designed to help students think through values issues in their data collections and to help lab leadership keep an eye on data-collection practices. These forms, and the practices necessary to secure consent even for prototype testing, became the most visible manifestation of the value of consent in the lab. Leader-mandated informed consent ensured that students pursued participatory, rather than opportunistic or secret, sensing systems. The Director felt that establishing this procedural check was a success because “…the participatory sensing data collection form has become part of the way that CENS projects are approached.” Laboratory procedures, as well as direct advocacy by leaders, helped to normalize antisurveillance values at CENS.</p></sec><sec id="section9-0162243912436985"><title>Navigating Institutional Mandates</title><p>Values levers were often deployed by people close to design, including colleagues like statisticians, as well as leaders and mentors. But CENS designers were also influenced by agents farther from design, including administrators responsible for the responsible conduct of research at UCLA. The university imposed its own ethical mandates on CENS design, enforced through the oversight of the Institutional Review Board (IRB).</p><p>CENS leaders were proactive about approaching the IRB and informing it of sensing developments, and all three laboratory leaders expressed appreciation for the IRB as an external check on sensitive data collections. The IRB was, in turn, flexible and accommodating of CENS research. The IRB considered most CENS projects to be technical pilots or services, rather than human subjects research, because project data were seldom used to draw generalizable conclusions about human behavior. Only a handful of CENS projects qualified for review by the IRB, and most of those received an “Exempt” status from the Board.</p><p>But though it was an infrequent requirement, and though the IRB demonstrated real willingness to work with CENS engineers, students considered seeking IRB approval to be undesirable or even painful, because it required paperwork, could take quite a bit of time, and therefore slowed down the pace of testing and implementation. The focus on paperwork made IRB discussions into administrative tasks, rather than central to design decision making. The IRB served as a hurdle to be cleared, and students offloaded much of the required writing to a staff member hired to interface with the IRB. In this way, the IRB functioned very differently than other values levers, which brought values discussions into design meetings. Graduate student T. explained the outside nature of the IRB as follows:<disp-quote><p>I feel like actually, as a system designer, the burden shouldn’t fall on me to get IRB approval. Not to say that my system shouldn’t get IRB approval, but I feel like somebody else should handle that… Because I don’t know how to put this, but I am designing a system and I am really concentrating on designing the system and then this is like another process and it is a little bit outside of my… I do not do this on a regular basis. … So that’s what I found really frustrating and that’s why I always hesitated to want to work on it.</p></disp-quote></p><p>IRB requirements occasionally inspired students to reexamine the security of their data or their collection procedures, but more often, the engineers expressed frustration with the administrative overhead rather than appreciation for the questions raised. Graduate student T. continued:<disp-quote><p>The second [frustrating aspect] is that I feel like [the IRB staff] do not really understand… They never will understand the system and the things that they are concerned about just make me sad. They are concerned about things like these forms that we have to fill out afterwards and stuff like that… Like if you understood the system properly then I would have more respect for you, and then I will put more time to actually do the application. But now that I know that you do not know what you're talking about, then why put the effort into it.</p></disp-quote></p><p>This attitude was fairly prevalent, shared by most staff and student interview subjects. The IRB’s intervention at CENS served as a values lever, helping to normalize and routinize values in much the same way as laboratory procedures imposed by leaders. It’s unclear, however, how much impact this lever had on design. The combination of outsider status and perceived lack of understanding frustrated the IRB’s effectiveness. The values applied by the IRB were obscured by translational issues and lack of respect.</p></sec></sec><sec id="section10-0162243912436985"><title>Internalizing Team Member Advocacy</title><p>The IRB was an example of a values lever applied from outside of the design space. A very different sort of lever was my own intervention at CENS. I engaged in a number of activities that helped to deploy values levers. I raised issues of privacy, consent, equity, and forgetting in group meetings, where the large and often interdisciplinary groups meant systems were discussed at the relatively high level. I also worked one-on-one with students on the more specific, lower-level details of system implementation. I asked students to describe the planned data flow and brainstorm what values problems might arise. We would then work together to translate potential solutions into technical implementations. For example, I worked with graduate student L. on filtering mechanisms for the PDV, and graduate student T. on privacy-preserving data alterations to a sensing project for cyclists.</p><p>Students had a variety of reactions to my work in the lab. Because I was the observer and interviewer, negative evaluation of my work was not articulated. But students provided honest accounts of the ways that my presence offloaded the work of ethical decision making. As postdoc M. put it:<disp-quote><p>And I guess the one thing that I can say is: it’s also a relief that we don’t have to think about it. Meaning like we know that somebody who knows what they’re doing is thinking about this problem and so it’s nice, because normally when we have to think about the problems that we know nothing about we just hack solutions together. So the relief part comes from, it’s like, we are not going to hack to get the solution for this.</p></disp-quote></p><p>L., who was particularly interested in privacy research, said:<disp-quote><p>Especially as a computer scientist, to develop any system related to privacy is really hard, because it’s just too sensitive a topic. So having a social scientists itself is really helpful, because people can say “this is said by a social scientist,” so people cannot really complain about it [laughter]. … Our social scientist defines, you know, what kind of things does this system support. So I just need to build the system based on those. So it really makes it easy.</p></disp-quote></p><p>While my presence seemed to normalize the discussion of antisurveillance values, it also helped move responsibility for those design decisions away from engineers. Further longitudinal study may be necessary to determine if CENS designers engage antisurveillance values in future research without an explicit values advocate.</p><sec id="section11-0162243912436985"><title>Gaining Funding</title><p>Resources and funding also encouraged practices that fostered attention to values at CENS. Funding greatly affected the trajectory of projects, guaranteeing graduate students to work on a project, staff to take duties unsuited to graduate students, and resources such as phones and server space to devote to a project. Better-funded CENS projects had correspondingly large development teams. Large teams required formal weekly planning meetings and fairly clear lines of communication. Antisurveillance values tended to come up in these meetings, due to a variety of factors. CENS leaders were often in these meetings, as was I. In addition, the discussions fostered by a larger group of people tended to reveal social worries and opinions, which then become design concerns.</p><p>The design of larger systems contrasted to smaller projects, which had little or no initial funding and only two or three developers. Design meetings for these projects were informal and often spur of the moment. Leaders and team members communicated about these projects largely over e-mail. These less complex systems, which harbored less obviously sensitive personal data, were perceived to need less planning in advance. And fewer ethical concerns surfaced in the discussions of the small working teams.</p><p>Several CENS leaders addressed the tensions around resources, project maturity, and values discussion in our interviews. The Director described it this way: “I feel like we’re just unfortunately below critical mass to be generating more of those [values-based projects].” She proposed that the funding landscape needed to change to support larger projects with diverse, explicitly interdisciplinary project teams. Much as working on interdisciplinary teams helped designers articulate social values, working on larger teams with better resources advanced the articulation process.</p><fig id="fig1-0162243912436985" position="float"><label>Figure 1.</label><caption><p>An evocative border crossing documented by a CENS carbon footprint application (screenshot captured by CENS Director).</p></caption><graphic alternate-form-of="fig1-0162243912436985" xlink:href="10.1177_0162243912436985-fig1.tif"/></fig></sec></sec><sec id="section12-0162243912436985"><title>Characterizing Effective Values Levers</title><p><xref ref-type="fig" rid="fig3-0162243912436985">Figure 3</xref> illustrates the values levers at work during design at CENS. Each activity leveraged a particular experience to articulate antisurveillance values and build consensus and action around these values as design criteria.</p><fig id="fig2-0162243912436985" position="float"><label>Figure 2.</label><caption><p>A design meeting in progress (photo by author).</p></caption><graphic alternate-form-of="fig2-0162243912436985" xlink:href="10.1177_0162243912436985-fig2.tif"/></fig><fig id="fig3-0162243912436985" position="float"><label>Figure 3.</label><caption><p>Values levers in design.</p></caption><graphic alternate-form-of="fig3-0162243912436985" xlink:href="10.1177_0162243912436985-fig3.tif"/></fig><p>These levers helped to promote social values alongside market and innovation values, changing which values were incorporated into CENS technology. Experiencing internal prototype testing, working on interdisciplinary teams, designing around constraints, leader advocacy, and the intervention of a values advocate all had demonstrable impacts on design decisions. The ethnographic data also point to at least one values lever—navigating institutional mandates—that needed further refinement to effectively change design. This raises a question: what features characterized an <italic>effective</italic> deployment of values levers? Analyzing the values levers deployed at CENS suggests that effective deployments result in: (1) changes in the design conversation; (2) consensus around a social values as relevant and even useful to design; and (3) values-based modifications to the technologies themselves.</p><p>An effective values lever changed the topic of conversation, making values such privacy, consent, equity, and forgetting a part of regular design meeting discussions. Over two years of observation, privacy and consent became regular topics at CENS. Privacy was invoked by internal testing as well as interdisciplinary conversations. Student designers regularly wrestled with consent as they filled out the required data-collection paperwork. Designers weighed memory and forgetting as they worked on data retention plans and system parsimony. Beyond changing the topic of conversation, effective values levers created consensus around social values as relevant and even useful. By making values something that directly applied to design and opened up new spaces of creativity, levers helped values became agreed-upon design criteria. For example, privacy and consent became major drivers for creation of the PDV. The final indicator of an effective values lever was values-based modifications to the technologies under production. At CENS, these ranged from simple anonymization measures built into battery use monitoring software to the complicated sharing filters developed as part of L.’s dissertation.</p></sec><sec id="section13-0162243912436985"><title>Enabling Values Levers</title><p>Values levers contribute to what <xref ref-type="bibr" rid="bibr1-0162243912436985">Agre (1997</xref>) critical technical practice or what <xref ref-type="bibr" rid="bibr20-0162243912436985">Sengers et al. (2005</xref>) refer to as “reflective design”: the process of questioning design as it happens, with the goal of building prosocial, carefully considered technologies. Without the intervention of values levers, algorithms and databases are often discussed as if they are ethically neutral (<xref ref-type="bibr" rid="bibr14-0162243912436985">Los 2006</xref>). By focusing on “neutral” code, developers risk leaving values-based inquiry outside of the scope of their design practice. Values levers emphasize moral values <italic>within </italic>discussions about systems. At CENS, values levers encouraged an environment in which questioning how antisurveillance values related to design was a feature of the lab’s work.</p><p>But in order for values levers to exist, the design lab had to allow for the presence of the practices and agents that deployed these levers. The structure given to a design lab by its membership and everyday work practices matters to the process of translating between social values and technological features. The structure of the CENS laboratory enabled values levers to flourish. Leaders took interdisciplinarity seriously, hiring social scientists and statisticians, giving them space and equipment side-by-side with computer scientists, and encouraging these outsiders to become full members of the design team. Similarly, CENS leaders encouraged laboratory practices, such as internal prototype testing, which enabled values levers. Leaders continually reinforced the importance of internal prototype tests, even though students sometimes complained about the time and resources such testing required. To counter these complaints, leaders provided administrative support to assist students with pilot testing, and often outright required students to test their colleagues’ projects.</p><p>The laboratory structure was also influenced by another important factor: funding for design activities. Money could restrict or expand value-centered design practices by affecting the availability and interplay of levers such as values advocates, interdisciplinary teams, and internal testing. Funding enabled the long-term intervention of an ethics advocate and provided legitimacy and security for my project. The declaration that money is important is hardly surprising, but the ways in which it affects values in design are intricate and worth discussing. There has been little literature focused on the role of funding on design values, although it was hinted at in foundational work by <xref ref-type="bibr" rid="bibr9-0162243912436985">Friedman and Nissenbaum (1997</xref>). The importance of funding to values levers in design at CENS indicates that studying funding cycles and pathways, and their effect on the values expressed during design, may be an interesting avenue for future work.</p></sec><sec id="section14-0162243912436985"><title>Remaining Challenges</title><p>Participatory sensing made for an excellent case study in values in design. Antisurveillance values such as privacy, consent, equity, and forgetting rose to the surface of design quickly due to the user-facing nature of the technologies under development, and the collection of recognizably sensitive data. CENS leaders were committed to antisurveillance values from the beginning of design and their attention helped make these concerns visible. CENS also had a preexisting commitment to interdisciplinary work, incorporating statisticians and social scientists into the team. The visibility of social values at CENS enabled the discovery of values levers and allowed me to explore their effectiveness and implications for design. But by presenting a single case study, this work does not address whether the levers explored here can be replicated in other design settings. Indeed, the levers presented here are likely an incomplete list of values levers even within participatory sensing design.<sup><xref ref-type="fn" rid="fn2-0162243912436985">2</xref></sup> And the conclusions drawn from one academic design laboratory will not hold true for all design situations. But there may be comparable values levers in other design settings, and they can be found by paying attention to practices and agents at work in design. To meet this challenge, this case study suggests concrete activities to examine in design environments.</p><p>Finally, a well-structured design laboratory where values levers reside close to the daily work of engineering addresses only one facet—the design—of just, equitable technologies. Though the affordances embedded by designers are important to technology ethics, contexts of use will change the nature and social import of participatory sensing. Issues of adoption and use are unaddressed by this study focused on design and will be critical to future investigations of pervasive data collection.</p></sec><sec id="section15-0162243912436985"><title>Conclusion</title><p>This study illustrates that the routinized practices of design work shape the choices and priorities of software engineers, and therefore the values incorporated into new technologies. The findings from CENS suggest that values levers, including working on interdisciplinary teams, experiencing internal prototype testing, designing around constraints, advocacy by leaders and a values worker, navigating institutional mandates, and gaining funding, promoted social values in design.</p><p>These findings contrast to current approaches to building values into design, which focus on either classroom education for engineers (e.g., the NSF’s Ethics Education in Science and Engineering program) or bringing outside ethics experts onto design teams (e.g. Ethical, Legal, and Social Impacts grant requirements). While both of these approaches are important and complimentary to this work, this study suggests that we pay attention to the structure of design labs: the space where student or industry designers spend most of their time. The forces of laboratory practice create values levers. Ethnography can call attention to these site-specific levers; further study might generalize types or kinds of levers across design labs. Articulating and amplifying these levers can, in turn, encourage their explicit use within laboratories. Leaders can be aware of their role as advocates and encourage students to participate in internal testing. Values advocates can help normalize discussions of ethics and also facilitate interdisciplinarity by bringing outsiders into the laboratory setting. By encouraging these levers, we can encourage attention to ethics within design.</p><p>As values levers became part of CENS design practice, designers began creating technologies that valued control of personal data, participation by affected community members, and transparency of the sensing process. The design team’s efforts illustrated that laboratory structures and agents can foster interest in antisurveillance values at the design level, and a willingness to address conditions that encourage prosocial sensing technologies. By enabling interdisciplinary laboratories with strong leadership, good data practices, and values advocates, just and equitable systems can be built by design.</p></sec></body><back><fn-group><fn fn-type="other" id="fn5-0162243912436985"><label>Author’s Note</label><p>This work is based on material compiled for the author’s doctoral dissertation, “Building Values into the Design of Pervasive Mobile Technologies.” </p></fn></fn-group><ack><title>Acknowledgments</title><p>The author would like to thank her committee: Jeffrey Burke, Deborah Estrin, Christopher Kelty, Ramesh Srinivasan, and chair Christine Borgman. Their ideas, feedback, and guidance have shaped this work immensely. The helpful feedback of anonymous reviewers has also shaped this article. </p></ack><fn-group><fn fn-type="conflict" id="fn3-0162243912436985"><label>Declaration of Conflicting Interests</label><p>The author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></fn><fn fn-type="financial-disclosure" id="fn4-0162243912436985"><label>Funding</label><p>The author disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was funded by the National Science Foundation under grant number 0832873.</p></fn></fn-group><notes><title>Notes</title><fn-group><fn fn-type="other" id="fn1-0162243912436985"><label>1.</label><p>Because I have identified the design lab, true anonymity for these leaders was impossible to maintain. The leaders identified here have therefore approved the included quotations. And in the spirit of collaborative ethnography, many members of the CENS team have been actively involved in the analysis and critique of these findings.</p></fn><fn fn-type="other" id="fn2-0162243912436985"><label>2.</label><p>At least one other values lever was suggested by data from this study: interactions with system users. Unfortunately because of the timing of the study, I was unable to observe enough interactions between designers and users to properly evaluate the effect of this lever.</p></fn></fn-group></notes><ref-list><title>References</title><ref id="bibr1-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Agre</surname><given-names>Philip E.</given-names></name></person-group> <year>1997</year>. <article-title>“Toward a Critical Technical Practice: Lessons Learned in Trying to Reform AI.”</article-title> In <source>Social Science, Technical Systems and Cooperative Work: The Great Divide</source>, edited by <person-group person-group-type="editor"><name><surname>Bowker</surname><given-names>Geoffrey C.</given-names></name><name><surname>Gasser</surname><given-names>Les</given-names></name><name><surname>Star</surname><given-names>Susan Leigh</given-names></name><name><surname>Turner</surname><given-names>Bill</given-names></name></person-group>, <fpage>131</fpage>–<lpage>158</lpage>. <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Lawrence Erlbaum</publisher-name>.</citation></ref><ref id="bibr2-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Burke</surname><given-names>Jeffrey</given-names></name><name><surname>Estrin</surname><given-names>Deborah</given-names></name><name><surname>Hansen</surname><given-names>Mark</given-names></name><name><surname>Parker</surname><given-names>Andrew</given-names></name><name><surname>Ramanathan</surname><given-names>Nithya</given-names></name><name><surname>Reddy</surname><given-names>Sasank</given-names></name><name><surname>Srivastava</surname><given-names>Mani B.</given-names></name></person-group>. <year>2006</year>. <article-title>“Participatory Sensing.”</article-title> In <source>World Sensor Web Workshop</source>. <publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>ACM</publisher-name>, <comment>accessed January 5, 2008</comment>, <ext-link ext-link-type="uri" xlink:href="http://escholarship.org/uc/item/19h777qd#page-1">http://escholarship.org/uc/item/19h777qd#page-1</ext-link>.</citation></ref><ref id="bibr3-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>A. T.</given-names></name><name><surname>Eisenman</surname><given-names>S. B.</given-names></name><name><surname>Lane</surname><given-names>N. D.</given-names></name><name><surname>Miluzzo</surname><given-names>E.</given-names></name><name><surname>Peterson</surname><given-names>R. A.</given-names></name></person-group>. <year>2006</year>. <article-title>“People-Centric Urban Sensing.”</article-title> In <source>Proceedings of the 2nd Annual International Workshop on Wireless Internet</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation></ref><ref id="bibr4-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cargo</surname><given-names>Margaret</given-names></name><name><surname>Mercer</surname><given-names>Shawna L.</given-names></name></person-group>. <year>2008</year>. <article-title>“The Value and Challenges of Participatory Research: Strengthening Its Practice.”</article-title> <source>Annual Review of Public Health</source> <volume>29</volume> (<issue>April</issue>): <fpage>325</fpage>–<lpage>50</lpage>.</citation></ref><ref id="bibr5-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Catalani</surname><given-names>Caricia</given-names></name><name><surname>Minkler</surname><given-names>Meredith</given-names></name></person-group>. <year>2010</year>. <article-title>“Photovoice: A Review of the Literature in Health and Public Health.”</article-title> <source>Health Education &amp; Behavior</source> <volume>37</volume> (<issue>3</issue>): <fpage>424</fpage>–<lpage>51</lpage>.</citation></ref><ref id="bibr6-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Christin</surname><given-names>Delphine</given-names></name><name><surname>Reinhardt</surname><given-names>Andreas</given-names></name><name><surname>Kanhere</surname><given-names>Salil S.</given-names></name><name><surname>Hollick</surname><given-names>Matthias</given-names></name></person-group>. <year>2011</year>. <article-title>“A Survey on Privacy in Mobile Participatory Sensing Applications.”</article-title> <source>Journal of Systems and Software</source> <volume>84</volume> (<issue>11</issue>): <fpage>1928</fpage>–<lpage>46</lpage>.</citation></ref><ref id="bibr7-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Collier</surname><given-names>Stephen J.</given-names></name><name><surname>Lakoff</surname><given-names>Andrew</given-names></name></person-group>. <year>2005</year>. <article-title>“On Regimes of Living.”</article-title> In <source>Global Assemblages: Technology, Politics and Ethics as Anthropological Problems</source>, edited by <person-group person-group-type="editor"><name><surname>Ong</surname><given-names>Aihwa</given-names></name><name><surname>Collier</surname><given-names>Stephen J.</given-names></name></person-group>, <fpage>22</fpage>–<lpage>39</lpage>. <publisher-loc>Malden, MA</publisher-loc>: <publisher-name>Blackwell</publisher-name>.</citation></ref><ref id="bibr8-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Eisenman</surname><given-names>S. B.</given-names></name><name><surname>Lane</surname><given-names>N. D.</given-names></name><name><surname>Miluzzo</surname><given-names>E.</given-names></name><name><surname>Peterson</surname><given-names>R. A.</given-names></name><name><surname>Ahn</surname><given-names>G. S.</given-names></name><name><surname>Campbell</surname><given-names>A. T.</given-names></name></person-group>. <year>2006</year>. <article-title>“MetroSense Project: People-Centric Sensing at Scale.”</article-title> In <source>Proceedings of the ACM Sensys World Sensor Web Workshop</source>, <issue>October 31</issue>, 6–11. <publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation></ref><ref id="bibr9-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>Batya</given-names></name><name><surname>Nissenbaum</surname><given-names>Helen</given-names></name></person-group>. <year>1997</year>. <article-title>“Bias in Computer Systems.”</article-title> In <source>Human Values and the Design of Computer Technology</source>, edited by <person-group person-group-type="editor"><name><surname>Friedman</surname><given-names>Batya</given-names></name></person-group><fpage>21</fpage>–<lpage>40</lpage>. <publisher-loc>Cambridge and New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation></ref><ref id="bibr10-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Goldman</surname><given-names>Jeffrey</given-names></name><name><surname>Shilton</surname><given-names>Katie</given-names></name><name><surname>Burke</surname><given-names>Jeffrey</given-names></name><name><surname>Estrin</surname><given-names>Deborah</given-names></name><name><surname>Hansen</surname><given-names>Mark</given-names></name><name><surname>Ramanathan</surname><given-names>Nithya</given-names></name><name><surname>Reddy</surname><given-names>Sasank</given-names></name><name><surname>Samanta</surname><given-names>Vidyut</given-names></name><name><surname>Srivastava</surname><given-names>Mani</given-names></name><name><surname>West</surname><given-names>Ruth</given-names></name></person-group>. <year>2009</year>. <source>Participatory Sensing: A Citizen-Powered Approach to Illuminating the Patterns that Shape our World</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Woodrow Wilson International Center for Scholars</publisher-name>.</citation></ref><ref id="bibr11-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hicks</surname><given-names>John</given-names></name><name><surname>Ramanathan</surname><given-names>Nithya</given-names></name><name><surname>Falaki</surname><given-names>Hossein</given-names></name><etal/></person-group> <year>2011</year>. <source>Ohmage: An Open Mobility System for Activity and Experience Sampling</source>. <comment>CENS Technical Report</comment>. <publisher-loc>Los Angeles, CA</publisher-loc>: <publisher-name>Center for Embedded Networked Sensing</publisher-name>, <issue>October</issue>.</citation></ref><ref id="bibr12-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hollander</surname><given-names>Rachelle</given-names></name></person-group>. <year>2009</year>. <source>Ethics Education and Scientific and Engineering Research: What’s Been Learned? What Should Be Done? Summary of a Workshop</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>National Academy of Engineering</publisher-name>.</citation></ref><ref id="bibr13-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Knobel</surname><given-names>Cory P.</given-names></name><name><surname>Bowker</surname><given-names>Geoffrey C.</given-names></name></person-group>. <year>2011</year>. <article-title>“Values in Design.”</article-title> <source>Communications of the ACM</source> <volume>54</volume> (<issue>7</issue>): <fpage>26</fpage>–<lpage>28</lpage>.</citation></ref><ref id="bibr14-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Los</surname><given-names>Maria</given-names></name></person-group>. <year>2006</year>. <article-title>“Looking into the Future: Surveillance, Globalization and the Totalitarian Potential.”</article-title> In <source>Theorizing Surveillance: The Panopticon and Beyond</source>, edited by <person-group person-group-type="editor"><name><surname>Lyon</surname><given-names>David</given-names></name></person-group><fpage>69</fpage>–<lpage>94</lpage>. <publisher-loc>Devon, England</publisher-loc>: <publisher-name>Willan</publisher-name>.</citation></ref><ref id="bibr15-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mehalik</surname><given-names>Matthew M.</given-names></name><name><surname>Gorman</surname><given-names>Michael E.</given-names></name></person-group>. <year>2006</year>. <article-title>“A Framework for Strategic Network Design Assessment, Decision Making, and Moral Imagination.”</article-title> <source>Science, Technology, &amp; Human Values</source> <volume>31</volume> (<issue>3</issue>): <fpage>289</fpage>–<lpage>308</lpage>.</citation></ref><ref id="bibr16-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Miluzzo</surname><given-names>E.</given-names></name><name><surname>Lane</surname><given-names>N. D.</given-names></name><name><surname>Eisenman</surname><given-names>S. B.</given-names></name><name><surname>Campbell</surname><given-names>A. T.</given-names></name></person-group>. <year>2007</year>. <article-title>“CenceMe - Injecting Sensing Presence into Social Networking Applications.”</article-title> <source>Lecture Notes in Computer Science</source> <volume>4793</volume>: <fpage>1</fpage>–<lpage>28</lpage>.</citation></ref><ref id="bibr17-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Mun</surname><given-names>Min</given-names></name><name><surname>Reddy</surname><given-names>Sasank</given-names></name><name><surname>Shilton</surname><given-names>Katie</given-names></name><name><surname>Yau</surname><given-names>Nathan</given-names></name><name><surname>Boda</surname><given-names>Peter</given-names></name><name><surname>Burke</surname><given-names>Jeffrey</given-names></name><name><surname>Estrin</surname><given-names>Deborah</given-names></name><name><surname>Hansen</surname><given-names>Mark</given-names></name><name><surname>Howard</surname><given-names>Eric</given-names></name><name><surname>West</surname><given-names>Ruth</given-names></name></person-group>. <year>2009</year>. <article-title>“PEIR, the Personal Environmental Impact Report, as a Platform for Participatory Sensing Systems Research.”</article-title> In <source>Proceedings of the International Conference on Mobile Systems, Applications, and Services</source>, 55–68<publisher-name>. Krakow</publisher-name>, <publisher-loc>Poland: ACM</publisher-loc>.</citation></ref><ref id="bibr18-0162243912436985"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Mun</surname><given-names>Min</given-names></name><name><surname>Hao</surname><given-names>Shuai</given-names></name><name><surname>Mishra</surname><given-names>Nilesh</given-names></name><name><surname>Shilton</surname><given-names>Katie</given-names></name><name><surname>Burke</surname><given-names>Jeffrey</given-names></name><name><surname>Estrin</surname><given-names>Deborah</given-names></name><name><surname>Hansen</surname><given-names>Mark</given-names></name><name><surname>Govindan</surname><given-names>Ramesh</given-names></name></person-group>. <year>2010</year>. <article-title>“Personal Data Vaults: A Locus of Control for Personal Data Streams.”</article-title> In <source>Co-NEXT ’10 Proceedings of the 6th International Conference</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>ACM</publisher-name>, <comment>accessed December 30</comment>, <ext-link ext-link-type="uri" xlink:href="http://portal.acm.org/citation.cfm?id=1921168.1921191">http://portal.acm.org/citation.cfm?id=1921168.1921191</ext-link>.</citation></ref><ref id="bibr19-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pinch</surname><given-names>Trevor J.</given-names></name><name><surname>Bijker</surname><given-names>Wiebe E.</given-names></name></person-group>. <year>1984</year>. <article-title>“The Social Construction of Facts and Artefacts: Or How the Sociology of Science and the Sociology of Technology might Benefit Each Other.”</article-title> <source>Social Studies of Science</source> <volume>14</volume> (<issue>3</issue>): <fpage>399</fpage>–<lpage>441</lpage>.</citation></ref><ref id="bibr20-0162243912436985"><citation citation-type="web"><person-group person-group-type="author"><name><surname>Sengers</surname><given-names>Phoebe</given-names></name><name><surname>Boehner</surname><given-names>Kirsten</given-names></name><name><surname>David</surname><given-names>Shay</given-names></name><name><surname>Kaye</surname><given-names>Joseph “Jofish”</given-names></name></person-group>. <year>2005</year>. <article-title>“Reflective Design.”</article-title> In <source>Proceedings of the 4th Decennial Conference on Critical Computing: Between Sense and Sensibility</source>, <fpage>49</fpage>–<lpage>58</lpage>. <comment>CC ’05</comment>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM</publisher-name>, <comment>accessed December 16, 2011</comment>, <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/1094562.1094569">http://doi.acm.org/10.1145/1094562.1094569</ext-link>.</citation></ref><ref id="bibr21-0162243912436985"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Shilton</surname><given-names>Katie</given-names></name></person-group>. <year>2010</year>. <article-title>“Technology Development with an Agenda: Interventions to Emphasize Values in Design.”</article-title> In <source>Proceedings of the 73rd Annual Meeting of the American Society for Information Science &amp; Technology (ASIST)</source>. <volume>Vol. 47</volume>. <publisher-name>Pittsburgh</publisher-name>, <publisher-loc>PA, October</publisher-loc>.</citation></ref><ref id="bibr22-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Star</surname><given-names>Susan Leigh</given-names></name></person-group>. <year>2010</year>. <article-title>“This is not a Boundary Object: Reflections on the Origin of a Concept.”</article-title> <source>Science, Technology, &amp; Human Values</source> <volume>35</volume> (<issue>5</issue>): <fpage>601</fpage>–<lpage>17</lpage>.</citation></ref><ref id="bibr23-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Star</surname><given-names>Susan Leigh</given-names></name><name><surname>Griesemer</surname><given-names>James R.</given-names></name></person-group>. <year>1989</year>. <article-title>“Institutional Ecology, ‘Translations’ and Boundary Objects: Amateurs and Professionals in Berkeley’s Museum of Vertebrate Zoology, 1907-39.”</article-title> <source>Social Studies of Science</source> <volume>19</volume> (<issue>3</issue>): <fpage>387</fpage>–<lpage>420</lpage>.</citation></ref><ref id="bibr24-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Suchman</surname><given-names>Lucy</given-names></name><name><surname>Blomberg</surname><given-names>Jeanette</given-names></name><name><surname>Orr</surname><given-names>Julian E.</given-names></name><name><surname>Trigg</surname><given-names>Randall</given-names></name></person-group>. <year>1999</year>. <article-title>“Reconstructing Technologies as Social Practice.”</article-title> <source>American Behavioral Scientist</source> <volume>43</volume> (<issue>3</issue>): <fpage>392</fpage>–<lpage>408</lpage>.</citation></ref><ref id="bibr25-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Poel</surname><given-names>van de Ibo</given-names></name><name><surname>Verbeek</surname><given-names>Peter-Paul</given-names></name></person-group>. <year>2006</year>. <article-title>“Editorial: Ethics and Engineering Design.”</article-title> <source>Science, Technology, &amp; Human Values</source> <volume>31</volume> (<issue>3</issue>): <fpage>223</fpage>–<lpage>36</lpage>.</citation></ref><ref id="bibr26-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Verbeek</surname><given-names>Peter-Paul</given-names></name></person-group>. <year>2006</year>. <article-title>“Materializing Morality.”</article-title> <source>Science, Technology, &amp; Human Values</source> <volume>31</volume> (<issue>3</issue>): <fpage>361</fpage>–<lpage>80</lpage>.</citation></ref><ref id="bibr27-0162243912436985"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Winner</surname><given-names>Langdon</given-names></name></person-group>. <year>1980</year>. <article-title>“Do Artifacts Have Politics?”</article-title> <source>Daedalus</source> <volume>109</volume> (<issue>1</issue>): <fpage>121</fpage>–<lpage>36</lpage>.</citation></ref></ref-list></back></article>